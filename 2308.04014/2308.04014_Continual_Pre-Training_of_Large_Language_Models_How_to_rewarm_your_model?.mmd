[MISSING_PAGE_FAIL:1]

rate to keep training a pre-trained language model on new data. We refer to this as _re-warming_ the model. Re-warming the model should improve learning efficiency by avoiding a vanishing learning rate. We study warm-up strategies on Pythia 410M model with various amounts of data, maximum learning rates and different pre-trained checkpoints. This would allow a model trained initially on a large dataset to benefit from resuming training on a newer large dataset without having to retrain from scratch. In order to simulate this setting, we fix our initial pre-training dataset to be Pile and the newer dataset to be SlimPajama. We hope that this may guide the adaptation of existing LLMs to future new datasets.

Our results show that:

1. Progressively increasing the learning rate to warm-up is not necessary but starting directly from the maximum learning rate creates an initial large spike in the loss (chaotic phase a.k.a stability gap) with no consequences later.
2. Adjusting the maximum learning rate can help trade-off between upstream and downstream performance; increasing the maximum learning rate leads to stronger adaptation to the downstream dataset (SlimPajama), while smaller learning rates preserve more performance on the upstream dataset (Pile).
3. Continual pre-training with the latest pre-trained checkpoint improves performance.

## 2 Setup

In our setup, the upstream (or pre-training) dataset is the Pile (Gao et al., 2020). The downstream (or fine-tuning) dataset is SlimPajama (Soboleva et al., 2023). SlimPajama is an extensively deduplicated version of RedPajama (Together.xyz, 2023) which is built based on the LLama dataset (Touvron et al., 2023). In this work, we use "fine-tuning" and downstream continual pre-training interchangeably. However, in our continual pre-training setting, we note that the downstream dataset is on the scale of the previous pre-training dataset (i.e. very large, unlike many fine-tuning datasets).

The SlimPajama dataset is built from similar sources as the Pile but with a higher quantity of data. Therefore, some upstream data may be repeated during downstream pre-training. Our experimental setup is comparable to the setup of (Ash and Adams, 2020), where they train a classifier on half of the samples of a dataset first, and fine-tune it later on all samples. They show that warm starting for image classification is challenging. Using a model pre-trained on the Pile and continuing the pre-training on SlimPajama, we follow an analogous setup for causal language modeling.

**Datasets -** We use the Pile with the same weights as Black et al. (2022) for validation. We shuffle and randomly sample the SlimPajama dataset (Soboleva et al., 2023) to form the \(\sim\)297B token training dataset and \(\sim\)316M validation token dataset. We do not use replay. We use the same tokenizer as (Black et al., 2022) that is trained specifically on the Pile.

**Model -** We use the 410M Pythia pre-trained on the Pile (Biderman et al., 2023), i.e. GPT-NeoX (Black et al., 2022) models. We do not use flash attention (Dao et al., 2022).

**Hyperparameters -** We use the AdamW optimizer with \(\beta_{1}=0.9,\beta_{2}=0.95\), \(\epsilon=10^{-8}\), and a weight decay of \(0.1\). The maximum learning rate is varied in our experiments \(\{1.5\cdot 10^{-4},3\cdot 10^{-4},6\cdot 10^{-4}\}\). We use cosine learning rate decay to a minimum of \(0.1\cdot MaxLr\). All warmup lengths are calculated based on the full downstream dataset size (297B tokens). We note that our cosine decay schedule reaches the minimum learning rate at \(240\)B tokens and is constant thereafter. We set gradient clipping to \(1.0\). Training is conducted at half-precision (FP16), without dropout.

## 3 Related Work

**Large Language Models:** LLMs are usually trained with Adam (e.g., GPT3 (Brown et al., 2020), BLOOM (Scao et al., 2022), Gopher (Rae et al., 2021), Pythia (Biderman et al., 2023)) or AdamW (e.g., Chinchilla (Hoffmann et al., 2022), LLaMA (Touvron et al., 2023)). In all the aforementioned models, the learning rate schedule consists of a warm-up followed by a cosine decay to 10% of the maximum learning rate.

**Unsupervised Continual Learning:** In this paper, we investigate various warm-up strategies for the continual pre-training of LLMs. Continual pre-training uses a similar type of training objectives as continual self-supervised training. Self-supervised pre-training was also studied in vision datasets for image generation (Seff et al., 2017; Lesort et al., 2019; Zhai et al., 2019; Nguyen et al., 2018; Davari et al., 2022) or representation learning (Fini et al., 2022; Madaan et al., 2021; Rao et al., 2019). In language, continual pre-training was studied under the name of domain adaptation

\begin{table}
\begin{tabular}{l r r r} \hline \hline Dataset & Sampling \% & Train & Val \\ \hline StackExchange & 2.0 & 9.95B & 13.08M \\ Arxiv & 2.5 & 13.77B & 22.73M \\ Wikipedia & 4.5 & 11.78B & 15.79M \\ Book & 4.5 & 14.22B & 22.04M \\ Github & 4.5 & 15.41B & 22.42M \\ C4 & 15.0 & 78.49B & 72.49M \\ Commoncrawl & 67.0 & 153.25B & 147.28M \\ \hline Totals & 100 & 296.86B & 315.83M \\ \hline \hline \end{tabular}
\end{table}
Table 1: Token counts and train data weights for our subsampled version of SlimPajama.

pre-training (Ke et al., 2023; Scialom et al., 2022; Gururangan et al., 2021; Qin et al., 2022) where the new dataset comes from a new domain. Another setting is where different datasets are generated at different points in time (Han et al., 2021; Jin et al., 2022; Jang et al., 2021, 2022; Loureiro et al., 2022). In our setup, the scenario is closer to domain adaptation pre-training, because we do not take into account the temporality of data.

**Monitoring Learning Rate for Continual Training of Language Models:** In continual learning (CL), models are trained on sequences of datasets. Therefore, the data is not _independent and identically distributed_ which can lead the model to lose plasticity or forget. In such situations, particular monitoring of the learning rate schedule can be beneficial. In CL of language models (Caccia et al., 2021; Ke et al., 2023a; Loureiro et al., 2022; Han et al., 2021; Loshchilov and Hutter, 2018; Scialom et al., 2022; Winata et al., 2023) different approaches have been evaluated: constant learning rate (Ke et al., 2023a; Scialom et al., 2022), progressive decrease (Winata et al., 2023) or warm-up then decrease (Caccia et al., 2021).

However, to the best of our knowledge, no existing work studies specifically the influence of the warm-up phase in the context of continual pre-training for large language models.

## 4 Continual Warm-up

### How long to warm up?

In the literature, warm-up is usually conducted on at most 1% of the data (Zhao et al., 2023). In this experiment, we investigate if the results are sensitive to this hyper-parameter.

**Setup:** We experiment with different warm-up lengths for a schedule of 297B tokens: 0%, 0.5%, 1%, and 2% of the data and measure the performance after the first 50B tokens. From a different perspective, we could see this experiment as running a 1% warm-up on different amounts of data. We hypothesize that warming up for a larger number of iterations could lead to a smoother transition with subsequent performance improvements.

**Results:** The results of this experiment are provided in Fig. 1. They show that the amount of data used for warming up the learning rate does not significantly influence the perplexity on the downstream task (learning) or the upstream task (forgetting). These results invalidate our hypothesis that using more tokens for warm-up can smooth the transition and show that linear warmup is useless in this setting. Nevertheless, the model trained without any progressive warm up experiences an initial "chaotic phase" causing a spike in the loss in its first few iterations of training, this phenomenon is also referred to as stability gap (Lange et al., 2023; Caccia et al., 2022).

**Takeaway 1:**

* The length of the warmup phase does not appear to have a significant effect on the Pile and SlimPajama validation losses.

### How high to warm up?

One objective of re-warming the learning rate is to enable compute-efficient continual pre-training. A learning rate that is too small may lead to inefficient learning on the downstream dataset, whereas, a learning rate that is too large may lead to catastrophic forgetting of the upstream dataset. One important aspect of re-warming the learning rate is to decide how high to increase it. Therefore, in this experiment, we vary the maximum learning rate to assess its effect on performance.

**Setup:** We fix the length of the warm-up phase to the default amount of \(1\%\) of the training data and vary the maximum learning rate. We experiment with the default value of \(3\cdot 10^{-4}\) used for pre-training Pythia 410M (Biderman et al., 2023), \(1.5\cdot 10^{-4}\), and \(6\cdot 10^{-4}\). For the post-warmup cosine decay phase, we set the final learning rate to \(10\%\) of the

Figure 1: (_top_) Evolution of perplexity on SlimPajama while fine-tuning with various amounts of tokens for warm-up. (_bottom_) perplexity on the same experiments on the Pile validation set (upstream). \(MaxLr=3\cdot 10^{-4}\), \(MinLr=0.1\cdot MaxLr\). This figure shows that at that scale, the length of the warm-up phase does not significantly influence results.

maximum learning rate. The learning rate schedule we used decays to the minimum learning rate at \(240\)B tokens and is constant thereafter. The runs are reported to the end of \(240\)B tokens (the end of decay period).

**Results:** The results of this experiment are provided in figures 2, 3, and 4. We observe, at the end of training, that larger maximum learning rates improve performance on downstream data, while they hurt performance on upstream data. Conversely, a smaller maximum learning rate improves performance on upstream data, while limiting adaptation to downstream data--causing decreased performance. These findings show that altering the maximum learning rate can be an effective way to tradeoff between downstream and upstream performance. Additionally, we observe a general trend: fine-tuning on SlimPajama, causes the model to forget what has been learned on the Pile leading to an increase in the Pile validation perplexity. Finally, we note that employing early stopping on the model trained from a constant learning rate (similar to traditional fine-tuning) is an economical way of adapting to the new data distribution while retaining strong performance on the upstream dataset.

**Takeaway 2:**

* Rewarning then decaying the learning rate appears necessary to learn well on the downstream task. Moreover, while keeping a constant learning is initially advantageous on Pile, this advantage vanishes when training long enough on SlimPajama.
* A model that only learns on SlimPajama performs worse on SlimPajama than models pre-trained on Pile in spite of being optimised solely for the downstream task, highlighting positive transfer between the two datasets.

### Comparing with from Scratch Training

In this experiment, we want to compare finetuned models with models trained from scratch.

**Setup:** We train a model from random initialization using the same cosine decay schedule as the _MaxLr_\(=3\cdot 10^{-4}\) model in Section 4.2.

**Results:** As we can see in Fig. 2 and Fig. 3, all the finetuned models with a warm-up perform better than the model

Figure 4: Perplexity downstream vs perplexity upstream, RP finetuning. Green points refer to the ends of the warm-up phases. The red point represents the perplexity before starting the downstream fine-tuning. Increasing the maximum learning rate improves performance on the downstream data, but causes forgetting on the upstream. This plot reports the same results as figures 2 and 3.

Figure 3: Evolution of loss on Pile for different maximum learning rates. The blue curve reports a model trained from scratch. Growing the maximum learning rate consistently increases the final loss on upstream data, i.e. it increases forgetting. The from-scratch baseline consistently improves its performance on Pile, while being trained on SlimPajama, showing the significant synergy between both datasets.

Figure 2: Evolution of loss on SlimPajama for different maximum learning rates. The blue curve reports a model trained from scratch. Growing the maximum learning rate consistently decreases the final loss on downstream data. At convergence, the models being continually pre-trained outperform the scratch and constant LR baselines. However, the constant learning rate model achieves best performance within the first 100B tokens.

trained from scratch. This shows that finetuning instead of retraining might improve performance even when the downstream dataset is on the scale of the upstream dataset and overlaps with the upstream dataset. We also observe that, after \(200\)B tokens, the model trained from scratch performs better than the model finetuned using a constant learning rate.

### Re-warming on the same data

In the previous experiments we have seen that finetuning on new data leads to a quick increase of loss on past data, that decrease later. The increase is higher when the max learning rate is bigger. One hypothesis for the increase in loss is that the distribution shift between upstream and downstream data disturbs the training process. To assess this hypothesis, we apply our warm-up policy in a setting with no distribution shift. That is, we replicate our experiments from figures 3 and 4 by fine-tuning on Pile.

**Setup:** In this experiment, instead of fine-tuning on SlimPajama data, we fine-tune on 50B tokens of the Pile data with the same parametrization of the warm-up policy as Sec. 4.2 experiments.

**Results:** Fig. 5, shows that re-warming the learning rate while continuing to pre-train on the Pile has a similar effect as re-warming on SlimPajama data Fig. 3 when looking at the downstream validation loss. This suggests that the distribution shift between Pile and SlimPajama is not solely to blame for the negative impact of re-warming the learning rate observed in sec. 4.2, and that the optimization dynamics also plays a role in this increase of loss.

Fig. 6 shows that the training first increases perplexity on both the Pile and SlimPajama data but reduces after on both. Interestingly, Fig. 6 show a linear relationship between SlimPajama perplexity and the Pile perplexity when fine-tuning on the Pile, while it was not the case while fine-tuning on SlimPajama (Fig. 3). One possible explanation for this relationship is that models trained on Pile climb out of a minimum during warmup and return towards the same minimum as the learning rate is decayed, yielding the linear trend.

**Takeaway 3:**

* Rewarning the learning rate appears to be a significant cause for the degradation of performance seen previously when starting to learn on the downstream task, as evidenced by re-warming then decaying the learning rate while training on the same dataset.
* The models do not appear to be able to recover from the performance hit due to rewarming the learning rate when training on the same dataset.

### Evaluating Earlier Checkpoints

**Setup:** We select three checkpoints from model pre-training to test if warm-up strategies benefit from starting with non-converged checkpoints. Our hypothesis is that selecting checkpoints farther from convergence may benefit adaptation to the downstream task as these checkpoints may be located at more favorable points in the loss landscape.

To select significantly different checkpoints, we compare the last pre-training checkpoint (i.e. Pythia 410M after \(143,000\)iters), to an earlier checkpoint achieving a Pile validation loss near the maximum Pile validation loss attained by all models in Fig. 1 (bottom) (\(\sim 2.5\)), and a third checkpoint in between the two other checkpoints.

Figure 5: Pile validation loss while fine-tuning again on the Pile. Warm-up phenomenon observed in Sec. 4.2 is also observed applied to fine-tuning again on the same data distribution. Warm-up token=\(1\%\) downstream tokens, \(MinLr=0.1\cdot MaxLr\).

Figure 6: Perplexity on the Pile vs perplexity on SlimPajama when fine-tuning on the Pile with various maximum learning rates. Warm-up token=\(1\%\) downstream tokens, \(MinLr=0.1\cdot MaxLr\). Green points refer to the end of the warm-up phase.

**Results:** The evolution of the validation losses on SlimPajama are provided in Fig. 7 and the evolution of the validation losses on the Pile is provided in appendix A. We see in Fig. 7 that, in our setup, selecting earlier checkpoints for later fine-tuning does not lead to improvement in downstream performance. Therefore, selecting the latest checkpoint is the best option. We can conclude that the pre-training did not lead the model into a loss of plasticity that would make the model difficult to re-warm.

**Local conclusion:** The experiments conducted in this section led to the conclusion that re-warming the pre-trained model on new data is a challenging task, even when the downstream data is of similar provenance to the upstream data. Our results show that the amount of tokens used for warm-up does not significantly alter performance, growing the maximum learning rate improves downstream performance of the final model while decreasing it improves upstream performance, and selecting earlier checkpoints decreases performance on both upstream and downstream data.

**Takeaway 4:**

* Using an earlier checkpoint when pretraining on the Pile does not lead to learning faster on SlimPajama.

## 5 Discussion / Limitation

**Data similarity and overlapping:** In our experimental setup, upstream and downstream data have a high similarity, notably because of data overlap. Since in continual learning, different types of shifts can lead to variations in performance (Lesort et al., 2021), our results may not generalize to setups with different distribution shifts, such as language domain adaptation pre-training setups (Xu et al., 2019; Gururangan et al., 2020; Ke et al., 2023; Chakrabarty et al., 2019; Ke et al., 2023). Nevertheless, comparing Fig. 4 and Fig. 6, we see that the results are not identical when fine-tuning on the Pile or when fine-tuning on SlimPajama. A possible explanation is that even a slight shift in data distribution can lead to a significant perturbation of the learning dynamics. For example, in the context of image classification, Igl et al. (2020) show how a sudden transition of 10 to 20 % of the labels in the dataset can have a significant impact on the downstream performance (see Fig. 5 of (Igl et al., 2020)).

**Experiments Scale:**

As described in Sec. 2, our investigation explores models of size 410M and fine-tuning dataset of size 297B tokens. While this is a preliminary study, in future work, we plan to verify whether our conclusions hold at different model scales (e.g., 3B and 7B) and different dataset scales (e.g., 100B and 600B). Moreover, we plan to test our models throughout using benchmarks such as HELM (Liang et al., 2022) or Harness (Gao et al., 2021) instead of only loss or perplexity, as these benchmarks can provide important insight into the evolution of model capabilities.

## 6 Conclusion

Our experiments demonstrate that warming up to higher maximum learning rates helps models pre-trained on the Pile adapt to SlimPajama, while a smaller maximum learning rate preserves performance on the pile. In both cases, however, models that are rewarmed improve over models trained from scratch. These results motivate the use of continual pre-training on new datasets rather than restarting training from scratch. More research is needed, however, to establish similar results for larger model scales, different distribution shifts, and verify that this strategy can be applied repeatedly to update models.

## Software and Data

GPT-NeoX (Andonian et al., 2021), DeepSpeed (Rasley et al., 2020), nccl (NVIDIA, 2016), Apex (NVIDIA, 2019), Pytorch (Paszke et al., 2017), HuggingFace Transformers library (Wolf et al., 2020).

## Acknowledgements

We acknowledge the support from Canada CIFAR AI Chair Program and from the Canada Excellence Research Chairs Program. We would also like to acknowledge funding from the FRQNT Doctoral (B2X) scholarship [B.T.], the scholarship for Artificial Intelligence of Universite de Montreal's

Figure 7: Pile validation loss of models trained from the fully converged checkpoint, the upstream saturation point, and \(1/2\) of the upstream saturation point. Black colour designs for the earlier checkpoint, red colour the latest checkpoint and blue colour the in-between one.

Etudes Superieures et Postdoctorales, and a fellowship of the IFI program of the German Academic Exchange Service (DAAD).This research was made possible thanks to the computing resources on the Summit supercomputer, provided as a part of the INCITE program award "Scalable Foundation Models for Transferable Generalist AI". These resources were provided by the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725.

## References

* A. Andonian, Q. Anthony, S. Biderman, S. Black, P. Gali, L. Gao, E. Hallahan, J. Levy-Kramer, C. Leahy, L. Nestler, K. Parker, M. Pieler, S. Purohit, T. Songz, W. Phil, and S. Weinbach (2021-08) GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch. Note: [https://www.github.com/eleutherai/gpt-neox](https://www.github.com/eleutherai/gpt-neox) Cited by: SS1.
* J. Ash and R. P. Adams (2020) On warm-starting neural network training. Advances in neural information processing systems33, pp. 33884-3894. Cited by: SS1.
* S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O'Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, and A. Pythia (2023) A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373. Cited by: SS1.
* S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy, K. McDonell, J. Pian, M. Pieshanth, U. S. Purohit, L. Reynolds, J. Tow, B. Wang, and S. Weinbach (2022) GPT-neox-20b: an open-source autoregressive language model. External Links: 2204.01373 Cited by: SS1.
* T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, pp. 1877-1901. External Links: Link, Document Cited by: SS1.
* L. Caccia, J. Xu, M. Ott, M. Ranzato, and L. Denoyer (2021) On anytime learning at macroscale. arXiv preprint arXiv:2106.09563. Cited by: SS1.
* L. Caccia, R. Aljundi, N. Asadi, T. Tuytelaars, J. Pineau, and E. Belilovsky (2022) New insights on reducing abrupt representation change in online continual learning. In International Conference on Learning Representations, External Links: Link Cited by: SS1.
* T. Chakrabarty, C. Hidey, and K. McKeown (2019) IMHO fine-tuning improves claim detection. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Minneapolis, Minnesota, pp. 558-563. External Links: Link, Document Cited by: SS1.
* T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Re (2022) Flashattention: fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems35, pp. 16344-16359. Cited by: SS1.
* M. Davari, N. Asadi, S. Mudur, R. Aljundi, and E. Belilovsky (2022) Probing representation forgetting in supervised and unsupervised continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16712-16721. Cited by: SS1.
* M. Farajtabar, N. Azizan, A. Mott, and A. Li (2020) Orthogonal gradient descent for continual learning. In International Conference on Artificial Intelligence and Statistics, pp. 3762-3773. External Links: Link, Document Cited by: SS1.
* E. Fini, V. G. T. da Costa, X. Alameda-Pineda, E. Ricci, K. Alahari, and J. Mairal (2022) Self-supervised models are continual learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9621-9630. Cited by: SS1.
* R. M. French (1999) Catastrophic forgetting in connectionist networks. Trends in Cognitive Sciences3 (4), pp. 128-135. External Links: ISSN 13646613, Document Cited by: SS1.
* L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, B. Wang, and A. Zou (2020) The pile: an 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Cited by: SS1.
* L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell, N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou (2021) A framework for few-shot language model evaluation. Note: [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628) Cited by: SS1.
* S. Gururangan, A. Marasovic, S. Swayamdipta, K. Lo, I. Beltagy, D. Downey, and N. A. Smith (2020) Don't stop pretraining: adapt language models to domains and tasks. arXiv preprint arXiv:2004.10964. External Links: Link, 2004.10964 Cited by: SS1.

Gururangan, S., Lewis, M., Holtzman, A., Smith, N. A., and Zettlemoyer, L. Demix layers: Disentangling domains for modular language modeling. _arXiv preprint arXiv:2108.05036_, 2021. URL [https://arxiv.org/abs/2108.05036](https://arxiv.org/abs/2108.05036).
* Han et al. (2021) Han, R., Ren, X., and Peng, N. ECONET: Effective continual pretraining of language models for event temporal reasoning. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 5367-5380, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.436. URL [https://aclanthology.org/2021.emnlp-main.436](https://aclanthology.org/2021.emnlp-main.436).
* Hoffmann et al. (2022) Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022. URL [https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556).
* Igl et al. (2020) Igl, M., Farquhar, G., Luketina, J., Boehmer, W., and Whiteson, S. The impact of non-stationarity on generalisation in deep reinforcement learning. _arXiv preprint arXiv:2006.05826_, 2020. URL [https://arxiv.org/abs/2006.05826.pdf](https://arxiv.org/abs/2006.05826.pdf).
* Jang et al. (2021) Jang, J., Ye, S., Yang, S., Shin, J., Han, J., Kim, G., Choi, S. J., and Seo, M. Towards continual knowledge learning of language models. _arXiv preprint arXiv:2110.03215_, 2021. URL [https://arxiv.org/abs/2110.03215](https://arxiv.org/abs/2110.03215).
* Jang et al. (2022) Jang, J., Ye, S., Lee, C., Yang, S., Shin, J., Han, J., Kim, G., and Seo, M. Temporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models. 2022.
* Workshop on Challenges & Perspectives in Creating Large Language Models_, pp. 1-16, May 2022. doi: 10.18653/v1/2022.bigscience-1.1. URL [https://aclanthology.org/2022.bigscience-1.1](https://aclanthology.org/2022.bigscience-1.1).
* Ke et al. (2023a) Ke, Z., Shao, Y., Lin, H., Konishi, T., Kim, G., and Liu, B. Continual pre-training of language models. In _The Eleventh International Conference on Learning Representations_, 2023a. URL [https://openreview.net/forum?id=m_GDIItaI3o](https://openreview.net/forum?id=m_GDIItaI3o).
* Ke et al. (2023b) Ke, Z., Shao, Y., Lin, H., Xu, H., Shu, L., and Liu, B. Adapting a language model while preserving its general knowledge. _arXiv preprint arXiv:2301.08986_, 2023b.
* Kirillov et al. (2023) Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., Dollar, P., and Girshick, R. Segment anything. _arXiv:2304.02643_, 2023.
* Kirkpatrick et al. (2017) Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. _Proc. of the national academy of sciences_, 2017. URL [https://www.pnas.org/content/pnas/114/13/3521.full.pdf](https://www.pnas.org/content/pnas/114/13/3521.full.pdf).
* Lange et al. (2023) Lange, M. D., van de Ven, G. M., and Tuytelaars, T. Continual evaluation for lifelong learning: Identifying the stability gap. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=2y350cRstc6](https://openreview.net/forum?id=2y350cRstc6).
* International Joint Conference on Neural Networks_, Budapest, Hungary, Jul 2019. URL [https://hal.archives-ouvertes.fr/hal-01951954](https://hal.archives-ouvertes.fr/hal-01951954).
* Lesort et al. (2021) Lesort, T., Caccia, M., and Rish, I. Understanding continual learning settings with data distribution drift analysis. _arXiv preprint arXiv:2104.01678_, 2021.
* Lesort et al. (2023) Lesort, T., Ostapenko, O., Rodriguez, P., Arefin, M. R., Misra, D., Charlin, L., and Rish, I. Challenging common assumptions about catastrophic forgetting. 2023.
* Liang et al. (2022) Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., et al. Holistic evaluation of language models. _arXiv preprint arXiv:2211.09110_, 2022.
* Loshchilov & Hutter (2018) Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In _International Conference on Learning Representations_, 2018. URL [https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101).
* Loureiro et al. (2022) Loureiro, D., Barbieri, F., Neves, L., Espinosa Anke, L., and Camacho-collados, J. TimeLMs: Diachronic language models from Twitter. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations_, pp. 251-260, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-demo.25. URL [https://aclanthology.org/2022.acl-demo.25](https://aclanthology.org/2022.acl-demo.25).
* Madaan et al. (2021) Madaan, D., Yoon, J., Li, Y., Liu, Y., and Hwang, S. J. Representational continuity for unsupervised continual learning. In _International Conference on Learning Representations_, 2021.

* Mirzadeh et al. (2020) Mirzadeh, S. I., Farajtabar, M., Pascanu, R., and Ghasemzadeh, H. Understanding the role of training regimes in continual learning. _Advances in Neural Information Processing Systems_, 33:7308-7320, 2020.
* Nguyen et al. (2018) Nguyen, C. V., Li, Y., Bui, T. D., and Turner, R. E. Variational continual learning. In _International Conference on Learning Representations_, 2018. URL [https://arxiv.org/abs/1710.10628](https://arxiv.org/abs/1710.10628).
* NVIDIA (2016) NVIDIA. NVIDIA Collective Communication Library (NCCL). [https://docs.nvidia.com/deeplearning/sdk/nccld-developer-guide/docs/index.html](https://docs.nvidia.com/deeplearning/sdk/nccld-developer-guide/docs/index.html), 2016. Accessed: September 8, 2023.
* NVIDIA (2019) NVIDIA. Pytorch extension with NVIDIA-maintained utilities to streamline mixed precision and distributed training. [https://nvidia.github.io/apex/](https://nvidia.github.io/apex/), 2019. Accessed: September 8, 2023.
* Oquab et al. (2023) Oquab, M., Darcet, T., Moutakanni, T., Vo, H. V., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., Howes, R., Huang, P.-Y., Xu, H., Sharma, V., Li, S.-W., Galuba, W., Rabbat, M., Assran, M., Ballas, N., Synnaeve, G., Misra, I., Jegou, H., Mairal, J., Labatut, P., Joulin, A., and Bojanowski, P. Dinov2: Learning robust visual features without supervision, 2023.
* Ostapenko et al. (2022) Ostapenko, O., Lesort, T., Rodriguez, P., Arefin, M. R., Douillard, A., Rish, I., and Charlin, L. Continual learning with foundation models: An empirical study of latent replay, 2022.
* Paszke et al. (2017) Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. Automatic Differentiation in PyTorch. 2017.
* Qin et al. (2022) Qin, Y., Zhang, J., Lin, Y., Liu, Z., Li, P., Sun, M., and Zhou, J. Elle: Efficient lifelong pre-training for emerging data. _arXiv preprint arXiv:2203.06311_, 2022. URL [https://arxiv.org/abs/2203.06311](https://arxiv.org/abs/2203.06311).
* Rae et al. (2021) Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et al. Scaling language models: Methods, analysis & insights from training gopher. _arXiv preprint arXiv:2112.11446_, 2021. URL [https://arxiv.org/abs/2112.11446](https://arxiv.org/abs/2112.11446).
* Rao et al. (2019) Rao, D., Visin, F., Rusu, A. A., Teh, Y. W., Pascanu, R., and Hadsell, R. Continual unsupervised representation learning. 2019. URL [https://arxiv.org/pdf/1910.14481.pdf](https://arxiv.org/pdf/1910.14481.pdf).
* Rasley et al. (2020) Rasley, J., Rajbhandari, S., Ruwase, O., and He, Y. Deep-speed: System optimizations enable training deep learning models with over 100 billion parameters. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pp. 3505-3506, 2020.
* Rebuffi et al. (2017) Rebuffi, S.-A., Kolesnikov, A., Sperl, G., and Lampert, C. H. icarl: Incremental classifier and representation learning. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pp. 2001-2010, 2017. URL [https://arxiv.org/abs/1611.07725](https://arxiv.org/abs/1611.07725).
* Scao et al. (2022) Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilic, S., Hesslow, D., Castagne, R., Luccioni, A. S., Yvon, F., Galle, M., et al. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_, 2022. URL [https://arxiv.org/abs/2211.05100](https://arxiv.org/abs/2211.05100).
* Scialom et al. (2022) Scialom, T., Chakrabarty, T., and Muresan, S. Fine-tuned language models are continual learners. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pp. 6107-6122, 2022.
* Seff et al. (2017) Seff, A., Beatson, A., Suo, D., and Liu, H. Continual learning in generative adversarial nets. _CoRR_, abs/1705.08395, 2017. URL [http://arxiv.org/abs/1705.08395](http://arxiv.org/abs/1705.08395).
* Soboleva et al. (2023) Soboleva, D., Al-Khateeb, F., Myers, R., Steeves, J. R., Hestness, J., and Dey, N. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. [https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama](https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama), 2023. URL [https://huggingface.co/datasets/cerebras/SlimPajama-627B](https://huggingface.co/datasets/cerebras/SlimPajama-627B).
* Together (2023) Together.xyz. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data).
* Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023. URL [https://arxiv.org/abs/2302.13971](https://arxiv.org/abs/2302.13971).
* Winata et al. (2023) Winata, G. I., Xie, L., Radhakrishnan, K., Wu, S., Jin, X., Cheng, P., Kulkarni, M., and Preotiuc-Pietro, D. Overcoming catastrophic forgetting in massively multilingual continual learning. _arXiv preprint arXiv:2305.16252_, 2023.
* Wolf et al. (2022) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. Transformers: State-of-the-Art Natural Language Processing. pp.

38-45. Association for Computational Linguistics, October 2020. URL [https://www.aclweb.org/anthology/2020.emnlp-demos.6](https://www.aclweb.org/anthology/2020.emnlp-demos.6).
* Xu et al. (2019) Xu, H., Liu, B., Shu, L., and Yu, P. S. Bert post-training for review reading comprehension and aspect-based sentiment analysis. _arXiv preprint arXiv:1904.02232_, 2019.
* Yang et al. (2022) Yang, G., Hu, E. J., Babuschkin, I., Sidor, S., Farhi, D., Pachocki, J., Liu, X., Chen, W., and Gao, J. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. In _NeurIPS 2021_, March 2022. URL [https://www.microsoft.com/en-us/research/publication/tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/](https://www.microsoft.com/en-us/research/publication/tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/).
* Zhai et al. (2019) Zhai, M., Chen, L., Tung, F., He, J., Nawhal, M., and Mori, G. Lifelong gan: Continual learning for conditional image generation. In _Proceedings of the IEEE/CVF international conference on computer vision_, pp. 2759-2768, 2019.
* Zhao et al. (2023) Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of large language models. _arXiv preprint arXiv:2303.18223_, 2023. URL [https://arxiv.org/abs/2303.18223](https://arxiv.org/abs/2303.18223).

[MISSING_PAGE_EMPTY:11]