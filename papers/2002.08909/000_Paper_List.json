{
    "2002.08909": {
        "paper_id": "2002.08909",
        "abs_url": "https://arxiv.org/abs/2002.08909",
        "pdf_url": "https://arxiv.org/pdf/2002.08909.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2002.08909_REALM_Retrieval-Augmented_Language_Model_Pre-Training.pdf",
        "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Kelvin Guu",
            "Kenton Lee",
            "Zora Tung",
            "Panupong Pasupat",
            "Ming-Wei Chang"
        ],
        "abstract": "We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.",
        "comments": "",
        "official_code_urls": [
            "https://github.com/google-research/language/tree/master/language/realm"
        ],
        "pwc_page_url": "https://paperswithcode.com/paper/realm-retrieval-augmented-language-model-pre",
        "bibtex": "@misc{guu2020realm,\n      title={REALM: Retrieval-Augmented Language Model Pre-Training}, \n      author={Kelvin Guu and Kenton Lee and Zora Tung and Panupong Pasupat and Ming-Wei Chang},\n      year={2020},\n      eprint={2002.08909},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
    }
}