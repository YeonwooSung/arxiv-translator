# 효율적인 언어 모델 학습

중간을 채웁니다.

Mohammad Bavarian

&Heewoo Jun1

&Nikolas Tezak

John Schulman

Christine McLeavey

Jerry Tworek

&Mark Chen

OpenAI

동등한 기여, 무작위 배정 respondence to: mobav@openai.com, heewoo@openai.com.

###### Abstract

우리는 자기회귀 언어 모델이 문서 중간에서 끝까지 텍스트의 범위를 이동하는 데이터 세트에 간단한 변환을 적용한 후 텍스트를 채우는 방법을 배울 수 있음을 보여준다. 이 데이터 증강은 최근 몇 년 동안 많은 관심을 얻었지만, 우리는 이러한 방식으로 변환된 데이터의 많은 부분을 가진 훈련 모델이 광범위한 규모에 걸쳐 복잡성과 샘플링 평가에 의해 측정된 원래 왼쪽에서 오른쪽 생성 능력에 해를 끼치지 않는다는 광범위한 증거를 제공한다. FIM(Fiddle-In-the-middle)을 채우기 위한 학습 모델의 유용성, 단순성 및 효율성을 고려할 때, 우리는 미래의 자기 회귀 언어 모델이 기본적으로 FIM으로 학습될 것을 제안한다. 이를 위해 데이터 변환 빈도, 변환의 구조 및 채움 스팬 선택 방법과 같은 주요 하이퍼 파라미터에 대한 일련의 삭제를 실행한다. 이러한 삭제를 사용하여 강력한 기본 설정과 모범 사례를 규정하여 FIM 모델을 훈련한다. API의 모범 사례로 훈련된 최상의 주입 모델을 출시했으며 향후 연구를 돕기 위해 주입 벤치마크를 출시했다.

###### Contents

* 1 소개
	* 1.1 우리의 기여
* 2 평가
	* 2.1 자기회귀 평가
	* 2.2 충진 평가
* 3 FIM 훈련 및 추론
	* 3.1 SPM 모드
	* 3.2 Context-level FIM
* 4 사전 트레이닝 결과
	* 4.1 다운스트림 벤치마크에서의 좌우 기능 평가
	* 4.2 FIM rate
	* 4.3 SPM vs PSM vs joint SPM+PSM training
	* 4.4 Context-level vs document-level FIM
	* 4.5 중간 범위 선택
* 5 핀튜닝 결과
*6 논의
* 7 관련 작업
* 8 결론
	* 8.1 권장 FIM 하이퍼파라미터
	* 8.2 미래 방향
* 아키텍처 및 데이터 세트
* FIM 레이트 삭제에 대한 B 스케일링 경향
* FIM 구현의 C 세부사항
* SPM 인코딩의 D 세부사항
* E 랜덤스팬 인플루언서 벤치마크
* F 다이내믹스와 핀튜닝의 학습 곡선
* G Top 모델 비교
* H 정성적 평가
* H.1 성공적인 주입 예
* H.2 한계
* H.3 Mitigations

## 1 Introduction

Transformer(Vaswani et al., 2017)의 도입에 이어, 다양한 인터넷 규모 데이터 세트에 대해 훈련된 대형 언어 모델(LLMs)이 괄목할 만한 성공을 거두었다. 이 모델은 자연어 프롬프트가 주어지면 일관되고 합리적인 완성도를 생성할 수 있으며 읽기 이해, 질문 응답, 논리적 추론 및 상식 추론을 포함한 많은 벤치마크에서 최첨단 성능을 달성한다.

변압기 기반 언어 모델에는 몇 가지 광범위한 클래스가 있다 : BERT와 같은 인코더 전용 모델(Devlin 등, 2019)은 일반적으로 마스킹된 언어 모델링 목표로 훈련되고, T5와 같은 인코더-디코더 모델(Raffel 등, 2019)은 일반적으로 스팬 예측 목표로 훈련된다(Song 등, 2019). 마지막으로, GPT 모델 시리즈(Radford et al., 2018, 2019; Brown et al., 2020)와 같은 인과적 디코더 기반 언어 모델은 좌-우 다음 토큰 예측 목표를 사용하여 트레이닝된다. GPT-3, Codex, LaMDA, GLaM, PaLM, Gopher, Jurassic-1, 및 Chinchilla(Brown et al., 2020; Chen et al., 2021; Thoppilan et al., 2022; Du et al., 2021; Chowdhery et al., 2022; Rae et al., 2021; Lieber et al., 2021; Hoffmann et al., 2022)와 같은 오늘날 가장 크고 가능한 생성 언어 모델은 후자의 클래스 모델에 속한다. 가장 큰 규모에서 인과적 디코더 기반 모델들의 압도적인 인기는 개방형 텍스트 생성, 인-컨텍스트 학습(소수의 샷 프라이밍 사용), 사전 트레이닝 계산 효율(Wang et al., 2022), 및 성공적인 스케일업에서의 어느 정도 역사적 선행성(Brown et al., 2020)에 기인한다. 이들 모델은 또한 작업 특정 피니튜닝 없이 구조적으로 더 간단하고 일반적으로 더 효과적이어서 추론 및 배포에 더 매력적이다.

모든 모델 클래스는 채우는 것과 관련하여 제한되며, 여기서 모델은 프롬프트 내의 특정 위치에서 텍스트를 생성하는 반면 접두사와 접미사 모두에서 컨디셔닝된다. 좌우 모델은 접두사에서만 조건을 지정할 수 있습니다. 인코더 전용 및 인코더-디코더 모델이 접미사 상에서 컨디셔닝할 수 있는 반면, 트레이닝 시간에 보여지는 인필 영역들의 길이는 일반적으로 실제에서 유용한 것보다 훨씬 짧다. 이것은 주입이 생성 시점 전후에 맥락이 있는 애플리케이션에서 자연적으로 발생하기 때문에 불행한 일이다. 예를 들어, 코딩 어시스턴트를 생성함에 있어서, 인필링은 docstring 생성, import statement 생성을 위해, 또는 부분적으로 기입된 기능을 완성하기 위해 사용될 수 있다.

본 연구의 목표는 현재 대규모 언어 모델링을 위한 가장 지배적인 패러다임인 인과적 디코더 기반 언어 모델에 FIM(fill-in-the-middle) 기능을 추가하여 이러한 한계를 해결하는 것이다(Brown et al., 2020; Hoffmann et al., 2022; Chowdhery et al., 2022). 우리는 학습 데이터에 대한 간단한 수정과 모델 아키텍처를 변경하지 않고도 인과적 디코더 기반 자기 회귀(AR) 언어 모델이 _그들의 정상적인 왼쪽에서 오른쪽 생성 능력을 손상시키지 않고_ 채움을 학습할 수 있음을 보여준다.

도 1: FIM은 무료로 학습할 수 있다. 우리는 자연어와 코드의 두 도메인에 대해 50% 및 0% FIM 비율로 언어 모델을 사전 훈련하고 모든 최종 스냅샷의 테스트 손실을 평가한다. 모든 모델은 100B 데이터 토큰으로 학습됩니다. FIM 모델은 원본 데이터를 50%만 보고 모델이 새로운 기능을 학습하고 있음에도 불구하고, 원본 좌/우 손실 추세가 동일하게 유지됨에 따라 공동 FIM 훈련은 비용이 발생하지 않는다는 것을 관찰한다. FIM이 없는 부동산에 대한 자세한 증거는 그림 3을 참조하십시오.

섹션 3에 설명된 접근법의 핵심은 데이터 세트의 일부에 적용되는 변환으로, 문서를 무작위로 세 조각으로 나누고 중간 조각을 끝으로 이동한다.

\[\text{document}\rightarrow\big{(}\text{prefix},\text{middle},\text{suffix} \big{)}\rightarrow\big{(}\text{prefix},\text{suffix},\text{middle}\big{)}\]

그런 다음 센티넬 토큰을 사용하여 세 조각을 연결합니다. 이는 (Donahue et al., 2020; Aghajanyan et al., 2022; Fried et al., 2022)에서 사용된 절차와 유사하다.

이전 작업과 비교하여 우리의 작업은 FIM 모델을 훈련하는 것의 계산 효율성을 강조한다. 이러한 강조는 훈련하기에 매우 비싸고 상당한 에너지 풋프린트를 갖는 매우 큰 언어 모델을 훈련하는 것에 대한 관심의 증가를 고려할 때 중요하다. 일반적으로 언어 모델에 새로운 목표 또는 능력을 추가할 때 가장 중요한 질문은 기존 능력과 계산 효율성 절충에 대한 영향이라고 생각한다.

다중 목적 및 데이터 세트에 대해 공동으로 훈련하는 대부분의 경우와 달리, FIM 변환된 데이터와 일반적인 좌우 데이터의 혼합물에 대해 공동으로 훈련된 모델이 중간 채우기 방법을 학습하면서 동일한 좌우 기능을 달성한다는 것을 보여준다. 우리는 이것을 FIM 무료 부동산이라고 부릅니다.

다음에서 우리는 FIM 모델이라는 용어를 사용하여 FIM 변환된 데이터와 정상적인 왼쪽에서 오른쪽 데이터의 혼합물에 대해 훈련된 모든 모델을 참조한다. 우리는 FIM 데이터(즉, 0% FIM 비율) 없이 훈련된 모델을 AR 모델로 지칭한다.

### Our contributions

이 논문에서 우리의 중심 기여는 다음과 같다.

* **FIM-for-free 속성**: FIM이 있거나 없는 8개 모델 제품군을 훈련하여 광범위한 크기 조정 연구를 수행하고 사전 훈련에서 왼쪽에서 오른쪽 기능을 손상시키지 않고 FIM을 학습할 수 있음을 보여줍니다. 우리는 복잡성과 샘플링 기반 벤치마크를 모두 사용하여 코드와 언어 모두에서 이 주장을 조사한다.
* **사전 훈련에서 FIM에 대 한 모범 사례**: 포괄적인 삭제를 사용 하 여 FIM 모델을 훈련 하는 것과 관련 된 많은 하이퍼 매개 변수의 영향을 명확히 합니다. 특히, FIM 비율(데이터에 FIM 변환이 적용될 확률), FIM 변환의 다양한 변형 및 중간 범위의 선택에 대해 연구한다.
* **비효율성 피니튜닝**: FIM 모델을 처음부터 학습하는 것의 대안은 기존 언어 모델을 피니튜닝하여 이 기능을 학습하는 것입니다. 우리는 FIM을 사용한 피니튜닝이 계산적으로 비효율적이라는 것을 보여준다. FIM은 프리트레이닝 동안 무료로 학습될 수 있지만, 핀튜닝 동안 FIM을 학습하는 것은 프리트레이닝과 유사한 수준의 성능에 도달하기 위해 상당한 양의 추가 컴퓨팅을 필요로 한다.

그림 2: FIM 테스트 손실을 사용하여 그림 1에서 동일한 모델 스캔의 주입 능력 평가. FIM(노란색)으로 훈련된 모델은 기준선(보라색) AR 모델보다 낮은 FIM 테스트 손실을 얻는다. 이는 FIM 모델이 실제로 접미사에 대한 조건을 학습하는 동시에 중간 섹션을 예측하여 FIM 테스트 세트에서 더 낮은 테스트 손실을 달성할 수 있음을 보여준다. 그림 1과 2는 함께 FIM 모델이 동일한 좌-우 자기회귀 손실을 달성하지만 FIM 손실은 낮기 때문에 AR 모델보다 엄격하게 더 나은 것으로 간주될 수 있음을 나타낸다.

* **새 채우기 벤치마크.** 모델의 생성 기능을 연구하기 위해 자유 형식 생성 샘플의 정확성을 평가해야 합니다. 이를 위해 긴 FIM 샘플의 정확성을 평가하기 위해 단위 테스트를 사용할 수 있는 코드에 중점을 둔다. 특히, HumanEval(Chen et al., 2021)의 정준해의 비어있지 않은 라인을 제거하여 (Fried et al., 2022)에서 소개한 단일 라인 및 다중 라인 채움 벤치마크를 사용한다. 그러나 라인 기반 평가는 FIM의 모든 사용 사례를 포착하지 못하기 때문에, 우리는 _랜덤 스팬 채움_과 _랜덤 스팬 채움 광_이라는 두 가지 새로운 벤치마크를 생성한다. 우리는 이러한 벤치마크와 평가 방법론에 대해 섹션 2에서 더 일반적으로 논의한다.
* **샘플링 평가 필요**. 섹션 4.2, 4.4 및 부록 B에서 FIM 훈련에서 다양한 하이퍼파라미터를 변경하면 FIM 테스트 손실에서는 무시할 수 있는 차이가 있지만 샘플링 기반 벤치마크에서는 큰 차이가 발생하는 경우가 많다는 것을 발견했다. 샘플링 벤치마크는 실제 사용 사례에 더 가까울 뿐만 아니라 테스트 손실을 사용하여 놓칠 수 있는 별도 이득을 놀릴 수 있는 것으로 판단된다. 이는 종종 스케일링 법칙 분석이 테스트 손실에만 의존하기 때문에 중요한 발견이며, 다른 평가와 함께 추가되지 않으면 오판의 소지가 있다는 것을 발견했다.

위의 첫 번째 글자와 세 번째 글자를 대조하는 것은 흥미롭다. 첫 번째는 프리트레이닝에서 FIM을 배우는 것은 무료이지만 피니튜닝에 맡기는 것은 놀라울 정도로 비용이 많이 든다는 것이다. 섹션 6에서 이 발견에 대한 잠재적인 설명에 대해 논의한다. FIM-for-free 속성을 설정하기 위해 다양한 규모에 걸쳐 코드와 언어 모두에 대한 절제 연구를 수행한다. FIM이 있거나 없는 50M에서 6.9B 매개변수로 8개의 모델을 훈련하고 다양한 자기회귀 벤치마크에서 성능을 비교한다. 특히, 우리는 100B 토큰에 대한 코드에서 16개의 모델을 학습하고 100B 토큰에 대한 자연어에서 또 다른 16개의 모델을 학습한다. 정상 자기 회귀 좌-우 언어 모델링 테스트 손실 측면에서 이러한 모델의 비교는 그림 1에 나와 있다. 두 도메인 모두에서 FIM 모델은 비 FIM 모델과 유사한 AR 테스트 손실을 달성한다.

섹션 4에서 손실 기반 벤치마크에 대한 FIM과 AR 모델을 비교하여 FIM-for-free 속성에 대한 더 많은 증거를 제공한다. 또한 섹션 4.2에서 FIM-for-free 속성의 _더 강한 형태_가 있음을 알 수 있다. 최종 검문소에서 FIM 훈련으로 인한 자기 회귀 능력에서 히트가 없을 뿐만 아니라 훈련 전반에 걸쳐 동일하게 적용된다. 이는 그림 4와 5의 좌-우 손실 및 HumanEval 평가에서 AR과 FIM 모델 간의 매칭 학습 곡선에 의해 입증된다.

FIM 훈련이 좌우 능력에 미치는 영향을 연구하는 것 외에도 모델이 실제로 FIM 훈련에서 채우는 것을 배우고 있음을 보여주는 것도 중요하다. 그림 2는 FIM 테스트 손실의 맥락에서 이에 대한 증거를 제공한다. 섹션 4와 부록 H에서 모델의 주입 기능을 보다 광범위하게 연구한다.

## 2 Evaluation

AR 및 FIM 평가 벤치마크를 모두 사용하여 모델의 성능을 분석합니다. 바닐라 AR 평가는 FIM 훈련이 좌-우 능력에 미치는 영향을 정량화하는 데 중요하며 섹션 1.1의 FIM-for-free 속성을 시연할 수 있다. FIM 평가는 FIM 훈련에 대한 다양한 하이퍼파라미터의 영향을 이해하고 스케일링 경향을 이해하는 데 중요하다.

논문 전체에서 우리는 AR과 좌우라는 용어를 혼용하여 사용한다. AR 손실은 정상 좌우 데이터에 대한 교차 엔트로피 손실과 100% FIM 변환 데이터에 대한 손실로 FIM 손실을 나타낸다. 모든 테스트 손실은 토큰 단위당 nats입니다. 모든 샘플링 기반 벤치마크에서, 우리는 0.95의 핵 파라미터를 갖는 핵 샘플링(Holtzman 등, 2020)을 사용한다.

### Autoregressive evaluation

모든 도메인에 대해 표준 자기회귀 차수에서 테스트 손실을 평가하여 FIM 증강에도 학습 곡선과 스케일링 경향이 동일하게 유지됨을 보여준다. 테스트 손실 외에도 표준 벤치마크에 대해 평가하여 모델의 기능이 FIM 훈련의 영향을 받지 않음을 보여준다. 자연어는 상식 추론에 PIQA(Bisk et al., 2020), Winograd(Levesque et al., 2012), WinoGrande(Sakaguchi et al., 2021), 읽기 이해에 DROP(Dua et al., 2019) 및 QuAC(Choi et al., 2018), 완성 작업에 HellaSwag(Zellers et al., 2019), LAMBADA(Paperno et al., 2016), StoryCloze(Mostafazadeh et al., 2016)를 사용한다. DROP 및 QuAC를 제외한 모든 벤치마크는 몇 번의 샷 프롬프트로 평가된다. 코드의 경우 HumanEval(Chen et al., 2021)에서 통과율을 측정한다.

### Infilling evaluation

FIM 테스트를 만들기 위해 FIM 비율이 100%인 AR 테스트 세트의 예제에 FIM 변환을 적용한다. FIM 및 AR 테스트 세트에서 동일한 기본 예를 사용하면 FIM 및 AR 테스트 손실을 비교할 수 있다. 또한 중간 범위 토큰의 손실만 측정하는 이러한 테스트 세트의 마스킹된 버전을 만듭니다. 후자의 테스트 세트를 사용하여 FIM 모델에 대한 \(P(\text{middle}|\text{prefix},\text{suffix})\)와 AR 모델에 대한 \(P(\text{middle}|\text{prefix})\)를 측정하여 접미사를 조건화할 수 있어 FIM 모델이 얻는 정보의 양을 조사할 수 있다.

생성적 채움 기능을 위해 우리는 클로즈 스타일의 자연어 벤치마크에서 흔히 볼 수 있는 단일 또는 소수의 토큰 세대와 대조적으로 자유 형태 생성에 관심이 있기 때문에 코드에 중점을 둔다. 코드로 작업하는 것의 이점은 개방형 세대의 긴 샘플을 평가할 때에도 테스트 슈트를 사용하여 작업에서 샘플의 정확성을 평가할 수 있다는 것이다.

우리가 사용하는 샘플링 기반 채우기 벤치마크는 모두 HumanEval(Chen et al., 2021)의 표준 솔루션에서 중간 스팬을 제거하여 생성된 부분 함수 완성 작업이다. 특히, HumanEval의 표준 솔루션에서 비어 있지 않은 라인의 다른 스팬이 FIM 태스크로 전환되는 (Fried et al., 2022)에 의해 제안된 단일 라인 및 다중 라인 채우기 벤치마크를 사용한다. 또한, 랜덤 스팬 인필링2라는 새로운 벤치마크를 생성하는데, 각 HumanEval 문제에 대해 정규 해로부터 중간 스팬을 랜덤으로 균일하게 선택하여 인필링 작업을 생성한다. 모델이 하이라이트된 섹션(또는 동일한 목표를 달성하는 대안적 완성)을 예측해야 하는 아래의 그러한 태스크의 예를 보여준다. 자세한 내용은 부록 E를 참조한다.

각주 2: [https://www.github.com/openai/human-eval-infilling](https://www.github.com/openai/human-eval-infilling)에서 릴리스됨

def unique(l: list):  """Return sorted unique elements in a list  >>> unique([5, 3, 5, 2, 3, 3, 9, 0, 123])  [0, 2, 3, 5, 9, 123]  """  Return sorted(list(set(l))) single-line, multi-line, random span infilling together constitute our infilling benchmark suite. 이러한 벤치마크에는 각각 1033개, 5815개 및 1640개의 작업이 있습니다. 우리는 이것이 원래 HumanEval 데이터 세트(164개 작업)의 작업 수보다 훨씬 더 커서 평가의 분산을 줄인다는 점에 주목한다. 그러나 모델의 최종 스냅샷에서 이러한 벤치마크를 평가할 때 분산을 추가로 줄이기 위해 작업당 최소 100~200개의 샘플을 취한다. 또한 HumanEval 문제당 하나의 무작위 FIM 작업과 164개의 작업만 사용 하 여 더 작은 버전의 무작위 범위 채우기 _light_ 를 사용 하 여 교육 중 채우기 기능 추세를 추적 합니다.

섹션 3에서 우리는 FIM이 PSM과 SPM으로 표시된 두 가지 다른 방식으로 준비될 수 있음을 발견한다. PSM의 사용이 결론을 변경하는 경우를 제외하고 간결성을 위해 SPM 주입 결과만 보고한다.

## 3 FIM 교육 및 추론

우리는 데이터 세트에 적용된 무작위 변환을 사용하여 FIM을 구현한다. 문서 레벨과 컨텍스트 레벨의 두 가지 다른 구현으로 실험을 한다. 이 둘 사이의 차이는 FIM 변환이 데이터 로딩 파이프라인의 어느 단계에서 발생하는가이다. 이러한 선택은 긴 문서가 많은 문맥으로 깨질 수 있거나, 문서가 작을 때 문맥이 여러 문서를 포함할 수 있기 때문에 자연스럽게 발생한다. 먼저 문서 수준 사례를 설명한 다음 섹션 3.2에서 컨텍스트 수준 FIM을 구현하는 데 필요한 변경 사항을 설명한다.

문서 수준 FIM에서 **FIM 비율** 이라고 하는 특정 확률 \(p\) (메인 모델 집합에 \(p=0.5\)을 사용)을 사용 하 여 각 문서를 접두사, 중간 및 접미사의 세 부분으로 자릅니다. 우리는 문서가 여전히 문자의 시퀀스일 때 토큰화 전에 이 분할을 수행한다. 우리는 무작위로 균일하게 분할하는데, 이는 접두사, 중간 및 접미사의 길이가 예상대로 전체 문서의 각각 1/3임을 의미한다.

그런 다음 세 섹션 각각을 별도로 인코딩하고 각 섹션의 시작 부분에 센티널 토큰을 미리 제공합니다. 우리는 이러한 센티넬 토큰을 <pre>, <mid>, <suf>로 표시한다. 마지막으로, FIM 문서의 토큰화된 버전을 형성하기 위해 그들의 센티넬 토큰과 함께 이 섹션들을 순서 접두사, 접미사 및 중간으로 연결하며,

<pre> \[\circ\] Enc(prefix) \[\circ\] <suf> \[\circ\] Enc(suffix) \[\circ\] <mid> \[\circ\] Enc(middle), (PSM)

여기서 \(\circ\)은 연접을 의미한다. FIM이든 AR이든 다른 문서는 <eot>과 연결되어 훈련 중에 모델에 제공된다. 우리는 세 섹션 접두사, 중간 및 접미사 모두에서 손실을 유지하므로 FIM 훈련이 자기 회귀 학습 신호의 감소를 일으키지 않는다는 것을 반복한다. 예비 실험은 여기에 보고되지 않았지만 이 선택이 FIM이 없는 속성을 유지하는 데 중요하다는 것을 시사한다. 이 속성은 센티널이 마스킹 되었는지 여부를 변경 하지 않습니다. 그러나 접미사에 성공적으로 조인 하는 신호를 표시 하기 때문에 항상 <eot> 토큰에서 훈련 하는 것이 중요 합니다.

추론을 위해, 우리는 주어진 접두사와 접미사를 인코딩하고 모델을 프롬프트한다.

<pre> \[\circ\] Enc(prefix) \[\circ\] <suf> \[\circ\] Enc(suffix) \[\circ\] <mid>.3 (PSM inference)

우리는 모델이 접두사와 접미사를 연결하는 방식인 <eot> 토큰을 생성할 때까지 모델에서 샘플링을 계속한다.

모델이 합리적인 할당 추론 토큰 예산 내에서 <eot> 토큰을 생성하지 못하면, 종종 모델이 접두사와 접미사를 연결하는 데 어려움을 겪고 있다는 사인이고, 결과 샘플들은 종종 품질이 더 나빠질 것이며, 이는 EOT 인식 베스트 오브 n 샘플링의 절차에 동기를 부여한다. 자세한 내용은 부록 H를 참조하십시오.

### SPM mode

또한 접두사, 접두사, 중간으로의 순서 변경을 강조하기 위해 SPM이라고 하는 접두사와 접미사의 순서를 바꾸는 위의 절차의 변형도 소개한다. SPM을 도입하기 위한 주요 동기는 추론 중에 향상된 키-값 캐싱이다. 이러한 이점이 있는 이유는 SPM을 사용 하 여 접두사에 토큰을 추가 하면 접미사 섹션에서 계산 된 키 및 값이 더 이상 무효화 되지 않습니다. SPM 캐싱의 우월성은 보편적이지 않고 애플리케이션들에 의존할 수 있다는 점에 유의한다. 특히 SPM 모드에서는 접미사에 대 한 사소한 변경으로 인해 접두사에 대 한 캐시가 무효화 되지만 실제 워크로드의 접두사 변경보다 접미사에 대 한 변경 내용이 드물 것으로 예상 됩니다. 흥미롭게도 캐싱 장점 외에도 섹션 4.3에서 SPM은 실제로 채우기 벤치마크에서 PSM보다 약간 우위를 가지고 있음을 발견했다.

메인 실행에서는 PSM 모드에서는 50% 확률로, SPM 모드에서는 50% 확률로 FIM 변환을 적용하여 추론에서 두 가지 형식의 포맷팅을 모두 처리할 수 있다. 즉, 각 모드는 전체 FIM 레이트 \(p\)의 절반을 상속한다. PSM 및 SPM에 대한 공동 훈련의 이러한 선택을 제거하고 순수한 PSM 및 SPM 실행과 비교한다. 표 1의 결과는 이 선택의 효능을 보여준다.

SPM 모드의 개념은 간단하지만, SPM과 PSM에서 공동으로 훈련할 때 특히 중요한 센티넬 토큰의 배치와 관련하여 몇 가지 미묘한 점이 있다. 우리는 부록 D에서 이러한 미묘함을 설명한다.

### Context-level FIM

언어 모델 트레이닝에서, 문서들은 종종 <eot>로 지칭되는 경계 토큰과 결합되고, 그 후 모델 컨텍스트 길이로 청크된다. 긴 문서들에 FIM을 적용할 때, 이 동작은 전체 접두사 또는 접미사가 청크링 동안 문맥에서 잘라낼 수 있는 단편화된 FIM 데이터를 초래할 수 있다. 이 문제를 해결하기 위해 청킹 단계 후에 FIM을 적용할 수 있습니다. 컨텍스트 슬라이스는 <eot> 경계 토큰과 결합된 다수의 문서들을 그들 내에 가질 수 있다. 따라서, 우리는 <eot>을 기반으로 분할하고, 문서들 중 일부를 FIM 레이트에 의해 주어진 확률로 FIM 예들로 변환하고, 예들을 <eot>과 다시 결합한다. 그 다음, 결과 슬라이스는 모델 컨텍스트 길이로 트리밍된다. FIM 변환에 대한 자세한 내용은 부록 C를 참조한다. 섹션 4.4에서 우리는 이 기술이 문서 수준 FIM에 비해 성능을 향상시킬 수 있음을 보여주고 이 작업에서 모든 주요 FIM 실행에서 컨텍스트 수준 FIM을 채택한다.

## 4 사전 학습 결과

1.1절에서는 FIM이 좌-우 능력에 아무런 영향 없이 학습될 수 있다는 FIM-for-free 속성에 대해 논의하였다. 우리는 이 결과에 대한 더 많은 증거를 제시함으로써 이 섹션을 시작한다. 다음으로, FIM 비율, PSM vs SPM vs joint training, context vs document-level FIM, 그리고 중간경간의 선택을 포함한 FIM 훈련의 하이퍼파라미터에 대해 연구한다. FIM은 AR 능력의 관점에서 자유롭지만, FIM 능력 자체는 이러한 하이퍼파라미터에 강하게 의존한다. 우리는 단위 테스트를 사용하여 생성된 샘플의 정확성을 측정할 수 있는 코드 도메인에서 이러한 선택을 연구한다.

달리 명시되지 않는 한 모델은 100B 토큰의 고정 지평선으로 훈련됩니다. 주요 검색의 경우 부록 A에 설명된 8개 모델을 모두 사용합니다. 섹션 4.2, 4.4 및 부록 B와 같은 보다 광범위한 검색의 경우 계산 비용을 제한하기 위해 더 짧은 수평선으로 훈련된 모델의 하위 집합을 사용합니다.

도 3: 자연어(상단) 및 코드(하단) 도메인에 대한 표준 벤치마크에 대한 성능 비교. 다음-토큰 예측과 FIM의 합동 훈련은 모델이 원래 능력에 영향을 주지 않고 새로운 채우기 작업을 학습할 수 있게 한다. 이것은 FIM이 없는 부동산에 대한 추가 증거를 제공한다.

### 다운스트림 벤치마크에서 왼쪽에서 오른쪽 기능 평가

자연어 및 코드 도메인에 대한 50% FIM 증강 유무에 관계없이 처음부터 50M에서 6.9B 매개변수로 일련의 모델을 훈련한다. 그림 1은 FIM 모델이 데이터를 반 시간 동안 원래 형태로 보고 동시에 새로운 기술을 학습하고 있음에도 불구하고 왼쪽에서 오른쪽 테스트 손실은 영향을 받지 않음을 보여준다.

그러나, 아래에서 설명하는 바와 같이(섹션 4.2 및 섹션 4.4 참조) 테스트 손실을 고려하는 것만으로는 충분하지 않은 경우가 많다. 위의 결과를 강화하기 위해, 우리는 일련의 표준 다운스트림 벤치마크에 대한 모델을 평가하는데, 그 결과는 그림 3에 나와 있다. 우리는 다시 합동 FIM 프리트레이닝이 자연어와 코드 모두에 대한 성능 오차 내에서 일치하기 때문에 표준 AR 벤치마크에서 어떠한 열화도 초래하지 않는다는 것을 발견한다.

### FIM rate

도 1 및 도 3으로부터, 50%의 FIM 비율이 좌-우 능력에서 히트하는 성능을 발생시키지 않는다는 것을 알 수 있다. 이것은 자연스럽게 몇 가지 질문을 제기합니다.

* FIM-for-free는 더 높은 FIM 비율에서도 유지되나요? 그렇다면 좌-우 역량의 저하 없이 FIM 비율을 얼마나 높일 수 있는가?
* FIM 속도가 높을수록 FIM 기능이 강화되나요? 아니면 이점은 한계점 이후에 포화되는가?

이 섹션에서는 이러한 질문에 답하기 위해 FIM 비율을 줄인다. 50B 토큰에 대해 FIM 요금(0, 0.25, 0.5, 0.75, 0.9, 1.0)으로 6개의 대형 모델(표 3 참조)을 훈련합니다. 결과는 그림 4와 5에 나와 있다. 그림 4의 왼쪽 그림은 FIM 비율이 최대 90%라도 좌-우 능력의 저하를 일으키지 않는다는 증거를 제공한다. 그러나 100% FIM 비율로 일반적인 AR 테스트 손실의 뚜렷한 징후가 있다. HumanEval의 경우 그림 5의 왼쪽 그림은 FIM 비율에 관계없이 모든 모델이 유사한 성능을 가지고 있음을 보여준다.

반면에, 우리는 FIM 비율이 충전 능력에 상당한 영향을 미친다는 것을 발견했다. 그림 4에서 더 높은 FIM 비율로 인한 FIM 복잡도의 이득은 무시할 수 있지만, 이 비율을 증가시키면 그림 5의 오른쪽 그림과 같이 채우기 통과율이 일관되게 향상된다. 이는 모델의 FIM 기능을 조사하기 위해 테스트 손실과 같은 언어 모델링 복잡도 측정을 고려하는 것이 충분하지 않지만 비손실 기반 평가도 고려해야 함을 나타낸다.

부록 B에서 더 높은 FIM 비율이 충전 성능을 향상시키지만 이 이득이 복잡성 평가에 반영되지 않는다는 다양한 규모에 걸쳐 추가 증거를 보여준다.

여기와 부록 B에서 결과를 감안할 때 FIM 비율이 90% 이상이 아닌 50%인 핵심 일련의 모델을 훈련하는 이유에 대해 의문을 제기하는 것은 당연하다. FIM 비율이 90%인 모델을 보여준다.

도 4: 50B 토큰에 대해 상이한 FIM 레이트로 트레이닝된 큰(표 3 참조) 모델의 학습 곡선의 비교. 최대 90%의 FIM 비율도 좌-우 테스트 손실에 눈에 띄는 영향을 미치지 않지만 100%의 FIM 비율에서는 저하가 있다. 또한 왼쪽 그림에서 _더 강한 FIM 속성_ 을 볼 수 있습니다. FIM 비율이 100% 미만인 모든 실행은 원래 왼쪽에서 오른쪽으로 테스트 손실을 매우 밀접하게 따릅니다.

FIM-for-free 속성을 유지하면서 성능이 우수합니다. 이것은 FIM 비율 절제 결과를 보기 전에 이미 주요 시리즈를 훈련시켰고 더 높은 비율로 모든 모델을 재훈련하는 데 엄청난 비용이 들었기 때문에 주로 우연한 일이었다.

각주 4: 특히 손실만을 기준으로 한 초기 삭제는 FIM 비율을 90%로 증가시키면 얻을 수 있는 이득은 무시할 수 있어야 하며 결과적으로 50%의 더 적당한 값을 선택하는 것으로 나타났다. 3개의 주입 벤치마크를 모두 사용한 보다 자세한 연구는 실제로 더 높은 FIM 비율을 사용하는 데 눈에 띄는 이득이 있음을 보여주었다.

이 결과는 이 규모에서 현재까지 가장 강력한 주입 모델을 얻기 위해 코드에서 FIM 비율이 90%인 두 번째 6.9B FIM 모델을 훈련하도록 동기를 부여했다. 결과 비교는 표 4에서 찾을 수 있다. 그러나 그림 13에서 FIM 비율이 50%인 것은 최적에서 너무 멀어 보이지 않는다는 점에 주목한다.

### SPM vs PSM vs joint SPM+PSM training

섹션 3에서 FIM 예제 구성 방법은 \(\left[\text{suffix},\text{prefix},\text{middle}\right]\)와 \(\left[\text{prefix},\text{suffix},\text{middle}\right]\)입니다. 여기서는 이러한 선택이 사전 훈련 및 평가 동안 성능에 어떤 영향을 미치는지 연구한다.

주요 발견은 그림 6에 의해 입증된 바와 같이 SPM이 일반적으로 벤치마크에서 PSM보다 약간 더 강하다는 것이다. 우리는 PSM과 SPM에 동등하게 할당된 FIM 비율로 50%의 FIM 비율을 갖는 일련의 FIM 모델을 훈련한다. SPM 모드에서 이러한 모델을 평가하는 것이 규모에 걸쳐 PSM보다 일관되게 더 높은 성능을 산출한다는 것을 발견했다. 이는 SPM에서 접두사와 중간 구간이 하나의 연속된 텍스트 시퀀스이기 때문에 구분이 없기 때문일 수 있다. 이렇게 하면 주의가 먼저 스팬 토큰이 있는 위치를 식별 해야 하는 PSM과 달리 모델이 접두사에서 계속 되는 것이 더 자연스럽습니다.

도 5: 태스크당 온도 0.8 및 25 샘플로 코딩 벤치마크의 실행 중 평가. 더 높은 FIM 비율을 사용하는 것은 인간 평가 성능에 눈에 띄는 영향을 미치지 않는다. FIM 비율이 높을수록 라이트 랜덤 스팬 주입 벤치마크에서 더 강한 주입 능력을 보여준다.

그림 6: SPM 모드는 규모에 따른 성능에서 약간의 이점을 보여준다. 이 도표의 모든 평가는 단일 라인 및 다중 라인 주입의 경우 작업당 온도 0.2 및 100 샘플이고 무작위 스팬 주입의 경우 작업당 200 샘플이다.

[MISSING_PAGE_FAIL:11]

### Context-level vs document-level FIM

섹션 3에서 우리는 FIM을 구현하는 두 가지 방법, 즉 컨텍스트 수준과 문서 수준 FIM에 주목했는데, 여기서 증강은 패킹 및 청킹 전후에 적용된다. 우리는 이제 50% FIM 비율과 기본 결합 PSM-SPM 믹스로 훈련된 일련의 코드 모델에 대한 이러한 선택을 제거한다.

그림 7에서 우리는 컨텍스트 수준 FIM이 모든 규모 범위에 걸쳐 문서 수준 FIM에 비해 일관되고 유의미한 개선을 산출한다는 것을 발견했다. 이것은 개선이 거의 무시할 수 있는 0.001 nats/token인 그림 8(오른쪽)의 복잡성 평가와 주목할 만한 대조이다. 이것은 복잡성 평가가 샘플링 성능의 이득을 항상 포착하는 것은 아니라는 섹션 4.2의 발견을 확증한다.

또한 문서 수준의 FIM은 데이터 로딩 파이프라인의 청킹 단계에서 누락된 접두사 및 접미사를 가진 단편화된 FIM 데이터를 초래할 수 있음을 이전에 설명했다. 그림 8(왼쪽)은 문서 수준 FIM에서 이러한 유효하지 않은 예제에 대한 훈련이 좌우 평가에 영향을 미치지 않음을 보여준다. 따라서 실무자들은 문서 수준의 FIM을 더 간단한 구현으로 인해 여전히 선호할 수 있다.

### 중간 스팬 선택

FIM 훈련에서 중요한 고려 사항은 중간 범위의 선택이다. 이 작업에서 중간 스팬은 문자 수준에서 접두사, 중간 접미사, 접미사 사이의 분할이 발생하는 무작위에서 균일하게 선택된다. 이 섹션에서는 이 선택을 검토한다. 함수 및 클래스 본문과 같은 구문 경계를 넘어 FIM을 시도하는 대신 언어 불가지론자인 간단하고 일반화할 수 있는 접근법으로 절제를 제한한다. 우리는 행, 토큰 및 문자로 무작위로 분할하는 세 가지 다른 방식으로 스팬을 선택한다. 횡단 경계는 스팬 유형에 따라 허용되는 분할 위치에서 무작위로 균일하게 선택됩니다. 여기서 토큰은 BPE(byte-pair encoding) 어휘 내의 단어를 의미한다. 실제로, 이는 문서들이 BPE로 인코딩된 후에 FIM 증강을 적용함으로써 구현된다(부록 C 참조). 단순화를 위해 이 절제에서 PSM 모드에서 모든 실험을 실행한다.

표 2에서 우리는 라인 기반 중간 스팬에 대한 훈련만이 단일 라인 및 다중 라인 채우기 벤치마크에서 모델에 약간의 이점을 제공한다는 것을 알 수 있다. 이러한 평가가 선 기반 중간 스팬 런을 사용하여 완전히 분포 상태에 있기 때문에 이것은 놀라운 일이 아니다. 반면에, 라인 기반 훈련은 랜덤 스팬 채우기 벤치마크에서 거의 완전히 실패한다. 흥미로운 사실은 훈련에서 모든 FIM 분포를 라인 기반 중간 경간에 집중함으로써 라인 기반 평가에서 제공되는 이점은 무작위 스팬 채우기 벤치마크에서 모델이 얼마나 손상되는지에 비해 상당히 작다.

토큰 레벨 랜덤 스팬을 사용한 트레이닝은 랜덤 스팬 채움에서 약간 더 우수하지만, 여전히 이 벤치마크에서 문자 레벨 런에 비해 경쟁력이 없다. 그 이유는 토큰 수준 FIM 모델이 접두사 또는 접미사를 사용 하 여 중간 경계를 넘어 두 부분으로 분할 된 경우에 대해 학습 되지 않기 때문입니다. 중간 구간을 문자 수준에서 완전히 무작위로 선택하면 중간 구간의 시작과 끝 경계에서 아토켄이 자연스럽게 도입된다. 트레인 테스트 불일치는 없으며 모델은 단일 라인 및 다중 라인 채움에서 여전히 잘 수행하면서 더 많은 무작위 스팬 채움 작업을 이해하고 해결할 수 있다.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Training middle span & Single-line infilling & Multi-line infilling & Random span infilling \\ \hline Line-level random span & 0.586 & 0.269 & 0.015 \\ Token-level random span & 0.548 & 0.242 & 0.102 \\ Character-level random span & 0.557 & 0.250 & 0.321 \\ \hline \hline \end{tabular}
\end{table}
표 2: 다양한 중간 스팬 선택 전략으로 사전 훈련된 중간 모델의 통과율. 라인 기반 스팬에 대한 훈련은 InCoder에서 보고된 단일 및 다중 라인 채우기 메트릭을 개선하지만 이전 연구에서 사용된 라인 및 토큰 수준 스팬은 스팬이 서브토켄에서 시작되거나 끝나는 실제 사용 사례를 강력하게 처리할 수 없다. 전반적으로 문자 수준 랜덤 스팬 실행은 랜덤 스팬 벤치마크에서 우세하지만 단일 및 다중 라인 채움에서도 크게 뒤지지 않는다.

## 5 Finuning 결과

이 섹션에서는 FIM 기능을 학습하기 위해 기존 AR 모델을 미세 조정할 수 있는지 여부를 조사한다. 이상적으로, 피니튜닝 후, AR 모델은 FIM으로 사전 훈련되었다면 달성했을 것과 동일한 수준의 FIM 평가에서 성능에 도달할 것이다. 추가 계산 비용 없이 사전 훈련 동안 FIM을 학습할 수 있다는 점을 감안할 때, 모델이 또한 피니튜닝에서 이 작업을 신속하게 학습할 수 있어야 한다고 예상하는 것은 당연하다. 놀랍게도, 피니튜닝된 모델이 기준선 사전 훈련된 모델과 동일한 수준의 성능에 도달하려면 사전 훈련된 컴퓨팅에 비해 많은 양의 컴퓨팅을 확장해야 한다는 것을 발견했다.

이를 보이기 위해 하이퍼파라미터 튜닝의 다양한 선택을 사용하여 FIM 없이 100B 토큰에 대해 사전 훈련된 XL 모델을 미세 튜닝한다. 구체적으로, 학습률 4가지(사전학습률 0.1배, 0.2배, 0.5배, 1배)와 FIM률 2가지(0.5배, 0.9배), 피니튜닝 지평(25B, 50B 토큰) 2가지 선택으로 16개의 피니튜닝 모델을 학습한다. 우리는 결론이 견고하고 하이퍼파라미터가 최종 성능에 미치는 영향을 더 잘 이해하기 위해 이 다양한 하이퍼파라미터 선택을 사용한다. 결과는 그림 9에 요약되어 있으며, 이 16개 모델의 성능을 피니튜닝 없이 FIM 비율이 50%인 100B 토큰에 대해 훈련된 XL 모델의 성능과 비교한다. 이 그림에서 상당한 추가 피니튜닝 계산이 있더라도 FIM으로 피니튜닝된 AR 모델은 FIM으로 사전 훈련된 모델과 동일한 성능에 도달하지 않는다는 것이 분명하다.

이 16개 모델 중 사전 훈련된 기준선과 미세 조정된 모델 사이의 간격이 닫히는 유일한 설정은 FIM 속도가 0.9이고 학습 속도 승수가 1.0인 50B 토큰 실행이다.

도 9: FIM이 없는 100B 토큰에 대해 사전 훈련된 다음 FIM이 있는 25B(행 a) 및 50B(행 b) 토큰에 대해 피니튜닝된 모델의 최종 스냅샷의 평가. x축은 사전 훈련 학습률에 대한 학습률 승수를 나타낸다. 점선은 추가 피니튜닝 없이 FIM 비율이 50%인 100B 토큰에 대해 사전 훈련된 모델의 기준선 성능을 나타낸다. 90% FIM 비율과 50B 토큰의 피니튜닝이 있는 1.0의 학습률 승수의 가장 공격적인 조합만이 기준선의 성능을 따라잡는다. 보고된 결과는 작업당 온도 0.2 및 100 샘플러이다.

훈련하기 전에 더 일반적으로, 우리는 더 높은 학습 속도, FIM 속도 및 더 긴 피니튜닝이 모두 피니튜닝에서 FIM 성능을 향상시키는 데 도움이 되는 것으로 보인다는 것을 발견했다.

우리는 유사한 수준의 성능에 도달하기 위해 그러한 높은 학습 속도와 긴 피니튜닝이 필요하다는 것이 특히 놀랍다는 것을 발견했다. 우리는 섹션 6에서 이 주제에 대해 더 논의한다. 사전 훈련에서와 동일한 수준의 성능에 도달하려면 많은 양의 계산이 필요하지만, 작은 양의 피니튜닝(특히 높은 FIM 및 학습률)은 모델이 우리의 메트릭에서 자명하지 않은 수준의 FIM 성능에 도달하기에 여전히 충분하다는 점에 주목한다. 우리는 부록 F에서 피니튜닝의 역학에 대한 추가 결과를 제시한다.

## 6 Discussion

**사전 훈련 대 피니튜닝** 이전 섹션에서 FIM을 인과 언어 모델에 효율적으로 가르치는 방법을 연구했습니다. 주요 발견은 FIM이 사전 훈련에서 무료로 배울 수 있다는 것이다. 대조적으로, 우리는 섹션 5에서 피니튜닝에서 FIM을 배우는 것이 상당히 비쌀 수 있다는 것을 보았다. 여기에서 이러한 발견에 대한 몇 가지 잠재적인 설명을 설명한다.

FIM이 프리트레이닝에서 무료로 학습될 수 있는 이유에 대한 주요 직관은 문서를 세 조각으로 나누고 중간 조각을 끝으로 이동시키면 세 개의 작은 문서가 효과적으로 생성된다는 것이다. 특히, 각 피스는 여전히 왼쪽에서 오른쪽으로 다음 토큰을 예측해야 하며, 자동 처리되는 토큰의 총 수는 동일하게 유지된다.

반면에, FIM 데이터가 자기회귀 데이터와 국부적으로 동일하더라도, FIM은 전체 문서에 대해 다른 글로벌 주의 패턴을 부과한다. 이를 가시화하기 위해 그림 10에서 FIM 문서의 인과적 주의 마스크를 보여준다. 이러한 새로운 주의 패턴은 핀튜닝에서 FIM을 학습하는 데 상대적으로 긴 토큰 지평선과 높은 학습률이 필요한 이유일 수 있다. 규칙적인 AR 사전 훈련에서 학습된 문서 전체 주의 패턴에는 골화(Hernandez et al., 2021)가 있을 수 있으며, 이는 FIM에서 필요한 주의 패턴에 적응하기 위해 긴 피니튜닝 단계가 필요하다.

**FIM 손실, AR 손실 및 FIM 작업의 어려움입니다.* * Naively, FIM은 AR 기능에 비용이 들지 않으므로 FIM이 쉬운 작업일 것으로 예상할 수 있습니다. 사실, 그 반대인 것 같다. FIM이 보통의 좌-우 세대보다 훨씬 더 어려울 수 있다는 실질적인 증거가 있다.

직관적으로, 특정 접미사로 끝나는 것을 조건으로 하는 텍스트를 계속하는 것보다 그럴듯한 방식으로 텍스트를 계속하는 것이 종종 더 쉽다. 후자는 두 조각을 연결하는 그럴듯한 내러티브를 계획하고 접두사와 일치하는 방식으로 세대를 시작하고 _적절한 시간에 세대를 중지_하여 접미사에 연결해야 한다. 특히, FIM에서 모델은 중간이 끝나고 접미사에 연결될 때 <eot>를 생성하도록 트레이닝된다. 반면에 모델이 할당된 예산에서 <eot>을 생성하지 못하면 접미사에 잘 연결되지 않는 잘린 샘플이 발생하는 경우가 많다. 예를 들어 다음을 고려 합니다.

도 10: FIM 데이터의 인과적 주의 패턴의 시각화. 정규 좌-우 순서로 질의 및 키 임베딩을 모두 다시 푸는 것은 FIM이 복잡한 아키텍처 변경 없이 중간 섹션을 디코딩할 때 트랜스포머가 미래의 컨텍스트에 참석할 수 있게 한다는 것을 보여준다. 한 가지 부작용은 접미사 확률이 더 이상 중간 범위에 의존하지 않는다는 것이다.

내가 어렸을 때, 나는 비디오 게임만 하는 것을 좋아했다. 시간이 지남에 따라, 저는 봇이 이 게임을 할 수 있는 어떤 인간보다 더 잘 할 수 있는지 생각하기 시작했습니다. 저는 결국 게임을 직접 하는 것보다 후자를 더 좋아하기로 결정했고 그것이 제가 처음으로 AI 연구에 관심을 갖게 된 이유입니다.

내가 어렸을 때, 나는 비디오 게임만 하는 것을 좋아했다. 나는 가끔 하루에 13시간 이상을 연주하곤 했다. 서두르기와 새로움, 다양성은 실생활이 제공할 수 있는 어떤 것도 뛰어넘었다. 나는 그 도전을 좋아했고 그것에 뛰어났다. 저는 종종 수업을 빼먹고 가곤 했는데, 그것이 제가 처음으로 인공지능 연구에 관심을 갖게 된 이유입니다.

위의 두 완성 모두 접두사에 잘 연결되지만 첫 번째 완성만 접미사에 잘 연결됩니다. 대조적으로 두 번째 완료는 할당된 예산에서 <eot>을 생성하지 못하여 잘못된 샘플을 생성한다.5 FIM 샘플링에서 일반적인 실패로 판명되었다. 좌우 샘플링도 때때로 관련 문제로 어려움을 겪지만, 접미사에 연결하지 못하는 것은 후처리로 쉽게 해결할 수 없기 때문에 FIM에서 이러한 유형의 실패는 더 번거롭다. 예를 들어, 샘플을 마지막 단락 또는 줄까지 트리밍하는 것은 종종 AR 샘플링에서 샘플 품질을 개선하는 효과적인 방법이지만 FIM에서는 도움이 되지 않는다. 우리는 부록 H에서 FIM 샘플링과 관련된 이 문제와 기타 문제에 대해 더 광범위하게 논의한다.

각주 5: 완성이 더 큰 예산으로 접미사에 연결할 수 있었을지 모르지만, 얼마나 많은 예산이 충분한지 불분명하다는 것이 과제이다. 실제로는 중간을 위한 토큰의 최대 수에 대한 합리적인 예산이 부과되어야 하는 경우가 많다.

AR 작업에 비해 FIM 작업의 난이도는 각 작업의 손실에도 반영된다. 이를 보기 위해 그림 11에서 FIM 비율이 50%인 FIM 모델 세트에 대한 FIM 손실을 AR 손실과 비교한다. 혼란을 제거하기 위해 AR 테스트 세트의 기초가 되는 문서가 FIM 테스트 세트를 구성하기 위해 FIM을 통해 변환되는 동일한 문서인지 확인한다. 우리는 FIM 복잡성이 규모에 걸쳐 AR 복잡성보다 일관되게 높다는 것을 발견했다. 즉, 평균적으로

\[P_{\text{FIM}}([\text{prefix},\text{suffix},\text{middle}])<P_{\text{AR}}([ \text{prefix},\text{middle},\text{suffix}]),\]

이는 AR 형식보다 FIM 형식으로 동일한 문서를 모델링하는 데 더 많은 시간이 소요된다는 것을 의미한다.

**컨텍스트 수준 대 문서 수준 FIM 및 FIM 비율입니다.* * 섹션 4.4에서 컨텍스트 수준 FIM이 일반적으로 문서 수준 FIM보다 성능이 우수함을 확인했습니다. 여기에서 FIM 비율에 대한 섹션 4.2 및 부록 B의 결과와 이 발견 사이의 연결에 주목한다.

기본 관찰은 문서 수준 FIM이 동일한 명목 값인 FIM 속도에도 컨텍스트 수준 FIM에 비해 효과적으로 낮은 FIM 속도_로 이어진다는 것입니다. 생각의 실험으로

도 11: 50% FIM 코드 모델의 전체(좌측) 및 중간 스팬(우측) 손실의 비교. 왼쪽 그림에서 AR 손실이 FIM 손실보다 일관되게 낮음을 알 수 있으며, 이는 다음 토큰 예측이 본질적으로 중간에 채우는 것보다 압축성이 더 높다는 것을 시사한다. 오른쪽 그림은 \(P_{\text{FIM}}(\text{middle}|\text{prefix},\text{suffix})>P_{\text{AR}}(\text{middle}|\text{prefix})\)를 보여주는 주변 컨텍스트를 감안할 때 중간 범위의 조건부 손실을 평가합니다. 여기서, FIM은 접미사를 처리할 수 있기 때문에 더 낮은 손실을 얻는다. 이 그림의 모든 모델이 FIM 모델이기 때문에 여기에서 좌-우 및 FIM은 모델 유형을 언급하지 않는다는 점을 강조한다. 그들은 평가에 사용되는 테스트 손실의 유형을 오히려 언급한다.

학습 데이터 세트의 모든 문서가 컨텍스트 크기보다 훨씬 긴 설정입니다. 이 설정에서, 문서 레벨 FIM을 사용할 때, 모델은 거의 청크 후 동일한 문서의 접두사, 중간 및 접미사가 함께 동일한 컨텍스트에 나타나는 것을 보지 못한다. 따라서 이 환경에서 모델이 FIM을 배우는 데 어려움을 겪을 것으로 예상한다. 덜 극단적인 상황에서는 문맥 크기보다 짧은 많은 문서가 있으므로 위의 현상은 덜 두드러진다. 그러나 학습 데이터의 긴 문서와 문서 패킹의 일반적인 아티팩트 때문에 문서 수준 FIM은 더 낮은 유효 FIM 비율을 초래한다. 여기서는 효과적인 FIM 비율을 FIM 형식이고 접두사, 중간 및 접미사 세 가지가 모두 동일한 컨텍스트 내에 나타나는 예제의 분수로 정의한다.

이러한 유효 FIM 비율의 감소는 섹션 4.4에서 컨텍스트 수준 FIM의 성능이 강화된 주요 원인일 수 있다. 우리는 유효 FIM 비율의 정확한 감소량이 문서 길이의 분포의 세부 사항에 따라 다르다는 점에 주목한다. 데이터 분포가 많은 긴 예들을 갖지 않더라도, 문서 패킹으로 인해 유효 FIM 레이트의 감소가 여전히 존재할 것이라는 것을 기억하는 것이 중요하다.

## 7 관련 작업

마스크된 언어 모델링은 마스크된 토큰의 연속 실행이 모델이 채워야 하는 스팬으로 해석될 수 있다는 점에서 텍스트 채움과 밀접한 관련이 있다. BERT(Devlin et al., 2019)와 같은 조기 마스킹 언어 모델이 토큰을 랜덤하게 마스킹한 반면, T5(Raffel et al., 2019), SpanBERT(Joshi et al., 2020) 및 BART(Lewis et al., 2020)는 토큰의 연속 실행이 마스킹될 때 개선을 보여주었다. 그러나, 이러한 모델들은 표현 학습에 초점을 맞추기 때문에, 스팬 길이들은 전형적으로 문장 또는 심지어 단일 라인의 코드보다 훨씬 짧다. 우리의 관심 모달리티 내에서, DOBF(Lachaux et al., 2021)는 코드 상에서 BERT를 트레이닝하고, HTLM(Aghajanyan et al., 2021)은 HTML 데이터 상에서 BART를 트레이닝한다.

텍스트 채우는 표준 좌-우 생성 순서가 보다 유연한 순서로 대체되는 자기회귀 언어 모델링의 특수한 경우라고도 볼 수 있다. XLNet(Yang et al., 2019)은 표준 트랜스포머에서 어텐션 마스크를 수정하여 임의의 사용자 지정 순서로 토큰 생성이 가능하도록 하는 반면, Insertion Transformer(Stern et al., 2019), KERMIT(Chan et al., 2019), InDIGO(Gu et al., 2019)는 모델이 토큰을 예측하기 전에 다음 토큰에 대한 위치를 예측할 수 있도록 한다. 유사하게, 블랭크 언어 모델들(Shen 등, 2020)은 블랭크를 반복적으로 선택하고 이를 토큰(및 선택적으로 더 많은 블랭크들)으로 대체함으로써 텍스트를 생성한다.

우리의 작업과 유사하게, Zhu et al. (2019), Donahue et al. (2020), GLM (Du et al., 2022), CM3 (Aghajanyan et al., 2022), 및 InCoder (Fried et al., 2022)는 인필 영역을 컨텍스트의 끝까지 이동시킴으로써 좌-우 자기회귀 모델링을 활용하고, 영역은 센티넬에 의해 분리된다. 특히, Donahue 등(2020)은 단어, 문장, 또는 단락과 같은 다양한 입도의 주입 스팬을 탐색하고, InCoder(Fried 등, 2022)는 HumanEval(Chen 등, 2021)로부터 생성된 샘플링 기반 벤치마크에 대한 주입 능력을 연구함으로써 우리와 유사한 평가 프레임워크를 사용한다. 이러한 작업들 중 몇몇은 다수의 스팬들을 채우는 것을 지원하지만, 실용성을 위해 단일 스팬 설정(예를 들어, 커서의 배치가 우리가 채우고자 하는 위치를 암시하는 컴퓨터 기반 텍스트 생성)에 초점을 맞춘다. 또한, 본 논문은 대규모 채움을 위한 훈련의 계산 효율성을 강조한다. 우리는 구문론적으로 또는 의미론적으로 동기화된 채우기 스팬을 연구하지 않지만 문자 수준에서 스팬을 선택하면 채우기 견고성이 향상된다는 것을 보여준다.

텍스트 인필링은 GAN(Fedus et al., 2018)을 사용하여도 가능하지만, 텍스트의 디스크리텐스를 다루기 위해서는 REINFORCE가 필요하다. 텍스트 인필링은 또한 그래디언트 서치(Liu et al., 2019)를 통해 수행될 수 있으며, 여기서 인필된 스팬 내의 토큰은 그래디언트 하강으로 최적화되고 가장 가까운 이웃으로 쓰러진다.

전반적으로, 주입 기능을 가진 모델을 도입하기 위한 두 가지 접근법이 있다: 첫째, SpanBERT 및 XLNet과 같은 새로운 아키텍처를 통해, 둘째, 데이터 포맷팅을 통해. 일반적으로 후자의 접근법은 제어 코드를 통해 언어 모델의 행동을 변경하는 것으로 볼 수 있으며, 이는 세대의 조향성을 향상시키기 위해 CTRL(Keskar 등, 2019)에서 동기화되었다. DistAug(Jun et al., 2020)는 변환 타입 상에서 컨디셔닝하면서 변환된 데이터에 대해 공동으로 트레이닝하는 또 다른 관련 작업이다. 주입은 아키텍처와 데이터 모두를 통해 실현될 수 있는 특정 사용 사례이지만, 일반적으로 새로운 훈련 배포판을 하드와이어링하는 것보다 도입함으로써 추가 기술을 배우는 것이 더 쉽고 보편적이다.

우리가 알고 있는 규모에서 가장 강력한 충전 시스템은 지난 3월 [20]에 발표된 코드-다빈치-002이다. 본 논문은 이 보다 강력한 모델의 충전 기능을 작동시키기 위한 초기 연구 중 일부에 대해 설명한다. 부록 4에서는 이 시스템, 6.9B 모델 및 InCoder 6.7B 모델을 채우는 벤치마크에 대해 비교한다.

## 8 Conclusion

본 연구에서는 인과적 디코더 기반 언어 모델이 전통적인 좌-우 변환 데이터와 FIM 변환 데이터의 혼합에 대해 공동으로 훈련된 후 문서의 중간을 채우는 것을 학습할 수 있음을 보여준다. 단일 FIM 모델은 모듈, 문서 스트링 및 완전한 기능을 가져올 수 있으며, 개별 작업에 대해 미세 조정된 특수 모델을 포함시켜 [21] 전통적인 좌-우 언어 모델에 비해 상당한 추가 기능을 제공한다.

여기서 중요한 발견 중 하나는 FIM이 없는 부동산입니다. 그림 1과 2는 동일한 양의 계산으로 FIM 모델이 더 낮은 FIM 손실을 달성하면서 왼쪽에서 오른쪽 테스트 손실에 대해 AR 모델과 동일한 테스트 손실을 달성한다는 것을 보여준다. 이는 4절에서 비손실 기반 평가를 사용하여 더욱 강화된다.

또한 기존의 많은 언어 모델들이 FIM 기능을 가지고 있지 않기 때문에 FIM 피니튜닝에 대해서도 조사한다. 우리의 결과는 정규적으로 사전 훈련된 좌-우 모델이 신중한 하이퍼파라미터 튜닝과 사전 훈련에 비해 상당한 양의 피니튜닝 계산으로도 주어진 모델 크기의 최대 범위까지 새로운 기술을 획득하지 못한다는 것을 보여준다. 이는 최상의 FIM 성능을 위해 권장되는 하이퍼파라미터와 함께 처음부터 공동으로 사전 훈련하는 것이 피니튜닝보다 더 효과적임을 시사한다.

FIM의 성능을 정확히 연구하기 위해 InCoder [14]의 인필링 코드 벤치마크를 사용하고 HumanEval [21]에 기반한 새로운 랜덤 스팬 인필링 벤치마크를 소개한다. 이로부터 우리는 몇 가지 중요한 교훈을 배웁니다. 첫째, 복잡성은 실제 채움 성능을 반영하지 않으며, 채움 벤치마크를 신중하게 설계하여 진행률을 측정해야 한다. 둘째, FIM 성능은 문맥 수준 FIM과 같이 FIM 속도와 구현에 크게 의존하지만, FIM 속도가 100% 미만으로 유지되는 한 좌-우 기능은 이러한 선택에 영향을 받지 않는다. 셋째, 문자 레벨에서 FIM을 적용하는 것은 서브토켄에 대한 자연스러운 견고성을 모델에 내재시키고, 예를 들어 코딩 어시스턴트로서 야생에서 모델을 전개하는 것을 가능하게 한다.

전반적으로, 우리는 적어도 우리가 고려하는 평가의 범위 내에서 FIM 모델이 표준적으로 훈련된 좌-우 모델보다 엄격하게 더 능력이 있음을 보여주고 FIM 모델을 효율적이고 경쟁적으로 훈련하는 방법을 보여준다.

### 권장 FIM 하이퍼파라미터

섹션 4에서 우리는 FIM 모델을 훈련하는 데 많은 하이퍼파라미터가 있음을 알 수 있다. 모든 경우 접두사와 접미사가 토큰 중간에 끝날 때에도 모델이 합리적인 완료를 생성할 수 있으므로 문자 수준에서 FIM 변환을 적용 하 고 항상 일부 문자 수준 랜덤 스팬을 포함 하는 것이 좋습니다. 우리는 중간 토큰 견고성을 위해 PSM 모드에서의 추론이 이 작업에서 탐색된 특정 SPM 모드보다 우수할 수 있다는 점에 주목한다. 그러나 공동 PSM과 SPM을 사용한 사전 훈련은 두 형식 간의 긍정적인 전달로 인해 최상의 성능을 산출한다. 구현 측면에서 컨텍스트 수준 FIM이 우수하지만 문서 수준 FIM도 더 간단한 구현을 원하는 경우 옵션이다. 마지막으로 AR 성능에서 비용 없이 최대 90%의 FIM 비율까지 향상된 성능을 관찰한다. 실제로, 50% 내지 90% 사이의 범위의 임의의 값은 합리적인 선택이다. 이것은 일반적으로 15%와 같은 더 낮은 FIM 비율 값을 사용하는 [14]와 같은 일부 관련 이전 작업과 대조되며, 이는 우리의 결과가 차선책임을 나타낸다.

### Future directions

우리가 여기서 다루지 않은 몇 가지 중요한 관련 지침이 있다. 예를 들어,

1. **스마트 스팬 선택**: 일반성을 위해 무작위로 균일하게 선택된 스팬만 고려하지만 의미론적 또는 구문적으로 의미 있는 스팬으로 혼합하면 [15, 16, 17] 채우기 성능이 상당히 향상될 수 있습니다. 섹션 4.5에서 문자 수준 스팬 대신 라인 수준 스팬에 대한 훈련이 라인 기반 채움 결과를 향상시킨다는 것을 알 수 있다. 예비 실험에서 중간 스팬을 정확히 하나의 단어로 선택하는 것이 클로즈 유사 작업에서 정확도를 크게 향상시키는 것으로 나타났다. 더 현명한 범위 선택은 언어별 구문 분석과 만들기 어려울 수 있는 새로운 벤치마크를 포함하지만, 우리는 이것이 더 강력한 FIM 모델을 생성할 것으로 기대한다.
2. **조향 가능한 생성**: FIM 모델은 사용자 요구 사항을 채우는 길이 또는 스타일을 알지 못하기 때문에 할당된 토큰 예산에서 가짜 콘텐츠를 생성하거나 합리적인 완료를 생성하기 위해 고군분투합니다. 인간 피드백으로부터의 RL(Stiennon et al., 2020) 및 후속 명령(Ouyang et al., 2022)과 같은 아이디어를 적용하는 것은 제어가능한 세대의 다른 방법들 중에서 사용자의 의도와 추가 정렬을 제공함으로써 이 문제를 해결할 수 있다.
3. **FIM-for-free 속성에 대 한 추가 조사**: FIM-for-free 속성에 대 한 실질적인 증거를 제공 하지만 FIM 모델이 AR 모델을 수행 하지 않는 여기에 고려 되지 않은 벤치마크가 있음을 완전히 배제할 수 없습니다. 이와 같이, FIM-for-free 속성을 더욱 강화하거나 반박하는 것은 흥미로운 방향으로 남아 있다.
4. **여러 채우기 슬롯**: 채우기 작업의 많은 이전 작업에서 여러 채우기 슬롯을 탐색했습니다 (Raffel et al., 2019; Fried et al., 2022). 단일 슬롯 모델을 훈련하는 데 이미 많은 고려 사항이 있고 채우는 데 고유한 추론 문제가 있기 때문에 이를 연구하지 않는다. 또한 대부분의 응용 분야에서 단일 슬롯 채움이 유용할 것으로 예상한다. 우리는 다중 슬롯 채움을 고려할 때 추론 도전과 실패 모드가 증가할 것으로 예상한다. 그러나 다중 슬롯 채움에서 진전을 이루기 위해서는 복잡성 기반 평가가 점점 더 도움이 되지 않기 때문에 적절한 샘플링 기반 벤치마크를 만드는 것이 필수적이다. 이러한 벤치마크를 위한 방대한 설계 공간과 단일 슬롯에서 다중 슬롯 채움으로 이동할 때 방대한 양의 추가 훈련 하이퍼파라미터가 있다.
5. **자연어 FIM 성능 개선**: 정성적으론 FIM 모델이 언어보다 코드에서 더 나은 성능을 발휘하는 경향이 있습니다. 코드가 형식 언어이고, 따라서 구조가 더 많고 불확실성이 적다는 점을 고려할 때 이것은 놀라운 일이 아니다. 자연어에 대한 채움 성능을 향상시키는 것은 흥미로운 미래 방향이지만 언어에서 자유 형태 생성에 대한 평가가 코드에서 기능적 정확성을 측정하는 것만큼 간단하지 않기 때문에 까다로울 수 있다. 우리는 더 의미론적으로 의미 있거나 짧은 경간에 대한 훈련이 여기에서 도움이 될 것으로 예상하지만 일반적인 경우 어떤 테스트 분포를 사용하고 이를 잘 평가하는 방법은 불분명하다.
6. **양방향성과 주의의 역할**: 자유 형식 채우기 성능에서 주의의 역할과 교육 목표에서 이해해야 할 사항이 많습니다. 본 연구에서는 현재 대규모 언어 모델링의 주요 패러다임인 디코더 기반 언어 모델을 사용한다. 그러나, 채움의 관점에서, 다른 트레이닝 목표들 및 아키텍처들이 우수할 수 있다. 이러한 방향에서, (Artetxe et al., 2022)는 BERT 스타일 아키텍처가 FIM-유사 모델들보다 더 나은 성능을 보여주지만, 그 결과는 대부분 단일-토큰 채움으로 제한된다. (Wang et al., 2022; Tay et al., 2022)와 유사하지만 자유 형식 주입 생성에 초점을 맞춘 보다 체계적인 연구는 이를 더욱 명확히 할 수 있다. 이와 다소 관련이 있는 것은 절대 및 상대 위치 임베딩 및 그 변형과 FIM의 상호 작용을 조사하는 것이 흥미롭다. 여기에 보고되지 않은 예비 결과는 FIM-for-free 속성이 절대 위치 임베딩과 함께 여전히 유지됨을 나타낸다.

마지막으로, FIM-for-free 속성에 대한 우리의 경험은 "언어 모델의 원래 기능에 전혀 또는 적은 비용으로 공동으로 배울 수 있는 다른 유용한 기술이 무엇인가"라는 흥미로운 질문을 제기한다. 이 주제에 대해 많은 흥미로운 작업이 있었고 우리는 훨씬 더 많이 따를 것으로 예상하지만 많은 작업은 보다 광범위한 채택 및 비교를 위해 비판적 분석을 생략하는 경우가 많다. 이 질문에 답하기 위한 연구를 발전시키는 데 도움이 되는 다음 방법론을 제안한다.

1. 새 기능을 학습하기 위해 기꺼이 희생할 원본 기능의 양에서 _예산_ 을 설정합니다.
2. 이 예산 내에서 새 기능을 최대화 합니다.

예산-능력 절충은 이론적으로 흥미로울 뿐만 아니라 실용적이어서 연구자들이 적절한 절충 분석을 기반으로 새로운 역량을 통합할 수 있다. 우리는 대형 언어 모델이 점점 다양해지고 높은 가치 역량을 갖춘 미래를 기대한다.

## Acknowledgments

프로젝트의 여러 단계에서 유익한 토론과 도움을 주신 샨타누 자인, 알렉스 파노, 알렉 래드포드, 닉 라이더, 프라나브 샤얌, 키밍 위안에 감사드린다. 우리는 또한 FIM의 API 인프라와 질적 평가에 도움을 준 크리스티나 킴, 레이첼 림, 앤드류 메인, 매디 지멘스, 나탈리 슈타처, 그리고 안젤라 장, 케이티 메이어, 라지브 나약, 앙리케 폰드, 펠리페 스승에게 귀중한 작업과 배치에 대한 엄청난 노력에 감사한다. 마지막으로, 우리는 프로젝트 내내 변함없는 지원을 해주신 밥 맥그루와 워지흐 자렘바, 그리고 신문에 대한 귀중한 피드백에 칼 코브, 안젤라 장, 알렉 래드포드, 프라나브 샤얌에게 감사드린다.

## References

* Aghajanyan et al. (2021) A. Aghajanyan, D. Okhonko, M. 루이스 Joshi, H. Xu, G. Ghosh, L. 제틀모이어 HTLM: 하이퍼-텍스트 사전 트레이닝 및 언어 모델의 프롬프트. _ CoRR_, abs/2107.06955, 2021. URL [https://arxiv.org/abs/2107.06955](https://arxiv.org/abs/2107.06955).
* Aghajanyan 등(2022) A. Aghajanyan, B. Huang, C. Ross, V. 카푸킨 고얄 오혼코 조시고시 Lewis, L. 제틀모이어 CM3: 인터넷의 인과 마스킹된 멀티모달 모델. _ CoRR_, abs/2201.07520, 2022. URL [https://arxiv.org/abs/2201.07520](https://arxiv.org/abs/2201.07520).
* Artetxe 등(2022) M. 정두남 고얄 Zettlemoyer, V. 스토야노프 언어 모델 사전 훈련에서 양방향성의 역할에 대해 2022. URL [https://arxiv.org/abs/2205.11726](https://arxiv.org/abs/2205.11726).
* Bisk 등(2020) Y. 비스크 Zellers, R. L. Bras, J. Gao, and Y. 최 피카: 자연어로 육체적 상식에 대해 추론합니다. 2020년 _30번째 AAAI 인공지능 콘퍼런스_에서.
* Brown 등(2020) T. B. Brown, B. Mann, N. 라이더 Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. _ arXiv preprint arXiv:2005.14165_, 2020.
* Chan 등(2019) W. 찬남 기태프 구민 Stern, J. Uszkoreit. KERMIT: 시퀀스에 대한 생성적 삽입 기반 모델링입니다. _ CoRR_, abs/1906.01604, 2019. URL [http://arxiv.org/abs/1906.01604](http://arxiv.org/abs/1906.01604).
* Chen et al.(2021) M. 진진투렉 Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. 부르다남 조셉, G. 브록맨 A. 레이, R. 부리규거 Petrov, H. Khafaif, G. Sastry, P. Mishkin, B. Chan, S. 그레이 엔 라이더 파블로프 카이저 Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. 테작제탕일바부슈킨 발라지 제인우 Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. 나이트엠 브런디지 무라티 Mayer, P. Welinder, B. McGrew, D. Amodei, S. 맥캔들리, I. 서츠키버, W. 자렘바 코드에 대해 학습 된 대규모 언어 모델을 평가 합니다. _ CoRR_, abs/2107.03374, 2021. URL [https://arxiv.org/abs/2107.03374](https://arxiv.org/abs/2107.03374).
* Choi et al. (2018) E. Choi, H. He, M. 아이이어 엠 야츠카르 -t. 이영 최필량 제틀모이어 QuAC: 문맥상 질의 응답. _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 2174-2184, page, Belgium, Brussels, Oct. 노브 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1241. URL [https://aclanthology.org/D18-1241](https://aclanthology.org/D18-1241).
* Chowdhery 등(2022) A. Chowdhery, S. 나랑 데블린 Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. _ arXiv preprint arXiv:2204.02311_, 2022.
* Dai 등(2019) Z. 대진 양영 양진카보넬 Le, R. 살라쿠트디노프 Transformer-XL: 고정 길이 컨텍스트를 넘어서는 주의 언어 모델. _Proceedings of the 57 Annual Meeting of the Association for Computational Linguistics_, pages 2978-2988, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL [https://aclanthology.org/P19-1285](https://aclanthology.org/P19-1285).

X. 등영 수애리 우씨, 유씨, 선씨 Reasonbert: 원격 감독으로 추론하도록 미리 훈련되었습니다. _ CoRR_, abs/2109.04912, 2021. URL [https://arxiv.org/abs/2109.04912](https://arxiv.org/abs/2109.04912).
* Devlin et al. [2019] J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, pages 4171-4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL [https://doi.org/10.18653/v1/n19-1423](https://doi.org/10.18653/v1/n19-1423).
* Donahue et al. [2020] C. Donahue, M. Lee, and P. Liang. Enabling language models to fill in the blanks. _CoRR_, abs/2005.05339, 2020. URL [https://arxiv.org/abs/2005.05339](https://arxiv.org/abs/2005.05339).
* Du et al. [2021] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. _arXiv preprint arXiv:2112.06905_, 2021.
* Du et al. [2022] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. GLM: General language model pretraining with autoregressive blank infilling. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 320-335, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.26. URL [https://aclanthology.org/2022.acl-long.26](https://aclanthology.org/2022.acl-long.26).
* Dua et al. [2019] D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 2368-2378, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1246. URL [https://aclanthology.org/N19-1246](https://aclanthology.org/N19-1246).
* Fedus et al. [2018] W. Fedus, I. Goodfellow, and A. M. Dai. Maskgan: Better text generation via filling in the____, 2018. URL [https://arxiv.org/abs/1801.07736](https://arxiv.org/abs/1801.07736).
* Fried et al. [2022] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W.-t. Yih, L. Zettlemoyer, and M. Lewis. Incooler: A generative model for code infilling and synthesis, 2022. URL [https://arxiv.org/abs/2204.05999](https://arxiv.org/abs/2204.05999).
* Gu et al. [2019] J. Gu, Q. Liu, and K. Cho. Insertion-based decoding with automatically inferred generation order. _Transactions of the Association for Computational Linguistics_, 7:661-676, 2019. doi: 10.1162/tacl_a_00292. URL [https://aclanthology.org/Q19-1042](https://aclanthology.org/Q19-1042).
* Hernandez et al. [2021] D. Hernandez, J. Kaplan, T. Henighan, and S. McCandlish. Scaling laws for transfer. _arXiv preprint arXiv:2102.01293_, 2021.
* Hoffmann et al. [2022] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.
* Holtzman et al. [2020] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi. The curious case of neural text degeneration. In _ICLR_. OpenReview.net, 2020. URL [http://dblp.uni-trier.de/db/conf/iclr/iclr2020.html#HoltzmanBDFC20](http://dblp.uni-trier.de/db/conf/iclr/iclr2020.html#HoltzmanBDFC20).
* Joshi et al. [2020] M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and O. Levy. SpanBERT: Improving pre-training by representing and predicting spans. _Transactions of the Association for Computational Linguistics_, 8:64-77, 2020. doi: 10.1162/tacl_a_00300. URL [https://aclanthology.org/2020.tacl-1.5](https://aclanthology.org/2020.tacl-1.5).
* Jun et al. [2020] H. Jun, R. Child, M. Chen, J. Schulman, A. Ramesh, A. Radford, and I. Sutskever. Distribution augmentation for generative modeling. In H. D. III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 5006-5019. PMLR, 13-18 Jul 2020. URL [https://proceedings.mlr.press/v119/jun20a.html](https://proceedings.mlr.press/v119/jun20a.html).

제이제이 카플란 맥캔들리 헤니건, T. B. 브라운, B. 체스, R. 아동석 Gray, A. Radford, J. Wu, and D. Amodei. 신경 언어 모델에 대한 법칙을 스케일링합니다. _ arXiv preprint arXiv:2001.08361_, 2020.
* Keskar 등(2019) N. S. Keskar, B. McCann, L. R. Varshney, C. Xiong, and R. Socher CTRL: 제어가능한 생성을 위한 조건부 트랜스포머 언어 모델. _ CoRR_, abs/1909.05858, 2019. URL [http://arxiv.org/abs/1909.05858](http://arxiv.org/abs/1909.05858).
* Lachaux 등(2021) M. 라코, B 로지에르, M. Szafraniec, G. Lample. DOBF: 프로그래밍 언어에 대한 디난독화 사전-훈련 목표. M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 14967-14979, 2021. URL [https://proceedings.neurips.cc/paper/2021/hash/7d6548bdc0082aacc950ed35e91fcccb-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/7d6548bdc0082aacc950ed35e91fcccb-Abstract.html)
* Levesque 등(2012) H. J. Levesque, E. Davis, and L. 모겐스턴 위노그라드 스키마 챌린지 _제13회 지식 표현 및 추론 원칙에 대한 국제 회의 회보_, KR'12, 552-561 페이지. AAAI Press, Rome, Italy, 2012. ISBN 978-1-57735-560-1. URL [https://cs.nyu.edu/faculty/advise/papers/WSKR2012.pdf](https://cs.nyu.edu/faculty/advise/papers/WSKR2012.pdf).
* 루이스 등(2020) M. 루이스 류남 고얄 Ghazvininejad A. Mohamed, O. 레비 Stoyanov, L. 제틀모이어 BART: 자연어 생성, 번역 및 이해를 위한 시퀀스 대 시퀀스 사전 트레이닝의 노이즈 제거. _제58회 Computational Linguistics 협회 연례 회의 진행률_에서 2020년 7월 온라인, 페이지 7871-7880. Computational Linguistics 협회. doi: 10.18653/v1/2020.acl-main.703. URL [https://aclanthology.org/2020.acl-main.703](https://aclanthology.org/2020.acl-main.703)
* Lieber 등(2021) O. 오이버 Sharir, B. Lenz, Y. 쇼함 Jurassic-1: 기술 세부 정보 및 평가 _ 백서 AI21 Labs_, 2021.
* Liu 등(2019) D. Liu, J. Fu, P. Liu, and J. Lv. TIGS: 기울기 탐색으로 텍스트를 채우는 추론 알고리즘. _Proceedings of the 57 Annual Meeting of the Association for Computational Linguistics_, pages 4146-4156, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1406. URL [https://aclanthology.org/P19-1406](https://aclanthology.org/P19-1406)
* Mostafazadeh et al. (2016) N. 모스타파자데 챔버스 D. 패릭, D. 바트라, L. 밴더웬드, P. 콜리, J. 앨런 상식 이야기에 대한 더 깊은 이해를 위한 말뭉치와 클로즈 평가. _Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 839-849, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1098. URL [https://aclanthology.org/N16-1098](https://aclanthology.org/N16-1098).
* OpenAI 등(2022) OpenAI, M. Bavarian, A. Jiang, H. Jun, and H. Ponde. 새 GPT-3 기능: 편집 및 삽입 _ OpenAI blog_, 2022. URL [https://openai.com/blog/gpt-3-edit-insert/](https://openai.com/blog/gpt-3-edit-insert/)
* Ouyang 등(2022) L. 오양정우 Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Jang, S. 아가왈 Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. 밀러 Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. 로우 언어 모델을 훈련하여 인간 피드백을 사용하여 지침을 따르도록 2022. URL [https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155).
* Paperno et al.(2016) D. Paperno, G. Kruszewski, A. Lazaridou, N. Q. Pham, R. 베르나디 페젤 바로니, G. 볼리다, 그리고 R. 페르난데스 LAMBADA 데이터 세트: 광범위한 담화 맥락을 요구하는 단어 예측. _The Proceedings of the 54th Annual Meeting of the Association of Computational Linguistics(Volume 1: Long Papers)_, pages 1525-1534, Berlin, Germany, 8월. 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1144. URL [https://aclanthology.org/P16-1144](https://aclanthology.org/P16-1144).
* Provilkov 등(2019) I. Provilkov, D. Emelianenko, and E. Voita. Bpe-dropout: 간단하고 효과적인 하위 단어 정규화입니다. _ CoRR_, abs/1910.13267, 2019. URL [http://arxiv.org/abs/1910.13267](http://arxiv.org/abs/1910.13267).
* Radford et al. (2018) A. Radford, K. 나라심한 살리만과 나, 서츠키버 생성 사전 교육을 통해 언어 이해를 향상시킵니다. 2018.
* Radford 등(2019) A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are nonsupervised multitask learners. _ OpenAI blog_, 1(8):9, 2019.
* Radford et al. (2019)J. W. Rae, S. 보르헤오티 카이광 Millican, J. Hoffmann, F. Song, J. Aslanides, S. 헨더슨 링성 Young, et al. Scaling language models: Methods, analysis & insights from training gopher. _ arXiv preprint arXiv:2112.11446_, 2021.
* Raffel 등(2019) C. Raffel, N. 셰이저 A 로버츠 K 이상 나랑모 마테나 주원 Li, P. J. Liu. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계를 탐구합니다. _ arXiv preprint arXiv:1910.10683_, 2019.
* Sakaguchi 등(2021) K. Sakaguchi, R. L. Bras, C. Bhagavatula, Y. 최 Winogrande: 스케일에서의 적대적인 winograd 스키마 챌린지. _ Commun. ACM_, 64(9):99-106, aug 2021. ISSN 0001-0782. doi: 10.1145/3474381. URL [https://doi.org/10.1145/3474381](https://doi.org/10.1145/3474381)
* Shaw et al. (2018) P. Shaw, J. Uszkoreit, and A. Vaswani. 상대적 위치 표현들을 갖는 자기-주의 _ arXiv preprint arXiv:1803.02155_, 2018.
* Shen 등(2020) T. 심병 락락 Barzilay, T. 자크콜라 빈 언어 모델입니다. _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 5186-5198, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.420. URL [https://aclanthology.org/2020.emnlp-main.420](https://aclanthology.org/2020.emnlp-main.420)
* Song et al.(2019) K. 송X 탄, 탄 진, J. 루, T. -Y. 류 Mass: 언어 생성을 위한 시퀀스 대 시퀀스 사전 트레이닝 마스킹된 시퀀스. _ arXiv preprint arXiv:1905.02450_, 2019.
* Stern 등(2019) M. 스턴원 찬, J. 키로스, J. 우즈코레이트. 삽입 변압기: 삽입 작업을 통한 유연한 시퀀스 생성입니다. In K. Chaudhuri와 R. Salakhutdinov, editors, _제36회 Machine Learning 국제 회의의 Proceedings_, _Machine Learning Research의 Proceedings_ 의 볼륨 97, 페이지 5976-5985. PMLR, 2019년 6월 09-15. URL [https://proceedings.mlr.press/v97/stern19a.html](https://proceedings.mlr.press/v97/stern19a.html)
* Stiennon 등(2020) N. 스티엔온 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. 인간의 피드백으로부터 요약하는 것을 배우는 것 _ CoRR_, abs/2009.01325, 2020. URL [https://arxiv.org/abs/2009.01325](https://arxiv.org/abs/2009.01325).
* Tay 등(2022) Y. 태민 드하니 브이큐트란 가르시아 바리 허성정 Houlsby와 D. Metzler 언어 학습 패러다임 통합, 2022. URL [https://arxiv.org/abs/2205.05131](https://arxiv.org/abs/2205.05131).
* Thoppilan 등(2022) R. 토필란, D. 드 프리타스, J. 홀, N. A. 컬슈레샤 샤지어 정아진 보스 베이커 Du, et al. Lamda: Language models for dialog applications. _ arXiv preprint arXiv:2201.08239_, 2022.
* Vaswani et al. (2017) A. Vaswani, N. 뉴저 J. Uszkoreit, L. 존스, A. N. 고메즈, L. u. 카이저 그리고 I. 폴로수킨 관심만 있으면 됩니다. I. Guyon, U. V. Luxburg, S. 벵지오 퍼거스 Vishwanathan, R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017. URL [https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
* Wang 등(2022) T. Wang, A. Roberts, D. Hesslow, T. L. Scao, H. W. Chung, I. Beltagy, J. Launay, and C. Raffel. 0샷 일반화에 가장 적합한 언어 모델 아키텍처와 사전 훈련 목표는 무엇입니까? 2022년입니다.
* Yang et al.(2019) Z. 양진 대영 양재카보넬 Salakhutdinov, Q. V. Le. _ XLNet: 언어 이해를 위한 일반화된 자기 회귀 사전 교육_입니다. Curran Associates Inc., Red Hook, USA, 2019.
* Zellers 등(2019) R. 젤러스 홀츠만 Bisk, A. Farhadi, Y. 최 HellaSwag: 기계가 정말로 당신의 문장을 끝낼 수 있을까요? _Proceedings of the 57 Annual Meeting of the Association for Computational Linguistics_, pages 4791-4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL [https://aclanthology.org/P19-1472](https://aclanthology.org/P19-1472).
* Zhu 등(2019) W. 주주 Hu, and E. P. Xing. Text Infilling. _ CoRR_, abs/1901.00158, 2019. URL [http://arxiv.org/abs/1901.00158](http://arxiv.org/abs/1901.00158).

아키텍처 및 데이터셋

Codex 및 GPT-3(Chen et al., 2021; Brown et al., 2020)과 유사한 아키텍처, 최적화 하이퍼파라미터 및 인코딩을 갖는 8개의 인과적 트랜스포머 디코더 모델(Vaswani et al., 2017)을 사용한다. 모델의 주요 아키텍처 세부 사항은 표 3에 요약되어 있다. 우리가 소개하는 유일한 아키텍처 수정은 학습된 위치 임베딩이 아닌 상대적 주의(Shaw et al., 2018; Dai et al., 2019)의 사용이다. 이는 파라미터 카운트를 무시할 수 있게 증가시키지만 향상된 성능으로 이어진다. 또한 GPT-3 시리즈 모델이 학습 속도의 다소 보수적인 선택을 사용하는 것으로 알려져 있기 때문에 향상된 최종 성능을 위해 3개의 가장 큰 모델의 학습 속도를 2배 증가시킨다. 모든 모델의 컨텍스트 크기는 \(2048\)입니다.

2020년 5월에 스크래핑된 159GB Python 데이터 집합인 Codex를 학습하는 데 사용된 동일한 데이터 집합에 코드 모델을 학습합니다. 따라서 HumanEval의 후속 공개 릴리스에서 열차 집합 오염이 발생하지 않을 것으로 예상합니다. GPT-3와 유사하고 코덱스와 달리 무작위 초기화에서 처음부터 모델을 훈련한다. 메인 스캔의 모든 모델은 크기에 관계없이 100B 토큰에 대해 학습됩니다. 이러한 고정된 토큰 예산으로 인해, 우리는 우리의 가장 큰 모델들이 착수될 것으로 예상하며(Hoffmann 등, 2022), 더 긴 트레이닝으로부터 상당한 이익을 얻을 것이다. 자연어 모델의 경우 GPT-3(Brown et al., 2020)에서 사용된 것과 동일한 데이터 세트를 사용하며, 자세한 내용은 해당 논문의 섹션 2.2에 설명되어 있다.

## Appendix B Scaling Trend for FIM rate abllations

섹션 4.2에서 우리는 더 높은 FIM 비율이 원래 기능에 영향을 미치지 않으면서 모델의 FIM 성능을 향상시키는 것을 본다. 이 결론은 사전 훈련 동안 적은 수의 샘플로 측정된 HumanEval 및 가벼운 랜덤 스팬 주입 통과율의 학습 곡선을 기반으로 했다. 이 주장을 추가로 입증하기 위해, 우리는 FIM 레이트들: 0, 0.25, 0.5, 0.75, 0.9, 및 1.0을 갖는 50B 토큰들에 대한 일련의 모델들을 훈련시킨다. 도 12 및 도 13에서, 상이한 FIM 레이트들이 사용될 때의 복잡성 및 샘플링 평가의 모델 스케일링 경향들을 제시한다.

다시 말하지만, 높은 비율의 훈련 데이터를 FIM으로 변환하는 것은 테스트 손실 및 HumanEval 통과율로 측정된 원래 능력의 저하를 초래하지 않는다는 것을 발견했다. 100% FIM 속도에서 복잡성 평가에서 유일하게 눈에 띄는 열화가 관찰된다. FIM 역량의 경우, FIM 비율을 증가시키면 채우기 벤치마크가 크게 향상되고 통과율의 모델 스케일링 경향의 기울기가 변경될 수 있다. 그러나 높은 FIM 비율은 FIM 손실의 상응하는 감소로 이어지지 않으며, 이는 복잡성이 항상 실제 세계 성능을 포착하는 것은 아님을 확증한다.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline Model Name & \(n_{\text{param}}\) & \(n_{\text{ne}}\) & \(n_{\text{layers}}\) & \(d_{\text{model}}\) & \(n_{\text{heads}}\) & \(d_{\text{head}}\) & Batch Size & Learning Rate \\ \hline XXS & 50M & 11M & 6 & 384 & 6 & 64 & 0.5M & \(1.6\times 10^{-3}\) \\ XS & 77M & 26M & 8 & 512 & 8 & 64 & 0.5M & \(1.4\times 10^{-3}\) \\ Small & 164M & 87M & 12 & 768 & 12 & 64 & 0.5M & \(6.0\times 10^{-4}\) \\ Medium & 411M & 308M & 24 & 1024 & 16 & 64 & 0.5M & \(3.0\times 10^{-4}\) \\ Large & 844M & 689M & 24 & 1536 & 16 & 96 & 0.5M & \(2.5\times 10^{-4}\) \\ XL & 1.4B & 1.2B & 24 & 2048 & 16 & 128 & 1M & \(4.0\times 10^{-4}\) \\
2.8B & 2.8B & 2.6B & 32 & 2560 & 32 & 80 & 1M & \(3.2\times 10^{-4}\) \\
6.9B & 6.9B & 6.5B & 32 & 4096 & 32 & 128 & 2M & \(2.4\times 10^{-4}\) \\ \hline \hline \end{tabular}
\end{table}
표 3: 모델 집합에 대한 모델 아키텍처입니다. 6개의 가장 큰 모델은 GPT-3 논문에서 소형에서 6.7B 모델과 유사한 아키텍처를 따른다. 표의 차이는 그 논문의 표 2.1의 사소한 계산 오류와 오타로 인한 것이다. \(n_{\text{param}}\) 열은 각 모델의 총 파라미터 수를 갖는 반면, \(n_{\text{ne}}\) 열은 임베딩 계층과 미 임베딩 계층을 제외한 파라미터의 수를 갖는다. (Kaplan et al., 2020)에 이어서, 우리는 스케일링 플롯에서 비-임베딩 파라미터의 수를 사용한다. 우리는 임베딩 층과 임베딩되지 않은 층에서 가중치를 묶지 않는다.

## FIM 구현 부록 C 세부 정보

패킹 전에 문서 수준에서 FIM을 적용 하는 경우 문자 수준 및 토큰 수준 FIM을 모두 구현 하는 것이 간단 합니다. 우리는 단순히 두 개의 위치를 무작위로 선택하여 문서를 세 개의 섹션으로 나누고 FIM 문서로 포맷한다. 아래 python pseudocode와 같이 인코딩 및 분할 순서만 변경됩니다.

그림 12: 다양한 FIM 비율에 따른 복잡성의 모델 스케일링 경향 비교. 좌우 손실은 100%의 FIM 비율을 사용하지 않는 한 눈에 띄는 열화가 없다(좌측). 또한 일부 FIM 변환(중간 및 오른쪽)으로 모델을 학습했을 때 FIM 손실이 서로 유사하다는 것을 발견했다.

그림 13: 다양한 FIM 비율에 따른 샘플링 평가의 모델 스케일링 경향 비교. FIM 비율을 높이는 것은 HumanEval에 영향을 미치지 않지만 90% FIM 이후 눈에 띄는 개선 없이 채우기 벤치마크에서 일관된 이득을 초래한다. 언뜻 보기에 좌-우 모델이 단일 및 다중 라인 벤치마크에서 자명하지 않은 수의 문제를 해결할 수 있다는 것은 직관에 어긋나는 것처럼 보일 수 있다. 이것은 버그가 아니라 기능입니다. SPM 모드에서 샘플링하고 일부 라인 기반 주입 문제에는 빈 접미사 또는 외부 접미사가 있다. 이러한 결과를 얻기 위해 HumanEval은 분산을 줄이기 위해 작업당 온도 0.8 및 500개의 샘플로 평가되었다. 인간 평가보다 훨씬 더 많은 문제를 가진 모든 주입 벤치마크는 작업당 온도 0.2 및 200개의 샘플로 평가되었다.

[MISSING_PAGE_FAIL:25]

중간 섹션은 선을 기반으로 선택되며 야생에서 보다 일반적인 사용 사례를 포착하지 않는다. 우리는 표준 해의 두 개의 무작위 위치에서 중간 범위를 선택하여 세 번째 채우기 벤치마크를 만들었다. 이 섹션에서는 독자가 새로운 벤치마크를 느낄 수 있도록 이러한 작업의 몇 가지 예를 보여준다. 목표는 강조 표시된 범위를 예측하는 것입니다.

```
fromtypingimportList defhas_close_elements(numbers:List[float],threshold:float)->bool: """Checkifingivenlistofnumbers,areanytwonumberscloserto eachotherthan giventhreshold" "" foridx,eleminenumerate(numbers): foridx2,elem2inenumerate(numbers): ifidx1=idx2: distance=abs(elem-elem2) ifdistance<threshold: returnTrue returnFalse
```

여기서, 모델이 통과하기 위해서는, 1) 가변 거리가 정의되지 않고, 2) 프리픽스가 서브토크에서 종료되고 이를 처리하지 않으면 압흔 오류가 발생하고, 3) 차이가 계산될 때 완료가 인라인으로 중단되어야 한다는 것을 알아야 한다.

```
defrounded_avg(n,m): """Youaregiventwopositiveintegersnandm,andyourtaskisto computethe averageoftheresfromnthroughm(includingnandm). Roundtheanswertothenearestintegerandconvertthattobinary. Ifnisgreaterthanm,return-1. Example: rounded_avg(1,5)=>"0b11" rounded_avg(7,5)=>-1 rounded_avg(10,20)=>"0b1111" rounded_avg(20,33)=>"0b11010" """ ifm<n: return-1 summation=0 forirange(n,m+1): summation+=i returnbin(round(summation/(m-n+1))
```

이것은 누락된 섹션이 여러 줄에 걸쳐 있고 BPE 인코딩 및 토큰 기반 FIM을 사용하는 이전의 모든 작업을 중단하는 서브토큰으로 끝나는 약간 더 어려운 예이다. 이와 같은 사용 사례는 사용자가 현재 구현을 좋아하지 않고 코드 모델로 대체하고자 하는 대략적인 스팬을 빠르게 삭제할 때 코딩 어시스턴트에서 발생할 수 있다.

랜덤 스팬 채움 작업을 무작위로 균일하게 만들기 때문에 실제 발생할 수 있는 다양한 어려움과 코너 케이스의 문제를 자연스럽게 포착한다. 1640개의 태스크가 평가 분산을 줄이는 것과 샘플링 시간 사이에 좋은 균형을 산출했기 때문에 HumanEval에서 문제당 10개의 무작위 태스크를 선택했다.

## 부록 F Dynamics 및 피니튜닝 곡선 학습

섹션 5의 결과에 대한 직관을 추가로 구축하기 위해 피니튜닝 동안 채우기 평가 벤치마크의 역학을 살펴보는 것이 유익하다. 이것은 그림 14에 제시되어 있다. 우리는 보통 인간시대가 특히 더 높은 학습률을 사용할 때 피니튜닝 초기에 크게 저하되지만 훈련 종료까지 사전 훈련과 유사한 수준을 따라잡는 것을 관찰한다. 한편, 랜덤 스팬 인필링 광에서의 성능은 예상대로 0으로 시작하여 피니튜닝 동안 서서히 상승한다.

## Appendix G Top 모델 비교

이 절에서는 단일 라인, 다중 라인 및 랜덤 스팬 주입 벤치마크에서 현재 최상의 주입 모델의 성능을 비교한다. 결과는 표 4에 보고되어 있다. 이 표의 InCoder의 숫자는 논문에서 자체 보고된 숫자이며 프레임워크에서 독립적으로 평가되지 않았다. 평가 프레임워크 간의 구현에 약간의 차이가 있으면 약간의 불일치가 발생할 수 있다.

\begin{table}
\begin{tabular}{c c c c} \hline Model & Single-line infilling & Multi-line infilling & Random span infilling \\ \hline FIM50 & 0.730 & 0.406 & 0.521 \\ FIM90 & 0.751 & 0.441 & 0.551 \\ InCoder & 0.690 & 0.386 & N/A \\ \hline code-davinci-002 & 0.916 & 0.699 & 0.742 \\ \hline \end{tabular}
\end{table}
표 4: 세 가지 주요 채우기 벤치마크 상에서 유사한 크기(6.7B) 및 코드-다빈치-002의 InCoder 모델과 100B 토큰에 대해 50% 및 90%의 FIM 비율로 트레이닝된 우리의 6.9B 파라미터(6.5B 비-임베딩 파라미터) FIM 모델의 비교. 모든 FIM 결과는 SPM 모드에서 얻어진다. 100개의 샘플을 사용하여 모델과 코드-다빈치-002를 평가했으며 샘플링 온도는 0.2이다.

도 14: finetuning 동안 HumanEval 및 랜덤 스팬 주입 광의 역학. 범례는 사전 트레이닝 학습률에 대한 피니튜닝 학습률의 분율에 대응한다. 여기서의 결과는 FIM 속도가 0.9이고 간결성을 위해 FIM 속도가 0.5인 유사한 역학 도표를 생략한다.

Qualitative evaluation

이전에 우리는 채움 능력을 평가하기 위해 코딩 벤치마크의 통과율을 측정했다. 본 절에서는 FIM의 강점 및 개선 영역을 이해하기 위해 샘플을 정성적으로 평가한다. 전반적으로, 우리는 주입이 언어보다 코드 도메인에서 더 잘 작동한다는 것을 발견했다. 그러나, 이전에 동기화된 바와 같이, 채우는 일반적으로 접두사를 확장하는 것보다 더 어려운 작업이다. 우리는 이러한 문제를 예시하고 가능한 완화를 보여준다.

### 성공적인 주입 예제

FIM을 사용하면 모델이 생성 시점 전후 모두에서 정보를 처리할 수 있습니다. 이렇게 하면 특정 작업에 대해 세분화된 특수 모델이 필요했던 새로운 기능이 해제됩니다. 예를 들어, 별도의 docstring 모델을 트레이닝한 Codex(Chen et al., 2021)와 달리, 이제 우리는 가져오기 모듈, 함수 이름, 인수, docstrings, 정의 등을 추론할 수 있는 단일 모델을 갖는다. 우리는 모델이 전체 소스 코드를 읽을 수 없는 한 완료할 수 없는 그러한 예를 아래에 보여준다. 이 예는 또한 접두사 "from sym" 및 접미사 둘 다 서브토켄을 포함한다는 점에서 흥미로우며, 이는 확률적 BPE(Provilkov 등, 2019)와 같은 기술 없이 훈련된 전통적인 언어 모델이 실패하게 하는 것으로 알려져 있다.

```
fromsympyimportisprime deflargestprimefactor(n):== Returnthelargestprimefactorofn;== ans=1 fornuminrange(2,n+1): ifn%num==0andisprime(num): ans=num returnans
```

이점은 코딩에만 국한되지 않습니다. 모델은 기존의 글쓰기에 적응하여 결말을 고려하는 자연스러운 방식으로 대목을 완성할 수 있다.

```
돌핀케어 매우 지능적인 동물, 포유류, 숨쉬는 동물, 바다와 관련된 토발레스, 포포리스, 돌핀케어 매우 유희적인 동물 상업용 잠수부는 마침내 '스네게게아비그캐치시어소우스 화이트'를 생각했다.Butthenheduckyrealizeditwasn!afish--hewaswranglinganalligator. Wikipediaisafree, 웹 기반;collaborative, multiilingualencyclopedia.ItisoverseenbythenonprofitWikimediaFoundation.Wikipediausesa collaborativesoftwareknownaswikithatfacilitatesthecreation anddevelopmentofarticles.
```

### Limitations

**어려운 프롬프트** 입니다. 끝에서 텍스트를 완료하는 것과 달리 채우기는 접두사와 접미사를 연결하는 누락된 범위를 유추해야 합니다. 접미사가 완전히 관련이 없을 때, 모델은 매우 긴 중간 섹션을 생성할 수 있다. 우리는 이 행동을 결말에 합류하는 그럴듯한 궤적을 생각해내려는 모델의 시도로 간주한다. 컨텍스트 크기가 제한적이기 때문에 모델은 일반적으로 조인하지 못합니다. 그러나 이러한 프롬프트 중 일부를 짧은 구절에 채우는 데 어려움이 있다는 점을 감안할 때 이 실패는 태스크 FIM이 얼마나 어려울 수 있는지 보여준다.

아래에서는 모델이 일반적으로 완전히 연결하지 못하거나 끊김 없는 방식으로 결합하지 못하는 어려운 프롬프트를 보여준다. 모델이 그럴듯해 보이는 중간 섹션을 작성하는 경우에도 품질은 종종 달라질 수 있습니다.

치과의사는 내 눈을 바라보며 "이빨을 다 뽑아야겠다"고 말했다.

나는 깜짝 놀랐다. "이빨 다? 우리가 할 수 있는 다른 방법이 없을까?"

'안 될 것 같다'

아무도 미래를 예측할 수 없다.

오스만 제국은 1차 세계대전에서 패배했고 프랑스인들은 워털루에서 패배했다.

**중단 시기를 결정 합니다.* * 모델은 그것이 접미사에 합류했다고 생각할 때 <EOT> 토큰을 예측하도록 트레이닝된다. 프롬프트가 간단해 보일 때에도 언제 끝날지 결정하는 것은 여전히 실무에서 도전이 될 수 있다. 다양한 길이를 갖는 동등하게 유효한 완성들이 많기 때문에, <EOT>를 출력할 확률은 다른 더 긴 후보들에 의해 할인되고 종종 예상보다 작다. 이는 샘플링으로 인해 터미널 심볼을 단순히 놓칠 수 있다는 사실에 의해 더욱 악화된다. 이는 모델이 적시에 끝나는 것처럼 보이지 않고 중간에 유효하지만 가짜 콘텐츠를 생성하는 행동을 초래한다. 이 과정에서 모델은 접두사에 자신의 어미를 쓰도록 선택할 수 있으며, 주어진 접미사를 효과적으로 무시한다.

개는 친근한 동물이다.

코알라는 유쾌한 동물입니다.

원숭이는 장난기 많은 동물이다.

고래는 거대한 동물이다.

올빼미는 현명한 동물이다.

펭귄은 우아한 동물입니다.

크로코다일은 사나운 동물이다.

정지 시점을 모르는 일반적인 문제는 정상적인 좌우 완성에도 적용되지만, 이는 접미사에 합류할 제약이 없기 때문에 채움만큼 큰 문제가 되지 않았다.

**반복**. 모델이 <EOT>를 생성 하지 못하고 접미사를 복사 하는 경우 패턴을 일치 하는 모델의 기능으로 인해 잠그고 프롬프트를 무한정 반복 합니다. 놀랍게도, 대형 모델도 이러한 실패 방식에 취약합니다. 아래 예는 "심장"으로 끝난다. 왜냐하면 모델이 터미널 심볼을 생성하지 못했고 여전히 누락된 스팬을 채우는 중이기 때문에 안타깝게도 멈추지 않을 것이다.

그 길은 하늘에 있지 않다. 그 길은 마음속에 있다.

그 길은 하늘에 있지 않다. 그 길은 마음속에 있다.

그 길은 하늘에 있지 않다. 그 길은 마음속에 있다.

그 길은 하늘에 있지 않다. 그 길은 마음속에 있다.

### Mitigations

성능이 프롬프트의 품질에 의존하는 GPT-3 [Brown et al., 2020]과 같이, 이전 섹션에서의 일부 실패는 프롬프트 엔지니어링으로 완화될 수 있다. 즉, 출력을 제한하기 위한 힌트를 제공하는 것은 모델이 중간 섹션이 얼마나 길어야 하는지에 대한 보다 구체적인 이해를 가지기 때문에 <EOT> 토큰을 생성하고 합리적인 토큰 예산 내에서 접미사에 연결하는 모델의 능력을 극적으로 향상시킬 수 있다.

그러한 아이디어 중 하나는 번호가 매겨진 아이템으로 시작과 끝 모두에 예를 제공하는 것이다. 이것은 모델이 내부적으로 위치를 추적하게 하고, 원하는 접두사 및 접미사에 주의를 기울이게 하며, 일반적으로 아래에 도시된 바와 같이 스퓨리어스 콘텐츠를 생성하는 것을 자제하게 한다. 어떠한 명시적인 단서들 없이 선행적인 예들만을 제공하는 것은 종종 문제를 악화시킬 수 있는데, 이는 모델이 접미사의 시작에 합류해야 하는지 또는 새로운 예의 일부로서 고려해야 하는지의 모호성을 해소하지 않기 때문이다.

1. 개는 친근한 동물이다.

2. 코알라는 수면 동물입니다.

3. 사자는 왕성한 동물이다.

Section 1:

1. 길은 하늘에 없다. 그 길은 마음속에 있다.

2. 평화는 내부에서 옵니다. 없이는 찾지 마세요.

섹션 2: 번호가 매겨진 소수의 샷 프롬프트는 상당히 도움이 되지만 모델이 여전히 실수로 새로운 아이템 목록을 시작할 수 있기 때문에 문제를 완전히 해결하지는 않는다는 점에 유의하는 것이 중요하다.

일반적으로, 모델은 단순히 <eot> 토큰을 샘플링하는 것을 놓칠 수 있으므로, 여러 샘플을 생성하고, 실제로 엔딩에 합류하는 샘플을 선택할 기회를 증가시키기 때문에 <eot>으로 끝나는 샘플을 선호할 것을 추천한다. 다수의 샘플이 <eot>에서 종료될 때, 이들은 관심의 가능성 또는 다른 휴리스틱에 의해 순위화될 수 있다. 우리는 이것을 EOT 인식 최고 표본 추출이라고 부른다.
