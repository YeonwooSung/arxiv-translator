# Scale에서 언어 피드백을 사용 하 여 언어 모델 훈련

Jeremy Scheurer

존 앤더 캠포스

Tomasz Korbak

전선찬

Angelica Chen

Kyunghyun Cho

Ethan Perez

###### Abstract

사전 훈련된 언어 모델은 종종 유해한 텍스트 또는 사실적으로 부정확한 요약과 같은 인간의 선호도와 일치하지 않는 출력을 생성한다. 최근 연구는 모델 생성 출력 쌍 간의 비교라는 단순한 형태의 인간 피드백에서 학습함으로써 위의 문제에 접근한다. 그러나 비교 피드백은 인간의 선호도에 대한 제한된 정보만을 전달한다. 본 논문에서는 보다 유익한 언어 피드백을 활용하는 새로운 접근법인 언어 피드백(ILF)에서 모방 학습을 소개한다. ILF는 입력에서 언어 모델을 컨디셔닝하는 단계, 초기 LM 출력 및 피드백을 생성하는 세 단계로 반복적으로 적용된다. 둘째, 피드백이 가장 많이 포함된 세분화를 선택하는 것이다. 셋째, 입력이 주어졌을 때 선택된 정제의 가능성을 최대화하기 위해 언어 모델을 피니튜닝한다. 우리는 ILF를 인간의 피드백으로부터 강화 학습과 유사하게 베이지안 추론으로 볼 수 있음을 이론적으로 보여준다. 우리는 신중하게 통제된 장난감 작업과 현실적인 요약 작업에 대한 ILF의 효과를 평가한다. 우리의 실험은 대규모 언어 모델이 피드백을 정확하게 통합하고 ILF 척도를 사용한 피니튜닝이 데이터 세트 크기와 잘 일치하여 인간 요약에서 피니튜닝보다 더 우수함을 보여준다. 언어 및 비교 피드백 둘 다로부터 학습하는 것이 각각 단독으로 학습하는 것보다 더 우수하여 인간 수준의 요약 성능을 달성한다.

## 1 Introduction

언어 모델(Language Models, LMs)은 요약부터 질의 응답 및 대화(Radford and Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020; Rae et al., 2021, _inter alia_)에 이르기까지 다양한 NLP 작업에 걸쳐 강력한 성능을 달성한다. 그러나 그들의 주요 한계들 중 하나는 그들이 오보(Lin et al., 2021), 불쾌한 언어(Gehman et al., 2020), 및 사실적으로 부정확한 요약(Stiennon et al., 2020)과 같은 인간의 선호를 침해하는 텍스트를 생성한다는 것이다. 이러한 문제를 완화하기 위해, 기존의 방법들은 LM들이 인간의 선호도 또는 그의 예측 모델에 따라 높은 점수를 매기는 텍스트를 생성하도록 훈련시킨다(Ziegler et al., 2019; Stiennon et al., 2020; Nakano et al., 2021; Ouyang et al., 2022). 이러한 접근법은 두 가지 출력 중 어느 것이 더 나은지에 대한 인간의 피드백에서 배운다. 그러나, 각각의 비교는 인간의 선호도에 대한 제한된 정보만을 전달한다.

우리는 정보의 풍부하고 자연스러운 형태인 언어 피드백에서 배우는 대안적인 접근법을 제안한다.

도 1: 언어 모델(LM) 출력에 대한 언어 피드백으로부터 학습하기 위해, 우리는 피드백에 기초하여 원래 출력의 다중 정제들을 생성하는 LM을 갖는다. 우리는 LM을 사용하여 최상의 정제를 선택하고 선택한 정제의 가능성을 최대화하기 위해 원래 LM을 미세 조정한다.

인간의 피드백. 우리는 언어 피드백으로부터 학습하기 위한 3단계 알고리즘인 언어 피드백(ILF)으로부터의 모방 학습을 소개한다(도 1). 먼저, 입력, 초기 LM 생성 출력 및 출력에 대한 인간 필기 피드백을 고려하여 LM 생성 출력의 다중 정제를 생성한다. 둘째, 피드백이 가장 잘 통합되는 세분화를 선택하기 위해 명령어 조정 LM을 사용한다. 셋째, 주어진 입력에서 선택된 정제에서 초기 출력을 생성한 LM을 미세 조정한다. 이러한 방식으로 언어 피드백을 사용하여 LM을 세분화하고 결과 모델을 사용하여 출력에 대한 더 많은 피드백을 수집하고 위의 정제 및 피네튜닝 접근법으로 학습할 수 있다. 알고리즘의 의사코드(알고리즘 1) 및 대응하는 그래픽 모델은 도 2에 도시되어 있다. ILF는 강화 학습(RL)(Ziegler et al., 2019; Stiennon et al., 2020, _inter alia_) 또는 보조 손실(Stacey et al., 2021)을 사용하는 선행 작업에서 출발하고 자유 형식 언어 피드백을 사용하는 것으로 간단히 일반화할 수 없다.

```
입력: 반복 횟수 \(K\), 원본 문서 집합의 시퀀스 \(\mathcal{C}=[\mathcal{C}_{1},...,\mathcal{C}_{K}]\), 언어 모델 \(\pi_{\theta}\), 정제 언어 모델 \(\pi_{\psi}\), 보상 모델 \(R\) \(1...K\)do에서 문서 \(k\)에 대한 핀튜닝 데이터셋 초기화 \(\mathcal{D}_{k}=\{\}\) \(\mathcal{C}_{k}\)do \(x_{0}\sim\pi_{\theta}(x_{0}|c)\)  인간이 \((c,x_{0})\) \(\{x_{1}^{1},\ldots,x_{1}^{N}\sim\pi_{\psi}(x_{1}|c,x_{0},f)\) \(x_{1}=\text{argmax}_{x_{1}^{n}}R(x_{1}^{ end
```

**알고리즘 1** 언어 피드백에서 모방 학습

우리는 우리의 접근법을 이론적으로 그리고 경험적으로 분석한다. 우리는 ILF가 KL 페널티를 갖는 Human Feedback을 갖는 RL과 유사하게 베이지안 추론으로 볼 수 있음을 보여준다(Korbak 등, 2022). 그런 다음 GPT-3 기반 모델(Brown et al., 2020; Ouyang et al., 2022)을 사용하여 문장에서 불쾌 단어를 제거하는 주의 깊게 제어된 합성 작업에 대해 알고리즘을 검증한다. 우리는 가장 큰 GPT-3 기반 모델(175B 매개변수)만이 출력을 정확하게 정제한다는 것을 발견했다. 이러한 통찰력을 이용하여, Stiennon 등(2020)에 이어, 텍스트 요약에 대한 우리의 알고리즘을 테스트하기 위해 가장 큰 GPT-3 모델을 사용한다. 우리의 작업은 이전 미공개 결과(Scheurer 등, 2022)를 확장하여 ILF가 제공된 피드백의 양으로 LM 생성 요약을 단조롭게 개선하여 최대 5k 샘플을 테스트한다는 것을 보여준다. 모든 데이터 체제에서 ILF는 _인간 작성_ 요약을 미세 조정하는 것과 비슷하거나 더 나은 결과를 도출하며, 이는 우리의 접근법이 인간 시연에 대한 감독 학습에 대한 강력한 대안임을 시사한다. 또한 비교 피드백으로 훈련된 모델을 사용하여 ILF 훈련된 모델에서 N개의 최상의 샘플을 선택하여 언어 및 비교 피드백 모두에서 학습하는 접근법을 소개한다. 하이브리드 접근법은 피드백의 각 형태로만 학습하는 것보다 더 나은 성능을 보여, 인간 평가자가 고품질 인간 참조 요약보다 선호하는 요약이 시간의 약 50.8%로 이어진다. 우리의 분석은 LM 생성 정제가 일반적으로 피드백을 통합한다는 것을 보여주는데, 특히 피드백을 가장 잘 통합하는 정제를 선택하기 위해 LM을 사용할 때 그렇다. 우리의 동시 논문(Chen et al., 2023)에서, 우리는 ILF가 또한 코드 생성에 대해 강한 성능을 달성한다는 것을 보여준다. 우리의 결과는 언어 피드백이 인간의 선호도를 배우는 유망한 방법임을 시사한다.

## 2 Methods

우리는 이제 문제 설정을 공식화하고 우리의 접근법을 설명한다. 우리는 인간의 기호에 따라 개선된 출력 \(x_{1}\)(예: 고품질 요약)을 생성하는 것을 목표로 한다. 초기 모델 생성 출력 \(x_{0}\)과 컨텍스트 \(c\)(예: 소스 문서)에 대해 주어진 언어 피드백 \(f\)을 제공한다. 우리는 언어 피드백에 의해 제공된 증거를 기반으로 LM \(\pi_{\theta}\)을 업데이트하여 이 문제를 해결한다.

본 논문의 목표는 문맥이 주어졌을 때 다양한 출력의 집합(x_{1}\)(예: 문서의 요약)을 샘플링하는 것이다. 여기서 문맥은 문맥 분포에서 추출된다. 보상함수 \(R\)에 의해 측정된 \(x_{1}\)의 품질에 비례하는 지상-진실 분포 \(p_{c}^{*}(x_{1})\)을 근사하기 위해 자기회귀 LM \(\pi_{\theta}\)을 피팅함으로써 그렇게 한다. 적합 \(\pi_{\theta}\)은 문맥 분포 \(p(c)\)에 걸쳐 참 분포 \(p_{c}^{*}(x_{1})\)에서 \(\pi_{\theta}\)으로 예상되는 KL-발산을 최소화하는 것으로 적을 수 있다 \(p(c)\):

\[\min_{\theta}\mathbb{E}_{c\sim p(c)}\mathrm{KL}(p_{c}^{*},\pi_{ \theta}), \tag{1}\] \[\text{where }p_{c}^{*}(x_{1})\propto\exp(\beta R(x_{1}|c))\]

식에서 목표를 최소화합니다. - 교차 엔트로피 손실을 최소화하는 것과 동등한 1(즉, 지도 학습):

\[\mathcal{L}(\theta) = -\mathbb{E}_{c\sim p(c)}\mathcal{L}_{\theta}(c),\] \[\text{where }\mathcal{L}_{\theta}(c) =\sum_{x_{1}}p_{c}^{*}(x_{1})\log\pi_{\theta}(x_{1}|c).\]

그림 2: **왼쪽 상단:** 알고리즘이 근사하는 대상 분포의 그래픽 모델 \(p_{\theta}\)입니다. \ (c\)는 컨텍스트이고 \(x_{1}\)은 고품질 LM 출력이다. **오른쪽 상단:** 중요도 샘플링에 대한 제안 배포의 그래픽 모델 \(q\)입니다. \ (x_{0}\)은 초기 LM 출력이고, \(f\)은 \(x_{0}\)에 대한 언어 피드백이다. **아래:** 학습 알고리즘의 의사 코드입니다.

이 손실은 \(x_{1}\)의 공간의 지수적 크기와 \(p_{c}^{*}(x_{1})\의 정규화 상수 계산의 난해성 등 여러 가지 이유로 정확히 계산하기가 어렵다. 첫 번째 문제를 피하기 위해 \(p_{c}^{*}\)에서 추출한 작은 샘플 집합을 사용하여 몬테카를로 근사 샘플링을 사용한다. 그러나 \(p_{c}^{*}\)에서 직접 샘플링하는 것은 여전히 어렵다. 따라서 샘플링이 더 간단한 제안 분포 \(q_{c}(x_{1})\)를 사용하여 중요도 샘플링을 사용한다.

\[\mathcal{L}_{\theta}(c)=\sum_{x_{1}}q_{c}(x_{1})\frac{p_{c}^{*}(x_{1})}{q_{c}(x_{1})}\log\pi_{\theta}(x_{1}|c) \tag{2}\]

분산을 최소화하려면 \(q_{c}\)을 \(p_{c}^{*}\)과 최대한 가깝게 설계해야 한다. 본 논문에서는 미지의 보상함수 \(R\)를 직접적으로 반영하는 인간의 피드백을 샘플링 과정에 반영하기 위해 \(q_{c}\)을 정의함으로써 이 목표를 달성한다. 문맥 \(c\)이 주어졌을 때 최적이 아닌 LM \(\pi_{\theta}\)으로부터 초기 출력 \(x_{0}\)을 먼저 그리면 된다. 둘째, 우리는 인간에게 \(x_{0}\)을 평가하고 \(c,x_{0})\) 쌍에 대한 언어 피드백 \(f\)을 제공하도록 요청한다. 셋째, 정제된 LM \(\pi_{\psi}\)은 \((c,x_{0},f)\)에 조건화된 정제된 출력 \(x_{1}\)을 생성한다. 이 샘플링 절차에 해당하는 제안 분포는 다음과 같이 적을 수 있다.

\[q_{c}(x_{1})=\sum_{f,x_{0}}\pi_{\psi}(x_{1}|x_{0},f)p(f|x_{0})\pi_{\theta}(x_{ 0}|c).\]

\(x_{1}^{i},\dots,x_{1}^{N}\)을 \(q_{c}(x_{1})\)에서 샘플링된 \(N\) 요약으로 하자. 그런 다음 우리는 목표를 식에서 근사화할 수 있다. 2 as:

\[\mathcal{L}_{\theta}(c)\approx\sum_{i=1}^{N}\underbrace{\frac{p_{c}^{*}(x_{1} ^{i})}{q_{c}(x_{1}^{i})}}_{=\omega^{i}}\log\pi_{\theta}(x_{1}^{i}|c), \tag{3}\]

여기서 \(\omega^{i}\)은 \(q_{c}\)의 \(i\)번째 샘플의 중요도 가중치이다. 중요도 가중치 \(\omega^{i}\)는 샘플을 추출할 수 있는 것 외에 \(q_{c}\)에 접근할 수 없기 때문에 계산이 불가능하다. 우리는 \(q_{c}(x_{1}^{i})\)이 일정하다고 가정함으로써 이 문제를 피하며, 이는 인간 피드백의 높은 품질로 인해 샘플이 모두 동등하게 우수하다는 것을 의미한다. 그런 다음 \(p_{c}^{*}\)의 정의에서 \(R(x_{1}^{i}|c)\)을 \(R(x_{1}^{i}|x_{0},f,c)\)으로 대체한다. 이를 통해 정규화되지 않은 \(p_{c}^{*}\)을 계산할 수 있고, 그 후 자기 정규화를 사용하여 최종적으로 위의 손실을 계산할 수 있다.

이 새로운 텍스트 \([x_{1}]\)는 초기 텍스트 \([x_{0}]\)에 제공된 피드백 \([f]\)을 통합할 수 있는가?와 같은 이진 질문에서 명령-동조된 LM을 컨디셔닝하여 \(R\)을 구현한다. 응답 예 또는 No._입니다. 여기서 레이블 \(y\)은 \(y_{\text{good}}\)("Yes") 또는 \(y_{\text{bad}}\)("No")입니다. 긍정 답변 \(y_{\text{good}\)의 확률을 \(R\)으로 사용합니다. 즉, \(R(x_{1}|x_{0},f,c)=\frac{p(p_{\text{hard}}|\text{prompt})}{p(y_{\text{good}}| \text{prompt})+p(y_{\text{bad}}|\text{prompt})}.\)

마지막으로, \(p_{c}^{*}\), 즉 \(\beta\to\infty\)을 계산할 때 극저온을 사용한다. 자체 정규화 때문에 손실 계산을 위해 \(q_{c}\)에서 샘플링된 컨텍스트 \(c\)당 최상의 요약 \(x_{1}^{*}\)만 사용하는 것과 동일하므로 다음과 같은 최종 목표가 생성됩니다.

\[\mathcal{L}(\theta)\approx\mathbb{E}_{c\sim p(c)}\log\pi_{\theta}(x_{1}^{*}|c) \tag{4}\]

그라운드 트루스 분포 \(p_{c}^{*}(x_{1})\)을 근사하는 우리의 목적은 보상 \(R\)에 비례하는 보상 \(R\)이 RL에서 보상을 최대화하는 것과 분명한 연관성을 가지고 있다는 것이다. 그러나 RL에서는 보상을 최대화하는 최상의 정책을 찾는 것이 목표인 반면, 본 알고리즘은 문서 \(c\)가 주어지면 고품질의 출력 \(x_{1}\)이 분포하여 높은 보상을 달성하는 다양한 출력 집합을 도출할 수 있다. 고품질 출력의 광범위한 다양성은 다운스트림 사용자 및 시스템이 어떤 측면을 선호하고 피하고 싶은지에 대한 더 많은 제어를 부여한다. App. A.1에서는 변분 추론을 따르는 ILF의 대안적 유도를 추가로 제공하고 ILF도 베이지안 추론으로 이해될 수 있음을 보여준다. 이 프로세스는 언어 피드백에 의해 제공된 증거에 기초하여 LM을 업데이트하는 것을 포함한다. 이러한 상이한 렌즈는 ILF와 RL 사이의 대응을 Human Feedback(Ziegler et al., 2019; Stiennon et al., 2020, _inter alia_)과 강조하는데, 이는 이전에 베이지안 추론과 동등한 것으로 증명되었다(Korbak et al., 2022).

## 3 언어 모델이 피드백을 사용할 수 있습니까?

알고리즘이 작동하려면 LM은 피드백을 정확하게 통합하여 정제 작업을 생성할 수 있어야 한다. 따라서, 먼저 주어진 문장에서 특정 불쾌 단어를 제거하는 주의 깊게 제어된 합성 작업에 대한 알고리즘의 정제 단계를 검증한다. 다양한 모델이 피드백을 얼마나 효과적으로 통합하여 출력을 정제하는 데 사용할 모델을 결정하는지를 조사한다.

실험 설정에서는 LM에게 \(\leq 3\)개의 특정 단어를 제거하여 \(\leq 10\)개의 불쾌한 단어로 자동 생성된 문장을 정제하도록 지시한다(자세한 설명과 예제는 부록 D 참조). 이 실험에서 우리는 탐욕스런 디코딩을 통해 샘플당 하나의 출력을 생성하는데, 즉 최우량\(N\)으로 샘플링하지 않는다. 우리는 생성된 정제가 목표 문장과 얼마나 자주 정확히 일치하는지 평가하며, 이는 자동으로 생성한다. 본 LM의 경우, 서로 다른 크기의 GPT-3 모델(Brown et al., 2020)과 텍스트-다빈치-001을 사용하는데, 명령어-피네튜닝된(Feedback Made Easy or FeedME) 대응물(Ouyang et al., 2022; OpenAI, 2022b.1)에 사용된 모든 하이퍼파라미터를 보고한다.

[MISSING_PAGE_FAIL:4]

정련은 초기 요약에 대한 언어 피드백을 포함하고 따라서 고품질의 요약, 즉 \(p(y_{\text{good}}|\text{prompt})\)이다. LMs는 사용된 정확한 프롬프트(Perez et al., 2021; Lu et al., 2021)에 민감하므로 5개의 다른 프롬프트를 작성하고(App. J.2 참조) 평균이 가장 높은 정제 \(p(y_{\text{good}}|\text{prompt})\)을 선택하여 이 방법을 InstructRM Ensemble이라고 부른다.

Scoring Refinements with Embedding Similarity 이전 작업(Scheurer et al., 2022)은 contrastive prerained text-embedding 함수(Neelakantan et al., 2022)를 사용하여 피드백 \(f\)과 정제 \(x_{1}^{1},...,x_{1}^{5}\)을 임베딩하고 피드백과 코사인 유사도가 가장 높은 정제 작업을 선택한다. 피드백은 종종 이상적인 텍스트가 어떻게 생겼는지 설명하기 때문에 이 점수 부여 기능을 사용합니다. 이 방법은 좋은 정제들이 피드백과 의미적으로 유사하다고 가정하기 때문에 덜 일반적이며, 이는 모든 태스크들 또는 피드백의 형태들에 대해 반드시 그럴 필요는 없다.

결과들은 이제 인간 평가자들에 의해 주어진 랭킹에 따라, 방법에 의해 선택된 세분화가 무작위로 선택된 세분화("승률")보다 더 나은 시간의 분율을 계산함으로써 개발 데이터세트에 대한 상기 랭킹 방법들을 평가한다(더 자세한 내용은 App. E 참조). 표 2에 도시된 결과는 임베딩 유사성 선택이 랜덤 선택을 능가하지 않는 반면, 대부분의 (4/5) InstructRM 프롬프트는 그렇지 않음을 보여준다. 임베딩 유사도는 이전 작업(Scheurer et al., 2022)에서 잘 작동했지만 우리의 데이터 세트에서는 잘 작동하지 않는다. 많은 주석자가 작성한 피드백이 훨씬 더 다양하기 때문이라고 생각하지만, Scheurer 등(2022)에서는 저자가 직접 피드백을 작성했다. InstructRM Ensemble은 랜덤 선택에 대한 승률이 \(56.0\pm 3.0\%\)으로 LM이 자신의 출력을 어느 정도 평가할 수 있음을 보여준다. 이러한 결과를 바탕으로, InstructRM Ensemble 방법은 성능이 좋고 특정 프롬프트에 덜 민감하기 때문에 사용하는 것이 좋다. 논문 전체에서 우리는 점수 함수로 InstructRM 앙상블을 사용하여 정제 항목을 선택하고 정제 항목을 생성 및 선택하는 방법을 _피드백 + N의 Best_로 참조한다.

### 피드백 학습 알고리즘 비교

본 절에서는 언어 피드백, 바이너리 피드백, 정상 지도 피니튜닝에서 학습하기 위한 다양한 알고리즘을 비교한다. 각 방법에 대한 개요를 제시한 다음 평가 결과를 제공한다.

#### 4.3.1 Methods

이 평가를 위해, 우리는 언어 피드백으로부터 학습하기 위해 ILF의 단일 반복을 사용한다. GPT3-175B (davinci) (Brown et al., 2020)5는 Refinement with Feedback + Best of N으로 생성된 refinement를 사용하여 입력 프롬프트(Reddit title, and post로 구성됨), 즉 \(\log p(x_{1}|\text{prompt})\이 주어졌을 때 정제의 로그 우도를 최대화하도록 조정한다. 모든 피니튜닝 방법에 대해 손실(Radford et al., 2018; OpenAI, 2022a)에 \(\lambda\log p(\text{prompt})\)을 추가하여 프롬프트의 로그 확률을 최대화한다. 프롬프트 손실 가중치 \(\lambda\in[0,1]\)는 개발 데이터 집합에서 선택 됩니다 (단락 _인간 요약에 대 한 Finetuning_ 참조). 선택한 하이퍼파라미터는 앱에 자세히 설명되어 있습니다. G와 피니튜닝 프롬프트는 App. J.3에 있다.

각주 5: FeedME는 OpenAI의 API를 통해 피니튜닝될 수 없다.

여기서 인간 요약에 대한 유한조정은 추가 손실 항인 \(\log p(x_{\text{human}}|\text{prompt})+\lambda\log p(\text{prompt})\)와 함께 입력 프롬프트(Reddit 제목과 포스트로 구성됨)가 주어졌을 때 인간 요약의 로그 확률을 최대화하기 위한 목적으로 인간 작성 요약 \(x_{\text{human}}\)의 데이터셋에서 GPT3-175B를 유한조정한다. 다양한 크기(100, 1K, 5K)의 인간이 작성한 요약 데이터 세트에 대한 철저한 하이퍼파라미터 튜닝을 수행하여 미세 조정된 모델의 최상의 성능을 보장한다. 최적화된 하이퍼파라미터는 OpenAI 문서(OpenAI, 2022a)에 설명된 대로 훈련 에포크 수, 프롬프트 손실 가중치 \(\lambda\), 학습률 승수를 포함한다. 개발 데이터 세트의 예측된 요약의 복잡성을 사용하여 가장 효과적인 하이퍼 매개 변수를 선택한다. 선택된 하이퍼파라미터는 모든 데이터 세트, 즉 정제, 초기 요약 및 인간에 대한 피니튜닝에 적용된다.

도 3: 인간 평가자들이 얼마나 자주 인간 요약보다 ILF, OPT-RM best-of-64 FeedME, ILF+OPT-RM(best-of-64), finetuning baselines 및 FeedME로부터의 요약을 선호하는지. ILF + OPT-RM(best-of-64)은 인간의 요약과 유사한 품질의 요약들을 생성한다.

동일한 표본 크기로 작성된 요약입니다. 하이퍼파라미터 튜닝에 대한 자세한 내용은 부록 G에서 확인할 수 있다.

초기 요약에 대한 피니튜닝(Finetuning on Initial Summaries)은 (FeedME에 의해 생성된) 초기 요약의 데이터세트에서 GPT3-175B를 피니튜닝한다. 목적은 추가 손실 항인 \(\log p(x_{0}|\text{prompt})+\lambda\log p(\text{prompt})\)를 사용하여 Reddit 제목과 포스트로 구성된 프롬프트가 주어졌을 때 초기 요약의 로그 확률을 최대화하는 것이다. 하이퍼파라미터 튜닝에 대한 자세한 내용은 단락 _인간 요약에 대한 피니튜닝_ 및 부록 G에서 찾을 수 있다.

Binary Feedback에서 학습: Best-of-\(N\)우리는 피드백에서 학습하기 위한 표준 접근법인 Binary Feedback과 ILF를 비교한다. 이진 피드백에서 학습하는 한 가지 방법은 보상 모델을 훈련하고 이를 사용하여 최상의 샘플링을 수행하는 것이다. 우리는 종종 인간 피드백으로부터의 RL과 경쟁적이기 때문에 best-of-N을 사용한다(Nakano et al., 2021), 매우 효과적이지만 보다 정교한 접근법(Stiennon et al., 2020; Ouyang et al., 2022). RM 학습을 위해 OPT-13B (OPT-RM) (Zhang et al., 2022)를 이용하여 요약 \(x_{0}\)이 고품질인지 아닌지를 분류한다. 그렇게 하기 위해, 우리는 지시문 _위 내용이 주어진 텍스트에 대한 훌륭한 요약문임?을 사용한다. 뛰어난 요약은 일관성 있고 정확하며 간결하고 상세합니다. 예 또는 No._로 응답 합니다. 여기서 레이블 \(y\)은 \(y_{\text{good}}\)(" Yes") 또는 \(y_{\text{bad}}\)(" No")입니다. 두 요약 중 선호하는 사람의 레이블이 주어지면 선호하는 요약은 \(y_{\text{good}}\)으로, 다른 요약은 \(y_{\text{bad}}\)으로 레이블링한다. 그런 다음 개발 데이터 세트를 사용하여 선택한 \(\log p(y|x_{0})+\lambda\log p(x_{0})\), 여기서 \(\lambda\in[0,1]\), \(y\in\{y_{\text{good}},y_{\text{bad}}\}\)을 최대화하도록 LM을 조정한다. 피니튜닝된 LM을 이용하여 \(p(y_{\text{good}}|x_{0})\)를 계산하여 주어진 요약을 평가하고 더 높은 확률로 요약을 선택한다. 우리는 이 접근법이 Stiennon 등(2020)의 일반적으로 사용되는 방법과 같은 다른 RM 훈련 방법보다 더 정확한 RM으로 이어진다는 것을 발견했으며, 비교는 부록 F를 참조하고 사용된 프롬프트는 부록 J.4를 참조한다. 개발 데이터 세트의 분류 정확도를 선택 기준으로 사용하여 OPT-RM에 대한 베이지안 하이퍼파라미터 최적화를 수행하고 학습 속도, 배치 크기 및 프롬프트 손실 가중치 \(\lambda\)를 스윕한다(자세한 내용은 부록 G 참조).

ILF + Binary Feedback으로부터의 학습 최종 단계로서, 먼저 단락 Finetuning on refinements(ILF)에 설명된 바와 같이 정제들에 대해 GPT3-175B를 finetuning함으로써, ILF와 이진 피드백으로부터의 학습을 결합한다. 그런 다음 보상 모델인 OPT-RM을 훈련하고 이진 피드백 학습 단락에 설명된 대로 최적의 샘플링을 수행하는 데 사용한다. 테스트 시간에는 피니튜닝된 모델을 사용하여 64개의 요약을 생성하고 OPT-RM을 사용하여 \(p_{\text{norm}}(y_{\text{good}}|x_{0})\)의 고품질 요약 확률에 따라 순위를 매긴다. 그 다음, 가장 높은 정규화된 확률을 갖는 요약이 선택된다.

#### 4.3.2 Evaluation

100, 1K, 5K 트레인 샘플을 사용하여 텍스트 요약 작업에 대한 인간의 서면 참조 요약, 몇 가지 피니튜닝 기준선 및 OPT-RM과 비교하여 학습 알고리즘의 효율성을 평가한다. 698개 샘플의 테스트 데이터세트를 사용하여, 각 방법에 대한 요약을 생성하고, 요약 간의 유대를 허용하는 표준 랭킹 스킴을 사용하여 품질을 기준으로 순위를 매기는 인간 평가자와 함께 평가한다(더 자세한 내용은 App. G 참조). 순위를 기반으로 각 방법의 샘플링 요약이 "승률"이라고 하는 인간이 쓴 참조 요약보다 더 나은 시간 비율을 계산한다. 핵 샘플링(Holtzman et al., 2019)과 온도 \(p=0.95\) 및 온도 \(t=1.0\)을 사용하여 최대 48개의 토큰 길이 요약(Stiennon et al.(2020)을 샘플링한다. 하이퍼파라미터 및 후처리에 대한 자세한 내용은 App. G 참조). 우리는 이진 피드백에서 학습하기 위해 FeedME에서 샘플링된 요약과 함께 64개 중 가장 좋은 샘플링을 사용한다.

#### 4.3.3 Results

우리의 결과는 그림 1에 나와 있다. 3은 Finetuning on refinements (ILF)가 FeedME(FeedME)의 샘플링을 포함한 다른 모든 Finetuning 방법들(6)보다 우수한 것으로 나타났으며, 인간 요약에 대한 승률은 \(31.3\pm 1.7\%\), 다른 방법은 \(27.3\pm 1.7\%\), \(28.9\pm 1.7\%\), 그리고 \(22.5\pm 1.6\%\)의 승률을 달성하였다. 인간이 작성한 요약이 일반적으로 더 높은 품질임에도 불구하고, ILF가 모든 샘플 크기에 걸쳐 인간 요약에 대한 피니튜닝보다 성능이 우수하다는 것은 놀라운 일이다(도 4, 상단 참조). 추가 평가(App. 참조). 도. 8) 1K 정제(ILF) 상에서 피니튜닝된 모델이 인간 요약의 검증 데이터세트에서 평가될 때 인간 요약에서 피니튜닝된 모델에 비해 정제들의 검증 데이터세트에서 평가될 때 상당히 낮은 손실을 나타내는 것을 보여주며, 이는 모델이 정제들의 분포를 근사화하는 데 더 능숙함을 시사한다. 또한 다양한 열차 데이터 세트의 1K 샘플 요약에서 GPT3-175B를 평가할 때 인간 요약 데이터 세트보다 정제 데이터 세트에서 상당히 낮은 손실을 관찰한다(표 6 참조). 전반적으로, 이러한 결과는 피드백을 정확하게 통합하고 모델 성능을 개선하는 데 있어 제안된 ILF 접근법이 인간 요약에서 피니튜닝보다 훨씬 더 우수함을 보여준다.

각주 6: 100개의 정제들에 대한 피니튜닝은 100개의 초기 요약들에 대한 피니튜닝과 결부된다.

(Scheurer et al., 2022)는 100개의 피드백 샘플을 갖는 ILF가 FeedME를 능가하는 반면, 여기서 우리는 100개의 피드백 샘플을 갖는 FeedME를 과소수행한다는 것을 발견했다. 이전 작업은 개선해야 할 사항을 종종 전달하는 저자 작성 피드백을 사용하지만 우리의 작업에는 더 다양하고 크라우드소싱된 피드백이 포함된다. 결과적으로 임베딩 유사성이 인간 피드백 데이터 세트에 대한 정제 순위를 제대로 매기지 않는다는 것을 관찰하며(표 2), 피드백의 차이가 이 섹션에서도 결과의 차이의 중요한 원인이 될 수 있다고 믿으며 자세한 내용은 부록 H.5를 참조한다.

우리의 결과는 FeedME 요약에서 64개 중 가장 좋은 샘플링을 위해 OPT-RM을 사용하는 것이 모든 샘플 크기에 걸쳐 모든 피니튜닝 방법 및 샘플링 접근법보다 우수함을 보여준다. OPT-RM best-of-64 FeedME의 향상된 성능은 best-of-\(N\) 샘플링을 위한 추가 추론 시간을 필요로 한다. ILF와 이진 피드백(ILF + OPT-RM(best-of-64))을 결합한 학습은 5K 샘플을 사용하여 \(50.8\pm 1.9\%\)의 승률로 인간 수준의 요약 성능을 달성한다. 이는 두 방법 모두 함께 사용할 때 누적될 수 있는 인간의 선호도에 대한 가치 있는 정보를 독립적으로 학습한다는 것을 시사한다. ILF + OPT-RM(best-of-64)에 대한 결과는 상이한 비교 요약을 갖는 별개의 인간 평가를 통해 획득된다는 점에 유의해야 한다(App. 도 9 참조). (p<0.05). 참고로 도 3과 같다. 앱 H.3에서 ILF의 여러 반복에 대한 몇 가지 초기 유망한 결과를 제시한다. 이러한 결과는 이 방법이 효과적임을 시사하지만 더 잘 이해하기 위해서는 추가 실험이 필요하다.

### 언어 피드백이 세분화를 개선합니까?

ILF의 개선은 미세조정에 사용되는 미세조정이 고품질임을 시사하므로 여기에서 언어 피드백이 고품질에 대한 책임이 있는지 여부를 조사한다. 이를 위해, 우리는 SS4.2와 유사한 여러 다른 방법의 요약에 대해 피드백 + N 요약의 Best of N 요약으로 세분화 순위를 갖는다. 우리는 인간 순위를 사용하여 각 방법과 초기 요약 사이의 승률을 계산한다. 우리는 정제 \(\in x_{1}^{1},\dots,x_{1}^{5}\)을 무작위로 선택하는 피드백과 정제 \(\in x_{1}^{1},\dots,\dots,\dots,\dots,\dots,\dots)을 비교한다. 이 절제는 채점 함수 \(R\), 즉 InstructRM Ensemble을 사용하여 정제 선택의 중요성을 평가하는 데 도움이 된다. 또한 LM이 초기 요약을 정제하도록 지시하지만 피드백이 없는 정제도 평가한다. 이 절제는 언어 피드백 사용의 중요성을 평가하는 데 도움이 된다. 마지막으로 FeedME에 의해 생성된 인간 요약과 초기 요약, 즉 초기 요약 \(x_{0}\)을 평가한다. 우리는 검증 데이터 세트의 모든 방법을 평가한다.

결과. 도 4(상단)은 초기 요약에 대한 다양한 방법으로부터의 요약의 승리율을 도시한다. 놀랍게도, 모델에 피드백 없이 출력을 개선하도록 지시하는 것은 이미 초기 요약에 비해 상당한 개선(승률 \(59.4\pm 2.1\%\)으로 이어진다. Refinements with Feedback은 향상된 승률 \(63.9\pm 2.0\%\)을 달성하여 언어 피드백이 정제 품질을 향상시키는 데 유용함을 보여준다. Feedback + Best of N을 사용한 개선은 \(69.1\pm 1.9\%\)의 훨씬 더 나은 승률을 달성하여 InstructRM Ensemble을 사용한 Best-of-N이 더욱 개선되었음을 강조한다. 전반적으로, 언어 피드백은 고품질 정제, 특히 Best-of-N 샘플링을 사용할 때 중요하다.

### Refinements Incorporate the feedback?

다른 방식으로 요약을 개선하기보다 피드백을 통합하여 개선이 더 고품질인지 여부를 결정하기 위해 크라우드 작업자가 다양한 방법으로 생성된 개선에 피드백의 가장 중요한 지점이 얼마나 자주 통합되는지 평가하는 검증 데이터 세트에 대한 연구를 수행한다. 에 도시된 바와 같이. 4, 바닥, 피드백 + N의 Best를 이용한 개선 방법은 피드백에서 가장 중요한 점을 가장 자주 포함(\(57.4\pm 2.2\%\)한다. 피드백을 이용한 개선은 피드백 \(49.6\pm 2.2\%\)의 시간을 통합하며, 최상의 N 샘플링은 피드백이 통합되는 빈도를 향상시킨다는 것을 보여준다. 참고로, 피드백이 없는 세분화는 모델이 언어를 받지 않았음에도 불구하고 피드백 \(30.8\pm 2.1\%\)에서 가장 중요한 점을 수정한다.

그림 4: **상단**: 인간 평가자는 모든 정제 방법에서 초기 요약(FeedME)보다 요약을 선호합니다. 피드백 + 베스트-of-5로 정제하면 최고 등급으로 평가됩니다. **아래**: 피드백 + 베스트 오브 5로 세분화는 일반적으로 가장 중요한 피드백 포인트를 통합합니다.

피드백 인간 요약은 피드백을 명시적으로 받지 않았음에도 불구하고 처음부터 요약을 작성할 때 피드백 \(74.0\pm 1.9\%\)에서 가장 중요한 점을 다룬다. 우리의 결과는 개선이 피드백에서 가장 중요한 점을 통합하기 때문에 부분적으로 고품질임을 시사한다.

### 데이터 세트 변경 모델을 가장 잘 튜닝하는 방법은 무엇인가요?

여기서는 피니튜닝에 사용된 요약이 피니튜닝 후 모델이 얼마나 변화하는지 어떻게 영향을 미치는지 이해하는 것을 목표로 한다. Gao 등(2022)은 이진 인간 피드백으로 최적화된 모델들이 그들의 출력 분포가 초기, 사전 훈련된 LM으로부터 더 많이 벗어날 때 바람직하지 않은 행동들을 학습할 가능성이 더 높다는 것을 발견한다. 이러한 결과가 언어 피드백으로 훈련된 모델에 적용되는지 여부는 불분명하지만, 우리는 언어 피드백 훈련된 모델을 이해하기 위해 이 방향의 예비 단계를 밟는다. 특히, ILF-finetuned 모델과 ILF-training 전 사전 훈련된 LM(\(D_{\text{KL}}(\text{finetuned}|\text{GPT3-175B})) 사이의 (역방향) KL 발산(following Gao et al., 2022)을 finetuned 모델에서 무조건 샘플링하고 GPT3-175B로 생성된 텍스트의 로그 우도를 평가하여 측정한다. 또한 전방 KL 발산 \(D_{\text{KL}}(\text{GPT3-175B}|\text{finetuned})\)을 보고한다. 참고로 초기 요약과 인간 요약에서 미세 조정된 모델에 대해 위의 두 가지 모두를 평가한다.

결과. 정제(Finetuning on refinements; ILF)는 양방향으로 가장 큰 KL 발산을 나타내고, 그 다음이 인간 요약에서 finetuning, 그리고 그 다음이 초기 요약에서 finetuning; App을 참조한다. 정확한 숫자에 대한 표 6. 정련에 대한 피니튜닝이 인간 요약에 대한 피니튜닝보다 더 높은 KL 분산을 초래한다는 것이 놀랍다는 것을 발견했으며, 우리는 정련이 인간 요약에 비해 모델의 초기 출력 분포에 더 가까울 것으로 예상하여 피니튜닝된 모델이 더 적은 변화를 겪도록 했다. ILF를 사용한 더 큰 KL 발산은 그림 3에서 관찰된 인간 평가의 더 큰 이득에 부분적으로 책임이 있을 수 있다.

## 5 관련 작업

우리의 작업은 우리의 이전 보고서(Scheurer et al., 2022)를 기반으로 하며, 이는 대형 LM이 언어 피드백으로 출력을 정제할 수 있음을 보여주었다. 이에 본 논문에서는 ILF가 구축하는 3단계 알고리즘을 소개하며, 여기서 우리는 LM, 즉 InstructRM Ensemble을 사용하여 세분화가 피드백을 통합하는지 여부를 평가하는 반면, Scheurer 등(2022)에서는 대조적인 사전 훈련된 텍스트 임베딩 함수(Neelakantan 등, 2022)를 사용한다. 인스트럭트RM 앙상블은 피드백에 대한 정제의 의미적 유사성을 가정하지 않기 때문에 이 임베딩 유사성보다 더 일반적이다. 또 다른 차이점은 ILF가 반복적인 정제 및 피네튠 알고리즘이며, 이는 인간 피드백이 있는 RL에 해당하는 베이지안 추론으로 이해될 수 있다는 것이다. 또한, 여기서 우리는 Scheurer 등(2022)에서와는 다르고 더 광범위한 실험을 수행하고 인간 주석기를 사용한다. 특히, 우리는 인간 요약에서 ILF가 피니튜닝보다 성능이 우수하며, 이진 피드백에 의한 학습과 ILF를 결합하는 것이 대략적인 인간 수준의 요약 성능을 달성한다는 것을 보여준다. Scheurer 등(2022)에 대한 보다 상세한 비교는 App. H.5를 참조한다.

우리의 후속 작업은 우리의 접근 방식을 개선하기 위한 몇 가지 방법을 제안한다. Saunders 등(2022)은 LMs 자체가 LM 출력에 대해 고품질 피드백을 기록한다는 것을 보여준다. Bai 등(2022)은 이어서 ILF를 사용하여 대화 어시스턴트를 트레이닝하여 LM-기입 언어 피드백으로부터 학습함으로써, 인간 피드백을 수집하는 비용 및 노력을 제거한다. Liu 등(2022); Schick 등(2022)은 LMs를 피드백에 기초하여 출력을 정제하도록 훈련시키는데, 이는 ILF에 통합될 때 결과를 개선하는 접근법으로서, 우리의 후속 작업(Shi 등, 2022)에서 보여진다.

다른 작업은 우리보다 다른 방식으로 언어를 사용하는 것을 목표로 한다. 일부 작업은 _금 라벨링된 출력_ 내지 _분류 작업_에 대한 설명을 사용하여 조사하는 반면, 우리의 작업은 분류 작업을 다음과 같이 공식화할 수 있는 보다 일반적인 텍스트 생성 설정을 다룬다(Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020). 설명에서는 레이블이 지정된 출력이 올바른 이유를 설명하는 반면 피드백에서는 후보의 출력을 개선하는 방법을 설명합니다. 이전 작업은 혼합된 결과와 함께, 텍스트 분류 모델을 트레이닝하기 위해 설명을 사용하는 방법을 탐색한다(Camburu et al., 2018; Stacey et al., 2021; Pruthi et al., 2021; Wiegreffe et al., 2021; Hase and Bansal, 2021; Lampinen et al., 2022, _inter alia_). 몇몇 선행 작업들은 또한 출력들을 생성하기 보다는 후보 출력들의 순위를 매길 목적으로 언어 피드백으로부터 학습한다(Weston, 2016; Li et al., 2016; Hancock et al., 2019; Li et al., 2022; Xu et al., 2022). Matiana 등(2021)은 언어 피드백의 텍스트 임베딩을 학습하며, 여기서 개선사항들은 우리 알고리즘의 정제-점수화 단계에 도움이 될 수 있다. 언어는 또한 앱 B에서 논의된 바와 같이 RL 설정에서도 다양한 목적으로 사용되었다.

다른 여러 연구에서는 베이지안 추론과 LMs에 대한 학습 알고리즘 사이의 연결을 도출한다. Korbak 등(2022)은 KL-정규화된 RL이 변분 추론과 동등하다는 것을 보여준다: 보상 함수에 의해 제공된 증거에 부합하도록 사전 LM을 업데이트하는 방법을 명시하는 베이지안 사후 근사화. 도한 등(2022)은 추가로, 프롬프트된 LM들과 다른 에이전트들(예를 들어, 언어 피드백을 제공하는 인간들) 사이의 다수의 라운드의 상호작용을 통해 출력을 생성하는 프로세스가 확률적 프로그램들을 실행하는 것으로 볼 수 있다고 주장한다.

## 6 Conclusion

본 논문에서는 LM이 인간의 선호도에 따라 행동하도록 학습하기 위한 반복 알고리즘인 언어 피드백(ILF: Language Feedback)으로부터 언어 피드백으로부터 학습함으로써 모방 학습을 제안한다. 우리는 주의 깊게 제어된 단어 제거 작업에 대한 접근법을 검증하여 큰 LMs(175B 매개변수)만이 피드백을 정확하게 통합한다는 것을 보여준다. 이 통찰력을 사용하여 텍스트 요약의 실제 작업에 대한 알고리즘을 테스트합니다. ILF와 이진 피드백의 학습을 결합하면 GPT-3 모델이 대략 인간 수준의 요약 능력에 도달했다. ILF 자체는 인간의 요약이 더 높은 품질임에도 불구하고 인간의 요약에서 피니튜닝을 능가하여 모델이 정제 분포를 근사화하는 데 더 우수함을 시사한다. 우리의 작업은 언어 학습을 위한 알고리즘 개선에서 희박하거나 이진 피드백으로 학습하기 어려운 설정 해결까지 향후 작업을 위한 많은 길을 열어줍니다.

## 7 Acknowledgements

우리는 냇 맥앨리스, 제프리 어빙, 제프 우, 얀 라이케, 캐시 예, 윌리엄 손더스, 조나단 워드, 샘 보먼, 다니엘 지글러, 세라피나 닉스, 퀸틴 포프, 케이 코자로네크, 피터 하세, 아사 쿠퍼 스틱랜드, 제이콥 파우, 데이비드 린드너, 레나트 하임, 니타르산 라즈쿠마르, 캐스룸판테, 파블로 모레나, 에드윈 첸, 스콧 하이너, 데이비드 도한에게 도움이 되는 대화와 피드백에 감사한다. Jeremy Scheurer와 Jun Shern Chan은 이 연구를 가능하게 한 자금 지원에 대해 Open Philanthropy에 감사한다. Ethan Perez는 국립과학재단과 열린 자선단체에 펠로우십 지원에 감사한다. 존 안더 캄포스는 스페인 MECD의 박사학위 보조금의 지원을 받는다. 안젤리카 첸과 조경현은 NYU 데이터사이언스 국립과학재단 센터(1922658호)의 지원을 받는다. KC는 42dot, 현대자동차(뉴럴 시퀀스 모델링의 불확실성 프로젝트 아래), 삼성종합기술원(차세대 딥러닝: 패턴 인식에서 AI에 이르는 프로젝트 아래), NSF 어워드 1922658 NRT-HDR: 미래 기반, 번역, 데이터 사이언스 책임 API 학술 접근 프로그램을 통해 모델에 접근과 크레딧을 제공한 OpenAI에 감사드린다.

## References

* J. Andreas, D. Klein, and S. Levine (2017-06)Modular multi-task reinforcement learning with policy sketches. In Proceedings of the 34th International Conference on Machine Learning, pp. 70. Cited by: SS1.
* J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Jerry, Q. V. Le, and C. Sutton (2021) Program synthesis with large language models. CoRRabs/2108.07732. External Links: Link, 2108.07732 Cited by: SS1.
*Y. 배성훈 카다바스 Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al. (2022)Contitutional ai: harmlessness from ai feedback. arXiv preprint arXiv:2212.08073. Cited by: SS1.
* T. B. Mann, N. 라이더 Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020)Language models are few-shot learners. Neural Information Processing Systems33, pp. 1877-1901. External Links: Link, 2005.14165 Cited by: SS1.
*O. 함부루 록태셀 Lukasiewicz, P. Blunsom (2018)Neural language inference with natural language explanation. 신경 정보 처리 시스템31의 발전. 외부 링크: 링크, 1802.01193에 의해 인용됨: SS1.
*O. 함부루 록태셀 Lukasiewicz, P. Blunsom (2018)Neural language inference with natural language explanation. 신경 정보 처리 시스템31의 발전. 외부 링크: 링크, 1802.01193에 의해 인용됨: SS1.
* A. Chen, J. Scheurer, T. Korbak, J. A. Campos, J. S. Chan, S. R. Bowman, K. Cho, and E. Perez (2023)Improved code generation by training with natural language feedback. arXiv preprint arXiv:2303.16749. Cited by: SS1.
* D. Dohan, W. Xu, A. Lewkowycz, J. Austin, D. Bieber, R. G. Lopes, Y. Wu, H. Michalewski, R. A. Saurous, J. Sohl-Dickstein, et al. (2022)Language model cascades. arXiv preprint arXiv:2207.10342. Cited by: SS1.
* A. Elgohary, S. Hosseini, and A. Hassan Awadallah (2020)Speak to your parser: interactive text-to-SQL with natural language feedback. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online, pp. 2065-2077. External Links: Link, 2007.10342 Cited by: SS1.
Gao, L., Schulman, J., and Hilton, J. Scaling laws for reward model overoptimization, 2022. URL [https://arxiv.org/abs/2210.10760](https://arxiv.org/abs/2210.10760).
* Gehman et al. (2020) Gehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith, N. A. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. _ arXiv preprint arXiv:2009.11462_, 2020. URL [https://aclanthology.org/2020.findings-emnlp.301.pdf](https://aclanthology.org/2020.findings-emnlp.301.pdf).
* Goyal 등(2019) Goyal, P., Niekum, S., and Mooney, R. J. Using Natural Language for Reward Shaping in Reinforcement Learning, 2019.
* Hancock et al. (2019) Hancock, B., Bordes, A., Mazare, P.-E., and Weston, J. Learning from dialogue after deployment: Feed yourself, chatbot! _ arXiv preprint arXiv:1901.05415_, 2019.
* Hase & Bansal (2021) Hase, P. and Bansal, M. 모델들은 설명으로 언제 배울 수 있나요? 설명 데이터의 역할을 이해하기 위한 형식적인 프레임워크입니다. _ arXiv preprint arXiv:2102.02201_, 2021. URL [https://arxiv.org/pdf/2102.02201.pdf](https://arxiv.org/pdf/2102.02201.pdf).
* Hermann et al. (2015) Hermann, K. M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., and Blunsom, P. Teaching machines to read and understandd. _ Neural Information Processing Systems_, 28, 2015에서의 발전.
* Hilton & Gao (2022) Hilton, J. and Gao, L. goodhart's law 측정. [https://openai.com/blog/measuring-goodharts-law/] (https://openai.com/blog/measuring-goodharts-law/), 2022.
* Holtzman 등(2019) Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. 신경 텍스트 변성의 이상한 사례. _ arXiv preprint arXiv:1904.09751_, 2019. URL [https://arxiv.org/pdf/1904.09751.pdf](https://arxiv.org/pdf/1904.09751.pdf).
* Kaplan 등(2017) Kaplan, R., Sauer, C., and Sosa, A. Beating Atari with Natural Language Guided Reinforcement Learning, 2017.
* Korbak et al. (2022) Korbak, T., Perez, E., and Buckley, C. L. Rl with kl penalty는 bayesian inference로 더 잘 보인다. _ arXiv preprint arXiv:2205.11275_, 2022.
* Lampinen et al. (2022) Lampinen, A. K., Dasgupta, I., Chan, S. C., Matthewson, K., Tessler, M. H., Creswell, A., McClelland, J. L., Wang, J. X., and Hill, F. Can language models learn from explanations in context? _ arXiv preprint arXiv:2204.02329_, 2022.
* Li et al. (2016) Li, J., Miller, A. H., Chopra, S., Ranzato, M., and Weston, J. Dialogue learning with human-in-the-loop. _ arXiv preprint arXiv:1611.09823_, 2016.
*Li 등(2022) Li, Z., Sharma, P., Lu, X. H., Cheung, J. C., and Reddy, S. 대화형 피드백을 사용하여 배포 후 질문 응답 시스템의 정확성과 설명성을 향상시킵니다. _ arXiv preprint arXiv:2204.03025_, 2022.
* Lin et al. (2021) Lin, J., Fried, D., Klein, D., and Dragan, A. Inferring rewards from language in context. _ arXiv preprint arXiv:2204.02515_, 2022.
* Lin 등(2021) Lin, S., Hilton, J., and Evans, O. TruthfulQA: Mimic Human Falsehood, 2021의 모델 측정 방법.
* Liu 등(2022) Liu, Y., Deb, B., Teruel, M., Halfaker, A., Radev, D., and Awadallah, A. H. On improving summarization factual consistency from natural language feedback. _ arXiv preprint arXiv:2212.09968_, 2022.
* Lu 등(2021) Lu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. _ arXiv preprint arXiv:2104.08786_, 2021.
* Luketina 등(2019) Luketina, J., Nardelli, N., Farquhar, G., Foerster, J., Andreas, J., Grefenstette, E., Whiteson, S., and Rocktaschel, T. 자연어로 알려지는 강화 학습에 대한 조사. _Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19_, pp. 6309-6317. International Joint Conferences on Artificial Intelligence Organization, 7 2019. doi: 10.24963/ijcai.2019/880. URL [https://doi.org/10.24963/ijcai.2019/880](https://doi.org/10.24963/ijcai.2019/880).
* Matiana 등(2021) Matiana, S., Smith, J., Teehan, R., Castricato, L., Biderman, S., Gao, L., and Frazier, S. 잉어 자르세요: 제로 샷 스토리 평가를 위한 낚시입니다. _ arXiv preprint arXiv:2110.03111_, 2021.
* Nakano et al. (2021) Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al. WebGPT: Browser-assisted question-answering with human feedback. _ arXiv preprint arXiv:2112.09332_, 2021. URL [https://arxiv.org/pdf/2112.09332.pdf](https://arxiv.org/pdf/2112.09332.pdf).
* Neelakantan 등(2022) Neelakantan, A., Xu, T., Puri, R., Radford, A., Han, J. M., Tworek, J., Yuan, Q., Tezak, N., Kim, J. W., Hallacy, C., Heidecke, J., Shyam, P., Power, B., Nekoul, T. E., Sastry, G., Krueger, G., Schnurr, D., Such, F. P., Hsu, K., Thompson, M., Khan, T., Sherbakov, T., Jang, J., Welinder, P., and Weng, L. 대비 사전 훈련에 의한 텍스트 및 코드 임베딩, 2022.
* Nguyen 등(2021) Nguyen, K. X., Misra, D., Schapire, R., Dudik, M., and Shafto, P. Interactive learning from activity description. _International Conference on Machine Learning_, pp. 8096-8108. PMLR, 2021.
* OpenAI(2022a) OpenAI. 오픈아이 OpenAI finetuning documentation. [https://beta.openai.com/docs/api-reference/fine-tunes/create] (https://beta.openai.com/docs/api-reference/fine-tunes/create), 2022a.
* OpenAI(2022b) OpenAI. Model index for researchers. [https://beta.openai.com/docs/model-index-for-researchers] (https://beta.openai.com/docs/model-index-for-researchers), 2022b.

* Ouyang 등(2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. _ Preprint_, 2022. URL [https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)입니다.
* Perez 등(2021) Perez, E., Kiela, D., and Cho, K. 언어 모델을 사용한 진정한 소수 샷 학습입니다. _ Advance of Neural Information Processing Systems_, 34:11054-11070, 2021.
* Pruthi 등(2021) Pruthi, D., Bansal, R., Dhingra, B., Soares, L. B., Collins, M., Lipton, Z. C., Neubig, G., and Cohen, W. W. Evaluating Explanations: How much do explanation from the teacher aid students?, 2021.
* Qi 등(2020) Qi, P., Zhang, Y., Zhang, Y., Bolton, J., and Manning, C. D. Stanza: A Python Natural Language Processing Toolkit for Many Human Languages. _Proceedings of the 58th Annual Meeting of the Association of Computational Linguistics: System Demonations_, 2020.
* Radford & Narasimhan (2018) Radford, A. and Narasimhan, K. Generative Pre-Training, 2018. URL [https://openai-assets.s3.amazonaws.com/research-covers/language-unsupervised/language_understanding_paper.pdf](https://openai-assets.s3.amazonaws.com/research-covers/language-unsupervised/language_understanding_paper.pdf).
* Radford 등(2019) Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. Improving language understanding by generative pre-training, 2018.
* Radford 등 (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language Models is Unsupervised Multitask Learners, 2019. URL [https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).
* Rae et al. (2021) Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et al. Scaling language models: Methods, analysis & insights from training gopher. _ arXiv preprint arXiv:2112.11446_, 2021. URL [https://arxiv.org/pdf/2112.11446.pdf](https://arxiv.org/pdf/2112.11446.pdf).
* Raffel 등(2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, 2020.
* Rupprecht 등(2018) Rupprecht, C., Laina, I., Navab, N., Hager, G. D., and Tombari, F. Guide me: Interacting with deep networks. _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pp. 8551-8561, 2018.
* Saunders 등(2022) Saunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J., and Leike, J. Self-critiquing models for assisting human evaluators. _ arXiv preprint arXiv:2206.05802_, 2022.
* Scheurer 등(2022) Scheurer, J., Campos, J. A., Chan, J. S., Chen, A., Cho, K., and Perez, E. Training language models with language feedback. The First Workshop on Learning with Natural Language Supervision at ACL_, 2022.
* Schick 등(2022) Schick, T., Dwivedi-Yu, J., Jiang, Z., Petroni, F., Lewis, P., Izacard, G., You, Q., Nalmparantis, C., Grave, E., and Riedel, S. 동료: 협력 언어 모델입니다. _ arXiv preprint arXiv:2208.11663_, 2022.
* Shi et al.(2022) Shi, W., Dinan, E., Shuster, K., Weston, J., and Xu, J. When life give you lemons, make cherryade: feedback from bad responses to good labels. _ arXiv preprint arXiv:2210.15893_, 2022.
* Stacey 등(2021) Stacey, J., Belinkov, Y., and Rei, M. 강인한 자연어 추론을 위한 인간 설명에 의한 감독 모델 주의_ arXiv preprint arXiv:2104.08142_, 2021. URL [https://arxiv.org/pdf/2104.08142.pdf](https://arxiv.org/pdf/2104.08142.pdf).
* Stiennon et al. (2020) Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. Learning to summarize with human feedback. _ Advances in Neural Information Processing Systems_, 33:3008-3021, 2020. URL [https://arxiv.org/pdf/2009.01325.pdf](https://arxiv.org/pdf/2009.01325.pdf).
* Sumers et al. (2021) Sumers, T. R., Ho, M. K., Hawkins, R. D., Narasimhan, K., and Griffiths, T. L. Learning rewards from linguistic feedback. _ feedback_, 1(2):3, 2021.
* Tam et al. (2022) Tam, A. C., Rabinowitz, N. C., Lampinen, A. K., Roy, N. A., Chan, S. C., Strouse, D., Wang, J. X., Banino, A., and Hill, F. Semantic exploration from language abstractions and pretrained representationations. _ arXiv preprint arXiv:2204.05080_, 2022.
* Volske 등(2017) Volske, M., Potthast, M., Syed, S., and Stein, B. TL;DR: Mining Reddit to learn automatic summarization. _Proceedings of the Workshop on New Frontiers in Summarization_, pp. 59-63, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4508. URL [https://aclanthology.org/W17-4508](https://aclanthology.org/W17-4508)
* Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. _ arXiv preprint arXiv:2201.11903_, 2022.
* Weston (2016) Weston, J. E. Dialog-based language learning. _ 신경 정보 처리 시스템_, 29, 2016의 발전.

* Wiegreffe 등(2021) Wiegreffe, S., Marasovic, A., and Smith, N. A. Measuring association between labels and free-text rationales. _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 10266-10284, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.804. URL [https://aclanthology.org/2021.emnlp-main.804](https://aclanthology.org/2021.emnlp-main.804)
* Xu 등(2022) Xu, J., Ung, M., Komeili, M., Arora, K., Boureau, Y. - L., and Weston, J. Learning new skills after deployment: Improving open-domain internet-driven dialogue with human feedback. _ arXiv preprint arXiv:2208.03270_, 2022.
* Zhang et al.(2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language models. _ arXiv preprint arXiv:2205.01068_, 2022.
* Ziegler et al. (2019) Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences. _ arXiv preprint arXiv:1909.08593_, 2019. URL [https://arxiv.org/pdf/1909.08593.pdf](https://arxiv.org/pdf/1909.08593.pdf).

## 부록 추가 파생

### 베이지안 추론으로 언어 피드백에서 모방 학습

언어 피드백은 가변 추론으로서 문맥 \(c\sim p(c)\)(예를 들어, 문서의 요약)에 대해 고품질의 출력 \(x_{1}\)을 생성하는 것을 목표로 한다. 우리는 LM \(\pi_{\theta}\)을 사용하여 문맥 \(c\), 즉 \(x_{1}\sim p_{\theta}(x_{1}|c)\에 대한 조건화에 의해 출력 \(x_{1}\)을 생성한다. 그리고 사람의 기호에 따라 출력이 높은 경우 \(\mathcal{I}=1\)의 확률변수인 술어 \(\mathcal{I}\)을 도입한다. 우리는 그림 1에 표시된 이 데이터 생성 프로세스를 나타낸다. 5 left, as:

\[p_{\theta}(c,x_{1},\mathcal{I})=p(c)\pi_{\theta}(x_{1}|c)p(\mathcal{I}|c,x_{1}). \tag{5}\]

우리의 목표는 문맥에 따른 품질의 한계 로그 확률을 최대화하는 것이다. \(\mathbb{E}_{c\sim p(c)}\log p(\mathcal{I}=1|c)\). 특정 컨텍스트 \(c\)에 대해 중요도 샘플링 제안 분포 \(q(x_{1}|c)\)를 도입하고 Evidence Lower Bound (ELBo)를 사용하여 \(\log p(\mathcal{I}=1|c)\)를 근사합니다.

\[\log p(\mathcal{I}=1|c) =\log\sum_{x_{1}}p_{\theta}(x_{1},\mathcal{I}=1|c) \tag{6}\] \[\geq\sum_{x_{1}}q(x_{1}|c)\log\frac{p_{\theta}(x_{1},\mathcal{I} =1|c)}{q(x_{1}|c)} \tag{7}\]

우리는 식에서 하한을 최대화한다. 6은 Expectation-Maximization (EM) 절차를 사용하여, 최대화 \(F\) w.r.t. 제안 분포 \(q\) (E-step)과 w.r.t. \(\pi_{\theta}\) (M-step)을 번갈아 가면서 이 알고리즘을 언어 피드백에서 모방 학습이라고 한다.

E-stepMaximizing \(F(\theta,q)\) w.r.t \(q\)는 고품질 텍스트에 더 높은 가능성을 할당하기 위해 제안 분포 \(q\)를 정제하는 것에 해당한다. 이것은 \(x_{1}\)을 인간을 포함하는 데이터-생성 프로세스에 임베딩함으로써, 초기 출력 \(x_{0}\) 및 인간 피드백 \(f\)을 도입함으로써 달성된다(합 규칙을 통해):

\[q(x_{1}|c) =\sum_{x_{0},f}p_{\theta}(x_{0},f,x_{1}|\mathcal{I}=1,c) \tag{8}\] \[\propto\sum_{x_{0},f}p_{\theta}(x_{0},f,x_{1}|c)p_{\theta}( \mathcal{I}=1|c,x_{0},f,x_{1})\] (9) \[=\sum_{x_{0},f}p_{\theta}(x_{0}|c)p(f|c,x_{0})p_{\theta}(x_{1}|c,x_{0},f)\] \[\qquad\qquad p_{\theta}(\mathcal{I}=1|c,x_{0},f,x_{1}). \tag{10}\]

Eq. 도 10은 다음의 샘플링 절차를 발생시킨다(도면 참조). 5, 우측): 먼저, LM은 문맥 \(c\)에 조건화되어 초기 출력 \(x_{0}\)을 생성한다. 둘째, 인간은 \((c,x_{0})\) 쌍에 대해 언어 피드백 \(f\)을 제공한다. 셋째, LM은 \((c,x_{0},f)\)에 조건화된 정제된 텍스트 \(x_{1}\)을 생성한다. 마지막으로 이진 변수 \(\mathcal{I}\)은 초기 출력 \(x_{0}\), 피드백 \(f\), 컨텍스트 \(c\)이 주어졌을 때 \(x_{1}\)이 고품질 텍스트인지 여부를 나타낸다. 우리는 \(p_{\theta}(\mathcal{I}=1|c,x_{0},f,x_{1})\)을 Boltzmann 분포로 모델링합니다.

\[p_{\theta}(\mathcal{I}=1|c,x_{0},f,x_{1})\propto\exp(R(c,x_{0},f,x_{1})/\beta), \tag{11}\]

(c,x_{0},f,x_{1}\); (\beta\)는 온도 하이퍼파라미터이다. 이 볼츠만 분포는 이전의 출력과 인간의 언어 피드백의 보상 함수 \(R\)로 표현하므로 품질을 평가하기 쉽다.

그림 5: **왼쪽:** 알고리즘이 근사하는 대상 분포의 그래픽 모델 \(p_{\theta}\)입니다. \ (c\)는 문맥이고 \(x_{1}\)은 고품질 LM 출력이며 \(\mathcal{I}\)은 인간의 기호에 따라 출력이 고품질인지 여부를 나타낸다. **오른쪽:** 제안 분포의 그래픽 모델 \(q\)을 중요도 샘플링에 사용합니다. \ (x_{0}\)은 초기 LM 출력이고, \(f\)은 \(x_{0}\)에 대한 언어 피드백이다.

우리는 이제 E-스텝이 원래의 분포 \(p_{\theta}(x_{1}|c)\보다 더 나은 제안 분포를 초래하는 이유, 즉 \(q(x_{1}|c)\)의 샘플이 \(p_{\theta}(x_{1}|c)\의 샘플보다 더 높은 품질을 갖는 경향이 있는 이유를 주장한다. 첫째, \(x_{0}\)은 이미 상당히 좋은 출력(\(\pi_{\theta_{\text{add}}}\approx\pi_{\theta}\))임을 알 수 있다. 우리는 피드백 \(f\)이 유익하고 고품질이라고 가정할 수 있다. 따라서 \(x_{1}\sim p_{\theta}(x_{1}|c,x_{0},f)\)는 피드백으로부터 유용한 정보를 활용하기 때문에 \(x_{0}\sim p_{\theta}(x_{0}|c)\)보다 품질이 높을 것이다. 또한, \(x_{0}\) w.r.t에서 \(f\) 및 \(c\)으로 개선되는 정제된 텍스트 \(x_{1}\)에 더 높은 값을 할당하기 위해 \(R\)을 선택하자. 결과적으로, Eq. 11은 고품질 출력 \(x_{1}\)에 더 높은 가능성을 할당하여 고품질 출력에 추가 가중치를 부여하고 제안 분포 \(q\)를 더 개선할 수 있습니다.

M-stepMaximizing \(F(\theta,q)\) w.r.t. policy \(\pi_{\theta}\)은 \(q\)으로 정의된 분포에서 지도 학습(교차 엔트로피 손실 최소화)과 동일하다. 이를 위해 식에서 모든 항을 삭제합니다. 7에 의존하지 않는 \(\theta\):

\[\operatorname*{argmax}_{\theta}F(\theta,q) =\operatorname*{argmax}_{\theta}\mathbb{E}_{x_{1}\sim q(x_{1}|c) }\log p_{\theta}(x_{1},\mathcal{I}=1|c)\] \[=\operatorname*{argmin}_{\theta}\mathbb{E}_{x_{1}\sim q(x_{1}|c) }-\log\pi_{\theta}(x_{1}|c). \tag{12}\]

ILF: 언어 피드백에서 모방 학습은 알고리즘 1의 의사코드를 사용하여 E-step과 M-step을 번갈아 사용한다. M-step에서는 이전 반복 \(\pi_{\theta_{\text{add}}}\)의 모델을 \(p_{\theta}(x_{0}|c)\)과 \(p_{\theta}(x_{1}|c,x_{0},f)\으로 사용한다. 실제로, 우리는 명령-운동된 LM을 조건화함으로써 \(R\)을 구현하는데, 이 새로운 텍스트가 제공된 피드백을 통합하는가?와 같은 이진 질문에 대해 실행된다. 응답 예 또는 No._ 레이블 \(y\)이 \(y_{\text{good}}\)(" Yes") 또는 \(y_{\text{bad}}\)(" No"). 우리는 프롬프트가 주어졌을 때 긍정적인 대답의 확률 \(y_{\text{good}\)을 보상으로 사용한다. 즉, \(p(y_{\text{good}}|\text{prompt})=\frac{p(y_{\text{good}}|\text{prompt})}{p(y_{ \text{good}}|\text{prompt})+p(y_{\text{bad}}|\text{prompt})}\). 이러한 가정으로 \(q\)는 다음과 같은 형식을 사용합니다.

\[q(x_{1}|c)\propto \mathbb{E}_{x_{0}\sim\pi_{\theta_{\text{add}}}(x_{0}|c)}\mathbb{E}_{f\sim p(f|c,x_{0})}\] \[\pi_{\theta_{\text{add}}}(x_{1}|c,x_{0},f)\exp(R(c,x_{0},f,x_{1})/\beta)\]

이를 이용하여 최적화된 데이터에 대한 M-step, 즉 \(\operatorname*{argmax}_{\theta}F(\theta,q)\)을 수행한다. 마지막으로, \(q(x_{1}|c)\)에서 가장 좋은 \(N\) 샘플링으로 샘플링을 근사화한다. 샘플 \(x_{1}\sim q\)을 얻기 위해, 우리는 \(N\) 정제 \(\{x_{1}^{1},\dots,x_{1}^{N}\}\sim\pi_{\theta_{\text{add}}}(x_{1}|c,x_{0},f)\)을 샘플링하고 연산한다.

\[x_{1}=\operatorname*{argmax}_{x_{1}^{i}}\exp R(c,x_{0},f,x_{1}^{i}).\]

요약하면, 우리는 ILF가 베이지안 추론으로 이해될 수 있음을 보여준다. 이 프로세스는 언어 피드백에 의해 제공된 증거에 기초하여 LM을 업데이트하는 것을 포함한다. 이 렌즈는 ILF와 RL 사이의 대응을 Human Feedback(Ziegler et al., 2019; Stiennon et al., 2020, _inter alia_)과 강조하는데, 이는 이전에 베이지안 추론과 동등한 것으로 입증되었다(Korbak et al., 2022).

## 부록 B RL 설정에서 언어에 대한 추가 관련 작업

언어는 다양한 목적으로 RL에서 널리 사용되어 왔다(Luketina et al., 2019, for a overview 참조). 예를 들어, 태스크("instruction following", _inter alia_) 운전 탐색(Tam et al., 2022), 보상 함수 추론(Lin et al., 2022; Sumers et al., 2021; Fidler et al., 2017, _inter alia_), 및 강력한 감독을 통해 모델을 트레이닝(Andreas et al., 2017; Kaplan et al., 2017), 보상 정형화(Goyal et al., 2019), 또는 궤적에 대한 설명을 제공함으로써(Nguyen et al., 2021). 대조적으로, 우리는 잘못된 행동을 바로잡기 위해 언어를 사용합니다. 다른 작업은 테스트 시간에 언어 피드백을 사용하여 모델의 행동에서의 실수, 예를 들어 이미지 분할(Rupprecht 등, 2018) 또는 코드 생성(Elgohary 등, 2020; Austin 등, 2021)을 수정한다. 대조적으로, 우리는 _train_ 모델에 피드백을 사용하며 우리의 접근법은 테스트 시간에 인간의 개입이 필요하지 않다.

## Appendix Cataset Collection and Analysis

주석 프로세스는 인간 주석의 높은 품질을 보장하기 위해 데이터 라벨링 회사 Surge AI를 통해 공급된 경험이 풍부한 주석기를 사용한다. 온보딩 및 평가 과정에서 이진 비교 작업에 대한 작성자-주석자 일치도를 계산하고 작성된 피드백과 이상적인 요약의 품질을 수동으로 검토하여 높은 품질을 보장한다. 그런 다음 모든 주석 작업에 대해 31명의 적격 주석자를 선택하지만 참여해야 하는 작업과 기간을 선택할 수 있다. 주석의 품질을 더욱 보장하기 위해 주석자에게 제공하는 자세한 지침을 제공하고 지속적인 개선을 보장하기 위해 프로세스 전반에 걸쳐 업데이트한다(이러한 지침은 부록 I에서 찾을 수 있다). 주석자와 저자 간의 일치율을 측정하기 위해 학습 데이터 집합에서 10개의 레딧 게시물 샘플을 금 표준으로 선택하고 17개의 주석자에게 레이블을 지정한다. 이진 비교 주석과 우리의 주석을 비교할 때, 이는 \(81.0\%\)의 저자-주석자 일치율을 초래한다. 또한 가능한 모든 주석자 조합 간의 평균 일치율을 계산하여 주석자-주석자 일치율 \(70\%\)을 산출한다. 이러한 철저한 프로세스와 평가를 활용하여 인간 주석의 정확성과 신뢰성을 보장할 수 있습니다.

데이터 세트 분석 우리가 수집하는 피드백은 일반적으로 요약의 가장 중요한 단점을 해결한다. 열차 샘플의 \(92.0\%\)에서 주석자의 피드백은 완료되었으며 주석자가 보고한 요약의 모든 중요한 단점을 해결했다. 열차 데이터셋에서 대부분의 피드백은 적용 범위(\(77.0\%\)), 정확도(\(16.0\%\)), 일관성(\(5.0\%\) 및 기타 범주(\(2.0\%\)와 관련하여 더 작은 백분율을 갖는 것으로 관찰되었다. 또한 평균 토큰 수로 측정한 다양한 요약 및 피드백의 길이를 분석한다. 인간이 작성한 요약은 평균 길이가 \(41.0\pm 0.1\) 토큰이고, Reddit에서 추출한 인간 요약은 평균 길이가 \(32.5\pm 0.1\) 토큰이며, FeedME에서 생성한 초기 요약은 평균 길이가 \(29.3\pm 0.1\) 토큰이며, 주석자가 작성한 피드백은 평균 길이가 \(20.4\pm 0.2\) 토큰이다.

이러한 분석 외에도 개발 데이터 세트에서 주석자가 다양한 작업(예: 이진 비교, 피드백 작성 및 이상적인 요약 작성)을 완료하는 데 걸리는 시간도 측정한다. 우리는 이상치를 무시하고 주석 시간이 최소 20초와 최대 420초(7분)인 샘플만 고려한다. 주석은 이진 비교 작업에서 평균 61.5\pm 5.3초, 피드백 작업에서 평균 182.5\pm 6.3초, 이상적인 요약 작업에서 평균 195.5\pm 6.1초를 소요한다. 그림 6에서 이진 비교 주석, 피드백 작성 및 이상적인 요약 작성 작업에 대한 개발 데이터 세트에 주석 시간을 히스토그램으로 표시한다. 주석자는 피드백 또는 이상적인 요약보다 이진 비교 주석이 훨씬 빠르다. 피드백 작성은 과제를 비평하는 것이 일반적으로 해결하는 것보다 쉽기 때문에 예상되는 이상적인 요약을 작성하는 것보다 시간이 덜 걸린다. 이러한 종합적인 평가는 데이터 세트 및 주석 프로세스의 고품질 및 철저성을 보여준다.

## 부록 D 대상 단어 제거 세부 정보

아래는 LM이 문장에서 특정하고 불쾌한 단어를 제거하도록 지시하거나 "촉촉"하는 방법의 예이다.

_"이 텍스트에서, 많은 독성 및 불쾌한 단어들이 사용된다: 당신은 정말 얼간이고, 좋은 사람이며, 바보이다. 이상적인 텍스트는 얼간이라는 단어를 제거해야 하지만, 그렇지 않으면 변하지 않는다: 당신은 "_"

여기서, 목표 완성은 _"그렇게 좋은 사람이고 바보이다."_ 보다 형식적으로, (교체 없이) 무작위로 균일하게 그려진 25개의 공격 단어들의 고정된 세트로부터 \(k\) 공격 단어들을 사용하여 공격 문장들을 샘플링한다. 각각의 모욕 문장은 또한 모든 모욕적인 단어들 외에 "좋은 사람"이라는 단어들을 포함한다. 각 \(k\in\{1,\dots,10\}\)에 대해 샘플

도 6: 이진 비교 태스크, 피드백 주석 태스크 및 인간 요약 작성 태스크의 주석 시간의 히스토그램 플롯(초 단위). 평가는 개발 데이터 세트에 대해 수행됩니다. 우리는 주석이 예상되는 이진 비교 작업에서 훨씬 더 빠르다는 것을 관찰한다. 결과는 또한 쓰기 피드백이 이상적인 요약을 쓰는 것보다 더 적은 시간이 걸린다는 것을 보여준다.

50개의 모욕적인 문장. 그런 다음 주어진 문장에서 \(k\geq l\)을 사용하여 \(l\in[1,2,3]\)의 불쾌한 단어를 제거하는 작업을 수행한다. 공격 문장에 "좋은 사람"이라는 단어를 포함시키기 때문에, 우리는 \(l=k\)개의 공격 단어를 제거할 수 있고, 여전히 직관적으로 말이 되는 목표 문장을 가질 수 있다.

## 랭킹 절차에 대한 부록 E 세부 정보

우리는 각각의 \(K\) 요약에 1과 \(K\) 사이의 순위가 주어지는 표준 순위 체계를 사용한다. 때때로 정제는 초기 요약을 정확하게 복사하거나 품질 면에서 매우 유사하므로 요약을 묶을 수 있습니다. 승률을 계산할 때 우리는 동점 표본에 \(0.5\)의 승수를 할당한다. 순위 \(r^{\prime}\)을 동점으로 순위가 매겨진 모든 요약에 할당하는데, 여기서 \(r^{\prime}=\frac{r+(r+n-1)}{2}\), \(r\)은 동점 요소의 순위이고, \(n\)은 동점 요소들의 수이다. 예를 들어, \((1,2,2,4,5)\rightarrow(1,2.5,2.5,4,5)\)의 순위와 \((1,2,3,3,3)\rightarrow(1,2,4,4)\)의 순위를 매핑한다.

## 부록 F 보상 모델

여기서는 우리가 평가하는 다양한 RM에 대해 더 자세히 설명한다. 언어 출력(예: "예" 또는 "아니오")을 생성하는 최종 RM과 스칼라 출력을 생성하는 표준 보상 모델을 평가한다.

Standard RM.Akin to (Stiennon et al., 2020), 우리는 언어 모델의 마지막 임베딩 레이어를 제거하고 스칼라 값을 출력하도록 훈련시킨다. 이 스칼라 값은 \(x\in\{x_{0}^{0},x_{0}^{1}\}\)의 요약이 문맥이 주어졌을 때 인간이 판단할 때 더 나은지 예측한다 \(c\). 우리는 (Zhang et al., 2022)에 소개된 OPT 13B LM을 RM에 대한 기본 모델로 사용하고 수집한 인간 선호도 비교에 대해 조정한다. API를 통해 제공되는 GPT-3 모델 위에 선형 레이어를 추가할 수 없다는 점에 주목할 필요가 있으며, 이것이 우리가 OPT 모델을 사용하는 이유이다.

Reward Model with Language Output.The classic RM(Stiennon et al., 2020) 외에도 스칼라 값 대신 언어 토큰을 출력하도록 RM을 훈련시킨다. 이를 위해 레이블 \(y\in\{y_{good},y_{bad}\}\)을 예측하도록 학습하여 요약 \(x_{0}\)이 고품질인지 아닌지를 분류하도록 LM을 조정한다. 그런 다음 개발 데이터 세트를 사용하여 선택한 \(\lambda\log p(x_{0})+\log p(y|x_{0})\), 여기서 \(\lambda\in[0,1]\을 최대화하도록 LM을 조정한다. 완전 손실은 또한 다음과 같이 기입될 수 있다:

\[\mathcal{L}(p_{\theta},x,y)=-\lambda\cdot\sum_{t=1}^{|x|}\log p_{\theta}(x_{t }|x_{<t})-\sum_{t=1}^{|y|}\log p_{\theta}(y_{t}|x,y_{<t}).\]

여기서 아래 첨자 \(t\)는 토큰 인덱스를 나타낸다. 우리는 주어진 요약 \(x_{0}\)에서 유한한 LM을 계산 \(p(y_{good}|x_{0})\)으로 평가한다. 최상의 RM은 다음과 같은 지침을 사용합니다. _위 내용은 지정된 텍스트의 훌륭한 요약입니까?* 뛰어난 요약은 일관성 있고 정확하며 간결하고 상세합니다. 우리는 OPT-RM(OPT-13B를 피니튜닝할 때) 및 GPT-3 바이너리(GPT-3-175B를 피니튜닝할 때)로 지칭하는 Yes 또는 No._로 대답한다. 우리는 또한 LM에 요약 \(A\)과 \(B\)을 모두 제공하고 어떤 요약이 더 나은지 지시하도록 다른 프롬프트에서 피니튜닝을 탐색한다. 뛰어난 요약은 일관성 있고 정확하며 간결하고 상세합니다. A 또는 B._로 답하다. 우리는 (이진 인간 피드백에 따라) 선호하는 요약의 레이블, 즉 \(y\in\{y_{A},y_{B}\}\)에서 LM을 조정한다. 우리는 주어진 요약 \(x_{0}\)에서 유한한 LM을 계산 \(p(y_{A}|x_{0})\)을 통해 평가한다. 이 RM을 _비교_ RM이라고 합니다. 우리는 두 개의 RM, 즉 OPT-13B Zhang 등(2022)과 GPT-3-175B를 탐색하고 우리가 사용하는 하이퍼파라미터는 부록 G를, 프롬프트 템플릿은 부록 J.4를 참조한다.

결과.검증 데이터 세트의 모든 RM을 평가하고 인간의 선호도를 기반으로 두 개 중 선호하는 요약을 예측하는 정확도를 계산한다. 표 4는 완전한 결과를 보여주며 여기에서 5K 샘플에 대해 훈련된 일부 RM에 대해 보고한다. 표준 RM 손실을 갖는 OPT 모델은 검증 데이터 세트에서 \(71.8\pm 2.0\%\)의 정확도를 달성한다. 그 결과, LM 손실로 OPT를 훈련하는 두 방법 모두 표준 RM 손실보다 우수한 성능을 보였으며, OPT 비교는 \(72.6\pm 1.9\%\), OPT-RM은 \(73.4\pm 1.9\%\)의 정확도를 보였다. GPT-3-175B는 GPT3 비교에서 \(71.2\pm 2.0\%\), GPT-3 Binary에서는 \(74.2\pm 2.0\%\)의 정확도를 보여 OPT-RM보다 우수한 성능을 보였다.

이러한 결과를 바탕으로 SS4.2에서 채점 함수를 평가하는 데 사용하는 개발 데이터 세트에 대해 OPT Binary 및 GPT-3-175B Binary 모델을 추가로 평가한다. 인간 평가자가 제공한 순위에 따라 RM에 의해 선택된 정제가 무작위로 선택된 정제("승률")보다 나은 시간 비율을 계산한다(자세한 내용은 App. E 참조). 그 결과는 표 3에서 확인할 수 있다. OPT-RM은 \(63.3\pm 2.7\%\), GPT-3-175B 바이너리 모델은 \(61.8\pm 2.9\%\)의 승률을 달성하였다. 이 평가에서 OPT-RM은 GPT-3 Binary보다 성능이 우수하다. 검증 및 개발 데이터 세트의 결과를 고려할 때 OPT-RM과 GPT-3-바이너리는 모두 유사하게 수행되는 것으로 판단된다. OPT의 훈련 과정, 모델의 공개 가능성, OpenAI의 API를 이용한 훈련에 수반되는 비용에 대해 더 많은 제어가 가능하다는 점을 고려하여, 우리는 ILF와의 비교를 위한 보상 모델로 OPT-RM 모델을 선택한다. 그림 7에서 로그-로그 도표에서 100, 1K 및 5K 샘플에 대해 훈련된 OPT-RM의 검증 정확도를 보여준다. 그림은 데이터 세트 크기를 늘릴 때 크기 조정을 보여 줍니다.

또한 Stiennon et al.(2020)의 데이터셋에서 OPT-RM의 피니튜닝에 대한 결과를 평가하고, 1.3B 파라미터로 모델을 평가한다. 학습 데이터셋의 이진 선호도 분포가 보상 모델의 성능에 유의한 영향을 미친다는 것을 관찰한다. 예를 들어, 우리의 자체 열차 데이터세트(즉, 우리의 최종 보상 모델)의 5K 샘플들에 대해 훈련된 OPT-RM은 Stiennon et al.(2020)의 테스트 세트(표 4에 도시되지 않음)에서 \(61.9\pm 0.2\%\)의 정확도를 달성한다. 동일한 모델이 Stiennon et al. (2020)의 train dataset으로부터 90K 샘플에 대해 훈련되었을 때, 그들의 테스트 세트(표 4에도 표시되지 않음)에서 \(69.3\pm 0.2\%\)의 정확도를 달성한다. 반면, 90K 샘플에 대해 훈련된 동일한 모델은 검증 데이터셋에서 \(68.6\pm 2.0\%\)의 정확도를 달성했으며, 이는 자체 훈련 데이터셋의 5K 샘플에 대해 훈련된 모델이 달성한 \(73.4\pm 1.9\%\)의 정확도보다 현저히 낮다. 유사한 패턴들이 대략적으로 Stiennon 등(2020)에 의해 트레이닝된 공개된 1.3B 보상 모델과 우리 자신의 기차 데이터세트의 5K 샘플들에 트레이닝된 1.3B 파라미터들과 OPT 바이너리 모델을 비교할 때 관찰될 수 있다. 자체 열차 데이터 세트의 64K 샘플입니다. 검증 데이터셋에서 전자의 모델은 \(69.6\pm 2.0\%\)의 정확도를 달성하지만, 후자의 모델은 \(63.8\pm 2.1\%\)의 정확도를 달성한다(그러나 RM은 서로 다른 손실 함수로 훈련된다). 이러한 결과는 다음과 같은 두 가지 중요한 고려 사항을 강조한다. (1) 선호도 분포는 크게 달라질 수 있고 보상 모델이 학습하는 것에 강한 영향을 미칠 수 있으며 (2) 보상 모델의 샘플 효율성은 기차 및 테스트 분포에 크게 의존한다. 테스트 분포가 기차 분포와 다른 경우, 보상 모델은 매우 샘플 비효율적이며 훨씬 더 많은 샘플이 주어진 경우에도 실제 분포를 정확하게 학습하지 못할 수 있다.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline  & Models & \# Params & Train Data Size & Development Accuracy (in \%) & Validation Accuracy (in \%) \\ \hline \multirow{6}{*}{LM Loss / Our dataset} & OPT Comparison & 13B & 5K & \(66.5\pm 3.3\) & \(72.6\pm 1.9\) \\  & OPT RM & 1.3B & 5K & \(70.0\pm 3.2\) & \(69.6\pm 2.0\) \\  & OPT RM & 13B & 100 & \(54.5\pm 3.5\) & \(53.4\pm 2.2\) \\  & OPT RM & 13B & 1K & \(68.5\pm 3.2\) & \(67.2\pm 2.1\) \\  & **OPT RM** & **13B** & **5K** & **69.5 \(\pm\) 3.2** & \(\mathbf{73.4\pm 1.9}\) \\  & GPT-3 Comparison & - & 5K & 68.0 & \(71.2\pm 2.0\) \\  & **GPT-3 Binary** & - & **5K** & - & \(\mathbf{74.2\pm 2.0}\) \\ \hline \multirow{2}{*}{RM Loss / Our dataset} & OPT & 13B & 5K & \(68.5\pm 3.2\) & \(71.8\pm 2.0\) \\  & Stiennon et al. (2020) train dataset & Stiennon et al. (2020) RM & 1.3B & 64K & \(58.0\pm 3.4\) & \(63.8\pm 2.1\) \\ \hline LM Loss / Stiennon et al. (2020) train dataset & OPT Binary & 13B & 90K & \(69.0\pm 3.2\) & \(68.6\pm 2.0\) \\ \hline \hline \end{tabular}
\end{table}
표 4: 인간 평가에서, 우리는 개발 데이터세트 및 검증 데이터세트에 대한 다양한 RM을 평가한다. 또한 Stiennon et al.(2020)의 훈련 데이터셋에 대한 훈련과 개발 및 검증 데이터셋에 대한 평가 결과를 보고한다. 우리는 두 요약 중 어떤 것이 사람이 선호하는지를 예측하는 정확도를 계산한다.

\begin{table}
\begin{tabular}{c c c} \hline \hline  & Scoring Function & Win Rate vs Random Selection (in \%) \\ \hline Task Specific Heuristic & Max Length & \(65.0\pm 2.7\) \\ \hline \hline Zero-Shot & Embedding Similarity & \(48.3\pm 3.0\) \\  & **InstructRM Ensemble** & \(\mathbf{56.0\pm 3.0}\) \\ \hline Finetuning on 5K samples & **OPT Binary** & \(\mathbf{63.3\pm 2.7}\) \\  & GPT-3 Binary & \(61.8\pm 2.9\) \\ \hline \hline \end{tabular}
\end{table}
표 3: 인간 평가에서, 개발 데이터세트에 대한 보상 모델 및 랭킹 방법을 비교한다(도 2에서와 동일한 방식으로). 두 RM은 모두 5K 샘플에 대해 훈련되고 제로 샷 방법을 능가한다.

## Appendix G Hyper Parameters

### Generating Refinements

타겟팅된 단어 제거 실험(SS3)을 위해, 200 토큰 또는/\(n\)이 생성될 때까지 그리디 디코딩을 사용한다. 모든 요약 실험에 대해 핵 샘플링(Holtzman et al., 2019)을 사용하여 최대 48개의 토큰(Stiennon et al., 2020)을 샘플링하고 \(p=0.95\) 및 온도 \(t=1.0\)을 샘플링한다. 샘플링된 요약의 처음부터 영숫자가 아닌 문자(예: 새 줄)를 제거합니다. 생성된 요약에서 빈 빈 공간을 더 제거하고 새 줄 토큰 / \(n\) 뒤에 오는 모든 텍스트를 제거합니다. 최대 토큰 길이로 인해 샘플링된 요약은 간혹 불완전한 문장으로 끝난다. 따라서, "\(\cdot\)", "\(\cdot\)", 또는 "\(\cdot\)"에서 끝나지 않는 어미 문장을 제거한다. 설명된 온도 및 후처리는 모든 요약 세대, 즉 초기 요약, 정제 및 테스트 요약을 생성하기 위해 적용된다.

### 요약에 대한 Finetuning

100, 1K, 5K 샘플의 세 가지 데이터 세트 크기를 사용하여 독립적인 하이퍼파라미터 최적화 스윕을 수행한 다음 초기 요약에서 Finetuning on refinements(ILF)와 Finetuning에 동일한 하이퍼파라미터를 사용한다. 우리는 인간 요약에서 하이퍼파라미터 스윕을 실행하기로 선택하는데, 이는 정제 작업을 마무리하는 알고리즘에 부당한 이점을 주지 않을 것이기 때문이다. 스윕을 위해 인간 요약(100, 1K 및 5K 샘플로 구성됨)의 트레인 데이터 세트를 활용하고 개발 데이터 세트에 대해 평가한다. 불행히도 OpenAI API는 개발 데이터 세트의 배치에 대한 유효성 검사 손실 및 토큰 정확도만 제공하므로 학습 중에 전체 개발 데이터 세트에 대한 모델을 평가할 수 없습니다. 그 결과, 모델 API를 활용하여 피니튜닝 후 전체 개발 데이터 세트에 대해 평가하고 생성된 요약의 복잡성을 성능 척도로 계산한다.

최적의 하이퍼파라미터를 결정하기 위해 _epochs_\(\{1,2,3,4\}\), _prompt loss weight_\(\{0,0.01,0.05,0.1\}\), _학습률_\(\{0.02,0.05,0.1,0.2\}\)에 대한 값 범위에 대해 스윕을 수행한다. 우리는 먼저 에포크를 스윕하고 최상의 값을 선택한 다음 프롬프트 손실 가중치에 대해 해당 값을 사용하여 스윕을 수행한다. 우리의 경험적 관찰은 에폭의 수가 과적합으로 이어지는 하나 이상의 에폭에 대한 훈련과 함께 당혹감에 가장 큰 영향을 미친다는 것을 나타낸다. 선택된 하이퍼파라미터는 표 5에서 찾을 수 있다.

각각 1K 샘플을 사용하여 정제 및 초기 요약 데이터 세트에 대한 피니튜닝 단계에서 하이퍼매개변수 선택에 오류를 범했다. 즉각적인 손실 가중치 \(0.05\)를 사용하는 대신, 우리는 인간 요약에서 피니튜닝할 때 실수로 0 값을 사용했다. 이 오류는 결과에 약간의 영향을 미쳤을 수 있지만, 두 설정 사이의 복잡성의 차이는 최소이며, 프롬프트 손실 가중치는 \(0.05\)와 프롬프트 손실 가중치는 \(6.71\)이다. 이러한 오류에도 불구하고, 본 방법은 여전히 1K 샘플의 인간 요약에 대한 핀 튜닝과 차선의 하이퍼 파라미터를 사용한 초기 요약에 대한 핀 튜닝보다 성능이 우수하다.

### ILF의 다중 반복

ILF의 여러 반복, 즉 정제 및 피네튜닝의 여러 반복을 평가하기 위해 200개 및 300개 샘플이 있는 정제 데이터세트에서 GPT-3-175B를 미세 조정한다. 따라서 본 논문에서는 200개 및 300개 정제의 열차 데이터셋에 대해 하이퍼파라미터 최적화를 수행하고, 200개 정제의 개발 데이터셋(인간 요약 대신)에 대해 평가한다. 최적의 하이퍼파라미터를 결정하기 위해 _epochs_\(\{1,2,3,4\}\), _prompt loss weight_\(\{0,0.01,0.05,0.1\}\), _학습률_\(\{0.02,0.05,0.1,0.2\}\)에 대한 값 범위에 대해 스윕을 수행한다. 우리는 먼저 에포크를 스윕하고 최상의 값을 선택한 다음 프롬프트 손실 가중치에 대해 해당 값을 사용하여 스윕을 수행한다. 200개의 세분화를 위해 하이퍼파라미터 \(\text{epochs}=1\), 프롬프트 손실 가중치 \(=0.05\), 학습률 곱셈기 \(=0.1\)을 선택한다. 300개의 정련을 위해 \(\text{epochs}=1\), prompt loss weight \(=0\), learning rate multiplier \(=0.2\)를 선택한다.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Samples & Epochs & Prompt Loss Weight & Learning Rate \\ \hline
100 & \(1\) & \(0\) & \(0.05\) \\
1K & \(1\) & \(0.05\)\(\ast\) & \(0.02\) \\
5K & \(1\) & \(0.1\) & \(0.2\) \\ \hline \hline \end{tabular}
\end{table}
표 5: 우리는 인간 요약의 100, 1K 및 5K 샘플에 대한 피니튜닝의 선택된 하이퍼파라미터를 보고한다.

### Finetuning Reward Models

OPT 보상 모델.OPT 보상 모델을 피니튜닝하기 위해, 우리는 세 가지 상이한 유형의 보상 모델들: _표준_, _비교_ 및 _분류_ 각각에 대해 베이지안 하이퍼파라미터 최적화를 수행한다(섹션 F 참조). 모든 모델에 대해 \([1\mathrm{e}^{-5},1\mathrm{e}^{-6}]\)과 배치 크기 \(\{32,64\}\)의 범위에서 학습률을 훑어본다. 언어 손실을 이용한 보상 모델의 경우, 프롬프트 손실 가중치 \(\{0.0,0.01,0.05,0.1,0.5,1.0\}\)를 최적화한다. 모델당 10회 반복을 실행하고 200개의 개발 예제를 사용하여 모든 스윕을 평가합니다. 모든 런에 대해 선형 학습 속도 스케줄러와 가중치 감쇠 \(0.1\)을 사용한다. 모든 모델에 대해 최적의 배치 크기는 32입니다. 가장 좋은 프롬프트 손실 가중치는 _비교_ 및 _분류_ RM 모두에 대해 \(0.01\)입니다. 학습률은 표준 RM은 \(9.3\mathrm{e}^{-6}\), 분류 RM은 \(5.8\mathrm{e}^{-6}\), 비교 RM은 \(1\mathrm{e}^{-6}\)을 사용하였다. 최종 피니튜닝에서는 10개 에폭에 걸친 검증 분할에서 가장 좋은 RM을 선택한다.

GPT-3 보상 모델. GPT-3-175B를 RM으로 미세화하기 위해 OpenAI API를 활용한다. 우리는 두 요약 중 어떤 요약이 우수한지 예측하는 _비교_ RM과 주어진 요약이 고품질인지 여부를 예측하는 _분류_ RM이라는 두 가지 유형의 RM을 세분화한다. 비용 고려를 위해 1K 샘플(5K 대신)의 학습 데이터 세트에 대해 하이퍼파라미터 튜닝을 수행하고 200개의 샘플 개발 데이터 세트에 대해 평가한다. 우리는 비용상의 이유로 1K 샘플이 포함된 데이터 세트를 사용한다. 그런 다음 에포크 측면에서 조기 정지를 구현하는 동안 5K 샘플에 대해 피니튜닝할 때 동일한 하이퍼파라미터를 적용한다. 분류 보상 모델에서 인간 선호 주석의 이진 특성으로 인해 이 모델에 대한 효과적인 훈련 데이터 세트 크기는 2K 샘플로 두 배로 증가한다.

최적의 하이퍼파라미터를 결정하기 위해 에폭수 \(\{1,2,3,4\}\)와 프롬프트 손실 가중치 \(\{0,0.001,0.005,0.01,0.05,0.1,0.5\}\)에 대한 값의 범위에 걸쳐 스윕을 수행한다. OpenAI API는 각 에포크 이후의 전체 개발 데이터 세트에 대한 분류 정확도(비교 및 분류 작업 모두에 대해)를 제공하여 적절한 수의 에포크를 선택하고 신속한 손실 가중치를 선택할 수 있다. 5K 샘플에 대한 피니튜닝 시, 비교 모델은 1에포크, 프롬프트 손실 가중치는 0, 분류 모델은 4에포크, 프롬프트 손실 가중치는 \(0.001\)을 사용하여 과적합을 방지하기 위해 조기 정지를 활용한다. 데이터 세트 크기에 따라 달라질 수 있는 다른 모든 하이퍼 매개 변수에 대해 기본값을 사용합니다.

## 부록 H 추가 결과

### Finetuned 모델 분석

표 6에서 우리는 미세 조정, 초기 요약 및 인간 요약과 같은 미세 조정에 사용되는 다양한 미세 조정 데이터 세트에 대해 GPT-3-175B를 평가한다. 다양한 열차 데이터 세트(초기 요약, 정제 및 인간 요약)의 1K 샘플 요약에서 GPT-3-175B의 로그 가능성을 평가한다. 구체적으로, 우리는 전체 프롬프트를 전달합니다.

그림 7: 여기에서 로그 로그 그림에 100, 1K 및 5K 샘플에 대해 훈련된 OPT-RM의 검증 정확도를 표시한다. 그림은 데이터 세트 크기를 늘릴 때 크기 조정을 보여 줍니다.

레딧 게시물을 포함하여 GPT-3-175B는 완료의 로그 가능성, 즉 생성된 요약만 평가한다. 또한 ILF 훈련 전에 ILF-finetuned 모델과 사전 훈련된 LM, \(D_{\text{KL}}(\text{finetuned}|\text{GPT-3-175B})\) 사이의 (역방향) KL 발산(후행 Gao et al., 2022)을 측정한다. 본 논문에서는 피니튜닝된 모델로부터 무조건적으로(즉, 문장 토큰의 시작을 사용하여) 샘플링하고 GPT-3-175B를 사용하여 생성된 텍스트의 로그 우도를 평가한다. 또한 전방 KL 발산 \(D_{\text{KL}}(\text{GPT-3-175B}|\text{finetuned})\)을 보고한다. 우리는 SS4.6의 결과에 대해 논의한다.

### 결과: ILF + OPT-RM

이 섹션에서는 가장 성능이 좋은 방법 ILF + OPT-RM 및 기타 추가 방법의 전체 결과를 제시한다(ILF + OPT-RM에 대한 설명은 SS4.3.1, 결과에 대한 설명은 SS4.3.3 참조). SS4.3.2에 설명된 것과 동일한 평가를 수행하며, 즉 인간 평가에서 주석자는 품질에 따라 다양한 테스트 요약의 순위를 매긴다. 그런 다음 평가 메트릭으로 사용하는 인간 서면 요약에 대한 승률을 계산한다. 중요한 것은 여기에서 평가된 모든 방법이 5K 샘플이 있는 데이터 세트에 대해 훈련된다는 것이다. 여기서 비교되는 방법들은 도 3에서 비교되는 방법들과 정확히 동일하지 않다는 점에 유의한다. 구체적으로, 방법들(ILF, finetuning on refinements), 인간 요약들(finetuning on human summaries) 및 OPT-RM best-of-64 FeedME에 의해 생성된 테스트 요약들은 도 3에서와 동일하다. 도 3을 참조하면, 5K 샘플들에 대해 트레이닝된 대응하는 방법들에 의해 생성된 테스트 요약들에 대해. 그러나 여기에서는 초기 요약에서 FeedME와 피니튜닝을 평가하지 않는다. 그러나 가장 성능이 좋은 모델인 ILF + OPT-RM(best-of-64)을 평가하여 그림 1에 추가했다. 참고로 도 3과 같다. 우리는 또한 아래에서 설명하는 _Finetuned on Feedback + Refinements_라는 새로운 방법을 평가한다.

피드백 + 정제에 대한 핀튜닝을 위해 제목, 게시글 및 요약문을 입력으로 사용하고 모델을 학습하여 예측한다.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Model &
\begin{tabular}{c} Neg. Log Likelihood of GPT-3-175B \\ on 1K train samples of respective distribution \\ \end{tabular} & \(D_{KL}(\text{GPT-3-175B}|\text{finetuned})\) (in nats) & \(D_{KL}(\text{finetuned}|\text{GPT-3-175B})\) (in nats) \\ \hline Finetuned on Initial Summaries & \(1.19\pm 0.01\) & \(0.43\pm 0.11\) & \(0.83\pm 0.08\) \\ Finetuned on Refinements & \(1.37\pm 0.01\) & \(0.60\pm 0.10\) & \(1.10\pm 0.06\) \\ Finetuned on Human Summaries & \(1.61\pm 0.01\) & \(0.12\pm 0.09\) & \(0.55\pm 0.01\) \\ OPT-RM best-of-64 FeedME & - & - & \(3.17\) \\ \hline \hline \end{tabular}
\end{table}
표 6: 먼저 우리가 미세 조정하는 다양한 데이터 분포의 1K 샘플에서 GPT-3-175B의 로그 가능성을 평가한다. 그런 다음 GPT-3-175B에서 길이 64 토큰의 2000개 텍스트를 샘플링하고 샘플에서 미세 조정된 모델의 로그 가능성을 평가하여 KL-발산을 경험적으로 계산한다(역 KL은 미세 조정된 모델에서 샘플링하고 샘플에서 GPT-3-175B를 평가한다). 우리는 2개의 런에 걸쳐 평균 및 표준 오차를 보고한다. 특정 보상 모델에서 Best of 64의 경우, 분석 공식 \(KL(N,RM)=\log N-\frac{N-1}{N}\)(또한 (Hilton & Gao, 2022) 참조)를 사용한다.

그림 8: 해당 검증 데이터 세트의 500개 샘플에 대한 5K 초기 요약, 정제 및 인간 요약에 대해 세분화된 모델의 평가. 예를 들어, 인간 요약에 대해 미세 조정된 모델은 검증 데이터세트로부터 500개의 인간 요약에 대해 평가된다. 정련에서 피닝된 모델은 인간 요약에서 피닝된 모델보다 음의 로그 가능성이 상당히 낮다.

대응하는 피드백 및 정제. 이 접근법에 대한 우리의 동기는 피드백을 먼저 생성하는 것이 자체 촉진 방법에 대한 이전 작업의 결과와 유사하게 결과 정제의 품질을 향상시킬 수 있다는 것이다. Saunders 등(2022); Bai 등(2022) 및 Chain of Thought(CoT) 프롬프트 기술 Wei 등(2022). CoT는 모델이 질문에 대답하기 전에 추론하도록 허용할 때 다양한 태스크 Wei 등(2022)에 걸쳐 모델의 성능을 향상시키는 것으로 나타났다. 피드백과 정제에 대한 피니튜닝을 위해 인간의 피드백을 수집하는 데 사용된 초기 요약과 방법에 의해 생성된 정제를 활용한다. 손실 \(\log p(x_{1},f|\text{prompt})+\lambda\log p(\text{prompt})\), 즉, 정제 및 피드백 예측을 학습한다. 개선 알고리즘(즉각 손실 가중치 포함)에서 피니튜닝과 동일한 하이퍼파라미터를 사용한다. 테스트하는 동안 초기 요약을 필요로 하며, 이 요약을 통해 피드백 및 개선 사항을 생성합니다. 초기 요약으로서, 우리는 FeedME(도 3에서 평가된 바와 같이)에 의해 생성된 테스트 샘플을 사용한다. 테스트 요약의 48 토큰 길이 제한과 호환성을 보장 하기 위해 훈련 중 피드백 및 정제 끝에 특수 끝 토큰 / _n ###_ 을 추가 합니다. 테스트 시간에 최대 토큰 수를 설정 하 여 300을 생성 하 고 중지 단어 / _n ###_가 표시 되 면 생성을 종료 합니다. 그런 다음 부록 G.1에 설명된 동일한 후처리 절차를 적용하여 정제를 48개의 토큰으로 단축한다. 우리가 사용한 정확한 프롬프트 템플릿은 부록 J.3을 참조한다.

우리는 그림 9에서 모든 결과를 제시한다. 우리는 5K 정제 세트에 대한 피니튜닝이 \(36.0\pm 1.8\%\)의 승률을 달성하는 반면 ILF + OPT-RM(best-of-64)은 \(50.8\pm 1.9\%\)의 승률을 가져 인간 수준의 요약 성능을 달성한다는 것을 발견했다(더 자세한 논의는 SS4.3.3 참조). OPT-rM best-of-64 FeedMe는 승률 \(45.1\pm 1.9\%\), 5K 인간 생성 요약 집합에 대한 피니튜닝은 승률 \(35.4\pm 1.8\%\), 5K 피드백과 정제의 조합에 대한 피니튜닝은 승률 \(26.1\pm 1.7\%\)이다. 피드백과 정제에 대한 피니튜닝의 성능이 정제에만 대한 피니튜닝의 성능보다 낮다는 점에 주목할 필요가 있다. 우리는 이것이 피드백과 정제 모두를 생성하는 데 어려움이 증가했기 때문이며 이러한 불일치가 모델, 데이터 세트 크기 또는 하이퍼파라미터의 제한 때문일 수 있다고 믿는다. 이전 연구는 피드백 Saunders 등(2022); Bai 등(2022)을 생성하기 위한 훈련 모델의 가능성을 입증했으며, 따라서 우리는 추가 최적화 및 실험이 이 방법의 성능을 향상시킬 수 있다고 믿는다. 또한, 5K 정제, 5K 인간 요약 및 최량-of-64 FeedME에 대한 피니튜닝에 대한 결과가 도 3의 결과로부터 벗어남을 주목하고자 한다. 이는 상이한 방법들을 서로 비교하고, 인간 주석은 일반적으로 어느 정도의 노이즈(상이한 사람들이 동일한 샘플에 주석을 다는 것을 감안할 때)를 포함하기 때문이다.

### ILF의 다중 반복

우리의 실험은 ILF가 LM의 훈련에서 언어 피드백을 활용하는 효과적인 방법임을 시사한다. 여기에서 우리는 정제 및 운동 조정을 여러 번 반복하여 가장 일반적인 형태로 ILF를 탐구한다.

데이터셋 개선.본 실험에서는 ILF를 이용하여 데이터셋 분포의 반복적 정제의 효과를 평가한다. 이를 위해, 먼저 ILF의 반복 1로부터 100개의 정제들에 대해 GPT-3-175B를 미세조정한다(즉, 하나의 반복을 행함).

도 9: 인간 평가자들이 얼마나 자주 ILF로부터 요약들을 선호하는지: Finetuned on Refinements, OPT-RM best-of-64 FeedME, ILF + OPT-RM(best-of-64), finetuning on human summaries, finetuning on feedback + refinements(모든 방법들은 5K 샘플들에서 finetuned). ILF + OPT-RM(best-of-64)은 인간의 요약과 유사한 품질의 요약들을 생성한다. 피드백에 대한 피니튜닝 + 정제들은 ILF(finetuning on refinements)보다 더 나쁜 성능을 수행한다.

본 논문의 주요 결과와 마찬가지로 초기 요약을 정제하는 데 SS4.3.2)를 참조하고 이 미세 조정된 모델을 \(M_{1}^{100}\)이라고 한다. 여기에서 사용하는 표기법은 첨자가 정제에서 생성된 ILF의 반복을 나타내고 위첨자는 모델이 미세 조정된 전체 샘플의 수를 나타낸다. 또한 반복 1의 100개 정제 데이터셋을 \(\mathcal{D}_{1}^{100}\)이라 한다. 기준선으로서, 우리는 ILF 반복 1에서 추가 100개의 정제물에 대해 \(M_{1}^{100}\)을 조정하여 \(M_{1}^{200}\), 즉 ILF 반복 1에서 200개의 정제물에 대해 훈련된 모델을 생성한다. 그리고 이 기준선을 ILF의 두 반복과 비교한다. 구체적으로, \(M_{1}^{100}\)을 사용하여 추가 100개의 샘플(기준선과 동일한 레딧 게시물)에 대한 요약을 생성하고 해당 요약에 대한 인간 피드백을 수집한다. 그런 다음 이 피드백을 사용하여 FeedME7을 사용하여 5개의 정제를 생성한 다음 InstructRM 방법을 사용하여 최상의 정제를 선택한다. ILF의 두 번째 반복에서 선택된 100개의 정제를 \(\mathcal{D}_{2}^{100}\)이라 한다. 마지막으로, \(\mathcal{D}_{2}^{100}\)에 대한 \(M_{1}^{100}\)을 조정하여 ILF의 첫 번째와 두 번째 반복에서 총 200개의 정제 과정을 학습한 모델 \(M_{1,2}^{200}\)을 얻는다. 모든 피니튜닝은 100개의 정제물에 대한 피니튜닝을 위해 부록 G에 설명된 것과 동일한 하이퍼파라미터를 사용하여 수행되었다. 모든 모델과 훈련 데이터 세트의 개요는 표 7을 참조한다.

각주 7: 이상적으로는 동일한 모델 \(M_{1}^{100}\)을 사용하여 정제를 생성한다. 그러나, 우리의 경우 명령-운동 모델이 아닌 GPT-3-175B를 피니튜닝했기 때문에 이것은 불가능하다.

이 인간 평가에서는 기본 모델(\(M_{1}^{200}\))에 의해 생성된 요약과 테스트 세트의 ILF(\(M_{1,2}^{200}\))를 두 번 반복하여 생성된 요약의 성능을 비교한다. 인간 평가자는 각 비교에 대해 선호하는 요약을 표시하도록 요청받고 \(M_{1,2}^{200}\)에 대한 \(M_{1}^{200}\)의 승률을 계산하고 그림에 표시한다. 10(왼쪽)8. 본 연구의 결과는 ILF의 두 번의 반복이 \(53.2\pm 1.9\%\)의 승률로 한 번의 반복을 능가하는 것으로 나타나 여러 라운드의 ILF를 적용하면 데이터 분포를 개선할 수 있음을 보여준다. 그러나 우리는 또한 여러 라운드의 ILF가 처음부터 첫 번째 라운드에서 동일한 개수의 정제에서 직접 피니튜닝하는 것보다 더 나은 모델로 이어지는지 조사하고자 한다. 즉, 현재 기준선은 추가 100개의 샘플에 대한 추가 finetuning \(M_{1}^{100}\)으로 구성되어 있지만, ILF의 첫 번째 반복, 즉 \(M_{scratch,1}^{200}\)에서 200개의 정제에서 GPT-3-175B를 직접 finetuning하는 것도 가능하다. 우리는 텍스트 요약 작업에 대한 모델 성능을 개선하는 데 있어 이 두 가지 접근법의 상대적 효과를 결정하는 것을 목표로 한다.

각주 8: 참고, 기준선은 ILF의 한 반복과 동일하기 때문에 100개의 샘플에서 수동으로 승률을 \(50\%\)으로 설정했다.

모델 개선.이 실험에서, 우리는 ILF의 첫 번째 반복에서 유사한 수의 정제에서 직접 피니튜닝하기 위해 여러 라운드의 ILF의 성능을 비교하는 것을 목표로 한다. 기준선으로서 우리는 ILF의 첫 번째 반복에서 200개 및 300개 정제에서 GPT-3-175B를 미세 조정하고 부록 G에 설명된 대로 하이퍼파라미터 조정을 수행한다. 그런 다음 이러한 기준선을 2개 및 3개의 ILF 라운드와 비교한다. 2라운드 ILF 모델의 경우 이전에 설명한 \(M_{1,2}^{200}\)을 사용한다. 3라운드 ILF 모델을 얻기 위해 \(M_{1,2}^{200}\)을 사용하여 추가 100개의 샘플(기준선과 동일한 Reddit 포스트에서)에 대한 요약을 생성하고, 인간 피드백을 수집하고, 피드백을 사용하여 GPT-3-175B로 5개의 정제를 생성하고, InstructRM을 사용하여 최상의 정제를 선택하여 \(\mathcal{D}_{3}^{100}\)을 생성한다. 그런 다음 \(\mathcal{D}_{3}^{100}\)에서 \(M_{1,2}^{200}\)을 미세 조정하여 \(M_{1,2}^{300}\) 모델을 얻는다. 본 연구의 기준선은 GPT-3-175B를 200과 300 정제에서 처음부터 다시 조정하는 반면, \(M_{1,2}^{200}\)과 \(M_{1,2,3}^{300}\)은 추가 정제에서 반복적으로 모델을 계속 조정하여 얻어진다. 이러한 접근 방식의 차이는 서로 다른 하이퍼파라미터와 데이터 세트 크기를 사용하기 때문에 결과에 불일치를 일으킬 수 있다.

\begin{table}
\begin{tabular}{|l|l||l|l|l||} \hline \multirow{2}{*}{Initial Model} & \multirow{2}{*}{Finetuned Model} & \multicolumn{3}{c||}{Finetuning dataset} & \multirow{2}{*}{Produces Dataset} \\ \cline{3-3} \cline{5-5}  & & ILF iteration 1 & ILF iteration 2 & ILF iteration 3 \\ \hline \hline GPT-3-175B & \(M_{1}^{100}\) & \(\mathcal{D}_{1}^{100}\) & & \(\mathcal{D}_{2}^{100}\) \\ \hline \(M_{1}^{100}\) & \(M_{1}^{200}\) & \(\mathcal{D}_{1}^{100*}\), & & \\ \hline GPT-3-175B & \(M_{scratch,1}^{200}\) & \(\mathcal{D}_{1}^{200}\) & & \\ \hline GPT-3-175B & \(M_{scratch,1}^{300}\) & \(\mathcal{D}_{1}^{300}\) & & \\ \hline \(M_{1}^{100}\) & \(M_{1,2}^{200}\) & \(\mathcal{D}_{1}^{100}\) & \(\mathcal{D}_{2}^{100}\) & \(\mathcal{D}_{3}^{100}\) \\ \hline \(M_{1,2}^{200}\) & \(M_{1,2}^{300}\) & \(\mathcal{D}_{1}^{100}\) & \(\mathcal{D}_{2}^{100}\) & \(\mathcal{D}_{3}^{100}\) \\ \hline GPT-3-175B & \(M_{scratch,1,2}^{200}\) & \(\mathcal{D}_{1}^{100}\) + \(\mathcal{D}_{2}^{100}\) & & \\ \hline GPT-3-175B & \(M_{scratch,1,2,3}^{300}\) & \(\mathcal{D}_{1}^{100}\) + \(\mathcal{D}_{2}^{100}\) + \(\mathcal{D}_{3}^{100}\) & \\ \hline \end{tabular}.

\end{table}
표 7: 모델 \(M\)이 트레이닝되고 이들이 생성하는 데이터 세트(정제). 위 첨자는 샘플 수를 나타내는 반면 아래 첨자는 ILF 단계를 나타낸다. 이 그림에서 우리는 주어진 피드백을 생성하는 데 사용되는 FeedME를 보여주지 않는다.

* 이러한 샘플은 \(\mathcal{D}_{1}^{200}\)의 간격 [100,200]에서 가져온 새로운 샘플로 학습 역학에 영향을 줄 수 있습니다. 이 전위차를 제어하기 위해 우리는 또한 ILF의 다양한 반복을 통해 생성된 정제에서 처음부터 GPT-3-175B를 미세 조정한다. 구체적으로, \(M_{1,2}^{200}\)의 대안으로 GPT-3-175B를 ILF의 첫 번째 라운드(즉, \(\mathcal{D}_{1}^{100}\))와 ILF의 두 번째 라운드(즉, \(\mathcal{D}_{2}^{100}\))에서 100개의 정제를 연결한 상태에서 처음부터 조정하고 결과 모델을 \(M_{scratch1,2}^{200}\)으로 참조한다. 마찬가지로, \(M_{1,2,3}^{300}\)의 대안으로 GPT-3-175B를 1차 ILF (\(\mathcal{D}_{1}^{100}\)), 2차 ILF (\mathcal{D}_{2}^{100}\) 및 3차 ILF (즉, \(\mathcal{D}_{3}^{100}\))에서 100개의 정제를 연결한 상태에서 처음부터 조정하고 결과 모델을 \(M_{scratch1,2,3}^{300}\)으로 참조한다. ILF의 두 번째와 세 번째 라운드(즉 \(\mathcal{D}_{2}^{100}\)와 \(\mathcal{D}_{3}^{100}\))의 정제는 연속적으로 미세 조정된 모델(즉 \(M_{1}^{100}\)과 \(M_{1,2}^{200}\))을 사용하여 생성된 요약을 기반으로 한다는 점에 주목할 필요가 있다. 따라서 모델 \(M_{scratch1,2}^{200}\)과 \(M_{scratch1,2,3}^{300}\)은 ILF의 직접적인 적용이 아니라 ILF에 의해 유도된 분포의 근사치이다. 모든 모델과 훈련 데이터 세트의 개요는 표 7을 참조한다.

인간 평가를 사용하여 테스트 데이터 세트에 대한 세 가지 방법, 즉 기준선, 연속 피니튜닝이 있는 ILF 및 처음부터 피니튜닝으로 근사화된 ILF의 성능을 비교한다. 결과는 그림 1에 나와 있다. 10(오른쪽). 이 보다 현실적인 기준선을 사용하여 ILF를 직접 적용하는 것은 200개 및 300개 샘플에 대해 각각 \(49.4\pm 1.9\%\) 및 \(50.9\pm 1.9\%\)의 승률로 기준선에 따라 개선되지 않는다는 것을 발견했다. 그러나 ILF에 의해 유도된 분포를 처음부터 다시 조정하여 ILF를 근사하는 것은 300개의 샘플에 대해 기준선에서 크게 향상되었으며 승률은 \(55.6\pm 1.9\%\)이다. 이 방법은 200개의 샘플에 대해 기준선보다 약간 더 나쁘며 승률은 \(48.9\pm 1.9\%\)이다. 우리는 현재 연속적인 피니튜닝이 재앙적인 망각으로 이어질 수 있는 반면, 처음부터 피니튜닝은 이러한 문제가 없을 수 있다고 가정한다. 이것은 300개의 샘플에 대해 \(M_{scratch1,2,3}^{300}\)이 \(M_{1,2,3}^{300}\)보다 훨씬 더 나은 성능을 보이는 이유를 설명할 수 있다. 구체적으로, \(M_{1,2}^{200}\)은 실제로 ILF의 세 번째 반복에서 개선된 분포를 생성할 수 있다. 그러나 이 개선된 분포 \(\mathcal{D}_{2}^{100}\)에 대해 \(M_{1,2}^{200}\)을 더 조정하면 모델이 이전에 학습한 것을 잊어버릴 수 있다. 반면에 ILF에 의해 생성된 모든 데이터 세트의 연결에 대해 처음부터 학습하는 모델 \(M_{scratch1,2,3}^{300}\)은 실제로 아무것도 학습하지 않기 때문에 개선된 데이터 세트 배포의 이점을 얻을 수 있다. 그러나 \(M_{scratch1,2}^{200}\)이 개선된 데이터 분포 \(\mathcal{D}_{2}^{100}\)의 이점을 얻지 못하는 이유는 불분명하다. 또한, 하이퍼파라미터가 다양한 모델의 최종 성능에 중요한 역할을 하고 데이터세트 크기가 모델 성능에 강한 영향을 미칠 수 있다(예를 들어, 더 많은 샘플에 대한 피니튜닝은 더 적은 샘플에 대한 피니튜닝보다 더 안정적일 수 있다). 향후 연구에서는 이러한 질문에 답하고 데이터 세트 크기와 반복 횟수가 ILF에 미치는 영향을 더 잘 이해하기 위해 보다 정교한 실험을 수행할 계획이다. 구체적으로, ILF의 다중 반복을 실행하고 \(M_{1,2}^{200}\) 대신 \(M_{scratch1,2}^{200}\)을 모델로 사용하여 ILF의 세 번째 라운드에서 요약을 생성하는 것을 목표로 한다. 이것은 우리가 또한 개선된 분포를 생성하는 동일한 모델을 미세 조정하기 때문에 ILF의 근사보다는 직접적인 구현일 것이다. 또한 데이터 세트 크기와 반복 횟수가 ILF에 미치는 영향을 조사하기를 바란다. 전반적으로, 우리의 결과는 ILF가 인간 피드백을 언어 모델의 훈련에 지속적으로 통합함으로써 자연 언어 처리 시스템의 성능을 향상시킬 가능성이 있음을 시사하지만 이 접근법을 활용하는 최상의 방법을 완전히 이해하기 위해서는 추가 연구가 필요하다.

### 파이닝 데이터 집합에 대 한 품사 배포

우리는 세 가지 미세 조정 데이터 세트, 즉 초기 요약, 정제 및 인간 요약에서 GPT-3-175B의 음의 로그 가능성을 평가한다. 1K 샘플로 훈련 데이터 세트를 사용하고 다른 품사 태그에 대해 음의 로그 가능성을 계산한다. 본 실험을 위해 Stanza(Qi et al., 2020)를 PoS 태거로 사용하고, 그 단어를 기능단어, 내용단어, 기타의 세 그룹으로 분리한다. 기능어는 어휘의미가 거의 없는 단어로서 관사, 대명사, 전치사, 접속사, 보조동사, 입자와 주사가 있다. 한편, 내용 단어는 의미 정보를 담고 있는 단어로 명사, 형용사, 부사 및 어휘 동사 등이 있다. 우리는 숫자와 기호를 그룹 _others_ 아래에 둔다. 이 분석을 통해 모델 생성(초기 요약 및 정제)과 인간 작성 요약 사이에 다른 패턴을 찾고자 한다. 높은 음의 로그 가능성은 높은 손실을 의미한다는 점에 유의해야 한다. 우리는 그림 11에 결과를 제시한다. 인간의 요약에 대해 평균 손실이 더 높기 때문에 평균 0과 표준 편차 1을 갖도록 변환하여 모든 손실 값을 정규화한다. 전반적으로 단어 분포는 세 가지 피니튜닝 데이터 세트 모두에서 매우 유사하다. 정규화된 평균 손실 측면에서 콘텐츠 단어가 정제 데이터 세트에 더 큰 영향을 미치는 방법이 흥미롭다. 우리는 이것이 섹션 4.3.3의 결과와 관련이 있다고 믿으며, 여기서 정제에서 피니튜닝할 때 최상의 결과를 얻는다.

### Comparison to Results of Scheurer et al. (2022)

여기서 우리는 우리의 결과를 Scheurer 등(2022)의 이전 작업과 관련시킨다. 인 것을 특징으로 하는 액정표시장치. Scheurer 등(2022)의 2는, 초기 요약에 대한 finetuning, FeedME(InstructGPT라고 함)로부터의 샘플링, GPT-3-175B로부터의 샘플링과 같은 다양한 기준선에 대해 정제에서 finetuning하는 그들의 방법을 비교한다. 그들은 레딧으로부터 자동으로 추출되는 인간 서면 요약(Volske 등, 2017)에 대한 모든 방법의 승률을 계산한다. SS4.1과 App.C에서 볼 수 있듯이, 우리의 인간 요약은 Volske 등(2017)의 인간 요약보다 \(72.3\pm 3.2\%\)을 선호한다. 이는 우리가 훨씬 더 강한 기준선을 사용하기 때문에 Scheurer 등(2022)의 승률이 우리의 경우보다 훨씬 높다는 것을 의미한다.

우리는 이제 Scheurer 등(2022)에서 발견된 결과와 본 논문에서 발견된 결과의 세 가지 차이점을 제시한다. 그런 다음 차이점을 설명할 수 있는 다양한 잠재적 이유를 제공할 것이다. 먼저, Scheurer 등(2022)의 결과(상대적 용어로)를 비교하면 다음과 같다. 그림 2에서 우리의 결과를 보여준다. 도 3을 참조하면, 100개의 샘플에 대해 미세조정을 하면, 성능의 차이를 볼 수 있다. Scheurer 등(2022)은 정제들에 대한 피니튜닝이 초기 요약들에 대한 피니튜닝보다 우수하다고 보고한다. 그리고 두 방법 모두 FeedME(즉, InstructGPT)로부터 샘플링을 능가한다. 100개의 정제기에 대한 피니튜닝은 인간의 요약에 대한 승률 \(19.6\pm 1.5\%\), 초기 요약에 대한 피니튜닝은 승률 \(19.6\pm 1.5\%\), FeedME는 승률 \(20.8\pm 1.5\%\)을 얻었다. 따라서 두 가지 미세 조정된 방법은 모두 동일하게 수행되며 FeedME의 샘플링보다 더 나쁘다.

둘째, 요약 요약 결과를 피드백과 비교한다. Scheurer 등(2022)은 정제들을 선택하기 위해 임베딩-기반 스코어링 함수를 사용하는 반면, 우리는 InstructRM을 사용한다. Scheurer 등(2022) 도. 3(왼쪽) Refine with Feedback + Best of N은 초기 요약(FeedME에서 샘플링)에 대해 \(67.0\pm 3.1\%\), Refine with Feedback은 \(60.5\pm 3.0\%\), Refine without Feedback은 \(50.3\pm 2.6\%\), Human Summaries는 \(60.8\pm 3.4\)의 승률을 보인다. 인 것을 특징으로 하는 방법. 4(왼쪽) Refine with Feedback + Best-of-5는 승률 \(69.1\pm 1.9\%\), Refine with Feedback은 승률 \(63.9\pm 2.0\%\), Refinement without Feedback은 승률 \(59.4\pm 2.0\%\), Human Summaries는 승률 \(83.2\pm 1.7\%\). 우리가 더 나은 인간 요약을 사용한다는 점을 감안할 때 인간 요약의 차이가 예상된다. Refinement without Feedback 방법은 Scheurer 등(2022)에 비해 본 연구에서 더 높은 결과를 얻을 수 있었다.

셋째, 임베딩 유사도를 스코어링 함수로 사용하는 것이 Scheurer 등(2022)에서 잘 작동했지만 우리의 설정에서는 작동하지 않는다는 점도 주목할 만하다(결과에 대한 논의는 표 2 및 SS4.2 참조). 이것은 우리가 수집하는 피드백이 많은 주석자에 의해 작성되어 훨씬 더 다양하기 때문이라고 믿는 반면, Scheurer 등(2022)에서는 저자 스스로 피드백을 작성했다.

여기서 우리는 이제 Scheurer 등(2022)의 설정과 우리의 논문에서 다양한 차이점을 나열하며, 이는 모두 다른 결과를 설명할 수 있다.

1. Scheurer 등(2022)은 임베딩 유사도를 스코어링 함수로 사용하는 반면, 우리는 InstructRM Ensemble을 사용한다. 탭을 보고 있어 2와 SS4.2의 해당 논의는 이미 방법이 매우 다르다는 것을 보여준다.
2. 인간-기록된 요약은 Scheurer et al.(2022) (SS4.1 참조 및

그림 10: **왼쪽**: ILF의 첫 번째 반복에서 동일한 수의 세분화에 대 한 Finetuning에 대 한 ILF의 2 반복의 승률입니다. **오른쪽**: ILF의 3회 반복의 승률 및 ILF의 첫 번째 반복에서 동일한 수의 세분화에 대한 finetuning에 대해 처음부터 finetuning으로 ILF의 3회 반복을 근사화합니다.

App. C)
3. Scheurer 등(2022)에서, 주석 명령어들은 피드백이 요약을 개선하는 방법을 언급해야 한다고 구체적으로 명시한다. 저희 작업에서는 훨씬 더 무제한적이고 다양한 피드백을 수집합니다. 이 차이는 임베딩 유사성이 설정에서 점수 함수로서 잘 작동하지 않는다는 사실에서도 분명하다.
4. Scheurer 등(2022)에서, 저자들 스스로 데이터에 주석을 달았고, 즉, 그들은 피드백을 작성하고 최종 요약을 평가했다. 우리의 경우, 우리는 이 과제에 대해 훈련받은 독립적인 평가자를 사용한다. 전체적으로 31개의 주석기를 사용하면 방법에 대한 보다 다양하고 덜 편향된 추정치를 얻을 수 있다. 또한 인간의 평가를 하는 것은 본질적으로 시끄럽고 결코 동일한 결과를 초래하지 않을 것이다.
5. Scheurer 등(2022)의 평가는 본 작업에서와 다른 데이터세트에서 수행되었다. 특히, 그들은 방법을 평가하기 위해 100개의 샘플만 사용했지만 우리는 698개의 샘플 테스트 세트를 사용했다.
6. 샘플링 및 피니튜닝에 사용되는 Scheurer 등(2022)의 하이퍼파라미터는 본 연구에서 사용된 하이퍼파라미터와 다르다.
7. 전반적으로 Scheurer 등(2022)과 다른 프롬프트를 사용한다(App. J.3 및 App. J.1 참조).

## Appendix I Annotator 명령어

전반적으로 우리는 데이터 세트를 만들고 알고리즘을 평가하기 위해 많은 주석을 완료했다. 지침은 작업에 따라 다르며 지속적으로 업데이트되었다. 다음에서는 열차 데이터 세트를 만드는 데 사용한 지침과 요약 품질을 평가하기 위해 제공한 지침을 제공한다(6개 요약). 간결함을 위해 더 많은 지침을 공유하지는 않지만 요청 시 제공할 수 있습니다.

### Train Dataset Annotation 명령어

#### Task Overview

먼저 주의 깊게 읽어야 하는 레딧 포스트가 제공됩니다. 그런 다음 두 요약 비교, 요약에 대한 피드백 쓰기, 피드백 유형 분류, 추가 피드백 여부 표시, 이상적인 요약 쓰기로 구성된 5개의 하위 작업을 완료해야 한다. 이러한 작업을 수행할 때 아래 지침을 준수하십시오.

#### 좋은 요약은 무엇인가요?

대략적으로 말하면, 좋은 요약은 원문의 본질을 가진 짧은 텍스트 조각이다. 좋은 요약은 동일한 목적을 달성하려고 시도하고 원문과 동일한 정보를 전달한다. 우리는 당신이 이러한 다양한 차원의 요약을 고려하기를 바랍니다:

**에센스**: 요약이 게시물의 좋은 표현입니까? 그 요약은 게시물의 중요한 정보를 얼마나 잘 다루나요?

도 11: 콘텐츠 및 기능 단어 측면에서 1K 샘플을 갖는 다양한 피니튜닝 데이터 세트의 토큰의 분포. 우리는 프롬프트가 모든 분포에 대해 동일하기 때문에 다양한 완성, 즉 요약만을 평가한다.

**명확성**: 요약 판독기가 친숙한가요? 아이디어를 명확하게 표현하나요?

**정확도**: 요약에 게시물과 동일한 정보가 포함되어 있습니까?

**목적**: 요약이 원래 게시물과 동일한 목적을 제공합니까?

**간결함**: 요약이 짧고 요점까지입니까?

**스타일**: 요약이 원본 게시물과 동일한 스타일로 작성되었습니까?

일반적으로 목록의 맨 위에 있는 치수에 더 높은 가중치를 부여합니다. 위의 차원 중 어떤 것도 단순한 예/아니오 문제이고 다른 차원을 거래하는 데 강력하고 빠른 규칙이 없기 때문에 평가는 복잡할 수 있다. 최선의 판단과 상식을 사용하여 이러한 절충안을 만들 수 있습니다. 하위 레딧, 제목 및 레딧 게시물이 발생한 일에 대해 약간의 모호성을 남기는 경우, 특정 방식으로 텍스트를 해석하는 것이 아니라 주석에 정확하게 반영하는 것이 중요하다. 항상 모든 하위 레딧, 제목 및 레딧 게시물을 보고 주어진 모든 정보를 사용하여 판단을 내린다(때로는 제목에는 게시물에 나타나지 않지만 그럼에도 불구하고 사용해야 하는 중요한 정보가 포함될 수 있다).

먼저 서브레딧 카테고리, 제목 및 게시물을 주의 깊게 읽으십시오. 서브레딧은 웹 사이트 레딧의 특정 주제에 대한 포럼입니다. 이 단계를 천천히 수행하고 처음에 이해하지 못했을 수 있는 부분을 다시 읽으십시오. 아래는 각 레딧 게시물에 대해 완료해야 하는 작업에 대한 자세한 설명입니다.

아래는 각 Reddit 게시물에 대해 완료해야 하는 각 작업에 대한 자세한 설명입니다.

1. **비교 작업**: 한 쌍의 요약이 주어지면 어느 것이 더 나은지 나타냅니다. _ 상세사항_: 양호한 요약을 만드는 것에 대한 상기 설명을 사용한다. 두 요약이 서로 동일한 사본이거나 하나의 요약이 다른 요약보다 우수하게 만드는 구별되는 기능이 없는 경우 요약 중 하나를 선택하는 것이 좋다. 그러나, 하나의 요약을 다른 것보다 더 좋게 만드는 작은 세부사항이 있다면, 그것은 그 요약을 선택하는 충분한 이유이다.
2. **피드백 작업**: 요약의 가장 중요한 단일 단점에 대해 지정된 요약에 대한 짧고 간단한 피드백을 씁니다. 피드백은 피드백이 어떤 카테고리(정확도, 커버리지, Coherence, 기타)에 속하는지 언급해서는 안 되며, "Coverage", "Accuracy", 또는 "Coherence"의 정의에 대한 지식을 가정해서는 안 된다(아래 참조). 그렇지 않으면, 피드백은 요약의 가장 중요한 결점을 여전히 다루면서 가능한 한 짧고 단순해야 한다. _ 세부사항_: 피드백을 하나 또는 여러 문장으로 작성할 수 있지만, 요약의 단일, 가장 중요한 단점만을 다루고 가능한 한 짧아야 한다. 피드백을 작성하는 방법과 주소가 정확히 무엇인지에는 다른 제한이 없습니다. 요약서에 부족한 점이 없다면 피드백에서도 요약에 대해 긍정적인 점을 언급할 수 있다. 좋은 요약을 만드는 것에 대한 설명을 사용하여 좋은 요약을 만드는 다양한 차원을 교환한다. 종종 피드백은 다음 축들 중 하나를 다룰 것이다(그러나 그럴 필요는 없다). * **커버리지**: 이 축에 대해 "요약에서 게시물의 중요한 정보를 얼마나 잘 커버합니까?"라는 질문에 답합니다. 요약은 게시물에 설명된 상황을 이해하는 데 중요한 게시물의 주요 정보를 언급하는 경우 커버리지가 좋습니다. 요약만 읽는 사람이 게시물의 상황에 대한 몇 가지 중요한 정보를 놓치는 경우 요약은 보장이 좋지 않다. 커버리지가 좋은 요약은 또한 (예를 들어, 조언을 요청하기 위한) 원래 게시물의 목적과 일치해야 한다. * **정확성**: 이 축의 경우 "요약에서 사실 정보가 게시물과 정확하게 일치합니까?"라는 질문에 답합니다. 요약은 기사에 없는 것, 사람을 혼동하지 않으며 일반적으로 오해의 소지가 없는 것을 말하지 않는 경우 정확합니다. 요약이 게시물에서 언급되지 않거나 게시물에서 어떤 것과 모순되는 것을 전혀 말하지 않는다면 정확하지 않다. * **일관성**: 이 축의 경우 "일관성은 요약 자체입니다."라는 질문에 답합니다. 요약은 스스로 읽을 때 이해하기 쉽고 영어 오류가 없는 경우 일관성이 있습니다. 요약이 말하고자 하는 바를 이해하기 어렵다면 요약은 일관성이 없다. 일반적으로 요약은 문법 오류가 없는 것보다 이해할 수 있는 것이 더 중요합니다. 추가 규칙: 피드백은 피드백이 어떤 카테고리(정확도, 커버리지, 일관성, 기타)에 속하는지 언급해서는 안 되며, "커버리지", "정확도", " 일관성", 또는 "기타"(위에서 정의한 바와 같이)의 정의에 대한 지식을 가정해서는 안 된다. 예: "이것은 커버리지 영역에서 누락되어 있음", 또는 "이 요약은 정확성의 범주에서 결여되어 있음, 왜냐하면..."을 기입해서는 안 된다. 피드백은 "커버리지", "정확성", 및 "일관성"의 정의를 읽어본 적이 없는 사람에게 이해될 수 있어야 한다. 그러나, 예를 들어 "이 요약은 텍스트의 중요한 부분을 다루지 않기 때문에" 또는 "이 요약은 명시한 대로 부정확합니다..." 또는 "이것은 논리정연한 요약이 아니기 때문에..."라고 말할 수 있다.
3. **피드백 유형 작업**: 피드백이 Accuracy 관련, Coherence 관련 또는 Coverage 관련 범주에 속하는 경우 관련 (단일) 범주의 해당 확인란을 확인 하 여 해당 항목으로 표시 합니다. 피드백이 이 세 가지 범주 중 하나와 관련이 없는 경우 "기타" 확인란을 선택합니다.
4. **추가 피드백 작업**: 언급하려는 요약의 중요한 단점에 대한 추가 피드백이 있는 경우 예를 사용하여 답변하고 그렇지 않은 경우 그렇지 않습니다.
5. **이상 요약 작업**: 이상 요약 작업: 보기에 이상적인 Reddit 게시물에 대한 짧은 요약을 작성합니다. _ 상세사항_: 이상적인 요약은 위에서 언급된 모든 기준, 즉 본질, 명확성, 정확성, 커버리지, 목적, 간결성, 일관성 및 스타일의 관점에서 이상적이어야 한다. 즉, 자신이 쓰는 이상적인 요약에 대한 명백한 비판을 찾을 수 없어야 한다. 이전 요약의 부품을 재사용해도 좋지만 해당 부품이 이상적인 요약의 부품이어야 하는 경우에만 가능합니다. 이상적인 요약은 최대 48 토큰 길이여야 합니다 (그렇지 않으면 주석을 제출할 수 없습니다). 토큰은 이상적인 요약을 취하고 특정 단어를 개별 조각으로 분할하여 생성됩니다(이것은 AI를 훈련시키는 데 필요합니다). 인터페이스는 이상적인 요약이 이미 사용 된 토큰 수를 보여 줍니다.

### 요약 품질 평가 지침

#### Task Overview

먼저 주의 깊게 읽어야 하는 서브레딧 카테고리, 제목 및 레딧 게시물이 제공됩니다. 그러면 6개의 요약을 비교하고 품질에 따라 순위를 매기는 작업이 수행됩니다.

**좋은 요약을 만드는 이유는 무엇입니까?* * 대략적으로 말하면 좋은 요약은 원본 텍스트의 본질이 있는 짧은 텍스트 조각입니다. 좋은 요약은 동일한 목적을 달성하려고 시도하고 원문과 동일한 정보를 전달한다. 우리는 당신이 이러한 다양한 차원의 요약을 고려하기를 바랍니다:

**에센스**: 요약이 게시물의 좋은 표현입니까? 그 요약은 게시물의 중요한 정보를 얼마나 잘 다루나요?

**명확성**: 요약 판독기가 친숙한가요? 아이디어를 명확하게 표현하나요?

**정확도**: 요약에 게시물과 동일한 정보가 포함되어 있습니까?

**목적**: 요약이 원래 게시물과 동일한 목적을 제공합니까?

**간결함**: 요약이 짧고 요점까지입니까?

**스타일**: 요약이 원본 게시물과 동일한 스타일로 작성되었습니까?

일반적으로 목록의 맨 위에 있는 치수에 더 높은 가중치를 부여합니다. 위의 차원 중 어떤 것도 단순한 예/아니오 문제이고 다른 차원을 거래하는 데 강력하고 빠른 규칙이 없기 때문에 평가는 복잡할 수 있다. 최선의 판단과 상식을 사용하여 이러한 절충안을 만들 수 있습니다. 하위 레딧, 제목 및 레딧 게시물이 발생한 일에 대해 약간의 모호성을 남기는 경우, 특정 방식으로 텍스트를 해석하는 것이 아니라 주석에 정확하게 반영하는 것이 중요하다. 항상 모든 하위 레딧, 제목 및 레딧 게시물을 보고 주어진 모든 정보를 사용하여 판단을 내린다(때로는 제목에는 게시물에 나타나지 않지만 그럼에도 불구하고 사용해야 하는 중요한 정보가 포함될 수 있다).

먼저 서브레딧 카테고리, 제목 및 게시물을 주의 깊게 읽으십시오. 서브레딧은 웹 사이트 레딧의 특정 주제에 대한 포럼입니다. 이 단계를 천천히 수행하고 처음에 이해하지 못했을 수 있는 부분을 다시 읽으십시오. 아래는 각 레딧 게시물에 대해 완료해야 하는 작업에 대한 자세한 설명입니다.

**비교 작업**: 6개의 요약이 제공되면 품질에 따라 순위를 매겨 어느 것이 더 나은지 나타냅니다. 랭킹 1은 최고 순위로 간주되고, 랭킹 6은 최저 순위로 간주된다. 최상의 품질을 갖는 요약은 가장 높은 순위, 즉 순위 1로 순위가 매겨져야 하고, 최악의 품질을 갖는 요약은 가장 낮은 순위, 즉 순위 6으로 순위가 매겨져야 한다. 좋은 요약을 만드는 것에 대한 상기 설명을 사용한다. 요약 간의 동조는 허용되지만 요약이 서로 정확한 사본이거나 하나의 요약이 다른 요약보다 우수하게 만드는 구별되는 기능이 없는 경우에만 허용된다. 그러나 한 요약을 다른 요약보다 더 잘 만드는 작은 세부 사항이 있으면 그 요약을 다른 요약보다 더 잘 순위를 매길 충분한 이유가 있다. 표준 경쟁 순위(예: 122456 순위)를 사용합니다. 표준 경쟁 순위에서는 균등하게 비교하는 항목이 동일한 순위 번호를 받은 후 순위 번호에 격차가 남는다. 이 갭에서 제외되는 순위 번호의 수는 동일하게 비교되는 항목의 수보다 1개 적다. 동등하게 각 항목의 순위 번호는 1에 그 위에 순위가 매겨진 항목의 수를 더한 것이다.

## Appendix J Prompts

### Summarization Prompts

우리는 표 8에 초기 요약, 피드백이 있는 정제 및 피드백이 없는 정제를 생성하는 데 사용되는 모든 프롬프트 템플릿을 보고한다.

### InstructRM Prompts

우리는 이 논문의 저자 중 한 명(당시 연구 프로젝트에 참여하지 않은 사람)에게 고품질 요약, 즉 개선 사항을 선택하는 목표를 달성할 5개의 프롬프트를 작성하도록 지시했다. 저자는 어떤 종류의 프롬프트가 작동할지에 대한 도메인 지식이나 사전 정보를 가지고 있지 않았다. 상기 저작자에게 제공되는 명령어는,

\begin{table}
\begin{tabular}{l l} \hline \hline
**Methods** & **Format** \\ \hline Initial Summary & Write an excellent summary of the given text. \\  & Title: \{title\} \\  & Text: \{text\} \\  & TL;DR: \\ \hline Refinement with Feedback & Write an excellent summary that incorporates the feedback on the given summary and is better than the given summary. \\  & Title: \{title\} \\  & Text: \{text\} \\  & Summary: \{summary\} \\  & Feedback on Summary: \{feedback\} \\  & Improved TL;DR: \\ \hline Refinement & without Feedback & Write an excellent summary that is better than the given summary. \\  & Title: \{title\} \\  & Text: \{text\} \\  & Summary: \{summary\} \\  & Improved TL;DR: \\ \hline \hline \end{tabular}
\end{table}
표 8: 요약에 사용되는 프롬프트 템플릿.

여기서 보게 되다. 우리는 표 9에 5개의 프롬프트 템플릿을 모두 보고한다.

\begin{tabular}{l l} \hline \hline
**InstructRM Prompts** & **Format** \\ \hline Prompt 1 & Here's a summary of a Reddit post, feedback on the summary, and a new summary. You will be asked to determine whether the new summary incorporates the feedback provided. \\ \end{tabular} A good summary is a short piece of text that has the essence of the original text. A good summary tries to accomplish the same purpose and conveys the same information as the original text.

포스트 타이틀 : {title}

아래에 요약된 게시물의 내용이 있습니다.

원문 게시물 : {text}

원본 요약 : {summary}

그런 다음 인간은 위의 요약에 대한 피드백을 제공했다.

Feedback: {feedback}

이러한 피드백을 바탕으로 새로운 요약을 작성하였다.

새로운 요약: {refinement}

이 새로운 요약은 제공된 피드백을 통합합니까? 예, 아니요로 대답하세요.

Answer:

프롬프트 2 & 포스트 제목 : {title}

원문 게시물 : {text}

원본 요약 : {summary}

Feedback: {feedback}

새로운 요약: {refinement}

질문: 새로운 요약에 제공된 피드백이 포함되어 있습니까? 예, 아니요로 대답하세요.

Answer:

프롬프트 3 & 사용자에게 레딧 게시물 제목, 내용, 해당 게시물의 원본 요약 및 해당 요약에 대한 피드백이 제공됩니다. 그런 다음 제공된 피드백과 관련하여 새로운 요약이 원본에 따라 개선되는지 여부를 결정하는 것이 목표입니다.

포스트 타이틀 : {title}

Post content: {text}

원본 요약 : {summary}

Feedback: {feedback}

새로운 요약: {refinement}

질문: 새로운 요약에 제공된 피드백이 포함되어 있습니까? 참 또는 거짓으로 대답하십시오.

Answer:

* 다음은 Reddit 게시물 요약, 요약에 대한 피드백 및 새 요약입니다. 새 요약에 제공된 피드백이 포함되어 있는지 여부를 확인하라는 메시지가 표시됩니다.

포스트 타이틀 : {title}

아래에 요약된 게시물의 내용이 있습니다.

Original Post : {text}

새 요약에 제공된 피드백이 포함되어 있는지 여부를 확인하라는 메시지가 표시됩니다. 여기 원본 요약이 있습니다.

원본 요약 : {summary}

새 요약에 제공된 피드백이 포함되어 있는지 여부를 확인하라는 메시지가 표시됩니다. 그런 다음 인간은 위의 요약에 대한 피드백을 제공했다.

Feedback: {feedback}

이러한 피드백을 바탕으로 새로운 요약을 작성하였다.

새로운 요약: {refinement}

이 새로운 요약은 제공된 피드백을 통합합니까? 예, 아니요로 대답하세요.

Answer:

### Finetuning Prompts

표 10에서는 요약에 대한 피니튜닝과 피드백 + 정제에 대한 피니튜닝에 사용하는 프롬프트를 보고한다. 요약에서 피니튜닝 완료는 다양한 출처에서 생성된 완성, 즉 _FeedMe_의 초기 요약, 방법으로 생성된 정제 또는 이상적인 인간 서면 요약을 가질 수 있음을 나타낸다. 피니튜닝 피드백 + 정제를 위해 먼저 피드백을 생성한 다음 정제를 생성한다.

\begin{table}
\begin{tabular}{p{142.3pt} p{142.3pt}} \hline \hline Prompt 5 & Here’s a summary of a Reddit post, feedback on the summary, and a new summary. You will be asked to determine whether the new summary incorporates the feedback provided. \\ \multicolumn{2}{p{142.3pt}}{The feedback was: Feedback: feedback} \\ \multicolumn{2}{p{142.3pt}}{Here’s the post that was summarized in the first place.} \\ \multicolumn{2}{p{142.3pt}}{Post title: \{title\}} \\ \multicolumn{2}{p{142.3pt}}{Original Post: \{text\}} \\ \multicolumn{2}{p{142.3pt}}{Remember, you will be asked to determine whether the new summary incorporates the feedback provided. Here’s the original summary.} \\ \multicolumn{2}{p{142.3pt}}{Original summary: \{summary\}} \\ \multicolumn{2}{p{142.3pt}}{Remember, you will be asked to determine whether the new summary incorporates the feedback provided. A human then provided feedback on the above summary.} \\ \multicolumn{2}{p{142.3pt}}{Here’s the feedback again.} \\ \multicolumn{2}{p{142.3pt}}{Feedback: \{feedback\}} \\ \multicolumn{2}{p{142.3pt}}{Based on this feedback, a new summary was written.} \\ \multicolumn{2}{p{142.3pt}}{New summary: \{refinement\}} \\ \multicolumn{2}{p{142.3pt}}{Does this new summary incorporate the feedback provided? Answer True or False.} \\ \multicolumn{2}{p{142.3pt}}{Answer:} \\ \hline \hline \end{tabular}
\end{table}
표 9: InstructRM Ensemble에 사용되는 프롬프트 템플릿.

### Reward Model Prompt

### Sample

\begin{table}
\begin{tabular}{c l c} \hline \hline
**Reward Model Type** & **Prompt** & **Completion** \\ \hline Binary RM & Title: \{title\} & \{” Yes”/” No”\} \\  & Text: \{post\} \\  & TL;DR: \{summary\_A/summary\_B\} \\  & Question: Is the above an excellent summary of the given text? An excellent summary is coherent, accurate, concise, and detailed. Answer with Yes or No. \\  & Answer: \\ \hline Comparison RM & Title: \{title\} & \{” A”/” B”\} \\  & Text: \{post\} \\  & Summary A: \{summary\_A\} \\  & Summary B: \{summary\_B\} \\  & Question: Which summary is the better one? An excellent summary is coherent, accurate, concise, and detailed. Answer with A or B. \\  & Answer: \\ \hline \hline \end{tabular}
\end{table}
표 11: 언어 모델 손실로 보상 모델을 트레이닝하기 위해 사용되는 프롬프트 템플릿. 분류 및 비교 프롬프트가 모두 표시됩니다.

\begin{table}
\begin{tabular}{c l c} \hline \hline Finetuning on & \{feedback\} & \{feedback\} \\ Feedback & Write an excellent summary that incorporates the feedback on the given summary and is better than & Improved TL;DR: \{refinement\} \\  & the given summary. & \#\#\# \\  & Title: \{title\} & \\ Text: \{post\} & Summary: \{summary\_A\} & \\  & Feedback on summary: & \\ \hline \hline \end{tabular}
\end{table}
표 10: 요약 및 피드백 + 세분화에 대한 피니튜닝에 사용되는 프롬프트 템플릿.
