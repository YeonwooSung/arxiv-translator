<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'just based on text features can be difficult. Thus, we propose learning a unified embedding model incorporating graph, transformer and term-based embeddings end to end for leveraging the best of all these approaches and addressing the dual problem of vocabulary gap and retrieving personalized products at scale.\n' +
      '\n' +
      'Another consideration beyond textual similarity in e-commerce search systems is inherent product quality independent of query (e.g average product review)[16]. However, prior work in embedding-based retrieval eschews explicit modeling of product quality, leaving this responsibility entirely to later stages. Oftentimes, even with additional context from specific users there can be more semantically relevant products in the catalog than what can be efficiently re-ranked by subsequent layers. Thus, to produce an optimal set of retrieval candidates, it is beneficial to consider quality as long as relevance is not sacrificed. To this end, we propose _ANN-Based Product Boosting_ where we augment existing embeddings with vectors tuned to balance quality and relevance when combined with inner product scoring. We employ black box optimization to identify globally optimal quality weights which directly maximize desired target metrics with realistic serving constraints.\n' +
      '\n' +
      'In this paper, we present the details of training and deploying our Unified Embedding Based Personalized Product Retrieval(UPPER) system for retrieving personalized products. We demonstrate the efficacy of such a system on both head and tail queries with both offline evaluations and live A/B testing. Our contributions can be summarized as follows:\n' +
      '\n' +
      'i) We present our novel two-tower model with a unified embedding based product encoder and joint user-query encoder deployed on an e-commerce website to handle diverse range of search queries.\n' +
      '\n' +
      'ii) We share our novel training strategies including mining hard negatives during training, our pre-training strategy for language models used for product text feature representation and other design choices such as loss function and approach to generating training data for training such a model.\n' +
      '\n' +
      'iii) Using ablation studies, we share our learnings in using diverse feature encoders and engineering tricks for encoding a variety of user, product and query features.\n' +
      '\n' +
      'iv) Our novel approach of _ANN-Based Product Boosting_ for re-ranking candidates based on listing quality scores.\n' +
      '\n' +
      '## 2. Related Work\n' +
      '\n' +
      'In recent years, deep learning techniques have been widely applied to information retrieval (IR) systems, specifically in neural augmented lexical retrieval. These techniques can be categorized into representation models (such as DSSM [13] and CLSM [26]) and query-document interaction models (such as MatchPyramid [23] and Match-SRNN [27]).\n' +
      '\n' +
      'Most e-commerce search systems employ the two-tower architecture with an ANN for embedding based retrieval (EBR) ([22], [11], [20], [29], [18]). Nigam, Song et al(22) leveraged shallow textual bag of word embeddings and categorical product feature embeddings. Recent work has sought to improve the representational capacity of EBR models ([2][4]). Personalized models incorporate user behavior and profile features to better tailor candidates for specific users [32][18]. Models enriched by graph-based embeddings [9][28] improve on purely content-based models by leveraging known relationships between distinct entities[19][34]. Thus, we use bipartite graph encodings as a part of our unified representation.\n' +
      '\n' +
      'Pre-trained transformer-based language models [17][35][14] improve upon shallow unordered term embeddings by enabling contextualized term representations [10] and deeper semantic understanding (though, increased modeling capacity comes at the cost of latency[5], and enriched language understanding may provide less benefit to e-commerce queries which are short and lack context.\n' +
      '\n' +
      '_Hard Negative Sampling_ is an area of active research focused on improving the training of dense retrieval models by incorporating hard negatives. [30] proposed an approach that uses hard negative samples to optimize the training of dense retrieval models. Similarly, [33] proposed an approximate nearest neighbor negative contrastive learning for dense text retrieval. These methods have shown significant improvements in retrieval performance. Facebook in their EBR model[11] introduced tricks for hard-negatives mining and benefits of combining them with random negatives.\n' +
      '\n' +
      'In summary, deep learning techniques have shown great potential in improving the accuracy and efficiency of IR systems. The above-mentioned works demonstrate the effectiveness of various deep learning techniques in addressing different challenges in IR, including modeling semantic relationships, personalization, and improving the training of retrieval models.\n' +
      '\n' +
      '## 3. Architecture\n' +
      '\n' +
      'Our proposed retrieval system, called Unified Embedding Based Personalized Product Retrieval(UPPER), is an integral part of Etsy\'s diverse multi-channel retrieval system. It is designed to select semantically relevant products while considering user preferences. Figure 2 and 3 provide an overview of our system\'s workflow. When a user search query is received on Etsy, our system retrieves the query, user profile, and history features, which are then passed to our Neural retrieval engine for query inferencing and ANN lookup. Our system consists of three main components: i) **UPPER** model described in section 4 highlights our jointly trained query-user and product towers. ii) **ANN Product Boosting** introduced in section 5 aims to select high-quality products using black-box optimization techniques on ANN. iii) **Online Servin\n' +
      '\n' +
      'Figure 2. Base architecture of UPPER\n' +
      '\n' +
      'section 6 describes our approach for addressing model deployment at industry scale.\n' +
      '\n' +
      '## 4. Unified Embedding Model\n' +
      '\n' +
      'Our proposed two-tower UPPER model consists of a product encoder \\(\\mathcal{P}\\) and a joint query-user encoder \\(\\mathcal{Q}\\). For a given query \\(q\\), user \\(u\\) we compute the personalized semantic score \\(y\\) for a given product \\(p\\) as follows:\n' +
      '\n' +
      '\\[y=cos(\\mathcal{P}(p),\\mathcal{Q}(q,u)) \\tag{1}\\]\n' +
      '\n' +
      'where both \\(\\mathcal{P}(p)\\) and \\(\\mathcal{Q}(q,u)\\) are d-dimensional vectors. Our choice to use cosine as the similarity function is based on efficiency since that keep the two-towers independent and allows offline indexing and efficient serving via ANN. Representations from \\(\\mathcal{P}\\) and \\(\\mathcal{Q}\\) are a result of pooling of constituent encoders used for representing relevant features. Our goal is to maximize Recall@K, a metric used widely in literature since it effectively measures retrieval performance. In this section, we present the architecture details of our proposed UPPER model, along with the key design choices in feature engineering. We begin by describing the design of our product encoder, followed by an explanation of the shared encoders between both towers. Subsequently, we provide a comprehensive overview of the joint query-user encoder.\n' +
      '\n' +
      'Furthermore, we delve into the critical approaches that were instrumental in the successful offline training of \\(\\mathcal{P}\\) and \\(\\mathcal{Q}\\). These include our novel loss function, the pre-training strategy employed for language models, the technique for mining positives from search logs to create labeled data, and our method for mining dynamic hard negatives.\n' +
      '\n' +
      '### Product Encoder\n' +
      '\n' +
      'In Figure 2 (shown on the right side), we present the product encoder \\(\\mathcal{P}\\), which is designed as a unified embedding model to capture various complementary aspects of a product. The product representations obtained from this encoder are pooled through concatenation and further projected using successive layers of feedforward networks and Batch Normalization to obtain the final product representation. In this section, we focus on describing the encoders dedicated to encoding crucial product features, as well as the relevant feature engineering techniques employed.\n' +
      '\n' +
      '#### 4.1.1. Transformer based representations\n' +
      '\n' +
      'Contextual representations from transformers can be helpful in biasing product representations towards important tokens from text features like title and tags. However, our findings indicate that fine-tuning two widely used text encoders, distilBERT (Zhu et al., 2017) and the encoder part of T5 (Zhu et al., 2017) did not yield any offline improvements. This could be attributed to our asymmetric architecture, where the query tower lacks an identical transformer due to latency considerations. Following that, we employed a pre-training approach influenced by docT5query (Chen et al., 2018). In this strategy, we trained a T5-small model to generate the most historically purchased query based on product text and utilized its encoder. Our hypothesis is that this approach would enhance the model\'s comprehension of crucial aspects such as product title/tag.\n' +
      '\n' +
      '#### 4.1.2. Bipartite graph encoder\n' +
      '\n' +
      'We employ bipartite graph-based embeddings (Han et al., 2017) to effectively address the semantic gap between query terms and product content that is relevant based on historical search. To construct a product-query graph, we utilize over a year of search logs containing positive interactions between queries and products. During training, we sample queries for a given product from the graph and encode them using parameters shared with the query tower. We use average pooling over query embeddings to get a one-hop graph embedding. To ensure generalization, we exclude the target query from the set of sampled neighbors. We also found that sharing parameters led to over-fitting, which we addressed with several interventions and found that simply freezing shared parameter updates due to graph encoder led to best performance.\n' +
      '\n' +
      '### Shared Encoders\n' +
      '\n' +
      '#### 4.2.1. Token and ID Embeddings\n' +
      '\n' +
      'Queries and products each contain sets of token fields \\(F_{q}\\) and \\(F_{p}\\). Each token field \\(f\\) consists of a bag of unordered tokens derived from raw inputs. As in (Zhu et al., 2017) we extract unigrams, bigrams, and character trigrams from textual fields. Additionally, we extract product category at each level of hierarchy (e.g #category_furniture, #category_furniture.bedroom, etc). For product attributes like color or material, we extract tokens for each key/value pair (e.g #attr_color_red), and also extract textual n-grams for attributes where sellers are able to input arbitrary values.\n' +
      '\n' +
      'Our product and query encoders both employ average embedding layers to encode a set of fields \\(F\\) containing \\(N_{t}(F)\\) total tokens via a simple average of token embeddings:\n' +
      '\n' +
      '\\[\\text{avg}(F)=\\frac{1}{N_{t}(F)}\\sum_{f\\in F}\\sum_{t\\in f}E[t] \\tag{2}\\]\n' +
      '\n' +
      'Initially, we employed an average embedding approach for the token input layer of our product and query-user towers, represented as token_rep\\((p)=\\text{avg}(F_{p})\\). However, we observed that incorporating certain new fields, especially noisy ones like keywords extracted from the product description, could lead to a degradation in model performance. To address this, we partitioned the fields into distinct groups, denoted as \\(F^{1}_{p},F^{2}_{p},...\\) (e.g., one group exclusively containing description tokens). We then concatenated the average embeddings of each group, resulting in the updated token representation token_rep\\((p)=\\text{concat}([\\text{avg}(F^{1}_{p});\\text{avg}(F^{2}_{p}),...])\\).\n' +
      '\n' +
      '#### 4.2.2. Location Encoder\n' +
      '\n' +
      'As in (Zhu et al., 2017), we incorporate both user location and listing location features in the query and listing towers respectively. We embed language, country, and zip code, and additionally account for location at multiple levels of granularity using both latitude/longitude and zip codes. We perform K-means clustering on customer latitude/longitude coordinates and extract location bucket IDs for users and listings with different values of k (50,100,500). We also capture coarse geographic regions by extracting prefixes of zip codes (e.g "54321" yields "5", "54", etc), as prefixes correspond to contiguous geographic regions. We found that zip codes and location buckets are non-redundant in terms of model performance, possibly because they capture somewhat different notions of proximity (for instance, k-means clusters can have large variability in geographic span due to dependence on population density, while zip codes tend to be more uniform in size). Using all location features together, we observed a relative gain of 8% in purchase recall for domestic users.\n' +
      '\n' +
      '### Joint Query-User Encoder\n' +
      '\n' +
      'We illustrate the architecture of our joint tower on the left side in Figure 2. To represent the query text we use the lightweight token encoder as described in section 4.2.1. At the token level these parameters are shared between product title/tags and the graph encoder. In addition, we use location encoders described earlier to represent user location. Further, we incorporate user engagement history from a variety of event types similar to (Wang et al., 2017) and (Kang et al., 2017). In particular, we leverage a user\'s recent searches, recent shop clicks, terms from recently clicked items, and tags of all-time purchased items. To minimize latency on the query-user tower we use lightweight token embedding as described in 4.2.1 and attention mechanisms to incorporate these in the final joint-query. As pointed out in in (Bang et al., 2017), not all historical user actions are equally relevant to the current search session, so it can be beneficial to weight events in relation to the current query and to control the total contribution of user events in personalization. Similar to (Bang et al., 2018), our query-user tower uses lightweight transformers applied at the event level to produce historical embeddings from the query and user actions. In particular, we apply a transformer to the sequence \\([q,u_{s}^{1},...,u_{s}^{n},u_{t}^{c}]\\), where \\(q\\) is search query, \\(u_{s}\\) is a sequence of a user\'s recent search queries and \\(u^{c}\\) is a user\'s recently clicked shops. Each vector in \\(u_{s}\\) is a simple average of a query\'s token embeddings, and each vector \\(u_{c}\\) is the shop ID embedding. We find that a 1-layer transformer contributes little to latency, and improves offline recall by 2%.\n' +
      '\n' +
      '### Negative Mining\n' +
      '\n' +
      'Our negative mining strategy employs three difference sources of negative products:\n' +
      '\n' +
      '* _Hard in-batch negatives_: In-batch negatives are positive products from other queries in a batch that are sampled as negative for a given query. Since product and query representations are independent in a two-tower model they don\'t need to be recomputed as in a cross encoder. Further, we explored allowing the model to focus on the _hardest_ examples by sampling most similar in-batch products for a given query and we found that it works better than uniform sampling.\n' +
      '* _Uniform negatives_: In addition, we also sample products uniformly from our entire corpus of products as negatives. Uniform negatives provide complementary value to hard negatives because they help _bottom ranking_ performance while hard negatives help top ranking performance (Wang et al., 2017). We have also observed that size of corpus from which negatives are sampled affects performance and since in-batch negatives only have positive products, diversity in negatives is limited and model can overfit.\n' +
      '* _Dynamic Hard Negatives from large batch_: Dynamic negatives, i.e., negatives with respect to model parameters during training, have been shown to improve dense retrieval performance(Wang et al., 2017) over static negatives. Our memory-efficient approach to sampling dynamic negatives has two steps: First, we randomly sample large batch of negative samples from the product corpus and use current model parameters to select top K most similar products, but do not update model parameters yet. After the initial selection the _dynamic hard negatives_ are combined with other sources of negative examples, which are used to update model parameters. Our loss from negative examples is a weighted sum of individual from each mining strategy and we linearly update the weights during training. We warmup training with only uniform negatives and linearly decay loss weight from uniform negatives and increase weight for hard negatives as we found that it is ideal for convergence and performance.\n' +
      '\n' +
      '### Loss Function\n' +
      '\n' +
      'Threshold-based pruning is a widely adopted approach in the retrieval layer, employed to eliminate irrelevant candidates. Though pairwise loss functions are widely used they aren\'t suited for threshold based pruning(Zhou et al., 2017). Our approach incorporates a hinge loss framework to establish a threshold during model training phase itself. Since our training data consists of different kinds of interactions types and each interaction type represents a different degree of relevance, we employ a multi part hinge loss where each part is associated with a different threshold. Given output score as \\(y\\) and true label as \\(\\hat{y}\\), our loss function can be expressed as:\n' +
      '\n' +
      '\\[L(y,\\hat{y})=\\left(\\sum_{i\\in I}\\mathbb{I}[\\hat{y}==i]\\cdot(-\\min(0,y-\\epsilon _{i}))\\right)+\\mathbb{I}-\\cdot max(0,y-\\epsilon)\\]\n' +
      '\n' +
      'where \\(I\\) is set of all positive interactions and \\(\\epsilon_{i}\\) is the threshold corresponding to it and \\(\\mathbb{I}-\\), \\(\\epsilon\\) are indicator variable and threshold for negative samples respectively.\n' +
      '\n' +
      '## 5. Ann-based Product Boosting\n' +
      '\n' +
      'When forming a candidate set of products for a query, it is beneficial to retrieve candidates that are both semantically relevant to the query and appealing to customers. Within Etsy, our inverted index candidate retrieval system associates products with a query-independent "quality score" \\(Q(p)\\), and employs multiplicative boosting to compute a candidate score as the product of it\'s quality score and query-listing relevance score \\(R(q,p)\\), or \\(S(q,p)=R(q,p)Q(p)\\). This quality score can account for properties such as high product rating, product freshness, and shop conversion rate that are known to increase engagement independently of query-listing relevance.\n' +
      '\n' +
      'We implement additive boosting within ANN-based semantic retrieval by enriching our model-derived product vectors with additional numerical features and add corresponding feature weights to query vectors. Given original product embedding \\(p\\) and query embedding \\(q\\), we create hydrated vectors \\(p^{\\prime}=\\text{concat}([p;f(p)])\\) and \\(q^{\\prime}=\\text{concat}([q;w])\\) where \\(f(p)\\) is a feature vector of numerical features, and \\(w\\) is a constant vector of the same dimension. We then model candidate score as \\(S(q^{\\prime},p^{\\prime})=\\text{dot}(p^{\\prime},q^{\\prime})=\\text{dot}(p,q)+ \\text{dot}(f(p),w)\\). For serving, we simply index hydrated product vectors rather than original product embeddings, and query our index with a hydrated query vector.\n' +
      '\n' +
      'In principal, we could learn query-side feature weights directly as part of end-to-end model training. However, compared to textual features like query or title text, static quality features like shop popularity are sensitive to negative sampling approaches and can easily over-fit on our proxy metrics without careful tuning and also do not optimize for recall directly. Instead we use black box optimization to identify query-side feature weights which directly maximize arbitrary target metrics. In particular, we use skopt(Bang et al., 2017) to perform bayesian optimization to learn query weights which optimize purchase recall on items purchased after our model training\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:5]\n' +
      '\n' +
      '### Evaluation Methodology\n' +
      '\n' +
      '* _Offline metric_ We use Recall@K, a metric used widely in literature since it effectively measures retrieval performance. Essentially, given a query \\(q\\), set of target relevant products \\(\\mathcal{T}\\) and set of top K products \\(\\mathcal{R}\\): \\[Recall@K=\\frac{\\sum_{t\\in\\mathcal{T}}\\mathbb{I}(t\\in\\mathcal{R})}{|\\mathcal{T}|}\\].\n' +
      '\n' +
      'We have also observed that offline Recall@K metrics positively correlates with online improvement in site-wide and search conversion rates We use \\(K\\in\\{100,1000\\}\\) for our offline evaluation.\n' +
      '* _Training & Evaluation Data_ We create our training data by mining positive interactions such as cartadds and purchases from our search logs using past month of data from across all our platforms. We used TensorFlow2.6 to train our model. We use MirroredStrategy to train model on 4 A100 GPUs and it takes about 72 hours for the model to converge. During evaluation, our objective is to assess the model\'s capacity to recall purchases in the days following the latest days training data. Consequently, we construct our evaluation dataset by employing sampled search logs that incorporate user purchases occurring over the next day after the training data. Overall, we report average over the next 5 days of evaluation data with each days data consisting of 11k unique queries having purchases.\n' +
      '* _Offline Baseline Model_ Our baseline embedding-based retrieval model referred as _Base_ is a shallow, symmetric two-tower model. This model incorporates term embeddings from raw query text and a product\'s title and tags. We use a multi-part hinge loss as described in section 4 and select negatives randomly from the entire catalog of listings, as well as from in-batch positives, as in (Kumar et al., 2019). In addition we also compare our model to highly tuned in-house classical inverted index and random walk based retrievers and demonstrate our effectiveness across query segments.\n' +
      '\n' +
      '### Offline Experiment Results\n' +
      '\n' +
      'In Table 1, we compare UPPER to several strong baselines and with different variants of our own model that show efficacy of individual components. We can see that each of our UPPER outperforms random walk based retrievers and inverted index retrievers on aggregate across query segments while our best non-personalized models are better at tail despite being worse at head queries. Adding personalization to query tower help performance on head queries significantly making it competitive with other methods.**UPPER rand** refers to our model only uses random and in-batch negatives in training. We can see that purchase recall@100 drops by 7% without hard negatives. Further, in table 2 we show ablation showing impact of using three negative mining strategies and show that combining the three hard negative mining strategies with dynamic weighting of loss. **UPPER boost** refers to model variant that incorporates our proposed ANN-based product boosting. From table 1 we can see that boosting leads to overall gains in recall@100 and as hypothesized, it leads to even more significant improvement on head queries than tail, which has more potentially relevant listings.\n' +
      '\n' +
      'The **Large vocab** variant model in table 1 is an improvement over **Base** in terms of vocab size and more hidden layers. We share an ablation study in Table 3 which shows the importance of each of our individual encoders when added to **Large vocab** that particularly highlights the significant improvement in performance when adding graph embeddings, as well as the other transformer and term based representations.\n' +
      '\n' +
      '#### 7.2.1. Impact of location embeddings\n' +
      '\n' +
      'Our location-aware model tends to produce fewer candidates that are very distant from the user. We observed that the 98th percentile distance to user for candidates in the location enabled model is 2600 miles, compared to 4200 miles in the baseline. We observed that for domestic users, our location-based model retrieved significantly fewer international listings. However, the domestic purchase recall limited to international listings was barely affected, suggesting that our model can distinguish exactly which international listings are likely to be relevant and appealing.\n' +
      '\n' +
      '#### 7.2.2. Qualitative Analysis\n' +
      '\n' +
      'In figure 1 we observe the impact of the degree of personalization based on location features and recent searches. For a query "bridge photos", we see that it increase gradually based on user/location features. For a user with location features in NYC/SF, they see more localized results compared to a user with no history. Furthermore, in 1 c, we see that the results are biased even more towards NYC (i.e Mario Cuomo bridge) when the user has recently searched for "NYC" in the session.\n' +
      '\n' +
      '### Online Evaluation\n' +
      '\n' +
      'We evaluated the effectiveness of our system by deploying it online and conducting A/B tests on live Etsy traffic. Our evaluation focused on two important metrics: the relative change in site-wide conversion rate (CVR) and the organic search purchase rate (OSPR). These metrics provide valuable insights into the overall search experience for customers on Etsy. On aggregate, our UPPER system improved CVR by 2.63% and OSPR by 5.58%. Notably, the personalized UPPER variants had a greater impact on the signed-in and habitual buyer segments, as expected.\n' +
      '\n' +
      '## 8. Conclusion\n' +
      '\n' +
      'In conclusion, our paper introduces a unified embedding-based personalized product retrieval system, called UPPER, and provides comprehensive details on its training and deployment. We have successfully demonstrated the system\'s efficacy through both offline evaluations and live A/B testing, both on head and tail queries.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* (1)\n' +
      '* (2) 2023. Scikit-Optimize: Sequential model-based optimization in Python. [https://scikit-optimize.github.io](https://scikit-optimize.github.io) Accessed: 2023-05-23.\n' +
      '* (3) Qingyao Ai, Daniel N Hill, SVN Vishwanathan, and W Bruce Croft. 2019. A zero attention model for personalized product search. In _Proceedings of the 28th ACM International Conference on Information and Knowledge Management_. 379-388.\n' +
      '* (4) F. Andreik, A.-M Kermarrec, and Nicolas Le Scournene. 2016. _Cache locality is not enough: High-performance nearest neighbor search with product quantization fast_. 288-299.\n' +
      '* (5) Keping Bil, Qingyao Ai, and W Bruce Croft. 2020. A transformer-based embedding model for personalized product search. In _Proceedings of the 3rd International ACM SIGIR Conference on Research and Development in Information Retrieval_. 1521-1524.\n' +
      '* (6) Wei-Cheng Chang, Daniel Jiang, Hsiang-Fu Yu, Choon Hui Teo, Jong Zhang, Kai Zhong, Kedarnath Kolluri, Qie Hu, Nikhil Shandiya, Vyacheslav leyergalov, et al. 2021. Extreme multi-label learning for semantic matching in product search. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_. 2643-2651.\n' +
      '* (7) David R. Cheriton. 2019. From doc2query to docTTTTquery.\n' +
      '* (8) Luyu Gao, Zhuyuan Dai, Tongfei Chen, Zhen Fan, Benjamin Van Durme, and Janie Callan. 2021. Complement lexical retrieval model with semantic residualembeddings. In Advances in Information Retrieval, pp. 1460-1460. Cited by: SS2.\n' +
      '* D. Golovin, B. Solnik, S. Moitra, G. Kochanski, J. Karoro, and D. Sculley (2017)Google vizer: a service for black-box optimization. In Proceedings of the 23rd ACM SIGKDD International conference on knowledge discovery and data mining, pp. 1487-1495. Cited by: SS2.\n' +
      '* W. L. Hamilton, R. Ying, and J. Leskovec (2017)Inductive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, Cited by: SS2.\n' +
      '* S. Hofstatter, M. Zhihagher, and A. Hanbury (2020)Interpretable & time-budget-constrained contextualization for re-ranking. arXiv preprint arXiv:2002.01842. Cited by: SS2.\n' +
      '* J. Huang, A. Sharma, S. Sun, L. Xia, D. Zhang, P. Pronin, J. Padmanabhan, G. Ottaviano, and L. Yang (2020)Embedding-based retrieval in facebook search. In Proceedings of the 22th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2553-2561. Cited by: SS2.\n' +
      '* J. Huang, A. Sharma, S. Sun, L. Xia, D. Zhang, P. Pronin, J. Padmanabhan, G. Ottaviano, and L. Yang (2020)Embedding-based retrieval in facebook search. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2553-2561. Cited by: SS2.\n' +
      '* P. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck (2013)Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM International Conference on Information & Knowledge Management, pp. 3233-3238. Cited by: SS2.\n' +
      '* Z. Jiang, A. El-Jaroudi, W. Hartmann, D. Karakos, and L. Zhao (2020)Cross-lingual information retrieval with BERT. In Proceedings of the workshop on Cross-Language Search and Summarization of Text and Speech (CLSSTS2020), pp. 149-157. Cited by: SS2.\n' +
      '* J. Johnson, M. Douze, and H. Jegou (2017)Billion-scale similarity search with GPUs. IEEE Transactions on Big Data 7, pp. 535-547. Cited by: SS2.\n' +
      '* S. K. Kamaker Santu, P. Sondhi, and C. Zhai (2017)On application of learning to rank for e-commerce search. In Proceedings of the 40th international ACM SIGIR conference on research and development in information retrieval, pp. 475-484. Cited by: SS2.\n' +
      '* J. Devlin M. C. Kenton and L. K. Toutanova (2019)Bert: pre-training of deep bidirectional transformers for language understanding. In Proceedings of a near-ILT, Vol. 1, pp. 2. Cited by: SS2.\n' +
      '* S. Li, Y. Jin, G. Lin, K. Yang, X. Zeng, X. Wu, and Q. Ma (2021)Embedding-based product retrieval in lnaoo search. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 3467-3471. Cited by: SS2.\n' +
      '* H. Lu, Y. Hu, T. Zhao, T. Wu, Y. Song, and B. Yin (2021)Graph-based multilingual product retrieval in e-commerce search. In NAACL, pp. 1177-1180. Cited by: SS2.\n' +
      '* A. Magnani, F. Liu, S. Chaidaron, S. Yadav, P. Reddy Suram, A. Buttyunthussery, S. Chen, M. Xie, A. Kashl, T. Lee, and C. Liao (2022)Semantic retrieval at wlamart. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 3495-3503. Cited by: SS2.\n' +
      '* Y. A. Malkov and D. A. Yashunin (2020)Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Trans. Pattern Anal. Mach. Intell.42 (4), pp. 824-836. External Links: Document, ISSN 1039-7498, Link Cited by: SS2.\n' +
      '* P. Nagan, Y. Zhang, S. Yao, Y. Mohan, V. Lakshman, W. Allen Ding, A. Shingavi, C. Hui Teo, H. Gu, and B. Yin (2019)Semantic product search. In Proceedings of the 28th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 191-197. Cited by: SS2.\n' +
      '* L. Pang, Y. Lan, J. Guo, J. Xu, S. Wan, and X. Cheng (2016)Text matching as image recognition. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, pp. 2793-2799. Cited by: SS2.\n' +
      '* C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020)Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res.21 (Article), pp. 6792-6802. Cited by: SS2.\n' +
      '* V. Sanh, L. Debut, J. Chaumond, and T. Wolf (2019)DistilBERT: a distilled version of BERT: smaller, faster, cheaper and lighter. ArXiv abs/1910.011018. Cited by: SS2.\n' +
      '* Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil (2014)A latent semantic model with convolutional-pooling structure for information retrieval. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pp. 101-110. Cited by: SS2.\n' +
      '* S. Wan, Y. Lan, J. Xu, J. Guo, L. Pang, and X. Cheng (2016)Match-sktnn: modeling the recursive matching structure with spatial rnn. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, pp. 2922-2928. Cited by: SS2.\n' +
      '* X. Wang, X. He, M. Wang, F. Feng, and T. Chua (2019)Neural graph collaborative filtering. In Proceedings of the 24th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 165-174. Cited by: SS2.\n' +
      '* Y. Xie, T. Na, X. Xiao, S. Manchanda, Y. Rao, Z. Xu, G. Shu, E. Vaiteg, J. Tennett, and H. Wang (2022)Embedding-imagen discovery search model at infaxax. arXiv. External Links: 2209.05555 Cited by: SS2.\n' +
      '* J. Zhan, J. Mao, Y. Liu, J. Guo, M. Zhang, and S. Ma (2021)Optimizing dense retrieval model training with hard negatives. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 1503-1512. Cited by: SS2.\n' +
      '* H. Zhang, S. Wang, K. Zhang, Z. Tang, Y. Jiang, Y. Xiao, W. Yan, and W. Yang (2020)Towards personalized and semantic retrieval: an end-to-end solution for e-commerce search via embedding learning. In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 2047-2046. Cited by: SS2.\n' +
      '* W. Zhang and K. Stratos (2021)Understanding hard negatives in noise contrastive estimation. In North American Chapter of the Association for Computational Linguistics, Cited by: SS2.\n' +
      '* Y. Zhang, D. Wang, and Y. Zhang (2019)Neural lr meets graph embedding: a ranking model for product search. In The World Wide Web Conference, pp. 2390-2400. Cited by: SS2.\n' +
      '* L. Zhang, L. Wayne, S. Ya, and Z. Jun (2021)A robustly optimized BERT pre-training approach with post-training. In Proceedings of the 20th Chinese National Conference on Computational Linguistics, pp. 1218-1227. Cited by: SS2.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>