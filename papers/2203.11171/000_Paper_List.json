{
    "2203.11171": {
        "paper_id": "2203.11171",
        "abs_url": "https://arxiv.org/abs/2203.11171",
        "pdf_url": "https://arxiv.org/pdf/2203.11171.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2203.11171_Self-Consistency_Improves_Chain_of_Thought_Reasoning_in_Language_Models.pdf",
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Xuezhi Wang",
            "Jason Wei",
            "Dale Schuurmans",
            "Quoc Le",
            "Ed Chi",
            "Sharan Narang",
            "Aakanksha Chowdhery",
            "Denny Zhou"
        ],
        "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
        "comments": "Published at ICLR 2023. V2: added PaLM results; V3: added UL2 results; V4: camera ready version at ICLR 2023",
        "official_code_urls": [],
        "pwc_page_url": "https://paperswithcode.com/paper/self-consistency-improves-chain-of-thought",
        "bibtex": "@misc{wang2023selfconsistency,\n      title={Self-Consistency Improves Chain of Thought Reasoning in Language Models}, \n      author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},\n      year={2023},\n      eprint={2203.11171},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
    }
}