[MISSING_PAGE_EMPTY:1]

이러한 모델을 훈련하는 데 필요한 것은 상당하므로 자원 할당과 모델 설계 모두에 문제가 발생한다. 예를 들어, 상이한 아키텍처들 또는 전략들을 탐색하는 비용은 어마어마하게 된다[329]. 더욱이, 그들의 큰 크기는 에지 디바이스들과 같은 자원-제한된 환경들에 적합하지 않게 하고, 따라서 그들의 적용 범위를 좁힌다[7]. 이러한 계산 부담은 또한 LLM의 개발을 풍부한 자원을 가진 대기업으로 제한한다[24; 196; 210]. 데이터 수집 파이프라인 및 교육 방법론과 같은 많은 필수 세부 사항은 독점적으로 남아 있어 학문적 연구를 방해하고 소규모 회사에 문제를 제기한다. 또한 이러한 모델을 훈련하는 것이 환경에 미치는 영향을 간과해서는 안 되며, 이는 탄소 배출과 윤리적 고려 사항에 대한 우려를 불러일으킨다[268; 270; 285]. 결과적으로 LLM의 _효율성_ 개선에 대한 강조가 증가하고 있다.

더 효율적인 LLM에 대한 이러한 긴급한 필요에 의해 이 조사는 주제에 대한 포괄적이고 최신 이해를 제공하는 것을 목표로 한다. 본 논문의 맥락에서 "효율성"은 모델 성능을 손상시키지 않으면서 계산 및 메모리 자원을 최적화하는 것으로 정의된다. 전체론적 접근법을 채택하여 LLM의 종단 간 개발에 중요한 효율성의 여러 차원을 탐구한다. 이러한 차원은 데이터 활용, 아키텍처 설계, 학습 및 조정 전략, 추론 기술을 포함 합니다. - 본질적으로 알고리즘 및 소프트웨어 관점에서 모델 개발의 전체 파이프라인을 포괄 합니다. 2 데이터 [316], 학습 [241; 333; 342], 튜닝 [323] 또는 추론 [295; 339]와 같은 LLM 효율성의 특정 측면에 중점을 둔 기존 조사가 있지만 포괄적인 보기를 제공 하지 않는 경우가 많습니다. [267]과 같은 다른 작업은 자연어 처리(NLP)에 대한 다양한 효율성 측면에 대한 귀중한 통찰력을 제공했지만 LLM 분야의 빠르게 진화하는 특성은 업데이트되고 포괄적인 검토를 요구한다. 대조적으로, 우리의 논문은 효율적인 LLM 개발에 기여하는 주요 방법론과 기술에 대한 보다 철저하고 현재의 개요를 제시하는 것을 목표로 한다.

각주 2: 여기서 주의할 점은 하드웨어 측면의 발전과 성과는 이 조사에서 생략되었다는 것이다.

이 조사의 나머지 부분은 알고리즘 관점에서 LLM 효율성의 여러 요인에 대한 포괄적인 이해를 제공하기 위해 다음과 같이 구성된다.

\(\bullet\) 섹션 2_Background_에서는 LLM의 핵심 개념을 소개하고 효율성을 평가하는 데 관련된 평가 메트릭을 간략하게 설명합니다.

\(\bullet\) 섹션 3_예산 효율성_에서는 주어진 자원 제약 내에서 LLM의 성능을 최적화하는 데 있어 스케일링 법칙과 같은 예측 접근법의 역할을 조사합니다.

\(\bullet\) 섹션 4_Data Efficiency_는 데이터 이용률을 최적화하기 위한 기술에 초점을 맞추어, 성능을 손상시키지 않으면서 자원 소비를 감소시킨다.

도 1: 다양한 스케일(80억, 620억, 540억)에 걸쳐 PaLM[54]의 능력 및 성능 트리[4]. 각 원 노드는 특정 능력을 나타내며, 그 크기는 해당 성능 수준을 나타내며, 원이 클수록 능력이 커진다. 모델 척도가 증가함에 따라 성능은 기존 작업 전반에 걸쳐 향상될 뿐만 아니라 새로운 능력을 드러낸다.

\(\bullet\) 섹션 5_아키텍처 효율성_은 혁신적인 아키텍처 설계를 검토하며, 아키텍처가 효율성에 어떻게 영향을 미치는지에 대한 상세한 검토를 제공한다.

\(\bullet\) 섹션 6_훈련 및 조정 효율성_에서는 LLM을 처음부터 효율적으로 훈련하고 특정 다운스트림 작업에 대해 사전 훈련된 모델을 미세 조정하기 위한 전략에 대해 설명합니다.

\(\bullet\) 섹션 7_Inference Efficiency_는 추론 속도를 가속화하고 메모리 풋프린트를 감소시키기 위해 설계된 모델 압축 기술의 영역을 탐구한다.

\(\bullet\) 섹션 8_Conclusion_은 이 조사의 주요 결과를 요약하고 효율적인 LLM의 발전을 위한 광범위한 함의를 논의한다.

LLM 효율성의 이러한 다양한 차원에 대한 개략적인 개요가 그림 2에 나와 있다.

## 2. Background

이 섹션에서는 LLM의 효율성을 평가하는 데 사용되는 주요 메트릭과 함께 LLM의 기초를 형성하는 핵심 개념에 대한 개요를 제시한다.

### LLMs의 핵심 개념

NLP 분야의 초석인 언어 모델링은 단어 시퀀스의 생성 가능성을 모델링하고 후속 토큰 또는 누락 토큰의 확률을 예측하는 것을 목표로 한다. 이 지역은 수십 년에 걸쳐 상당히 발전해 왔다. 초기

도 2. LLM 효율의 다면 차원들에 대한 개략적인 개요. 이 다이어그램은 데이터 활용, 아키텍처 설계, 훈련 및 튜닝 전략, 추론 기술을 포함하여 이 조사에서 다루는 주요 영역을 보여줌으로써 LLM 효율성에 기여하는 요인에 대한 총체적인 관점을 제공한다.

통계 언어 모델들[16; 30; 116; 222; 317]에 뿌리를 두고, 포커스는 사전 트레이닝된 신경 언어 모델들[128; 153; 185; 202; 210; 158]로 점진적으로 이동하였고, 더 최근에는 LMM(Large Language Models)들[28; 111; 302; 329]로 이동하였다. LLM에 대한 표준화된 정의는 없지만 일반적으로 광범위한 매개변수 크기와 놀라운 학습 능력으로 구별된다. 이 절에서는 10억 개 이상의 매개 변수를 가진 언어 모델을 중심으로 [329]에 요약된 기준을 채택하고 핵심 개념을 자세히 논의한다.

**건축 기초들.** LLM은 일반적으로 BERT [128]에 의해 예시되는 인코더-디코더 모델들 [146; 148; 171; 202; 212; 228] 및 디코더 전용 모델들 [13; 263; 264; 209; 210; 201; 230; 265; 287; 266; 208; 267; 268; 269; 270; 265; 288; 263; 264; 265; 287; 266; 288; 267; 269; 290; 211]과 같은 GPT 시리즈 [24; 196; 209; 210]로 분류될 수 있다. BERT는 마스킹된 언어 모델링을 사용하여 훈련되어 마스킹되거나 누락된 토큰을 예측하여 문맥 이해에 탁월할 수 있다. 반면에 GPT 모델은 시퀀스에서 후속 토큰을 예측하여 강력한 생성 능력을 갖추는 자기회귀 모델링을 사용하여 훈련된다. 이러한 차이점에도 불구하고, 두 유형의 모델은 공통적으로 트랜스포머[269] 아키텍처에 의존하는데, 이는 특히 자기 주의 메커니즘에서 주목할 만하다. 자체 주의에서 각 토큰은 _키, 값_ 및 _쿼리_ 로 표시 됩니다. _query_는 특정 토큰을 이해 하는 데 있어 _keys_ 로 표시 되는 다른 토큰의 중요도를 측정 합니다. 이러한 가중치는 _값_ 에 적용되어 상황 인식 표현을 만듭니다. 이 메커니즘은 시퀀스의 각 토큰이 다른 모든 토큰을 동시에 고려할 수 있게 하여 순차 데이터의 병렬 처리 및 긴 시퀀스 종속성의 효과적인 캡처를 용이하게 한다. 그 결과, LLM에서 딥 네트워크를 형성하기 위해 멀티헤드 어텐션 레이어가 종종 적층된다. 오늘날, GPT-4[196] 및 LLMa[264; 265]와 같은 디코더 전용 모델이 점점 더 널리 보급되고 있지만, 자기 주의의 핵심 아키텍처 모듈은 이러한 변형들에 걸쳐 일정하게 유지된다.

**학습 필수 사항.** LLM은 확장되고 다양한 데이터 세트에 대한 초기 _사전 학습_ 단계에서 범용 기능을 획득합니다. [302; 24]. 이러한 데이터 세트는 책, 과학 논문, 코드 및 웹사이트와 같은 광범위한 소스를 포함한다[316]. 그런 다음 이러한 기초 지식은 LLM이 인간 명령을 준수할 수 있도록 하는 것을 목표로 하는 감독 방식으로 상대적으로 더 작은 데이터 세트에 대해 미세 조정되며, 이는 _명령 튜닝_[50; 55; 57; 167; 251; 259; 258; 278; 332; 337]으로 알려져 있는 프로세스이다. 그런 다음, 인간 피드백을 갖는 _강화 학습_은 인간의 선호 및 지시에 따라 모델 동작을 추가로 정렬하기 위해 지시된 미세 조정된 LLM으로 진행된다[265]. 현재 환경에서 디코더 전용 모델은 특히 우수한 생성 능력으로 인해 표준이 되었다. 이러한 모델들은 선행 컨텍스트에 기초하여 후속 토큰들을 예측할 가능성을 최대화하기 위해 사전-트레이닝 단계에서 자기회귀 목적들을 채용한다. 이 자기회귀 사전 훈련의 스케일링[106; 124]은 GPT 및 PaLM 시리즈[13; 54]와 같은 모델에 의해 입증된 바와 같이 LLM 능력을 상당히 향상시킨다. 예를 들어, PaLM [54]는 780B-토큰 데이터세트에 대해 트레이닝되었고 540B-파라미터 트랜스포머 아키텍처를 활용하였다. 후계자인 PaLM-2 [13]는 이를 더욱 발전시켰으며, 가장 큰 변형에는 1.1T 매개변수가 있으며 1.5T 토큰을 사용하여 보다 다양하고 다국어 데이터 세트에 대해 훈련되었다. 그러나, 이러한 스케일링은 그들 자신의 도전 과제 세트를 도입하며, 효율적인 트레이닝 인프라 및 최적화 전략을 필요로 한다. 딥스피드[216] 및 완전 샤드드 데이터 병렬(FSDP)[3]과 같은 데이터 병렬성에 초점을 맞춘 분산 트레이닝 프레임워크 또는 Gpipe[112] 및 PipeDream[191]과 같은 파이프라인 병렬성이 일반적으로 채용된다. 또한, 메가트론-LM[193; 245] 및 SARATHI[6]와 같은 텐서 병렬화 기술도 활용된다. 혼합-정밀 트레이닝[230; 65; 190] 및 양자화 인식 트레이닝[172; 165; 290]과 같은 특수화된 기술은 종종 트레이닝 프로세스를 효율화하기 위해 통합된다. 이러한 방법은 LLM의 규모와 관련된 계산 문제를 해결할 뿐만 아니라 점점 더 능력 있는 모델의 개발을 용이하게 한다.

**Prompt 엔지니어링을 사용한 다양 한 기능.** 다양한 작업에서 LLM의 다양 한 기능을 활용 하는 주요 메커니즘 중 하나는 신속한 엔지니어링을 통한 것입니다 [168; 284; 336]. 이 설정에서 프롬프트는 LLM의 동작을 안내하기 위해 사용자가 지정한 자연어 지침으로 구성됩니다. 신속한 엔지니어링의 기술은 모델에서 구체적이고 맥락적으로 적절한 반응을 이끌어내기 위해 이러한 지침을 만드는 데 있다. 두 가지 두드러진 기술은 적은 샷(24; 281)과 제로 샷(137) 프롬프팅이다. 0-샷 프롬프트는 작업 설명에만 의존하는 반면, 몇 가지 샷 프롬프트는 예제 작업 및 해당 솔루션을 모델에 제공합니다. 이러한 프롬프트 형태는 GPT-3(24)에서 처음 관찰된 개념인 인 컨텍스트 학습(ICL)과 밀접한 관련이 있으며, 이는 모델이 재학습 없이 새로운 작업에 적응할 수 있도록 한다. 특히 보이지 않는 다양한 작업을 처리하는 LLM의 비상 능력은 ICL이 잘 설계된 프롬프트와 결합될 때 크게 향상된다. Chain-of-Thoughts(CoT)(277; 281), Tree-of-Thoughts(ToT)(174; 293; 304) 및 Graph of Thoughts(GoT)(20)와 같은 고급 프롬프트 전략은 인간의 추론 및 인지 구조에서 영감을 얻는다. 이러한 전략은 LLM이 역추적, 사고 병합 및 아이디어 제거와 같은 새로운 기능을 탐색할 수 있도록 하여 산술(198), 상식 추론(257), 질문 응답(85)과 같은 복잡한 추론 작업에서 응답의 품질을 향상시킨다. 결과적으로 신속한 엔지니어링은 LLM의 다양성과 효율성을 증폭시키는 중추적인 메커니즘 역할을 한다.

### 효율성 평가 메트릭

LLM의 효율성을 평가하기 위해서는 다양한 성과 지표를 고려한 다각적인 접근이 필요하다. 이러한 메트릭은 종종 LLM의 전반적인 효율성과 효과에 대한 총체적인 평가를 제공하기 위해 정확성과 다용도 측정과 함께 제시된다. 다음 단락에서 LLM 영역에서 효율성을 이해하는 데 일반적으로 사용되는 주요 메트릭을 탐색할 것이다.

**매개 변수 수.** LLM의 매개 변수 수는 모델의 학습 용량 및 복잡성에 직접적인 영향을 미치는 핵심 요소입니다. 가중치 및 편향과 같은 요소를 포함하는 이러한 매개변수는 훈련 또는 미세 조정 단계에서 학습할 수 있다. 더 높은 매개변수 계수는 보통 모델이 더 복잡한 데이터 패턴을 파악할 수 있게 하여 다양한 창발 능력의 발달에 기여한다. 그러나, 이는 훈련 및 추론 모두에 대한 증가된 계산 요구의 단점과 함께 온다. 추가적으로, 너무 많은 파라미터들을 갖는 것은 특히 트레이닝 데이터가 부족할 때 과적합으로 이어질 수 있다. 이를 완화하기 위해 정규화 및 조기 중단과 같은 일반적인 기술이 자주 사용된다.

**모델 크기.** 전체 모델을 저장하는 데 필요한 디스크 공간으로 정의되는 모델 크기는 새 LLM을 학습하거나 미리 학습된 모델로 작업할 때 초기 고려 사항인 경우가 많습니다. 매우 큰 모델이 저장 또는 실행이 불가능할 수 있다는 점을 감안할 때 이 메트릭은 특히 에지 장치와 같은 스토리지 제약 환경에서 실제 배포에 특히 중요합니다. 기가바이트(GB) 또는 메가바이트(MB)와 같은 단위로 표현될 때, 모델 크기는 여러 요인에 의해 영향을 받는다. 파라미터들의 수가 중요한 역할을 하지만, 파라미터들에 사용되는 데이터 타입(_e.g._, float16, int8) 및 특정 아키텍처 선택들과 같은 다른 요소들도 기여한다. 모델 크기는 저장 요구 사항에 대한 직접적인 영향 외에도 훈련 및 추론 모두에 필요한 계산 자원의 간접적인 지표 역할을 한다.

**부동 소수점 연산(FLOP)**.** 부동 소수점 연산(FLOP)은 일반적으로 LLM의 계산 복잡도를 측정하는 데 사용됩니다. 이 메트릭은 덧셈, 뺄셈, 곱셈, 나눗셈과 같은 부동 소수점 연산의 수를 계산하여 단일 순방향 패스 동안 수행된 계산의 추정치를 제공한다. FLOP는 계산 요구와 잠재적 에너지 사용에 대한 귀중한 통찰력을 제공하지만 완전한 측정은 아니다. 시스템 병렬성 및 아키텍처 선택과 같은 다른 요인들도 모델의 전반적인 계산 효율성을 결정하는 역할을 한다. 더 높은 FLOPs 카운트는 일반적으로 모델이 더 계산적으로 요구된다는 것을 의미하는데, 이는 제한된 리소스를 갖는 환경에서의 배치에 대한 도전일 수 있다. 결과적으로, 이 메트릭을 최적화하는 것은 종종 더 효율적인 LLM 개발에 핵심 초점이 된다.

**추론 시간/초당 토큰** 입니다. 레이턴시 또는 지연이라고도 알려진 추론 시간은 LLM이 입력을 처리하고 추론 스테이지 동안 응답을 생성하는 데 걸리는 기간을 측정한다. 추론 시간은 계산 요구의 이론적 추정치를 제공하는 FLOP와 달리 실제 성능을 측정하는 실용적인 게이지이다. 특정 하드웨어 및 최적화를 고려하여 실제 배포 설정에서 평가되기 때문입니다. 일반적으로 밀리초 (ms) 또는 초 (s)로 표시 되는 이 메트릭은 빠른 응답이 필요 하거나 엄격한 대기 시간 제약 조건이 있는 실시간 응용 프로그램에 중요 합니다. 시간 경과별로 추론 시간을 정규화하면 초당 토큰이 생성되는데, 이는 언어 모델이 한 초 안에 처리(읽기, 분석, 생성 등)할 수 있는 토큰의 개수를 의미한다. 이는 모델의 속도와 효율성을 반영한 핵심 성과 지표이다. 초당 빠른 추론 시간/토큰과 높은 일반화 사이의 균형을 달성하는 것은 효율적인 LLM의 개발에 있어서 핵심적인 초점이다.

**메모리 풋프린트.** 메모리 풋프린트는 추론 또는 학습 중에 모델을 로드하고 실행하는 데 필요한 RAM(Random Access Memory)의 양을 나타냅니다. 이 메트릭은 특히 메모리 용량이 제한된 에지 장치 또는 서버와 같은 리소스가 제한된 환경에서 모델의 운영 요구 사항을 이해하는 데 중요합니다. MB 또는 GB로 표현된 메모리 풋프린트는 모델 파라미터뿐만 아니라 중간 변수 및 데이터 구조와 같은 다른 런타임 필수품을 포함한다. 더 큰 메모리 풋프린트는 모델의 배포 가능성을 제한할 수 있고 이를 감소시키기 위해 모델 프루닝 또는 양자화와 같은 최적화 기술이 필요할 수 있다.

**탄소 배출.** 탄소 배출은 이러한 모델을 교육 하 고 실행 하는 환경 영향을 반영 하 여 대규모 모델의 평가에서 점점 더 중요 한 메트릭입니다. 이 메트릭은 일반적으로 훈련에서 추론에 이르기까지 모델의 수명 주기 동안 배출되는 킬로그램 또는 톤의 CO2 등가물로 측정됩니다. 탄소 발자국은 사용되는 하드웨어의 에너지 효율, 전기 공급원, 모델 훈련 및 작동 기간 등 다양한 요인의 영향을 받는다. 높은 탄소 배출은 환경적 영향을 미칠 뿐만 아니라 LLMs 배치에 대한 사회적, 윤리적 고려에도 영향을 미칠 수 있다. 결과적으로 모델을 보다 에너지 효율적으로 최적화하여 탄소 발자국을 줄이는 데 중점을 두고 있다. 이는 종종 하드웨어 가속, 알고리즘 개선 또는 데이터 센터에 대한 더 친환경적인 에너지원을 선택함으로써 달성된다.

## 3. 예산 효율성: Scaling Laws

### Introduction

대형 언어 모델(LLM)의 성능은 학습 데이터, 모델 크기, 아키텍처, 컴퓨팅 자원, 학습 방법론 자체를 포함한 다양한 요인에 의해 크게 영향을 받는다. LLM을 훈련하려면 광범위한 리소스가 필요하며, 이러한 요소를 최적화하기 위한 기존의 시행착오 방법은 비실용적이고 리소스 집약적이다. 결과적으로, 훈련 전에 LLM 성능을 예측하는 것은 유익할 뿐만 아니라 종종 필요하다. 이러한 예측 접근법은 자원의 보다 효과적인 계획 및 할당을 가능하게 한다. 예를 들어 컴퓨팅 리소스가 제한된 시나리오를 고려합니다. 최소 목적 함수 값을 얻기 위해 모델 크기와 학습 데이터의 균형을 어떻게 최적으로 맞출 수 있습니까? 이러한 질문에 미리 답변하면 LLM 교육 프로세스의 효율성과 효율성을 크게 높일 수 있다.

대용량 언어 모델(LLM) 성능을 예측하는 최근 연구는 _스케일링 법칙_을 이해하는 데 집중되어 있다. 이 법칙은 LLM 성능이 모델 아키텍처, 신경망 모델 크기, 학습을 위한 컴퓨팅 파워 및 사용 가능한 데이터와 같은 요인에 의해 어떻게 영향을 받는지 설명한다. 모델 일반화를 예측하기 위한 통계적 역학 접근법에 뿌리를 둔 스케일링 법칙의 개념은 1990년대 초반(11; 18; 95; 235)으로 거슬러 올라가는 풍부한 역사를 가지고 있다. 그것의 관련성은 최근 현대 딥러닝 모델(10; 124; 101; 102; 103; 106; 124; 188; 221; 248; 260; 262)의 맥락에서 다시 활성화되었다. 이 섹션에서는 LLM에 적용되는 스케일링 법칙의 최신 발전과 통찰력을 조사하여 이러한 모델이 다양한 조건에서 어떻게 진화하고 수행하는지를 강조한다.

### Scaling Law

본 작업(Kapol et al., 2019)은 변압기 기반 대형 언어 모델의 경험적 스케일링 법칙에 대한 철저한 연구를 제시한다. 저자들은 모델 성능(목적 함수 \(L\))은 주로 모델 매개변수 수 \(N\), 데이터 세트 크기 \(D\), 훈련용 컴퓨팅 예산의 세 가지 요인에 의해 좌우된다는 것을 관찰한다. 그들은 모델 성능(목적 함수에서 측정됨, \(L\))과 이러한 요인들 사이의 멱함수 관계를 보여준다. 예를 들어, 성능과 데이터셋 크기 사이의 관계는 \(L(D)\approx(5.4\times 10^{13}/D)^{0.095}\)로 표현될 수 있음을 발견하였다. 이 공식은 데이터 세트 크기가 증가함에 따라 모델의 성능이 특정 패턴에 따라 향상됨을 시사한다. 이론 일반화 바운드는 유사한 멱-법칙 관계를 제안할 수 있지만, 일반적으로 Kaplan 등의 작업에서 식별된 것과 같은 특정 계수를 제공하지 않는다(Kapol 등, 2019). 이러한 특이성은 모델 성능을 정확하게 예측하는 데 중요하다. 또한, 장거리 데이터 종속성을 효과적으로 처리하는 것으로 알려진 변압기가 확장됨에 따라 LSTM(Long Short-Term Memory Network) (Kapol et al., 2019)을 능가하는 경향이 있음을 강조한다. 이 관찰은 대규모 언어 처리 작업에서 변압기의 잠재력을 강조한다.

**스케일링 법칙을 통한 계산 최적화 모델.** 고정 계산 예산 내에서 작업할 때 모델 크기(\(N\))와 데이터 세트 크기(\(D\)) 간에 올바른 균형을 찾는 것이 중요합니다. 여기서 스케일링 법칙 곡선 \(L(N,D)\)이 중요한 도구가 된다. 이 두 요인 사이의 가장 효과적인 절충안을 결정하는 데 도움이 된다. 스케일링 법칙은 (Kapol et al., 2019)에서 관찰된 바와 같이 1,750억 매개변수 언어 모델인 GPT-3을 설계하는 데 중요한 역할을 했다. 흥미롭게도, GPT-3은 그 시간에 대해 전형적인 것보다 더 적은 토큰들에 대해 트레이닝되었다(Kapol 등, 2019). 스케일링 법칙 곡선의 다른 형태는 후속 연구에서 볼 수 있듯이 다양한 모델의 개발로 이어졌다(Kapol et al., 2019; Kapol et al., 2019; Kapol et al., 2019). 이러한 예측된 스케일링 법칙의 주목할만한 적용이 (Kapol et al., 2019)에서 발견된다. 그들의 연구에 따르면 고퍼(고퍼, 2019)를 포함하여 이전에 훈련된 많은 LLM이 동일한 컴퓨팅 예산 내에서 더 나은 성능을 달성할 수 있었다. 그들은 비슷한 계산 예산을 사용하면서 더 큰 2,800억 매개변수 고퍼 모델(고퍼, 2019)을 능가하는 700억 매개변수로 더 작은 모델인 친칠라를 훈련시켜 이를 입증했다.

**전이 학습을 위한 크기 조정 법칙.** 사전 훈련된 LLM의 크기 조정 동작은 광범위하게 연구되었으며 명확한 예측 가능성을 나타내지만 다운스트림 작업에서 사전 훈련된 LLM의 성능을 예측할 때 덜 명확해집니다. (Kapol et al., 2019)의 작업은 사전 훈련된 모델을 미세 조정할 때 스케일링 동작을 조사하고 (Kapol et al., 2019)의 것과 유사한 유리한 스케일링 법칙이 NLP 작업의 전달 및 적은 샷 설정에 적용된다는 것을 보여준다. 처음부터 훈련된 모델과 비교하여, 사전 훈련된 모델은 낮은 데이터 시나리오에서 더 유리한 스케일링 법칙을 나타낸다. 상류 및 하류 구성 간의 다른 스케일링 거동이 (Kapol et al., 2019)에서 관찰되었으며, 이는 모델 크기 외에도 모델의 아키텍처가 하류 미세 조정에서 중요한 역할을 한다는 것을 나타낸다. 구체적으로, 본 연구는 재설계된 모델이 널리 채택된 T5-베이스 모델(Kapol et al., 2019)에 비해 50% 더 적은 파라미터를 갖고 40% 더 빠르게 트레이닝하면서 유사한 다운스트림 미세 조정 품질을 달성할 수 있음을 입증한다.

**데이터 제한 체제의 크기 조정 법칙.** 학습 데이터가 제한되면 어떻게 하나요? 작업(Kapol et al., 2019)은 데이터 제약 레짐을 조사하고, 고정된 컴퓨팅 예산에 대해, 최대 4 에포크의 반복된 데이터를 갖는 트레이닝이 단일 에포크3에 비해 목적 함수에서 무시할 수 있는 변화를 산출한다는 것을 관찰한다. 그러나, 에포크의 수를 더 증가시키면 학습된 모델의 성능이 감소될 것이다.

**데이터 품질의 영향.** 머신 러닝 영역에서 중요한 질문은 데이터의 품질이 모델 성능에서 멱법칙에서 지수 크기 조정으로 전환할 수 있는지 여부입니다. (Zhu and Chen, 2018)의 작업은 이 문제에 대한 흥미로운 통찰력을 제공한다. 그들은 특정 비전 분류 작업에 대해 목적 함수가 프루닝된 데이터 세트에서 관찰된 전통적인 멱법칙 스케일링에서 벗어나 데이터 세트 크기의 증가와 함께 지수 스케일링을 나타낼 수 있음을 보여준다. 이러한 현상은 처음에 비전 과제에서 관찰되는 반면, (Zhu and Chen, 2018; Chen et al., 2019; Chen et al., 2020)에 의한 작업을 포함한 최근의 연구는 이러한 개념을 다른 영역으로 확장한다. 이러한 연구는 일관성 있는 영어 생성, 코딩 및 상식 추론과 같은 작업에서 고품질 데이터의 영향을 탐구한다. 그들은 고품질 데이터가 스케일링 법칙의 궤적을 크게 변경할 수 있다고 제안한다. 이러한 변화는 더 효율적인 모델에 대한 가능성을 나타내며, 이는 더 적은 수의 데이터 토큰에 대해 아직 고품질로 훈련되었음에도 불구하고 충분한 품질 제약 없이 방대한 데이터 세트에 대해 훈련된 대규모 모델의 성능과 일치할 수 있다. 데이터 품질의 역할을 이해하는 이러한 변화는 LLM을 훈련하고 최적화하는 접근법에 혁명을 일으킬 수 있다.

**아키텍처의 효과.** 모델 크기 조정 영역에서 (Zhu and Chen, 2018; Chen et al., 2020)과 같은 연구에서 지원되는 통념은 Transformers의 너비 또는 깊이와 같은 모델의 고유한 속성이 성능에 최소한의 영향을 미친다는 것을 시사합니다. 그러나 (Zhu and Chen, 2018)의 작업은 대조적인 관점을 제시한다. 본 연구는 다양한 건축설계가 스케일링 법칙에 미치는 영향을 규명하고, 스케일링 과정에서 건축이 중요한 역할을 한다는 것을 밝힌다. Tay et al. (2019)는 척도에 따라 가장 효과적인 모델 아키텍처가 크게 달라질 수 있음을 보여준다. 이 발견은 이전 가정을 보완하고 다양한 규모에서 최적의 모델 성능을 위한 탐색에서 아키텍처 변화를 고려하는 것의 중요성을 강조한다.

## 4. 데이터 효율성

### Introduction

대규모 모델에 의한 데이터에 대한 만족할 수 없는 수요는 데이터 수집 및 준비 산업의 성장을 크게 촉진했다. 그러나 수년에 걸쳐 종종 축적되는 방대한 데이터 세트에 대한 이러한 의존은 모델 훈련에 상당한 어려움을 초래한다. 여기에는 장기간의 훈련 기간뿐만 아니라 광범위한 전력 소비와 더 큰 데이터 저장 용량의 필요성으로 인한 비용 증가도 포함된다. 결과적으로, 훈련 및 검증 단계 모두에서 데이터를 더 효율적으로 사용하는 방법을 찾는 것이 가장 중요하다. 이 섹션에서는 관련 비용과 자원 수요를 완화하면서 데이터의 유용성을 최대화하는 방법을 다루면서 데이터 효율성을 향상시키기 위한 전략과 고려 사항을 조사할 것이다.

### Data Filtering

데이터 필터링은 정보 가치가 낮은 예제에 집중하기보다 불규칙한 문자 또는 패턴을 제거하여 더 유익한 샘플로 훈련 초점을 지시하는 데 중추적이다.

**중복 제거.** 프라임 데이터 필터는 _복제 제거, 즉 중복 제거_입니다. 이 간단하면서도 효과적인 접근법은 훈련 기간을 단축할 뿐만 아니라 (Zhu 및 Chen, 2018)에 의해 입증된 바와 같이 모델 성능을 향상시킨다. 이 중복 제거 작업의 유용성은 모델 개발의 사전 훈련 및 미세 조정 단계 모두에서 분명하다. 사전 훈련과 미세 조정 모두에서 연구자들은 훈련 데이터 세트에서 중복을 제거하기 위해 MinhashLSH(Zhu and Chen, 2018), CC-NET(Zhu and Chen, 2018) 및 적대적 필터링(Zhu and Chen, 2018)과 같은 기술을 사용한다(Zhu and Chen, 2018; Chen et al., 2020; Chen et al., 2020).

**데이터 Undersampling.** 중복 제거 외에도 인스턴스 선택이라고도 하는 _데이터 언더샘플링_ 은 또 다른 유망한 데이터 필터링 기술로 등장 합니다 (Zhu 및 Chen, 2018). 이 접근법은 대규모 데이터 세트를 서브샘플링하여 훈련 샘플의 볼륨을 줄이는 것을 목표로 하지만 원래 훈련 데이터의 분포 특성을 확실하게 유지한다. 더욱이, 이러한 서브-샘플링 프로세스는 데이터 불균형의 문제들을 완화시키는데 도움을 줄 수 있다. 예를 들어, Prusa 등(2020)은 다수의 클래스에서 랜덤 언더샘플링의 효과를 입증했는데, 이는 이중 목적을 제공하며, 트레이닝 세트의 중복성을 감소시키고 다양한 클래스에 걸친 데이터 분포의 균형을 맞춘다. 또한, MESA(Prusa et al., 2020)는 방대한 데이터 세트를 효과적으로 언더샘플링하는 방법을 학습하기 위해 Meta Learning을 채택함으로써 이 분야의 새로운 발전을 나타낸다. 이러한 기술은 양적 측면뿐만 아니라 품질과 균형을 보장하는 측면에서 선택적 데이터 사용의 전략적 중요성을 강조한다.

### Active learning / Importance Sampling

능동 학습 또는 중요도 샘플링은 기계 학습 알고리즘이 더 적은 주석이 달린 훈련 샘플로 더 나은 또는 동등한 성능을 달성하는 데 도움이 된다. 광범위한 데이터 세트를 사용한 훈련에서의 그들의 적용은 LLM의 출현 이전에 있다(Katharopoulos and Fleuret, 2019; Zhang et al., 2020). 이러한 방법론은 다양한 기준을 적용하여 훈련 샘플의 총 수를 전략적으로 줄인다. 이들은 데이터 주석 절차 동안 가장 유용한 인스턴스만을 선택하고 주석함으로써 데이터 수집 및 선택 절차를 최적화하는 것을 목표로 한다(Zhang et al., 2019; Zhang et al., 2020; Zhang et al., 2020). 본질은 학습 과정에 대한 표본의 중요도에 따라 표본의 우선순위를 결정하여 대규모 데이터를 다루는 모델에 대한 학습 효율성을 최적화하는 능력에 있다.

**Gradient Norm.** Katharopoulos and Fleuret (Katharopoulos and Fleuret, 2019)는 중요도 샘플링을 통해 분산 감소에 대한 추정량을 제안하면서 Gradient norm의 상한을 기반으로 하는 접근법을 도입했습니다. 또한, Zhang 등(Zhang et al., 2020)은 배치 내에서 더 큰 구배를 갖는 샘플을 식별하기 위한 선택기 메커니즘을 구현하여 중요도 샘플링에 대한 유효한 근사치로 취급하였다. 방법론에 따르면 기울기가 작은 샘플은 주어진 시대에 '좋은 점'으로 간주되므로 기울기가 큰 샘플인 '나쁜 점'에 대한 초점을 늘려야 한다. 이러한 패러다임에 따라 더 큰 기울기를 가진 샘플에 대한 훈련 및 미세 조정은 모델 성능을 더 효과적으로 향상시킬 수 있다.

**목적 함수/예측 점수.** 기울기를 넘어 Katharopoulos 및 Fleuret도 별도의 작업(Katharopoulos 및 Fleuret, 2019)에서 목적 함수(손실 값) 자체가 중요도 샘플링을 위한 실행 가능한 메트릭 역할을 할 수 있다고 제안했습니다. 이러한 개념을 확장하여, Jiang et al.(Jiang et al., 2019)은 '선택적 역전파'를 도입하였다. 이 방법은 적은 손실을 보이는 샘플을 훈련하기 위해 역전파 단계를 생략함으로써 훈련을 가속화하는데, 이는 전체 데이터 세트에 대한 수렴 및 기존 확률 경사 하강(Stochastic Gradient Descent, SGD)을 향상시키기 위한 전략이다. 텍스트 검색의 맥락에서, 유사한 _관련성 샘플링_(Zhou et al., 2019; Zhang et al., 2020)이 제안되었다. 현재 모델에 의해 예측된 점수를 활용하여 샘플의 관련성을 평가하고 주석자가 더 높은 점수를 가진 점수에 대해서만 주석을 달 수 있다. 이러한 방법은 높은 점수를 갖는 샘플에 초점을 맞추어 과적합(overfitting)을 초래할 수 있다. 일부 연구자들은 _불확실성-기반 샘플링_ 방법(Zhou et al., 2019; Katharopoulos and Fleuret, 2019; Zhang et al., 2020; Zhang et al., 2020; Zhang et al., 2020)으로 돌아간다. 이 방법 스레드는 불확실성이 낮은 인스턴스가 성능 향상에 더 유용하고 주석의 가치가 더 높다고 정의합니다.

**다양성 샘플링.** 훈련 및 주석이 어려운 예제를 찾는 샘플링 방법과 달리 _다양성 샘플링_ 전략을 따르는 방법(Zhou et al., 2019; Katharopoulos and Fleuret, 2019; Zhang et al., 2020; Zhang et al., 2020; Zhang et al., 2020; Zhang et al., 2020; Zhang et al., 2020)은 훈련 데이터의 이질성을 강화하고자 합니다. 다양성 샘플링의 경우 반복 선택과 군집 기반 선택의 두 가지 주요 접근법이 있다. 반복 선택에 속하는 방법들은 각 인스턴스가 트레이닝 데이터 다양성을 개선하는 데 도움이 될 수 있는지 반복적으로 조사하고, 자격을 갖춘 인스턴스들에만 주석을 달 수 있다. 예를 들어, (Zhang et al., 2020)에서 제안된 Vote-k는 각각의 라벨링되지 않은 인스턴스의 임베딩과 그것의 k-최근접 이웃들 사이에서 유사하게 코사인을 채택하여 인스턴스 대표성을 정의하고; ALISH(Zhang et al., 2020)는 데이터 인스턴스의 로컬 민감도에 따른 다양성을 나타낸다(Zhang et al., 2020). 클러스터 기반 메서드는 주석이 없는 데이터 세트를 클러스터링하고 클러스터 레이블을 기준으로 인스턴스를 선택합니다. 이 카테고리의 방법들은 (가중치) K-평균(Zhang et al., 2020; Zhang et al., 2020; Zhang et al., 2020; Zhang et al., 2020; Zhang et al., 2020)과 같은 샘플링과 같은 많은 클러스터링 전략을 탐색했다.

**하이브리드 샘플링.** 기울기 또는 목적 함수 기반 샘플링은 샘플 선택이 모델의 예측 출력에 따라 달라지기 때문에 상대적으로 신뢰할 수 있는 초기 LLM(Large Language Model)을 필요로 합니다. 반면에 다양성 샘플링은 _콜드 시작_ 시나리오를 수용할 수 있습니다. 그러나 이 접근법은 실수로 이상치를 도입하여 잠재적으로 최종 모델의 성능을 손상시킬 수 있다. 양 샘플링 스레드 사이의 트레이드-오프를 해결하기 위해, 최근의 연구는 도전을 극복하기 위해 양 요소를 통합하는 _하이브리드 샘플링 방법_으로 이동했다(Shi et al., 2018; Shi et al., 2018; Shi et al., 2018; Shi et al., 2018; Shi et al., 2018). 예를 들어, Yuan 등(Yuan 등, 2019)은 BADGE(Shi 등, 2018)를 언어 처리에 통합하였다. 이 방법은 주석이 없는 인스턴스를 모델 신뢰도를 캡슐화하는 표현으로 변환한 다음 이러한 변환된 인스턴스를 클러스터링하는 것을 포함한다. 이와 병행하여, Margatina et al.(Margatina et al., 2019)은 Contrastive Active Learning(CAL)을 도입하였다. CAL은 주석이 없는 원시 데이터의 풀로부터 대조적인 샘플을 선택적으로 획득하여, 모델의 특징 공간(_e.g._, 유사한 어휘 또는 모델 인코딩을 공유함)에 근접하지만 발산 예측 가능성을 산출하는 인스턴스로 정의한다. 실험 결과는 CAL이 개별 전략에 비해 더 효과적인 균형을 실현하여 샘플링 프로세스를 정제하는 가능성을 보여준다.

**기타 샘플링.** 위의 기술 외에도 더 넓은 규모로 작동하는 샘플링 방법의 스펙트럼이 있습니다. Chen 등(Chen et al., 2019)은 특정 입력으로부터 역전파를 중지하기 위해 파라미터를 마스킹함으로써 새로운 접근법을 도입하였다. 이 방법은 순방향 그래프를 보존하여 모든 배치의 통계적 특성이 특히 정규화와 같은 계층에서 유지되도록 하여 전체 데이터 세트의 매핑을 반영한다. 다른 접근법을 취하면, Xie 등(Xie et al., 2019)은 감소된 특징 공간에서 중요도 샘플링을 구현한다. 이 전략은 대규모 언어 모델을 효율적으로 훈련하기 위해 KL(Kullback-Leibler) 감소를 활용하여 확장 텍스트 공간에서 중요도 가중치 추정의 다루기 용이성을 촉진한다. 더욱이, Sujit et al.(Sujit et al., 2020)은 강화 학습 컨텍스트에서의 관련성에 기초하여 훈련 샘플을 클리핑하는 것을 옹호하며, 중요도에 따라 샘플을 우선한다. Yao 등(Yao et al., 2020)은 준 지도 언어 미세 조정에서 관련 훈련 샘플을 선택하기 위한 키워드 기반 검색 시스템을 제안하며, 일반적으로 훈련 중에 각 대상 예제에 대해 상위 k개의 이웃을 선택한다. 이러한 다양한 방법론은 중요 샘플링의 진화하는 풍경을 강조하여 LLM에 대한 광범위한 훈련 전략을 포함하도록 표준 배치 중심 접근법을 넘어 적용을 확장한다.

### Curriculum Learning

교육과정 학습(Shi et al., 2018; Shi et al., 2018)은 훈련 데이터에서 사례들의 피딩 순서를 세심하게 설계하여 모델 훈련 효율을 향상시키는 것을 목표로 하는 전략이다. 이 접근법의 원리는 더 간단한 샘플이나 하위 작업으로 훈련을 시작하고 점점 더 어려운 작업으로 확대되는 것이다. 교육과정 학습 방법의 설계에는 두 가지 중요한 구성 요소가 필수적이다. (i)__난이도 메트릭_(또는 난이도 기준)은, 이들의 복잡도에 기초하여 트레이닝 샘플들의 순위를 매기는 것을 담당한다. 이 메트릭은 학습 인스턴스를 가장 단순한 것에서 가장 복잡한 것으로 분류하는 가이드 역할을 합니다. _ (ii)_ 이러한 랭킹된 샘플들이 모델 트레이닝에 공급되는 속도를 결정하는 _페이싱 함수_(커리큘럼 스케줄러 또는 배열로도 알려져 있음) 이 함수는 학습 곡선을 조정하여 모델이 너무 일찍 작업의 복잡성에 압도되지 않도록 합니다.

**난이도 메트릭.** 자연어 처리를 위한 커리큘럼 학습의 영역에서 가장 널리 사용되는 난이도 메트릭은 아마도 시퀀스 길이입니다 (Shi et al., 2018; Shi et al., 2018; Shi et al., 2018). 근본적인 가정은 긴 문장을 처리하는 것이 짧은 문장보다 더 큰 도전을 제기한다는 것이다. 또 다른 널리 퍼진 메트릭은 어휘 희귀성(Shi et al., 2018; Shi et al., 2018)이며, 트레이닝 세트에서 덜 자주 사용되는 단어를 갖는 문장이 본질적으로 이해하기 더 복잡하다는 직관에 기초한다. 또한, 이 메트릭은 능동 학습에서 불확실성 샘플링 원리에 의해 측정될 수 있으며, 여기서 미리 훈련된 다른 모델에 의해 표시된 불확실성은 또한 난이도 측정기 역할을 할 수 있다.

[MISSING_PAGE_FAIL:11]

### Efficient Attention

Vaswani 등(2020)이 소개한 Transformer 모델은 입력 시퀀스에서 조밀한 쌍대 관계를 계산하는 바닐라 주의 메커니즘을 이용한다. 이는 이차 복잡도를 초래한다. 그러나 이러한 모든 관계가 동일한 의미를 갖는 것은 아니라는 점을 인식하여 최근 연구에서는 이러한 과정을 간소화하는 방법에 초점을 맞추고 있다. 이러한 방법들은 오직 가장 중요한 관계만을 식별하고 유지하는 것을 목표로 하고, 이에 의해 주의력 계산의 효율성을 향상시킨다(Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020). 본 서브섹션에서는, _(i)_ 빠른 또는 희박한 주의의 사용(Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020) 및 _(ii)_ 하드웨어 공동 설계를 갖는 IO-인식 주의 계산(Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020)에 대한 논의를 포함한다. 두 접근법 모두 효율적인 주의력 계산을 위해 하드웨어 로딩 시간을 줄인다.

**빠른 주의 계산** 빠른 주의 영역에서 연구자들은 효율성을 향상시키기 위한 혁신적인 전략을 개발하고 있습니다. 주요 초점은 특정 상황에서 종종 불필요한 주의력 계산을 줄이는 것을 목표로 하는 _주의력 인수분해_에 있다. 이 기술은 직접 쌍별 주의 계산이 계산 집약적으로 되는 긴 순차 입력을 처리할 때 특히 유용하다. 주의 인수분해를 채용함으로써, 계산 요구들이 상당히 감소될 수 있어서, 2-D 계산들을 보다 관리가능한 1-D 포맷들로 변환한다(Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020). 또한, 이러한 요인화된 주의 방법은 밀접하게 위치된 토큰과 시간에 따른 각각의 변화 사이의 주의 차이를 식별하고 강조하도록 설계되었다. 이러한 미묘한 접근법은 계산 자원이 데이터의 가장 영향력 있는 요소에 전념하도록 보장한다. 또 다른 혁신적인 방법은 고속 푸리에 변환(FFT) 및 해시 표현과 같은 _주파수 기반 기술_의 사용을 포함한다. 이들 기술은 하드웨어 기능과 잘 정렬되는 방식으로 주의를 모델링하여, 실제 응용에 대해 보다 효율적이다(Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020). 그들은 거의 0에 가까운 주의를 걸러내고 최종 계산을 위해 가장 중요한 것에 계산 노력을 집중한다. 이러한 선택적 주의는 상대적으로 중요하지 않은 데이터를 처리하는 데 리소스가 낭비되지 않도록 하여 모델의 전체 효율성을 더욱 최적화한다.

쌍별 주의력을 직접 계산하는 것에서 벗어나, 일부 방법(Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020)은 계산의 병렬화를 가능하게 하여 효율성을 상당히 향상시키는 _블록 레벨_에서의 주의력 계산 가능성을 탐색한다. 예를 들어, 모나크 프레임워크(Vaswani et al., 2020) 및 그것의 고급 버전인 모나크 믹서(M2)(Vaswani et al., 2020)는 새로운 전략을 채택한다. 그들은 조밀한 주의행렬을 순열과 블록 대각행렬의 조합으로 분해하여 희소화한다. 이러한 분해는 주의력 계산의 보다 효율적인 처리를 가능하게 한다. 나아가, BST(Blockwise Self-Attention) 방법(Vaswani et al., 2020)은 셀프-어텐션 및 피드-포워드 네트워크 모두에 대한 블록 단위 계산을 소개한다. 이 기술은 일반적으로 전통적인 주의 메커니즘과 관련된 메모리 요구 사항을 낮추는 것을 목표로 한다. 더욱이, LongNet(LongNet(LongNet, 2018)과 같은 일부 방법은 원래의 밀집된 주의 대신에 확장된 주의를 통합하여 훨씬 더 긴 토큰 시퀀스의 처리를 허용하여 LLM의 능력을 확장한다.

**하드웨어 관련 효율적인 주의** 소프트웨어 수준에서 보다 효율적인 주의 메커니즘을 설계하는 것과 함께 중요한 초점이 하드웨어 수준에서 이러한 메커니즘을 최적화 하는 것으로 전환 되었습니다. 이 도메인의 주요 과제 중 하나는 GPU 상에서 HBM(High Bandwidth Memory) 및 SRAM(Static Random-Access Memory)과 같은 계산 자원을 효율적으로 활용하는 것이다. 이와 관련하여, 최근 FlashAttention(Vaswani et al., 2020) 및 그의 후속인 FlashAttention-2(Vaswani et al., 2020)와 같은 진보들이 등장하고 있다. 플래시 어텐션은 I/O 중심의 관점을 채택하여 어텐션 계산을 재고한다. HBM과 SRAM 간의 데이터 전송을 최소화하여 GPU 처리 시 중요한 병목 현상을 해결합니다. 이 방법은 블록 단위 소프트맥스 값 계산 및 업데이트를 통계와 통합합니다. 이러한 통합은 모든 주의 값이 결정된 후에만 소프트맥스 함수를 계산하는 종래의 요구 사항을 제거한다. Building upon FlashAttention (Krish et al., 2020), FlashAttention-2 (Krish et al., 2020)는 추가로 작업 분할을 최적화하고 매트릭스 연산에 대한 GPU의 최적화를 이용하여 비-매트릭스 곱셈을 감소시킨다. 이러한 알고리즘은 하드웨어 고려 사항에 맞게 조정되며 GPU 컴퓨터의 모델을 가속화할 수 있습니다.

대용량 언어 모델(Large Language Model: LMM) 시스템을 향상시키기 위한 탐색에서, 일부 연구자들은 현재의 하드웨어 아키텍처로부터 영감을 창의적으로 이끌어내고 있다. 주목할 만한 예는 LLM에서의 메모리 한계를 극복하기 위해 운영 체제에서 일반적으로 사용되는 가상 메모리 및 페이징 기술을 적응시키는 PagedAttention(PagedAttention, 2019)이다. PagedAttention은 가상 메모리 시스템을 에뮬레이트함으로써 메모리 관리에 대한 혁신적인 접근법을 도입한다. 미리 할당 된 연속 메모리에 의존 하는 것과 달리 요청과 연결 된 키 값 (KV) 캐시를 블록으로 분할 합니다. 이 방법은 전통적인 LLM 메모리 할당 전략에서 공통 이슈인 메모리 단편화를 상당히 감소시킨다. 결과적으로, 그것은 LLM이 제한된 메모리 자원의 제약 내에서 더 긴 시퀀스들을 프로세싱할 수 있게 한다.

### 효율적인 Positional Encoding

LLM은 입력으로서 긴 시퀀스를 처리해야 할 수도 있기 때문에, 바닐라 트랜스포머(Petershak 등, 2019)에서 사용되는 절대 위치 인코딩(APE)은 이러한 요구 사항에 미치지 못한다. 아키텍처의 효율성을 향상시키기 위해, 연구자들은 상대적 위치(샌드위치, 2018; 샌드위치, 2019; 샌드위치, 2020) 또는 회전 위치 인코딩(샌드위치, 2020; 샌드위치, 2020)을 갖는 더 긴 시퀀스를 수용할 수 있는 새로운 위치 인코딩(PE) 방법을 탐색하고 있다. 그들은 또한 무작위 위치 인코딩(샌드위치, 2020) 또는 위치 인코딩 생략(샌드위치, 2020)을 통해 보다 일반화할 수 있는 솔루션을 찾고 있다. 우리는 이 섹션의 최신 개발 사항에 대해 논의합니다.

**추가 기반 상대 위치 인코딩** 상대 위치 인코딩 방법은 단일 토큰의 절대 위치가 아닌 두 토큰 간의 상대 위치를 활용합니다. 그들 중 일부는 상대적 위치들을 인코딩하고, 인코딩된 위치들을 후속 주의에 추가하며, _부가 기반_ 상대적 위치 인코딩 방법들을 참조한다. T5(샌드위치, 2019), TISA(샌드위치, 2020), FIRE(샌드위치, 2020)가 이러한 패러다임의 대표이다. 이러한 모델들에서, 위치 임베딩은 개별 토큰들의 절대 위치에 대한 이전의 초점으로부터의 이탈인, 셀프-어텐션 메커니즘 내의 질의와 키 요소들 사이의 상호작용에 적용된다. T5(샌드위치, 2019)에서의 상대 위치 인코딩은 룩업 테이블을 사용하여 상대 위치 차이를 스칼라 바이어스 값으로 변환하고 모든 OOD(out-of-distribution) 시퀀스 길이에 대해 동일한 임베딩을 채용한다. TISA는 토큰 간의 위치 차이에 초점을 맞춘 훈련 가능한 가우시안 커널(샌드위치, 2020)을 통합한다. 반면에, FIRE(Sandwich, 2020)는 토큰들 간의 인덱스 차이를 두 인덱스들 중 더 작은 것으로 나눔으로써 정규화된 포지션 인덱스를 갖는 점진적 보간법을 채용한다. APE에 비해, 상대 위치 인코딩(RPE)은 토큰들 사이의 상대 거리들을 모델링하는 보다 효과적인 방법을 제공한다. 이는 토큰 관계에 대한 모델의 이해를 향상시킬 뿐만 아니라 언어 처리에서 다양하고 복잡한 시퀀스를 처리하는 데 중요한 특징인 길이 외삽을 용이하게 한다.

**부패 함수를 사용 하 여 상대 위치 인코딩** 또 다른 추세는 부패 함수를 사용 하는 훈련 가능한 상대 위치 인코딩 (RPE)을 사용 하는 것입니다. ALBi(Sandwich, 2020), KERPLE(Sandwich, 2018), Sandwich(Sandwich, 2018)와 같은 모델로 예시된 이 접근법은 모델의 주의를 주로 이웃 토큰에 집중하는 것을 목표로 한다. 이러한 방법들에서 쇠퇴 함수들의 사용은 토큰들 사이의 거리가 증가함에 따라 주의력이 감소함을 보장한다. ALBi는 토큰 간의 관계를 모델링하기 위해 선형 부패 함수를 도입하며, 특히 거리가 증가함에 따라 토큰의 관련성이 감소하는 것을 포착하는 데 효과적이다. KERPLE(샌드위치, 2018)는 조건적으로 양의 정(CPD) 커널의 두 가지 변형, 즉 로그 변종과 파워 변종을 사용한다. 이러한 정교한 커널은 원격 토큰 관계의 감소하는 중요성을 적응적으로 모델링하기 위해 RPE 계산 동안 두 토큰 간의 연결을 붕괴시킨다. 한편, 샌드위치(Sandwich, 2018)는 토큰 간의 차이를 나타내기 위해 일련의 코사인 함수를 채택한다. 샌드위치는 코사인 함수의 주기적 특성을 활용하여 토큰 관계의 순환 패턴을 포착한다. 먼 위치들 사이의 주의를 감소시킴으로써, 이러한 방법들은 멀리 떨어져 있는 토큰들보다 더 즉각적이고 맥락적으로 관련된 토큰들에 모델의 포커스가 남아있게 한다.

**회전 위치 인코딩.** 인코딩된 위치를 주의 계산에 추가하는 추가 기반 상대 위치 인코딩 외에도 위치 임베딩을 위해 회전 행렬을 활용하는 RPE 방법이 있습니다 (35; 201; 250). RoPE(250)는 질의 및 키 벡터를 회전시키기 위해 두 개의 회전 행렬을 도입한다. 회전 각도는 이들의 절대 위치에 비례하고, 이는 이후 내적 주의 메커니즘에 통합된다. 이러한 방식으로, RoPE는 토큰들 간의 상대적 차이들을 직접 컴퓨팅하는 대신, 토큰들 간의 상대적 거리에 기초하여 주의를 발생시킬 수 있다. 그러나 RoPE는 훈련된 것 이상의 서열 길이로 일반화하는 데 한계가 있다. RoPE를 기반으로, PI(35)는 위치 보간(PI)으로 기능을 확장한다. 적당한 양의 데이터를 미세 조정한 후 PI는 RoPE의 주요 제한 사항 중 하나를 해결하는 매우 긴 컨텍스트 창을 처리하는 유망한 능력을 보여준다. YaRN(2017)은 NTK-인식 보간법 및 동적 NTK 보간을 도입함으로써 이 필드를 더욱 발전시킨다. 이 방법은 제한된 데이터 세트에 대한 미세 조정이 있거나 없는 시나리오에서 고빈도 정보의 손실을 효과적으로 해결한다. YaRN의 접근법은 광범위한 미세 조정의 필요 없이 컨텍스트 크기를 확장하는 모델의 능력을 크게 향상시킨다. 이러한 방법들 중 공통 스레드는 질의 및 키 벡터에서 회전 행렬을 사용하는 것인데, 이는 트랜스포머 모델에서 보다 효과적인 RPE를 확립하는 데 유망한 결과를 보여주는 기술이다.

**다른 위치 인코딩.** 상대 위치 인코딩(RPE) 방법을 넘어 탐색, Randomized PE(224) 및 NoPE(127)는 입력 쿼리에서 토큰의 연속 위치를 모델링하는 데 의존하지 않는 접근 방식을 제공합니다. 흥미롭게도, 그들은 훈련 분포의 길이 이외의 위치를 포함하거나 위치 인코딩을 모두 포기함으로써 모델이 더 긴 토큰 길이를 갖는 분배 외 경우를 처리할 수 있고 다운스트림 작업에서 향상된 일반화 가능성을 나타낼 수 있다고 가정한다. 랜덤화된 PE(224)는 훈련 중에 마주치는 가장 긴 시퀀스보다 큰 수를 사용한다. 정수의 범위를 무작위로 샘플링하여 정렬 후 인덱스로 사용합니다. 이 접근법은 모델이 최대 토큰 길이에 대한 사전 지식을 필요로 하지만 추론 동안 더 긴 시퀀스로 일반화할 수 있게 한다. 한편, NoPE는 자기-주의 메커니즘에서 위치 인코더를 완전히 포기한다. 이는 모델의 자기 주의력이 문장의 토큰에 걸쳐 RPE를 본질적으로 학습할 수 있음을 보여준다. 이러한 생략은 모델 아키텍처를 단순화할 뿐만 아니라 일반화 가능성 측면에서 특히 학습 분포를 넘어 확장되는 질의가 있는 문장에 대해서도 유망한 결과를 보여준다.

### Sparse Modeling

효율성을 위해 트랜스포머를 최적화하기 위한 탐색에서 연구의 또 다른 핵심 영역은 이러한 주의 기반 아키텍처 내에서 희소 모델링을 통합하는 데 중점을 둔다. 이 접근법은 특히 많은 수의 매개변수를 가진 모델에서 계산 요구를 줄이는 데 중추적이다. 희소 모델링에서 두 가지 주요 방향이 나타났다: 전문가 혼합물(MoE)(345; 44; 72; 150; 189; 214; 239; 345)과 스파스파인더(266).

MoE 접근법(307; 44; 45; 72; 78; 243; 43)은 각각 다른 하위 작업을 전문으로 하는 모델에 여러 가지 분기 또는 '전문가'를 통합한다. 추론 동안, 이들 경로들의 서브세트만이 활성화되어, 잠재적으로 성능을 향상시키면서 계산 효율을 유지한다. 이 설계는 GLaM과 같은 모델이 인상적으로 확장할 수 있도록 하며, 총 1조 2천억 개 이상의 매개변수를 가지고 있음에도 불구하고 추론 중에 990억 개의 매개변수만 활성화한다. Sparse MoE(345)와 같은 MoE의 추가 개발은 표현 붕괴와 같은 문제를 해결하여 전문가의 더 동등한 활성화와 효율적인 정보 처리를 보장한다. 반면, 스파스파인더(266)는 주의 메커니즘 자체 내에서 희소성을 밝히는 데 초점을 맞추어 다른 접근법을 취한다. 이 방법은 주의 기법을 통해 핵심 패턴을 식별하여 모델의 가장 영향력 있는 영역에 계산 자원을 효율적으로 할당하는 데 도움이 된다.

### Attention-free

바닐라 주의 메커니즘(Zhou 등, 2018)의 하나의 중요한 단점은 주의 계산의 이차 복잡성이며, 특히 긴 시퀀스를 처리하는데 비효율적이다. 효율적/희박한 주의가 약간의 완화를 제공하지만, 그것의 최악의 경우 이론적 복잡성은 변하지 않는다. 이를 해결하기 위해, 이차적 주의 행렬의 계산을 회피하는 대안들을 제공하는 다양한 주의-프리 방법들이 제안되었다(Zhou et al., 2018; Chen et al., 2018; Chen et al., 2019; Li et al., 2020; Li et al., 2021; Li et al., 2020). 이러한 방법들은 크게 주의 메커니즘을 반복 계산으로 대체하는 것(Li et al., 2020; Li et al., 2021; Li et al., 2020) 및 상태 공간 표현을 이산화하는 것(Li et al., 2020; Li et al., 2021; Li et al., 2020)으로 분류될 수 있다. 특히, RWKV(Li et al., 2020), H3(Zhou et al., 2020), Hyena(Li et al., 2020), 및 RetNet(Li et al., 2021)과 같은 이러한 새로운 방법들은 표준 트랜스포머에 필적하는 성능을 달성한다. RWKV(Li 등, 2020)는 시퀀스 프로세싱을 능률화하기 위해 순환 신경망(RNN)을 이용함으로써, 긴 시퀀스들을 다루는 복잡성을 감소시킨다. 상태 공간 모델(SSM)에 기초한 H3(Li 등, 2020)는 데이터 표현 및 처리를 위한 효율적인 대안을 제공한다. Hyena(Li et al., 2020)는 자신을 전통적인 주의 메커니즘에 대한 드롭-인 대체물로 제시하여 트랜스포머 아키텍처를 단순화한다. RetNet(Li et al., 2021)은 피드-포워드 네트워크 모듈과 결합된 멀티 스케일 리텐션 모듈을 도입하여 병렬성 및 재발성을 향상시키며, 이는 훈련 및 추론 단계 모두에서 효율성을 상당히 향상시킨다. 우리는 표 1에 길이 \(n\)의 입력 쿼리를 가진 바닐라 변압기와 비교하여 이러한 방법의 복잡도 분석을 포함한다. 각 방법이 복잡도에서 어떻게 확장되는지 개요를 제공하여 주의력 없는 기술의 발전에 대한 통찰력을 제공한다.

## 6. 훈련 및 조정 효율성

### Introduction

LLM을 위한 훈련 및 조정 기술의 개발은 끊임없이 증가하는 데이터 및 모델의 크기에 의해 제기된 문제를 해결해야 한다. 이 섹션에서는 확장 가능한 훈련과 LLM의 튜닝 모두에 중요한 효율성 측면을 조사하여 주요 초점 영역을 강조한다.

**메모리 효율성.** 대형 변압기 모델의 매개 변수 수가 2년마다 약 \(410\times\) 증가 하는 빠른 증가는 중요한 메모리 문제를 제시 합니다. 이러한 성장은 같은 기간 동안 16GB에서 80GB로 증가(5\times\)한 GPU 메모리 확장을 앞질렀다. 트레이닝 동안의 실제 메모리 소비는 모델 상태들(파라미터들, 그라디언트들, 최적화기 상태들), 뿐만 아니라 잔차 상태들(중간 활성화들, 임시 버퍼들, 메모리 단편화)을 포괄하는 원시 파라미터 카운트를 훨씬 초과한다. 이러한 제약 조건을 감안할 때 단일 GPU 설정은 전체 모델을 처리하기에 충분하지 않으므로 효과적인 메모리 관리를 위해 텐서 병렬(TP) 및 파이프라인 병렬(PP)과 같은 분산 학습 접근법이 필요하다.

\begin{table}
\begin{tabular}{l l c c c} \hline \hline Method & Parallelization & Time Cost & Memory Cost & Performance \\ \hline Transformer (Zhou et al., 2018) & Yes & \(O(n)\) & \(O(n^{2})\) & ++++(+) \\ RWKV (Li et al., 2020) & No & \(O(1)\) & \(O(n)\) & + \\ H3 (Li et al., 2020) & Yes & \(O(1)\) & \(O(n\log n)\) & ++ \\ Hyena (Li et al., 2020) & Yes & \(O(n)\) & \(O(n\log n)\) & + \\ RetNet (Li et al., 2021) & Yes & \(O(1)\) & \(O(n)\) & +++ \\ \hline \hline \end{tabular}
\end{table}
표 1. 길이 \(n\)의 시퀀스를 입력으로 사용할 때 Transformer와 attention-free 방법 간의 추론 평가에서 훈련 시간에서의 병렬화와 시간/메모리 비용의 비교. 성능이 + 이하인 모델이 성능이 더 좋습니다.

**컴퓨팅 효율성.** 분산 교육은 대규모 모델의 교육을 빠르게 하는 데 잠재적인 이점을 제공 하지만 확장성에 영향을 주는 복잡성도 도입 합니다. 주목할 만한 관찰은 단일 GPU 설정과 달리 훈련이 여러 GPU에 분산될 때 GPU당 FLOP의 감소이다. 이러한 감소는 증가하는 계산 자원을 효율적으로 활용하는 어려움에서 비롯된다. 따라서 확장성은 특히 다중 GPU 설정에서 훈련 과정 중 연산 효율을 높이는 데 중요한 요소가 된다.

**통신 효율성.** 이 측면은 훈련 중 서로 다른 디바이스 또는 계층 간의 매개 변수 및 기울기의 교환에 관한 것입니다. 모든 감소(all-reduce)와 같은 기법들은 데이터 병렬 트레이닝에서 후방 전파의 종료 시에 모든 디바이스들에 걸쳐 기울기들을 동기화하기 위해 채용된다. 방송, 축소, 전면 축소, 전면 수집 등 집단 운영 시 통신 데이터 양을 최소화하는 것이 목표다.

요컨대, LLM을 훈련하고 조정하는 것은 포괄적인 접근을 요구하는 복잡한 도전이다. 이러한 모든 효율성 측면을 고려하는 통합 전략은 LLM의 효과적이고 확장 가능한 훈련 및 조정을 위해 필수적이다. 후속 섹션에서는 이러한 측면에 대한 자세한 탐색을 제공할 것이다.

### Scalable Training

#### 6.2.1. 안정적인 교육 전략

LLM의 사전 훈련 중에 훈련 안정성을 보장하는 것은 효율성의 중요한 측면이다. 흔히 소실되거나 폭발하는 기울기로 나타나는 훈련 불안정성은 훈련 과정을 상당히 방해할 수 있다. 이러한 문제를 완화하기 위해서는 하이퍼 파라미터의 신중한 선택과 조정이 필수적이다. 한 가지 효과적인 접근법은 배치 크기의 전략적 조작을 포함한다. 예를 들어, PaLM(Wang et al., 2019)과 같은 모델은 트레이닝 동안 배치 크기를 100만 토큰에서 400만 토큰으로 점진적으로 증가시킨다. 이러한 점진적 크기 조정은 안정성을 손상시키지 않고 더 큰 데이터 볼륨을 처리할 수 있는 모델의 확장 용량을 수용하는 데 도움이 됩니다. 또 다른 키 하이퍼파라미터는 학습률이며, 여기서 워밍업 코사인 스케줄러가 일반적으로 채용된다. 이 스케줄러는 초기에 훈련의 초기 단계(전형적으로 전체 훈련 단계의 0.1% 내지 0.5%) 동안 학습률을 증가시킨 다음 코사인 붕괴 전략을 구현한다. 이 접근법은 점진적으로 학습률을 최대값의 약 10%로 감소시켜 훈련이 진행됨에 따라 빠른 학습과 안정성 사이의 균형을 보장한다. 최적화기의 선택은 또한 LLM의 훈련을 안정화하는 데 중추적인 역할을 한다. 아담(Kingmae and Ba, 2015)과 아담W(Kingmae and Ba, 2015)와 같은 최적화기들은 GPT-3(Kingmae and Ba, 2015)와 OPT(Kingmae and Ba, 2015)와 같은 모델들에 널리 사용되고 있는데, 이는 과거의 기울기 정보를 활용하여 수렴을 가속화하는 모멘텀 특성 때문이다. 또한 GPU 메모리 효율로 알려진 Adafactor(Kingmae and Ba, 2015) 최적화기는 PaLM 및 T5(Kingmae and Ba, 2015)와 같은 모델에 사용된다. 하이퍼파라미터 튜닝을 넘어 무게 감소 및 구배 클리핑과 같은 안정화 전략을 구현하는 것은 폭발하는 구배를 방지하기 위해 일반적이다. 그러나 이러한 조치에도 훈련 손실 스파이크가 여전히 발생할 수 있으며, 종종 현재 모델 상태와 처리 중인 데이터 모두에 의해 영향을 받는다. 이를 해결하기 위해 PaLM 및 OPT와 같은 모델은 스파이크가 감지되면 이전 체크포인트에서 훈련을 다시 시작하여 불안정성을 유발한 데이터를 효과적으로 건너뛰는 전략을 사용한다. 이 접근법은 비생산적인 훈련의 장기간을 피함으로써 훈련 안정성뿐만 아니라 계산 자원의 효율적인 사용을 보장한다.

#### 6.2.2. Mixed Precision Training

LLM 사전 훈련의 영역에서 혼합 정밀 훈련은 메모리와 계산 효율성을 모두 향상시키는 중요한 전략으로 등장한다. 전통적으로, 신경망 트레이닝은 가중치들, 그라디언트들, 및 액티베이션들을 완전-정밀(FP32) 포맷으로 저장하는 것을 포함한다. 그러나 매우 큰 모델의 경우 이 접근법은 자원 집약적일 수 있다. 이를 해결하기 위해 FP16 또는 INT8과 같은 축소 정밀도 형식이 채택된다. 이러한 형식은 메모리 사용량을 줄일 뿐만 아니라 모델 내에서 통신 프로세스를 신속하게 처리합니다. 또한, 현대의 GPU는 일반적으로 FP32에 비해 FP16 계산을 처리하는 데 더 능숙하여 계산 속도의 추가 부스트를 제공한다.

이러한 장점에도 불구하고 FP32에서 FP16으로 직접 전환하면 FP16에 내재된 오버플로우 또는 언더플로우와 같은 문제로 인해 성능 저하[114; 335]가 발생할 수 있다. 이러한 문제를 피하기 위해 자동 혼합 정밀도(AMP) [184] 방법이 개발되었다. AMP는 전진 및 후진 패스 동안 계산을 위해 FP16을 사용하는 동안 FP32에서 가중치의 마스터 사본을 유지한다. 계산 후, 가중치들은 마스터 가중치들을 업데이트하기 위해 FP32로 다시 변환된다. 이 방법은 작은 기울기 값을 보존하는 손실 스케일링 기술과 결합되어 광범위한 하이퍼파라미터 튜닝 없이도 AMP가 FP32 훈련의 정확도를 일치시킬 수 있게 한다. 정밀도의 추가의 발전은 새로운 반 정밀 포맷인 BF16(Brain Floating Point)[123]의 도입으로 이어졌다. FP16에 비해 지수에 더 많은 비트를 할당하고 의미에 더 적은 비트를 할당하여 FP32와 동일한 범위를 포함하도록 설계된BF16은 FP16 하에서 혼합 정밀도보다 최신 성능과 더 높은 신뢰성을 입증했다.

또 다른 혁신적인 접근법은 액티베이션 압축 트레이닝(Activation Compressed Training; ACT)[33]이며, 이는 여러 작업에 걸쳐 액티베이션들을 평균 2 비트로 압축하는 것에 초점을 맞춘다. ACT는 역방향 프로세스 동안 저장된 활성화의 압축된 버전을 사용하여 기울기를 계산하여 활성화에 대한 메모리 요구 사항을 크게 감소시킨다. 이 압축은 기존의 방법보다 6.6\(\times\)에서 14\(\times\)까지 훨씬 더 큰 배치 크기로 훈련을 가능하게 한다. 전반적으로 혼합 정밀 훈련은 혁신적인 기술을 통해 효율성과 성능이 지속적으로 균형을 이루는 LLM 훈련의 진화하는 풍경을 보여주는 증거이다.

#### 6.2.3. Parallelism-Based Techniques

LLM들의 트레이닝에서의 병렬화는 GPU들 또는 TPU들과 같은 다수의 가속기들에 걸쳐 계산 워크로드를 분배하는 것을 포함하는 전략이다. 이 접근법은 LLM 훈련에서 필요한 상당한 데이터와 복잡한 계산을 관리하여 보다 진보되고 유능한 모델의 개발을 촉진하는 데 중요하다. 본 절에서는 다양한 병렬 훈련 스키마에 대해 논의한다.

**Data Parallelism (DP).** Data Parallelism [76; 157; 160; 294]는 간단하면서도 효과적인 분산 훈련 형태입니다. 이 접근법에서, 데이터세트는 더 작은 서브세트들로 분할되고, 그 다음, 다수의 가속기들에 걸쳐 병렬로 프로세싱된다. 이 모델은 이러한 장치에서 복제되며 각 복제본은 별도의 장치에서 작동합니다. 그런 다음 각 단위는 할당된 하위 집합에 대해 독립적으로 순방향 및 역방향 계산을 수행한다. DP의 핵심 측면은 각 훈련 단계가 끝날 때 기울기의 동기화이다. 각 장치에서 계산된 기울기가 평균화되어 전체 배치를 나타내는 기울기가 생성된다. 이 프로세스는 병렬 처리에도 불구하고 모델이 모든 데이터 하위 집합에 걸쳐 일관되게 학습하도록 보장한다. DP는 GPU 활용도를 극대화할 수 있는 기능으로 특히 유명합니다. 그러나, 디바이스들 간의 통신 요구를 효율적으로 처리하기 위해서는 높은 대역폭의 상호연결이 필요하다. DP를 활용함으로써 대규모 LLM을 훈련하는 것이 더 실현가능해지고, 더 빠른 개발 사이클과 더 복잡한 모델 아키텍처의 탐색을 가능하게 한다.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \multirow{2}{*}{Parallelism Strategy} & \multicolumn{3}{c}{Resource Efficiency} \\ \cline{2-5}  & Memory & Computation & Communication \\ \hline Data Parallelism (DP) & Low & High & High \\ \hline \multirow{4}{*}{Model Parallelism (MP)} & Tensor Parallelism (TP) & High & Low & Low \\  & (Intra-layer) & High & Low & High \\ \cline{2-5}  & Pipeline Parallelism (PP) & High & Low & High \\ \cline{1-1} \cline{2-5}  & (Inter-layer) & High & Low & High \\ \hline \hline \end{tabular}
\end{table}
표 2. 효율성을 위한 상이한 병렬화 전략의 요약.

**모델 병렬성(MP)** 모델 병렬성은 여러 가속기에 걸쳐 모델 자체를 분할하는 데 중점을 둔 DP에 대한 대안적인 접근 방식입니다. 이 방법은 특히 단일 GPU가 전체 모델을 저장할 용량이 부족한 경우, 많은 수의 파라미터 및 크기를 갖는 모델을 처리하는데 특히 유용하다. 모델 병렬성은 텐서 병렬성(TP)과 파이프라인 병렬성(PP)의 두 가지 유형으로 더 분류될 수 있다.

\(\bullet\) 텐서 병렬 [238; 245; 297]은 _intra-layer_ 모델 병렬의 형태입니다. 그것은 다수의 가속기에 걸쳐 개별 레이어의 텐서를 분할하는 것을 포함하고, 단일 GPU의 메모리 용량보다 더 큰 모델의 훈련을 허용한다. 이 접근법의 주목할만한 예는 MLP 층, 셀프-어텐션 층 및 출력 임베딩 층과 같은 변압기 파라미터의 상이한 컴포넌트를 슬라이싱하기 위한 전략을 제공하는 메가트론-LM[245]이다. 이 슬라이싱은 수평 또는 수직으로 수행할 수 있습니다. 처음에 TP는 2차원 행렬 분할에 초점을 맞추었지만 이후 다차원 분할을 포함하도록 진화했다. 거대-AI[21; 296; 272]와 같은 프레임워크는 TP를 더 높은 차원으로 확장하고 시퀀스 데이터를 처리하기 위해 시퀀스 병렬성[139; 158]을 도입했다. TP는 메모리 사용 측면에서 효율적이지만 효과적인 계층 통신을 위해서는 높은 상호 연결 대역폭을 필요로 한다.

\(\bullet\) 파이프라인 병렬성 [112; 118; 131; 140; 164; 191; 192; 193; 300] 은 _inter-layer_ 모델 병렬성의 형태입니다. 파이프라인 구성에서 여러 가속기에 걸쳐 모델의 계층을 분할 하는 것이 포함 됩니다. 각 가속기는 다른 레이어를 계산한 다음 출력을 조립 라인과 유사한 다음 레이어에 전달하는 역할을 합니다. 이 설정을 사용하면 순방향 및 역방향 패스를 순차적으로 동시에 처리할 수 있습니다. 모델의 하나의 세그먼트가 데이터의 하나의 부분을 처리하는 동안, 다른 세그먼트들은 상이한 부분들에서 동시에 작업할 수 있다. PP의 효율성의 핵심은 GPU의 유휴 시간을 최소화하기 위해 신중한 스케줄링을 요구하지만 모델의 모든 부분을 활성화하고 생산적으로 유지하는 능력에 있다.

PP에 대한 초기 제안 중 하나인 Gpipe [112]는 PP와 미니 배치 분할을 결합한다. 여러 GPU에 걸쳐 큰 모델을 분할하고 입력 미니 배치를 더 작은 마이크로 배치로 처리합니다. 이 접근법은 상당히 큰 모델의 효율적인 훈련을 허용한다. GPipe는 또한, 이들을 저장하는 대신 역방향 전파 동안 활성화들을 재계산함으로써 메모리 사용을 감소시키기 위해 재물질화[43]라는 전략을 사용한다. 그러나, GPipe는 백워드 계산 동안 캐시 활성화의 필요성으로 인해 여전히 메모리 비효율성에 직면한다. PipeDream[191; 98]은 자신의 One Forward Pass에 이어 One Backward Pass(1F1B) 전략으로 PP 접근법을 더욱 세분화한다. 이 방법은 마이크로 배치의 순방향 통과에 따른 즉각적인 역방향 전파를 허용하여 파이프라인의 초기 단계가 더 빨리 역방향 계산을 시작할 수 있게 한다. PipeDream은 또한 가중치들의 상이한 버전들을 사용하는 비동기 그래디언트 업데이트들을 채택하고, 보다 효율적인 프로세싱을 위해 파이프라인 스테이지들에 걸쳐 메모리 할당을 최적화한다. BPipe[131]는 파이프라인 전체에 걸쳐 메모리 사용의 균형을 맞추는 기술을 도입한다. 후기의 유휴 메모리를 활용하여 초기 단계를 지원하며, GPT-3 96B, GPT-3 134B와 같은 대형 모델의 학습 프로세스를 메가트론-LM에 비해 1.25~2.17배 크게 가속화한다. TeraPipe [164]는 광범위한 시퀀스 길이를 갖는 대형 모델을 훈련시키는 과제를 해결하며, 이는 파이프라인에서 더 작은 미니 배치와 증가된 유휴 시간을 초래할 수 있다. 트랜스포머 아키텍처에서 일부 레이어의 계산은 미래의 숨겨진 상태에 의존하지 않는다. TeraPipe는 이러한 사실을 이용하여 입력 시퀀스를 분할하여 병렬 처리를 가능하게 한다. 동적 프로그래밍을 사용하여 토큰에 걸쳐 최상의 지점에서 시퀀스를 효과적으로 분할하여 병렬 처리의 효율성을 향상시킨다.

**자동 병렬 처리.** 자동 병렬 처리는 최적의 성능을 위해 다양한 병렬 처리 방법을 결합 하 여 최신 LLM을 확장 하는 데 핵심 접근 방식이 되었습니다. DeepSpeed[216], Megatron-LM[245], Colossal-AI[296]와 같은 시스템은 3D 병렬 접근 방식을 채택했으며, 이는 훈련 데이터를 작업자 전체에 균일하게 배포하고 모델을 수동으로 분할하고 각 파이프라인 단계에서 레이어를 배포한다. 그러나, 병렬 유형들의 이러한 수동 오케스트레이션은 복잡하고 상이한 모델들 및 컴퓨팅 환경들에 걸쳐 쉽게 적응될 수 없다.

이 프로세스를 간소화하기 위해 자동화된 병렬화 솔루션이 개발되고 있다. 이러한 솔루션은 모델 배포 속도를 높이고 다양한 모델에 대한 적응성을 보장하는 것을 목표로 합니다. 모델 및 컴퓨팅 클러스터가 증가함에 따라 병렬 구성 구성의 복잡성도 증가한다. 두부(Tofu, 2017)는 데이터 흐름 그래프 분할을 최적화하는 동적 프로그래밍 알고리즘으로 이 문제를 해결한다. Dapple(Tofu, 2017)은 최적의 분할 전략을 통해 파이프라인 대기 시간을 최소화하는 데 초점을 맞추고 있다. 그러나 이러한 솔루션은 현재 서로 다른 병렬 전략 간의 복잡한 상호 작용으로 인해 데이터 병렬과 하나의 모델 병렬 유형만 결합하는 것으로 제한된다. 알파(Alpa, 2018)는 데이터, 모델, 파이프라인 병렬성을 계층적 구조로 구성하여 보다 포괄적인 접근을 취하고 있다. 모델 병렬 계획에는 정수 선형 프로그래밍을 사용하고 파이프라인 병렬 계획에는 동적 프로그래밍을 사용한다. 알파는 GPT 모델을 훈련할 때 메가트론-LM과 같은 특수 시스템에 필적하며 복잡한 병렬화 문제를 처리하는 데 효과가 있음을 보여준다.

FlexFlow(FlexFlow, 2018)는 연산 출력 텐서를 상이한 차원(샘플, 연산, 속성, 파라미터)에 걸쳐 분할하는 방법을 제안함으로써 3D 병렬성의 개념을 확장한다. 계산 그래프의 각 연산에는 특정 병렬화 구성이 할당된다. 최상의 병렬화 전략을 찾기 위해 FlexFlow는 주어진 디바이스 토폴로지에서 연산자 그래프를 실행하는 데 필요한 시간을 예측하는 실행 시뮬레이터를 사용한다. 그런 다음 마코프 체인 몬테카를로 샘플링을 사용하여 운영자 그래프와 장치 토폴로지를 모두 고려하여 최적의 전략을 체계적으로 검색한다.

#### 6.2.4. Memory Optimization

크기가 증가하는 LLMs 훈련 영역에서 모델 파라미터, 구배 및 최적화 상태를 저장하는 데 필요한 메모리가 크게 증가한다. 이 문제는 DP에서 특히 심각하며, 여기서 각 GPU는 전통적으로 모델의 매개변수의 완전한 사본을 저장하여 상당한 메모리 중복성을 초래한다. 효율적인 메모리 최적화 전략들은 제한된 하드웨어 자원들에 대해 더 큰 모델들을 훈련시키기 위해 필수적이며, 훈련 인프라의 상이한 컴포넌트들에 걸쳐 메모리 부하를 밸런싱한다.

ZeRO(Zebro, 2017)는 GPU에 걸친 메모리 부하를 분할함으로써 데이터 병렬에서 메모리 중복 문제를 해결한다. 각 GPU가 모델 매개 변수, 기울기 및 최적화기 상태의 전체 집합을 저장하는 대신 ZeRO는 이러한 요소를 분할하여 각 GPU가 데이터의 일부만 보유할 수 있습니다. 나머지 데이터는 필요에 따라 다른 GPU로부터 검색될 수 있다. 이 접근법에는 각각 모델의 메모리 요구 사항의 특정 측면을 대상으로 하는 매개변수 분할, 구배 분할 및 최적화기 상태 분할의 세 가지 주요 전략이 포함된다. ZeRO를 기반으로 하는 ZeRO 오프로드(Zebro, 2018)는 이러한 개념을 확장하여 CPU 및 GPU 기능을 모두 사용하여 교육을 가능하게 하며, GPU의 메모리 부담을 줄이기 위해 일부 계산 및 스토리지를 CPU에 오프로딩한다. 그러나 이러한 오프로딩은 CPU와 GPU 간의 통신을 증가시켜 주의 깊게 관리하지 않으면 병목 현상이 될 수 있다. 전략은 트레이닝 프로세스를 데이터 흐름 그래프로서 보는 것을 포함하며, 상이한 계산 노드들은 상이한 디바이스들에 할당된다. 포워드 및 백워드 프로세스는 GPU에 의해 처리되고, 파라미터 업데이트 및 정밀 변환은 CPU에 의해 관리된다. 이 접근법은 CPU 계산을 최소화하고 통신 오버헤드를 감소시켜 훈련 프로세스에서 CPU 및 GPU 리소스의 효율적인 사용을 보장하는 것을 목표로 한다.

이러한 발전을 통합 하 여 DeepSpeed (Zebro, 2017)와 같은 시스템은 다양한 수준의 메모리 최적화를 제공 합니다. 첫 번째 단계는 GPU에 최적화 상태만을 분할하여 메모리를 최적화하는 ZeRO-DP(Data Parallelism)이다. 두 번째 단계는 그래디언트 및 옵티마이저 상태를 분할하여 메모리 사용량을 더욱 줄이기 위한 ZeRO-R(Reduction and Partitioning)이다. 세 번째 단계는 메모리 최적화를 GPU에서 사용할 수 있는 것 이상으로 확장하는 ZeRO-Infinity로 CPU와 NVMe 메모리를 모두 활용하여 매우 큰 모델의 훈련을 가능하게 한다.

### Scalable Tuning

대규모 및 다양한 데이터 세트에 대해 훈련된 대규모 언어 모델은 놀라운 일반적인 문제 해결 능력을 보여주었다. 그러나 표적 적응을 통해 특정 도메인이나 작업에 대해 성능이 크게 향상될 수 있다. 최근 몇 년 동안 이러한 적응 과정을 용이하게 하기 위한 다양한 기술이 등장했다. 이 섹션에서는 사전 훈련된 LLM의 효율적인 적응을 위한 두 가지 주요 접근법: _(i)_ 매개변수 효율적인 미세 조정, 어댑터 계층을 통합하거나 사전 훈련된 모델의 기존 매개변수를 미세 조정하는 것과 _(ii)_ 프롬프트 엔지니어링을 통한 작업별 컨텍스트 통합에 대해 논의한다. 이러한 방법은 LLM을 특정 응용 프로그램에 맞춤화하는 핵심 전략을 나타내며 다양한 NLP 작업에서 다양성과 효율성을 모두 보장한다.

#### 6.3.1. 파라미터 효율적인 Fine-Tuning (PEFT)

사전 훈련된 LLM의 상당한 크기는 다운스트림 작업 또는 애플리케이션 도메인에 대한 전체 모델을 완전히 미세 조정하는 데 비용이 많이 들거나 비실용적이다. 전체 LLM을 직접 미세 조정하는 것을 피하기 위해 다양한 매개변수 효율적인 조정 방법이 등장했다. 이러한 방법들은 원래의 사전 훈련된 파라미터들의 대부분 또는 전부를 고정시킨 채로, 적은 수의 훈련 가능한 파라미터들을 조정하거나 도입함으로써 LLM들을 정제하는 것에 초점을 맞춘다. 이러한 방법은 일반적으로 칭찬할 만한 성능을 달성하고 훈련 가능한 매개변수의 양을 크게 감소시킨다. 그들은 전체 파라미터 튜닝에 비해 메모리와 계산 효율을 모두 향상시켜 LLM을 특정 작업에 적용하기 위한 보다 실용적인 솔루션을 제공한다.

**부분 매개 변수 튜닝.** LLM을 적용하는 데 간단하지만 효과적인 방법은 부분 매개 변수 튜닝이며, 여기서 미리 훈련된 매개 변수의 선택된 부분만 미세 조정되어 나머지는 변경되지 않습니다. 이 방법은 널리 입증되었다. 예를 들어, 작업들(Liu et al., 2019; Liu et al., 2020)은 몇 개의 최종 층들만을 미세 조정하여, 완전 미세 조정된 모델의 성능의 최대 90%를 달성한다. Xie 등(2021)은 그들의 숨겨진 상태들, 특히 분류 태스크들에 대한 가변성에 기초하여 미세 조정을 위한 계층들의 서브세트를 선택하는 것을 포함한다. 추가적으로, BitFit(Liu et al., 2020)은 트랜스포머 기반 LLM에서 바이어스 항만을 조정하여 경쟁 성능을 산출함으로써 대안적인 전략을 제시한다. 이러한 예들은 다양한 애플리케이션들에 LLM들을 적응시키기 위한 자원-효율적인 방법으로서 부분 파라미터 튜닝의 가능성을 강조한다. 그러나, 이러한 방법들은 통상적으로 추가 튜닝을 위해 파라미터들의 서브세트를 선택하는 방법을 안내하는 상세한 원리가 부족하다.

**모델-어댑터 튜닝.** 미세 조정을 위한 특정 매개 변수를 선택하는 문제를 해결하기 위해 어댑터 튜닝 기술이 도입되었으며, 어댑터(Liu 등, 2020)로 알려진 추가 소규모 학습 가능한 블록으로 사전 훈련된 모델을 보강하는 작업이 포함됩니다. 이러한 접근법은 사전 훈련된 모델의 무결성을 유지하지만 사전 훈련된 LLM의 하나 또는 여러 모듈에 어댑터 블록을 내장한다. 이러한 어댑터는 일반적으로 컴팩트한 병목 층의 형태를 취한다. 하나의 예는 비선형성 함수를 갖는 2층 MLP(Multi-Layer Perceptron) 및 은닉층 내의 소수의 뉴런을 포함한다. 어댑터 통합은 트랜스포머 아키텍처의 어텐션 및 피드-포워드 층으로 직렬(Liu et al., 2020) 또는 병렬(Liu et al., 2020)로 실행될 수 있거나, 트랜스포머 아키텍처의 외부(Liu et al., 2020)로 실행될 수 있다. 어댑터의 재사용성 및 범용성을 더욱 향상시키기 위해, AdapterHub(Liu et al., 2020)가 개발되었다. 이 프레임워크는 사전 훈련된 어댑터의 동적 통합을 허용하여 다양한 작업 및 LLM을 제공한다. 어댑터의 사용은 미세 조정 프로세스를 가속화하고 저장 요구 사항을 완화하지만 각 변압기 층에 깊이 또는 너비를 추가하여 계산 그래프를 수정한다. 이러한 수정은 연구들(Liu et al., 2020)에서 관찰된 바와 같이, 추론 레이턴시의 약간의 증가를 초래하며, 여기서 추론 속도는 대략 4-6%만큼 느린 것으로 밝혀졌다.

**매개 변수-어댑터 조정** 관련 또 다른 방법은 모델 매개 변수에 직접 어댑터를 추가 하는 것입니다. 사전 학습된 네트워크 파라미터들을 \(\mathbf{\theta}\)로 표현하면, 이 부류의 기법들은 모델 파라미터들을 \(\mathbf{\theta}+\Delta\theta\)으로 확장하며, \(\theta\)은 고정되고 \(\Delta\theta\)은 저순위 근사치로 학습된다. 이 기법의 구현은 디프-프루닝(Zhu et al., 2020)으로, 미세 조정 동안 희소 촉진 규칙화를 추가하여 태스크별 희소 파라미터 \(\Delta\theta\)를 학습한다. LoRA(Liu et al., 2020)는 각 선형 레이어에 대한 저순위 변환을 학습한다. 특히 LoRA는 가중치 행렬을 \(\theta+\Delta\theta\approx\theta+BA\)로 재매개하여 미리 훈련된 가중치 행렬 \(\theta\)은 고정되어 있지만 낮은 순위의 행렬 \(B\)과 \(A\)은 학습할 수 있다. LoRA에서 모든 가중치 행렬은 각 하위 행렬에 대해 일정한 고유 순위를 공유하지만 다른 모듈에 걸쳐 다양한 중요성을 설명하지 않는다. AdaLoRA(Srivastava et al., 2017)는 그들의 중요도 스코어들에 기초하여 가중치 매트릭스들에 파라미터 예산들을 동적으로 할당함으로써 이러한 제한을 해결한다. 더 중요한 증분 행렬에 더 높은 순위를 할당하여 더 자세한 작업별 정보를 캡처하는 동시에 덜 중요한 행렬의 순위를 줄여 과적합을 방지하고 계산 자원을 절약한다. SoRA(Srivastava et al., 2017)는 근위 기울기 방법을 사용하여 증분 매트릭스의 랭크를 동적으로 조정하는 최적화 가능 게이트를 소개한다. QLoRA(Srivastava et al., 2017)에서, 사전 훈련된 모델은 초기에 4비트로 양자화된다. 이어서, 학습가능한 낮은-랭크 어댑터 가중치들의 작은 세트가 양자화된 가중치들에 걸쳐 역전파된 구배들을 사용하여 증강되고 미세 조정된다. QLoRA는 16비트, 8비트 또는 4비트 어댑터로도 16비트 풀파라미터 미세조정의 성능을 맞출 수 있다.

#### 6.3.2. Data-Efficient Tuning

데이터 효율적인 튜닝은 사전 훈련된 LLM을 미세 조정하는 대신 다운스트림 작업에 대한 제한된 프롬프트 파라미터 세트를 업데이트하는 프로세스를 지칭한다. 이는 전형적으로 프롬프트 튜닝을 통해 달성되며, 여기서 사전 훈련된 모델의 가중치는 고정된 상태로 유지되지만, 추가된 프롬프트 토큰만이 조정된다. 이러한 접근법은 특히 모델 파라미터의 규모가 증가함에 따라 데이터의 보다 효율적인 사용을 가능하게 하고 종종 향상된 성능을 산출한다.

**Prompt Tuning.** Prompt Tuning은 감독 된 다운스트림 작업에서 LLM의 성능을 향상 시키는 데 사용 되는 기술입니다. 다운스트림 작업을 마스킹된 언어 문제로 공식화하고 원본 토큰 입력을 템플릿으로 변환하고 LLM이 완료할 수 있도록 채워지지 않은 특정 토큰을 마스킹합니다. 튜닝 가능한 템플릿 임베딩을 수정함으로써, 프롬프트 튜닝은 미리 트레이닝된 태스크들과 특정된 다운스트림 태스크들 사이의 분배 시프트를 감소시킴으로써 다운스트림 태스크들에서의 성능을 향상시키는 것을 목표로 한다. 이 방법은 또한 LLM이 새로운 프롬프트 템플릿을 생성함으로써 제한된 감독 데이터를 갖는 시나리오에서 특히 유용한 소수의 샷 또는 심지어 제로 샷 학습에 참여할 수 있게 한다.

전통적인 방법에는 신속한 템플릿과 구두화기의 수동 설계가 필요했으며, 이는 종종 민감하고 다양한 효능을 초래했다. 그러나, 최근의 신속한 학습의 발전은 신속한 구성의 자동화 및 최적화로 이어졌다. AutoPrompt(Wang et al., 2019)는 효과적인 템플릿에 대한 검색을 자동화하기 위한 그래디언트 기반 접근법을 소개한다. LM-BFF(Wang et al., 2019)는 라벨 단어를 검색하고 이산 프롬프트 공간에서 T5 기반 템플릿 생성 방법을 사용함으로써 자동화된 프롬프트 생성을 위한 보다 효율적인 솔루션을 제공한다. 이산 최적화 문제를 해결하기 위해, Prefix-Tuning(Zhou 등, 2019)은 LLM이 변경되지 않은 채로 유지되는 동안 프롬프트만 미세 조정되는 파라미터화된 프롬프트를 추천한다. P-튜닝(Zhou et al., 2019)은 템플릿이 자연어로 구성되어야 한다는 종래의 제약에서 벗어나, 템플릿 구성을 연속적인 파라미터 최적화 챌린지로 변환한다. CP-튜닝(Zhou et al., 2019)은 수동 구두기 설계를 대체하는 역할을 하는 임베딩의 분포를 자동으로 학습하기 위한 대조적 학습의 사용을 옹호한다. UPT(Wang et al., 2019)는 Prompt-Options-Verbalizer 패러다임을 도입하여, 다양한 NLP 태스크에 걸쳐 공동 프롬프트 학습을 용이하게 하고 LLM이 태스크 불변 프롬프트 지식을 획득하도록 장려한다.

## 7. 추론 효율성

### Introduction

대용량 언어 모델(Large Language Models, LMM)의 수많은 파라미터는 클라우드 서비스 및 리소스 제한 장치에 배포하기 위한 중요한 문제를 제기하며, 추론 지원을 위한 높은 유지 관리 비용을 초래한다. 결과적으로 추론을 가속화하는 것은 업계와 학계의 주목을 받는 시급한 문제가 되었다. 한 가지 일반적인 방법은 전체 모델에 경쟁 성능에 도달할 수 있는 컴팩트 모델을 구성하는 것인데, 이 방법은 크게 가지치기, 지식 증류, 양자화 및 저순위 분해의 네 가지 범주로 분류할 수 있다. 프루닝 기법들은 딥 뉴럴 네트워크들(DNNs)의 오퍼레이터들 내에서 중복성을 식별하고 제거하는 것에 초점을 맞추고, 이에 의해 더 많은 것을 생성한다

[MISSING_PAGE_FAIL:22]

LoRAShear(Srivastava et al., 2017)는 최근 제한된 자원 설정에서 제안되었다. LoRAshear는 LHSPG(LoRA Half-Space Projected Gradient)라는 새로운 구조의 희소 최적화기를 사용하여 점진적인 구조적 가지치기를 수행하고 지식을 전달한다. LoRAShear는 지시된 미세 조정 데이터만을 사용하는 기존 작업과 달리, 전체 LLM과 압축 LLM 사이의 성능 격차를 효과적으로 줄이기 위해 다단계 지식 복구 메커니즘을 적용한다. 풀-리소스 셋업을 위해, Sheared-LLaMA(Srivastava et al., 2017)는 처음부터 훈련된 동일한 크기의 LLM들을 능가하는 컴팩트한 모델들을 생성하기 위해 원래의 LLM들에 대해 구조화된 프루닝을 수행한다. 그러나, 이는 상당한 GPU 전력 및 데이터 자원을 필요로 하며, 이는 공용 사용자에게 실현 가능하지 않을 수 있다. 구조적 가지치기 중-소형 DNN의 상대적으로 성숙한 도메인과 비교하여 LLM의 구조적 가지치기는 아직 초기 단계에 있으며 추가 탐색을 기다리고 있다.

### Knowledge Distillation

지식 증류의 개념은 더 유능한 대형 '교사' 모델의 감독 신호를 활용하여 컴팩트한 '튜던트' 모델을 훈련시키는 것을 포함한다. 이러한 접근법은 종종 학생 모델이 그러한 안내 없이 훈련된 유사한 크기의 모델의 성능을 능가하는 결과를 초래한다(Krizhevsky et al., 2017). 지식증류는 크게 반응기반, 특징기반, 관계기반 지식증류로 분류할 수 있다(Rasmaldi et al., 2017). 응답 기반 지식은 교사 모형의 최종 산출층에 초점을 맞춘다. 가설은 학생 모델이 교사 모델의 예측을 모방하는 것을 학습할 것이라는 것이다(Krizhevsky et al., 2017; Krizhevsky et al., 2017). 훈련된 교사 모델은 또한 그 중간 층들에서의 데이터의 특징 기반 지식을 캡처하는데, 이는 특히 심층 신경망에 적절하다(Srivastava et al., 2017; Li et al., 2018). 특징 맵들 간의 관계를 캡처하는 지식은 또한 관계 기반(Srivastava et al., 2017)으로 참조하면서 학생 모델을 트레이닝하는 데 사용될 수 있다. NLP 도메인에서의 초기 연구는 주로 태스크-특정 모델의 증류에 집중되었다(Li et al., 2018). 나중에, 더 많은 연구들이 사전 훈련된 모델들을 증류하는 쪽으로 초점을 이동시켰는데, 이는 후속적으로 특화된 다운스트림 태스크들에 대해 미세 조정될 수 있다(Krizhevsky et al., 2017; Krizhevsky et al., 2017; Li et al., 2018). 최근 LLMs에 대한 증류 방법이 등장하고 있다(Krizhevsky et al., 2017; Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018). LLM에 대한 현재 지식 증류 방법의 한 가지 주요 초점은 교사 모델에서 학생 모델로 지식을 보다 효과적으로 전달하기 위해 도전적인(지시된) 샘플을 생성하고 활용하는 방법(Li et al., 2018; Li et al., 2018; Li et al., 2018)에 있으며, 이는 섹션 4의 데이터 언더샘플링 방법과 일부 중복된다. Chain-of-thought prompting은 일반적으로 데이터 생성을 달성하기 위한 증류 접근법(Krizhevsky et al., 2017; Li et al., 2018)에서 사용된다.

### Quantization

양자화 방법은 재학습의 필요성에 기초하여 구분될 수 있다(Srivastava et al., 2017). 양자화-인식 트레이닝(Quantization-Aware Training, QAT)은 모델 재훈련을 의무화하고, 양자화 후 정확도를 회복하기 위해 그 가중치를 조정한다(Krizhevsky et al., 2017; Li et al., 2018; Li et al., 2018). 대조적으로, 포스트-트레이닝 양자화(post-Training Quantization; PTQ)는 어떠한 재트레이닝 없이 양자화를 달성한다(Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018). QAT는 종종 우수한 정확도를 산출하지만, 재교육의 막대한 비용 및 일반적으로, 원래의 트레이닝 데이터 및 프로세싱 인프라에 대한 액세스 부족으로 인해 LLM(Large Language Model)에 대해서는 종종 비실용적이다. 결과적으로 LLM 양자화에 대한 대부분의 연구는 PTQ 기술에 중점을 둔다.

다른 관점에서 양자화 방법은 크게 균일 접근법과 비균일 접근법으로 분류될 수 있다(Srivastava et al., 2017). SPQR(Srivastava et al., 2017), GPTQ(Srivastava et al., 2017) 및 기타(Krizhevsky et al., 2017; Li et al., 2018)와 같은 작업에서 탐색된 바와 같이, 균일 양자화는 가중치의 범위를 동일한 크기의 빈으로 분할하는 것을 포함한다. 이 방법은 완전한 정밀도가 아닌 양자화된 정밀도에서 산술 연산을 허용함으로써 계산을 가속화하는 능력으로 대중화되었다. 추가로, 균일한 양자화는 LLM들에서 종종 관찰되는 바와 같이, 가중치 분포가 불균일한 경우에 최적이 아닐 수 있다. 반대로, 불균일한 양자화는 이러한 과제들에 대한 해결책을 제공한다. 스퀴즈LLM(Krizhevsky et al., 2017)에서 연구된 바와 같이, 이 접근법은 양자화 빈을 불균일하게 할당하여, 특히 불균일한 가중치 분포를 다룰 때 더 많은 유연성과 잠재적으로 더 나은 성능을 허용한다.

구조화된 프루닝과 비교하여 양자화는 메모리 비용과 추론 속도를 줄이기 위해 저비트 정밀도의 현실적인 이점을 실현하기 위해 지정된 하드웨어를 필요로 한다. LLM의 경우, 학습 데이터 또는 컴퓨팅 리소스의 부족으로 인해, 구조화된 프루닝은 일반적으로 높은 압축률 하에서 손실된 지식을 효과적으로 복구하기 어렵다. 그러나 양자화는 일반적으로 LLM의 성능을 효과적으로 보존할 수 있다. 따라서, 현재 양자화는 LLM 압축에서 더 대중적이고 성숙하게 사용된다. 구조화된 가지치기 및 양자화가 일반적으로 (공동으로) 사용되는 중소형 모델 크기 시나리오(Levy and Zisserman, 2017; Levy and Zisserman, 2018)와 극명한 대조를 이룬다.

### Low-Rank Decomposition

DNN 내의 가중치 매트릭스들은 종종 낮은-순위이며, 이는 모델 가중치들에서의 중복성을 나타낸다(Zhou et al., 2017; Li et al., 2018; Li et al., 2019). 따라서, 자연스러운 아이디어는 가중치를 두 개 이상의 더 작은 행렬로 인수분해하여 파라미터를 절약하는 것이다. LLM에서 가중치 행렬은 자기 주의 계층과 MLP 계층을 포함하는 선형 계층과 임베딩 계층에 존재한다. 파라미터 양을 절약하고 추론을 가속화하기 위해 이러한 가중치 행렬을 인수분해하는 연구가 있다.

**선형 계층에서 분해** 다중 선형 주의(Kumar et al., 2018)는 블록 용어 텐서(BTT) 분해(Kumar et al., 2018)를 사용하여 다중 헤드 주의를 인수분해합니다. 특이값 분해(Singular Value Decomposition; SVD)(Kumar et al., 2018)도 일반적으로 사용되며 통상적으로 이단계 방식으로 수행된다. 첫 번째 단계는 분해를 확립하고 두 번째 단계는 지식 증류를 통해 낮은 순위 가중치를 미세 조정하는 것이다(Kumar et al., 2018). 게다가, BTT 및 SVD의 대안으로서, 크로네커 분해는 매트릭스의 랭크를 유지하고 BERT 및 GPT-2를 압축하는 동안 개선을 보여주었다(Kumar et al., 2018; Li et al., 2019).

**Embedding Layer에 대 한 분해.** ALBERT (Han et al., 2019)는 모델 매개 변수의 가장 큰 소비자 중 하나인 Embedding Layer에 대 한 인수 분해를 사용 합니다. 트랜스포머의 힘은 주로 문맥 학습 능력에서 비롯되기 때문에 토큰 임베딩 계층의 파라미터는 효율적이지 않다. 임베딩 행렬을 인수분해하여 줄이는 것이 직관적으로 타당하다. Self-Attention Factorized embeddings (SAFE) (Zhou et al., 2017)는 대안보다 더 나은 성능을 달성하기 위해 선형 투영을 기반으로 작은 자기 주의 레이어를 추가하여 변압기에서 가중치를 공유하는 방법을 연구했다. LightFormer(Kumar et al., 2018)는 잘 훈련된 Transformer의 파라미터 지식을 보다 효과적으로 활용하고, 임베딩 레이어에 대한 모델 인수분해의 수렴을 가속화한다.

## 8. Conclusion

결론적으로 대형 언어 모델(LLM)의 진화는 인공 지능 분야에서 중요한 이정표를 나타내며 다양한 영역에 걸쳐 변혁적 변화를 가져온다. 그러나 이러한 모델의 급속한 확장은 계산 요구 사항과 메모리 요구 사항 측면에서 상당한 문제를 야기하여 학술 연구와 실용적인 배치 모두에 장애물을 만든다. 이 조사는 LLM의 효율성을 향상시키기 위한 알고리즘 혁신에 대한 포괄적인 개요를 제공했으며, 2023년 9월까지의 연구 개발을 대부분 포착했다. 이 조사는 종종 훈련이나 모델 압축과 같은 측면을 분리하는 데 중점을 둔 기존 조사의 범위를 넘어 LLM의 전체 알고리즘 개발에 중요한 효율성의 여러 차원을 조사했다. 그것은 스케일링 법칙, 데이터 활용, 아키텍처 설계, 교육, 조정 및 추론 전략을 포함한 광범위한 효율성 관련 주제에 걸쳐 있다. 여기에 제시된 통찰력과 분석은 해당 분야의 연구자와 실무자 모두에게 귀중한 요약이 되는 것을 목표로 한다. 본 논문은 현재 지식과 접근 방식의 견고한 기반을 마련함으로써 LLM 효율성의 중요한 연구 영역에서 미래의 돌파구와 지속적인 혁신의 장을 마련한다.

[MISSING_PAGE_EMPTY:25]

* Chen 등(2023) Hongzhan Chen, Siyue Wu, Xiaojun Quan, Rui Wang, Ming Yan, and Ji Zhang. 2023. MCC-KD: Multi-CoT Consistent Knowledge Distillation. _ arXiv preprint arXiv:2310.14747_ (2023).
* Chen 등(2023) Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu, Xuetao Ma, Yifan Yangong, and Junbo Zhao. 2023. 아마도 0.5% 데이터만이 필요할 것이다: 낮은 트레이닝 데이터 명령어 튜닝의 예비 탐색. _ arXiv preprint arXiv:2305.09246_ (2023).
* Chen 등(2021) Jianfiei Chen, Liammin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica, Michael Mahoney, and Joseph Gonzalez. 2021. Actm: 2비트 활성화 압축 트레이닝을 통해 트레이닝 메모리 풋프린트를 감소시킨다. _Machine Learning에 대 한 국제 회의_ 에서입니다. PMLR, 1803-1813.
* Chen et al.(2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. _ arXiv preprint arXiv:2107.03374_ (2021).
* Chen 등(2023) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023. 위치 보간을 통한 대형 언어 모델의 컨텍스트 윈도우 확장_ arXiv preprint arXiv:2306.15595_ (2023).
* Chen 등(2020) Tianyi Chen, Tianyu Ding, Bo Ji, Guanyi Wang, Yixin Shi, Jing Tian, Sheng Yi, Xiao Tu, and Zhihui Zhu. 2020. Orthant Based Proximal Stochastic Gradient Method for l1-Regularized Optimization. _ECML PKDD_에서. 57-73
* Chen 등(2023) Tianyi Chen, Tianyu Ding, Badad Yadav, Ilya Zharkov, and Luming Liang. 2023. LoRSAber: Efficient Large Language Model Structured Pruning and Knowledge Recovery. _ arXiv preprint arXiv:2310.18356_ (2023).
* Chen 등(2021) Tianyi Chen, Bo Ji, Tianyu Ding, Biyi Fang, Guanyi Wang, Zhihui Zhu, Luming Liang, Yixin Shi, Sheng Yi, and Xiao Tu. 2021. Only train once: One-shot neural network training and pruning framework. _ Advances in Neural Information Processing Systems_ (2021), 19637-19651.
* Chen 등(2020) Tianyi Chen, Bo Ji, Yixin Shi, Tianyu Ding, Biyi Fang, Sheng Yi, and Xiao Tu. 2020. 희소 최적화를 통한 신경망 압축 _ arXiv preprint arXiv:2011.04868_ (2020).
* Chen 등(2023) Tianyi Chen, Luming Liang, Tianyu Ding, and Ilya Zharkov. 2023. General Super-Networks 내의 자동 신경 구조 탐색을 향하여. _ arXiv preprint arXiv:2305.18030_ (2023).
* Chen 등(2023) Tianyi Chen, Luming Liang, Tianyu Ding, Zhihui Zhu, and Ilya Zharkov. 2023. OTOv2: Automatic, Generic, User-Friendly. _학습 표현에 관한 제11회 국제 콘퍼런스_에서.
* Chen 등(2020) Tianyi Chen, Guanyi Wang, Tianyu Ding, Bo Ji, Sheng Yi, Zhihui Zhu. 2020. Half-space proximal stochastic gradient method for group-sparsity regularized problem. _ arXiv preprint arXiv:2009.12078_ (2020).
* Chen et al.(2016) Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training deep nets with sublinear memory cost. _ arXiv preprint arXiv:1604.06174_ (2016).
* Chen 등(2023) Tianlong Chen, Zhenyu Zhang, Ajay Jaiswal, Shiwei Liu, and Zhangyang Wang. 2023. Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers. _ arXiv preprint arXiv:2303.01610_ (2023).
* Chen 등(2023) Xin Chen, Hengheng Zhang, Xiaotao Gu, Kaifeng Bi, Lingxi Xie, and Qi Tian. 2023. 파이프라인 MoE: 파이프라인 병렬성을 갖는 유연한 MoE 구현. _ arXiv preprint arXiv:2304.11414_ (2023).
* Chen et al.(2022) Yunhong Chen, Zhenxiang Xiao, Lin Zhao, Lu Zhang, Haixing Dai, David Weizhong Liu, Zihao Wu, Changhe Li, Tuo Zhang, Changying Li, et al. 2022. Mask-guided vision transformer (mg-vii) for few-shot learning. _ arXiv preprint arXiv:2205.09995_ (2022).
* Chen 등(2023) Zeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal, and Kyle Richardson. 2023. DISCO: 대형 언어 모델들로 역사실들을 증류하는 단계. _전산언어학회 제41차 연차총회 회보(제1권: 장문)_에 기재되어 있다. 5514-5528.
* Chi 등(2022) Ta-Chung Chi, Ting Han Fan, Peter J Ramadge, and Alexander Rudnicky. 2022. Kerple: 길이 외삽을 위한 커널화된 상대적 위치 임베딩. _ NeurIPS_35 (2022), 8386-8399).
* Chi 등(2023) Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. 2023. 수용 필드 분석의 렌즈를 통한 변압기 길이 외삽을 해부한다. _ACL_에서. 13522-13537.
* Chiang 등(2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhang, Yonghao Zhang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality.
* Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. _ arXiv preprint arXiv:1904.10509_ (2019).
* Choquette 등(2021) Jack Choquette, Wishwesh Gandhi, Olivier Giroux, Nick Stam, and Ronny Krshinsky. 2021. NVIDIA A100 tensor core GPU: Performance and innovation. _ IEEE Micro_41, 2(2021), 29-35.
* Choromanski et al.(2021) Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. 2021. 재생각 attention with performers. _ ICLR_(2021).
* Chowbary et al. (2022) Aakankankha Chowbary, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. _ arXiv preprint arXiv:2204.02311_ (2022).
* Christiino 등(2017) Paul F Christiino, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. _ NeurIPS_30(2017).
* Chung 등(2023) John Joon Young Chung, Ece Kamar, and Saleema Amershi. 2023. 정확도를 유지하면서 다양성 증대: 대용량 언어 모델 및 인간 개입으로 텍스트 데이터 생성_ arXiv preprint arXiv:2306.04140_ (2023).
* Dai 등(2023) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. 2023. InstructHILP: 명령어 튜닝을 갖는 범용 비전-언어 모델들을 향한다. _ ArXiv_ abs/2305.06500 (2023).

* Dai 등(2019) Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. _ arXiv preprint arXiv:1901.02860_ (2019).
* Dao(2023) Tri Dao. 2023. Flashattention-2: 더 나은 병렬성 및 작업 분할로 더 빠른 주의력. _ arXiv preprint arXiv:2307.08691_ (2023).
* Dao 등(2022) Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Attri Rudra, and Christopher Re. 2022. 모나크: 효율적이고 정확한 훈련을 위한 표현 구조화된 행렬. _ICML_에서. 4690-4721.
* Dao 등(2022) Tri Dao, Dan Fu, Stefafano Ermon, Atri Rudra, and Christopher Re. 2022. Flashattention: io-awareness와 함께 빠르고 메모리 효율적인 정확한 attention. _ NeurIPS_35 (2022), 16344-16359.
* Dao 등(2022) Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Re. 2022. 굶주린 허포: 상태 공간 모델을 사용한 언어 모델링을 향한다. _ arXiv preprint arXiv:2212.14052_ (2022).
* Dao 등(2019) Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher Re. 2019. Learning fast algorithm for linear transform using butterfly factorizations. _ICML_에서. PMLR, 1517-1527.
* Le Lathauwer (2008) Lieven De Lathauwer. 2008. Decomposition of higher-order tensor in block terms-Part II: Definitions and Uniqueness. _ SIAM J. Matrix Anal. Appl._ 30, 3(2008), 1033-1066.
* Dettmers 등(2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. Gpt3. int8() : 스케일에서의 트랜스포머에 대한 8비트 행렬 곱셈. _ NeurIPS_35(2022), 30318-30332.
* Dettmers 등(2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: 양자화된 llms의 효율적인 finetuning. _ arXiv preprint arXiv:2305.14314_ (2023).
* Dettmers 등(2023) Tim Dettmers, Ruslan Svischevski, Vage Egiazarian, Denis Kuzneedelev, Elias Frantar, Saleh Ashkhoos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. 2023. SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression. _ arXiv preprint arXiv:2306.03078_ (2023).
* Ding 등(2023) Jiyu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. 2023. Longnet: Scaling transformers to 1,000,000,000 tokens. _ arXiv preprint arXiv:2307.02486_ (2023).
* Ding 등(2023) Ning Ding, Xingtai Lv, Qiasoen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, and Maosong Sun. 2023. Sparse Low-rank Adaptation of Pre-trained Language Models. _ arXiv preprint arXiv:2311.11696_ (2023).
* Ding 등(2022) Tianyu Ding, Luming Liang, Zhihui Zhu, Tianyi Chen, and Ilya Zharkov. 2022. 프레임 보간을 위한 희소도-유도 네트워크 설계. _ arXiv preprint arXiv:2209.04551_ (2022).
* Ding 등(2021) Tianyu Ding, Luming Liang, Zhihui Zhu, and Ilya Zharkov. 2021. Cdfi: 프레임 보간을 위한 압축-구동 네트워크 설계. _컴퓨터 비전 및 패턴 인식에 관한 IEEE/CVF 회의의 진행사항_에서. 8001-8011.
* Du et al.(2022) Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022. Glam: Mixed-of-experts를 갖는 언어 모델의 효율적인 스케일링. _ICML_에서. PMLR, 5547-5569.
* Edalati 등(2021) Ali Edalati, Marzieh Tahaei, Ahmad Rashid, Vahid Partovi Nia, James J Clark, and Mehdi Rezagholizadeh. 2021. Kronecker decomposition for gpt compression. _ arXiv preprint arXiv:2110.08152_ (2021).
* 엘단 및 리(2023) 로넨 엘단 및 위안지 리. 2023. TinyStories: 언어 모델이 어떻게 작고 여전히 일관성 있는 영어를 말할 수 있는가? _ arXiv preprint arXiv:2305.07759_ (2023).
* Elman (1993) Jeffrey L Elman. 1993. Learning and Development in neural networks: The importance of start small. _ Cognition_48, 1(1993), 71-99.
* Fan et al.(2021) Shiqing Fan, Yi Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen Zheng, Chuan Wu, Guoping Long, Jun Yang, Lixue Xia, et al. 2021. DAPPLE: A Pipelined Data Parallel Approach for Training Large Models. (2021).
* Fang 등(2023) Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang. 2023. Depgraph: any structural pruning. _ arXiv preprint arXiv:2301.12900_ (2023).
* Fedus 등(2022) William Fedus, Barret Zoph, and Noam Shazeer. 2022. 스위치 트랜스포머: 간단하고 효율적인 희소성을 갖는 조 단위의 파라미터 모델들로 스케일링. _ The Journal of Machine Learning Research_23, 1(2022), 5232-5270.
* Frantar 등(2022) Elias Frantar, Saleh Ashkhoos, Torsten Hoefler, and Dan Alistarh. 2022. Gptq: 생성 사전 훈련된 트랜스포머에 대한 정확한 사후 훈련 양자화. _ arXiv preprint arXiv:2210.17323_ (2022).
* Fu 등(2023) Daniel Y Fu, Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, Armin W Thomas, Benjamin Spector, Michael Poli, Atri Rudra, and Christopher Re. 2023. 모나크 믹서: 간단한 서브-이차 GEMM-기반 아키텍처. _NeurIPS_에서입니다.
* Gal 등(2017) Yarin Gal, Rishad Islam, Zoubin Ghahramani. 2017. Deep bayesian active learning with image data. _ICML_에서. PMLR, 1183-1192.
* Gae et al. (1992) William A Gae, Kenneth W Church, and David Yarowsky. 1992. A method for disambiguation of word sense in large corpus. _ Computers and the Humanities_ 26 (1992), 415-439.
* Gao 등(2020) Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. 사전 훈련 된 언어 모델을 보다 적은 수의 학습자로 만들 수 있습니다. _ arXiv preprint arXiv:2012.15723_ (2020).
* Geiping and Goldstein (2023) Jonas Geiping and Tom Goldstein. 2023. Cramming: 하루 만에 하나의 GPU에서 언어 모델을 학습시킨다.. _ICML_에서. PMLR, 11117-11143.
* Greva 등(2021) Mor Greva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021년. 아리스토텔레스가 노트북을 사용했나요? 암묵적 추론 전략을 사용한 질문 응답 벤치마크 _ TACL_9 (2021), 346-361.
* Gholami 등(2022) Amir Gholami, Kim Sehoon, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. 2022. 효율적인 신경망 추론을 위한 양자화 방법들의 조사. [저전력 컴퓨터 비전]에 있습니다. Chapman and Hall/CRC, 291-326.
* Gissin and Shalev-Shwartz (2019) Daniel Gissin and Shai Shalev-Shwartz. 2019. Discrimative Active Learning. _ arXiv preprint arXiv:1907.06347_ (2019).

* Gou 등(2021) Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. 2021. Knowledge Distillation: A survey. _ IJCV_ 129 (2021), 1789-1819.
* Gu et al.(2022) Albert Gu, Karan Goel, Ankit Gupta, and Christopher Re. 2022. 대각 상태 공간 모델들의 파라미터화 및 초기화에 관하여. _ NeurIPS_35(2022), 35971-35983.
* Gu et al.(2021) Albert Gu, Karan Goel, and Christopher Re. 2021. 구조화된 상태 공간을 사용하여 긴 시퀀스를 효율적으로 모델링합니다. _ arXiv preprint arXiv:2111.00396_ (2021).
* Gu et al.(2023) Yunian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023. 대용량 언어 모델의 지식 증류. _ arXiv preprint arXiv:2306.08543_ (2023).
* Gunasekar et al.(2023) Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikiv, et al. 2023. Textbooks Are All You Need. _ arXiv preprint arXiv:2306.11644_ (2023).
* Guo 등(2021) Demi Guo, Alexander M Rush, and Yoon Kim. 2021. Diff Pruning을 이용한 파라미터 효율적인 전이 학습. _ACL-IJCNLP_4884-4896에서.
* Gupta 등(2022) Ankit Gupta, Albert Gu, and Jonathan Berant. 2022. 대각 상태 공간은 구조화된 상태 공간만큼 효과적이다. _ NeurIPS_35 (2022), 22982-22994.
* 교르기 및 티쉬비(1990) 게자 교르기 및 나프탈리 티쉬비. 1990. Statistical theory of learning a rule. _ Neural networks and spin glasses_ (1990), 3-36.
* Ham 등(2020) Ham 태준, Sung Jun Jung, Sunghak Kim, Young H Oh, Yeonhong Park, Yoonho Song, Jung-Hun Park, Sanghee Lee, Kyung Park, Jae W Lee, et al. 2020. A'3: Accelerating attention mechanisms in neural networks with approximation. _HPCA_에서. 328-341
* Ham 등(2021) 함태준, 이예진, 서성훈, 김수성, 최현지, 정성준, 이재우. 2021. ELSA: 신경망에서 효율적이고 경량의 셀프-어텐션 메커니즘을 위한 하드웨어-소프트웨어 공동 설계. _ISCA_에서. 692-705
* Harlap 등(2018) Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg Ganger, and Phil Gibbons. 2018. Pipedream: Fast and efficient pipeline parallel dnn training. _ arXiv preprint arXiv:1806.03377_ (2018).
* Hassid 등(2022) Michael Hassid, Hao Peng, Daniel Rotem, Jungo Kasai, Ivan Montero, Noah A Smith, and Roy Schwartz. 2022. 실제로 얼마나 많은 관심이 참석하나요? 사전 훈련된 변압기에서 주의의 중요도에 대해 질문합니다. _ ACL Findings_ (2022).
* Hendy 등(2023) Amr Hendy, Mohamed Abdelrahim, Amr Sharaf, Vikas Rausk, Mohamed Gahr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. 기계 번역에서 gpt 모델은 얼마나 좋은가요? 종합적인 평가. _ arXiv preprint arXiv:2302.09210_ (2023).
* Henighan et al. (2020) Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. 2020. Scaling laws for autoregressive generationative modeling. _ arXiv preprint arXiv:2010.14701_ (2020).
* Hernandez 등(2021) Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 2021. Scaling laws for transfer. _ arXiv preprint arXiv:2102.01293_ (2021).
* Hestness 등(2017) Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. 2017. Deep learning scaling is predictable, empirically. _ arXiv preprint arXiv:1712.00409_ (2017).
* Hinton 등(2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in the neural network. _ arXiv preprint arXiv:1503.02531_ (2015).
* Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long short-term memory. _ Neural computation_9, 8(1997), 1735-1780.
* Hoffmann et al.(2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. _ arXiv preprint arXiv:2203.15556_ (2022).
* Houlsby 등(2019) Neil Houlsby, Andrei Giurgui, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. _ICML_에서. PMLR, 2790-2799.
* Hsieh 등(2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023. 단계별 증류! 더 적은 학습 데이터와 더 작은 모델 크기로 더 큰 언어 모델을 수행할 수 없습니다. _ arXiv preprint arXiv:2305.02301_ (2023).
* Hu 등(2021) Edward J Hu, Yelong Shen, Phillip Walls, Zeyuan Allen-Zhu, Yuanli Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. _ arXiv preprint arXiv:2106.09685_ (2021).
* Hu 등(2010) Rong Hu, Brian Mac Namee, Sarah Jane Delany. 2010. Off to good start: Clustering을 이용하여 능동적 학습에서 초기 트레이닝 세트를 선택한다. (2010).
* Huang and Chang (2022) Jie Huang and Kevin Chen-Chuan Chang. 2022. 대언어 모델에서의 추론을 향하여: 설문조사. _ arXiv preprint arXiv:2212.10403_ (2022).
* Huang et al. (2019) Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, Hyouk Joongong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. 2019. Gpipe: 파이프라인 병렬성을 이용한 거대 신경망의 효율적인 훈련_ NeurIPS_32(2019).
* Huang 등(2023) Yafeng Huang, Huanrui Yang, Zhen Dong, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Yuan Du, Shanghang Zhang, and Kurt Keutzer. 2023. Output Sensitivity-Aware DETR Quantization. (2023).
* Hubara 등(2017) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 2017. 양자화 신경망: 낮은 정밀도의 가중치와 활성화로 신경망을 훈련시킨다. _ The Journal of Machine Learning Research_ 18, 1(2017), 6869-6898.
* Jaiswal 등(2023) Ajay Jaiswal, Shiwei Liu, Tianlong Chen, and Zhangyang Wang. 2023. 대형 사전 훈련 모델에서 필수 희소성의 출현: 중요한 가중치. _ arXiv preprint arXiv:2306.03805_ (2023).
* Jelinek (1998) Frederick Jelinek. 1998. _음성 인식을 위한 통계적 방법_. MIT 프레스.
* Ji and Chen (2022) Bo Ji and Tianyi Chen. 2022. FSCNN: 빠른 희소 컨볼루션 신경망 추론 시스템. _ arXiv preprint arXiv:2212.08815_ (2022).
* Jia 등(2019) Zhihao Jia, Matei Zaharia, and Alex Aiken. 2019. Beyond Data and Model Parallelism for Deep Neural Networks. _ Proceedings of Machine Learning and Systems_ 1 (2019), 1-13.
* Jiang et al.(2019) Angela H Jiang, Daniel L-K Wong, Giulio Zhou, David G Andersen, Jeffrey Dean, Gregory R Ganger, Gauri Joshi, Michael Kaminksy, Michael Kozuch, Zachary C Lipton, et al. 2019. Accelerating deep learning by focusing on the largest losers. _ arXiv preprint arXiv:1910.00762_ (2019).
* Jiang 등(2023) Yuxin Jiang, Chunkit Chan, Mingyang Chen, and Wei Wang. 2023. Lion: Closed-Source Large Language Model의 적대적 증류. _ arXiv preprint arXiv:2305.12870_ (2023).

* Jiao 등(2019) Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding. _ arXiv preprint arXiv:1909.10351_ (2019).
* Kadotani 등(2021) Sora Kadotani, Tomoyuki Kajiwara, Yuki Arase, and Makoto Onizuka. 2021. Edit distance based curriculum learning for paraphrase generation. _ACL-IJCNLP Workshop_에서. 229-234
* Kalamkar et al. (2019) Dhiraj Kalamkar, Dheevats Mukojere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammmalamadaka, Jianyu Huang, Hector Yuen, et al. 2019. A study of BFLOAT16 for deep learning training. _ arXiv preprint arXiv:1905.12322_ (2019).
* Kaplan 등(2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. _ arXiv preprint arXiv:2001.08361_ (2020).
* Katharopoulos and Fleuret (2017) Angelos Katharopoulos and Francois Fleuret. 2017. Biased importance sampling for deep neural network training. _ arXiv preprint arXiv:1706.00043_ (2017).
* Katharopoulos and Fleuret (2018) Angelos Katharopoulos and Francois Fleuret. 2018. 모든 샘플이 동일하게 생성되는 것은 아니다: 중요도 샘플링을 갖는 딥 러닝. _기계 학습에 대 한 국제 회의_ 에서입니다. PMLR, 2525-2534.
* Kazemmejad 등(2023) Amihossein Kazemmejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. 2023. Positional Encoding이 Transformers의 Length Generalization에 미치는 영향. _ NeurIPS_(2023).
* Ming-Wei Chang Kenton and Toutanova (2019) Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT: 언어 이해를 위한 Deep Bidirectional Transformers Pre-training. _NAACL-HLT_에서. 4171-4186.
* Kim 등(2021) Kim Sehoon, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. 2021. I-bert: Integer-only bert 양자화. _ICML_에서. PMLR, 5506-5518.
* Kim 등(2023) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. 2023. SqueezeLLM: Dense-and-Sparse Quantization. _ arXiv preprint arXiv:2306.07629_ (2023).
* Kim 등(2023) Thebaum Kim, Hyoungio Kim, Kyung-In Yu, and Byung-Gon Chun. 2023. BPipe: Memory-Balanced Pipeline Parallelism for Training Large Language Models. 「제40회 기계 학습에 관한 국제 회의의 회보」(기계 학습 연구의 회보, 제202권)_에서, 안드레아스 크라우세, 엠마 브룬스킬, 조경현, 바바라 엥겔하르트, 시반 사바토, 조나단 스칼렛(Eds.) 16639-16653에 기재되어 있다.
* Kim and Rush (2016) Yoon Kim and Alexander M Rush. 2016. Sequence-level Knowledge Distillation. _ arXiv preprint arXiv:1606.07947_ (2016).
* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. _ arXiv preprint arXiv:1412.6980_ (2014).
* Kirsch 등(2019) Andreas Kirsch, Joost Van Amersfoort, and Yarin Gal. 2019. Batchbald: deep bayesian active learning을 위한 효율적이고 다양한 batch acquisition. _ NeurIPS_32(2019).
* Kitaev 등(2021) Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2021. Reformer: The efficient transformer. _ ICLR_(2021).
* Kocmi and Bojar (2017) Tom Kocmi and Ondrej Bojar. 2017. Curriculum learning and minibatch Bucketing in neural machine translation. _ arXiv preprint arXiv:1707.09533_ (2017).
* Kojima 등(2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. 대형 언어 모델은 제로 샷 추론기입니다. _ NeurIPS_35(2022), 22199-22213.
* Komatsuzaki (2019) Aran Komatsuzaki. 2019. One epoch만 있으면 됩니다. _ arXiv preprint arXiv:1906.06669_ (2019).
* Korthikanti 등(2023) Vijay Anand Korthikanti, Jared Casper, Sangkug Iym, Lawrence McKee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. 2023. 대형 변압기 모델에서의 활성화 재계산 감소. _ Proceedings of Machine Learning and Systems_5 (2023).
* Kosson 등(2021) Arik Kosson, Vitally Chiley, Abhinav Venigalla, Joel Hestness, and Urs Koster. 2021. 파이프라인 역 전파: 대규모 모델을 배치 없이 훈련합니다. _ Proceedings of Machine Learning and Systems_ 3 (2021), 479-501.
* Kovaleva 등(2019) Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019년 BERT의 어둠의 비밀 공개 _EMNLP-IJCNLP_에서. 4365-4374
* Kumar 등(2010) M Kumar, Benjamin Packer, and Daphne Koller. 2010. Self-paced learning for latent variable models. _ NeurIPS_23 (2010).
* Kurtic 등(2023) Eldar Kurtic, Denis Kuzmedekiev, Elias Frantar, Michael Goin, and Dan Alistarh. 2023. 대언어 모델의 추론 가속을 위한 희소 Finetuning. _ arXiv preprint arXiv:2310.06927_ (2023).
* Kwon 등(2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhang, Ying Sheng, Liannin Zheng, Cody Yu, Joey Gonzalez, Hao Zhang, and Ion Stoica. 2023. vllm: 페이징된 주의력으로 쉽고, 빠르고, 저렴한 llm 서빙.
* Lan 등(2019) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Kimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representation. _ arXiv preprint arXiv:1909.11942_ (2019).
* Lan 등(2020) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Kimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. _ICLR_에서.
* Lee et al.(2019) Jaejun Lee, Raphael Tang, and Jimmy Lin. 2019년 엘사는 어떻게 할까? 트랜스포머 미세 조정 중 레이어를 동결합니다. _ arXiv preprint arXiv:1911.03090_ (2019).
* Lee 등(2020) 이진혁, 윤원진, 김성동, 김동현, 김선규, 소찬호, 강재우. 2020. BioBERT: 생체 의학 텍스트 마이닝을 위한 사전 훈련된 생체 의학 언어 표현 모델. _ Bioinformatics_ 36, 4(2020), 1234-1240.
* Lee 등(2021) Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2021. Deduplicating training data makes language models. _ arXiv preprint arXiv:2107.06499_ (2021).
* Lepikhin 등(2020) Dmitry Lepikhin, Hyouk중 Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020. Gshard: Scaling giant models with conditional computation and automatic sharding. _ arXiv preprint arXiv:2006.16668_ (2020).
* Leskovec 등(2014) J Leskovec, A Rajaraman, and JD Ullman. 2014. Mining of Massive Datasets, Cambridge University Press, Cambridge.

* Lewis (1995) David D Lewis. 1995. A sequential algorithm for training text classifier: Corrigendum and additional data. _Acm Sigir Forum_, Vol. 29. ACM New York, NY, USA, 13-19.
* Lewis 등(2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. _ACL_에서. 7871-7880.
* Li 등(2021) Conglong Li, Minjia Zhang, and Yuxiong He. 2021. Curriculum learning: 효율적이고 안정적인 billion-scale gpt model pre-training을 위한 정규화 방법. (2021).
* Li 등(2022) Conglong Li, Minjia Zhang, and Yuxiong He. 2022. 안정성-효율 딜레마: GPT 모델들을 트레이닝하기 위한 시퀀스 길이 워밍업을 조사한다. _ NeurIPS_35 (2022), 26736-26750.
* Li 등(2023) Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. 2023. Symbolic Chain-of-Thought Distillation: Small Models Can Can'Think' step-by-Step. _ arXiv preprint arXiv:2306.14050_ (2023).
* Li 등(2014) Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. 2014. Scaling distributed machine learning with the parameter server. _11th USENIX Symposium on operating systems design and implementation (OSDI 14)_에서. 583-598.
* Li 등(2021) Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, Yang You. 2021. Sequence Parallelism: 시스템 관점에서 긴 Sequence training. _ arXiv preprint arXiv:2105.13120_ (2021).
* Li 등(2023) Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sangghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli. 2023. 상대 위치를 위한 기능적 보간은 긴 컨텍스트 트랜스포머를 향상시킨다. _ arXiv preprint arXiv:2310.04418_ (2023).
* Li et al. (2020) Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. 2020. PyTorch distributed experiences on accelerating data parallel training. _ Proceedings of the VLDB Endowment_13, 12 (2020), 3005-3018.
* Li 등(2023) Xiuyu Li, Long Lian, Yijiang Liu, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. 2023. Q-diffusion: Quantizing diffusion models. _ arXiv preprint arXiv:2302.04304_ (2023).
* Li and Liang (2021) Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: 생성을 위한 연속 프롬프트 최적화_ arXiv preprint arXiv:2101.00190_ (2021).
* Li 등(2023) Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks Are All You Need II: phi-1.5 technical report. _ arXiv preprint arXiv:2309.05463_ (2023).
* Li 등(2021) Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, Dawn Song, and Ion Stoica. [n. d.]. TeraPipe: 대용량 언어 모델 학습을 위한 토큰 수준 파이프라인 병렬 처리입니다. ([n. d.]).
* Lin 등(2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. 2023. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. _ arXiv preprint arXiv:2306.00978_ (2023).
* Liu and Abbeel (2023) Hao Liu and Pieter Abbeel. 2023. Long Context Large Models용 Blockwise Parallel Transformer. _ arXiv preprint arXiv:2305.19370_ (2023).
* Liu 등(2023) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. _ arXiv preprint arXiv:2304.08485_ (2023).
* Liu 등(2023) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _ 컴퓨팅. Surveys_55, 9(2023), 1-35.
* Liu 등(2020) Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Haotang Deng, and Qi Ju. 2020. Fastbert: a self-distilling bert with adaptive inference time. _ arXiv preprint arXiv:2004.02178_ (2020).
* Liu 등(2023) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2023. GPT도 이해 합니다. _ AI Open_(2023).
* Liu 등(2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: Robustly optimized bert pretraining approach. _ arXiv preprint arXiv:1907.11692_ (2019).
* Liu 등(2023) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. 2023. LLM-QAT: 대용량 언어 모델에 대한 데이터 프리 양자화 인식 훈련. _ arXiv preprint arXiv:2305.17888_ (2023).
* Liu 등(2020) Zhining Liu, Pengfei Wei, Jing Jiang, Wei Cao, Jiang Bian, and Yi Chang. 2020. MESA: boost ensemble imbalanced learning with meta-sampler. _ NeurIPS_ 33 (2020), 14463-14474.
* Long(2023) Jieyi Long. 2023. Large Language Model Guided Tree-of-Thought. _ arXiv preprint arXiv:2305.08291_ (2023).
* Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. _ arXiv preprint arXiv:1711.05101_ (2017).
* Lv 등(2023) Xiuqing Lv, Peng Zhang, Sunzhu Li, Guobing Gan, and Yueheng Sun. 2023. LightFormer: SVD 기반 가중치 전달 및 파라미터 공유를 이용한 경량 변압기. _Findings of the Association for Computational Linguistics: ACL 2023_. 10323-10335.
* Ma 등(2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. LLM-Pruner: 대형 언어 모델의 구조적 프루닝에 관한 것이다. _ arXiv preprint arXiv:2305.11627_ (2023).
* Ma 등(2019) 신디언 마, 펑장, 슈아이 장, 난두안, 유엑시안 호우, 밍저우, 다웨이 송. 2019. A tensorized transformer for language modeling. _ 신경 정보 처리 시스템_32(2019).
* Ma et al.(2023) Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. 2023. 메가: 이동 평균 장착 게이트 어텐션. _ ICLR_(2023).
* Maekawa 등(2022) Seiji Maekawa, Dan Zhang, Hannah Kim, Sajjadur Rahman, and Eatevam Hruschka. 2022. Low-resource interactive active labeling for fine-tuning language models. _Findings of EMNLP_. 3230-3242.
* Margatina 등(2021) Katerina Margatina, Giorgos Vernikos, Loic Barrault, and Nikolaos Aletras. 2021. Active learning by acquire contrast contrast examples. _ arXiv preprint arXiv:2109.03764_ (2021).

* Martins 등(2022) Pedro Henrique Martins, Zita Marinho, Andre FT Martins. 2022. \(inf\)\(f\)-former: 무한 메모리 트랜스포머. _ ACL_ (2022).
* Mehta 등(2023) Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Belham Neyshabur. 2023. gated state spaces를 통한 Long range language modeling. _ ICLR_(2023).
* Micikevicius et al.(2017) Paulus Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Olekski Kuchaiev, Ganesh Venkatesh, et al. 2017. Mixed precision training. _ arXiv preprint arXiv:1710.03740_ (2017).
* Mikolov et al.(2010) Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model.. _Interspeech_, Vol. 2. Makhaturi, 1945-1048.
* Mikolov 등(2011) Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. 2011. Extensions of Recurrent Neural Network Language Model. _2011 IEEE International conference on acoustics, speech and signal processing (ICASSP)_ 에서. IEEE, 5528-5531.
* Mishra and Sachdeva (2020) Swaroop Mishra and Bhavedeep Singh Sachdeva. 2020. 작업을 학습하기 위해 큰 데이터 세트를 만들어야 합니까? _SustainNLP Workshop_ 에서입니다. 169-173
* Muemighoff 등(2023) Niklas Muemighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. 2023. Scaling Data-Constrained Language Models. _ arXiv preprint arXiv:2305.16264_ (2023).
* Mustafa 등(2022) Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. 2022. Multimodal contrastive learning with limo: language-image mixture of experts. _ NeurIPS_35 (2022), 9564-9576.
* Nair 등(2023) Lakshmi Nair, Mikhail Bernadskiy, Arulesivan Madhavan, Craig Chan, Ayon Basumallik, and Darius Bunandar. 2023. INT-FP-QSim: 대형 언어 모델 및 비전 트랜스포머를 위한 혼합 정밀도 및 포맷. _ arXiv preprint arXiv:2307.03712_ (2023).
* Narayanan 등(2019) Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. 2019. PipeDream: Generalized pipeline parallelism for DNN training. 《운영체제 원리에 관한 제27차 ACM 심포지엄의 진행사항》에서. 1-15
* Narayanan 등(2021) Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. 2021. Memory-efficient pipeline-parallel dnn training. _Machine Learning에 대 한 국제 회의_ 에서입니다. PMLR, 7937-7947.
* Narayanan et al.(2021) Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. 2021. megatron-lm을 이용한 gpu 클러스터에 대한 효율적인 대규모 언어 모델 트레이닝. _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_. 1-15
* Noach and Goldberg (2020) Matan Ben Noach and Yoav Goldberg. 2020. 행렬 분해에 의한 사전 훈련된 언어 모델 압축. 전산언어학회 아시아태평양 제1차 회의와 제10차 자연어처리 국제공동회의의 회보_에서. 884-889
* Oh 등(2022) 오상윤, 심현욱, 김정현, 이종은. 2022. 정확한 훈련 후 양자화를 위한 불균일한 스텝 크기 양자화. _유럽 컴퓨터 비전 회의_ 에서. 스프링거, 658-673
* OpenAI(2023) OpenAI. 2023. GPT-4 Technical Report. _ ArXiv_ abs/2303.08774 (2023).
* Ouyang et al.(2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. _ NeurIPS_35 (2022), 27730-27744).
* Patel 등(2021) Arkil Patel, Satwik Bhattacharya, Navin Goyal. 2021. NLP 모델이 정말 간단한 수학 단어 문제를 해결할 수 있을까요? _ACL_에서. ACL, 2080-2094.
* Peng et al.(2023) Bo Peng, Eric Alcaie, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. 2023. RWRV: Reinventing RNNs for Transformer Era. _ arXiv preprint arXiv:2305.13048_ (2023).
* Peng 등(2023) Bo Peng, Ben Burns, Ziti Chen, Srinivasan Parthasarathy, and Xia Ning. 2023. 순차적 추천을 위한 대용량 언어 모델의 효율적이고 효과적인 적응을 위하여. _ arXiv preprint arXiv:2310.01612_ (2023).
* Peng 등(2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarm: 대용량 언어 모델들의 효율적인 컨텍스트 윈도우 확장. _ arXiv preprint arXiv:2309.00071_ (2023).
* Peters 등(2018) Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representation. _NAACL-HLT_에서. 2227-2237.
* Pfeiffer 등(2020) Jonas Pfeiffer, Andreas Ruckle, Clifton Poth, Aishwarya Kamath, Ivan Vulic, Sebastian Ruder, Kyungghyun Cho, and Iryua Gurevych. 2020. Adapterhub: A framework for adapters. _ arXiv preprint arXiv:2007.07779_ (2020).
* Pfeiffer 등(2020) Jonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Sebastian Ruder. 2020. Mad-x: An adapter based framework for multi-task cross-lingual transfer. _ arXiv preprint arXiv:2005.00052_ (2020).
* Poli 등(2023) Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. 2023. Hyena hierarchy: 더 큰 convolutional language model을 향합니다. _ arXiv preprint arXiv:2302.10866_ (2023).
* Press 등(2020) Ofir Press, Noah A Smith, and Mike Lewis. 2020. Shortformer: 더 짧은 입력을 사용하는 언어 모델링이 더 좋습니다. _ arXiv preprint arXiv:2012.15832_ (2020).
* Press et al.(2023) Ofir Press, Noah A Smith, and Mike Lewis. 2023. Train short, test long: Attention with linear bias enables input length extrapolation. _ ICLR_(2023).
* Prusa 등(2015) Joseph Prusa, Taghi M Khoshgoftaar, David J Dittman, and Amri Napolitano. 2015. Random undersampling을 사용하여 트윗 감성 데이터의 클래스 불균형을 완화한다. _2015 IEEE 국제회의에서 정보 재사용 및 통합에 관한 것이다. IEEE, 197-202.
* Radford et al.(2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Generative Pre-training에 의한 언어 이해력 향상. (2018).
* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are notsupervised multitask learners. _ OpenAI blog_1, 8(2019), 9.
* Rae et al. (2021) Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis & insights from training gopher. _ arXiv preprint arXiv:2112.11446_ (2021).

* Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_ 21, 1 (2020), 5485-5551.
* Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. _Journal of Machine Learning Research_ 21, 140 (2020), 1-67.
* Rajbhandari et al. [2022] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. 2022. Deepspeed-more: Advancing mixture-of-experts inference and training to power next-generation ai scale. In _ICML_. 18332-18346.
* Rajbhandari et al. [2020] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In _SC_. 1-16.
* Rasley et al. [2020] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In _SIGKDD_. 3505-3506.
* Reid et al. [2021] Michel Reid, Edison Marrese-Taylor, and Yutaka Matsuo. 2021. Subformer: Exploring weight sharing for parameter efficiency in generative transformers. _arXiv preprint arXiv:2101.00234_ (2021).
* Ren et al. [2021] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. 2021. ZeRO-Offload: Democratizing Billion-Scale Model Training. _arXiv preprint arXiv:2101.06840_ (2021).
* Robinson and Wingate[2022] Joshua Robinson and David Wingate. 2022. 대용량 언어 모델을 이용한 객관식 질의 응답 방법. _ICLR_에서.
* Romero et al. [2014] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. 2014. Fitnets: Hints for thin deep nets. _arXiv preprint arXiv:1412.6550_ (2014).
* Rosenfeld et al. [2019] Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. 2019. A constructive prediction of the generalization error across scales. _arXiv preprint arXiv:1909.12673_ (2019).
* Rosenfeld[2000] Ronald Rosenfeld. 2000. 20년 통계 언어 모델링: 이제 어디로 가나요? _ IEEE_88, 8(2000), 1270-1278.
* Ruckle et al. [2021] Andreas Ruckle, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych. 2021. AdapterDrop: On the Efficiency of Adapters in Transformers. In _EMNLP_. 7930-7946.
* Ruoss et al. [2023] Anian Ruoss, Gregoire Deletang, Tim Genewein, Jordi Grau-Moya, Robert Csordas, Mehdi Bennani, Shane Legg, and Joel Veness. 2023. Randomized Positional Encodings Boost Length Generalization of Transformers. _arXiv preprint arXiv:2305.16843_ (2023).
* Sahu et al. [2023] Gaurav Sahu, Olga Vechtomova, Dzmitry Bahdanau, and Issam H Laradji. 2023. Promptmix: A class boundary augmentation method for large language model distillation. _arXiv preprint arXiv:2310.14192_ (2023).
* Sainath et al. [2013] Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. 2013. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In _2013 IEEE international conference on acoustics, speech and signal processing_. IEEE, 6655-6659.
* Salton and Buckley [1990] Gerard Salton and Chris Buckley. 1990. Improving retrieval performance by relevance feedback. _ Journal of the American society for information science_41, 4(1990), 288-297.
* Sanh et al. [2019] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. _arXiv preprint arXiv:1910.01108_ (2019).
* Sredker [2010] AMissing SERET SAUCE. [n. d.]. OUTLIER WEIGHED LAYERWISE SPARSITY (OWL): LIMS를 높은 SPARSITY로 PRUNING SERET SUCE. ([n. d.]).
* [26] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_ (2022).
* Sener and Savarese [2017] Ozan Sener and Silvio Savarese. 2017. Active learning for convolutional neural networks: A core-set approach. _ arXiv preprint arXiv:1708.00489_ (2017).
* Settles [2017] Burr Settles. 2009. Active Learning literature survey. 위스콘신대학교 컴퓨터과학과
* Settles [2011] Burr Settles. 2011. theories to query: Active learning in practice. AISTATS 2010_과 연계한 능동 학습 및 실험 설계 워크숍에서. JMLR Workshop and Conference Proceedings, 1-18.
* Settles [2012] Burr Settles. 2012. Active Learning. 인공지능과 기계 학습에 관한 강의들 1-114.
* Seung et al. [1992] Hyunjune Sebastian Seung, Haim Sompolinsky, and Naftali Tishby. 1992. Statistical mechanics of learning from examples. _Physical review A_ 45, 8 (1992), 6056.
* Shanahan[2022] Murray Shanahan. 2022. 대형 언어 모델에 대해 설명합니다. _ arXiv preprint arXiv:2212.03551_ (2022).
* Shao et al. [2023] Hang Shao, Bei Liu, and Yanmin Qian. 2023. One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models. _arXiv preprint arXiv:2310.09499_ (2023).
* Shazeer et al. [2018] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koaantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. 2018. Mesh-tensorflow: Deep learning for supercomputers. _Advances in neural information processing systems_ 31 (2018).
* Shazeer et al. [2017] Noam Shazeer, Aazalia Mirhoseini, Krzysztof Maziraz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. _ICLR_ (2017).
* Shazeer and Stern [2018] Noam Shazeer and Mitchell Stern. 2018. Adafactor: sublinear memory cost를 갖는 Adaptive learning rate. _Machine Learning에 대 한 국제 회의_ 에서입니다. PMLR, 4596-4604.
* Shen et al. [2023] Li Shen, Yan Sun, Zhiyuan Yu, Liang Ding, Xinmei Tian, and Dacheng Tao. 2023. On Efficient Training of Large-Scale Deep Learning Models: A Literature Review. _arXiv preprint arXiv:2304.03589_ (2023).

* Shen 등(2020) Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Q-bert: Hessian based ultra low precision quantization of bert. _AAAI_에서, Vol. 34. 8815-8821.
* Shen et al.(2023) Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, et al. 2023. Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts. _ arXiv preprint arXiv:2305.14705_ (2023).
* Shin 등(2020) Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: 자동으로 생성된 프롬프트를 사용하여 언어 모델에서 지식을 이끌어냅니다. _ arXiv preprint arXiv:2011.15980_ (2020).
* Shoeybi 등(2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: 모델 병렬 처리를 사용 하 여 수십억 매개 변수 언어 모델을 훈련 합니다. _ arXiv preprint arXiv:1909.08053_ (2019).
* Siddhant and Lipton (2018) Aditya Siddhant and Zachary C Lipton. 2018. Deep bayesian active learning for natural language processing: Results of the large-scale empirical study. _ arXiv preprint arXiv:1808.05697_ (2018).
* Siddiqui 등(2022) Shoaib Ahmed Siddiqui, Nitarshan Rajkumar, Tegan Maharaj, David Krueger, and Sara Hooker. 2022. 메타데이터 고고학: 학습 역학을 활용하여 데이터 하위 집합을 발굴합니다. _ arXiv preprint arXiv:2209.10015_ (2022).
* Sorscher 등(2022) Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. 2022. Beyond neural scaling laws: beating power law scaling via data pruning. _ NeurIPS_35 (2022), 19523-19536.
* Su et al. (2022) Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayu Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. 2022. Selective annotation makes language models better few-shot learners. _ arXiv preprint arXiv:2209.01975_ (2022).
* Su et al.(2021) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu. 2021. Roformer: 회전 위치 매립을 갖는 향상된 변압기. _ arXiv preprint arXiv:2104.09864_ (2021).
* Su 등(2023) Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. 2023. Pandagpt: one model to instruction-follow them all. _ arXiv preprint arXiv:2305.16355_ (2023).
* Sujit 등(2022) Shivakanth Sujit, Somjit Nath, Pedro HM Braga, and Samira Ebrahimi Kahou. 2022. 환원성 손실을 갖는 강화 학습에서 샘플들의 우선순위 결정_ arXiv preprint arXiv:2208.10483_ (2022).
* Sun et al.(2023) Mingjie Sun, Zhu Liu, Anna Bair, and J Zico Kolter. 2023. Simple and Effective Pruning Approach for Large Language Models. _ arXiv preprint arXiv:2306.11695_ (2023).
* Sun 등(2023) Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. 2023. Retentive network: A successor to transformer for large language models. _ arXiv preprint arXiv:2307.08621_ (2023).
* Sun 등(2022) Zhiqing Sun, Yiming Yang, and Shinjae Yoo. 2022. 해쉬에 대한 학습으로 관심을 희소화한다. _ICLR_에서.
* Tahaei 등(2021) Marzieh S Tahaei, Ella Charlaix, Vahid Partovi Nia, Ali Ghodsi, and Mehdi Rezagholizadeh. 2021. Kroneckerbert: 지식 증류를 통해 사전 훈련된 언어 모델에 대한 학습 Kronecker 분해_ arXiv preprint arXiv:2109.06243_ (2021).
* Talmor 등(2019) Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: Question Answering Challenge Targeting Commonsense Knowledge. _ACL_에서. ACL, 미네소타, 미네소타, 4149-4158
* Tang 등(2002) Min Tang, Xiaoguang Luo, and Salim Roukos. 2002. Active learning for statistical natural language parsing. _ACL_에서. 120-127
* Taori 등(2023) Rohan Taori, Ishan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: Instruction-following LLAA 모델. [https://github.com/tatsu-lab/stanford_alpaca] (https://github.com/tatsu-lab/stanford_alpaca).
* Tay 등(2022) Yi Tay, Mostafa Dehghani, Samira Ahnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. 2022. 스케일링 법칙 대 모델 아키텍처: 귀납적 바이어스가 스케일링에 어떻게 영향을 미치는가? _ arXiv preprint arXiv:2207.10551_ (2022).
* Tay 등(2022) Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2022. 효율적인 트랜스포머: Survey. _ ACM Comput. Surv._ 55, 6, 제109조(2022년 12월), 28쪽.
* Tay 등(2021) Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Ahnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. 2021. Scalely: Pretraining and Finetuning Transformers로부터의 Insights. _학습 표현에 대 한 국제 회의_ 에서입니다.
* Thoppilan et al.(2022) Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. _ arXiv preprint arXiv:2201.08239_ (2022).
* Touvron et al.(2023) Hugo Touvron, Thibaut Lavril, Gautier Izcard, Xavier Martinet, Marie-Anne Lachaux, Timothete Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Achar, et al. 2023. Llama: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_ (2023).
* Touvron et al.(2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_ (2023).
* Treviso 등(2021) Marcos Treviso, Antonio Gois, Patrick Fernandes, Eric Fonseca, Andre FT Martins. 2021. Prediction attention sparsity in Transformers. _ arXiv preprint arXiv:2109.12188_ (2021).
* Treviso et al.(2023) Marcos Treviso, Ji-Uing Lee, Tianchu Ji, Betty van Aken, Qingqing Cao, Manuel R Ciosici, Michael Hassid, Kenneth Heafield, Sara Hooker, Colin Raffel, et al. 2023. Efficient methods for natural language processing: survey. _ TACL_ 11(2023), 826-860).
* Van Wynsberghe (2021) Aimee Van Wynsberghe. 2021. Sustainable AI: AI for sustainability and the sustainability of AI. _ AI and Ethics_1, 3(2021), 213-218.
* Vaswani 등(2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. 주의만 있으면 됩니다. _ NeurIPS_30(2017).

* Vinuesa 등(2020) Ricardo Vinuesa, Hossein Azizpour, Iolanda Leite, Madeline Balaam, Virginia Dignum, Sami Domisch, Anna Fellander, Simone Daniela Langhans, Max Tegmark, and Francesco Fuso Nerini. 2020. 지속 가능한 개발 목표를 달성하는 데 있어 인공지능의 역할입니다. _ Nature communications_11, 1(2020), 1-10.
* Wan 등(2020) Yu Wan, Baosong Yang, Derek F Wong, Yikai Zhou, Lidia S Chao, Haibo Zhang, and Boxing Chen. 2020. Self-paced learning for neural machine translation. _ arXiv preprint arXiv:2010.04505_ (2020).
* Wang 등(2022) Boxing Wang, Qifan Xu, Zhengda Bian, and Yang You. 2022. Tesseract: 텐서 병렬성을 효율적으로 병렬화한다. _ICPP_에서. 1-11.
* Wang 등(2022) Jing Wang, Jie Shen, Xiaofei Ma, and Andrew Arnold. 2022. 독해력 향상을 위한 불확실성 기반 능동학습. _ TMLR_(2022).
* Wang 등(2022) Jianing Wang, Chengyu Wang, Fuli Luo, Chuanqi Tan, Minghui Qiu, Fei Yang, Qiuhui Shi, Songfang Huang, and Ming Gao. 2022. few-shot text classification에 대한 통일된 프롬프트 튜닝을 향하여. _ arXiv preprint arXiv:2205.05313_ (2022).
* Wang 등(2023) Minjie Wang, Chien-chin Huang, and Jinyang Li. [n. d.]. Automatic Dataflow Graph Partitioning을 사용하여 초대형 모델 지원. ([n. d.]).
* Wang 등(2023) Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. 2023. SCOTT: Self-consistent chain-ofthought distillation. _ arXiv preprint arXiv:2305.01879_ (2023).
* Wang 등(2023) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. 자기일관성은 언어모델에서 사고추론의 연쇄를 향상시킨다. _ICLR_에서.
* Wang 등(2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannanehajishirzi. 2022. 자체 지시: 언어 모델을 자체 생성된 명령어와 정렬. _ arXiv preprint arXiv:2212.10560_ (2022).
* Wang 등(2022) Yile Wang, Yue Zhang, Peng Li, and Yang Liu. 2022. Language Model Pre-training with Linguistically Motivated Curriculum Learning. (2022).
* Wei 등(2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. 대형 언어 모델의 출현 능력. _ TMLR_(2022). 설문 인증
* Wei et al.(2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. _ NeurIPS_35(2022), 24824-24837.
* Wennberg and Henter (2021) Ulme Wennberg and Gustav Eje Henter. 2021. Case for translation-invariant self-attention in transformer-based language models. _ arXiv preprint arXiv:2106.01950_ (2021).
* Wenzek 등(2020) Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. 2020. CCNet: Web Crawd Data로부터 고품질의 단설형 데이터셋 추출. <제12회 언어 자원 및 평가 회의의 진행>에서. 4003-4012.
* White 등(2023) Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. 2023. 채팅으로 프롬프트 엔지니어링을 향상시키기 위한 프롬프트 패턴 카탈로그입니다. _ arXiv preprint arXiv:2302.11382_ (2023).
* Wu et al. (2022) Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. 2022. Sustainable ai: Environmental implications, challenges and opportunities. _ Proceedings of Machine Learning and Systems_4(2022), 795-813.
* Wu 등(2023) Minghao Wu, Abdul Wahaeed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. 2023. Lamini-lm: 대규모 지침으로부터 다양한 증류 모델 무리. _ arXiv preprint arXiv:2304.14402_ (2023).
* Wu 등(2023) Shijie Wu, Qcan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023. Bloomberggpt: A large language model for finance. _ arXiv preprint arXiv:2303.17564_ (2023).
* Xia 등(2023) Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, and Shuaiwen Leon Song. 2023. Flash-LLM: Unstructured Sparsity를 이용한 Cost-Effective 및 Highly-Efficient Large Generative Model 추론을 가능하게 한다. _ arXiv preprint arXiv:2309.10285_ (2023).
* Xia 등(2023) Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 2023. 전단 LLaMA: 구조화된 프루닝을 통한 가속 언어 모델 사전 트레이닝. _ arXiv preprint arXiv:2310.06694_ (2023).
* Xiao 등(2023) Guangcuan Xiao, Ji Lin, Mickal Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. Smoothbquant: 대형 언어 모델에 대한 정확하고 효율적인 사후 트레이닝 양자화. _ICML_에서. PMLR, 38087-38099.
* Xie 등(2022) Shuo Xie, Jiahao Qiu, Ankita Pasad, Li Du, Qing Qu, and Hongyuan Mei. 2022. 사전 훈련된 언어 모델의 숨겨진 상태 가변성은 전이 학습을 위한 계산 감소를 안내할 수 있다. _EMNLP_에서.
* Xie 등(2023) Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. 2023. 중요도 재샘플링을 통한 언어 모델에 대한 데이터 선택. _ arXiv preprint arXiv:2302.03169_ (2023).
* Xie 등(2023) Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. 2023. Decomposition enhance reasoning via self-evaluation guided decoding. _ arXiv preprint arXiv:2305.00633_ (2023).
* Xing 등(2015) Eric P Xing, Qirong Ho, Wei Dai, Jin-Kyu Kim, Jinliang Wei, Seungak Lee, Xun Zheng, Pengtao Xie, Abhimanu Kumar, and Yacliang Yu. 2015. Petuum: 빅 데이터에 대한 분산 머신 러닝을 위한 새로운 플랫폼. 「제21차 ACM SIGKDD 국제 지식 검색 및 데이터 마이닝 회의의 진행례」에 있다. 1335-1341.
* Xu and McAuley (2023) Canwen Xu and Julian McAuley. 2023. 사전 훈련된 언어 모델에 대한 모델 압축 및 가속에 대한 조사. _AAAI_에서 Vol. 37. 10566-10575.
* Xu and You (2023) Qifan Xu and Yang You. 2023. 초대형 딥러닝 모델을 학습시키기 위한 효율적인 2d 방법. _2023 IEEE International Parallel and Distributed Processing Symposium(IPDPS)_에서. IEEE, 222-232.
* Xu et al.(2021) Yuanzhong Xu, HyoukJung Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et al. 2021. GSPMD: General and scalable parallelization for ML computation graph. _ arXiv preprint arXiv:2105.04663_ (2021).

* Xu 등(2023) Ziyun Xu, Chengyu Wang, Minghui Qiu, Fuli Luo, Runxin Xu, Songfang Huang, and Jun Huang. 2023. 대조적인 프롬프트 튜닝으로 사전 트레이닝된 언어 모델들을 단대단 소수 샷 학습자로 만드는 것. <제16회 ACM 국제회의 웹 검색 및 데이터 마이닝의 진행>에서. 438-446
* Xu 등(2003) Zhao Xu, Kai Yu, Volker Tresp, Xiaowei Xu, and Jizhi Wang. 2003. Representative sampling for text classification using support vector machines. _ECIR_에서. 393-407
* Yang 등(2021) Bowen Yang, Jian Zhang, Jonathan Li, Christopher Re, Christopher Aberger, and Christopher De Sa. 2021. PiPemare: Asynchronous pipeline parallel dnn training. _ Proceedings of Machine Learning and Systems_ 3 (2021), 269-296.
* Yang et al.(2020) Haichuan Yang, Shupeng Gui, Yuhao Zhu, and Ji Liu. 2020. 희소-양자화 결합 학습에 의한 자동 신경망 압축: 제약 최적화 기반 접근법. _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 2178-2188.
* Yang 등(2023) Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. Harnessing the power of llms in practice: survey on chatgpt and beyond. _ arXiv preprint arXiv:2304.13712_ (2023).
* Yang 등(2023) Xianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, and Wei Cheng. 2023. 질의 또는 양상 기반 텍스트 요약에 대한 채팅의 한계를 탐색. _ arXiv preprint arXiv:2302.08081_ (2023).
*Yao 등(2023) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. _ arXiv preprint arXiv:2305.10601_ (2023).
*Yao 등(2022) Xingcheng Yao, Yanan Zheng, Xiaocong Yang, and Zhilin Yang. 2022. Nlp from scratch without large-scale preraining: simple and efficient framework. _Machine Learning에 대 한 국제 회의_ 에서입니다. PMLR, 25438-25451.
*Yao 등(2022) Zhewei Yao, Reza Yandani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. 2022. 제로양자: 대규모 변압기에 대한 효율적이고 저렴한 사후 훈련 양자화. _ NeurIPS_35(2022), 27168-27183.
* Yi 등(2023) Rongie Yi, Liwei Guo, Shiyun Wei, An Zhou, Shangguang Wang, and Mengwei Xu. 2023. EdgeMoE: MoE 기반 대용량 언어 모델의 빠른 On-Device 추론. _ arXiv preprint arXiv:2308.14352_ (2023).
*임 등(2017) 준호임, 주동규, 배지훈, 김준모. 2017. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. _CVPR_에서입니다. 4133-4141.
* Yu 등(2022) Yue Yu, Lingkai Kong, Jieyu Zhang, Rongzhi Zhang, and Chao Zhang. 2022. AcTune: 사전 훈련된 언어 모델의 능동적 미세조정을 위한 불확실성 기반 능동적 자가-훈련. _NAACL_에서입니다. 1422-1436.
* Yuan 등(2020) Michelle Yuan, Hsuan-Tien Lin, and Jordan Boyd-Graber. 2020. Cold-start active learning through self-supervised language modeling. _ arXiv preprint arXiv:2010.09535_ (2020).
* Yuan 등(2023) Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. 2023. RPTQ: Reorder-based Post-training Quantization for Large Language Models. _ arXiv preprint arXiv:2304.01089_ (2023).
* Zagoruyko and Komodakis (2016) Sergey Zagoruyko and Nikos Komodakis. 2016. Paying More Attention to Attention: Attention Transfer를 통한 Convolutional Neural Networks의 성능 향상. _ICLR_에서.
* Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big bird: Transformers for longer sequences. _ NeurIPS_ (2020), 17283-17297.
* Zaken 등(2022) Elad Ben Zaheer, Yoay Goldberg, and Shuali Ravfogel. 2022. BitFit: Transformer-based Masked Language-models에 대한 간단한 파라미터-효율적인 미세 조정. _ACL_에서. 119
* Zellers 등(2018) Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: 근거 있는 상식 추론을 위한 대규모 적대적 데이터 세트입니다. _ arXiv preprint arXiv:1808.05326_ (2018).
* Zha 등(2023) Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong, and Xia Hu. 2023. 데이터 중심 인공지능: 설문조사. _ arXiv preprint arXiv:2303.10158_ (2023).
* Zhai et al. (2008) ChengXiang Zhai et al. 2008. Statistical language models for information retrieval a critical review. _ Foundations and Trends(r) in Information Retrieval_ 2, 3(2008), 137-213.
* Zhai 등(2021) Shuangfei Zhai, Walter Talbotti, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. 2021. An attention free transformer. _ arXiv preprint arXiv:2105.14103_ (2021).
* Zhang 등(2023) Chen Zhang, Yang Yang, Jiahao Liu, Jingang Wang, Yunsen Xian, Benyou Wang, and Dawei Song. 2023. 증류 언어 모델에서 용량 격차의 저주를 해제. _ arXiv preprint arXiv:2305.12129_ (2023).
* Zhang et al.(2019) Jiong Zhang, Hsiang-Fu Yu, and Inderjit S Dhillon. 2019. Autoassist: 심층 신경망의 학습을 가속화하기 위한 프레임워크. _ Neural Information Processing Systems_ 32(2019).
* Zhang et al.(2023) Mingyang Zhang, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan Zhuang, et al. 2023. Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning. _ arXiv preprint arXiv:2305.18403_ (2023).
* Zhang 등(2023) Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. 2023. Adaptive budget allocation for parameter-efficient fine-tuning. _ arXiv preprint arXiv:2308.10512_ (2023).
* Zhang et al.(2023) Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. 2023. Instruction Tuning for Large Language Models: Survey. _ arXiv preprint arXiv:2308.10792_ (2023).
* Zhang et al.(2022) Shujian Zhang, Chengyue Gong, Xingchao Liu, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. 2022. Allsh: 국소 민감도와 경도에 의해 안내되는 능동적 학습_ arXiv preprint arXiv:2205.04980_ (2022).

* [325] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_ (2022).
* [326] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. 2020. Ternarybert: Distillation-aware ultra-low bit bert. _arXiv preprint arXiv:2009.12812_ (2020).
* [327] Yuxin Zhang, Lirui Zhao, Minghao Lin, Yunyun Sun, Yiwu Yao, Xingjia Han, Jared Tanner, Shiwei Liu, and Rongrong Ji. 2023. Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs. _arXiv preprint arXiv:2310.08915_ (2023).
* [328] Mingjun Zhao, Haijiang Wu, Di Niu, and Xiaoli Wang. 2020. Reinforced curriculum learning on pre-trained neural machine translation models. In _Proceedings of the AAAI Conference on Artificial Intelligence_, Vol. 34. 9652-9659.
* [329] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. _arXiv preprint arXiv:2303.18223_ (2023).
* [330] Fedor Zhdanov. 2019. Diverse mini-batch active learning. _arXiv preprint arXiv:1901.05954_ (2019).
* [331] Linamin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing, et al. 2022. Alpa: Automating inter-and {Intra-Operator} parallelism for distributed deep learning. In _16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)_, 559-578.
* [332] Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and Hongsheng Li. 2021. Learning n: m fine-grained structured sparse neural networks from scratch. _arXiv preprint arXiv:2102.04010_ (2021).
* [333] Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, et al. 2023. A comprehensive survey on pretrained foundation models: A history from bert to chatpgt. _arXiv preprint arXiv:2302.09419_ (2023).
* [334] Jinxin Zhou, Xiao Li, Tianyu Ding, Chong You, Qing Qu, and Zhihui Zhu. 2022. On the Optimization Landscape of Neural Collapse under MSE Loss: Global Optimality with Unconstrained Features. In _Proceedings of the 39th International Conference on Machine Learning_. 27179-27202.
* [335] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yubeng Zou. 2016. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. _arXiv preprint arXiv:1606.06160_ (2016).
* [336] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silvin Pitis, Harris Chan, and Jimmy Ba. 2023. Large Language Models are Human-Level Prompt Engineers. In _ICLR_.
* [337] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-t: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_ (2023).
* [338] Qingqing Zhu, Xinying Chen, Pengfei Wu, JunFei Liu, and Dongyan Zhao. 2021. Combining curriculum learning and knowledge distillation for dialogue generation. In _Findings of the Association for Computational Linguistics: EMNLP 2021_. 1284-1295.
* [339] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2023. A Survey on Model Compression for Large Language Models. _arXiv preprint arXiv:2308.07633_ (2023).
* [340] Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. 2021. A geometric analysis of neural collapse with unconstrained features. _Advances in Neural Information Processing Systems_ (2021), 29820-29834.
* [341] Zeyuan Allen Zhu and Yuanzhi Li. 2023. Physics of Language Models: Part 3.1, Knowledge Storage and Extraction. arXiv:2309.14316 [cs.CL]
* [342] Bohan Zhuang, Jing Liu, Zizheng Pan, Haoyu He, Yuetian Weng, and Chunhua Shen. 2023. A survey on efficient training of transformers. _arXiv preprint arXiv:2302.01107_ (2023).
* [343] Yimeng Zhuang, Jing Zhang, and Mei Tu. 2022. Long-range Sequence Modeling with Predictable Sparse Attention. In _ACL_. 234-243.
* [344] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alee Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. _arXiv preprint arXiv:1908.08593_ (2019).
* [345] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. 2022. Designing effective sparse expert models. _IPDPSW_ (2022).
