[MISSING_PAGE_EMPTY:1]

needed for training these models are substantial, creating challenges in both resource allocation and model design. For example, the cost of exploring different architectures or strategies becomes prohibitive [329]. Furthermore, their large size makes them unsuitable for resource-constrained environments like edge devices, thus narrowing their range of applications [7]. This computational burden also confines the development of LLMs to large companies with abundant resources [24; 196; 210]. Many essential details, such as the data collection pipeline and training methodologies, remain proprietary, which hinders academic research and poses challenges for smaller companies. Additionally, the environmental impact of training these models is not to be overlooked, raising concerns about carbon emissions and ethical considerations [268; 270; 285]. Consequently, there is a growing emphasis on improving the _efficiency_ of LLMs.

Motivated by this pressing need for more efficient LLMs, this survey aims to provide a comprehensive and up-to-date understanding of the subject. For the context of this paper, "efficiency" is defined as the optimization of computational and memory resources without compromising model performance. Adopting a holistic approach, we explore multiple dimensions of efficiency that are crucial for the end-to-end development of LLMs. These dimensions encompass data utilization, architectural designs, training and tuning strategies, and inference techniques, --essentially covering the entire pipeline of model development from algorithmic and software perspective.2 While there are existing surveys that focus on specific aspects of LLM efficiency, such as data [316], training [241; 333; 342], tuning [323], or inference [295; 339], they often do not provide a comprehensive view. Other works, like [267], have offered valuable insights into various efficiency aspects for Natural Language Processing (NLP), yet the rapidly evolving nature of the LLM field calls for an updated and comprehensive review. In contrast, our paper aims to present a more thorough and current overview of key methodologies and techniques that contribute to the development of efficient LLMs.

Footnote 2: Remark here that the advancements and achievements from hardware aspect are omitted in this survey.

The remainder of this survey is organized as follows to offer a comprehensive understanding of the multiple facets of LLM efficiency from the algorithmic perspective:

\(\bullet\) Section 2_Background_ introduces the core concepts of LLMs and outlines the evaluation metrics pertinent to assessing their efficiency.

\(\bullet\) Section 3_Budget Efficiency_ examines the role of predictive approaches like scaling laws in optimizing the performance of LLMs within given resource constraints.

\(\bullet\) Section 4_Data Efficiency_ focuses on techniques for optimizing data utilization, thereby reducing resource consumption without compromising performance.

Figure 1: Capability and performance tree [4] of PaLM [54] across different scales (8-billion, 62-billion, 540-billion). Each circle node represents a specific capability, and its size indicates the corresponding performance levelâ€”the larger the circle, the greater the capability. As the model scale increases, performance not only improves across existing tasks but also reveals new capabilities.

\(\bullet\) Section 5_Architecture Efficiency_ reviews innovative architectural designs, providing a detailed examination of how architecture influences efficiency.

\(\bullet\) Section 6_Training and Tuning Efficiency_ discusses strategies for efficiently training LLMs from scratch and fine-tuning pre-trained models for specific downstream tasks.

\(\bullet\) Section 7_Inference Efficiency_ explores the realm of model compression techniques designed to accelerate inference speed and reduce memory footprint.

\(\bullet\) Section 8_Conclusion_ summarizes the key findings of this survey and discusses their broader implications for the advancement of efficient LLMs.

A schematic overview of these various dimensions of LLM efficiency is presented in Figure 2.

## 2. Background

In this section, we present an overview of the core concepts that form the basis of LLMs, along with the key metrics used for assessing their efficiency.

### Core Concepts of LLMs

Language modeling, a cornerstone in the field of NLP, aims to model the generative likelihood of word sequences and predict the probabilities of subsequent or missing tokens. This area has evolved significantly over several decades. Initially

Figure 2. The schematic overview of the multi-faceted dimensions of LLM Efficiency. This diagram illustrates the key areas covered in this survey, including data utilization, architectural designs, training and tuning strategies, and inference techniques, thereby providing a holistic view of the factors contributing to LLM efficiency.

rooted in statistical language models [16; 30; 116; 222; 317], the focus has gradually shifted to pre-trained neural language models [128; 153; 185; 202; 210; 158] and, more recently, to Large Language Models (LLMs) [28; 111; 302; 329]. While there is no standardized definition for LLMs, they are typically distinguished by their extensive parameter sizes and extraordinary learning capabilities. In this section, we adopt the criteria outlined in [329], focusing on language models with more than one billion parameters, and discuss their core concepts in detail.

**Architectural Foundations.** LLMs can generally be categorized into two main paradigms: encoder-decoder models [146; 148; 171; 202; 212; 228], exemplified by BERT [128], and decoder-only models [13; 263; 264; 209; 210; 201; 230; 265; 287; 266; 208; 267; 268; 269; 270; 265; 288; 263; 264; 265; 287; 266; 288; 267; 269; 290; 211], such as the GPT series [24; 196; 209; 210]. BERT is trained using masked language modeling, enabling it to excel in contextual understanding by predicting masked or missing tokens. On the other hand, GPT models are trained using autoregressive modeling, which equips them with strong generative capabilities by predicting subsequent tokens in a sequence. Despite these differences, both types of models commonly rely on the Transformer [269] architecture, which is particularly noteworthy for its self-attention mechanism. In self-attention, each token is represented as a _key, value_, and _query_. The _query_ weighs the importance of other tokens, represented as _keys_ in understanding a particular token. These weights are applied to the _values_ to create context-aware representations. This mechanism allows each token in the sequence to consider all other tokens simultaneously, facilitating parallel processing of sequential data and effective capture of long-sequence dependencies. As a result, multi-head attention layers are often stacked to form deep networks in LLMs. Nowadays, decoder-only models like GPT-4 [196] and LLMa [264; 265] are becoming increasingly prevalent, yet the core architectural module of self-attention remains a constant across these variations.

**Training Essentials.** LLMs acquire their general-purpose capabilities from an initial _pre-training_ phase on expansive and diverse datasets [302; 24]. These datasets cover a broad spectrum of sources such as books, scientific papers, code, and websites [316]. This foundational knowledge is then fine-tuned on relatively smaller datasets in a supervised manner, aimed at enabling the LLMs to adhere to human instructions, a process known as _instruction tuning_[50; 55; 57; 167; 251; 259; 258; 278; 332; 337]. A _reinforcement learning with human feedback_ is then proceeded to a instructed fine-tuned LLM to further align model behavior upon human preferences and instructions [265]. In the current landscape, decoder-only models have become the norm, particularly for their superior generative abilities. These models employ autoregressive objectives in the pre-training phase to maximize the likelihood of predicting subsequent tokens based on their preceding context. The scaling [106; 124] of this autoregressive pre-training significantly enhances LLM capabilities, as demonstrated by models like the GPT and PaLM series [13; 54]. For instance, PaLM [54] was trained on a 780B-token dataset and utilized a 540B-parameter Transformer architecture. PaLM-2 [13], its successor, advances this further; its largest variant has 1.1T parameters and was trained on a more diverse, multilingual dataset with 1.5T tokens. However, this scaling introduces its own set of challenges, requiring efficient training infrastructures and optimization strategies. Distributed training frameworks that focus on data parallelism, such as DeepSpeed [216] and Fully Sharded Data Parallel (FSDP) [3], or pipeline parallelism like Gpipe [112] and PipeDream [191], are commonly employed. Additionally, tensor parallelism techniques like Megatron-LM [193; 245] and SARATHI [6] are also utilized. Specialized techniques, such as mixed-precision training [230; 65; 190] and quantization-aware training [172; 165; 290], are often incorporated to streamline the training process. These methods not only address the computational challenges tied to the scale of LLMs but also facilitate the development of increasingly capable models.

**Versatile Abilities with Prompt Engineering.** One of primary mechanisms for harnessing the versatility of LLMs across diverse tasks is through prompt engineering [168; 284; 336]. In this setting, a prompt consists of natural language instructionsgiven by users to guide the LLM's behavior. The art of prompt engineering lies in crafting these instructions to elicit specific and contextually appropriate responses from the model. Two prominent techniques are few-shot (24; 281) and zero-shot (137) prompting. Few-shot prompting provides the model with example tasks and corresponding solutions, while zero-shot prompting relies solely on task descriptions. This form of prompting is closely related to in-context learning (ICL), a concept first observed in GPT-3 (24), which allows the model to adapt to new tasks without retraining. The emergent abilities of LLMs, particularly in tackling a wide array of unseen tasks, are significantly enhanced when ICL is combined with well-designed prompts. Advanced prompting strategies like Chain-of-Thoughts (CoT) (277; 281), Tree-of-Thoughts (ToT) (174; 293; 304), and Graph of Thoughts (GoT) (20) draw inspiration from human reasoning and cognitive structures. These strategies enable LLMs to explore novel capabilities like backtracking, thought merging, and idea elimination, thereby enhancing the quality of responses in complex reasoning tasks such as arithmetic (198), commonsense reasoning (257), and question answering (85). As a result, prompt engineering serves as a pivotal mechanism for amplifying the versatility and effectiveness of LLMs.

### Evaluation Metrics for Efficiency

Evaluating the efficiency of LLMs requires a multi-faceted approach that considers various performance indicators. These metrics are often presented alongside measures of accuracy and versatility to provide a holistic assessment of an LLM's overall efficiency and effectiveness. In the paragraphs that follow, we will explore key metrics commonly used to understand efficiency in the realm of LLMs.

**Number of Parameters.** The number of parameters in an LLM is a key factor that directly affects the model's learning capacity and complexity. These parameters, which include elements like weights and biases, are learnable during training or fine-tuning phases. A higher parameter count usually enables the model to grasp more complex data patterns, contributing to the development of various emergent abilities. However, this comes with the downside of increased computational demands for both training and inference. Additionally, having too many parameters can lead to overfitting, especially when the training data is scarce. To mitigate this, common techniques like regularization and early stopping are frequently used.

**Model Size.** Model size, defined as the disk space required for storing the entire model, is often the initial consideration when training a new LLM or working with a pre-trained model. Given that exceedingly large models may be infeasible to store or run, this metric is particularly crucial for real-world deployments, especially in storage-constrained environments like edge devices. Expressed in units such as gigabytes (GB) or megabytes (MB), model size is influenced by several factors. While the number of parameters plays a significant role, other elements like the data type used for parameters (_e.g._, float16, int8) and specific architectural choices also contribute. In addition to its direct impact on storage requirements, model size serves as an indirect indicator of the computational resources needed for both training and inference.

**Floating Point Operations (FLOPs).** Floating-point operations (FLOPs) is commonly used to gauge the computational complexity of LLMs. This metric counts the number of floating-point operations like addition, subtraction, multiplication, and division, giving an estimate of the computation done during a single forward pass. While FLOPs offer valuable insights into computational needs and potential energy use, they are not a complete measure. Other factors, such as system parallelism and architectural choices, also play a role in determining a model's overall computational efficiency. A higher FLOPs count usually means the model is more computationally demanding, which can be a challenge for deployment in environments with limited resources. As a result, optimizing this metric is often a key focus in the development of more efficient LLMs.

**Inference Time / Tokens per Second**. Inference time, also known as latency or delay, measures the duration it takes for an LLM to process input and generate a response during the inference stage. Unlike FLOPs, which provide a theoretical estimate of computational needs, inference time offers a practical gauge of real-world performance. This is because it's assessed in actual deployment settings, taking into account specific hardware and optimizations. Usually expressed in milliseconds (ms) or seconds (s), this metric is crucial for real-time applications that need quick responses or have stringent latency constraints. Normalizing the inference time by time elapsed results in tokens per second, which refers to the number of tokens that a language model can process (read, analyze, generate, etc.) in one second. This is a key performance indicator that reflects the model's speed and efficiency. Achieving a balance between fast inference time / tokens per second and high generalization is a key focus in the development of efficient LLMs.

**Memory Footprint.** Memory footprint refers to the amount of Random Access Memory (RAM) required to load and run a model during inference or training. This metric is crucial for understanding the model's operational demands, especially in resource-constrained environments like edge devices or servers with limited memory capacity. Expressed in MB or GB, the memory footprint includes not just the model parameters but also other runtime necessities such as intermediate variables and data structures. A larger memory footprint can limit the model's deployability and may require optimization techniques like model pruning or quantization to reduce it.

**Carbon Emission.** Carbon emission is an increasingly important metric in the evaluation of large models, reflecting the environmental impact of training and running these models. This metric is usually measured in terms of kilograms or tons of CO2 equivalent emitted during the model's lifecycle, from training to inference. The carbon footprint is influenced by various factors, including the energy efficiency of the hardware used, the source of electricity, and the duration of model training and operation. High carbon emissions not only have environmental implications but can also affect the social and ethical considerations of deploying LLMs. As a result, there is a growing emphasis on optimizing models to be more energy-efficient, thereby reducing their carbon footprint. This is often achieved through hardware acceleration, algorithmic improvements, or even selecting greener energy sources for data centers.

## 3. Budget Efficiency: Scaling Laws

### Introduction

The performance of large language models (LLMs) is significantly influenced by various factors, including training data, model size, architecture, computing resources, and the training methodology itself. Training LLMs requires extensive resources, making the conventional trial-and-error method for optimizing these factors both impractical and resource-intensive. As a result, predicting LLM performance before training is not just beneficial, but often necessary. This predictive approach allows for more effective planning and allocation of resources. For instance, consider a scenario with limited computing resources: How can we optimally balance the model size and training data to achieve minimal objective function value? Answering such questions beforehand can significantly enhance the efficiency and effectiveness of LLM training processes.

Recent research in predicting large language model (LLM) performance has concentrated on understanding the _scaling law_. This law delineates how LLM performance is influenced by factors such as model architecture, neural model size, computing power for training, and available data. The concept of scaling law, rooted in statistical mechanics approaches for predicting model generalization, has a rich history dating back to the early 1990s (11; 18; 95; 235). Its relevance has been reinvigorated recently in the context of modern deep learning models (10; 124; 101; 102; 103; 106; 124; 188; 221; 248; 260; 262). This section willdelve into the latest advancements and insights in the scaling law as applied to LLMs, highlighting how these models evolve and perform under varying conditions.

### Scaling Law

The work (Kapol et al., 2019) presents a thorough study of the empirical scaling laws of transformer-based large language models. The authors observe that model performance (objective function \(L\)) primarily depends on three factors: the number of model parameters \(N\), dataset size \(D\), and the computing budget for training. They demonstrate a power-law relationship between model performance (measured in objective function, \(L\)) and these factors. For instance, they found that the relationship between performance and dataset size can be represented as \(L(D)\approx(5.4\times 10^{13}/D)^{0.095}\). This formula suggests that as the dataset size increases, the model's performance improves following a specific pattern. While theoretical generalization bounds may suggest a similar power-law relationships, they generally do not provide specific coefficients like those identified in Kaplan et al.'s work (Kapol et al., 2019). This specificity is crucial for accurately predicting model performance. Additionally, the study highlights that transformers, known for their effective handling of long-range data dependencies, tend to outperform Long Short-Term Memory networks (LSTMs) (Kapol et al., 2019) as they scale. This observation underscores the potential of transformers in large-scale language processing tasks.

**Compute-Optimal Models via Scaling Law.** When working within a fixed computational budget, it is crucial to find the right balance between model size (\(N\)) and dataset size (\(D\)). This is where the scaling law curve, \(L(N,D)\), becomes a vital tool. It helps determine the most effective trade-off between these two factors. The scaling law, as observed in (Kapol et al., 2019), was instrumental in designing GPT-3, a 175 billion parameter language model. Interestingly, GPT-3 was trained on fewer tokens than was typical for its time (Kapol et al., 2019). Different forms of the scaling law curve have led to the development of diverse models, as seen in subsequent studies (Kapol et al., 2019; Kapol et al., 2019; Kapol et al., 2019). A notable application of these predicted scaling laws is found in (Kapol et al., 2019). Their research revealed that many previously trained LLMs, including Gopher (Gopher, 2019), could have achieved better performance within the same compute budget. They demonstrated this by training a smaller model, Chinchilla, with 70 billion parameters, which outperformed the larger 280 billion parameter Gopher model (Gopher, 2019) while using a similar compute budget.

**Scaling Law for Transfer Learning.** While the scaling behavior of pretrained LLMs has been extensively studied and exhibits clear predictability, it becomes less clear when predicting the performance of pretrained LLMs on downstream tasks. The work in (Kapol et al., 2019) investigates the scaling behavior when fine-tuning a pretrained model and demonstrates that favorable scaling laws, akin to those in (Kapol et al., 2019), apply to transfer and few-shot settings in NLP tasks. In comparison to models trained from scratch, pretrained models exhibit a more advantageous scaling law in low-data scenarios. Different scaling behaviors between upstream and downstream configurations are observed in (Kapol et al., 2019), indicating that, in addition to model size, the model's architecture plays a critical role in downstream fine-tuning. Specifically, the study demonstrates that redesigned models can attain similar downstream fine-tuning quality while having 50% fewer parameters and training 40% faster when compared to the widely adopted T5-base model (Kapol et al., 2019).

**Scaling Law in the Data-Constrained Regime.** What if the training data are limited? The work (Kapol et al., 2019) examines a data-constrained regime and observes that, for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes in the objective function compared to a single epoch3. However, further increasing the number of epochs will lead to a decrease in the performance of the learned model.

**Effect of Data Quality.** A pivotal question in the realm of machine learning is whether the quality of data can lead to a transition from power-law to exponential scaling in model performance. The work by (Zhu and Chen, 2018) provides an intriguing insight into this matter. They demonstrate that for certain vision classification tasks, the objective function can exhibit exponential scaling with an increase in dataset size, deviating from the traditional power-law scaling observed with pruned datasets. While this phenomenon is initially observed in vision tasks, recent research, including works by (Zhu and Chen, 2018; Chen et al., 2019; Chen et al., 2020), expands this concept to other domains. These studies explore the impact of high-quality data in tasks like generating coherent English, coding, and common sense reasoning. They suggest that high-quality data can significantly alter the scaling laws' trajectory. This change indicates the potential for more efficient models, which, despite being trained on fewer data tokens yet with high quality, could match the performance of large-scale models trained on vast datasets without sufficient quality constraints. This shift in understanding the role of data quality could revolutionize the approach to training and optimizing LLMs.

**Effect of Architecture.** In the domain of model scaling, conventional wisdom, as supported by studies like (Zhu and Chen, 2018; Chen et al., 2020), suggests that the inherent attributes of models, such as the width or depth of Transformers, have a minimal impact on performance. However, the work by (Zhu and Chen, 2018) presents a contrasting viewpoint. This study delves into the influence of different architectural designs on the scaling law and reveals that architecture plays a crucial role in scaling processes. Tay et al. (2019) demonstrate that depending on the scale, the most effective model architecture can vary significantly. This finding complements previous assumptions and underscores the importance of considering architectural variations in the quest for optimal model performance at different scales.

## 4. Data Efficiency

### Introduction

The insatiable demand for data by large-scale models has significantly fueled the growth of the data collection and preparation industry. However, this reliance on vast datasets, often accumulated over years, introduces substantial challenges in model training. These include not only prolonged training durations but also escalated costs due to extensive power consumption and the need for larger data storage capacities. Consequently, finding ways to use data more efficiently in both training and validation phases is of paramount importance. In this section, we will delve into strategies and considerations for enhancing data efficiency, addressing how to maximize the utility of data while mitigating the associated costs and resource demands.

### Data Filtering

Data filtering is pivotal in directing training focus towards more informative samples, thereby eliminating irregular characters or patterns, rather than concentrating on examples with lesser informational value.

**Deduplication.** A prime data filter is _removing duplications, i.e., deduplication_. This straightforward yet efficacious approach not only abbreviates training duration but also enhances model performance, as evidenced by (Zhu and Chen, 2018). The utility of this de-duplication operation is evident at both the pre-training and fine-tuning stages of model development. In both pre-training and fine-tuning, the researchers utilizes techniques such as MinhashLSH (Zhu and Chen, 2018), CC-NET (Zhu and Chen, 2018), and adversarial filtering (Zhu and Chen, 2018), as demonstrated by (Zhu and Chen, 2018; Chen et al., 2020; Chen et al., 2020), for purging duplicates from the training datasets.

**Data Undersampling.** Beside deduplication, _data undersampling_, also referred to as instance selection, emerges as another promising data filtering technique (Zhu and Chen, 2018). This approach aims to reduce the volume of training samples by sub-sampling large datasets, yet crucially retains the distribution characteristics of the original training data. Moreover, this sub-samplingprocess can help mitigate issues of data imbalance. For instance, Prusa et al. (2020) demonstrated the effectiveness of random undersampling in majority classes, which serves dual purposes: it reduces redundancy in the training set and balances the data distribution across various classes. Additionally, MESA (Prusa et al., 2020) represents a novel advancement in this area by adopting Meta Learning to learn how to undersample vast datasets effectively. These techniques highlight the strategic importance of selective data use, not only in terms of quantity but also in ensuring quality and balance.

### Active learning / Importance Sampling

Active Learning or importance sampling helps a machine learning algorithm achieve better or equivalent performance with fewer annotated training samples. Their applications in training with extensive datasets predates the emergence of LLMs (Katharopoulos and Fleuret, 2019; Zhang et al., 2020). These methodologies strategically reduce the total number of training samples by applying various criteria. They aim to optimize the data collection and selection procedure by selecting and annotating only the most useful instances during the data annotation procedure (Zhang et al., 2019; Zhang et al., 2020; Zhang et al., 2020). The essence lies in the ability to prioritize samples based on their significance to the learning process, thereby optimizing the training efficiency for models dealing with large-scale data.

**Gradient Norm.** Katharopoulos and Fleuret (Katharopoulos and Fleuret, 2019) introduced an approach based on the upper bound of the gradient norm, along with proposing an estimator for variance reduction through importance sampling. Furthermore, Zhang et al. (Zhang et al., 2020) implemented a selector mechanism to identify samples with larger gradients within a batch, treating them as valid approximations for importance sampling. According to their methodology, samples with smaller gradients are deemed 'good points' in a given epoch, necessitating increased focus on 'bad points', _i.e._, samples with larger gradients. Upon these paradigm, training and fine-tuning over the samples with larger gradients could enhance the model performance more effectively.

**Objective Function / Predicted Score.** Beyond the gradient, Katharopoulos and Fleuret also suggested in a separate work (Katharopoulos and Fleuret, 2019) that the objective function (loss value) itself could serve as a viable metric for importance sampling. Expanding on this concept, Jiang et al. (Jiang et al., 2019) introduced'selective back-propagation'. This method accelerates training by omitting the back-propagation stage for training samples exhibiting low loss, a strategy claimed to enhance convergence and outpace traditional Stochastic Gradient Descent (SGD) on full datasets. In the context of text retrieval, a similar _relevance sampling_(Zhou et al., 2019; Zhang et al., 2020) has been proposed. It leverages the scores predicted by current model to evaluate the relevance of the samples and let the annotators only annotate those with higher scores. Such methods may focus on the samples with high scores thereby resulting in overfitting. Some researchers turn to _uncertainty-based sampling_ methods (Zhou et al., 2019; Katharopoulos and Fleuret, 2019; Zhang et al., 2020; Zhang et al., 2020; Zhang et al., 2020). This thread of methods defines that the instances with lower uncertainties are more useful for improving performance and are worthier of annotation.

**Diversity Sampling.** In contrast to the sampling methods which find difficult examples to train and annotate, methods that follow the strategy of _diversity sampling_(Zhou et al., 2019; Katharopoulos and Fleuret, 2019; Zhang et al., 2020; Zhang et al., 2020; Zhang et al., 2020; Zhang et al., 2020) want to enhance the heterogeneity of training data. For diversity sampling, there are two major approaches: iterative selection and cluster-based selection. Methods that belong to iterative selection iteratively examine if each instance can help improve the training data diversity and only annotate qualified instances. For example, Vote-k proposed in (Zhang et al., 2020) adopts the cosine similarly between the embedding of each unlabeled instance and its k-nearest neighbors to define instances representativeness; ALISH (Zhang et al., 2020) represents the diversity according to the data instance's local sensitivity (Zhang et al., 2020). Cluster-based methods will cluster unannotated datasets and select instances based on the cluster labels. Methods in this category have explored many clustering strategies for instance samplings, such as (weighted) K-means (Zhang et al., 2020; Zhang et al., 2020; Zhang et al., 2020; Zhang et al., 2020).

**Hybrid Sampling.** Gradient or objective-function based sampling necessitates a relatively reliable initial Large Language Model (LLM), as sample selection depends on the model's predicted outputs, a process often termed a _warm start_(Shi et al., 2018). On the other hand, diversity sampling can accommodate a _cold start_ scenario. However, this approach may inadvertently introduce outliers, potentially compromising the final model's performance. To address the trade-off between both sampling threads, recent researches have shifted towards _hybrid sampling methods_ that integrate both elements to surmount the challenge (Shi et al., 2018; Shi et al., 2018; Shi et al., 2018; Shi et al., 2018; Shi et al., 2018). For instance, Yuan et al. (Yuan et al., 2019) incorporated BADGE (Shi et al., 2018) into language processing. This method involves transforming unannotated instances into representations that encapsulate model confidence, followed by clustering these transformed instances. In parallel, Margatina et al. (Margatina et al., 2019) introduced Contrastive Active Learning (CAL). CAL selectively acquires contrastive samples from the pool of raw unannotated data, defining them as instances that are proximate in the model's feature space (_e.g._, sharing similar vocabulary or model encodings) but yield divergent predictive likelihoods. Experimental results affirm that CAL realizes a more effective balance compared to individual strategies, showcasing its potential in refining sampling processes.

**Other Sampling.** In addition to the above techniques, there exists a spectrum of sampling methods that operate on a broader scale. Chen et al. (Chen et al., 2019) introduced a novel approach by masking parameters to halt the back-propagation from certain inputs. This method preserves the forward graph, ensuring that the statistical characteristics of all batches are maintained, especially in layers such as normalization, thereby reflecting the overall dataset's mapping. Taking a different approach, Xie et al. (Xie et al., 2019) implement importance sampling in a reduced feature space. This strategy facilitates the tractability of importance weight estimation across the expansive text space, leveraging Kullback-Leibler (KL) reduction to efficiently train Large Language Models. Moreover, Sujit et al. (Sujit et al., 2020) advocate for clipping training samples based on their relevance in reinforcement learning contexts, prioritizing samples according to their importance. Yao et al. (Yao et al., 2020) propose a keyword-based retrieval system for selecting relevant training samples in semi-supervised language fine-tuning, usually picking the top-k neighbors for each target example during training. These diverse methodologies highlight the evolving landscape of importance sampling, extending its application beyond standard batch-centric approaches to encompass broader training strategies for LLMs.

### Curriculum Learning

Curriculum learning (Shi et al., 2018; Shi et al., 2018) is a strategy that aims to improve the model training efficiency by carefully designing the feeding order of the instances in the training data. The principle of this approach is to initiate training with simpler samples or subtasks and progressively escalate to more challenging ones. Two critical components are integral to the design of a curriculum learning method. _(i)_ The _difficulty metric_ (or difficulty criterion), responsible for ranking the training samples based on their complexity. This metric serves as a guide to categorize training instances from the simplest to the most complex. _(ii)_ The _pacing function_ (also known as curriculum scheduler or arrangement), which determines the rate at which these ranked samples are fed to the model training. This function modulates the learning curve, ensuring that the model is not overwhelmed by the complexity of the tasks too early.

**Difficulty Metric.** In the realm of curriculum learning for natural language processing, the most widely used difficulty metric is perhaps the sequence length (Shi et al., 2018; Shi et al., 2018; Shi et al., 2018). The underlying assumption is that processing longer sentences poses greater challenges than shorter ones. Another prevalent metric is vocabulary rarity (Shi et al., 2018; Shi et al., 2018), based on the intuition that sentences with less frequently used words in the training set are inherently more complex to comprehend. In addition, this metric could be measured by the uncertainty sampling principle in active learning, where the uncertainty indicated by other pre-trained models could serves as a gauge of difficulty as well.

[MISSING_PAGE_FAIL:11]

### Efficient Attention

The Transformer model, as introduced by Vaswani et al. (2020), utilizes a vanilla attention mechanism that computes dense pairwise relations in the input sequence. This results in quadratic complexity. However, recognizing that not all these relations hold equal significance, recent research has focused on methods to streamline this process. These methods aim to identify and maintain only the most crucial relations, thereby enhancing the efficiency of the attention calculation (Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020). In this subsection, we include the discussion of two main branches: _(i)_ the use of fast or sparse attentions (Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020) and _(ii)_ IO-aware attention calculation with hardware co-design (Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020). Both approaches reduce hardware loading time for efficient attention calculations.

**Fast Attention Calculation.** In the realm of fast attention, researchers are developing innovative strategies to enhance efficiency. A primary focus is on _attention factorization_, which aims to reduce attention calculations that are often unnecessary in certain contexts. This technique is particularly useful when dealing with lengthy sequential inputs, where direct pairwise attention computations become computationally intensive. By employing attention factorization, computational demands can be significantly reduced, transforming 2-D computations into more manageable 1-D formats (Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020). Furthermore, these factorized attention methods are designed to discern and emphasize the attention differences between closely positioned tokens and their respective changes over time. This nuanced approach ensures that the computational resources are dedicated to the most impactful elements of the data. Another innovative method involves the use of _frequency-based techniques_, such as Fast Fourier Transform (FFT) and hash representations. These techniques model attention in a manner that aligns well with hardware capabilities, making them more efficient for practical applications (Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020). They filter out near-zero attentions and focus computational efforts on the most significant ones for the final calculations. Such selective attention ensures that resources are not wasted on processing relatively unimportant data, further optimizing the overall efficiency of the model.

Moving away from directly calculating pairwise attention, some methods (Vaswani et al., 2020; Vaswani et al., 2020; Vaswani et al., 2020) explore the possibility of computing attention at a _block level_, which enables parallelization of the computation, significantly enhancing efficiency. For instance, the Monarch framework (Vaswani et al., 2020) and its advanced version, Monarch Mixer (M2) (Vaswani et al., 2020), adopt a novel strategy. They sparsify the dense attention matrix by decomposing it into a combination of permutation and block diagonal matrices. This decomposition allows for more efficient processing of attention calculations. Furthermore, the Blockwise Self-Attention (BST) method (Vaswani et al., 2020) introduces blockwise computation for both self-attention and feed-forward networks. This technique aims to lower memory requirements typically associated with traditional attention mechanisms. Moreover, some methods like LongNet (LongNet (LongNet, 2018) incorporate dilated attention in place of the original dense attention, allowing the processing of much longer token sequences, thereby expanding the capabilities of LLMs.

**Hardware-Related Efficient Attention.** Along with designing more efficient attention mechanisms at the software level, a significant focus has shifted to optimizing these mechanisms at a hardware level. One of the main challenges in this domain is efficiently utilizing computational resources, such as High Bandwidth Memory (HBM) and Static Random-Access Memory (SRAM), on GPUs. In this regard, recent advancements like FlashAttention (Vaswani et al., 2020) and its successor, FlashAttention-2 (Vaswani et al., 2020), have emerged. FlashAttention reconsiders the attention computation by adopting an I/O-centric perspective. It minimizes data transfers between HBM and SRAM, addressing a critical bottleneck in GPU processing. This method integrates block-wise softmax value calculations and updates with statistics. Such integration eliminates the conventional requirement of computingthe softmax function only after all attention values have been determined. Building upon FlashAttention (Krish et al., 2020), FlashAttention-2 (Krish et al., 2020) further optimizes work partitioning and reduces non-matrix multiplications, capitalizing on the GPUs' optimization for matrix operations. These algorithms are tailored to hardware considerations and accelerate the models on GPU machines.

In the quest to enhance Large Language Model (LLM) systems, some researchers are creatively drawing inspiration from current hardware architectures. A notable example is the PagedAttention (PagedAttention, 2019), which adapts virtual memory and paging techniques, commonly used in operating systems, to overcome memory limitations in LLMs. PagedAttention introduces an innovative approach to memory management by emulating the virtual memory system. It segments the Key-Value (KV) cache associated with a request into blocks, as opposed to relying on pre-allocated, contiguous memory. This method significantly reduces memory fragmentation, a common issue in traditional LLM memory allocation strategies. As a result, it allows the LLM to process longer sequences within the constraints of limited memory resources.

### Efficient Positional Encoding

Since LLMs may need to process long sequences as input, the absolute positional encoding (APE) used in the vanilla Transformer (Petershak et al., 2019) falls short of this requirement. To enhance the architecture's efficiency, researchers are exploring novel positional encoding (PE) methods that can accommodate longer sequences with relative positions (Sandwich, 2018; Sandwich, 2019; Sandwich, 2020) or rotary positional encoding (Sandwich, 2020; Sandwich, 2020). They are also seeking more generalizable solutions through randomized positional encoding (Sandwich, 2020) or even omitting positional encoding (Sandwich, 2020). We discuss some of the latest developments in this section.

**Addition-based Relative Positional Encoding.** Relative positional encoding methods utilize the relative position between two tokens rather than the absolute position of a single token. Some of them encode the relative positions and add the encoded positions to the subsequent attention, referring to _addition-based_ relative positional encoding methods. T5 (Sandwich, 2019), TISA (Sandwich, 2020), and FIRE (Sandwich, 2020) are representatives of this paradigm. In these models, the position embedding is applied to the interaction between the query and key elements within the self-attention mechanism, a departure from the earlier focus on the absolute position of individual tokens. The relative positional encoding in T5 (Sandwich, 2019) translates the relative position difference into a scalar bias value using a lookup table and employs the same embedding for all out-of-distribution (OOD) sequence lengths. TISA incorporates (Sandwich, 2020) a trainable Gaussian kernel that focuses on the positional differences between tokens. FIRE (Sandwich, 2020), on the other hand, employs progressive interpolation with a normalized position index by dividing the index difference between tokens by the smaller of the two indices. Compared to APE, relative positional encoding (RPE) offers a more effective way of modeling the relative distances between tokens. This not only enhances the model's understanding of token relationships but also facilitates length extrapolation, a critical feature for handling varied and complex sequences in language processing.

**Relative Positional Encoding with Decay Functions.** Another trend is to employ trainable relative positional encodings (RPE) that use decaying functions. This approach, exemplified by models like ALBi (Sandwich, 2020), KERPLE (Sandwich, 2018), and Sandwich (Sandwich, 2018), aims to focus the model's attention predominantly on neighboring tokens. The use of decaying functions in these methods ensures that the attention diminishes as the distance between tokens increases. ALBi introduces a linear decaying function to model the relationship between tokens, particularly effective for capturing the diminishing relevance of tokens as their distance increases. KERPLE (Sandwich, 2018) uses two variations of conditionally positive definite (CPD) kernels: a logarithmic variant and a power variant. These sophisticated kernels decay the connection between two tokens during RPE computation to adaptively model the decreasing significance of distant token relationships. Sandwich (Sandwich, 2018), meanwhile, adopts a series of cosine functions to represent the differences between tokens. Sandwich leverages the periodic nature of cosine functions to capture the cyclical patterns in token relationships. By diminishing attention between distant positions, these methods ensure the model's focus remaining on the more immediate and contextually relevant tokens rather than the tokens that are far away.

**Rotary Positional Encoding.** Beyond addition-based relative positional encoding, which adds encoded positions to the attention calculation, there are RPE methods that utilize rotary matrices for position embeddings (35; 201; 250). RoPE (250) introduces two rotation matrices to rotate the query and key vectors. The rotation angle is proportional to their absolute positions, which is then integrated into the dot product attention mechanism. This manner allows RoPE to generate attention based on the relative distance between tokens, instead of directly computing their relative differences. However, RoPE faces limitations in generalizing to sequence lengths beyond what it was trained on. Building upon RoPE, PI (35) extends its capabilities with Position Interpolation (PI). After fine-tuning on a moderate amount of data, PI shows a promising ability to handle very long context windows, addressing one of RoPE's primary limitations. YaRN (2017) further advances this field by introducing NTK-aware interpolation and dynamic NTK interpolation. This method effectively addresses the loss of high-frequency information in scenarios with and without fine-tuning on limited datasets. YaRN's approach significantly improves the model's ability to expand context size without the necessity for extensive fine-tuning. The common thread among these methods is their use of rotary matrices in the query and key vectors, a technique that has shown promising results in establishing more effective RPE in Transformer models.

**Other Positional Encodings.** Exploring beyond relative positional encoding (RPE) methods, Randomized PE (224) and NoPE (127) present approaches that do not rely on modeling the consecutive positions of tokens in the input query. Intriguingly, they posit that by including positions outside the length of the training distribution or by forgoing positional encodings altogether, the model can handle out-of-distribution cases with longer token lengths and exhibit enhanced generalizability on downstream tasks. Randomized PE (224) employs a number greater than the longest sequence encountered during training. It randomly samples from a range of integers, using them as indices after sorting. This approach enables the model to generalize to longer sequences during inference, though it requires prior knowledge of the maximum token length. On the other hand, NoPE completely forgoes the positional encoder in the self-attention mechanism. It demonstrates that the model's self-attention can inherently learn the RPE across tokens in a sentence. This omission not only simplifies the model architecture but also shows promising results in terms of generalizability, especially for sentences with queries extending beyond the training distribution.

### Sparse Modeling

In the quest to optimize Transformers for efficiency, another key area of research focuses on integrating sparse modeling within these attention-based architectures. This approach is pivotal in reducing computational demands, especially in models with a large number of parameters. Two primary directions have emerged in sparse modeling: the Mixture of Experts (MoE) (345; 44; 72; 150; 189; 214; 239; 345) and Sparsefinder (266) from different manners.

The MoE approach (307; 44; 45; 72; 78; 243; 43), incorporates multiple branches or 'experts' in the model, each specializing in different subtasks. During inference, only a subset of these paths is activated, maintaining computational efficiency while potentially enhancing performance. This design enables models like GLaM to scale impressively, activating only 99 billion parameters during inference despite having over 1.2 trillion parameters in total. Further developments in MoE, such as Sparse MoE (345), address issues like representation collapse, ensuring more equal activation of experts and efficient information processing. On the other hand, Sparsefinder (266) takes a different approach by focusing on uncovering sparsity within the attention mechanism itself. This method identifies key patterns through the attention scheme, which helps in efficiently allocating computational resources to the most impactful areas of the model.

### Attention-free

One significant drawback of the vanilla attention mechanism (Zhou et al., 2018) is the quadratic complexity of attention computation, making it especially inefficient for handling long sequences. Although efficient / sparse attention offers some relief, its worst-case theoretical complexity remains unchanged. To address this, various attention-free methods have been proposed, providing alternatives that avoid the computation of the quadratic attention matrix (Zhou et al., 2018; Chen et al., 2018; Chen et al., 2019; Li et al., 2020; Li et al., 2021; Li et al., 2020). These methods could be largely categorized into those that replace the attention mechanism with recurrent computation (Li et al., 2020; Li et al., 2021; Li et al., 2020), and those that discretize state space representations (Li et al., 2020; Li et al., 2021; Li et al., 2020). Notably, these new methods like RWKV (Li et al., 2020), H3 (Zhou et al., 2020), Hyena (Li et al., 2020), and RetNet (Li et al., 2021) achieve performance comparable to the standard Transformer. RWKV (Li et al., 2020) utilizes recurrent neural networks (RNNs) to streamline sequence processing, thereby reducing the complexity of handling long sequences. H3 (Li et al., 2020), based on state space models (SSMs), offers an efficient alternative for data representation and processing. Hyena (Li et al., 2020) presents itself as a drop-in replacement for traditional attention mechanisms, simplifying the Transformer architecture. RetNet (Li et al., 2021) introduces a multi-scale retention module coupled with a feed-forward network module, enhancing parallelism and recurrence, which significantly improves efficiency in both training and inference phases. We include the complexity analysis of these methods compared with the vanilla Transformer with an input query of length \(n\) in Table 1. It provides an overview of how each method scales in complexity, offering insights into the advancements in the attention-free technologies.

## 6. Training and Tuning Efficiency

### Introduction

The development of training and tuning techniques for LLMs must address the challenges posed by the ever-increasing size of data and models. This section delves into the efficiency aspects crucial for both scalable training and tuning of LLMs, highlighting key areas of focus.

**Memory Efficiency.** The rapid growth in the number of parameters in large transformer models, increasing by approximately \(410\times\) every two years, presents significant memory challenges. This growth has outpaced the expansion of GPU memory, which has seen only a \(5\times\) increase (from 16GB to 80GB) over the same period. The actual memory consumption during training far exceeds the raw parameter count, encompassing model states (parameters, gradients, optimizer states), as well as residual states (intermediate activations, temporary buffers, memory fragmentation). Given these constraints, single GPU setups are insufficient for handling entire models, necessitating distributed training approaches like tensor parallelism (TP) and pipeline parallelism (PP) for effective memory management.

\begin{table}
\begin{tabular}{l l c c c} \hline \hline Method & Parallelization & Time Cost & Memory Cost & Performance \\ \hline Transformer (Zhou et al., 2018) & Yes & \(O(n)\) & \(O(n^{2})\) & ++++(+) \\ RWKV (Li et al., 2020) & No & \(O(1)\) & \(O(n)\) & + \\ H3 (Li et al., 2020) & Yes & \(O(1)\) & \(O(n\log n)\) & ++ \\ Hyena (Li et al., 2020) & Yes & \(O(n)\) & \(O(n\log n)\) & + \\ RetNet (Li et al., 2021) & Yes & \(O(1)\) & \(O(n)\) & +++ \\ \hline \hline \end{tabular}
\end{table}
Table 1. Comparisons of parallelization in training time and time/memory cost in inference evaluation between Transformer and attention-free methods when using a sequence of length \(n\) as input. Models with more + under performance are of better performance.

**Computation Efficiency.** While distributed training offers potential benefits in speeding up the training of large models, it also introduces complexities that affect scalability. A notable observation is the decrease in FLOPs per GPU when training is distributed across multiple GPUs, as opposed to a single GPU setup. This decrease stems from the challenges in efficiently utilizing an increasing number of computational resources. Therefore, scalability becomes a crucial element in boosting computation efficiency during the training process, particularly in multi-GPU settings.

**Communication Efficiency.** This aspect relates to the exchange of parameters and gradients between different devices or layers during training. Techniques like all-reduce are employed to synchronize gradients across all devices at the end of backward propagation in data parallel training. The goal is to minimize the volume of communication data during collective operations such as broadcast, reduce, all-reduce, and all-gather.

In short, training and tuning LLMs is a complex challenge that demands a comprehensive approach. An integrated strategy that considers all these efficiency aspects is vital for the effective and scalable training and tuning of LLMs. The subsequent sections will provide a detailed exploration of these aspects.

### Scalable Training

#### 6.2.1. Stable Training Strategies

During the pre-training of LLMs, ensuring training stability is a critical aspect of efficiency. Training instability, often manifested as vanishing or exploding gradients, can significantly hinder the training process. To mitigate these issues, careful selection and adjustment of hyperparameters are essential. One effective approach involves the strategic manipulation of batch size. For example, models like PaLM (Wang et al., 2019) gradually increase their batch size from 1 million to 4 million tokens during training. This gradual scaling helps in accommodating the model's growing capacity to process larger data volumes without compromising stability. Another key hyperparameter is the learning rate, where the warm-up cosine scheduler is commonly employed. This scheduler initially increases the learning rate during the early stages of training (typically 0.1% to 0.5% of total training steps) and then implements a cosine decay strategy. This approach gradually reduces the learning rate to about 10% of its peak value, ensuring a balance between rapid learning and stability as training progresses. The choice of optimizer also plays a pivotal role in stabilizing the training of LLMs. Optimizers like Adam (Kingmae and Ba, 2015) and AdamW (Kingmae and Ba, 2015) are popular choices for models such as GPT-3 (Kingmae and Ba, 2015) and OPT (Kingmae and Ba, 2015), owing to their momentum feature that accelerates convergence by leveraging past gradient information. Additionally, the Adafactor (Kingmae and Ba, 2015) optimizer, known for its GPU memory efficiency, is utilized in models like PaLM and T5 (Kingmae and Ba, 2015). Beyond hyperparameter tuning, implementing stabilizing strategies like weight decay and gradient clipping is common to prevent exploding gradients. However, even with these measures, training loss spikes can still occur, often influenced by both the current model state and the data being processed. To address this, models like PaLM and OPT employ a strategy of restarting the training from a previous checkpoint when a spike is detected, effectively skipping over the data that triggered the instability. This approach ensures not only training stability but also efficient use of computational resources by avoiding prolonged periods of unproductive training.

#### 6.2.2. Mixed Precision Training

In the realm of LLM pre-training, mixed precision training emerges as a critical strategy for enhancing both memory and computational efficiency. Traditionally, neural network training involves storing weights, gradients, and activations in full-precision (FP32) format. However, for extremely large models, this approach can be resource-intensive. To address this, reduced-precision formats like FP16 or INT8 are adopted. These formats not only reduce memory usage but also expedite communication processes within the model. In addition, modern GPUs are typically more adept at handling FP16 computations compared to FP32, offering a further boost in computational speed.

Despite these advantages, transitioning directly from FP32 to FP16 can sometimes lead to performance degradation [114; 335] due to issues like overflow or underflow inherent in FP16. To circumvent these challenges, the automatic mixed-precision (AMP) [184] method has been developed. AMP maintains a master copy of weights in FP32 while employing FP16 for computations during the forward and backward passes. Post-calculation, the weights are converted back to FP32 for updating the master weights. This method, coupled with a loss scaling technique that preserves small gradient values, enables AMP to match the accuracy of FP32 training without the need for extensive hyperparameter tuning. Further advancements in precision reduction have led to the introduction of Brain Floating Point (BF16) [123], a novel half-precision format. BF16, designed to cover the same range as FP32 by allocating more bits to the exponent and fewer to the significand compared to FP16, has demonstrated state-of-the-art performance and more reliability than mixed precision under FP16.

Another innovative approach is Activation Compressed Training (ACT) [33], which focuses on compressing activations to an average of 2 bits across multiple tasks. ACT computes gradients using a compressed version of activations saved during the backward process, leading to a significant reduction in memory requirements for activations. This compression enables training with substantially larger batch sizes, ranging from 6.6\(\times\) to 14\(\times\) larger than traditional methods. Overall, mixed precision training stands as a testament to the evolving landscape of LLM training, where efficiency and performance are continually balanced through innovative techniques.

#### 6.2.3. Parallelism-Based Techniques

Parallelism in the training of LLMs is a strategy that involves distributing the computational workload across multiple accelerators, such as GPUs or TPUs. This approach is crucial for managing the substantial data and complex computations required in LLM training, facilitating the development of more advanced and capable models. In this section, various parallel training schemas are discussed.

**Data Parallelism (DP).** Data parallelism [76; 157; 160; 294] a straightforward yet effective form of distributed training. In this approach, the dataset is divided into smaller subsets, which are then processed in parallel across multiple accelerators. The model is replicated across these devices, with each replica operating on a separate unit. Each unit then independently performs forward and backward computations on its assigned subsets. A key aspect of DP is the synchronization of gradients at the end of each training step. The gradients calculated on each device are averaged, resulting in a gradient representative of the entire batch. This process ensures that despite the parallel processing, the model learns consistently across all subsets of data. DP is particularly noted for its ability to maximize GPU utilization. However, it requires high-bandwidth interconnects to efficiently handle the communication demands between devices. By leveraging DP, training large-scale LLMs becomes more feasible, enabling faster development cycles and the exploration of more complex model architectures.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \multirow{2}{*}{Parallelism Strategy} & \multicolumn{3}{c}{Resource Efficiency} \\ \cline{2-5}  & Memory & Computation & Communication \\ \hline Data Parallelism (DP) & Low & High & High \\ \hline \multirow{4}{*}{Model Parallelism (MP)} & Tensor Parallelism (TP) & High & Low & Low \\  & (Intra-layer) & High & Low & High \\ \cline{2-5}  & Pipeline Parallelism (PP) & High & Low & High \\ \cline{1-1} \cline{2-5}  & (Inter-layer) & High & Low & High \\ \hline \hline \end{tabular}
\end{table}
Table 2. Summary of different parallelism strategies for efficiency.

**Model Parallelism (MP).** Model Parallelism is an alternative approach to DP, focusing on dividing the model itself across multiple accelerators. This method is particularly useful for handling models with a large number of parameters and sizes, especially when a single GPU lacks the capacity to store the entire model. Model Parallelism can be further categorized into two types: Tensor Parallelism (TP) and Pipeline Parallelism (PP).

\(\bullet\) Tensor Parallelism [238; 245; 297] is a form of _intra-layer_ model parallelism. It involves dividing the tensors of individual layers across multiple accelerators, allowing for the training of models that are larger than the memory capacity of a single GPU. A notable example of this approach is Megatron-LM [245], which provides strategies for slicing different components of transformer parameters, such as MLP layers, self-attention layers, and output embedding layers. This slicing can be done either horizontally or vertically. Initially, TP focused on splitting two-dimensional matrices, but it has since evolved to include multi-dimensional splitting. Frameworks like Colossal-AI [21; 296; 272] have expanded TP to higher dimensions and introduced sequence parallelism [139; 158] for handling sequence data. While TP is efficient in terms of memory usage, it requires high interconnect bandwidth for effective layer communications.

\(\bullet\) Pipeline Parallelism [112; 118; 131; 140; 164; 191; 192; 193; 300], on the other hand, is a form of _inter-layer_ model parallelism. It involves splitting the layers of a model across multiple accelerators in a pipeline configuration. Each accelerator is responsible for computing a different layer and then passing its output to the next, akin to an assembly line. This setup allows for sequential yet concurrent processing of both forward and backward passes. While one segment of the model processes one part of the data, other segments can work on different parts simultaneously. The key to the efficiency of PP lies in its ability to keep all parts of the model active and productive, though it requires careful scheduling to minimize idle time on the GPUs.

Gpipe [112], one of the earliest proposals for PP, combines PP with mini-batch splitting. It segments a large model across multiple GPUs and processes input mini-batches as smaller micro-batches. This approach allows for the efficient training of significantly large models. GPipe also uses a strategy called rematerialization [43] to reduce memory usage by recalculating activations during backward propagation instead of storing them. However, GPipe still faces memory inefficiencies due to the need to cache activations during backward computations. PipeDream [191; 98] further refines the PP approach with its One Forward pass followed by One Backward pass (1F1B) strategy. This method allows for the immediate backward propagation of a micro-batch following its forward pass, enabling earlier stages of the pipeline to start their backward computations sooner. PipeDream also employs asynchronous gradient updates using different versions of weights and optimizes memory allocation across the pipeline stages for more efficient processing. BPipe [131] introduces a technique to balance memory usage throughout the pipeline. It leverages idle memory in later stages to support earlier stages, significantly speeding up the training process for large models like GPT-3 96B and GPT-3 134B by 1.25 to 2.17 times compared to Megatron-LM. TeraPipe [164] addresses the challenge of training large models with extensive sequence lengths, which can lead to smaller mini-batches and increased idle time in the pipeline. In transformer architectures, some layers' calculations don't depend on future hidden states. TeraPipe uses this fact to enable parallel processing by dividing the input sequence. It uses dynamic programming to effectively split the sequence at the best points across tokens, improving the efficiency of parallel processing.

**Automated Parallelism.** Automated Parallelism has become a key approach in scaling up modern LLMs, combining various parallelism methods for optimal performance. Systems like DeepSpeed [216], Megatron-LM [245], and Colossal-AI [296] have adopted a 3D parallelism approach, which involves distributing training data uniformly across workers, manually partitioning the model, and distributing layers in each pipeline stage. However, this manual orchestration of parallelism types is complex and not easily adaptable across different models and computing environments.

To streamline this process, automated parallelization solutions are being developed. These solutions aim to speed up model deployment and ensure adaptability across different models. As models and computing clusters grow, the complexity of parallelism configurations also increases. Tofu (Tofu, 2017) tackles this challenge with a dynamic programming algorithm that optimizes dataflow graph partitioning. Dapple (Tofu, 2017) focuses on minimizing pipeline latency through optimal partitioning strategies. However, these solutions are currently limited to combining data parallelism with only one model parallelism type, due to the complex interactions between different parallelism strategies. Alpa (Alpa, 2018) takes a more comprehensive approach by organizing data, model, and pipeline parallelism into a hierarchical structure. It uses integer linear programming for model parallelism plans and dynamic programming for pipeline parallelism plans. Alpa is comparable to specialized systems like Megatron-LM in training GPT models, demonstrating its effectiveness in handling complex parallelization challenges.

FlexFlow (FlexFlow, 2018) extends the concept of 3D Parallelism by proposing a method to divide operation output tensors across different dimensions (Sample, Operation, Attribute, Parameter). Each operation in the computation graph is assigned a specific parallelization configuration. To find the best parallelization strategy, FlexFlow uses an execution simulator that predicts the time required to run an operator graph on a given device topology. It then employs Markov Chain Monte Carlo sampling to systematically search for the optimal strategy, considering both the operator graph and device topology.

#### 6.2.4. Memory Optimization

In the realm of training LLMs with increasing sizes, the memory needed to store model parameters, gradients, and optimization states grows significantly. This challenge is particularly acute in DP, where each GPU traditionally stores a complete copy of the model's parameters, leading to considerable memory redundancy. Efficient memory optimization strategies are essential to train larger models on limited hardware resources, balancing the memory load across different components of the training infrastructure.

ZeRO (Zebro, 2017) addresses the issue of memory redundancy in data parallelism by partitioning the memory load across GPUs. Instead of each GPU storing the entire set of model parameters, gradients, and optimizer states, ZeRO divides these elements, allowing each GPU to hold only a portion of the data. The remaining data can be retrieved from other GPUs as needed. This approach includes three key strategies: parameter partitioning, gradient partitioning, and optimizer state partitioning, each targeting a specific aspect of the model's memory requirements. Building on ZeRO, ZeRO offload (Zebro, 2018) extends these concepts to enable the training with the usage of both CPU and GPU capabilities, offloading some computations and storage to the CPU to alleviate the memory burden on GPUs. However, this offloading increases communication between the CPU and GPU, which can become a bottleneck if not managed carefully. The strategy involves viewing the training process as a data flow graph, with different computation nodes assigned to different devices. The forward and backward processes are handled by the GPU, while parameter updates and precision conversions are managed by the CPU. This approach aims to minimize CPU computation and reduce communication overhead, ensuring efficient use of CPU and GPU resources in the training process.

Integrating these advancements, systems like DeepSpeed (Zebro, 2017) offer different levels of memory optimization. The first stage is ZeRO-DP (Data Parallelism) which optimizes memory by partitioning only the optimizer states across GPUs. The second stage is ZeRO-R (Reduction and Partitioning) to further reduce memory usage by partitioning gradients and optimizer states. The third stage is ZeRO-Infinity that extends the memory optimization beyond what is available on the GPU, utilizing both CPU and NVMe memory to enable training of extremely large models.

### Scalable Tuning

Large Language Models trained on massive and varied datasets have demonstrated remarkable general problem-solving capabilities. However, their performance can be significantly enhanced for specific domains or tasks through targeted adaptation. In recent years, a range of techniques has emerged to facilitate this adaptation process. This section discusses two primary approaches for the efficient adaptation of pretrained LLMs: _(i)_ parameter-efficient fine-tuning, which involves incorporating adapter layers or fine-tuning existing parameters of the pretrained models, and _(ii)_ the integration of task-specific context via prompt engineering. These methods represent key strategies in tailoring LLMs to specific applications, ensuring both their versatility and effectiveness in diverse NLP tasks.

#### 6.3.1. Parameter-Efficient Fine-Tuning (PEFT)

The substantial size of pretrained LLMs makes them expensive or impractical to fully fine-tune the entire models for the downstream task or application domain. To avoid directly fine-tuning the full LLMs, a range of parameter-efficient tuning methods have a variety of parameter-efficient tuning methods have emerged. These methods focus on refining LLMs by adjusting or introducing a small number of trainable parameters, while keeping most or all of the original pretrained parameters fixed. Such methods typically attain commendable performance and bring significant reductions in the quantity of trainable parameters. They enhance both memory and computational efficiency in comparison to full parameter tuning, offering more practical solutions for adapting LLMs to specific tasks.

**Partial Parameter Tuning.** A straightforward yet effective approach in adapting LLMs is partial parameter tuning, where only a selected fraction of pretrained parameters are fine-tuned, leaving the rest unchanged. This method has been widely demonstrated. For example, the works (Liu et al., 2019; Liu et al., 2020) fine-tune only a few final layers, achieving up to 90% of the performance of a fully fine-tuned model. Xie et al. (2021) involves selecting a subset of layers for fine-tuning based on the variability in their hidden states, particularly for classification tasks. Additionally, BitFit (Liu et al., 2020) presents an alternative strategy by adjusting only the bias terms in transformer-based LLMs, yielding competitive performance. These examples underscore the potential of partial parameter tuning as a resource-efficient way to adapt LLMs for various applications. However, these methods typically lack detailed principle to guide how to select a subset of parameters for further tuning.

**Model-Adapter tuning.** To tackle the issue of selecting specific parameters for fine-tuning, the technique of adapter tuning has been introduced, which involves augmenting the pre-trained model with additional small-scale learnable blocks, known as adapters (Liu et al., 2020). Such approaches maintain the integrity of the pre-trained model, yet embed adapter blocks into one or several modules of the pretrained LLMs. These adaptors typically take the form of compact bottleneck layers. One example is comprising a two-layer MLP (Multi-Layer Perceptron) with a nonlinearity function and a small number of neurons in the hidden layer. The adaptor integration can be executed in series (Liu et al., 2020) or in parallel (Liu et al., 2020) with the attention and feed-forward layers of the Transformer architecture, or outside of the Transformer architecture (Liu et al., 2020). To further enhance the reuse and versatility of adapters, AdapterHub (Liu et al., 2020) has been developed. This framework allows for the dynamic integration of pre-trained adapters, catering to a variety of tasks and LLMs. Although the use of adapters accelerate the fine-tuning process and mitigates storage requirements, its does modify the computational graph by adding depth or width to each transformer layer. Such modification results in a slight increase in inference latency, as observed in studies (Liu et al., 2020), where inference speeds were found to be slower by approximately 4-6%.

**Parameter-Adapter tuning.** Another related approach is to directly add an adapter to the model parameters. Denoting the pre-trained network parameters as \(\mathbf{\theta}\), this class of techniques expands the model parameters to \(\mathbf{\theta}+\Delta\theta\), with \(\theta\) being fixed and \(\Delta\theta\) being learned by low-rank approximations. An implementation of this technique is diff-pruning (Zhu et al., 2020) that learns task-specific sparse parameters \(\Delta\theta\) by adding a sparse promoting regularization during fine-tuning. The method LoRA (Liu et al., 2020) learns low-rank transformations for each linear layer. In particular, LoRA reparameterize the weight matrix as \(\theta+\Delta\theta\approx\theta+BA\), where the pretrained weight matrix \(\theta\) is fixed, yet the low-rank matrices \(B\) and \(A\) are learnable. In LoRA, all weight matrices share a constant intrinsic rank for each low-rank sub-matrix, while not accounting for the varying importance across different modules. AdaLoRA (Srivastava et al., 2017) addresses this limitation by dynamically allocating parameter budgets to weight matrices based on their importance scores. It assigns higher ranks to more critical incremental matrices, capturing more detailed task-specific information, while reducing the rank of less important matrices to avoid overfitting and save computational resources. SoRA(Srivastava et al., 2017) introduces an optimize-able gate that dynamically adjusts the rank of the incremental matrix using the proximal gradient method. In QLoRA (Srivastava et al., 2017), the pretrained model is initially quantized to 4 bits. Subsequently, a small set of learnable low-rank adapter weights are augmented and fine-tuned using backpropagated gradients over the quantized weights. QLoRA can match the performance of 16-bit full-parameter fine-tuning even with 16-bit, 8-bit, or 4-bit adapters.

#### 6.3.2. Data-Efficient Tuning

Data-efficient tuning refers to the process of updating a limited set of prompt parameters for downstream tasks, instead of fine-tuning the pretrained LLM. It is typically achieved through prompt tuning, where the weights of a pretrained model are kept fixed, yet only the added prompt tokens are adjusted. Such approaches enable a more efficient use of data and often yield enhanced performance, particularly as the scale of the model parameters increases.

**Prompt Tuning.** Prompt tuning is a technique used to enhance the performance of LLMs in supervised downstream tasks. It formulates the downstream task into a masked language problem and converts the original token input into a template and masking certain tokens unfilled for the LLMs to complete. By modifying the tunable template embedding, prompt tuning aims to improving performance in the downstream tasks via reducing the distribution shift between the pretrained tasks and the specified downstream tasks. This method also enables the LLM to engage in few-shot or even zero-shot learning, especially useful in scenarios with limited supervised data, by generating new prompt templates.

Traditional methods required manual design of prompt templates and verbalizers, which often resulted in sensitive and varying efficacy. However, recent advancements in prompt learning have led to the automation and optimization of prompt construction. AutoPrompt (Wang et al., 2019) introduces a gradient-based approach to automate the search for effective templates. LM-BFF (Wang et al., 2019) offers a more efficient solution for automated prompt generation by searching for label words and using a T5-based template generation method in the discrete prompt space. To tackle the challenges of discrete optimization, Prefix-Tuning (Zhou et al., 2019) recommends parameterized prompts, where only the prompt is fine-tuned while the LLM remains unaltered. P-tuning (Zhou et al., 2019) breaks away from the conventional constraint that templates must be composed of natural language, transforming template construction into a continuous parameter optimization challenge. CP-tuning (Zhou et al., 2019) advocates the use of contrastive learning to automatically learn the distribution of embeddings, serving as a substitute for manual verbalizer design. UPT (Wang et al., 2019) introduces the Prompt-Options-Verbalizer paradigm, facilitating joint prompt learning across diverse NLP tasks and encouraging LLMs to acquire task-invariant prompting knowledge.

## 7. Inference Efficiency

### Introduction

The enormous number of parameters in Large Language Models (LLMs) poses significant challenges for deployment on cloud services and resource-limited devices, leading to high maintenance costs for inference support. Consequently, accelerating inference has become a pressing issue garnering attention from both industry and academia. One common way is to construct compact model that could reach competitive performance to the full model, which methods can be broadly classified into four categories: pruning, knowledge distillation, quantization, and low-rank decomposition. Pruning techniques focus on identifying and eliminating redundancy within the operators of Deep Neural Networks (DNNs), thereby creating more 

[MISSING_PAGE_FAIL:22]

LoRAShear (Srivastava et al., 2017) is recently proposed in the limited resource setup. LoRAShear utilizes a novel structure sparse optimizer called LoRA Half-Space Projected Gradient (LHSPG) to conduct progressive structured pruning and transfer the knowledge. Unlike the prior works only using instructed-fine-tuning data, a multi-stage knowledge recovery mechanism is applied for LoRAShear to effectively narrow down the performance gap between the full and compressed LLMs. For full-resource setups, Sheared-LLaMA (Srivastava et al., 2017) performs structured pruning on original LLMs to create compact models that outperform equally sized LLMs trained from scratch. However, it requires significant GPU power and data resources, which may not be feasible for the public users. Compared with the relatively mature domain of structurally pruning middle-small size DNNs, structured pruning on LLMs is still in the early stage and awaiting for further explorations.

### Knowledge Distillation

The concept of knowledge distillation involves utilizing supervisory signals from a large, more capable 'teacher' model to train a compact'student' model. This approach often results in the student model surpassing the performance of a similarly sized model that was trained without such guidance (Krizhevsky et al., 2017). Knowledge distillation can be largely categorized into response-based, feature-based, and relation-based knowledge distillation (Rasmaldi et al., 2017). Response-based knowledge focuses on the final output layer of the teacher model. The hypothesis is that the student model will learn to mimic the predictions of the teacher model (Krizhevsky et al., 2017; Krizhevsky et al., 2017). A trained teacher model also captures feature-based knowledge of the data in its intermediate layers, which is especially pertinent for deep neural networks (Srivastava et al., 2017; Li et al., 2018). Knowledge that captures the relationship between feature maps can also be used to train a student model, referring as relation-based (Srivastava et al., 2017). Initial research in NLP domain primarily concentrated on the distillation of task-specific models (Li et al., 2018). Later on, more studies have shifted their focus towards distilling pre-trained models, which can subsequently be fine-tuned for specialized downstream tasks (Krizhevsky et al., 2017; Krizhevsky et al., 2017; Li et al., 2018). Recently, there emerge distillation methods for LLMs (Krizhevsky et al., 2017; Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018). One main focus of the current knowledge distillation methods on LLMs lies in how to generate and utilize challenging (instructed) samples (Li et al., 2018; Li et al., 2018; Li et al., 2018) to more effectively transfer the knowledge from teacher to student model, which has some overlapping to the data undersampling methods in Section 4. Chain-of-thought prompting is commonly used in distillation approaches (Krizhevsky et al., 2017; Li et al., 2018) to accomplish the data generations.

### Quantization

Quantization methods can be divided based on the necessity for retraining (Srivastava et al., 2017). Quantization-Aware Training (QAT) mandates model retraining, adjusting its weights to recover accuracy post-quantization (Krizhevsky et al., 2017; Li et al., 2018; Li et al., 2018). In contrast, Post-Training Quantization (PTQ) achieves quantization without any retraining (Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018). Although QAT often yields superior accuracy, it is frequently impractical for Large Language Models (LLMs) due to the prohibitive costs of retraining and, usually, the lack of access to original training data and processing infrastructure. As a result, most research on LLM quantization gravitates towards PTQ techniques.

From another perspective, quantization methods can be broadly classified into uniform and non-uniform approaches (Srivastava et al., 2017). Uniform quantization, as explored in works like SPQR (Srivastava et al., 2017), GPTQ (Srivastava et al., 2017), and others (Krizhevsky et al., 2017; Li et al., 2018), involves dividing the range of weights into equally sized bins. This method has become popular for its ability to accelerate computation by allowing arithmetic operations in quantized precision rather than full precision. Additionally, uniform quantization may not be optimal in cases where the weight distribution is non-uniform, as often observed in LLMs. Contrarily, non-uniform quantization offers a solution to these challenges. As studied in SqueezeLLM (Krizhevsky et al., 2017), this approach allocates quantization bins non-uniformly, allowing for more flexibility and potentially better performance, especially when dealing with non-uniform weight distributions.

Compared with structured pruning, quantization requires specified hardware to realize the realistic advantage of low-bit precision to reduce the memory cost and inference speedup. For the LLM, due to lack of training data or the computing resources, structured pruning is typically difficult to effectively recover lost knowledge under high compression ratio. However, quantization could typically preserve the performance of LLM effectivelly. Therefore, quantization at the present is more popular and mature used in the LLM compression. It is in sharp contrast to the middle-small model size scenarios, where structured pruning and quantization are both commonly (jointly) used (Levy and Zisserman, 2017; Levy and Zisserman, 2018).

### Low-Rank Decomposition

The weight matrices in a DNN are often low-rank, indicating redundancy in model weights (Zhou et al., 2017; Li et al., 2018; Li et al., 2019). Thus, a natural idea is to factorize the weight matrices into two or more smaller matrices to save parameters. In LLMs, the weight matrices exist in linear layers including self-attention layers and MLP layers, and the embedding layers. There exists studies to factorize these weight matrices for saving parameter quantity and accelerate inference.

**Decomposition on Linear Layer.** Multi-linear attention (Kumar et al., 2018) uses block-term tensor (BTT) decomposition (Kumar et al., 2018) to factorize multi-head attention. Singular value decomposition (SVD) (Kumar et al., 2018) is also commonly used and typically performed with a two-stage manner. The first stage is to establish the decomposition followed by a second stage to fine-tune the low-rank weights via knowledge distillation (Kumar et al., 2018). Besides, as an alternative to BTT and SVD, Kronecker decomposition retains the rank of the matrix and has shown improvement during compressing BERT and GPT-2 (Kumar et al., 2018; Li et al., 2019).

**Decomposition on Embedding Layer.** ALBERT (Han et al., 2019) uses factorization for the embedding layer, which is one of the largest consumers of model parameters. Since the power of Transformer mainly comes from its contextual learning ability, the parameters in the token embedding layer are not efficient. It intuitively makes sense to reduce them by factorizing the embedding matrix. Self-Attentive Factorized embeddings (SAFE) (Zhou et al., 2017) studied ways to share weights in transformers by adding a small self-attention layer on the basis of linear projection to achieve better performance than the alternatives. LightFormer (Kumar et al., 2018) more effectively utilizes the parameter knowledge of the well-trained Transformer, and accelerates the convergence of the model factorization on the embedding layers.

## 8. Conclusion

In conclusion, the evolution of Large Language Models (LLMs) marks a significant milestone in the field of artificial general intelligence, bringing transformative changes across various domains. However, the rapid expansion of these models brings forth substantial challenges in terms of computational demands and memory requirements, creating hurdles for both academic research and practical deployment. This survey provided a comprehensive overview of the algorithmic innovations aimed at enhancing the efficiency of LLMs, capturing research developments mostly up to September 2023. Moving beyond the scope of the existing surveys that often focus on isolating aspects such as training or model compression, this survey delved into the multiple dimensions of efficiency that are crucial for the holistic algorithmic development of LLMs. It has spanned a broad array of efficiency-related topics including scaling laws, data utilization, architectural designs, as well as training, tuning, and inference strategies. The insights and analyses presented here aim to serve as valuable summarization for both researchers and practitioners in the field. By laying a solid foundation of current knowledge and approaches, this paper sets the stage for future breakthroughs and continued innovation in the crucial research area of LLM efficiency.

[MISSING_PAGE_EMPTY:25]

* Chen et al. (2023) Hongzhan Chen, Siyue Wu, Xiaojun Quan, Rui Wang, Ming Yan, and Ji Zhang. 2023. MCC-KD: Multi-CoT Consistent Knowledge Distillation. _arXiv preprint arXiv:2310.14747_ (2023).
* Chen et al. (2023) Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu, Xuetao Ma, Yifan Yangong, and Junbo Zhao. 2023. Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning. _arXiv preprint arXiv:2305.09246_ (2023).
* Chen et al. (2021) Jianfiei Chen, Liammin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica, Michael Mahoney, and Joseph Gonzalez. 2021. Actm: Reducing training memory footprint via 2-bit activation compressed training. In _International Conference on Machine Learning_. PMLR, 1803-1813.
* Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. _arXiv preprint arXiv:2107.03374_ (2021).
* Chen et al. (2023) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023. Extending context window of large language models via positional interpolation. _arXiv preprint arXiv:2306.15595_ (2023).
* Chen et al. (2020) Tianyi Chen, Tianyu Ding, Bo Ji, Guanyi Wang, Yixin Shi, Jing Tian, Sheng Yi, Xiao Tu, and Zhihui Zhu. 2020. Orthant Based Proximal Stochastic Gradient Method for l1-Regularized Optimization. In _ECML PKDD_. 57-73.
* Chen et al. (2023) Tianyi Chen, Tianyu Ding, Badad Yadav, Ilya Zharkov, and Luming Liang. 2023. LoRSAber: Efficient Large Language Model Structured Pruning and Knowledge Recovery. _arXiv preprint arXiv:2310.18356_ (2023).
* Chen et al. (2021) Tianyi Chen, Bo Ji, Tianyu Ding, Biyi Fang, Guanyi Wang, Zhihui Zhu, Luming Liang, Yixin Shi, Sheng Yi, and Xiao Tu. 2021. Only train once: A one-shot neural network training and pruning framework. _Advances in Neural Information Processing Systems_ (2021), 19637-19651.
* Chen et al. (2020) Tianyi Chen, Bo Ji, Yixin Shi, Tianyu Ding, Biyi Fang, Sheng Yi, and Xiao Tu. 2020. Neural network compression via sparse optimization. _arXiv preprint arXiv:2011.04868_ (2020).
* Chen et al. (2023) Tianyi Chen, Luming Liang, Tianyu Ding, and Ilya Zharkov. 2023. Towards Automatic Neural Architecture Search within General Super-Networks. _arXiv preprint arXiv:2305.18030_ (2023).
* Chen et al. (2023) Tianyi Chen, Luming Liang, Tianyu Ding, Zhihui Zhu, and Ilya Zharkov. 2023. OTOv2: Automatic, Generic, User-Friendly. In _The Eleventh International Conference on Learning Representations_.
* Chen et al. (2020) Tianyi Chen, Guanyi Wang, Tianyu Ding, Bo Ji, Sheng Yi, and Zhihui Zhu. 2020. Half-space proximal stochastic gradient method for group-sparsity regularized problem. _arXiv preprint arXiv:2009.12078_ (2020).
* Chen et al. (2016) Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training deep nets with sublinear memory cost. _arXiv preprint arXiv:1604.06174_ (2016).
* Chen et al. (2023) Tianlong Chen, Zhenyu Zhang, Ajay Jaiswal, Shiwei Liu, and Zhangyang Wang. 2023. Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers. _arXiv preprint arXiv:2303.01610_ (2023).
* Chen et al. (2023) Xin Chen, Hengheng Zhang, Xiaotao Gu, Kaifeng Bi, Lingxi Xie, and Qi Tian. 2023. Pipeline MoE: A Flexible MoE Implementation with Pipeline Parallelism. _arXiv preprint arXiv:2304.11414_ (2023).
* Chen et al. (2022) Yunhong Chen, Zhenxiang Xiao, Lin Zhao, Lu Zhang, Haixing Dai, David Weizhong Liu, Zihao Wu, Changhe Li, Tuo Zhang, Changying Li, et al. 2022. Mask-guided vision transformer (mg-vii) for few-shot learning. _arXiv preprint arXiv:2205.09995_ (2022).
* Chen et al. (2023) Zeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal, and Kyle Richardson. 2023. DISCO: distilling counterfactuals with large language models. In _Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_. 5514-5528.
* Chi et al. (2022) Ta-Chung Chi, Ting Han Fan, Peter J Ramadge, and Alexander Rudnicky. 2022. Kerple: Kernelized relative positional embedding for length extrapolation. _NeurIPS_ 35 (2022), 8386-8399.
* Chi et al. (2023) Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. 2023. Dissecting transformer length extrapolation via the lens of receptive field analysis. In _ACL_. 13522-13537.
* Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality.
* Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. _arXiv preprint arXiv:1904.10509_ (2019).
* Choquette et al. (2021) Jack Choquette, Wishwesh Gandhi, Olivier Giroux, Nick Stam, and Ronny Krshinsky. 2021. NVIDIA A100 tensor core GPU: Performance and innovation. _IEEE Micro_ 41, 2 (2021), 29-35.
* Choromanski et al. (2021) Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. 2021. Rethinking attention with performers. _ICLR_ (2021).
* Chowbary et al. (2022) Aakankankha Chowbary, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_ (2022).
* Christiino et al. (2017) Paul F Christiino, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. _NeurIPS_ 30 (2017).
* Chung et al. (2023) John Joon Young Chung, Ece Kamar, and Saleema Amershi. 2023. Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions. _arXiv preprint arXiv:2306.04140_ (2023).
* Dai et al. (2023) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. 2023. InstructHILP: Towards General-purpose Vision-Language Models with Instruction Tuning. _ArXiv_ abs/2305.06500 (2023).

* Dai et al. (2019) Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. _arXiv preprint arXiv:1901.02860_ (2019).
* Dao (2023) Tri Dao. 2023. Flashattention-2: Faster attention with better parallelism and work partitioning. _arXiv preprint arXiv:2307.08691_ (2023).
* Dao et al. (2022) Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Attri Rudra, and Christopher Re. 2022. Monarch: Expressive structured matrices for efficient and accurate training. In _ICML_. 4690-4721.
* Dao et al. (2022) Tri Dao, Dan Fu, Stefafano Ermon, Atri Rudra, and Christopher Re. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. _NeurIPS_ 35 (2022), 16344-16359.
* Dao et al. (2022) Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Re. 2022. Hungry hungry huppos: Towards language modeling with state space models. _arXiv preprint arXiv:2212.14052_ (2022).
* Dao et al. (2019) Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher Re. 2019. Learning fast algorithms for linear transforms using butterfly factorizations. In _ICML_. PMLR, 1517-1527.
* Le Lathauwer (2008) Lieven De Lathauwer. 2008. Decompositions of a higher-order tensor in block terms-Part II: Definitions and uniqueness. _SIAM J. Matrix Anal. Appl._ 30, 3 (2008), 1033-1066.
* Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. _NeurIPS_ 35 (2022), 30318-30332.
* Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. _arXiv preprint arXiv:2305.14314_ (2023).
* Dettmers et al. (2023) Tim Dettmers, Ruslan Svischevski, Vage Egiazarian, Denis Kuzneedelev, Elias Frantar, Saleh Ashkhoos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. 2023. SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression. _arXiv preprint arXiv:2306.03078_ (2023).
* Ding et al. (2023) Jiyu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. 2023. Longnet: Scaling transformers to 1,000,000,000 tokens. _arXiv preprint arXiv:2307.02486_ (2023).
* Ding et al. (2023) Ning Ding, Xingtai Lv, Qiasoen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, and Maosong Sun. 2023. Sparse Low-rank Adaptation of Pre-trained Language Models. _arXiv preprint arXiv:2311.11696_ (2023).
* Ding et al. (2022) Tianyu Ding, Luming Liang, Zhihui Zhu, Tianyi Chen, and Ilya Zharkov. 2022. Sparsity-guided Network Design for Frame Interpolation. _arXiv preprint arXiv:2209.04551_ (2022).
* Ding et al. (2021) Tianyu Ding, Luming Liang, Zhihui Zhu, and Ilya Zharkov. 2021. Cdfi: Compression-driven network design for frame interpolation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_. 8001-8011.
* Du et al. (2022) Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022. Glam: Efficient scaling of language models with mixture-of-experts. In _ICML_. PMLR, 5547-5569.
* Edalati et al. (2021) Ali Edalati, Marzieh Tahaei, Ahmad Rashid, Vahid Partovi Nia, James J Clark, and Mehdi Rezagholizadeh. 2021. Kronecker decomposition for gpt compression. _arXiv preprint arXiv:2110.08152_ (2021).
* Eldan and Li (2023) Ronen Eldan and Yuanzhi Li. 2023. TinyStories: How Small Can Language Models Be and Still Speak Coherent English? _arXiv preprint arXiv:2305.07759_ (2023).
* Elman (1993) Jeffrey L Elman. 1993. Learning and development in neural networks: The importance of starting small. _Cognition_ 48, 1 (1993), 71-99.
* Fan et al. (2021) Shiqing Fan, Yi Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen Zheng, Chuan Wu, Guoping Long, Jun Yang, Lixue Xia, et al. 2021. DAPPLE: A Pipelined Data Parallel Approach for Training Large Models. (2021).
* Fang et al. (2023) Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang. 2023. Depgraph: Towards any structural pruning. _arXiv preprint arXiv:2301.12900_ (2023).
* Fedus et al. (2022) William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. _The Journal of Machine Learning Research_ 23, 1 (2022), 5232-5270.
* Frantar et al. (2022) Elias Frantar, Saleh Ashkhoos, Torsten Hoefler, and Dan Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained transformers. _arXiv preprint arXiv:2210.17323_ (2022).
* Fu et al. (2023) Daniel Y Fu, Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, Armin W Thomas, Benjamin Spector, Michael Poli, Atri Rudra, and Christopher Re. 2023. Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture. In _NeurIPS_.
* Gal et al. (2017) Yarin Gal, Rishad Islam, and Zoubin Ghahramani. 2017. Deep bayesian active learning with image data. In _ICML_. PMLR, 1183-1192.
* Gae et al. (1992) William A Gae, Kenneth W Church, and David Yarowsky. 1992. A method for disambiguating word senses in a large corpus. _Computers and the Humanities_ 26 (1992), 415-439.
* Gao et al. (2020) Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. _arXiv preprint arXiv:2012.15723_ (2020).
* Geiping and Goldstein (2023) Jonas Geiping and Tom Goldstein. 2023. Cramming: Training a Language Model on a single GPU in one day.. In _ICML_. PMLR, 11117-11143.
* Greva et al. (2021) Mor Greva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. _TACL_ 9 (2021), 346-361.
* Gholami et al. (2022) Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. 2022. A survey of quantization methods for efficient neural network inference. In _Low-Power Computer Vision_. Chapman and Hall/CRC, 291-326.
* Gissin and Shalev-Shwartz (2019) Daniel Gissin and Shai Shalev-Shwartz. 2019. Discriminative active learning. _arXiv preprint arXiv:1907.06347_ (2019).

* Gou et al. (2021) Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. 2021. Knowledge distillation: A survey. _IJCV_ 129 (2021), 1789-1819.
* Gu et al. (2022) Albert Gu, Karan Goel, Ankit Gupta, and Christopher Re. 2022. On the parameterization and initialization of diagonal state space models. _NeurIPS_ 35 (2022), 35971-35983.
* Gu et al. (2021) Albert Gu, Karan Goel, and Christopher Re. 2021. Efficiently modeling long sequences with structured state spaces. _arXiv preprint arXiv:2111.00396_ (2021).
* Gu et al. (2023) Yunian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023. Knowledge Distillation of Large Language Models. _arXiv preprint arXiv:2306.08543_ (2023).
* Gunasekar et al. (2023) Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikiv, et al. 2023. Textbooks Are All You Need. _arXiv preprint arXiv:2306.11644_ (2023).
* Guo et al. (2021) Demi Guo, Alexander M Rush, and Yoon Kim. 2021. Parameter-Efficient Transfer Learning with Diff Pruning. In _ACL-IJCNLP_ 4884-4896.
* Gupta et al. (2022) Ankit Gupta, Albert Gu, and Jonathan Berant. 2022. Diagonal state spaces are as effective as structured state spaces. _NeurIPS_ 35 (2022), 22982-22994.
* Gyorgyi and Tishby (1990) Geza Gyorgyi and Naftali Tishby. 1990. Statistical theory of learning a rule. _Neural networks and spin glasses_ (1990), 3-36.
* Ham et al. (2020) Tae Jun Ham, Sung Jun Jung, Seonghak Kim, Young H Oh, Yeonhong Park, Yoonho Song, Jung-Hun Park, Sanghee Lee, Kyoung Park, Jae W Lee, et al. 2020. A '3: Accelerating attention mechanisms in neural networks with approximation. In _HPCA_. 328-341.
* Ham et al. (2021) Tae Jun Ham, Yejin Lee, Seong Hoon Seo, Soosung Kim, Hyunji Choi, Sung Jun Jung, and Jae W Lee. 2021. ELSA: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks. In _ISCA_. 692-705.
* Harlap et al. (2018) Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg Ganger, and Phil Gibbons. 2018. Pipedream: Fast and efficient pipeline parallel dnn training. _arXiv preprint arXiv:1806.03377_ (2018).
* Hassid et al. (2022) Michael Hassid, Hao Peng, Daniel Rotem, Jungo Kasai, Ivan Montero, Noah A Smith, and Roy Schwartz. 2022. How much does attention actually attend? Questioning the Importance of Attention in Pretrained Transformers. _ACL Findings_ (2022).
* Hendy et al. (2023) Amr Hendy, Mohamed Abdelrahim, Amr Sharaf, Vikas Rausk, Mohamed Gahr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How good are gpt models at machine translation? a comprehensive evaluation. _arXiv preprint arXiv:2302.09210_ (2023).
* Henighan et al. (2020) Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. 2020. Scaling laws for autoregressive generative modeling. _arXiv preprint arXiv:2010.14701_ (2020).
* Hernandez et al. (2021) Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 2021. Scaling laws for transfer. _arXiv preprint arXiv:2102.01293_ (2021).
* Hestness et al. (2017) Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. 2017. Deep learning scaling is predictable, empirically. _arXiv preprint arXiv:1712.00409_ (2017).
* Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_ (2015).
* Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long short-term memory. _Neural computation_ 9, 8 (1997), 1735-1780.
* Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_ (2022).
* Houlsby et al. (2019) Neil Houlsby, Andrei Giurgui, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In _ICML_. PMLR, 2790-2799.
* Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. _arXiv preprint arXiv:2305.02301_ (2023).
* Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Walls, Zeyuan Allen-Zhu, Yuanli Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_ (2021).
* Hu et al. (2010) Rong Hu, Brian Mac Namee, and Sarah Jane Delany. 2010. Off to a good start: Using clustering to select the initial training set in active learning. (2010).
* Huang and Chang (2022) Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: A survey. _arXiv preprint arXiv:2212.10403_ (2022).
* Huang et al. (2019) Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. 2019. Gpipe: Efficient training of giant neural networks using pipeline parallelism. _NeurIPS_ 32 (2019).
* Huang et al. (2023) Yafeng Huang, Huanrui Yang, Zhen Dong, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Yuan Du, Shanghang Zhang, and Kurt Keutzer. 2023. Output Sensitivity-Aware DETR Quantization. (2023).
* Hubara et al. (2017) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 2017. Quantized neural networks: Training neural networks with low precision weights and activations. _The Journal of Machine Learning Research_ 18, 1 (2017), 6869-6898.
* Jaiswal et al. (2023) Ajay Jaiswal, Shiwei Liu, Tianlong Chen, and Zhangyang Wang. 2023. The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter. _arXiv preprint arXiv:2306.03805_ (2023).
* Jelinek (1998) Frederick Jelinek. 1998. _Statistical methods for speech recognition_. MIT press.
* Ji and Chen (2022) Bo Ji and Tianyi Chen. 2022. FSCNN: A Fast Sparse Convolution Neural Network Inference System. _arXiv preprint arXiv:2212.08815_ (2022).
* Jia et al. (2019) Zhihao Jia, Matei Zaharia, and Alex Aiken. 2019. Beyond Data and Model Parallelism for Deep Neural Networks. _Proceedings of Machine Learning and Systems_ 1 (2019), 1-13.
* Jiang et al. (2019) Angela H Jiang, Daniel L-K Wong, Giulio Zhou, David G Andersen, Jeffrey Dean, Gregory R Ganger, Gauri Joshi, Michael Kaminksy, Michael Kozuch, Zachary C Lipton, et al. 2019. Accelerating deep learning by focusing on the biggest losers. _arXiv preprint arXiv:1910.00762_ (2019).
* Jiang et al. (2023) Yuxin Jiang, Chunkit Chan, Mingyang Chen, and Wei Wang. 2023. Lion: Adversarial Distillation of Closed-Source Large Language Model. _arXiv preprint arXiv:2305.12870_ (2023).

* Jiao et al. (2019) Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding. _arXiv preprint arXiv:1909.10351_ (2019).
* Kadotani et al. (2021) Sora Kadotani, Tomoyuki Kajiwara, Yuki Arase, and Makoto Onizuka. 2021. Edit distance based curriculum learning for paraphrase generation. In _ACL-IJCNLP Workshop_. 229-234.
* Kalamkar et al. (2019) Dhiraj Kalamkar, Dheevats Mukojere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammmalamadaka, Jianyu Huang, Hector Yuen, et al. 2019. A study of BFLOAT16 for deep learning training. _arXiv preprint arXiv:1905.12322_ (2019).
* Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_ (2020).
* Katharopoulos and Fleuret (2017) Angelos Katharopoulos and Francois Fleuret. 2017. Biased importance sampling for deep neural network training. _arXiv preprint arXiv:1706.00043_ (2017).
* Katharopoulos and Fleuret (2018) Angelos Katharopoulos and Francois Fleuret. 2018. Not all samples are created equal: Deep learning with importance sampling. In _International conference on machine learning_. PMLR, 2525-2534.
* Kazemmejad et al. (2023) Amihossein Kazemmejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. 2023. The Impact of Positional Encoding on Length Generalization in Transformers. _NeurIPS_ (2023).
* Ming-Wei Chang Kenton and Toutanova (2019) Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In _NAACL-HLT_. 4171-4186.
* Kim et al. (2021) Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. 2021. I-bert: Integer-only bert quantization. In _ICML_. PMLR, 5506-5518.
* Kim et al. (2023) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. 2023. SqueezeLLM: Dense-and-Sparse Quantization. _arXiv preprint arXiv:2306.07629_ (2023).
* Kim et al. (2023) Thebaum Kim, Hyoungio Kim, Gyeng-In Yu, and Byung-Gon Chun. 2023. BPipe: Memory-Balanced Pipeline Parallelism for Training Large Language Models. In _Proceedings of the 40th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 202)_, Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). 16639-16653.
* Kim and Rush (2016) Yoon Kim and Alexander M Rush. 2016. Sequence-level knowledge distillation. _arXiv preprint arXiv:1606.07947_ (2016).
* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_ (2014).
* Kirsch et al. (2019) Andreas Kirsch, Joost Van Amersfoort, and Yarin Gal. 2019. Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning. _NeurIPS_ 32 (2019).
* Kitaev et al. (2021) Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2021. Reformer: The efficient transformer. _ICLR_ (2021).
* Kocmi and Bojar (2017) Tom Kocmi and Ondrej Bojar. 2017. Curriculum learning and minibatch bucketing in neural machine translation. _arXiv preprint arXiv:1707.09533_ (2017).
* Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. _NeurIPS_ 35 (2022), 22199-22213.
* Komatsuzaki (2019) Aran Komatsuzaki. 2019. One epoch is all you need. _arXiv preprint arXiv:1906.06669_ (2019).
* Korthikanti et al. (2023) Vijay Anand Korthikanti, Jared Casper, Sangkug Iym, Lawrence McKee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. 2023. Reducing activation recomputation in large transformer models. _Proceedings of Machine Learning and Systems_ 5 (2023).
* Kosson et al. (2021) Arik Kosson, Vitally Chiley, Abhinav Venigalla, Joel Hestness, and Urs Koster. 2021. Pipelined backpropagation at scale: training large models without batches. _Proceedings of Machine Learning and Systems_ 3 (2021), 479-501.
* Kovaleva et al. (2019) Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. Revealing the Dark Secrets of BERT. In _EMNLP-IJCNLP_. 4365-4374.
* Kumar et al. (2010) M Kumar, Benjamin Packer, and Daphne Koller. 2010. Self-paced learning for latent variable models. _NeurIPS_ 23 (2010).
* Kurtic et al. (2023) Eldar Kurtic, Denis Kuzmedekiev, Elias Frantar, Michael Goin, and Dan Alistarh. 2023. Sparse Finetuning for Inference Acceleration of Large Language Models. _arXiv preprint arXiv:2310.06927_ (2023).
* Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Liannin Zheng, Cody Yu, Joey Gonzalez, Hao Zhang, and Ion Stoica. 2023. vllm: Easy, fast, and cheap llm serving with pagedattention.
* Lan et al. (2019) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representations. _arXiv preprint arXiv:1909.11942_ (2019).
* Lan et al. (2020) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. In _ICLR_.
* Lee et al. (2019) Jaejun Lee, Raphael Tang, and Jimmy Lin. 2019. What would elsa do? freezing layers during transformer fine-tuning. _arXiv preprint arXiv:1911.03090_ (2019).
* Lee et al. (2020) Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. _Bioinformatics_ 36, 4 (2020), 1234-1240.
* Lee et al. (2021) Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2021. Deduplicating training data makes language models better. _arXiv preprint arXiv:2107.06499_ (2021).
* Lepikhin et al. (2020) Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020. Gshard: Scaling giant models with conditional computation and automatic sharding. _arXiv preprint arXiv:2006.16668_ (2020).
* Leskovec et al. (2014) J Leskovec, A Rajaraman, and JD Ullman. 2014. Mining of Massive Datasets, Cambridge University Press, Cambridge.

* Lewis (1995) David D Lewis. 1995. A sequential algorithm for training text classifiers: Corrigendum and additional data. In _Acm Sigir Forum_, Vol. 29. ACM New York, NY, USA, 13-19.
* Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In _ACL_. 7871-7880.
* Li et al. (2021) Conglong Li, Minjia Zhang, and Yuxiong He. 2021. Curriculum learning: A regularization method for efficient and stable billion-scale gpt model pre-training. (2021).
* Li et al. (2022) Conglong Li, Minjia Zhang, and Yuxiong He. 2022. The stability-efficiency dilemma: Investigating sequence length warmup for training GPT models. _NeurIPS_ 35 (2022), 26736-26750.
* Li et al. (2023) Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. 2023. Symbolic Chain-of-Thought Distillation: Small Models Can Also' Think' Step-by-Step. _arXiv preprint arXiv:2306.14050_ (2023).
* Li et al. (2014) Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. 2014. Scaling distributed machine learning with the parameter server. In _11th USENIX Symposium on operating systems design and implementation (OSDI 14)_. 583-598.
* Li et al. (2021) Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You. 2021. Sequence parallelism: Long sequence training from system perspective. _arXiv preprint arXiv:2105.13120_ (2021).
* Li et al. (2023) Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli. 2023. Functional Interpolation for Relative Positions Improves Long Context Transformers. _arXiv preprint arXiv:2310.04418_ (2023).
* Li et al. (2020) Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. 2020. PyTorch distributed experiences on accelerating data parallel training. _Proceedings of the VLDB Endowment_ 13, 12 (2020), 3005-3018.
* Li et al. (2023) Xiuyu Li, Long Lian, Yijiang Liu, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. 2023. Q-diffusion: Quantizing diffusion models. _arXiv preprint arXiv:2302.04304_ (2023).
* Li and Liang (2021) Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. _arXiv preprint arXiv:2101.00190_ (2021).
* Li et al. (2023) Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks Are All You Need II: phi-1.5 technical report. _arXiv preprint arXiv:2309.05463_ (2023).
* Li et al. (2021) Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, Dawn Song, and Ion Stoica. [n. d.]. TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models. ([n. d.]).
* Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. 2023. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. _arXiv preprint arXiv:2306.00978_ (2023).
* Liu and Abbeel (2023) Hao Liu and Pieter Abbeel. 2023. Blockwise Parallel Transformer for Long Context Large Models. _arXiv preprint arXiv:2305.19370_ (2023).
* Liu et al. (2023) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_ (2023).
* Liu et al. (2023) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _Comput. Surveys_ 55, 9 (2023), 1-35.
* Liu et al. (2020) Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Haotang Deng, and Qi Ju. 2020. Fastbert: a self-distilling bert with adaptive inference time. _arXiv preprint arXiv:2004.02178_ (2020).
* Liu et al. (2023) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2023. GPT understands, too. _AI Open_ (2023).
* Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_ (2019).
* Liu et al. (2023) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. 2023. LLM-QAT: Data-Free Quantization Aware Training for Large Language Models. _arXiv preprint arXiv:2305.17888_ (2023).
* Liu et al. (2020) Zhining Liu, Pengfei Wei, Jing Jiang, Wei Cao, Jiang Bian, and Yi Chang. 2020. MESA: boost ensemble imbalanced learning with meta-sampler. _NeurIPS_ 33 (2020), 14463-14474.
* Long (2023) Jieyi Long. 2023. Large Language Model Guided Tree-of-Thought. _arXiv preprint arXiv:2305.08291_ (2023).
* Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_ (2017).
* Lv et al. (2023) Xiuqing Lv, Peng Zhang, Sunzhu Li, Guobing Gan, and Yueheng Sun. 2023. LightFormer: Light-weight Transformer Using SVD-based Weight Transfer and Parameter Sharing. In _Findings of the Association for Computational Linguistics: ACL 2023_. 10323-10335.
* Ma et al. (2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. LLM-Pruner: On the Structural Pruning of Large Language Models. _arXiv preprint arXiv:2305.11627_ (2023).
* Ma et al. (2019) Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, and Dawei Song. 2019. A tensorized transformer for language modeling. _Advances in neural information processing systems_ 32 (2019).
* Ma et al. (2023) Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. 2023. Mega: moving average equipped gated attention. _ICLR_ (2023).
* Maekawa et al. (2022) Seiji Maekawa, Dan Zhang, Hannah Kim, Sajjadur Rahman, and Eatevam Hruschka. 2022. Low-resource interactive active labeling for fine-tuning language models. In _Findings of EMNLP_. 3230-3242.
* Margatina et al. (2021) Katerina Margatina, Giorgos Vernikos, Loic Barrault, and Nikolaos Aletras. 2021. Active learning by acquiring contrastive examples. _arXiv preprint arXiv:2109.03764_ (2021).

* Martins et al. (2022) Pedro Henrique Martins, Zita Marinho, and Andre FT Martins. 2022. \(inf\)\(f\)-former: Infinite Memory Transformer. _ACL_ (2022).
* Mehta et al. (2023) Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Belham Neyshabur. 2023. Long range language modeling via gated state spaces. _ICLR_ (2023).
* Micikevicius et al. (2017) Paulus Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Olekski Kuchaiev, Ganesh Venkatesh, et al. 2017. Mixed precision training. _arXiv preprint arXiv:1710.03740_ (2017).
* Mikolov et al. (2010) Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model.. In _Interspeech_, Vol. 2. Makhaturi, 1945-1048.
* Mikolov et al. (2011) Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. 2011. Extensions of recurrent neural network language model. In _2011 IEEE international conference on acoustics, speech and signal processing (ICASSP)_. IEEE, 5528-5531.
* Mishra and Sachdeva (2020) Swaroop Mishra and Bhavedeep Singh Sachdeva. 2020. Do we need to create big datasets to learn a task?. In _SustainNLP Workshop_. 169-173.
* Muemighoff et al. (2023) Niklas Muemighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. 2023. Scaling Data-Constrained Language Models. _arXiv preprint arXiv:2305.16264_ (2023).
* Mustafa et al. (2022) Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. 2022. Multimodal contrastive learning with limo: the language-image mixture of experts. _NeurIPS_ 35 (2022), 9564-9576.
* Nair et al. (2023) Lakshmi Nair, Mikhail Bernadskiy, Arulesivan Madhavan, Craig Chan, Ayon Basumallik, and Darius Bunandar. 2023. INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers. _arXiv preprint arXiv:2307.03712_ (2023).
* Narayanan et al. (2019) Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. 2019. PipeDream: Generalized pipeline parallelism for DNN training. In _Proceedings of the 27th ACM Symposium on Operating Systems Principles_. 1-15.
* Narayanan et al. (2021) Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. 2021. Memory-efficient pipeline-parallel dnn training. In _International Conference on Machine Learning_. PMLR, 7937-7947.
* Narayanan et al. (2021) Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. 2021. Efficient large-scale language model training on gpu clusters using megatron-lm. In _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_. 1-15.
* Noach and Goldberg (2020) Matan Ben Noach and Yoav Goldberg. 2020. Compressing pre-trained language models by matrix decomposition. In _Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing_. 884-889.
* Oh et al. (2022) Sangyun Oh, Hyeonuk Sim, Jounghyun Kim, and Jongeun Lee. 2022. Non-Uniform Step Size Quantization for Accurate Post-Training Quantization. In _European Conference on Computer Vision_. Springer, 658-673.
* OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. _ArXiv_ abs/2303.08774 (2023).
* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. _NeurIPS_ 35 (2022), 27730-27744.
* Patel et al. (2021) Arkil Patel, Satwik Bhattacharya, and Navin Goyal. 2021. Are NLP Models really able to Solve Simple Math Word Problems?. In _ACL_. ACL, 2080-2094.
* Peng et al. (2023) Bo Peng, Eric Alcaie, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. 2023. RWRV: Reinventing RNNs for the Transformer Era. _arXiv preprint arXiv:2305.13048_ (2023).
* Peng et al. (2023) Bo Peng, Ben Burns, Ziti Chen, Srinivasan Parthasarathy, and Xia Ning. 2023. Towards Efficient and Effective Adaptation of Large Language Models for Sequential Recommendation. _arXiv preprint arXiv:2310.01612_ (2023).
* Peng et al. (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarm: Efficient context window extension of large language models. _arXiv preprint arXiv:2309.00071_ (2023).
* Peters et al. (2018) Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In _NAACL-HLT_. 2227-2237.
* Pfeiffer et al. (2020) Jonas Pfeiffer, Andreas Ruckle, Clifton Poth, Aishwarya Kamath, Ivan Vulic, Sebastian Ruder, Kyunghyun Cho, and Iryua Gurevych. 2020. Adapterhub: A framework for adapting transformers. _arXiv preprint arXiv:2007.07779_ (2020).
* Pfeiffer et al. (2020) Jonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Sebastian Ruder. 2020. Mad-x: An adapter-based framework for multi-task cross-lingual transfer. _arXiv preprint arXiv:2005.00052_ (2020).
* Poli et al. (2023) Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. 2023. Hyena hierarchy: Towards larger convolutional language models. _arXiv preprint arXiv:2302.10866_ (2023).
* Press et al. (2020) Ofir Press, Noah A Smith, and Mike Lewis. 2020. Shortformer: Better language modeling using shorter inputs. _arXiv preprint arXiv:2012.15832_ (2020).
* Press et al. (2023) Ofir Press, Noah A Smith, and Mike Lewis. 2023. Train short, test long: Attention with linear biases enables input length extrapolation. _ICLR_ (2023).
* Prusa et al. (2015) Joseph Prusa, Taghi M Khoshgoftaar, David J Dittman, and Amri Napolitano. 2015. Using random undersampling to alleviate class imbalance on tweet sentiment data. In _2015 IEEE international conference on information reuse and integration_. IEEE, 197-202.
* Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. (2018).
* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. _OpenAI blog_ 1, 8 (2019), 9.
* Rae et al. (2021) Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis & insights from training gopher. _arXiv preprint arXiv:2112.11446_ (2021).

* Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_ 21, 1 (2020), 5485-5551.
* Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. _Journal of Machine Learning Research_ 21, 140 (2020), 1-67.
* Rajbhandari et al. [2022] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. 2022. Deepspeed-more: Advancing mixture-of-experts inference and training to power next-generation ai scale. In _ICML_. 18332-18346.
* Rajbhandari et al. [2020] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In _SC_. 1-16.
* Rasley et al. [2020] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In _SIGKDD_. 3505-3506.
* Reid et al. [2021] Michel Reid, Edison Marrese-Taylor, and Yutaka Matsuo. 2021. Subformer: Exploring weight sharing for parameter efficiency in generative transformers. _arXiv preprint arXiv:2101.00234_ (2021).
* Ren et al. [2021] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. 2021. ZeRO-Offload: Democratizing Billion-Scale Model Training. _arXiv preprint arXiv:2101.06840_ (2021).
* Robinson and Wingate [2022] Joshua Robinson and David Wingate. 2022. Leveraging Large Language Models for Multiple Choice Question Answering. In _ICLR_.
* Romero et al. [2014] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. 2014. Fitnets: Hints for thin deep nets. _arXiv preprint arXiv:1412.6550_ (2014).
* Rosenfeld et al. [2019] Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. 2019. A constructive prediction of the generalization error across scales. _arXiv preprint arXiv:1909.12673_ (2019).
* Rosenfeld [2000] Ronald Rosenfeld. 2000. Two decades of statistical language modeling: Where do we go from here? _IEEE_ 88, 8 (2000), 1270-1278.
* Ruckle et al. [2021] Andreas Ruckle, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych. 2021. AdapterDrop: On the Efficiency of Adapters in Transformers. In _EMNLP_. 7930-7946.
* Ruoss et al. [2023] Anian Ruoss, Gregoire Deletang, Tim Genewein, Jordi Grau-Moya, Robert Csordas, Mehdi Bennani, Shane Legg, and Joel Veness. 2023. Randomized Positional Encodings Boost Length Generalization of Transformers. _arXiv preprint arXiv:2305.16843_ (2023).
* Sahu et al. [2023] Gaurav Sahu, Olga Vechtomova, Dzmitry Bahdanau, and Issam H Laradji. 2023. Promptmix: A class boundary augmentation method for large language model distillation. _arXiv preprint arXiv:2310.14192_ (2023).
* Sainath et al. [2013] Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. 2013. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In _2013 IEEE international conference on acoustics, speech and signal processing_. IEEE, 6655-6659.
* Salton and Buckley [1990] Gerard Salton and Chris Buckley. 1990. Improving retrieval performance by relevance feedback. _Journal of the American society for information science_ 41, 4 (1990), 288-297.
* Sanh et al. [2019] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. _arXiv preprint arXiv:1910.01108_ (2019).
* Sredker [2010] AMissing SERET SAUCE. [n. d.]. OUTLIER WEIGHED LAYERWISE SPARSITY (OWL): AMissing SERET SAUCE FOR PRUNING LIMS TO HIGH SPARSITY. ([n. d.]).
* [26] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_ (2022).
* Sener and Savarese [2017] Ozan Sener and Silvio Savarese. 2017. Active learning for convolutional neural networks: A core-set approach. _arXiv preprint arXiv:1708.00489_ (2017).
* Settles [2017] Burr Settles. 2009. Active learning literature survey. University of Wisconsin-Madison Department of Computer Sciences.
* Settles [2011] Burr Settles. 2011. From theories to queries: Active learning in practice. In _Active learning and experimental design workshop in conjunction with AISTATS 2010_. JMLR Workshop and Conference Proceedings, 1-18.
* Settles [2012] Burr Settles. 2012. Active Learning. In _Synthesis Lectures on Artificial Intelligence and Machine Learning_. 1-114.
* Seung et al. [1992] Hyunjune Sebastian Seung, Haim Sompolinsky, and Naftali Tishby. 1992. Statistical mechanics of learning from examples. _Physical review A_ 45, 8 (1992), 6056.
* Shanahan [2022] Murray Shanahan. 2022. Talking about large language models. _arXiv preprint arXiv:2212.03551_ (2022).
* Shao et al. [2023] Hang Shao, Bei Liu, and Yanmin Qian. 2023. One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models. _arXiv preprint arXiv:2310.09499_ (2023).
* Shazeer et al. [2018] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koaantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. 2018. Mesh-tensorflow: Deep learning for supercomputers. _Advances in neural information processing systems_ 31 (2018).
* Shazeer et al. [2017] Noam Shazeer, Aazalia Mirhoseini, Krzysztof Maziraz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. _ICLR_ (2017).
* Shazeer and Stern [2018] Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In _International Conference on Machine Learning_. PMLR, 4596-4604.
* Shen et al. [2023] Li Shen, Yan Sun, Zhiyuan Yu, Liang Ding, Xinmei Tian, and Dacheng Tao. 2023. On Efficient Training of Large-Scale Deep Learning Models: A Literature Review. _arXiv preprint arXiv:2304.03589_ (2023).

* Shen et al. (2020) Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Q-bert: Hessian based ultra low precision quantization of bert. In _AAAI_, Vol. 34. 8815-8821.
* Shen et al. (2023) Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, et al. 2023. Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts. _arXiv preprint arXiv:2305.14705_ (2023).
* Shin et al. (2020) Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. _arXiv preprint arXiv:2011.15980_ (2020).
* Shoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. _arXiv preprint arXiv:1909.08053_ (2019).
* Siddhant and Lipton (2018) Aditya Siddhant and Zachary C Lipton. 2018. Deep bayesian active learning for natural language processing: Results of a large-scale empirical study. _arXiv preprint arXiv:1808.05697_ (2018).
* Siddiqui et al. (2022) Shoaib Ahmed Siddiqui, Nitarshan Rajkumar, Tegan Maharaj, David Krueger, and Sara Hooker. 2022. Metadata archaeology: Unearthing data subsets by leveraging training dynamics. _arXiv preprint arXiv:2209.10015_ (2022).
* Sorscher et al. (2022) Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. 2022. Beyond neural scaling laws: beating power law scaling via data pruning. _NeurIPS_ 35 (2022), 19523-19536.
* Su et al. (2022) Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayu Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. 2022. Selective annotation makes language models better few-shot learners. _arXiv preprint arXiv:2209.01975_ (2022).
* Su et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding. _arXiv preprint arXiv:2104.09864_ (2021).
* Su et al. (2023) Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. 2023. Pandagpt: One model to instruction-follow them all. _arXiv preprint arXiv:2305.16355_ (2023).
* Sujit et al. (2022) Shivakanth Sujit, Somjit Nath, Pedro HM Braga, and Samira Ebrahimi Kahou. 2022. Prioritizing samples in reinforcement learning with reducible loss. _arXiv preprint arXiv:2208.10483_ (2022).
* Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. 2023. A Simple and Effective Pruning Approach for Large Language Models. _arXiv preprint arXiv:2306.11695_ (2023).
* Sun et al. (2023) Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. 2023. Retentive network: A successor to transformer for large language models. _arXiv preprint arXiv:2307.08621_ (2023).
* Sun et al. (2022) Zhiqing Sun, Yiming Yang, and Shinjae Yoo. 2022. Sparse attention with learning to hash. In _ICLR_.
* Tahaei et al. (2021) Marzieh S Tahaei, Ella Charlaix, Vahid Partovi Nia, Ali Ghodsi, and Mehdi Rezagholizadeh. 2021. Kroneckerbert: Learning kronecker decomposition for pre-trained language models via knowledge distillation. _arXiv preprint arXiv:2109.06243_ (2021).
* Talmor et al. (2019) Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. In _ACL_. ACL, Minneapolis, Minnesota, 4149-4158.
* Tang et al. (2002) Min Tang, Xiaoguang Luo, and Salim Roukos. 2002. Active learning for statistical natural language parsing. In _ACL_. 120-127.
* Taori et al. (2023) Rohan Taori, Ishan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following LLAA model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).
* Tay et al. (2022) Yi Tay, Mostafa Dehghani, Samira Ahnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. 2022. Scaling laws vs model architectures: How does inductive bias influence scaling? _arXiv preprint arXiv:2207.10551_ (2022).
* Tay et al. (2022) Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2022. Efficient Transformers: A Survey. _ACM Comput. Surv._ 55, 6, Article 109 (dec 2022), 28 pages.
* Tay et al. (2021) Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Ahnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. 2021. Scale Efficiently: Insights from Pretraining and Finetuning Transformers. In _International Conference on Learning Representations_.
* Thoppilan et al. (2022) Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. _arXiv preprint arXiv:2201.08239_ (2022).
* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izcard, Xavier Martinet, Marie-Anne Lachaux, Timothete Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Achar, et al. 2023. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_ (2023).
* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_ (2023).
* Treviso et al. (2021) Marcos Treviso, Antonio Gois, Patrick Fernandes, Eric Fonseca, and Andre FT Martins. 2021. Predicting attention sparsity in transformers. _arXiv preprint arXiv:2109.12188_ (2021).
* Treviso et al. (2023) Marcos Treviso, Ji-Uing Lee, Tianchu Ji, Betty van Aken, Qingqing Cao, Manuel R Ciosici, Michael Hassid, Kenneth Heafield, Sara Hooker, Colin Raffel, et al. 2023. Efficient methods for natural language processing: A survey. _TACL_ 11 (2023), 826-860.
* Van Wynsberghe (2021) Aimee Van Wynsberghe. 2021. Sustainable AI: AI for sustainability and the sustainability of AI. _AI and Ethics_ 1, 3 (2021), 213-218.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. _NeurIPS_ 30 (2017).

* Vinuesa et al. (2020) Ricardo Vinuesa, Hossein Azizpour, Iolanda Leite, Madeline Balaam, Virginia Dignum, Sami Domisch, Anna Fellander, Simone Daniela Langhans, Max Tegmark, and Francesco Fuso Nerini. 2020. The role of artificial intelligence in achieving the Sustainable Development Goals. _Nature communications_ 11, 1 (2020), 1-10.
* Wan et al. (2020) Yu Wan, Baosong Yang, Derek F Wong, Yikai Zhou, Lidia S Chao, Haibo Zhang, and Boxing Chen. 2020. Self-paced learning for neural machine translation. _arXiv preprint arXiv:2010.04505_ (2020).
* Wang et al. (2022) Boxing Wang, Qifan Xu, Zhengda Bian, and Yang You. 2022. Tesseract: Parallelize the tensor parallelism efficiently. In _ICPP_. 1-11.
* Wang et al. (2022) Jing Wang, Jie Shen, Xiaofei Ma, and Andrew Arnold. 2022. Uncertainty-based active learning for reading comprehension. _TMLR_ (2022).
* Wang et al. (2022) Jianing Wang, Chengyu Wang, Fuli Luo, Chuanqi Tan, Minghui Qiu, Fei Yang, Qiuhui Shi, Songfang Huang, and Ming Gao. 2022. Towards unified prompt tuning for few-shot text classification. _arXiv preprint arXiv:2205.05313_ (2022).
* Wang et al. (2023) Minjie Wang, Chien-chin Huang, and Jinyang Li. [n. d.]. Supporting Very Large Models using Automatic Dataflow Graph Partitioning. ([n. d.]).
* Wang et al. (2023) Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. 2023. SCOTT: Self-consistent chain-of-thought distillation. _arXiv preprint arXiv:2305.01879_ (2023).
* Wang et al. (2023) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency Improves Chain of Thought Reasoning in Language Models. In _ICLR_.
* Wang et al. (2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. _arXiv preprint arXiv:2212.10560_ (2022).
* Wang et al. (2022) Yile Wang, Yue Zhang, Peng Li, and Yang Liu. 2022. Language Model Pre-training with Linguistically Motivated Curriculum Learning. (2022).
* Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent Abilities of Large Language Models. _TMLR_ (2022). Survey Certification
* Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. _NeurIPS_ 35 (2022), 24824-24837.
* Wennberg and Henter (2021) Ulme Wennberg and Gustav Eje Henter. 2021. The case for translation-invariant self-attention in transformer-based language models. _arXiv preprint arXiv:2106.01950_ (2021).
* Wenzek et al. (2020) Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. 2020. CCNet: Extracting High Quality Monolingual Datasets from Web Crawd Data. In _Proceedings of The 12th Language Resources and Evaluation Conference_. 4003-4012.
* White et al. (2023) Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. 2023. A prompt pattern catalog to enhance prompt engineering with chatspt. _arXiv preprint arXiv:2302.11382_ (2023).
* Wu et al. (2022) Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. 2022. Sustainable ai: Environmental implications, challenges and opportunities. _Proceedings of Machine Learning and Systems_ 4 (2022), 795-813.
* Wu et al. (2023) Minghao Wu, Abdul Wahaeed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. 2023. Lamini-lm: A diverse herd of distilled models from large-scale instructions. _arXiv preprint arXiv:2304.14402_ (2023).
* Wu et al. (2023) Shijie Wu, Qcan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023. Bloomberggpt: A large language model for finance. _arXiv preprint arXiv:2303.17564_ (2023).
* Xia et al. (2023) Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, and Shuaiwen Leon Song. 2023. Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity. _arXiv preprint arXiv:2309.10285_ (2023).
* Xia et al. (2023) Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 2023. Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning. _arXiv preprint arXiv:2310.06694_ (2023).
* Xiao et al. (2023) Guangcuan Xiao, Ji Lin, Mickal Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. Smoothbquant: Accurate and efficient post-training quantization for large language models. In _ICML_. PMLR, 38087-38099.
* Xie et al. (2022) Shuo Xie, Jiahao Qiu, Ankita Pasad, Li Du, Qing Qu, and Hongyuan Mei. 2022. Hidden state variability of pretrained language models can guide computation reduction for transfer learning. In _EMNLP_.
* Xie et al. (2023) Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. 2023. Data selection for language models via importance resampling. _arXiv preprint arXiv:2302.03169_ (2023).
* Xie et al. (2023) Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. 2023. Decomposition enhances reasoning via self-evaluation guided decoding. _arXiv preprint arXiv:2305.00633_ (2023).
* Xing et al. (2015) Eric P Xing, Qirong Ho, Wei Dai, Jin-Kyu Kim, Jinliang Wei, Seunghak Lee, Xun Zheng, Pengtao Xie, Abhimanu Kumar, and Yacliang Yu. 2015. Petuum: A new platform for distributed machine learning on big data. In _Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_. 1335-1341.
* Xu and McAuley (2023) Canwen Xu and Julian McAuley. 2023. A survey on model compression and acceleration for pretrained language models. In _AAAI_, Vol. 37. 10566-10575.
* Xu and You (2023) Qifan Xu and Yang You. 2023. An efficient 2d method for training super-large deep learning models. In _2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)_. IEEE, 222-232.
* Xu et al. (2021) Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et al. 2021. GSPMD: general and scalable parallelization for ML computation graphs. _arXiv preprint arXiv:2105.04663_ (2021).

* Xu et al. (2023) Ziyun Xu, Chengyu Wang, Minghui Qiu, Fuli Luo, Runxin Xu, Songfang Huang, and Jun Huang. 2023. Making pre-trained language models end-to-end few-shot learners with contrastive prompt tuning. In _Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining_. 438-446.
* Xu et al. (2003) Zhao Xu, Kai Yu, Volker Tresp, Xiaowei Xu, and Jizhi Wang. 2003. Representative sampling for text classification using support vector machines. In _ECIR_. 393-407.
* Yang et al. (2021) Bowen Yang, Jian Zhang, Jonathan Li, Christopher Re, Christopher Aberger, and Christopher De Sa. 2021. PiPemare: Asynchronous pipeline parallel dnn training. _Proceedings of Machine Learning and Systems_ 3 (2021), 269-296.
* Yang et al. (2020) Haichuan Yang, Shupeng Gui, Yuhao Zhu, and Ji Liu. 2020. Automatic neural network compression by sparsity-quantization joint learning: A constrained optimization-based approach. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 2178-2188.
* Yang et al. (2023) Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. Harnessing the power of llms in practice: A survey on chatgpt and beyond. _arXiv preprint arXiv:2304.13712_ (2023).
* Yang et al. (2023) Xianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, and Wei Cheng. 2023. Exploring the limits of chatgpt for query or aspect-based text summarization. _arXiv preprint arXiv:2302.08081_ (2023).
* Yao et al. (2023) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. _arXiv preprint arXiv:2305.10601_ (2023).
* Yao et al. (2022) Xingcheng Yao, Yanan Zheng, Xiaocong Yang, and Zhilin Yang. 2022. Nlp from scratch without large-scale pretraining: A simple and efficient framework. In _International Conference on Machine Learning_. PMLR, 25438-25451.
* Yao et al. (2022) Zhewei Yao, Reza Yandani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. 2022. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. _NeurIPS_ 35 (2022), 27168-27183.
* Yi et al. (2023) Rongie Yi, Liwei Guo, Shiyun Wei, An Zhou, Shangguang Wang, and Mengwei Xu. 2023. EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models. _arXiv preprint arXiv:2308.14352_ (2023).
* Yim et al. (2017) Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. 2017. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In _CVPR_. 4133-4141.
* Yu et al. (2022) Yue Yu, Lingkai Kong, Jieyu Zhang, Rongzhi Zhang, and Chao Zhang. 2022. AcTune: Uncertainty-based active self-training for active fine-tuning of pretrained language models. In _NAACL_. 1422-1436.
* Yuan et al. (2020) Michelle Yuan, Hsuan-Tien Lin, and Jordan Boyd-Graber. 2020. Cold-start active learning through self-supervised language modeling. _arXiv preprint arXiv:2010.09535_ (2020).
* Yuan et al. (2023) Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. 2023. RPTQ: Reorder-based Post-training Quantization for Large Language Models. _arXiv preprint arXiv:2304.01089_ (2023).
* Zagoruyko and Komodakis (2016) Sergey Zagoruyko and Nikos Komodakis. 2016. Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer. In _ICLR_.
* Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big bird: Transformers for longer sequences. _NeurIPS_ (2020), 17283-17297.
* Zaken et al. (2022) Elad Ben Zaheer, Yoay Goldberg, and Shuali Ravfogel. 2022. BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. In _ACL_. 1-9.
* Zellers et al. (2018) Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. _arXiv preprint arXiv:1808.05326_ (2018).
* Zha et al. (2023) Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong, and Xia Hu. 2023. Data-centric artificial intelligence: A survey. _arXiv preprint arXiv:2303.10158_ (2023).
* Zhai et al. (2008) ChengXiang Zhai et al. 2008. Statistical language models for information retrieval a critical review. _Foundations and Trends(r) in Information Retrieval_ 2, 3 (2008), 137-213.
* Zhai et al. (2021) Shuangfei Zhai, Walter Talbotti, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. 2021. An attention free transformer. _arXiv preprint arXiv:2105.14103_ (2021).
* Zhang et al. (2023) Chen Zhang, Yang Yang, Jiahao Liu, Jingang Wang, Yunsen Xian, Benyou Wang, and Dawei Song. 2023. Lifting the Curse of Capacity Gap in Distilling Language Models. _arXiv preprint arXiv:2305.12129_ (2023).
* Zhang et al. (2019) Jiong Zhang, Hsiang-Fu Yu, and Inderjit S Dhillon. 2019. Autoassist: A framework to accelerate training of deep neural networks. _Advances in Neural Information Processing Systems_ 32 (2019).
* Zhang et al. (2023) Mingyang Zhang, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan Zhuang, et al. 2023. Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning. _arXiv preprint arXiv:2305.18403_ (2023).
* Zhang et al. (2023) Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. 2023. Adaptive budget allocation for parameter-efficient fine-tuning. _arXiv preprint arXiv:2308.10512_ (2023).
* Zhang et al. (2023) Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. 2023. Instruction Tuning for Large Language Models: A Survey. _arXiv preprint arXiv:2308.10792_ (2023).
* Zhang et al. (2022) Shujian Zhang, Chengyue Gong, Xingchao Liu, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. 2022. Allsh: Active learning guided by local sensitivity and hardness. _arXiv preprint arXiv:2205.04980_ (2022).

* [325] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_ (2022).
* [326] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. 2020. Ternarybert: Distillation-aware ultra-low bit bert. _arXiv preprint arXiv:2009.12812_ (2020).
* [327] Yuxin Zhang, Lirui Zhao, Minghao Lin, Yunyun Sun, Yiwu Yao, Xingjia Han, Jared Tanner, Shiwei Liu, and Rongrong Ji. 2023. Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs. _arXiv preprint arXiv:2310.08915_ (2023).
* [328] Mingjun Zhao, Haijiang Wu, Di Niu, and Xiaoli Wang. 2020. Reinforced curriculum learning on pre-trained neural machine translation models. In _Proceedings of the AAAI Conference on Artificial Intelligence_, Vol. 34. 9652-9659.
* [329] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. _arXiv preprint arXiv:2303.18223_ (2023).
* [330] Fedor Zhdanov. 2019. Diverse mini-batch active learning. _arXiv preprint arXiv:1901.05954_ (2019).
* [331] Linamin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing, et al. 2022. Alpa: Automating inter-and {Intra-Operator} parallelism for distributed deep learning. In _16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)_, 559-578.
* [332] Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and Hongsheng Li. 2021. Learning n: m fine-grained structured sparse neural networks from scratch. _arXiv preprint arXiv:2102.04010_ (2021).
* [333] Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, et al. 2023. A comprehensive survey on pretrained foundation models: A history from bert to chatpgt. _arXiv preprint arXiv:2302.09419_ (2023).
* [334] Jinxin Zhou, Xiao Li, Tianyu Ding, Chong You, Qing Qu, and Zhihui Zhu. 2022. On the Optimization Landscape of Neural Collapse under MSE Loss: Global Optimality with Unconstrained Features. In _Proceedings of the 39th International Conference on Machine Learning_. 27179-27202.
* [335] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yubeng Zou. 2016. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. _arXiv preprint arXiv:1606.06160_ (2016).
* [336] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silvin Pitis, Harris Chan, and Jimmy Ba. 2023. Large Language Models are Human-Level Prompt Engineers. In _ICLR_.
* [337] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-t: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_ (2023).
* [338] Qingqing Zhu, Xinying Chen, Pengfei Wu, JunFei Liu, and Dongyan Zhao. 2021. Combining curriculum learning and knowledge distillation for dialogue generation. In _Findings of the Association for Computational Linguistics: EMNLP 2021_. 1284-1295.
* [339] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2023. A Survey on Model Compression for Large Language Models. _arXiv preprint arXiv:2308.07633_ (2023).
* [340] Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. 2021. A geometric analysis of neural collapse with unconstrained features. _Advances in Neural Information Processing Systems_ (2021), 29820-29834.
* [341] Zeyuan Allen Zhu and Yuanzhi Li. 2023. Physics of Language Models: Part 3.1, Knowledge Storage and Extraction. arXiv:2309.14316 [cs.CL]
* [342] Bohan Zhuang, Jing Liu, Zizheng Pan, Haoyu He, Yuetian Weng, and Chunhua Shen. 2023. A survey on efficient training of transformers. _arXiv preprint arXiv:2302.01107_ (2023).
* [343] Yimeng Zhuang, Jing Zhang, and Mei Tu. 2022. Long-range Sequence Modeling with Predictable Sparse Attention. In _ACL_. 234-243.
* [344] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alee Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. _arXiv preprint arXiv:1908.08593_ (2019).
* [345] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. 2022. Designing effective sparse expert models. _IPDPSW_ (2022).