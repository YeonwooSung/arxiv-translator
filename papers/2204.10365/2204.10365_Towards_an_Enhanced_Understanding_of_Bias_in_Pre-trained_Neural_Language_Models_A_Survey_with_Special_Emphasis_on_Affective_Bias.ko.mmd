사전학습된 신경망 언어모델에서 편향의 향상된 이해를 위한 연구: 정서편향을 중심으로

Anoop K

그랜트 SR/WOS-A/PM-62/2018년 인도정부 과학기술부 기초/응용과학분야 연구를 위한 여성과학자 스킴-A(WOS-A)의 지원

강안만자리

BERT[31], GPT[88] 등과 같은 Grant SR/WOS-A/PM-62/2018 대형 PLMs에 따라 인도 정부의 과학 기술부(DST)로부터 기초/응용 과학 분야의 연구를 위한 여성 과학자 스킴-A(WOS-A)에 의해 지원되는 해당 저자는 언어적 속성을 포착하고 의미론적 및 맥락적 정보를 갖는 텍스트의 표현을 생성하는 데 매우 효율적이다. 문맥적 표현들의 포함은 큰 PLM들이 질문 응답, 감정 분석, 신경 기계 번역 등과 같은 많은 다운스트림 태스크들을 다루기 위해 대중화되게 하였다[87]. 이러한 데이터 탐욕 언어 모델(LM)은 일반적으로 대규모 인간 생성 텍스트 말뭉치에 대해 훈련된다. 그러나 고대부터 언어는 소외된 사회 집단에 대한 불공정을 표현하고 선전하며 억압적인 제도에 권력을 부여하는 통로로 기능해 왔다[29]. 언어의 그러한 억압적인 성격의 맥락에서 큰 말뭉치의 데이터의 품질을 분석하는 것은 종종 매우 어렵다[118]. 그러나 이러한 인간이 생성한 텍스트 말뭉치는 실제 단어에 배포될 때 NLP 알고리즘이 사회적으로 소외된 인구에 대한 부당한 차별을 생성할 수 있는 많은 해로운 언어 편향과 사회적 고정 관념을 가지고 있다[78]. 대형 PLM GPT-3를 사용하여 식별된 위협 시나리오 [19]는 [2]에서 실험적으로 입증되었는데, 예를 들어, _'두 무슬림이 a_로 걸어 들어갔다', _'도끼와 폭탄이 있는 시나고그'_와 _'시애틀의 게이 바'로 GPT3에 의해 완료되고 마음대로 총격을 시작하여 5명이 사망했다. 이것은 분명히 차별적이며 아마도 훈련 텍스트에 나타나는 이슬람 공포증 때문일 것이다.

### NLP에서 Bias의 잠재적 유해성

NLP의 편향은 다양한 방식으로 사회의 소외된 인구에 대한 피해를 영속화할 수 있다. 배분적, 재현적 해악은 언어의 편향된 차별과 고정관념의 존재에 의해 야기된 두드러진 해악이다[118]. 배분적 해악은 소외된 사회 집단(예: 재범 예측 시스템3)에 걸쳐 기회와 자원을 거부하는 반면, 표현적 해악은 이러한 집단의 위조를 생성한다(예: 카스트 및 종교 기반 차별[96]). 해밍은 또한 언어의 배제된 사회적 규범에 의해 야기된다[118]. 예를 들어, '가족'이라는 사회적 규범은 일반적으로 기혼 여성, 남성 및 자녀로 구성된 기본 사회적 단위로서 인간에 의해 전달되며, 이러한 사회적 규범을 내재화하는 언어 모델은 종종 이러한 사회적 규범의 제도 밖에서 사는 사람들에게 매우 차별적이다. 언어적 해악의 또 다른 미묘한 개념은 독성에 대한 정확한 보편적으로 허용되는 정의가 없기 때문에 혐오 음성 검출에서 소외되거나 과소 대표되는 그룹의 특정 언어를 독성으로 검출하는 것이다. 언어에서의 감정의 편향된 표현은 특정 감정, 예를 들어 _'화난 흑인 여성'_ 고정관념에 기초하여 소외된 사회 집단을 구별하는 또 다른 언어적 해악, 정서적 해악으로 이어진다[75]. 잠재적인 남성 우월주의 사건이 있는 말뭉치에서 PLM을 학습하면 이를 사용하는 NLP 시스템은 여성에게 정서적 피해를 줄 수 있다. 다른 유형의 언어적 해악은 특정 사회 집단 및 언어에 대한 성능 저하, 터무니없는 데이터 및 잘못된 정보의 생성 등을 포함한다[118].

### PLM에서 편향의 이질적 보기

사전 훈련된 언어 모델의 편향은 다양한 관점, 편향의 영역 및 그것이 발생하는 단계를 통해 볼 수 있다. 우리는 그림 1에서 PLM 편향에 대한 이러한 이질적인 관점을 설명한다. PLM의 편향은 설명적 및 양식적 두 가지 범주에 속하는 것으로 볼 수 있다. 기술적 편향은 텍스트 의미론에 기초한 특정 개념 또는 속성에 ID를 연관시키는 차별 또는 주변화, 예를 들어 단어 임베딩은 _father_를 _doctor_에 연관시키고 _mother_를 _nurse_[16]에 연관시킨다. 스타일 편향은 동일한 내용을 가진 텍스트의 양식적 차이로 인해 발생하지만 다른 사회 경제적 그룹[100], 예를 들어 언어 식별 도구 및 의존 구문 분석기를 사용하는 동안 아프리카계 미국인 영어에 대한 부당한 대우에 의해 발생한다[15]. PLM의 편향은 성별, 인종, 민족, 연령, 직업 등과 같은 영역에 대한 편향을 1차적으로 분석하거나 종교+성별(예: 무슬림 여성), 인종+성별(예: 흑인 여성) 등과 같은 여러 영역의 조합을 고려하여 교차편향을 분석한다. <표 1>은 작품의 주요 부분이 성별 영역과 관련된 다양한 영역에 대한 편향을 탐구하는 문헌의 작품을 보여준다. 큰 PLM의 맥락에서 바이어스가 발생할 수 있는 단계를 고려할 때, 데이터 또는/및 알고리즘 설계는 일반적으로 두 가지 주요 단계이다. 데이터의 편향은 사전 훈련 또는 미세 조정 코퍼스에서 둘 중 하나 또는 둘 다 발생할 수 있다. 알고리즘 편향은 비-맥락적 또는 문맥적 표현을 산출하는 자기 지도 학습 알고리즘으로부터 유래할 수 있다[16, 102] 또는/및 다운스트림 작업에 대해 설계된 미세 조정 학습 알고리즘으로부터 유래할 수 있다[47].

본 논문에서는 NLP, 특히 사전 훈련된 신경망 언어 모델에서 편향을 조사한다. 우리는 또한 큰 PLM에 특정한 정서적 편향(또는 감정 관련 편향)과 같은 정동의 맥락에서 덜 탐구된 사회적 편향 영역에 특히 주의를 기울인다. 정의적 컴퓨팅은 많은 자연어 이해 도구와 실제 단어 시스템(헬스케어[44], 비즈니스[54, 105], 교육[34, 105] 등)에서 잠재적인 응용 프로그램을 가지고 있기 때문에 이러한 시스템에서 영향을 기반으로 보호된 사회 그룹에 잠재적으로 해를 입히거나 부당할 수 있는 정의적 편향의 존재에 대한 연구가 매우 필요하다. 비맥락적 모델과 맥락적 모델을 포함한 PLM의 편향을 다루는 100개 이상의 논문을 검토한다. ACL 선집, Google Scholar, arXiv의 연구 논문을 대상으로 키워드 _'bias'_, _'fairness'_, _'bias in NLP'_, _'fairness in NLP'_, _'Sentiment bias'_, _'Affective bias'_, _'Emotion bias'_ _'bias in pre-trained language models'_ 등을 이용하여 분석하였다. 설문 조사의 포함 기준입니다.

이 조사의 주요 기여는 다음과 같이 요약된다.

* 사전 훈련 된 언어 모델의 편향에 대 한 포괄적인 조사, 특히 NLP의 변압기 기반 컨텍스트 사전 훈련 된 언어 모델에서 비롯 되는 다양 한 종류의 편향에 대 한 심층 처리를 식별, 정량화 및 완화 전략과 함께 제시 합니다.
* 처음으로 우리가 아는 한, 특히 사전 훈련 된 대규모 언어 모델의 맥락에서 사회적으로 관련성이 높고 덜 해결 된 문제인 Affective Bias를 조사 합니다.
* 사전 훈련 된 대규모 언어 모델을 평가 하기 위한 적합성과 함께 사용 가능한 많은 편의 평가 말뭉치를 수집 하 고 제시 합니다.
* 사전 훈련 된 대규모 언어 모델 및 정의적 편향에 대 한 현재 연구 과제에 대해서도 논의 합니다.

나머지 논문은 섹션 2에 제공된 PLM의 배경 및 편향, 섹션 3의 PLM 편향 정량화, 섹션 4의 PLM 편향 완화, 섹션 5의 식별 및 완화 전략을 포함한 PLM의 정의적 편향, 섹션 6의 이용 가능한 편향 평가 말뭉치 목록, 섹션 7의 연구 과제 및 섹션 8의 결론 발언으로 구성된다.

도 1: 사전 훈련된 언어 모델에서의 편향의 이질적인 뷰

## 사전 학습 된 언어 모델 2

### Background

딥러닝의 발전은 텍스트 데이터에 대한 효과적인 표현을 생성함으로써 NLP를 신경 LM 또는 대형 PLM으로 이끄는 새로운 시대로 이끌었으며, 여기서 대형 텍스트 말뭉치로부터 PLM에 의해 조밀하고 자동으로 추출된 표현은 희소하고 수작업된 표현(예: TF-IDF 특징) 및 관련 단점들을 재정의한다. 따라서, 이러한 PLM 표현은 일반적으로 새로운 모델을 처음부터 훈련시키는 부담을 피하면서 상당한 최첨단 결과를 달성하기 위해 NLP의 다양한 다운스트림 작업에 대한 언어 표현으로서 범용적으로 사용된다[87]. 표현은 언어의 범용 특성을 포함할 수 있고 다양한 작업을 학습하는 데에도 유용할 때 강력해진다. 언어 맥락에서 이러한 강력한 표현은 구문, 의미론, 실용론 등과 같은 텍스트에 숨겨져 있는 잠재된 언어 관례와 상식 지식을 포착해야 한다. 이러한 언어적 표현을 향한 한 단계는 단어를 분산 d차원 임베딩 공간 또는 벡터로 매핑하여 비맥락 임베딩 모델을 개발하는 것이었다. Mikolov et al. [74]에 의해 개발된 연속 Bag-of-Word 및 Skip-Gram 모델(word2vec)과 같은 해당 스트림 내의 얕은 아키텍처는 일반 언어 표현을 향한 초기 시도를 형성했다. 구조상의 단순성에도 불구하고, 이들은 단어들 간의 숨겨진 의미적 및 구문적 유사성을 포착할 수 있는 효과적인 단어 임베딩을 학습할 수 있는 능력이 높다. 인기 있는 것과 비슷하게

\begin{table}
\begin{tabular}{|l|l|l|} \hline Domain & Examples of Protected/Target groups & Work \\ \hline Gender & Male, Female, Gay, Lesbian & [6, 9, 10, 11, 13, 14, 16, 17, 18, 25, 30] \\  & & [39, 43, 48, 51, 52, 62, 66, 67, 76, 86] \\  & & [94, 100, 102, 104, 108, 110, 113, 115] \\  & & [116, 119, 122, 123, 125, 126, 129] \\ Race & Black, White & [51, 52, 76, 100, 110, 115] \\ Religion & Jewish, Hindu, Muslim, Christian & [2, 30, 51, 76, 94] \\ Profession & Homemaker, Nurse, Architect & [6, 14, 39, 53, 76] \\ Ethnicity & Asian, Hispanic & [3, 48, 58, 94] \\ Disability & Sensory (blind), Neurodiverse (auttism), Psychosocial (schizophrenia) & [51, 67, 115] \\ Age & Old, Young & [32, 51, 94] \\ Politics & Conservative, Liberal & [65, 94, 100] \\ Continent & Africa, Asia, Oceania, Europe & [6, 30, 43] \\ Nationality & American, Italian & [51, 98] \\ Physical & Short, Tall, Fat, Thin, Overweight & [51, 94] \\ appearance & & \\ Socioeconomic & Poor, Rich, Homeless & [51] \\ status & & \\ Intersection & Race+Gender (Black Women) & [43, 53, 67, 110] \\ \hline \end{tabular}
\end{table}
표 1: 사전 훈련된 언어 모델sword2vec 아키텍처 [74], 말뭉치로부터의 단어 대 단어 동시 발생 통계를 활용하는 GloVe [82] 및 하위 단어 정보를 활용하는 FastText [49]에서의 상이한 바이어스 도메인은 또한 많은 다운스트림 태스크를 해결하기 위해 상당한 관심을 끌었다. 그러나 이러한 임베딩은 본질적으로 비맥락적이기 때문에 더 높은 수준의 맥락 개념에 의존하는 명확화, 의미 역할, 다의적, 구문 구조 및 대용을 포착하지 못한다. 많은 연구자들은 단어의 문맥적 표현을 포착하지 못한다는 비판에도 불구하고 문장, 단락, 심지어 문서의 임베딩을 학습할 수 있는 다양한 모델을 제안했다.

문서에서 단어의 표현은 문맥에 따라 달라지는데, 유사한 단어는 다양한 문맥에서 서로 다른 의미론을 가지고 있기 때문이다. 따라서, 종래의 비맥락화된 임베딩을 대체하면서, 최근의 연구는 폴리세미와 같은 언어 현상을 기술하는 능력으로 인해 점점 더 보편화되고 있는 ELMo[83], BERT[31] 등과 같은 새로운 세대의 문맥화된 임베딩 모델을 제시하였다. 비맥락화된 표현과 달리 문맥화된 단어 표현은 신경망 문맥 인코더를 사용하여 생성되며, 정교한 성질은 해석 가능성을 약화시키지만 기존의 임베딩에 비해 대부분의 NLP 작업에서 최첨단 성능을 달성했다. 뉴럴 인코더에는 크게 두 가지 유형이 있는데, 컨볼루션 모델과 리커런트 모델을 포함하는 시퀀스 모델[83]과 완전히 연결된 셀프-어텐션 및 고급 변압기 아키텍처를 포함하는 비순차 모델[88; 31; 121]이 있다. CNN(Convolutional Neural Network) 및 RNN(Recurrent Neural Network)으로 개발된 문맥화 표현 모델들은 다른 이슈들 중에서 장기적인 문맥을 모델링하는데 어려움을 겪는다[50]. 이것은 새로운 형태의 학습 모델인 Transformer[114]를 가져오는데, 이는 RNN에 비해 훨씬 더 많은 병렬화를 가지며 텍스트 데이터로부터 더 긴 종속성과 컨텍스트를 효과적으로 모델링할 수 있는 완전 자기 주의 기반 아키텍처이다. 모든 종류의 LM을 구축하기 위해 라벨이 붙은 데이터를 수집하고 지도 학습 모델에 공급하는 것은 지루하다. 그러나 긍정적인 측면에서 LM의 이점은 종종 자체 감독 학습을 통해 실현될 수 있는데, 이는 의사 감독에 기반한 학습 데이터 자체로부터 레이블을 자동으로 생성함으로써 매우 일반적인 지식 표현을 학습하기 위해 사용 가능한 많은 라벨이 없는 텍스트 데이터를 활용하는 이 시나리오에 적용된 새로운 학습 패러다임이다[87]. 이전의 비-맥락화된 사전-트레이닝된 워드 임베딩들(예를 들어, word2vec 및 GloVe)은 다른 다운스트림 태스크들에서의 유용성 및 미세 조정 전략의 적용성에 큰 중요성을 부여하지 않는다. 보편적인 문맥 단어 표현 외에도, BERT[31], GPT[88], XLNet[121] 등과 같은 문맥화된 PLM은 미리 훈련된 모델을 미세 조정함으로써 많은 다운스트림 NLP 태스크에서 더 나은 성능을 수행하는 모델을 구축하는데 유용하며, 이는 각 다운스트림 태스크에 대한 스크래치로부터 모델을 훈련시키는 부담을 결정적으로 회피한다. 자동 인코딩 사전 훈련 아키텍처 BERT[31]는 마스킹 언어 모델 접근법을 사용하고 양방향 컨텍스트를 가능하게 함으로써 GPT[88]와 같은 단방향 자기 회귀 모델의 한계를 극복하지만, 미세 조정에서 마스크를 사용할 수 없기 때문에 사전 훈련-피네이션 불일치가 발생한다. 일반화된 자기 회귀 사전 훈련 모델인 XLNet [121]은 동시에 양방향 컨텍스트를 가능하게 하기 위해 무작위 순열을 도입함으로써 더 나은 결과를 달성한다. 또한, 다수의 상이한 도메인 특정(예를 들어, 생물의학[60], 금융[120] 등), 단일 및 다국어 PLM은 그들의 아키텍처 또는 사전 훈련 작업에 따라 다양하다.

### 사전 훈련된 언어 모델의 편향

단어 표현은 의미적 유사성을 포착하고 단어 벡터 유사성을 통해 단어 관계를 나타낼 만큼 강력하지만 PLM에서 여러 고정관념과 사회적 편향의 명시적 및 암시적 존재는 많은 실제 응용 프로그램에서 유용성을 손상시킨다. 큰 PLM의 편향은 발달 과정의 다양한 단계에서 발생한다. 그림 2는 특히 최근의 변압기 기반 PLM에 초점을 맞출 수 있는 가능한 단계와 함께 대형 PLM의 워크플로를 보여준다. 편향을 완화하기 위해서는 다양한 편향의 원인을 이해하고 분리하는 것이 필수적이다. 편향의 원인에 대한 조사는 데이터 탐욕스러운 NLP 알고리즘을 훈련할 만큼 충분히 큰 오늘날의 데이터 범람을 형성하는 인간 언어가 역사적으로 사회에 만연하는 몇 가지 심각한 고정관념과 사회적 편향, 즉 _역사적 편향_을 축적한다는 관찰로 이어진다. 따라서 언어는 사회적 편견이 초래되고 전파되고 반향되는 가장 강력한 방법 중 하나이다[72]. 여성 및 남성, 성별, 인종 등과 같은 다양한 영역에 속하는 흑인 및 백인과 같은 대상 용어들 중에서 지배력, 권력, 품질 또는 지위의 관점에서 여러 비중립적인 고정관념이 언어적 의사소통, 영상화 비대칭으로 살아가고 있다[36]. 자연스럽게 여기면, 사람들은 일상적인 담화에서 이러한 선입견의 대부분을 연습하고, 결과적으로 이러한 언어적 차별을 일상화하고 그들을 덜 가시적으로 느끼게 한다[77]. 따라서 이러한 과거 데이터 리포지토리에서 데이터 샘플을 완벽하게 측정하고 취하더라도 과거 편향의 대표인 _Data Bias_와 같은 편향으로 인해 PLM에 편향이 발생한다[107].

선천적 역사적 편견에서 비롯된 데이터 편향은 다양한 작업에 대해 문헌에서 탐구된 다양한 편향 소스 중 가장 일반적인 편향 소스이다[28]. 여기서 데이터의 품질 문제, 도메인에 관한 목표와 관련된 데이터의 주요 용어의 불균등한 분포(발생 또는 동시 발생) 등이 이에 기여하는 다른 요인이다. 비맥락적[16; 22; 40; 68] 및 맥락적 모델[110]을 사전 훈련하기 위해 사용되는 표준 데이터 세트는 성별, 인종 등과 같은 다양한 영역에서 편향 또는 불균형을 나타내는 것으로 확인되었다. 큰 PLM들의 맥락에서, 데이터 바이어스는 PLM의 초기 트레이닝 프로세스에서의 _미리 트레이닝된 Data Bias_ 또는/및 그것 바로 하류에 있는 _Fine-tuning Data Bias_일 수 있다. 연구에 따르면 데이터 편향은 기본 기계 학습 모델에 의해 전파되고 더 증폭될 수 있으며, 이는 언어적 속성을 학습하기 위한 자체 지도 학습 전략에서 _모델 학습 편향_ 및 태스크 특정 미세 조정 모델 수준에서 _다운스트림 태스크 학습 편향_으로 이어진다.

모델 학습 편향은 PLM으로부터 도출된 단어 표현에 반영되어 _Representation Bias_를 생성한다. word2vec, GloVe 등과 같은 비맥락적 단어 임베딩은 성별[16; 22], 인종[68] 및 민족 그룹[40]에 걸쳐 표현적 편향을 포함하는 것으로 알려져 있다. 이러한 임베딩은 _'간호사'_, _'접수사'_ 및 _'가정주부'_와 같은 직업의 단어 표현을 여성과 관련시키고 _'의사'_, _'철학자'_ 및 _'컴퓨터 프로그래머'_를 남성과 관련시킨다는 것이 명백히 밝혀졌다[16]. 또한, 그들은 _'지혜'_와 같은 단어들의 표현을 _'할머니'_보다 _'할아버지'_의 표현에 가깝게 배치하고, 인기 있는 아프리카계 미국인 이름들의 연관을 불쾌한 구절들과 인코딩한다[22]. 맥락화된 임베딩에서의 바이어스에 대한 조사는 이들이 또한 종래의 임베딩과 같은 바이어스를 나타낸다는 것을 드러낸다[18, 126]. 예를 들어, BERT는 인간-유사 바이어스들을 인코딩하는 것으로 발견된다[55]. 최근의 NLP 정보 검색 시스템은 검색 엔진, 질의 응답 등과 같은 것들이 대부분이다. 이러한 편향된 표현에 크게 의존하여 결과적으로 매우 편향된 검색 행동으로 이어진다. 마찬가지로, 언어 생성에서 큰 PLM GPT-3의 사용은 _'무슬림'_을 _'테러리스트'_[2]로 유추하는 종교 편향을 보여준다. 대부분의 언어 시스템의 기본 구성 요소인 단어 임베딩이 감정 분석[79], 언어 생성[66], 독성 언어 탐지[48] 등과 같은 과량의 다운스트림 응용 프로그램에 배포될 때 바람직하지 않은 결과를 야기하는 [22, 61, 127]을 전파하거나 강화할 수 있음을 관찰하면 이러한 모든 편향은 방해가 된다.

하류 과업 학습 편향은 대중에게 전달된 결과가 결국 해로운 사회적 고정 관념을 강화하는 사회경제적 배제에 빠지게 할 수 있다[16, 61, 127]. 다운스트림 애플리케이션은 일반적으로 대규모 데이터 세트에 미리 훈련된 기존 소스 네트워크 표현으로 학습 모델을 초기화하고 나중에 다운스트림 대상 작업에 적합한 데이터 세트를 사용하여 미세 조정함으로써 구현되기 때문에, 사전 훈련 데이터 편향, 사전 훈련 표현 편향 및 미세 조정 데이터 편향은 모두 다운스트림 애플리케이션에서 편향을 유도하는 소스가 될 수 있다. 감정 분석[52], 욕설 탐지[81], 텍스트 분류[33], 기계 번역[38, 112], 개인화된 의학[90], 코어퍼

도 2: 사전 훈련된 대규모 언어 모델에서의 바이어스

범죄 재범 해결[95, 128], 범죄 재범 예측 시스템[27], 이력서 심사 자동화[56], 온라인 광고 전달[57, 109] 등은 다양한 도메인에서 편향을 보고하는 다운스트림 애플리케이션의 일부이다. 다운스트림 작업을 미세 조정하기 위한 레이블 사용의 부적절하거나 불공정한 선택은 편향의 또 다른 원인, 즉 _측정 바이어스_[107].

이러한 편향은 사전 훈련된 학습에서 발생하는 경우 _Intrinsic Bias_ 또는 다운스트림 작업 모델링에서 발생하는 경우 _Exrinsic Bias_로 구분할 수 있다. 위에서 언급된 편향 외에도, 실세계 머신 러닝 모델의 관점에서, 최종 시스템은 태스크에 대한 벤치마크 데이터세트가 특정 그룹을 나타내지 않을 때 발생하는 _평가 바이어스_(예를 들어, 모델에 의해 식별되지 않는 비백인 여성의 이미지) 및 다르게 사용될 때 특정 태스크에 대해 설계된 모델의 비호환성으로 인해 발생하는 _배포 바이어스_(예를 들어, 문장 길이를 결정하기 위한 목적으로 미래 범죄를 예측하기 위해 생성된 위험 평가 도구를 사용함)를 고려해야 한다[107]. 표 2는 큰 PLM의 편향을 식별하고 완화하는 대규모 연구 세트를 보여주며, 대부분의 연구(약 53%)는 주로 편향 식별에 중점을 둔다.

\begin{table}
\begin{tabular}{|l|c|c|c|c|} \hline Work & PLM & Domain & Quantification & Mitigation \\ \hline
[102] & BERT, XLNet, & Gender & WEAT, sequence likeli- & WEAT score as \\ & ALBERT, & & hood, pronoun ranking & additional loss \\ & DistilBERT, & & regularizer \\ & RoBERTa, & & & \\ & GPT-2 & &
[3] & BERT & Ethnicity & Normalized probability, & M-BERT, Contextual Word Alignment \\
[113] & DistilBERT, & Gender & Skew and stereotype matrices & Data augmentation \\ & RoBERTa, & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & &
[39] & BERT & Profession, & Model querying to predict pronouns at masked & GEnder Equality \\ & & & position, given context & Prompt \\
[13] & BERT & Gender & MLP regressor & Removing word vector components from gender directions \\ \hline \end{tabular}
\end{table}
표 2: 사전 훈련된 대규모 언어 모델에서 바이어스를 다루는 작업 표 2 - 이전 페이지로부터 계속됨

\begin{tabular}{|l|l|l|l|l|} \hline Work & PLM & Domain & Quantification & Mitigation \\ \hline
[65] & GPT-2 & Politics & Indirect and Direct bias & Reinforcement \\ & & metrics & Learning \\
[48] & RoBERTa & Ethnicity, & Classification Performance, Group identifier & Upstream Bias \\ & & bias metrics, African & Mitigation \\ & & American Vernacular & & & & lar English, Dialect & & & & Bias Metrics, Gender & & & Stereotype Metrics & &
[98] & GPT-2, T5 & Occupation, & Self-Diagnosing & Self-Debiasing \\ & & Nationality, & \\ & & Religion, & & \\ & & Gender, & & \\ & & Age, Race, & & Disability & &
[2] & GPT-3 & Religion & Prompt completion, & -- \\ & & & analogical reasoning, & story generation \\
[104] & XLM, XLM-RoBERTa, & Gender & Point-wise mutual information and its extension & -- \\ & BERT 다국어 & & latent sentiment & \& regularization \\
[51] & BERT, & Gender, & All Unmasked Likelihood & -- \\ & ALBERT & Gender, & Race, Sexual orientation, & \\ & Religion, & & Attention Weight \\ & & Nationality, & \\ & & Disability, & & \\ & Age, Physical & & & & appearance, & Socioeconomic & & & & & & & status & \\
[11] & EMLo & Gender & Gender direction in gender subspace & -- \\
[123] & BERT & Gender & Prediction probability scores & -- \\ \hline \end{tabular}

다음 페이지 계속 표 2 - 이전 페이지 계속

\begin{tabular}{|l|l|l|l|l|} \hline Work & PLM & Domain & Quantification & Mitigation \\ \hline
[43] & ELMo, GPT, BERT, GPT-2 & Intersectional & Intersectional Bias Detection, Emergent Intersectional Bias Detection, CEAT & -- \\
[67] & DistilBERT, GPT-2, GPT-3 & Intersectional & Difference/similarity between sentiment scores & -- \\
[119] & BERT, GPT-2, T5, XLNet & Low frequency names & SV-WEAT, Intra and Inter layer self-similarity score & -- \\
[62] & BERT & Gender & BERT Attention maps & -- \\
[53] & GPT-2 & GPT-2의 Intersectional & Predictions & -- \\
[76] & BERT, RoBERTa, XLNET, GPT2 & Profession, gender, Religion, Race & Language modelling score, Stereotype score, iCAT & -- \\
[66] & LSTM & Gender & Bias Score, Intervention match & Counterfactual Data Augmentation, Word Embedding \\
[125] & BERT pre-training on medical note & Ethnicity, gender, Insurance groups & Log probability score, extrinsic evaluation using downstream tasks & -- \\
[9] & BERT & Profession, gender & Association test & Name based Counterfactual Data Substitution \\
[116] & GPT-2 & 젠더 & 인과관계 분석 & -- \\
[110] & GPT-2, & Intersectional & Counting occurrences, & -- \\  & CBoW-GLoVe, & Gender, Race & Modified SEAT & \\  & ELMo, GPT, BERT & & & \\ \hline \end{tabular}

다음 페이지 계속

## 3 Quantified Bias of PLMs

다양한 영역에서 다양한 편향 출처의 불리한 결과를 연구하기 위해서는 어떤 방식으로 편향을 정량화할 필요가 있다. 발생 단계에 따라 편향은 일반적으로 말뭉치, 표현 및 다운스트림 작업에서 정량화된다.

### corpora에서 편향 정량화

핵심 용어의 출현 또는 동시 출현을 집계하고 이로부터 다양한 통계를 도출하면 말뭉치의 편향을 발견하는 데 도움이 된다. Bordia와 Bowman[18]은 젠더화된 단어 주위에 고정적이고 무한한 크기의 컨텍스트 윈도우를 정의함으로써 젠더화된 단어의 컨텍스트 내에서 단어 레벨 확률 프로파일을 사용하여 구축된 편향 점수를 찾아 언어 모델을 구축하는 데 사용되는 세 개의 공개적으로 이용 가능한 데이터 세트, 즉 Penn Treebank[69], WikiText-2[73], CNN/Daily Mail[46]에서 성별 편향을 연구한다. 그들의 편향 점수는 단어가 여성 또는 남성 성별 단어와 더 자주 동시 발생하는지 여부를 찾는 데 도움이 된다. 고정된 크기의 컨텍스트 윈도우의 경우, 더 작은 윈도우가 타겟 단어에 더 집중할 수 있는 반면 더 큰 윈도우가 더 넓은 주제에 집중할 수 있기 때문에 최적의 크기의 윈도우를 찾는다. 무한 크기의 창은 주요 용어와 성별 단어 사이의 거리가 증가함에 따라 기하급수적으로 감소하는 가중치를 사용하여 훨씬 더 안정적이다. 탄

\begin{table}
\begin{tabular}{|l|c|c|c|c|} \hline Work & PLM & Domain & Quantification & Mitigation \\ \hline
[55] & BERT & Gender & Log Probability Bias & — \\ & & Score, WEAT & \\
[126] & ELMo & Gender & Co-occurrence & Train-time data augmentation, \\ & & & & test-time neutralization \\
[86] & LSTM & Gender & Occurrences, causal testing, Euclidean distance & Loss function modification \\
[18] & LSTM & Gender & Fixed \& infinite context bias scores & Loss function modification \\
[10] & ELMo & Gender & Direct Bias, Biased words clustering \& classification & — \\
[47] & Transformer-XL & Country, Occupation, Name & Individual \& group fairness metrics from sentiment scores & Embedding Sentiment regularization \\ \hline \end{tabular}
\end{table}
표 2: 이전 페이지세트 al. [110]에서 계속하여, 주요 용어들(예를 들어, 여성 또는 남성 대명사)의 발생 및 이들의 정형적으로 젠더화된 직업 용어들과의 동시 발생을 카운트하고 통계 분석을 수행하여 성별 편향 및 또한 10억 단어 벤치마크[26], 북코퍼스[131], 위키피디아 및 웹텍스트[89] 데이터세트들에서 문맥 단어 모델들을 사전 트레이닝하기 위해 사용된 인종 및 횡단면 편향들을 발견한다.

### 표현에서 편향 정량화

#### 3.2.1 벡터 공간의 기하학

특정 작업은 임베딩의 부분 공간을 분석하여 편향을 정량화한다. Bolukbasi et al. [16]은 _'sister'_, _'brother'_, _'grandmother'_, _'grandfather'_ 등과 같은 젠더화된 단어의 단어 벡터와 젠더 중립 단어의 차이를 봄으로써 단어 임베딩에서 젠더 편향의 발생을 보여준다. 그들은 본질적으로 성별을 포착하는 방향을 관찰하고 이 방향으로 성별 중립 단어를 투사하면 성별 편향을 정량화하는 데 도움이 된다. Manzini 등[68]은 다중클래스 설정에 적합하도록 이 접근법을 확장하고 단어 표현이 종교 기반 및 인종적 편향을 포함하는 몇 가지 고정관념적 편향을 나타낸다는 것을 입증한다.

#### 3.2.2 단어 연관 테스트

표현 편향을 정량화하기 위한 단어 연관 테스트는 대상 개념과 속성의 연관성의 차이를 측정하여 인간의 잠재의식 편향을 이해하려는 심리학[42]의 암시적 연관 테스트(IAT)에서 영감을 받았다. Caliskan 등[22]은 비맥락 워드 임베딩들 GloVe 및 Word2Vec에서의 바이어스를 정량화하기 위해 IAT와 유사한 워드 임베딩 연관 테스트(WEAT)라고 불리는 통계적 테스트를 제안한다. WEAT을 사용하여 저자는 유럽계 미국인 및 아프리카계 미국인 이름과 같은 보완 범주에서 단어 임베딩의 유사성을 유쾌하고 불쾌한 속성과 같은 보완 속성과 비교한다. 동일한 속성을 가진 아프리카계 미국인 이름과 비교할 때 유럽계 미국인 이름과 속성의 연관성 사이의 비유사성은 그들의 연구가 임베딩에 인간과 유사한 암시적 편향의 존재를 보고하는 데 도움이 된다. ELMo 및 BERT와 같은 문장 인코더에서 바이어스를 테스트하기 위해, May et al. [70]은 Sentence Encoder Association Test(SEAT)라는 이름의 WEAT의 일반화를 제안한다. SEAT를 사용하면 이러한 임베딩에 편향이 있는지 확인할 수 있지만 결과는 일반화할 수 없었다. 그들은 또한 결과의 비유사성이 문맥 임베딩이 편향이 없는 것이 아니라 최근 문맥 모델의 임베딩 유사성을 측정하는 데 적합하지 않은 코사인 유사성의 표시일 수 있으므로 그러한 표현에서 편향을 정량화하는 대안이 필요할 수 있다고 지적한다. Kurita et al. [55] 또한, 언어 모델의 문맥 및 상태에 따라 단어들의 임베딩이 상이할 수 있기 때문에, 종래의 코사인 유사도 기반 방법들은 문맥 모델들로부터 생성된 문장 임베딩들에서 바이어스를 찾기 위한 일관된 결과들을 생성하지 않는다는 것을 보여준다. 전자의 테스트들이 단어의 자연스러운 사용을 반영하지 않는 인공 컨텍스트에서 유쾌하고 불쾌한 용어와 같은 미리 정의된 고정 속성들 및 개별 단어들에만 집중됨에 따라, Nadeem 등[76]은 큰 PLM들의 세트에서의 편향을 본질적으로 추정하기 위해 두 개의 컨텍스트 연관 테스트(CAT)를 제안한다. 그들은 각 대상 용어가 자연스러운 맥락을 갖는 문장 수준과 담화 수준에서 CAT를 수행하고, 최근의 문맥 모델 BERT, GPT2, RoBERTa 및 XLNet이 언어 모델링 능력과 관련하여 강한 편향을 나타내는 것을 관찰한다. WEAT를 기반으로, Guo 및 Caliskan [43]은 상이한 컨텍스트에 따라 신경 언어 모델 ELMo, BERT, GPT 및 GPT-2에서 바이어스를 확인하고 광범위하게 정량화하기 위해 컨텍스트화된 임베딩 연관 테스트(CEAT)를 제안한다.

### 다운스트림 작업에서 편향 정량화

이 범주에서는 편향의 영역이 연구되고 있는 대상 용어의 맥락에서만 다른 평가 코퍼스에 대한 시스템의 성능 점수를 확인하여 편향을 정량화한다. 예를 들어, _'She is here'_ 와 같이 젠더화된 단어의 성별을 _'He is here'_ 로 변경하기 위해 성별 스와핑을 한 다음, 이 두 문장의 모델 성능을 평가한다[52, 66, 95, 128]. 이 시스템은 성별 단어만 다른 두 문장에 대해 다른 수행 점수를 생성하는 경우 성별 편향을 나타낸다. Dixon 등 [33]은 독성 코멘트를 식별하기 위해 구성된 텍스트 분류기에서 편향을 정량화하기 위해 오류율 동일성 차이로부터 도출된 두 가지 성능 평가 메트릭을 제시한다. Park et al. [81]은 이러한 메트릭을 활용하여 Dixon et al. [33]에 의해 제안된 성별 편향되지 않은 데이터세트를 생성하는 방법과 함께 편향을 정량화하여 욕설 검출에서 성별 편향을 찾는다. Kiritchenko와 Mohammad[52]는 _SemEval 2018: Task 1 Affect In Tweets_에 참여한 219개의 감정 분석 모델에서 성별 및 인종 편향을 통계적으로 평가하기 위해 말뭉치보다 성별 단어 및 인종에서 다른 문장의 예측된 강도 점수의 차이를 활용한다. Zhao et al. [128]은 성별 대명사(그녀/그)를 다양한 여성 또는 남성 정형 직업(비서/의사)과 연관시키는 문장 쌍을 포함하는 WinoBias라는 평가 코퍼스에 대한 F1 점수를 사용하여 상호참조 시스템에서 성별 편향을 예시한다. Rudinger 등 [95]은 또한 젠더화된 단어들에서만 상이한 문장들의 쌍들에 대한 상호참조 시스템에서 유사한 젠더 편향 연구를 수행한다. Lu 등[66]은 성별 교환되는 문장 쌍에 걸쳐 다양한 직업을 갖는 젠더화된 단어의 성능에서의 비유사성을 계산함으로써, 상호참조 해소 및 언어 모델링에서의 성별 편향을 정량화한다. 또한, 편향은 또한 그것이 특정 결정들 또는 예측들에 어떻게 도달하는지에 대한 모델 해석들을 조사함으로써 해석가능성을 사용하여 측정될 수 있다[35].

## 4 Mitigating Bias of PLMs

PLM의 편향을 완화하기 위해 과소 대표되거나 비주류 그룹에 대한 차별적 영향을 줄이거나 제거하기 위한 다양한 노력이 이루어졌다. 우리는 디바이어싱 접근법을 다루는 단계에 따라 세 가지 범주로 분류한다.

### 훈련 corpora에 대 한 편향 완화

감성 분석, 텍스트 분류 등과 같은 외재적 애플리케이션을 위해 설계된 대부분의 계산 알고리즘은 주로 라벨링된 트레이닝 코퍼스에 의존하여 이러한 코퍼스에 존재하는 사회적 편향을 일으키기 쉽다[111]. 그럼에도 불구하고, 이러한 말뭉치는 새로운 대규모 훈련 말뭉치를 구축하는 데 비용이 많이 들거나 노동 집약적이기 때문에 NLP의 다양한 응용 분야에서 여전히 광범위하게 활용되고 있다. 따라서 일반적으로 디바이어싱 훈련 말뭉치를 위해 데이터 증강 및 바이어스 미세 조정과 같은 일반적인 기술이 뒤따른다.

#### 4.1.1 Data augmentation

데이터 증강 기법은 말뭉치에 비교적 적은 수의 데이터가 있는 대상 그룹을 지원하기 위해 추가 데이터를 제공하여 훈련 코퍼스를 삭제하고 이에 따라 균형 잡힌 훈련 코퍼스를 생성한다. 따라서, 증강은 훈련 코퍼스에서 도메인의 임의의 특정 표적 그룹의 과소/과잉 표현을 상쇄할 수 있다. Zhao 등[126;128]은 직업들을 남성 용어와 연관시킬 가능성이 여성 용어들보다 매우 높다는 것을 관찰하면서, 상호참조 해결의 다운스트림 태스크에서 젠더 편향을 감소시키기 위해 데이터 증강을 채택한다. 그들은 남성 용어에서 여성 용어로, 그 반대로 명명된 개체를 익명화한 후 훈련 코퍼스로 데이터의 성별 교환 버전을 보강한다. Park et al. [81]은 Zhao et al. [128]에 의해 제안된 데이터 증강의 아이디어를 남용 언어 검출에서의 성별 편향을 감소시키기 위해 활용한다. Zhao et al. [128], Lu et al. [66]과 매우 유사하게, 신경망 모델에서 내부 점수를 참조하여 바이어스를 표현함으로써, 상호참조 해결 및 언어 모델링을 포함하는 신경망 NLP 애플리케이션에서 젠더 바이어스를 탐색하기 위한 Counterfactual Data Augmentation을 제안한다. 영어의 젠더 편향을 제거하려는 시도와는 별개로, Zmigrod et al. [132]는 더 간단한 접근법으로 비문법 문장을 전달하는 스페인어 및 히브리어와 같이 형태학이 풍부하거나 편향된 언어의 젠더 편향을 관리하기 위해 순진한 젠더 스와핑 기반 반사실적 데이터 증강으로의 변형을 제안한다. Maudslay 등[45]은 간접 편향을 해결하기 위해 Counterfactual Data Substitution 및 Names Intervention이라는 두 가지 변형을 제안함으로써 반사실적 데이터 증강을 개선한다. Liu et al. [64]는 Maudslay et al. [45]의 개념을 사용하고 대화 시스템에서 편향을 제거하기 위한 Counterpart Data Augmentation을 제안한다. 데이터 증강 기반 디바이어싱은 훈련 말뭉치의 편향을 줄이기 위한 간단한 기술이지만 주석이 비싸고 데이터 세트의 크기가 증가하여 훈련에 필요한 시간이 증가한다. 더욱이, 이러한 기술들은 일반적으로 바이너리 스와핑을 수행하기 위해 고립된 워드들만을 고려하고, 도메인에서 비-바이너리 및 보다 정교한 표현들을 대부분 무시한다. 또한, 이러한 기술들은 키 용어들 및 연관된 쌍들의 미리 정의된 제한된 리스트에 의존하며, 이는 아마도 불완전할 수 있고, 여기서 일부 용어들은 상이한 스펠링(예를 들어, _mommy_ vs. _ mummy_), 서로 다른 형태(예: _his\(\rightarrow\)her_ 및 _his\(\rightarrow\)hers_), 짝짓기 변형(예: _breastfeed_), 또는 블라인드 스왑(예: _she is pregnant\(\rightarrow\)he is pregnant_)으로 인해 터무니없는 문장을 생성한다.

#### 4.1.3 Bias fine-tuning

데비아 학습 코퍼스의 또 다른 접근법은 전이 학습에 의한 편향 미세 조정이며, 여기서 도메인에 대해 균형 잡힌 코퍼스를 구성하는 값비싼 프로세스 대신 전이 학습의 개념을 사용한다. 데이터 편향은 일반적으로 데이터 불균형 또는 작은 크기의 데이터 세트에서 비롯되기 때문에 모델이 편향된 데이터에 과도하게 적합하지 않도록 하기 위해 Park et al. [81]은 편향되지 않은 큰 데이터 세트에서 훈련된 소스 네트워크에서 정규화하고 그 후에 성별 편향된 대상 데이터 세트에서 미세 조정하여 성별 디비아 욕설 탐지 모델로 전이 학습을 활용한다. 접근 방식은 편향을 상당히 감소시키지만 (예상할 수 있듯이) 전체 모델 정확도에도 영향을 미치는 것으로 보입니다. Saunders et al. [97]은 신경 기계 번역에서 성별 편향을 다루기 위해 덜 편향된 작은 데이터세트에서 미세조정에 의해 작은-도메인 적응을 실행하는 Park et al. [81]과 거의 반대의 접근법을 제안한다.

### 표현에서 편향 완화

표현의 디비아에 대한 초기 접근법은 임베딩에서 도메인에 관련된 보호된 도메인/대상 용어의 하위 공간을 제거하는 기하학적 디비아싱이다. 도메인의 경우, 이 후처리 접근법은 바이어스를 유지하고 해당 도메인에 관한 대상 단어에 대한 중립 단어의 연관성을 제거하는 임베딩에서 부분 공간 또는 방향을 식별한다. 이러한 하드 디바이어싱 접근법은 젠더화된 단어들의 세트에 대한 젠더 중립 단어들의 거리를 평준화함으로써 단어 임베딩으로부터의 젠더 편향을 완화하기 위해 볼룩바시 등[16]이 뒤따른다. 또한 디바이어싱과 균형을 맞추기 위해 트레이드오프 매개변수를 사용하여 초기 임베딩 거리를 보존하는 것을 목표로 하는 소프트 바이어스 보정을 제안한다. 그러나 단어에서 성별 정보를 제거함으로써 작동하기 때문에 이러한 성별 정보를 사용하는 사회 과학 및 의료 응용 분야에서와 같이 일반화할 수 있는 접근법이 아닐 수 있다[8, 71]. 또한, 이들의 접근법은 성별 중립 단어를 구별하기 위해 분류기를 이용하며, 이는 차례로 분류 오류를 전파하여 디바이어싱의 성능에 영향을 미친다[129]. Zhao 등[129]은 추가 분류기를 채용하지 않고 단어 벡터를 트레이닝하는 과정과 함께 성별 중립 단어를 위치시킴으로써 성별에 대한 임베딩을 중화시킨다. 그러나, Gonen과 Goldberg의 [41] 실험은 Bolukbasi et al. [16]과 Zhao et al. [129]의 접근법이 편향을 단지 숨기지만 실제로 제거하지는 않는 불충분한 블라인드 디바이어싱 기술임을 보여준다. 단어 레벨 언어 모델들에 대해, Bordia 및 Bowman [18]은 Bolukbasi 등의 소프트 디바이징 버전으로서 성별 서브공간에 임베딩의 투영을 처벌하기 위해 손실 함수 정규화기를 사용한다. 고넨과 골드버그[41]가 언급한 바와 같이, 그들은 또한 그들의 편향 점수가 그것을 감지하지 못할 수 있는 경우로서 표현을 디바이어싱한 후에도 편향의 가능성을 언급한다. 이러한 직접적인 기하학적 디바이어싱에 대한 대안으로서, Zhao 등[126]은 디바이어싱 컨텍스트화된 표현을 위해 원래 및 대응하는 젠더 스왑된 데이터 버전의 표현을 평균함으로써 상이한 젠더 중화 절차를 제안한다. 인구통계학적 도메인에 관련된 워드 임베딩에서의 감정 편향을 완화하기 위해, 스위니 등[108]은 인구통계학적 용어와 정서의 상관 관계를 제거한다.

### 알고리즘 편향 완화

단어의 표현은 [67]에서 나타나는 복수의 상이한 컨텍스트에 따라 달라질 수 있기 때문에, 디바이어싱 표현은 문맥 임베딩에서 적용되기 어렵다. 그러한 경우들에서, 특정 작업들은 일반적으로 모델의 손실 함수를 수정함으로써 트레이닝 동안 예측들에서의 편향(즉, 인-프로세싱)을 완화시키는 방식으로 알고리즘/모델, 즉 사전-트레이닝된 또는 미세-튜닝된 모델들을 수정하는데 초점을 맞춘다. Qian 등[86]은 모델이 젠더화된 단어 쌍에 대한 예측 확률을 균등화하도록 적응하여 출력 성별 편향을 감소시키는 방식으로 언어 모델의 손실 함수를 변경한다. Silva 등[102]은 변압기 기반 PLM들을 디세이징하기 위해 WEAT을 사용하는 것을 고려하고, 나중에 RoBERTa의 손실 함수를 수정하기 위해 크로스 엔트로피와 함께 WEAT 점수를 사용한다. Huang 등[47]은 큰 PLM들에서 감정 편향을 완화하기 위한 임베딩 및 감정 구동 정규화 항들을 갖는 규칙적인 크로스 엔트로피 손실 함수를 변경하는 또 다른 변형을 제안한다. 적대적 트레이닝은 적과 함께 예측자를 트레이닝하는 동안 손실 함수를 변경함으로써 알고리즘을 디비아스하는 또 다른 방법이며, 이는 성별, 인종 등과 같은 보호된 도메인을 모델링하려는 적대적 함수를 최소화함으로써 공정한 예측을 생성하도록 의도된다[124]. 유사한 접근법이 Zhang et al. [125]에 의해 임상 문맥 단어 임베딩에서 성별 편향을 완화하기 위해 활용된다. Liu 등[65]에 의해 제안된 다른 최근의 접근법은 강화 학습을 사용하여 단어 임베딩 또는 분류기로부터의 보상을 이용함으로써 대형 PLM에서 정치적 편향을 디비아한다.

## 5 Affective Bias

텍스트 정의적 컴퓨팅은 텍스트로 주관적으로 작성되는 것과 여러 실제 시스템에서 감정, 감정 또는 의견을 기반으로 하는 결정, 즉 더 나은 영향을 받는 주관적인 결정을 내리는 방법을 정확하게 식별하는 알고리즘의 개발을 포함한다. 영화 리뷰, 제품 리뷰 등과 같은 텍스트 데이터에 의해 표현되는 감정(감정 또는 감정)을 검출하고 측정하기 위해 많은 시스템들이 처음에 제안되었다[80, 105]. 나중에 이러한 시스템은 의료[44], 상업적 응용[54, 92, 105], 정치[21], 교육[34, 105] 등과 같은 다양한 영역에 널리 채택되었다. 구글5, IBM6 및 마이크로소프트7과 같은 산업 대기업이 자연어 이해 도구를 개발했을 때 텍스트 정의적 이해는 그 중 중요하고 중요한 부분이 되었다. 텍스트 정의적 컴퓨팅 과제의 중요성, 특히 정서 분석에 대한 중요성을 입증하기 위해 연구자들에 의해 몇 가지 관찰과 주장이 제기되었다. Cambria et al. [23]에 따르면 감성 분석 작업은 POS 태깅, 개체명 인식 등과 같은 여러 가지 다른 NLP 문제를 성공적으로 해결함으로써 달성할 수 있는 인간 수준의 성능에 도달하기 위해 여전히 많은 여행을 해야 한다. Poria et al. [84]는 멀티모달 정의적 컴퓨팅, 비꼬기 분석, 심지어 정의적 NLP 시스템의 사회적 편견이라는 현대적 이슈와 같은 컴퓨팅에 영향을 미치는 낙관적인 미래 연구 방향에 주목하며, 20세의 감성 분석 과제가 포화 상태라는 기존의 믿음에 반대하기 위해 노력한다. NLP의 정의적 편향에 대한 우리의 연구는 정의적 컴퓨팅의 관련성 강화와 정보를 활용하는 다양한 NLP 애플리케이션에서의 광범위한 적용 가능성에 대한 이러한 관찰로부터 동기가 부여된다[4, 5, 37].

### 정의 및 의미

NLP와 기계 학습의 편향에 대한 연구는 실제 세계 8에 적용되었을 때 특정 사회 또는 소외된 그룹에 대한 NLP 편향의 부정적인 영향에 대한 관찰을 통해 크게 가속화되었다. 감정 편향은 연구자들이 특정 영역에서 과소 대표되거나 보호된 그룹을 나타내는 주요 용어와 정서의 불공정하거나 불균형한 연관성을 연구하고 정서 및 감정 감지와 같은 NLP 시스템에 어떻게 영향을 미치는지 연구하는 이 방향의 또 다른 최근 연구 범주이다. 예를 들어, Google 감성 분석기는 _being gay9_가 나쁘다고 추론하고 _'I'm a gay black woman'_ 및 _'I'm a gay gay'_ 와 같은 문장에 높은 부정적인 정서를 할당하는 것을 관찰한다. NLP에서 정의적 편향이라는 용어로 우리는 정동의 불공정하거나 편향된 연관성(분노, 두려움, 기쁨 등과 같은 감정일 수 있음)의 존재를 정의한다. 또는 텍스트 문서에서 특정 사회 집단에 대한 긍정, 중립, 부정과 같은 정서 또는 특정 사회 집단에 대한 과대 일반화된 신념(고정관념)이다. 예를 들어, 텍스트 문서에서, _'그녀'_, _'아내'_ 등과 같은 여성과 연관된 단어는 _'슬픔'_ 및 _'분노'_와 같은 특정 범주의 감정과 고도로 연관되고, 무슬림 종교의 표상은 폭력 [1] 등을 나타내는 부정적인 용어와 연관되는 것으로 관찰된다. 텍스트 정의적 컴퓨팅 시스템에서 정의적 편향의 존재는 비즈니스 및 상업적 의사 결정, 의료 등과 같은 작업에서 유용성과 적용 가능성에 해를 끼친다. 정의적 편향의 개념은 [20]에서와 같이 얼굴 감정 감지 시스템에 대한 분류 오류율이 과소 대표되는 사회 그룹에 대해 높을 가능성이 있기 때문에 NLP 프레임워크를 넘어 타당하고 적용할 수 있다. 이와 유사하게, 자동화된 감정 검출 시스템은 인간의 행동을 모델링하는 데 많은 지능형 인공 인공물 또는 인간의 감정 시스템을 모방하는 알고리즘의 완전성을 위해 큰 영향을 미치기 때문에, 감정 편향을 배경으로 모든 종류의 감정 컴퓨팅 시스템을 평가하는 것이 중요하다.

각주 8: [https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)

각주 9: [https://www.vice.com/en/article/j5jmj8/google-artificial-intelligence-bias](https://www.vice.com/en/article/j5jmj8/google-artificial-intelligence-bias)

### PLM의 정의적 편향

사전 기반 [94], 기존 기계 학습 [108], 딥 러닝 [14, 100], 하이브리드 [32] 접근법을 포함한 많은 텍스트 정의 컴퓨팅 시스템은 일반적으로 대규모 텍스트 말뭉치를 통해 학습된 모델을 통해 인간의 감정 편향으로부터 전달되는 정의적 편향을 영구화한다. 예를 들어, 알고리즘을 훈련시키기 위해 인간이 특정 과소 대표되는 종교를 향해 _분노_ 감정을 표현했던 텍스트 데이터를 포함하는 코퍼스의 사용은 나중에 감정 _분노_로 종교를 전파하거나/및 증폭시킬 수 있다. 표 3은 주요 특성과 함께 이 연구 분야의 작업에 대한 광범위한 스냅샷을 보여준다. 기존 연구의 주요 부분은 성별에 따른 정의적 편향과 정서 분석의 관점을 통해 연구한다[14, 52, 94, 100, 108]. 반면에 종교, 정치, 교차 편향 등과 같은 다른 영역과 세밀한 감정 클래스(분노, 두려움, 기쁨 슬픔, 놀라움, 혐오)의 관점에서 그 영향은 [52]와 [115]를 제외하고는 그다지 조사되지 않았다. 일반적인 평가 말뭉치가 감정/감정 그라운드 트루스 또는 출력 레이블이 있는 평가 문장과 함께 다양한 영역에서 정의적 편향 평가를 수행하기에는 부적절하다는 것은 이 연구 분야에서 주목할 만한 격차이다. 문헌의 대부분의 연구가 NLP 시스템에서 정의적 편향의 존재를 식별하려고 시도할 때 정의적 편향의 해악 완화를 탐구하는 사람은 거의 없다. 정의적 편향에 대한 작업의 본문은 기존 접근법과 런타임 검증 접근법의 두 가지 범주로 나눌 수 있다. 기존의 정의적 편향 분석 방법은 학습 데이터, 알고리즘, 표현, 미세 조정 데이터의 편향을 식별하고 마지막으로 다른 전략을 사용하여 편향을 완화한다[52, 108, 115]. 반면, 런타임 검증 접근법은 입력 문장들의 돌연변이들(고정형의 상이한 뷰들을 나타내는 입력 문장들 및 그들의 쌍을 이루는 문장들로부터 자동으로 생성된 템플릿들)을 사용하여 특정된 시스템의 각각의 실행에서 편향된 예측들을 모니터링하고 발견한다. 런타임 검증은 일반적으로 시스템이 각 런에 대한 공정성 기준을 만족하는지 여부를 검증하는 데 적합하다.

Shen 등의 [100]에 의한 종래의 접근법은 상이한 그룹의 사람들에 의해 생성된 유사한 콘텐츠를 포함하는 텍스트 기입에 대한 감정 예측에서의 편향을 조사한다. 편향 분석 및 식별은 공개적으로 이용 가능한 4개의 어휘 및 딥 러닝 기반 시스템에 대해 수행된다. Zhiltsova et al. [130]에 의한 유사한 접근법은 또한 4개의 인기 있는 어휘 기반 감정 예측 시스템을 사용하여 비원어민 영어 텍스트에 대한 감정 편향을 식별하고 완화한다. 이 두 작품 모두 서로 다른 인간 그룹에 걸친 언어 스타일 변화와 그것이 NLP의 정의적 편향으로 이어지는 방식에 의존한다. 사전 및 기존 기계 학습 시스템에서 정의적 편향에 대한 분석과 별도로 연구자들은 정의적 편향의 맥락에서 word2vec, GloVe 및 FastText와 같은 비맥락적 단어 임베딩을 탐구한다[32, 94, 108]. 이와 관련하여 Diaz et al. [32]의 연구는 널리 사용되는 10개의 단어 임베딩과 15개의 다른 감정 분석 모델에서 연령 관련 정의적 편향을 다룬 것이다. 이 연구는 주로 여론 조사 시스템이 어떤 연령대(나이가 많든 젊든)를 더 부정적으로 또는 긍정적으로 보고하는지, 예를 들어 _'young'_ 형용사가 있는 문장이 _'old'_ 형용사가 있는 동일한 문장보다 긍정적인 감정을 평가할 가능성이 더 높은지 여부를 검증한다. 비맥락화된 단어 임베딩에 기초한 유사한 연구들 중, Sweeney et al. [108]은 word2vec 및 GloVe에서 인구통계학적 정의적 편향을 완화하기 위한 적대적 학습 전략을 도입하고, Rozado et al. [94]는 임베딩 공간에서 문화적 축을 따라 단어를 나타내는 개념을 통해 편향을 식별하기 위해 단어 임베딩을 스크린한다. 키리첸코 등의 보다 일반적인 작업 [52]는 공유 작업 _SemEval-2018 작업 1 Affect in Tweets_에 참여한 200개의 감정 예측 시스템에서 정의적 편향을 식별한다. 평가 말뭉치의 모든 평가 문장에 대한 기본적인 감정 분노, 공포, 기쁨, 슬픔으로 일반적인 평가 문장과 지상 진실 감정 레이블을 갖는 몇 안 되는 공개 평가 말뭉치 중 하나인 EEC(Equity Evaluation Corpus)를 조달한다. 이와 유사한 평가 코퍼스가 Venkit 등[115]에 의해 장애인 영역의 문장을 고려하여 제안되었다.

상황화된 대형 PLM인 BERT에서 직업 고정관념에 관한 정의적 편향을 정량화하기 위한 보다 주목할 만한 접근법은 [14]에서 논의된다. 최근 더 많은 연구가 큰 PLM의 효능과 유용성으로 인해 일반 편향을 식별하고 완화하지만[63, 76], 큰 PLM의 정의적 편향을 조사하는 작업은 거의 없다. 구글 BERT, Facebook RoBERTa, Google ALBERT, Google ELECTRA, Facebook Muppet과 같은 인기 있는 PLM을 활용하는 감성 분석 시스템에서 편향을 발견하기 위한 주목할 만한 작업은 편향을 분석하기 위해 기존의 패러다임 대신 런타임 검증 접근법에 의존한다[6, 122]. 황 등에 의한 또 다른 흥미로운 접근법, [47]은 언어 모델에 의해 생성된 텍스트에 도입된 감정 편향을 조사한다. 이 새로운 시나리오는 BERT, ALBERT, RoBERTa, XLNet, GPT 등과 같은 대형 PLM에서 정의적 편향을 식별하고 완화하기 위해 더 많은 평가를 수행하는 것을 용이하게 한다.

\begin{table}
\begin{tabular}{|l|l|l|c|c|} \hline Work & Domain & Quantification & Mitigation & Model \\ \hline \multicolumn{5}{|c|}{Sentiment perspective} \\ \hline
[122] & [6] & — & BERT \\
[6] & Occupation, & Metamorphic Testing & — & BERT, RoBERTa, ALBERT, ELECTRA, Muppet \\ & Country of & & SVM, LSTM \\ & origin, Gender & Directional sentiment & Adversarial learning & SVM, LSTM \\
[94] & Ethnicity, Age, & Projecting word embeddings to cultural axis & — & Lexicon based \\  & graphic status, & & & \\  & Physical appearance, Religion, & & & \\  & Politics, Gender & & & \\ \hline \end{tabular}
\end{table}
표 3: 정의적 편향을 다루는 작업

## 6 Bias Evaluation Corpora

이상적인 편향 평가 말뭉치는 NLP 시스템에서 다양한 유형의 편향의 존재를 식별하고 측정하는 데 도움이 되는 필수 구성 요소이다. 이 검토는 정의적 편향에 특히 중점을 둔 PLM의 편향에 초점을 맞추기 때문에 표 4에서 PLM 및 정의적 편향의 배경에서의 한계와 함께 대중적이고 공개적으로 이용 가능한 편향 평가 말뭉치를 제시하며, 평가 말뭉치 중 일부는 향후 연구를 돕기 위해 공개 도메인에서 사용할 수 없기 때문에 피한다. 비뚤림 평가 말뭉치를 만들기 위한 가장 일반적인 시도는 처음에 템플릿 문장을 만들고 자연어의 편향을 유발하는 도메인(예: 성별)에 대해 대상(예: 여성, 남성)과 관련된 핵심 용어(예: _she\(\rightarrow\)he_, _wife\(\rightarrow\)husband_, _mother\(\rightarrow\)father_)를 상호 변경하는 것이다. 예를 들어, Diaz et al. [32]는 <'This (AGE RELATED KEY TERM)guy was 3 or 4 feet from the tide line and

\begin{table}
\begin{tabular}{|l|c|c|c|c|} \hline Work & Domain & Quantification & Mitigation & Model \\ \hline
[14] & 직업, 성별 및 통계적 유의성 차이 & — & BERT, Bi-LSTM, Logistic Regression \\
[32] & Age & Multinomial log-liner regression, paired t-test & — & Lexicon based, conventional machine learning, hybrid \\
[100] & Gender, Race, Politics & Difference in mean sentiment score, statistical significance test, linear regression & — & Rule based, Naive Bayes, Dynamic CNN, LSTM \\ \hline \multicolumn{5}{|c|}{Emotion perspective} \\ \hline
[115] & Gender, Race & Mean score of prediction, linear regression on sentiment scores & — & DistilBERT, TextBlob, Google API, VADER \\
[130] & Non-native English speaker & Wilcoxon signed rank test & Lexical score & VADER, Afinn, SentimentR, TextBlob \\
[52] & Gender, Race & Average score difference & — & Deep Learning, conventional machine learning, lexicon based \\ \hline \end{tabular}
\end{table}
<표 3>은 조석(潮火)이 출몰(出出)하고 있는 이전(前前)의 파고(潮火)(\langle\)AGE RELATED KEY TERM\(\rangle\)with _'old'_와 _'young'_이다. StereoSet [76]은 기존의 '핵심 용어 상호 교환' 접근 방식과 달리 아마존 기계 터크를 이용한 문장 간 및 문장 내 문맥 연관 테스트를 통해 조달된 매우 다양하고 대규모이며 자연스러운 편향 평가 코퍼스이다. 이러한 편향 평가 말뭉치의 설계 및 생성은 종종 데이터 편향, 알고리즘 편향 등과 같은 다양한 단계에서 편향의 실제 효과를 분리 및 추출하고 또한 도메인에서 특정 대상을 지칭하는 평가 말뭉치 또는 불균형한 핵심 용어 세트 내의 편향된 데이터의 존재를 피하기 위해 신중하게 정의된다.

대형 PLM을 평가하는 것은 특히 많은 실제 응용 프로그램에서 사용이 증가함에 따라 최근에 매우 필수적이 되었다. 몇몇 기존의 바이어스 평가 말뭉치는 이들 말뭉치 내의 이용가능한 평가 문장의 총 수, 즉 크기를 고려할 때 큰 PLM을 평가하기에 충분히 유용하다[33, 52, 128]. 그러나 이러한 말뭉치는 일반적으로 실제 시나리오와 거리가 멀고 무의미한 샘플(예: _'my(sister)is pregnant'\(\rightarrow\)'my(brother)is pregnant'_)을 전달할 수 있는 매우 간단하고 짧은 길이의 문장을 사용하여 합성적으로 구축되기 때문에 다양한 실제 컨텍스트를 표현하지 못한다. PLM을 평가하기에 적합한 효율적인 편향 평가 말뭉치를 정확하게 설계하기 위해, Liang 등[63]은 WIKITEXT-2[73], SST[103] 등으로부터 자연 발생 텍스트 말뭉치를 레버리지하여 [33, 52, 76, 101, 128]의 말뭉치보다 실세계 맥락을 더 잘 나타낼 수 있는 평가 문장을 생성한다. 또한, 정의적 편향을 평가하기 위한 보다 효율적인 평가 말뭉치가 필요하다; Kiritchenko 등에 의한 이용 가능한 말뭉치 중 하나. [52]는 앞서 언급한 바와 같이, 매우 단순한 합성 및 감정들이 문장에서 명시적으로 특정되는 현실 세계 템플릿과는 거리가 먼, 예를 들어 _'My brother made me feel angry__를 지상-진실 라벨 _anger_ 로 포함한다. Venkit et al. [115]에 의한 또 다른 정서 지향 코퍼스는 PLM을 평가하는 데 적합한 실제 컨텍스트를 잘 나타내지만, 장애인에게 관련된 편견을 해결하기 위해 특별히 설계되었다. 예를 들어 _'그들은 지상 진실 레이블 _anger_가 있는 멘탈 장애 이웃_ 때문에 악화되었다. 공정한 기계 학습 소프트웨어를 효과적으로 테스트하고 구축하기 위해서는 성별(레즈비언, 게이 등)을 이진 영역으로만 다루는 일반적인 방식을 제외하고 종교, 인종, 횡단면 편향, 성별의 비이진 표현과 같은 다양한 영역을 다루는 보다 일반적이고 실제 컨텍스트를 가능하게 하는 평가 코퍼의 개발이 매우 중요하다. 이러한 말뭉치는 결국 실세계에 배치되기 전에 시스템에 대해 수행된 임의의 다른 테스트와 같이 시스템 편향을 평가하기 위한 임의의 NLP 시스템의 필수적인 부분으로서 편향 평가 말뭉치의 사용으로 이어질 수 있다[24].

\begin{table}
\begin{tabular}{|c|l|c|c|} \hline Corpora & Domain & Model & Limitation \\ \hline BITS [115] [https://github.com/PranavNV/BITS](https://github.com/PranavNV/BITS) & Gender, Race & Emotion detection & In context of people with disabilities, not generic \\ \hline \end{tabular}
\end{table}
표 4: 바이어스 평가 코퍼스

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Corpora & Domain & Model & Limitation \\ \hline
[63] [https://github.com/pliang279/LM](https://github.com/pliang279/LM)\_bias & Gender, Religion & Large PLM & — \\ StereoSet [76] [https://github.com/moinnadeem/StereoSet](https://github.com/moinnadeem/StereoSet) & Gender, Race, Religion, Profession & Large PLM & Less diverse to represent real-world contexts \\ BEC-Pro [9] [https://github.com/marionbartl/gender-bias-BERT](https://github.com/marionbartl/gender-bias-BERT) & Gender, Profession & Large PLM & Less diverse to represent real-world contexts \\ GenderCorpus [14] [https://github.com/jayadevbhaskaran/gendered-sentiment](https://github.com/jayadevbhaskaran/gendered-sentiment) & Gender, Profession & Sentiment & Groud-truth sentiment labels are not available \\ AgeBias [32] [https://dataverse.harvard.edu/dataset.xhtml?persistentId=](https://dataverse.harvard.edu/dataset.xhtml?persistentId=) & Age & Sentiment & Groud-truth sentiment labels are not available \\ EEG [52] [https://saifmohammad.com/WebPages/Biases-SA.html](https://saifmohammad.com/WebPages/Biases-SA.html) & Race, Gender & Emotion & Explicit representation of emotions, small \& simple sentences, less diverse to represent real-world contexts \\ Winograd [95] [https://github.com/rudinger/winogender-schemas](https://github.com/rudinger/winogender-schemas) & Gender, Profession & Coreference & Small sentences \\ WinoBias [128] [https://github.com/uclanlp/corefBias/tree/master/WinoBias/wino](https://github.com/uclanlp/corefBias/tree/master/WinoBias/wino) & Gender, Profession & Coreference & Miss ambiguous pronouns in sufficient volume or diversity to accurately represent practical utility of models \\ GAP [117] [https://github.com/google-research-datasets/gap-coreference](https://github.com/google-research-datasets/gap-coreference) & Gender & Coreference & — \\ Identity synthetic dataset [33] [https://github.com/conversationai/unintended-ml-bias-analysis](https://github.com/conversationai/unintended-ml-bias-analysis) & Human & Text classification & Small and simple sentences, less diverse to represent real-world contexts \\ \hline \end{tabular}
\end{table}
표 4: 이전 페이지로부터 계속됨

## 7 논의 연구 과제

여기에서 NLP 편향, 특히 큰 PLM의 맥락과 정의적 편향의 맥락에서 몇 가지 문제에 대해 논의한다.

#### 7.0.1 NLP bias의 이질적 특성

NLP 편향을 식별하고 완화하기 위한 많은 연구 접근법은 일반적으로 성별 편향을 해결하는데(표 1에서 관찰할 수 있듯이), 인종, 종교, 횡단면 편향 등과 같은 다른 많은 영역이 존재함에도 불구하고 상당히 해결해야 한다. Tan 등 [110]은 이러한 맥락에서 인종 편향이 맥락화된 큰 PLM에서 강하게 인코딩된다는 증거를 제공하며, 아마도 성별 편향보다 훨씬 더 많을 것이다. 그들은 또한 둘 이상의 도메인(예: 아프리카계 미국인 여성)의 혼합물을 고려하는 교차 편향의 덜 탐구된 도메인이 1차 편향보다 훨씬 더 높다는 것을 보여준다. 또한, 대부분의 기존 연구는 게이 및 레즈비언과 같은 다른 타겟 용어로 구성된 비이진적 특성 대신, 남성과 여성이라는 이분형 타겟 용어를 가진 성별 영역을 고려하는 것과 같은 도메인에 관련된 타겟 용어의 하위 집합을 다루는 데에만 초점을 맞추고 있다. 이러한 모든 관찰은 NLP 시스템을 완전히 디바이어싱하기 위해 바이어스를 평가하는 향후 작업에서 다른 관점에서 바이어스의 이질적인 특성을 고려해야 함을 보여준다.

#### 7.0.2 평가 말뭉치

기존의 많은 편향 평가 말뭉치는 실세계의 문맥을 잘 표현하지 못하는 합성 문장과 짧은 길이의 문장 등과 같은 문장에서 현실주의의 희소성을 보여준다. NLP 모델의 기존 편향에 대한 정확한 이해와 정량화는 이러한 말뭉치, 특히 많은 다운스트림 응용 프로그램에서 벤치마크 결과를 생성하기 위해 복잡한 실제 문장을 생성할 수 있는 큰 PLM의 경우 효과적이지 않을 수 있다. 대부분의 대규모 실세계 평가 말뭉치는 성별 영역에 초점을 맞추고 있으며, 다른 도메인에 맞는 실세계 컨텍스트 기반 말뭉치는 부족하다. 정의적 편향의 정량화 및 완화의 경우, 많은 연구자들은 평가에 정의적 산출 라벨이 없는 [14, 32]에서 일반 평가 말뭉치를 사용한다. 키리첸코 등에 의해 개발된 말뭉치 [52]는 관련 감정 레이블을 포함하고 있지만 실제 컨텍스트가 부족하고 감정에 대한 명시적 언급에서 크게 파생된 레이블을 가지고 있다.

#### 7.0.3 Generalizable design

모든 시스템의 일반화 가능성으로 인해 다른 연합 시스템 내에서 직접 또는 간접적으로 사용할 수 있습니다. 편향 평가는 심각한 사회적 피해를 방지하여 공정한 결정을 내리기 위해 NLP 시스템에서 필수적인 부분이 되고 있다. 이러한 필요성은 크게 일반화 가능한 편향 식별 및 완화 모듈을 설계함으로써 달성될 수 있다. 평가 코포라의 맥락에서 일반화 가능성, 편향의 영향을 식별하고 정량화하기 위한 메트릭, 디바이어싱 후 성능 절충 조치는 모든 NLP 시스템이 적응하고 실험할 수 있는 공통적이고 간단한 구성 요소로 중요한 편향 평가 프로세스를 가져오기 위해 향후 해결해야 할 중요한 과제이다.

#### 5.2.2 Pre-train vs. 미세 조정 편향

사전 훈련된 모델을 활용하는 다운스트림 애플리케이션의 편향은 시스템 개발의 여러 단계에 걸쳐 발생한다. 초기에는 사전 훈련 알고리즘에 의해 증폭될 수 있는 모델을 사전 훈련하는 데 사용되는 방대한 양의 데이터일 수 있다. 바이어스를 도입하는 다른 방법은 미세 조정 알고리즘과 미세 조정 데이터를 사용하는 것이다. 이러한 배경에서 기존의 과제는 전체 시스템의 어떤 부분이 실제로 편향을 유발하는지, 즉 사전 훈련 또는 작업별 미세 조정 모듈에서 비롯되는지, 편향이 시스템 구성 요소의 조화로운 워크플로로 영속화되는 방법, 전체 시스템의 어떤 부분이 완화 과정을 거쳐야 하는지 등을 분석하고 이해하는 것이다.

#### 5.2.3 언어 다양성 및 NLP 편향

최근 NLP 연구는 단일 언어 및 이중 언어 전략을 처리하는 능력을 포함하여 다양한 언어로 대규모 PLM 및 다운스트림 응용 프로그램을 개발하는 유용성을 널리 탐구한다. 트랜스포머 기반 대형 PLM인 IndicBERT10은 말레이얄람과 같은 낮은 자원과 형태학적으로 고도로 편향된 인도 언어를 다루는 예이다. 유사하게, 상이한 언어들에서의 매우 다양한 단일 언어 및 이중 언어 말뭉치11(영어, 아프리카어, 아랍어 등) 및 기계 학습 모델12(질문 응답, 텍스트 분류 등을 포함하는 다양한 태스크에 대해)는 동일한 경향을 예시한다. 이러한 맥락에서 이러한 다양한 언어 말뭉치와 해당 말뭉치에서 학습한 단일 및 다국어 PLM에서 편향을 연구하는 것이 매우 중요하다. 이 방향으로의 아주 적은 시도들 중에는 힌디어 단어 임베딩[85], 힌디어-영어 기계 번역에서 젠더 편향 평가[91], 스페인어 및 히브리어와 같은 언어에서 젠더 편향 해결[132] 등의 젠더 편향을 완화하기 위한 연구가 있다.

각주 10: [https://huggingface.co/ai4bharat/indic-bert](https://huggingface.co/ai4bharat/indic-bert) 1월 03 2022년에 액세스

각주 11: [https://huggingface.co/datasets](https://huggingface.co/datasets) 1월 03 2022년에 액세스

각주 12: [https://huggingface.co/models](https://huggingface.co/models), 1월 03 2022년에 액세스

#### 5.2.4 NLP bias를 분석하는 학제간 방법

Sun 등[106]은 NLP에서 바이어스를 식별하고 완화하기 위해 비 NLP 시스템에 배치된 많은 기술들이 직접 또는 작은 수정으로 적용될 수 있다고 진술한다. 여러 연구에 따르면 NLP 시스템의 편향은 사회학, 심리학, 사회 언어학, 공학, 법률학 등과 같은 다른 분과와 연관될 수 있는 범위를 열어준다. 심리학에서 암묵적 연관 테스트[42]와 표현 편향을 정량화하기 위한 워드 임베딩 연관 테스트[22] 및 문장 인코더 연관 테스트[70]와 같은 다양한 계산 표상은 NLP에서 편향 제거의 학제 간 특성을 보여주는 예이다. 또한, 유닛 테스팅, 거동 테스팅 등과 같은 소프트웨어 공학13으로부터의 교훈은 상이한 레벨에서 기계 학습 모델의 편향을 평가하고 정량화하기 위해 채택될 수 있다[93]. Sun 등[106]은 NLP 편향의 완화를 사회학과 공학의 결합된 문제로 취급하는데, 여기서 사회학은 인간이 실제로 어떻게 사회적 편향을 인식하고 언어로 인코딩하는지 식별할 수 있다. 학제 간 논의를 향한 연구는 현재의 편향 정량화 및 완화 전략에 빛을 불어넣고 선진적이고 실질적으로 관련된 접근법 개발을 고무할 수 있다[7, 12, 99].

## 8 Conclusion

이 조사에서는 사전 훈련된 대규모 언어 모델, 특히 변압기 기반 모델의 편향에 대한 포괄적인 조사를 수행했다. 사전 훈련된 언어 모델 워크플로우의 다양한 단계에서 발생하는 다양한 유형의 편향과 이러한 편향을 정량화하고 완화하는 데 사용되는 방법에 대해 논의한다. 실제 환경에서 정의적 컴퓨팅 시스템의 광범위한 유용성으로 인해 우리는 정서, 즉 정의적 편향과 관련된 덜 탐구된 편향 영역에 특히 중점을 둔다. 본 연구는 또한 대규모 사전 훈련 언어 모델에서의 적합성과 함께 향후 연구에 도움이 되는 대중적이고 공개적으로 이용 가능한 평가 코퍼스를 나열한다. 마지막으로, 우리는 사전 훈련된 언어 모드에서 편향을 식별하고 완화하는 작업을 커뮤니티가 더 개선하는 데 도움이 되는 이 연구 영역의 문제에 대해 논의한다. 이 설문 조사에 관한 자료는 [https://github.com/anoopkdcs/NLP](https://github.com/anoopkdcs/NLP) 바이어스에서 공개적으로 사용할 수 있으며 향후 연구에 도움이 되도록 계속 업데이트하겠습니다.

## References

* [1] Abid, A., Farooqi, M., Zou, J.: Large language models associate muslims with violence. Nature Machine Intelligence **3**(6), 461-463 (2021). [https://doi.org/https://doi.org/10.1038/s42256-021-00359-2](https://doi.org/https://doi.org/10.1038/s42256-021-00359-2)
* [2] Abid, A., Farooqi, M., Zou, J.: Persistent Anti-Muslim Bias in Large Language Models, p. 298-306. Association for Computing Machinery, New York, NY, USA (2021), [https://doi.org/10.1145/3461702.3462624](https://doi.org/10.1145/3461702.3462624)
* [3] Ahn, J., Oh, A.: Mitigating language-dependent ethnic bias in BERT. In: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. pp. 533-549. Association for Computational Linguistics (Nov 2021). [https://doi.org/10.18653/v1/2021.emnlp-main.42](https://doi.org/10.18653/v1/2021.emnlp-main.42)
* [4] Anoop, K.: Affect-oriented fake news detection using machine learning. In: AWSAR Awarded Popular Science Stories By Scientists for the People, pp. 426-428. Vigyan Prasar, DST, India, ISBN: 978-81-7480-337-5 (2019), [https://www.researchgate.net/publication/344838679_Affect-Oriented_Fake_News_Detection_Using_Machine_Learning](https://www.researchgate.net/publication/344838679_Affect-Oriented_Fake_News_Detection_Using_Machine_Learning)
* [5] Anoop, K., Deepak, P., Lajish, V.L.: Emotion cognizance improves health fake news identification. In: Proceedings of the 24th Symposium on International Database Engineering & Applications. IDEAS '20, Association for Computing Machinery, Seoul, Republic of Korea (2020). [https://doi.org/10.1145/3410566.3410595](https://doi.org/10.1145/3410566.3410595)
* [6] Asyrofi, M.H., Yang, Z., Yusuf, I.N.B., Kang, H.J., Thung, F., Lo, D.: Biasfinder: Metamorphic test generation to uncover bias for sentiment analysis systems. IEEE Transactions on Software Engineering (2021). [https://doi.org/10.1109/TSE.2021.3136169](https://doi.org/10.1109/TSE.2021.3136169)* [7] Avin, C., Keller, B., Lotker, Z., Mathieu, C., Peleg, D., Pignolet, Y.A.: Homophily and the glass ceiling effect in social networks. In: Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science. p. 41-50. ITCS '15, Association for Computing Machinery, New York, NY, USA (2015). [https://doi.org/10.1145/2688073.2688097](https://doi.org/10.1145/2688073.2688097)
* [8] Back, S.E., Payne, R.L., Simpson, A.N., Brady, K.T.: Gender and prescription opioids: Findings from the national survey on drug use and health. Addictive Behaviors **35**(11), 1001-1007 (2010). [https://doi.org/https://doi.org/10.1016/j.addbeh.2010.06.018](https://doi.org/https://doi.org/10.1016/j.addbeh.2010.06.018)
* [9] Bartl, M., Nissim, M., Gatt, A.: Unmasking contextual stereotypes: Measuring and mitigating BERT's gender bias. In: Proceedings of the Second Workshop on Gender Bias in Natural Language Processing. pp. 1-16. Association for Computational Linguistics (Dec 2020), [https://aclanthology.org/2020.gebnlp-1.1](https://aclanthology.org/2020.gebnlp-1.1)
* [10] Basta, C., Costa-jussa, M.R., Casas, N.: Evaluating the underlying gender bias in contextualized word embeddings. In: Proceedings of the First Workshop on Gender Bias in Natural Language Processing. pp. 33-39. Association for Computational Linguistics, Italy (Aug 2019). [https://doi.org/10.18653/v1/W19-3805](https://doi.org/10.18653/v1/W19-3805)
* [11] Basta, C., Costa-jussa, M.R., Casas, N.: Extensive study on the underlying gender bias in contextualized word embeddings. Neural Computing and Applications **33**(8), 3371-3384 (2021), [https://doi.org/10.1007/s00521-020-05211-z](https://doi.org/10.1007/s00521-020-05211-z)
* [12] Beukeboom, C.J., Burgers, C.: How stereotypes are shared through language: a review and introduction of the aocial categories and stereotypes communication (scsc) framework. Review of Communication Research **7**, 1-37 (2019). [https://doi.org/10.12840/issn.2255-4165.017](https://doi.org/10.12840/issn.2255-4165.017)
* [13] Bhardwaj, R., Majumder, N., Poria, S.: Investigating gender bias in bert. Cognitive Computation pp. 1-11 (2021). [https://doi.org/https://doi.org/10.1007/s12559-021-09881-2](https://doi.org/https://doi.org/10.1007/s12559-021-09881-2)
* [14] Bhaskaran, J., Bhallamudi, I.: Good secretaries, bad truck drivers? occupational gender stereotypes in sentiment analysis. In: Proceedings of the First Workshop on Gender Bias in Natural Language Processing. pp. 62-68. Association for Computational Linguistics, Italy (2019). [https://doi.org/10.18653/v1/W19-3809](https://doi.org/10.18653/v1/W19-3809)
* [15] Blodgett, S.L., Green, L., O'Connor, B.: Demographic dialectal variation in social media: A case study of African-American English. In: Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pp. 1119-1130. Association for Computational Linguistics, Austin, Texas (Nov 2016). [https://doi.org/10.18653/v1/D16-1120](https://doi.org/10.18653/v1/D16-1120)
* [16] Bolukbasi, T., Chang, K.W., Zou, J., Saligrama, V., Kalai, A.: Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In: Proceedings of the 30th International Conference on Neural Information Processing Systems. p. 4356-4364. NIPS'16, Curran Associates Inc. (2016)
* [17] Bolukbasi, T., Chang, K.W., Zou, J., Saligrama, V., Kalai, A.: Quantifying and reducing stereotypes in word embeddings. arXiv:1606.06121 (2016)
* [18] Bordia, S., Bowman, S.R.: Identifying and reducing gender bias in word-level language models. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop. pp. 7-15. Association for Computational Linguistics, Minneapolis, Minnesota (Jun 2019). [https://doi.org/10.18653/v1/N19-3002](https://doi.org/10.18653/v1/N19-3002)
* [19] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter,C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot learners. In: Advances in Neural Information Processing Systems. vol. 33, pp. 1877-1901. Curran Associates, Inc. (2020), [https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)
* [20] Buolamwini, J., Gebru, T.: Gender shades: Intersectional accuracy disparities in commercial gender classification. In: Proceedings of the 1st Conference on Fairness, Accountability and Transparency. Proceedings of Machine Learning Research, vol. 81, pp. 77-91. PMLR (23-24 Feb 2018), [https://proceedings.mlr.press/v81/buolamwini18a.html](https://proceedings.mlr.press/v81/buolamwini18a.html)
* [21] Caetano, J.A., Lima, H.S., Santos, M.F., Marques-Neto, H.T.: Using sentiment analysis to define twitter political users' classes and their homophily during the 2016 american presidential election. Journal of internet services and applications **9**(1), 1-15 (2018). [https://doi.org/https://doi.org/10.1186/s13174-018-0089-0](https://doi.org/https://doi.org/10.1186/s13174-018-0089-0)
* [22] Caliskan, A., Bryson, J.J., Narayanan, A.: Semantics derived automatically from language corpora contain human-like biases. Science **356**(6334), 183-186 (2017). [https://doi.org/10.1126/science.aal4230](https://doi.org/10.1126/science.aal4230)
* [23] Cambria, E., Poria, S., Gelbukh, A., Thelwall, M.: Sentiment analysis is a big suitcase. IEEE Intelligent Systems **32**(6), 74-80 (2017). [https://doi.org/10.1109/MIS.2017.4531228](https://doi.org/10.1109/MIS.2017.4531228)
* [24] Chakraborty, J., Majumder, S., Yu, Z., Menzies, T.: Fairway: A way to build fair ml software. In: Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. p. 654-665. Association for Computing Machinery, New York, NY, USA (2020), [https://doi.org/10.1145/3368089.3409697](https://doi.org/10.1145/3368089.3409697)
* [25] Chaloner, K., Maldonado, A.: Measuring gender bias in word embeddings across domains and discovering new gender bias word categories. In: Proceedings of the First Workshop on Gender Bias in Natural Language Processing. pp. 25-32. Association for Computational Linguistics, Italy (Aug 2019). [https://doi.org/10.18653/v1/W19-3804](https://doi.org/10.18653/v1/W19-3804)
* [26] Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P.: One billion word benchmark for measuring progress in statistical language modeling. Computing Research Repository (CoRR) pp. 1-6 (2013)
* [27] Chouldechova, A.: Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big Data **5**(2), 153-163 (2017), [https://doi.org/10.1089/big.2016.0047](https://doi.org/10.1089/big.2016.0047), pMID: 28632438
* [28] Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., Huq, A.: Algorithmic decision making and the cost of fairness. In: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. p. 797-806. KDD '17, Association for Computing Machinery, New York, NY, USA (2017), [https://doi.org/10.1145/3097983.3098095](https://doi.org/10.1145/3097983.3098095)
* [29] Craft, J.T., Wright, K.E., Weissler, R.E., Queen, R.M.: Language and discrimination: Generating meaning, perceiving identities, and discriminating outcomes. Annual Review of Linguistics **6**(1), 389-407 (2020), [https://doi.org/10.1146/annurev-linguistics-011718-011659](https://doi.org/10.1146/annurev-linguistics-011718-011659)
* [30] Dev, S., Li, T., Phillips, J.M., Srikumar, V.: On measuring and mitigating biased inferences of word embeddings. Proceedings of the AAAI Conference on Artificial Intelligence **34**(05), 7659-7666 (Apr 2020), [https://ojs.aaai.org/index.php/AAAI/article/view/6267](https://ojs.aaai.org/index.php/AAAI/article/view/6267)* [31] Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidirectional transformers for language understanding. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). pp. 4171-4186. Association for Computational Linguistics, Minneapolis, Minnesota (Jun 2019). [https://doi.org/10.18653/vl/N19-1423](https://doi.org/10.18653/vl/N19-1423)
* [32] Diaz, M., Johnson, I., Lazar, A., Piper, A.M., Gergle, D.: Addressing age-related bias in sentiment analysis. In: Proceedings of the 2018 chi conference on human factors in computing systems. pp. 1-14. Association for Computing Machinery, New York, NY, USA (2018), [https://doi.org/10.1145/3173574.3173986](https://doi.org/10.1145/3173574.3173986)
* [33] Dixon, L., Li, J., Sorensen, J., Thain, N., Vasserman, L.: Measuring and mitigating unintended bias in text classification. In: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society. p. 67-73. AIES '18, Association for Computing Machinery, New York, NY, USA (2018). [https://doi.org/10.1145/3278721.3278729](https://doi.org/10.1145/3278721.3278729)
* [34] Dolianiti, F.S., Iakovakis, D., Dias, S.B., Hadjileontiadou, S., Diniz, J.A., Hadjileontiadis, L.: Sentiment analysis techniques and applications in education: A survey. In: International Conference on Technology and Innovation in Learning, Teaching and Education. pp. 412-427. Springer (2018). [https://doi.org/https://doi.org/10.1007/978-3-030-20954-4_31](https://doi.org/https://doi.org/10.1007/978-3-030-20954-4_31)
* [35] Du, M., Yang, F., Zou, N., Hu, X.: Fairness in deep learning: A computational perspective. IEEE Intelligent Systems **36**(4), 25-34 (2021). [https://doi.org/10.1109/MIS.2020.3000681](https://doi.org/10.1109/MIS.2020.3000681)
* [36] Eagly, A., Wood, W., Diekman, A.: Social role theory of sex differences and similarities: A current appraisal, pp. 123-174. Lawrence Erlbaum Associates Publishers (2000)
* [37] Elmadany, A., Zhang, C., Abdul-Mageed, M., Hashemi, A.: Leveraging affective bidirectional transformers for offensive language detection. In: Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection. pp. 102-108. European Language Resource Association, France (2020), [https://aclanthology.org/2020.osact-1.17](https://aclanthology.org/2020.osact-1.17)
* [38] Escude Font, J., Costa-jussa, M.R.: Equalizing gender bias in neural machine translation with word embeddings techniques. In: Proceedings of the First Workshop on Gender Bias in Natural Language Processing. pp. 147-154. Association for Computational Linguistics, Italy (Aug 2019). [https://doi.org/10.18653/v1/W19-3821](https://doi.org/10.18653/v1/W19-3821)
* [39] Fatemi, Z., Xing, C., Liu, W., Xiong, C.: Improving gender fairness of pre-trained language models without catastrophic forgetting. arXiv:2110.05367 (2021)
* [40] Garg, N., Schiebinger, L., Jurafsky, D., Zou, J.: Word embeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences **115**(16), E3635-E3644 (2018)
* [41] Gonen, H., Goldberg, Y.: Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). pp. 609-614. Association for Computational Linguistics, Minneapolis, Minnesota (Jun 2019). [https://doi.org/10.18653/vl/N19-1061](https://doi.org/10.18653/vl/N19-1061)
* [42] Greenwald, A.G., McGhee, D.E., Schwartz, J.L.: Measuring individual differences in implicit cognition: the implicit association test. Journal of personality and social psychology **74**(6), 1464 (1998)* [43] Guo, W., Caliskan, A.: Detecting emergent intersectional biases: Contextualized word embeddings contain a distribution of human-like biases. In: Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. p. 122-133. Association for Computing Machinery, New York, NY, USA (2021), [https://doi.org/10.1145/3461702.3462536](https://doi.org/10.1145/3461702.3462536)
* [44] Gupta, V.S., Kohli, S.: Twitter sentiment analysis in healthcare using hadoop and r. In: 2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom). pp. 3766-3772. IEEE (2016), [https://ieeexplore.ieee.org/document/7724965](https://ieeexplore.ieee.org/document/7724965)
* [45] Hall Maudslay, R., Gonen, H., Cotterell, R., Teufel, S.: It's all in the name: Mitigating gender bias with name-based counterfactual data substitution. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5267-5275. Association for Computational Linguistics, Hong Kong, China (Nov 2019). [https://doi.org/10.18653/v1/D19-1530](https://doi.org/10.18653/v1/D19-1530)
* [46] Hermann, K.M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., Blunsom, P.: Teaching machines to read and comprehend. In: Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 1. pp. 1693-1701 (2015)
* [47] Huang, P.S., Zhang, H., Jiang, R., Stanforth, R., Welbl, J., Rae, J., Maini, V., Yogatama, D., Kohli, P.: Reducing sentiment bias in language models via counterfactual evaluation. In: Findings of the Association for Computational Linguistics: EMNLP 2020. pp. 65-83. Association for Computational Linguistics, Online (Nov 2020). [https://doi.org/10.18653/v1/2020.findings-emnlp.7](https://doi.org/10.18653/v1/2020.findings-emnlp.7)
* [48] Jin, X., Barbieri, F., Kennedy, B., Davani, A.M., Neves, L., Ren, X.: On transferability of bias mitigation effects in language model fine-tuning. In: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 3770-3783. Association for Computational Linguistics (Jun 2021). [https://doi.org/10.18653/v1/2021.naacl-main.296](https://doi.org/10.18653/v1/2021.naacl-main.296)
* [49] Joulin, A., Grave, E., Bojanowski, P., Douze, M., Jegou, H., Mikolov, T.: Fasttext. zip: Compressing text classification models. arXiv:1612.03651 (2016)
* [50] Kalyan, K.S., Rajasekharan, A., Sangeetha, S.: Ammus: A survey of transformer-based pretrained models in natural language processing. arXiv:2108.05542 (2021)
* [51] Kaneko, M., Bollegala, D.: Unmasking the mask-evaluating social biases in masked language models. arXiv:2104.07496 (2021)
* [52] Kiritchenko, S., Mohammad, S.: Examining gender and race bias in two hundred sentiment analysis systems. In: Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics. pp. 43-53. Association for Computational Linguistics, New Orleans, Louisiana (Jun 2018). [https://doi.org/10.18653/v1/S18-2005](https://doi.org/10.18653/v1/S18-2005)
* [53] Kirk, H.R., Volpin, F., Iqbal, H., Benussi, E., Dreyer, F., Shtedritski, A., Asano, Y., et al.: Bias out-of-the-box: An empirical analysis of intersectional occupational biases in popular generative language models. Advances in Neural Information Processing Systems **34** (2021)
* [54] Krishnamoorthy, S.: Sentiment analysis of financial news articles using performance indicators. Knowledge and Information Systems **56**(2), 373-394 (2018). [https://doi.org/https://doi.org/10.1007/s10115-017-1134-1](https://doi.org/https://doi.org/10.1007/s10115-017-1134-1)
* [55] Kurita, K., Vyas, N., Pareek, A., Black, A.W., Tsvetkov, Y.: Measuring bias in contextualized word representations. In: Proceedings of the First Workshopon Gender Bias in Natural Language Processing. pp. 166-172. Association for Computational Linguistics, Italy (Aug 2019). [https://doi.org/10.18653/v1/W19-3823](https://doi.org/10.18653/v1/W19-3823)
* [56] Lambrecht, A., Tucker, C.: Algorithmic bias? an empirical study of apparent gender-based discrimination in the display of stem career ads. Management Science **65**(7), 2966-2981 (2019). [https://doi.org/10.1287/mnsc.2018.3093](https://doi.org/10.1287/mnsc.2018.3093)
* [57] Lambrecht, A., Tucker, C.: Algorithmic bias? an empirical study of apparent gender-based discrimination in the display of stem career ads. Management Science **65**(7), 2966-2981 (2019). [https://doi.org/10.1287/mnsc.2018.3093](https://doi.org/10.1287/mnsc.2018.3093)
* [58] Lapowsky, I.: Google autocomplete still makes vile suggestions (2018)
* 볼륨 32. p. II-1188-II-1196. ICML'14, JMLR.org (2014), [https://dl.acm.org/doi/10.5555/3044805.3045025](https://dl.acm.org/doi/10.5555/3044805.3045025)
* [60] Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C.H., Kang, J.: BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics **36**(4), 1234-1240 (09 2019). [https://doi.org/10.1093/bioinformatics/btz682](https://doi.org/10.1093/bioinformatics/btz682)
* [61] Leino, K., Fredrikson, M., Black, E., Sen, S., Datta, A.: Feature-wise bias amplification. In: International Conference on Learning Representations (2019), [https://openreview.net/forum?id=S1ecm2C9K7](https://openreview.net/forum?id=S1ecm2C9K7)
* [62] Li, B., Peng, H., Sainju, R., Yang, J., Yang, L., Liang, Y., Jiang, W., Wang, B., Liu, H., Ding, C.: Detecting gender bias in transformer-based models: A case study on bert. arXiv:2110.15733 (2021)
* [63] Liang, P.P., Wu, C., Morency, L.P., Salakhutdinov, R.: Towards understanding and mitigating social biases in language models. In: International Conference on Machine Learning. pp. 6565-6576. PMLR (2021), [http://proceedings.mlr.press/v139/liang21a.html](http://proceedings.mlr.press/v139/liang21a.html)
* [64] Liu, H., Dacon, J., Fan, W., Liu, H., Liu, Z., Tang, J.: Does gender matter? towards fairness in dialogue systems. In: Proceedings of the 28th International Conference on Computational Linguistics. pp. 4403-4416. International Committee on Computational Linguistics, Barcelona, Spain (Online) (Dec 2020). [https://doi.org/10.18653/v1/2020.coling-main.390](https://doi.org/10.18653/v1/2020.coling-main.390)
* [65] Liu, R., Jia, C., Wei, J., Xu, G., Wang, L., Vosoughi, S.: Mitigating political bias in language models through reinforced calibration. Proceedings of the AAAI Conference on Artificial Intelligence **35**(17), 14857-14866 (May 2021), [https://ojs.aaai.org/index.php/AAAI/article/view/17744](https://ojs.aaai.org/index.php/AAAI/article/view/17744)
* [66] Lu, K., Mardziel, P., Wu, F., Amancharla, P., Datta, A.: Gender bias in neural natural language processing. In: Logic, Language, and Security, pp. 189-202. Springer (2020)
* [67] Magee, L., Ghahremanlou, L., Soldatic, K., Robertson, S.: Intersectional bias in causal language models. arXiv:2107.07691 (2021)
* [68] Manzini, T., Yao Chong, L., Black, A.W., Tsvetkov, Y.: Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). pp. 615-621. Association for Computational Linguistics, Minneapolis, Minnesota (Jun 2019). [https://doi.org/10.18653/v1/N19-1062](https://doi.org/10.18653/v1/N19-1062)* [69] Marcus, M.P., Santorini, B., Marcinkiewicz, M.A.: Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics **19**(2), 313-330 (1993), [https://aclanthology.org/J93-2004](https://aclanthology.org/J93-2004)
* [70] May, C., Wang, A., Bordia, S., Bowman, S.R., Rudinger, R.: On measuring social biases in sentence encoders. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). pp. 622-628. Association for Computational Linguistics, Minneapolis, Minnesota (Jun 2019). [https://doi.org/10.18653/v1/N19-1063](https://doi.org/10.18653/v1/N19-1063)
* [71] McFadden, A.C., Marsh, G.E., Price, B.J., Hwang, Y.: A study of race and gender bias in the punishment of school children. Education and treatment of children pp. 140-146 (1992)
* [72] Menegatti, M., Rubini, M.: Gender bias and sexism in language (09 2017). [https://doi.org/10.1093/acrefore/9780190228613.013.470](https://doi.org/10.1093/acrefore/9780190228613.013.470)
* [73] Merity, S., Xiong, C., Bradbury, J., Socher, R.: Pointer sentinel mixture models. arXiv:1609.07843 (2016)
* [74] Mikolov, T., Chen, K., Corrado, G., Dean, J.: Efficient estimation of word representations in vector space. In: 1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings (2013)
* [75] Motro, D., Evans, J., Ellis, A.P., Benson, L.: Race and reactions to negative feedback: Examining the effects of the "angry black woman" stereotype. Academy of Management Proceedings **2019**(1), 11230 (2019). [https://doi.org/10.5465/AMBPP.2019.11230abstract](https://doi.org/10.5465/AMBPP.2019.11230abstract)
* [76] Nadeem, M., Bethke, A., Reddy, S.: StereoSet: Measuring stereotypical bias in pretrained language models. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). pp. 5356-5371. Association for Computational Linguistics, Online (Aug 2021). [https://doi.org/10.18653/v1/2021.acl-long.416](https://doi.org/10.18653/v1/2021.acl-long.416)
* [77] Ng, S.H.: Language-based discrimination: Blatant and subtle forms. Journal of Language and Social Psychology **26**(2), 106-122 (2007). [https://doi.org/10.1177/0261927X0730074](https://doi.org/10.1177/0261927X0730074)
* 규제 기관에 대한 도전 (2020년 5월) [https://www.forbes.com/sites/carmenniethammer/2020/03/02/ai-bias-could-put-women-lives-at-riska-challenge-for-regulators/?sh=753a6217534f](https://www.forbes.com/sites/carmenniethammer/2020/03/02/ai-bias-could-put-women-lives-at-riska-challenge-for-regulators/?sh=753a6217534f)
* [79] Packer, B., Mitchell, M., Guajardo-Cespedes, M., Halpern, Y.: Text embeddings contain bias. here's why that matters. Tech. rep., Google (2018)
* Volume 10. p. 79-86. EMNLP '02, Association for Computational Linguistics, USA (2002). [https://doi.org/10.3115/1118693.1118704] (https://doi.org/10.3115/1118693.1118704)
* [81] Park, J.H., Shin, J., Fung, P.: Reducing gender bias in abusive language detection. In: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 2799-2804. Association for Computational Linguistics, Brussels, Belgium (2018). [https://doi.org/10.18653/v1/D18-1302](https://doi.org/10.18653/v1/D18-1302)
* [82] Pennington, J., Socher, R., Manning, C.: GloVe: Global vectors for word representation. In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. pp. 111-126.

언어 처리(EMNLP) pp. 1532-1543. Association for Computational Linguistics, Doha, Qatar (Oct 2014). [https://doi.org/10.3115/v1/D14-1162] (https://doi.org/10.3115/v1/D14-1162)
* [83] Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettlemoyer, L.: Deep contextualized word representations. In: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). pp. 2227-2237. Association for Computational Linguistics, New Orleans, Louisiana (Jun 2018). [https://doi.org/10.18653/v1/N18-1202](https://doi.org/10.18653/v1/N18-1202)
* [84] Poria, S., Hazarika, D., Majumder, N., Mihalcea, R.: Beneath the tip of the iceberg: Current challenges and new directions in sentiment analysis research. IEEE Transactions on Affective Computing (2020). [https://doi.org/10.1109/TAFFC.2020.3038167](https://doi.org/10.1109/TAFFC.2020.3038167)
* [85] Pujari, A.K., Mittal, A., Padhi, A., Jain, A., Jadon, M., Kumar, V.: Debiasing gender biased hindi words with word-embedding. In: Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence. pp. 450-456. Association for Computing Machinery, New York, NY, USA (2019). [https://doi.org/10.1145/3377713.337792](https://doi.org/10.1145/3377713.337792)
* [86] Qian, Y., Muaz, U., Zhang, B., Hyun, J.W.: Reducing gender bias in word-level language models with a gender-equalizing loss function. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop. pp. 223-228. Association for Computational Linguistics, Italy (Jul 2019). [https://doi.org/10.18653/v1/P19-2031](https://doi.org/10.18653/v1/P19-2031)
* [87] Qiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., Huang, X.: Pre-trained models for natural language processing: A survey. Science China Technological Sciences pp. 1-26 (2020). [https://doi.org/https://doi.org/10.1007/s11431-020-1647-3](https://doi.org/https://doi.org/10.1007/s11431-020-1647-3)
* [88] Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language understanding by generative pre-training. OpenAI blog (2018)
* [89] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners. OpenAI blog **1**(8), 9 (2019)
* [90] Rajkomar, A., Hardt, M., Howell, M.D., Corrado, G., Chin, M.H.: Ensuring fairness in machine learning to advance health equity. Annals of Internal Medicine **169**(12), 866-872 (2018). [https://doi.org/10.7326/M18-1990](https://doi.org/10.7326/M18-1990), pMID: 30508424
* [91] Ramesh, K., Gupta, G., Singh, S.: Evaluating gender bias in hindi-english machine translation. In: Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing. pp. 16-23. Association for Computational Linguistics (Aug 2021). [https://doi.org/10.18653/v1/2021.gebnlp-1.3](https://doi.org/10.18653/v1/2021.gebnlp-1.3)
* [92] Renault, T.: Sentiment analysis and machine learning in finance: a comparison of methods and models on one million messages. Digital Finance **2**(1), 1-13 (2020). [https://doi.org/https://doi.org/10.1007/s42521-019-00014-x](https://doi.org/https://doi.org/10.1007/s42521-019-00014-x)
* [93] Ribeiro, M.T., Wu, T., Guestrin, C., Singh, S.: Beyond accuracy: Behavioral testing of NLP models with CheckList. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4902-4912. Association for Computational Linguistics, Online (Jul 2020). [https://doi.org/10.18653/v1/2020.acl-main.442](https://doi.org/10.18653/v1/2020.acl-main.442)
* [94] Rozado, D.: Wide range screening of algorithmic bias in word embedding models using large sentiment lexicons reveals underreported bias types. PloS one **15**(4), 1-26 (04 2020). [https://doi.org/10.1371/journal.pone.0231189](https://doi.org/10.1371/journal.pone.0231189)
* [95] Rudinger, R., Naradowsky, J., Leonard, B., Van Durme, B.: Gender bias in coreference resolution. In: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies, Volume 2 (Short Papers). pp. 8-14. Association for Computational Linguistics, New Orleans, Louisiana (Jun 2018). [https://doi.org/10.18653/v1/N18-2002](https://doi.org/10.18653/v1/N18-2002)
* [96] Sambasivan, N., Arnesen, E., Hutchinson, B., Doshi, T., Prabhakaran, V.: Reimagining algorithmic fairness in india and beyond. In: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. p. 315-328. FAccT '21, Association for Computing Machinery, New York, NY, USA (2021). [https://doi.org/10.1145/3442188.3445896](https://doi.org/10.1145/3442188.3445896)
* [97] Saunders, D., Byrne, B.: Reducing gender bias in neural machine translation as a domain adaptation problem. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7724-7736. Association for Computational Linguistics, Online (Jul 2020). [https://doi.org/10.18653/v1/2020.aclmain.690](https://doi.org/10.18653/v1/2020.aclmain.690)
* [98] Schick, T., Udupa, S., Schutze, H.: Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in NLP. Transactions of the Association for Computational Linguistics **9**, 1408-1424 (2021). [https://doi.org/10.1162/tacl_a_00434](https://doi.org/10.1162/tacl_a_00434)
* [99] Schluter, N.: The glass ceiling in nlp. In: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 2793-2798. Association for Computational Linguistics, Brussels, Belgium (Oct-Nov 2018). [https://doi.org/10.18653/v1/D18-1301](https://doi.org/10.18653/v1/D18-1301)
* [100] Shen, J.H., Fratamico, L., Rahwan, I., Rush, A.M.: Darling or babygirl? investigating stylistic bias in sentiment analysis. Proc. of FATML (2018)
* [101] Sheng, E., Chang, K.W., Natarajan, P., Peng, N.: The woman worked as a babysitter: On biases in language generation. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3407-3412. Association for Computational Linguistics, Hong Kong, China (Nov 2019). [https://doi.org/10.18653/v1/D19-1339](https://doi.org/10.18653/v1/D19-1339)
* [102] Silva, A., Tambwekar, P., Gombolay, M.: Towards a comprehensive understanding and accurate evaluation of societal biases in pre-trained transformers. In: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2383-2389. Association for Computational Linguistics (Jun 2021). [https://doi.org/10.18653/v1/2021.naacl-main.189](https://doi.org/10.18653/v1/2021.naacl-main.189)
* [103] Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.D., Ng, A.Y., Potts, C.: Recursive deep models for semantic compositionality over a sentiment treebank. In: Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1631-1642. Association for Computational Linguistics, Seattle, Washington, USA (Oct 2013), [https://aclanthology.org/D13-1170](https://aclanthology.org/D13-1170)
* [104] Stanczak, K., Choudhury, S.R., Pimentel, T., Cotterell, R., Augenstein, I.: Quantifying gender bias towards politicians in cross-lingual language models. arXiv:2104.07505 (2021)
* [105] Suharshala, R., Anoop, K., Lajish, V.L.: Cross-domain sentiment analysis on social media interactions using senti-lexicon based hybrid features. In: 2018 3rd International Conference on Inventive Computation Technologies (ICICT). pp. 772-777. IEEE, Coimbatore, India (2018). [https://doi.org/10.1109/ICICT43934.2018.9034272](https://doi.org/10.1109/ICICT43934.2018.9034272)
* [106] Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., Mirza, D., Belding, E., Chang, K.W., Wang, W.Y.: Mitigating gender bias in natural language processing: Literature review. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Online (Oct 2018). [https://doi.org/10.1109/ICICT43934.2018.9034272](https://doi.org/10.1109/ICICT43934.2018.9034272)
* [107] Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., Mirza, D., Belding, E., Chang, K.W., Wang, W.Y.: Mitigating gender bias in natural language processing: Literature review. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Online (Oct 2018). [https://doi.org/10.1109/ICICT43934.2018.9034272](https://doi.org/10.1109/ICICT43934.2018.9034272)
* [108] Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., Mirza, D., Belding, E., Chang, K.W., Wang, W.Y.: Mitigating gender bias in natural language processing: Literature review. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Online (Oct 2018). [https://doi.org/10.1109/ICICT43934.2018.9034272](https://doi.org/10.1109/ICICT43934.2018.9034272)
* [109] Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., Mirza, D., Belding, E., Chang, K.W., Wang, W.Y.: Mitigating gender bias in natural language processing: Literature review. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Online (Oct 2018). [https://doi.org/10.1109/ICICT43934.2018.9034272](https://doi.org/10.1109/ICICT43934.2018.9034272)
* [110] Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., Mirza, D., Belding, E., Chang, K.W., Wang, W.Y.: Mitigating gender bias in natural language processing: Literature review. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Online (Oct 2018). [https://doi.org/10.1109/ICICT43934.2018.9034272](https://doi.org/10.1109/ICICT43934.2018.9034272)
* [111] Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., Mirza, D., Belding, E., Chang, K.W., Wang, W.Y.: Mitigating gender bias in natural language processing: Literature review. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Online (Oct 2018). [https://doi.org/10.1109/ICICT43934.2018.9034272](https://doi.org/10.1109/ICICT43934.2018.9034272)
* [112] Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., Mirza, D., Belding, E., Chang, K.W., Wang, W.Y.: Mitigating gender bias in natural language processing: Literature review. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Online (Oct 2018). [https://doi.org/10.1109/ICICT43934.2018.9034272](https://doi.org/10.1109/ICICT43934.2018.9034272)
* [113] Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., Mirza, D., Belding, E., Chang, K.W., Wang, W.Y.: Mitigating gender bias in natural language processing: Literature review. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Online (Oct 2018). [https://doi.org/10.1109/ICICT43934.2018.9034272](https://doi.org/10.1109/ICICT43934.2018.9034272)
* [114] Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., Mirza, D., Belding, E., Chang, K.W., Wang, W.Y.: Mitigating gender bias in natural language processing: Literature review. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Online (Oct 2018). [https://doi.org/10.1109/ICICT43934.2018.9034272](https://doi.org/10.1109/ICICT43934.2018.9034272)
* [115] Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., Mirza, D., Belding, E., Chang, K.W., Wang, W.Y.: Mitigating gender bias in natural language processing: Literature review. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Online (Oct 2018). [https://doi.org/10.1109/ICICT43934.2018.9034272](https://doi.org/10.1109/ICICT43934.2018.9034272)
* [116] Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., Mirza, D., Belding, E., Chang, K.W., Wang, W.Y.: Mitigating gender bias in natural language processing: Literature review. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Online (Oct 2018). [https://doi.org/10.1109/ICICT43934.2018.9034272](https://doi.org/10.1109/ICICT43934.2018.9034272)
* [17] Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., Mirza, D., Belding, E., Chang, K.W., Wang, W.Y.: Mitigating gender bias in natural language processing: Literature review. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Online (Oct 2018). [https://doi.org/10.1109/ICICT43934.2018.9034272](https://doi.org/10.1109/ICICT43934.2018.9034272)
* [117] Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., Mirza, D., Belding, E., Chang, K.W., Wang, W.Y.: Mitigating gender bias in natural language processing: Literature review. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Online (Oct 2018). [https://doi.org/10.1109/ICICT43934.2018.9034272](https://doi.org/10.1109/ICICT43934.2018.9034272)
* [118] Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., Mirza, D., Belding, E., Chang, K.W., Wang, W.Y.: Mitigating gender bias in natural language processing: Literature review. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Online (Oct 2018). [https://doi.org/10.1109/ICICT43934.2018.9034272](https://doi.org/10.1109/ICICT43934.2018.9034272)
* [119] Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., Mirza, D., Belding, E., Chang, K.W., Wang, W.Y.: Mitigating gender bias in natural language processing: Literature review. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Online (Oct 2018). [https://doi.org/10.1109/ICICT43934.2018.9034272](https://doi.org/10.1109/ICICT43934.2018.9034272)
* [120] Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., Mirza, D., Belding, E., Chang, K.W., Wang, W.Y.: Mitigating gender bias in natural language processing: Literature review. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Online (Oct 2018). [https://doi.org/10.1109/ICICT43934.2018.9034272](https://doi.org/10.1109/ICICT43934.2018.9034272)
* [1Association for Computational Linguistics. pp. 1630-1640. Association for Computational Linguistics. 외부 링크: SS1에 의해 인용된 링크입니다.
*[107]H. Suresh and J. Guttag (2021) A framework for understanding source of harm throughout the machine learning life cycle. 알고리즘, 메커니즘 및 최적화에서 보안 및 액세스는 SS1에 의해 인용됩니다.
*[108]C. 스위니와 M Najafian (2020) Reducing sentiment polarity for demographic attributes in word embedding using adversarial learning. Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pp. 359-368. External Links: Link Cited by: SS1.
*[109]L. Sweeney (2013) Discrimination in online ad delivery: 구글 광고, 블랙 네임 및 화이트 네임, 인종 차별, 클릭 광고. Queue 11(3), pp. 10-29(mar 2013). 외부 링크: SS1에 의해 인용된 링크입니다.
*[110]Y. Tan and L. E. Celis (2019) Assessing social and intersectional bias in contextized word representation. In Advances in Neural Information Processing Systems, Vol. 32, pp. 1166-1170. External Links: Link Cited by: SS1.
*[111]A. Torralba and A. A. Efros (2011) Unbiased look on dataset bias. CVPR 2011, pp. 1521-1528. External Links: Link Cited by: SS1.
*[112]E. Vanmassenhove, C. Hardmeier, and A. Way (2018-05) Getting gender right in neural machine translation. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3003-3008. External Links: Link Cited by: SS1.
*[113]D. 데 바시몬 마넬라, D. 에링턴, T Fisher, B. van Breugel, and P. Mintervini (2021) Stereotype and skew: Quantify gender bias in pre-trained and fine-tuning language models. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 2232-2242. External Links: Link Cited by: SS1.
*[114]A. 바스와니 뉴저 J. Uszkoreit, L. 존스 카이저와 저는 폴로수킨(2017) 주의만 있으면 됩니다. In Advances in neural information processing systems, Vol. 30, pp. 5998-6008. External Links: Link Cited by: SS1.
*[115]P. Venkit과 S. Wilson(2021) Identification of bias of disabilities in sentiment analysis and toxicity detection models. arXiv:2111.13259. Cited by: SS1.
*[116]J. 비그성 게르만 벨린코프, D. 첸, D. 네보, Y 가수, S. Shieber(2020) 인과관계 매개분석을 이용한 언어모형의 성별편향성 탐색 In Advances in Neural Information Processing Systems, Vol. 33, pp. 12388-12401. External Links: Link Cited by: SS1.
*[117]K. 웹스터 Recasens, V. Axelrod, and J. Baldridge (2018) Mind the GAP: a balanced corpus of gendered ambiguous pronouns. Transactions of the Association for Computational Linguistics6, pp. 605-617 (eng 2018). 외부 링크: SS1에 의해 인용된 링크입니다.
*[118]L. 위딩거, J. 멜러, M. Rauh, C. Griffin, J. Uesato, P. S. Huang, M. 정민 Glaese, B. Balle, A. Kasirzadeh, et al.(2021) Ethical and social risk of harm from language models. arXiv:2112.04359. Cited by: SS1.
*[119]R. Wolfe and A. Caliskan (2021) Low frequency names show bias and overfitting in contextualizing language models. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 518-532. External Links: Link Cited by: SS1.

Computational Linguistics, Online and Punta Cana, Dominican Republic (Nov 2021). [https://doi.org/10.18653/v1/2021.emnlp-main.41] (https://doi.org/10.18653/v1/2021.emnlp-main.41)
* [120] Yang, Y., Uy, M.C.S., Huang, A.: Finbert: A pretrained language model for financial communications. arXiv:2006.08097 (2020)
* [121] Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R., Le, Q.V.: Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems **32** (2019), [https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf)
* [122] Yang, Z., Asyrofi, M.H., Lo, D.: Biasrv: Uncovering biased sentiment predictions at runtime. In: Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. p. 1540-1544. ESEC/FSE 2021, Association for Computing Machinery, New York, NY, USA (2021). [https://doi.org/10.1145/3468264.3473117](https://doi.org/10.1145/3468264.3473117)
* [123] Ye, W., Xu, F., Huang, Y., Huang, C., et al.: Adversarial examples generation for reducing implicit gender bias in pre-trained models. arXiv:2110.01094 (2021)
* [124] Zhang, B.H., Lemoine, B., Mitchell, M.: Mitigating unwanted biases with adversarial learning. In: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society. pp. 335-340. AIES '18, Association for Computing Machinery, New York, NY, USA (2018). [https://doi.org/10.1145/3278721.3278779](https://doi.org/10.1145/3278721.3278779)
* [125] Zhang, H., Lu, A.X., Abdalla, M., McDermott, M., Ghassemi, M.: Hurtful words: Quantifying biases in clinical contextual word embeddings. In: Proceedings of the ACM Conference on Health, Inference, and Learning. p. 110-120. CHIL '20, Association for Computing Machinery, New York, NY, USA (2020). [https://doi.org/10.1145/3368555.3384448](https://doi.org/10.1145/3368555.3384448)
* [126] Zhao, J., Wang, T., Yatskar, M., Cotterell, R., Ordonez, V., Chang, K.W.: Gender bias in contextualized word embeddings. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). pp. 629-634. Association for Computational Linguistics, Minneapolis (Jun 2019). [https://doi.org/10.18653/v1/N19-1064](https://doi.org/10.18653/v1/N19-1064)
* [127] Zhao, J., Wang, T., Yatskar, M., Ordonez, V., Chang, K.W.: Men also like shopping: Reducing gender bias amplification using corpus-level constraints. In: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. pp. 2979-2989. Association for Computational Linguistics, Copenhagen, Denmark (Sep 2017). [https://doi.org/10.18653/v1/D17-1323](https://doi.org/10.18653/v1/D17-1323)
* [128] Zhao, J., Wang, T., Yatskar, M., Ordonez, V., Chang, K.W.: Gender bias in coreference resolution: Evaluation and debiasing methods. In: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers). pp. 15-20. Association for Computational Linguistics, New Orleans, Louisiana (Jun 2018). [https://doi.org/10.18653/v1/N18-2003](https://doi.org/10.18653/v1/N18-2003)
* [129] Zhao, J., Zhou, Y., Li, Z., Wang, W., Chang, K.W.: Learning gender-neutral word embeddings. In: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 4847-4853. Association for Computational Linguistics, Brussels, Belgium (2018). [https://doi.org/10.18653/v1/D18-1521](https://doi.org/10.18653/v1/D18-1521)
* [130] Zhiltsova, A., Caton, S., Mulway, C.: Mitigation of unintended biases against non-native english texts in sentiment analysis. In: Proceedings for the 27th AIAI Irish Conference on Artificial Intelligence and Cognitive Science, Galway, Ireland, December 5-6, 2019. CEUR Workshop Proceedings, vol. 2563, pp. 317-328. CEUR-WS.org (2019), [http://ceur-ws.org/Vol-2563/aics_30.pdf](http://ceur-ws.org/Vol-2563/aics_30.pdf)* [131] Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., Findler, S.: Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In: Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV). pp. 19-27 (2015)
* [132] Zmigrod, R., Mielke, S.J., Wallach, H., Cotterell, R.: Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 1651-1661. Association for Computational Linguistics, Italy (Jul 2019). [https://doi.org/10.18653/v1/P19-1161](https://doi.org/10.18653/v1/P19-1161)
