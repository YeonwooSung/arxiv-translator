# Multimodal Large Language Models: Survey

지양우\({}^{1}\), 원성간\({}^{1,2}\)1, 제펑첸\({}^{1}\), 시성완\({}^{3}\), 필립 S. Yu\({}^{4}\)

\({}^{1}\) 중국 광저우 510632 진안대학교

중국 광저우 510330 파주연구소

\({}^{3}\)중국 남중국공과대학 광저우 510641, 중국

\({}^{4}\)미국 일리노이시카고대학

###### Abstract

멀티모달 언어 모델의 탐색은 이미지, 텍스트, 언어, 오디오 및 기타 이질성과 같은 여러 데이터 유형을 통합한다. 최신 대형 언어 모델은 텍스트 기반 작업에서 탁월하지만 다른 데이터 유형을 이해하고 처리하는 데 어려움을 겪는 경우가 많다. 멀티모달 모델은 다양한 모달리티를 결합하여 이러한 한계를 해결하여 다양한 데이터에 대한 보다 포괄적인 이해를 가능하게 한다. 본 논문은 멀티모달의 개념을 정의하고, 멀티모달 알고리즘의 역사적 발전을 살펴보는 것으로 시작한다. 또한 주요 기술 기업의 노력을 중심으로 다양한 멀티모달 제품을 소개합니다. 멀티모달 모델의 기술적 측면에 대한 통찰력을 제공하는 실용적인 가이드가 제공된다. 또한, 최신 알고리즘과 일반적으로 사용되는 데이터 세트의 컴파일을 제시하여 연구자에게 실험 및 평가를 위한 귀중한 리소스를 제공한다. 마지막으로, 우리는 멀티모달 모델의 적용을 탐색하고 개발과 관련된 문제에 대해 논의한다. 이러한 측면을 해결함으로써 본 논문은 다양한 영역에서 멀티모달 모델과 그 잠재력에 대한 더 깊은 이해를 촉진하는 것을 목표로 한다.

 모달리티, 언어 모델, 멀티모달 모델, 대형 모델, 설문 조사.

## I Introduction

멀티모달 모델은 이미지, 텍스트, 오디오 등을 포함한 여러 데이터 유형을 결합합니다. 전통적인 LMM(Large Language Model) [1, 2]는 주로 텍스트 데이터에 학습되어 적용되지만, 다른 데이터 타입을 이해하는데 한계가 있다. GPT-3 [3], BERT [4], RoBERTa [5]와 같은 순수 텍스트 LLM은 텍스트 생성 및 인코딩과 같은 작업에서 탁월하지만 다른 데이터 유형에 대한 포괄적인 이해와 처리가 부족하다. 이 문제를 해결하기 위해 멀티모달 LLM은 여러 데이터 유형을 통합하여 순수 텍스트 모델의 한계를 극복하고 다양한 데이터 유형을 처리할 수 있는 가능성을 열어준다. GPT-4 [6]은 멀티모달 LLM의 우수한 예로서의 역할을 한다. 이미지와 텍스트의 형태로 입력을 수용할 수 있으며, 다양한 벤치마크 테스트에서 인간 수준의 성능을 보여준다. 멀티모달 인식은 지식 습득과 현실 세계와의 상호작용을 위해 매우 중요하기 때문에 일반적인 인공 지능을 달성하기 위한 기본적인 요소이다. 또한 멀티모달 입력의 적용은 멀티모달 로봇 공학, 문서 지능, 로봇 기술 등 고부가가치 영역에서 언어 모델의 잠재력을 크게 확장한다. 연구에 따르면 멀티모달 인식에 대한 네이티브 지원은 새로운 작업에 멀티모달 LLM을 적용할 수 있는 새로운 기회를 제공한다. 다양한 실험을 통해 멀티모달 LLM은 단일 모달리티 모델에 비해 상식 추론에서 우수한 성능을 보여 지식 획득을 위한 교차 모달 전달의 이점을 강조한다.

최근 몇 년 동안, 멀티모달 모델의 개발은 추가적인 적용 가능성을 보여주었다. 텍스트 생성 모델과는 별도로 인간-컴퓨터 상호작용, 로봇 제어, 이미지 검색, 음성 생성 등의 분야에서 멀티모달 모델이 점점 더 많이 적용되고 있다. 그러나 순수 텍스트 LLM은 일반적으로 텍스트 말뭉치에서만 학습되고 시각 신호에 대한 지각 능력이 부족하기 때문에 LLM의 기능을 멀티모달 텍스트와 이미지 영역으로 이전하는 것은 연구의 활성 영역으로 남아 있다. 멀티모달 모델에 대한 여러 리뷰가 있지만 이러한 기사 각각은 초점이 다르다. Summaira _et al._[7]은 모드를 기반으로 분류함으로써 다양한 모달리티의 적용에 대한 자세한 소개를 제공했다. Wang _et al._[8]은 멀티모달 대규모 모델에 사용된 최신 알고리즘과 최근 실험에 사용된 데이터 세트를 종합적으로 편집하여 독자에게 편의를 제공했다. 음 _et al._[9]은 최근 몇 년 동안 다양한 유형의 멀티모달 알고리즘을 검토 범위 내에서 분류하고 차별화했다.

그러나 이러한 기사는 주로 대규모 모델에 대한 소개로 시작되며, 멀티모달 모델의 개발 과정과 실제 적용에 대한 개요가 부족하다. 본 논문은 멀티모달의 근본적인 정의에서 출발하여 이러한 간극을 해결하는 것을 목적으로 한다. 그것은 멀티모달 알고리즘의 역사적 발전에 대한 개요를 제공하고 이 분야의 잠재적인 응용 및 문제에 대해 논의한다.

* 멀티모달 모델/알고리즘의 개념을 정의 하는 것으로 시작 하 고, 이후 멀티모달 알고리즘의 역사적 개발을 조사 합니다.
* 지식 표현, 학습 목표 선택, 모델 구성, 정보 융합 및 프롬프트 사용을 포함하여 멀티모달 모델과 관련된 다양한 기술적 측면에 대한 실용적인 가이드를 제공합니다.
* 일반적으로 사용되는 데이터 세트와 함께 멀티모달 모델에 사용되는 최신 알고리즘을 검토합니다. 이는 향후 연구 및 평가를 위한 기초 자료를 제공한다.
* 마지막으로 다중 모드 모델의 여러 응용 프로그램을 탐색 하 고 현재 개발에서 발생 하는 몇 가지 주요 문제에 대해 논의 합니다.

이 글의 나머지 부분은 다음과 같이 정리된다. II절에서는 멀티모달의 관련 개념에 대해 논의한다. 섹션 III에서는 기술적 요점에 대한 실용적인 지침을 나타낸다. 또한 섹션 IV에서는 관련 모델을 구성했다. 또한 섹션 V에서 멀티모달 및 다양한 유형의 데이터 세트에 대한 몇 가지 유망한 방향을 제시하고 섹션 VI의 문제를 강조한다. 마지막으로, 우리는 VII절에서 이 조사를 마무리한다.

## II 관련 개념

멀티모달은 그림 1과 같이 복합적인 것을 여러 모달리티를 통해 표현하거나 인지하는 것을 말한다.

다중 모달리티는 서로 다른 두 카메라에서 촬영된 이미지와 같은 동종 모달리티와 이미지와 텍스트 언어의 관계와 같은 이종 모달리티로 분류할 수 있다. 의미적 지각 관점에서 멀티모달 데이터는 시각, 청각, 촉각 및 후각 입력과 같은 다양한 감각 양식으로부터의 정보를 통합하여 환경의 통일되고 의미 있는 표현을 형성하는 것을 지칭한다[10]. 데이터 관점에서 멀티모달 데이터는 이미지, 수치 데이터, 텍스트, 기호, 오디오, 시계열 또는 세트, 트리, 그래프로 구성된 복잡한 데이터 구조, 심지어는 서로 다른 데이터베이스 또는 지식 베이스로부터의 다양한 정보 자원의 조합과 같은 서로 다른 데이터 유형의 조합으로 볼 수 있다. 이질적인 데이터 소스의 탐색과 분석은 멀티모달 학습으로 이해될 수 있다. 멀티모달 데이터를 사용하면 사물을 보다 포괄적이고 총체적으로 표현할 수 있어 멀티모달 연구를 중요한 연구 영역으로 만든다. 멀티모달 접근법을 활용하여 감성 분석, 기계 번역, 자연어 처리 및 최첨단 생의학 연구[11]와 같은 분야에서 상당한 돌파구를 달성했다.

멀티모달 연구의 진화 동안 그림 2와 같이 4개의 별개의 단계를 식별할 수 있다.

**단일 양식(1980-2000)** 입니다. 기본적인 컴퓨팅 능력에 의존하는 것이 특징이었다. 1980년대에는 통계적 알고리즘과 이미지 처리 기술이 얼굴 인식 시스템에 사용되었다. 그 작업은 이 분야의 초기 방법의 기초를 마련했다. 동시에 IBM의 연구팀은 음성 인식 기술의 정확성과 신뢰성을 향상시킨 은닉 마르코프 모델(HMM) [12]의 사용과 같은 음성 인식에 상당한 기여를 했다. 1990년대에 더 많은 진전이 있었다. 카나데 팀은 얼굴 인식을 위한 아이겐페이스 방법을 개발했다[13]. 이는 주성분 분석(PCA)을 활용하여 얼굴 이미지에서 통계적 패턴을 기반으로 얼굴 특징을 추출하고 개인을 인식하였다[14]. 드래곤 시스템과 같은 회사들은 음성 인식 시스템을 발전시키는 데 초점을 맞췄고, 정확도를 높여 음성 언어를 문자 텍스트로 변환할 수 있는 기술을 개발했다[15].

**촬영장비 변환(2000-2010)** 이 단계에서 연구자들은 인간-컴퓨터 상호 작용 연구에 상당한 자원을 할애했다. 컴퓨터가 인간의 행동을 시뮬레이션하고 사람들의 일상생활에서 편리성을 높일 수 있도록 하는 것이 목표였다. 이 기간 동안 몇 가지 주목할 만한 발전이 있었다. 2001년 AMI 프로젝트는 회의 데이터를 기록하고 처리하기 위한 컴퓨터의 활용을 제안했다. 이 프로젝트는 회의의 오디오, 비디오, 텍스트 데이터를 분석할 수 있는 기술을 개발하여 보다 효율적인 정보 검색 및 협업을 가능하게 하는 것을 목표로 했다[16]. 2003년 CALO 프로젝트는 시리의 전신 역할을 했던 챗봇 기술을 도입하여 상당한 기여를 하였다. "학습하고 조직하는 인지 어시스턴트"의 약자인 CALO 프로젝트는 인간의 언어를 이해하고 반응하며 작업을 수행할 수 있는 지능형 가상 어시스턴트를 개발하는 것을 목표로 했다[17]. 2008년 사회 신호 처리(SSP) 프로젝트는 사회 신호 처리 네트워크의 개념을 도입했다. 이 프로젝트는 사회적 상호작용을 이해하고 보다 자연스러운 인간-컴퓨터 통신을 용이하게 하기 위해 얼굴 표정, 제스처, 음성 톤과 같은 비언어적 단서를 분석하는 데 중점을 두었다[18].

**촬영장비 융합(2010-2020)** 이 단계에서 딥러닝 기술과 신경망의 통합은 현장에서 주목할 만한 발전으로 이어졌다. 2011년 Ngiam[19]에 의해 선구적인 멀티모달 딥러닝 알고리즘이 도입되었다. 이 알고리즘은 이미지 및 텍스트와 같은 여러 모달리티의 융합 및 분석을 가능하게 함으로써 분야를 발전시키는 데 중요한 역할을 했다. 다양한 양식의 특징의 공동 학습을 촉진하고 이미지 분류, 음성 인식 및 비디오 분석과 같은 작업에서 향상된 성능에 기여했다. 2012년에 딥 볼츠만 기계(DBM) [20]에 기반한 멀티모달 학습 알고리즘은 서로 다른 모달리티 간의 종속성과 상호 작용을 모델링하는 것을 목표로 했다. 딥러닝의 힘과 DBM의 생성 모델링 기능을 활용하여 모달리티 간의 복잡한 관계를 포착하고 복잡한 멀티모달 데이터의 이해와 표현을 향상시킬 수 있다. 2016년에

도. 1: 멀티모달의 정의.

도. 2: 멀티모달 연구의 4개의 별개의 단계.

의미론적 주의를 갖는 신경 이미지 캡션 알고리즘이 소개되었고[21], 이미지가 처리되고 기술되는 방식에 혁명을 일으켰다. 이 알고리즘은 이미지에 대한 설명 캡션을 생성하는 기능을 가지고 있어 자동화된 이미지 이해 및 해석이 가능하다. 컴퓨터 비전 기술과 심층 신경망을 결합하여 알고리즘은 이미지의 시각적 내용을 분석하고 인간과 유사한 설명을 생성하여 접근성을 개선하고 자동 이미지 태깅, 이미지 검색 및 시각 장애인을 위한 보조 기술과 같은 응용 프로그램을 가능하게 할 수 있다.

**대규모 멀티모달(2020-7).** 대규모 모델의 급속한 개발은 멀티모달 알고리즘에 대한 새로운 기회를 열었습니다. 2021년 CLIP 모델이 도입되었다[22]. 고정 카테고리 라벨의 종래의 패러다임을 산산조각냄으로써, CLIP는 미리 결정된 클래스 카운트로 방대한 데이터 세트를 조립하는 부담을 덜어준다. 대신, CLIP는 이미지-텍스트 쌍의 컬렉션을 가능하게 하고 유사도를 예측하거나 생성하기 위해 감독되지 않은 기술을 활용한다. 2022년 오픈아이의 제품 DALL-E 2는 CLIP 이미지 임베딩을 조건으로 한 확산 모델을 활용한다[23]. 텍스트 프롬프트를 기반으로 고품질 이미지와 작품을 생성할 수 있습니다. Microsoft도 BEiT-3(BERT Pretraining of Image Transformers)[24]를 소개하였다. BEiT-3는 공유된 멀티웨이 변압기 구조를 사용하여 마스킹된 데이터를 통해 사전 학습을 완료한다. 시각 및 시각 언어의 다양한 다운스트림 작업으로 마이그레이션할 수 있습니다. 2023년, KOSMOS-1은 마이크로소프트[25]에 의해 출시되었다. KOSMOS-1은 다양한 모달리티의 정보를 처리하고 통합하고, 정밀하게 지침을 따르고, 맥락 내 학습을 통해 새로운 맥락에 적응할 수 있는 능력을 포함하여 인상적인 다양한 능력을 자랑하는 최첨단 멀티모달 LLM이다. 이 모델은 언어와 지각을 통합하여 스스로 보고 말할 수 있도록 하여 시각적 대화, 이미지 캡셔닝, 제로샷 이미지 분류와 같은 작업에 능숙하게 만든다. 또 다른 주목할만한 모델, 즉 PaLM-E[26]는 고급 언어 및 비전 모델, 예를 들어 PaLM 및 ViT-22B를 결합한다. 그들은 객체 검출 및 장면 분류와 같은 시각적 작업에서 탁월할 수 있는 동시에 코드 생성 및 수학 방정식 해결과 같은 언어 작업의 숙련도를 입증할 수 있다. PaLM-E는 작업별 미세 조정 없이 시각 언어 성능에서 새로운 벤치마크를 제공한다.

## III 기술 요점에 대 한 실용 지침입니다.

멀티모달 대형 모델의 기술적 요점은 그림 3과 같이 지식 표현, 학습 목표 선택, 모델 구조 구성, 정보 융합 및 프롬프트 사용을 포함하지만 이에 국한되지 않는다.

**지식 표현.** 텍스트와 이미지 모두 토큰화 및 임베딩이 필요합니다. 토큰은 모델에 대한 입력의 기본 단위인 반면 임베딩은 계산에 사용되는 토큰의 벡터 표현이다. 텍스트의 경우 CBOW 및 Skip-gram과 같은 일부 방법을 포함하여 Word2Vec [27]이 토큰화에 일반적으로 사용되었다. Word2Vec은 계산 효율이 높지만 어휘 제한이 있다. 결과적으로, 바이트 쌍 인코딩[28]과 같은 서브워드 토큰화 방법들은 워드들을 더 작은 유닛들로 분할한다. 이 방법은 BERT와 같은 다양한 변압기 모델에 적용되었다. 대조적으로, 이미지 토큰화는 텍스트보다 더 복잡하다. 지역 기반, 격자 기반, 패치 기반 등 3가지 유형[29]으로 분류할 수 있다. 영역 기반 방법은 미리 훈련된 객체 검출기를 활용하여 특징을 추출한다. 그리드 기반 방법은 컨볼루션 신경망을 직접 적용하여 이미지에서 그리드 기반 정보를 추출한다. 패치 기반 방법은 이미지를 더 작은 블록으로 분할하고 해당 블록에서 선형 투영을 추출하는 것을 포함한다. METER 모델[30]로부터의 데이터에 따르면, 시각적 특징 측면을 최적화하는 것은 텍스트 측면을 최적화하는 것보다 결과에 훨씬 더 큰 영향을 미친다. 멀티모달 프리트레이닝 모델의 구성에서 시각적 특징의 임베딩 레이어 또는 복잡도가 텍스트 특징의 임베딩 레이어 또는 복잡도를 능가하여 시각적 정보의 중요성을 강조한다. 멀티모달 모델은 시각적 특징으로부터 더 많은 지식을 학습할 수 있다.

**학습 목표 선택** 멀티모달 사전 교육에서 매우 중요합니다. 현재, 멀티모달 프리트레이닝에서의 공통 학습 과제는 이미지-텍스트 콘트라스트(ITC), 마스킹 언어 모델링(MLM), 마스킹 시각 모델링(MVM), 및 이미지-텍스트 매칭(TM)을 포함한다[31]. ITC는 대조적 학습을 통해 이미지와 텍스트를 정렬하기 위해 긍정 및 부정 샘플 쌍을 구성하는 것을 포함한다. 또한, MLM과 MVM 기법을 활용하여, 언어적 지식과 시각적 단서들의 조합으로부터 마스킹된 언어적 토큰들을 재구성함으로써 언어와 시각적 데이터 사이의 미묘한 연결을 추론하는 것을 학습할 수 있다. 이를 통해 멀티모달 콘텐츠를 이해하고 생성하는 능력을 향상시킬 수 있다. TM은 이미지와 텍스트 쌍의 일치 여부를 예측하는 것을 목표로 하는 이진 분류 작업으로 볼 수 있다. 일반적으로 서로 다른 학습 목표를 조합하여 사용하면 멀티모달 모델의 성능을 향상시킬 수 있다. 예를 들어, UNITER 모델에서, 더 많은 학습 목표를 통합하는 것은 일반적으로 더 나은 결과로 이어진다. UNITER는 MLM 및 ITC와 같은 여러 학습 목표를 활용하고 다양한 전문 시나리오에 걸쳐 잘 수행한다. 그러나 너무 많은 학습 목표를 사용하는 것이 항상 유리한 결과를 산출하는 것은 아니다. 이는 METER[30]에 대한 실험에서 검증되었다.

**모델 구성.** 다양한 모델 구조를 기반으로 다중 모드 모델을 인코더 전용으로 분류할 수 있습니다.

도. 3: 멀티모달 모델의 기술적 요점.

및 인코더-디코더 모델들을 포함한다. 인코더 전용 모델은 트랜스포머의 인코더 부분만 사용합니다. 멀티모달 입력은 출력을 생성하기 위해 인코더에 의해 직접 처리된다. 인코더 전용 모델의 일반적인 예는 이미지-텍스트 검색과 같은 작업에 적합하지만 이미지 캡셔닝과 같은 작업에 이상적이지 않은 CLIP[23] 및 ALBEF[32]를 포함한다. 인코더-디코더 모델은 트랜스포머의 인코더 및 디코더 부분을 모두 통합한다. 디코더는 이전에 생성된 토큰들 및 그 자신의 출력을 수신하여 출력 시퀀스를 자동-회귀적으로 생성한다. T5[33] 및 SimVLM[34]과 같은 인코더-디코더 모델은 디코더의 능력을 레버리지하고 생성 작업에 적합하지만 이미지-텍스트 검색과 같은 작업에 적합하지 않을 수 있다.

**정보 융합.** 다른 모달리티를 별도로 인코딩한 후에는 멀티모달 인코딩을 위한 인코더를 설계해야 합니다. 상이한 융합 방법에 기초하여, 멀티모달 모델은 융합 인코더 및 듀얼 인코더 모델로 분류될 수 있다[35]. 퓨전 인코더는 모달리티들 간에 상호작용하기 위해 퓨전 방법들을 이용한다. 자기-주의 또는 교차-주의 동작들을 통해, 융합 인코더는 모달리티들의 융합된 표현들을 생성한다. 융합 방법에는 주로 단일 스트림 및 이중 스트림 접근법이 포함된다. 단일 스트림 접근법은 두 양식 사이에 단순한 정렬 또는 상관 관계가 있다고 가정하고 자기 주의 메커니즘을 양식을 연결하기 전에 양식에 직접 적용한다. 이중 스트림 모델은 교차 주의 메커니즘을 사용하여 더 나은 다중 모드 표현을 얻기 위해 모달 내 및 교차 모달 상호 작용을 별도로 모델링해야 한다고 가정한다. 융합 인코더는 서로 다른 레벨에서 교차-모달 상호작용을 모델링하고 특정 추론 태스크에서 우수한 성능을 달성했다. 그러나 이미지-텍스트 검색과 같은 작업에서 모든 이미지-텍스트 쌍의 상호 작용을 인코딩하면 추론 속도가 느려진다. 듀얼 인코더는 두 모달리티를 인코딩하기 위해 별도의 싱글 모달 인코더를 사용한다. 충분한 인코딩 후, 복잡한 트랜스포머 구조에 의존하지 않고, 간단한 내적 또는 얕은 주의 계층을 사용하여 그들 사이의 유사성 점수를 계산한다. 퓨전 인코더는 추론 작업에 적합하고, 듀얼 인코더는 검색 작업에 적합하다. 따라서 멀티모달 모델의 성능을 향상시키기 위해 서로 다른 모델 아키텍처 또는 정보 융합 방법을 결합한다. 이는 복합적 통일 이행의 배후에 있는 기제이기도 하다. 예를 들어, VLMO는 상이한 모달리티를 처리하기 위해 이미지 전용, 텍스트 전용, 및 이미지-텍스트 데이터에 대해 사전 트레이닝하는 "세 전문가" 접근법을 채택하고, 추론 및 검색과 같은 태스크에서 양호한 성능을 달성한다[36].

**프롬프트 사용.** 프롬프트 메서드는 주로 다운스트림 작업에서 사전 훈련과 미세 조정 간의 간격을 줄이는 데 사용됩니다. 다운스트림 태스크의 템플릿을 수정함으로써 프롬프트는 사전 훈련과 미세 조정 사이의 차이를 최소화하여 미세 조정 비용을 줄이고 다운스트림 애플리케이션에서 모델의 성능을 향상시키는 것을 목표로 한다. 그것은 제로 또는 작은 데이터 샘플들을 처리하는 능력을 가지며, 이는 다양한 LLM들[37]에서 널리 채택되어 왔다. 신속한 방법은 멀티모달 사전 훈련 작업에서도 중요한 역할을 한다. 예를 들어, 시각적 ChatGPT[38]에서, 프롬프트 관리자는 ChatGPT의 이해 및 관련 이미지의 생성을 용이하게 하는 정보적 프롬프트를 생성하는 데 사용된다. CLIP에서 프롬프트 메소드는 텍스트에 대한 유익한 프롬프트를 생성함으로써 제로 샷 태스크에서 적용되어 성능이 향상된다[23].

## IV 실용적 알고리즘 가이드

멀티모달의 알고리즘은 기초 모델과 대규모 멀티모달 사전 훈련 모델의 두 가지 유형으로 분류할 수 있다. 기초 모달은 멀티모달의 기본 프레임워크입니다. 많은 새로운 대규모 멀티모달 사전 훈련 모델이 이를 기반으로 개선된다.

### _Foundation model._

**Transformer**[39]는 2017년에 제안 되어 기존 딥러닝 모델을 방해 하 고 기계 번역 작업에서 좋은 성능을 달성 했습니다. 대규모 말뭉치에 대한 자체 감독 사전 훈련과 다운스트림 작업에 대한 후속 미세 조정을 거치는 능력으로 주목을 받았다. 이러한 패러다임은 많은 사전 훈련된 대규모 모델이 뒤따랐다. 입력 시퀀스 길이에 무관한 Transformer의 가중치 공유 특성은 멀티모달 응용에 적합하다. 모델 내의 특정 모듈들은 가중치 파라미터들을 공유할 수 있다. 트랜스포머에서 가중치 공유 개념은 자기 주의 모듈과 피드 포워드 신경망 모두 입력 시퀀스의 길이에 영향을 받지 않는다는 사실에서 발생한다. 이러한 가중치 공유 개념은 멀티모달 모델에도 적용될 수 있다. 예를 들어, 이미지 및 텍스트를 포함하는 멀티모달 설정에서, 이미지 트레이닝으로부터 학습된 가중치 파라미터는 텍스트 트레이닝에 사용될 수 있고, 결과는 때때로 미세 조정의 필요없이도 유효하게 유지된다.

**VIT.** 자연어 처리 (NLP) 영역에서 자체 주의 메커니즘을 사용 하는 Transformer 모델의 예외적인 성능은 컴퓨터 비전에서 많은 관심을 끌었습니다. 많은 연구들이 트랜스포머 메커니즘을 컴퓨터 비전 작업에 통합하기 시작했다. 그러나 Transformer는 입력 데이터 크기 측면에서 한계가 있어 입력 전략에 대한 세심한 고려가 필요하다. 이전 연구에서 영감을 얻은 구글은 강력한 계산 자원으로 힘을 얻는 비전 트랜스포머(ViT) 모델을 제안했다. ViT 모델은 이미지를 패치로 분할(예를 들어, 이미지를 16개의 패치로 분할)함으로써 입력 크기 제한을 해결한다[40]. 그런 다음 이러한 패치는 선형 매핑을 통해 트랜스포머가 처리할 수 있는 입력으로 처리되고 변환된다. 이 돌파구는 컴퓨터 비전과 NLP 사이의 격차를 해소했다. ViT는 Transformer가 영상을 처리할 수 있도록 할 뿐만 아니라 기존의 방법에 비해 보다 효율적인 영상 특징 추출 전략을 소개한다.

**BEiT.** ViT가 컴퓨터 비전에서 Transformer 모델의 적응으로 볼 수 있는 경우 BEiT는 컴퓨터 비전에서 BERT의 적응으로 간주할 수 있습니다 [24]. 생성 사전 훈련은 모델이 레이블이나 수동 주석에 의존하지 않고 데이터를 생성하는 방법을 학습하는 자기 지도 학습에서 중요한 방법 및 훈련 목표이다. 생성 사전 훈련은 자연어 처리에서 상당한 성공을 거두었다. BEiT는 컴퓨터 비전을 위한 생성 사전 훈련에서 두 가지 주요 과제를 해결한다. 첫 번째 과제는 이미지 정보를 NLP와 유사한 이산 토큰으로 변환하는 방법이다. BEiT는 이산 시각 임베딩 집계 방법을 사용하여 이미지를 이산화한다. 두 번째 과제는 이미지 정보를 사전 훈련 프로세스에 효과적으로 통합하는 방법이다. BEiT는 잘 정립된 ViT 구조를 활용하여 이미지 정보를 처리한다. 이 두 가지 점을 해결함으로써 BEiT는 마스크 언어 모델링(MLM)과 마스크 이미지 모델링(MIM) 방법을 컴퓨터 비전 분야에 성공적으로 적용하여 생성 사전 교육을 컴퓨터 비전 영역으로 가져오고 대규모 자체 감독 사전 교육을 가능하게 한다.

### _대규모 멀티모달 사전 훈련 모델_

**Visual ChatGPT**[38]는 이미지 이해 및 생성과 같은 다양한 시각적 작업을 처리하기 위해 다양한 시각적 기초 모델(VFM)을 통합합니다. 이를 통해 사용자는 언어뿐만 아니라 이미지를 주고받을 수 있어 여러 AI 모델의 협업이 필요한 복잡한 시각적 질문과 지시를 다단계로 할 수 있다. 이 시스템은 또한 VFM을 활용하고 반복적인 방식으로 피드백을 받는 데 도움이 되는 프롬프트 매니저를 소개한다. 이러한 반복 프로세스는 시스템이 사용자의 요구 사항을 충족하거나 종료 조건에 도달할 때까지 계속된다. 프롬프트를 통해 ChatGPT에 시각적 모델 정보를 주입함으로써 시스템은 시각적 특징을 텍스트 공간과 정렬하여 ChatGPT의 시각적 이해 및 생성 기능을 향상시킨다. Visual ChatGPT는 언어와 이미지를 넘어 모달리티를 처리할 수 있는 기능이 있습니다. 시스템은 처음에는 언어와 이미지에 초점을 맞추지만 비디오나 음성과 같은 다른 모달리티를 통합할 수 있는 가능성을 열어줍니다. 이러한 유연성은 새로운 모달리티 또는 기능이 도입될 때마다 완전히 새로운 멀티 모달리티 모델을 트레이닝할 필요성을 제거한다.

**MM-REACT**[41]은 ChatGPT와 다양한 시각적 모델을 결합하여 주로 VQA 형식을 통해 시연되는 다중 모달 작업을 활성화합니다. 질문에 대한 답변에서 ChatGPT는 시각적 모델을 도구로 활용하고, 구체적인 질문을 바탕으로 활용 여부를 결정한다. 이 시스템은 VQA를 위해 캡션 모델 및 언어 이미지 모델을 사용한 이전 연구와 유사성을 공유한다. 이러한 접근법에서 캡션 모델은 이미지를 텍스트로 변환한 다음 더 큰 모델에 의해 증거로 사용되어 답변을 생성했다. 그러나 MM-REACT는 시각 모델을 호출할지 여부를 자율적으로 결정하는 능력이 다르다.

**Frozen**[42]는 다중 모드 인 컨텍스트 학습에 LLM을 사용 하는 새로운 개념을 도입 했습니다. 특정 접근법은 시각적 인코더를 사용하여 이미지를 임베딩으로 변환하는 것을 포함한다. 이러한 임베딩은 텍스트와 연결되어 두 모달리티를 통합하는 결합된 데이터 형식을 생성한다. 이어서, 모델은 다음 토큰을 예측하기 위해 자기회귀 접근법을 사용한다. 훈련 프로세스 전반에 걸쳐, LLM은 동결된 상태로 유지되는 반면, 시각적 인코더는 훈련가능하다. 이를 통해 최종 모델은 멀티 모달 설정에서 상황 학습을 수행할 수 있는 능력을 획득하면서 언어 모델링 능력을 유지할 수 있다.

**BLIP-2**[43]은 이미지 특징을 추출 하기 위해 Qformer 모델을 사용 하 여 이미지를 인코딩 하는 데 Flamingo와 유사한 방법을 채택 합니다. Qformer는 Flamingo의 수신기 리샘플러와 유사한 역할을 한다. 그런 다음 이 모델은 교차 주의를 통해 이미지-텍스트 상호 작용을 촉진합니다. 훈련 중에 BLIP-2는 비주얼 인코더와 LLM을 모두 얼리고 Qformer만 미세 조정합니다. 그러나, 특정 다운스트림 작업 데이터세트를 미세 조정할 때, BLIP-2는 비주얼 인코더의 잠금을 해제하고 Qformer와 함께 미세 조정한다. BLIP-2에 대한 훈련 과정은 두 단계로 구성된다. i) Qformer와 비주얼 인코더만이 훈련에 참여한다. 이들은 이미지-텍스트 매칭, 대조적 학습 및 이미지-접지 텍스트 생성과 같은 고전적인 멀티모달 사전 훈련 작업을 사용하여 훈련된다. 이 스테이지는 Qformer가 비주얼 인코더로부터 텍스트 관련 특징들을 신속하게 추출하는 방법을 학습할 수 있게 한다. ii) Qformer로 인코딩된 벡터들은 캡션 생성을 위해 LLM에 삽입된다. BLIP-2는 VQA에 대한 제로 샷 및 미세 조정 시나리오 모두에서 유망한 성능을 보여준다. 동일한 작업에 대해 서로 다른 데이터 세트에 걸쳐 우수한 전송 가능성을 가지고 있습니다.

**LLaMA-Adapter**[44]는 어댑터를 삽입 하 여 LLaMA에서 효율적인 미세 조정을 도입 하 여 다중 모드 시나리오로 확장할 수 있습니다. 어댑터는 튜닝 가능한 파라미터로서 트랜스포머의 마지막 레이어에 연결되는 적응 프롬프트 벡터이다. 멀티 모달 설정에 적용될 때, 이미지는 먼저 동결된 시각적 인코더를 사용하여 멀티스케일 특징 벡터로 인코딩된다. 그런 다음 이러한 벡터는 적응 프롬프트 벡터에 요소별로 추가되기 전에 연결 및 투영 연산을 통해 집계된다.

**MiniGPT-4**[45]는 BLIP-2와 Vicuna의 조합을 기반으로 하는 GPT-4의 특정 기능을 재현한 것입니다. BLIP-2에서 Qformer와 비주얼 인코더를 직접 전달하여 LLM과 함께 얼려서 미세 조정을 위해 비주얼 쪽에 선형 레이어만 남깁니다. 이러한 튜닝 가능한 파라미터의 압축은 15 M의 모델 크기를 초래한다. 또한 2단계 미세 조정 전략이 채택됩니다. i) 캡션 생성이 훈련 과제로 사용된다. 모델은 여러 자막을 생성 한 다음 ChatGPT를 사용 하 여 이러한 자막을 다시 작성 하 여 상세 하 고 생생 한 설명을 만듭니다. ii) 더 미세한 조정을 위해 고품질의 이미지-텍스트 쌍들의 세트가 구성된다. 이 이미지-텍스트 쌍 세트는 모델을 정제하는 데 사용됩니다.

**LLaVA**[46] 및 MiniGPT-4는 둘 다 다중 모드 명령 미세 조정을 달성 하는 것을 목표로 하기 때문에 유사 합니다. 그러나 데이터 생성 및 훈련 전략 측면에서 차이가 있어 LLaVA 모델의 개발로 이어진다. 데이터 생성에서 LLaVA는 GPT-4를 활용하여 멀티턴 QA, 이미지 설명 및 복잡한 추론 작업을 포함한 다양한 명령어 미세 조정 데이터를 생성한다. 이렇게 하면 모델이 광범위한 쿼리를 처리할 수 있습니다. 현재 GPT-4의 인터페이스는 텍스트 입력만 허용하기 때문에 이미지 정보는 텍스트 형식으로 변환될 필요가 있다. 본 연구는 GPT-4에 입력된 텍스트 설명으로 COCO 데이터셋의 각 이미지에 대해 제공된 5개의 캡션과 바운딩 박스 좌표를 사용한다. 학습 전략과 관련하여 LLaVA는 2단계 접근법을 채택한다. i) 모델은 특정 규칙에 따라 cc3m 데이터세트로부터 필터링된 600,000개의 이미지-텍스트 쌍을 사용하여 미세 조정된다. 미세 조정 프로세스는 선형 레이어를 미세 조정하는 데만 초점을 맞추어 시각 및 언어 모델을 동결합니다. ii) 전술한 데이터 생성 전략을 이용하여, 160,000개의 명령어 미세 조정 데이터 샘플들이 생성된다. 그런 다음 언어 모델 손실을 사용하여 모델을 더욱 미세 조정합니다. 이 단계에서 시각적 모델은 동결되고 선형 레이어와 언어 모델 모두 미세 조정된다.

**PICA**[47]은 VQA 작업을 해결 하기 위해 LLM을 사용 하는 첫 번째 시도였습니다. 그 목적은 LLM이 이미지 정보를 이해하고 처리할 수 있도록 하는 것이었다. 이를 위해 선행 연구에서는 자막 모델을 사용하여 이미지를 해당 텍스트 설명으로 변환했다. 그런 다음 질문과 함께 캡션을 GPT-3에 입력하여 삼중항(질문, 캡션, 답변)을 형성하고 문맥 내 학습을 활용하여 GPT-3을 훈련하여 새로운 질문에 답하도록 하였다. 몇 번의 샷 내 맥락 학습 시나리오에서 PICa는 겨울왕국보다 더 나은 성능을 달성했지만 여전히 플라밍고에 미치지 못했다. 이는 이미지가 캡션으로 변환되는 동안 시각적 정보의 손실에 기인할 수 있다. 시각 정보는 질문에 답하는 데 중요한 역할을 하며 이미지를 텍스트로 변환하는 과정은 필연적으로 시각적 세부 사항과 의미론의 손실로 이어져 모델의 성능을 제한한다.

**PNP-VQA**[48]은 캡션 모델과 미리 훈련 된 언어 모델 (PLM)을 사용 하 여 VQA 작업을 해결 합니다. 그러나, 통합QAv2라는 질문-응답 모델을 사용하기 때문에 PLM의 선택 측면에서 PICa와 다르다. PNP-VQA는 제로 샷 VQA 기능을 달성하는 데 중점을 둔다. 캡션에서 이미지 정보의 손실 문제를 해결하기 위해, PNP-VQA는 캡션을 생성하기 전에 이미지-질문 매칭 모듈을 도입한다. 이 모듈은 이미지에서 주어진 질문과 가장 관련이 있는 패치를 식별합니다. 그런 다음 이러한 선택된 패치에 대해 캡션이 구체적으로 생성된다. 이러한 캡션 패치 쌍은 원래 질문과 함께 컨텍스트로 사용되며 통합QAv2 모델에 공급된다. 이 접근법은 관련 이미지 패치를 컨텍스트로 통합함으로써 생성된 캡션이 질문과 밀접하게 관련되어 있음을 보장한다. 이미지-질문 매칭 모듈을 통합하고 통합 QAv2를 PLM으로 활용함으로써 PNP-VQA는 VQA에 대해 생성된 캡션의 관련성과 정확성을 향상시키는 것을 목표로 한다. 이 전략은 모델이 보다 맥락적으로 관련된 답변을 생성하기 위해 이미지와 질문 정보를 효과적으로 활용할 수 있도록 한다.

**Img2LLM**[49]는 VQA 작업에 LLM을 사용할 때 두 가지 주요 문제를 해결하는 것을 목표로 합니다. i) LLM이 시각적 정보를 효과적으로 처리할 수 없는 촬영장비 단절; ii) 텍스트 생성을 통해 사전 훈련된 LLM이 미세 조정 없이 VQA에 대한 캡션을 활용하기 위해 고군분투하는 작업 단절. 이러한 문제를 극복하기 위해 저자는 (질문, 답변) 쌍을 통해 시각적 정보를 전달하는 것을 제안한다. 구체적으로, 접근법은 캡션 모델 또는 PNP-VQA와 유사한 방법을 사용하여 이미지에 대한 캡션을 생성하는 것을 포함한다. 이러한 자막에서 잠재적으로 특정 질문에 대한 답변 역할을 할 수 있는 명사 및 형용사와 같은 관련 단어가 추출된다. 이어서, 질문 생성 모델을 사용하여 대응하는 질문을 생성하고, 따라서 (질문, 답변) 쌍을 생성한다. 이러한 쌍은 주어진 이미지에 대한 질문에 답하는 LLM을 지원하는 맥락 내 학습에서 데모 역할을 한다. (질문, 답변) 쌍을 통해 시각적 정보를 전송함으로써, Img2LLM은 모달리티 단절 및 작업 단절 문제를 해결하여 LLM이 VQA 작업에 시각적 정보를 더 잘 활용할 수 있게 한다.

## 다양한 작업에 대한 실용적인 가이드

**이미지 캡션.** 이미지 캡션은 지정된 이미지에 대한 짧은 텍스트 설명을 생성하는 작업을 포함합니다. 이미지와 짧은 텍스트 설명으로 구성된 멀티모달 데이터 세트를 다루는 멀티모달 작업이다. 멀티모달 번역 작업은 개방형이고 주관적이어서 생성된 콘텐츠가 유일하지 않다. 이 과제의 목표는 시각적 표현을 텍스트 표현으로 변환하여 번역 문제를 해결하는 것이다. 시각적 모달리티를 텍스트로 변환하는 모델은 이미지의 의미 정보를 캡처할 필요가 있고 객체의 주요 객체, 동작 및 특징을 검출할 필요가 있다. 더욱이, 이미지 내의 객체들 간의 관계를 추론해야 한다. 이미지 캡셔닝은 이미지에 대한 텍스트 대안을 제공하는데 사용될 수 있으며, 이는 시각장애인 및 시각장애인 사용자에게 특히 도움이 된다[50]. 짧은 텍스트 설명들을 생성함으로써, 이러한 사용자들은 이미지들의 콘텐츠를 더 잘 이해하고 인지할 수 있다. 그것은 그들에게 시각적 세계와 상호 작용할 수 있는 기회를 제공하여 경험과 참여를 향상시킵니다.

**텍스트 이미지 생성** 텍스트 이미지 생성은 실제로 멀티모달 학습의 가장 인기 있는 애플리케이션 중 하나입니다. 텍스트를 이미지로 변환하는 문제를 해결합니다. 오픈아이의 DALL-E 2[23], 구글의 Imagen[51]과 같은 모델들이 이 분야에서 상당한 돌파구를 만들어 널리 주목을 받고 있다. 이러한 모델들의 작업은 이미지 캡셔닝의 역과정일 수 있다. 짧은 텍스트 설명을 프롬프트로서 제공함으로써, 텍스트 대 이미지 모델은 텍스트의 의미론을 정확하게 반영하는 신규 이미지를 생성할 수 있다. 최근에는 텍스트-비디오 모델의 등장도 있었습니다. 이러한 모델은 광범위한 응용 프로그램을 가지고 있습니다. 그들은 사진 편집과 그래픽 디자인을 도울 수 있는 동시에 디지털 아트에 영감을 줄 수 있습니다. 그들은 사용자에게 텍스트를 시각적 콘텐츠로 직접 변환할 수 있는 도구를 제공하여 창조 산업의 발전과 혁신을 주도합니다. 이러한 기술의 발전은 이미지를 만들고 이해하기 위한 새로운 가능성을 제공한다.

**수화 인식.** 이 작업의 목표는 수화 제스처를 인식하고 텍스트로 변환하는 것입니다. 제스처는 카메라를 통해 캡처됩니다. 제스처들을 정확하게 인식하기 위해, 대응하는 오디오 및 양 모달리티들이 정렬되어야 한다. 수화 인식은 모델이 비디오 프레임과 같은 시각의 시간적 정보와 오디오 파형과 같은 오디오 모달리티를 정렬할 것을 요구하기 때문에 정렬 방법에 기초한 작업이다[52]. 이것은 제스처들 및 그들의 대응하는 음성 언어를 식별하기 위해 비디오 프레임들 및 오디오 파형들 사이의 시간을 정렬하는 것을 포함한다. 수어 인식을 위해 일반적으로 사용되는 오픈 소스 데이터 세트 중 하나는 RWTH PHOENIX Weather2014T 데이터 세트[53]이며, 이는 서로 다른 서명자의 독일 수어에 대한 비디오 녹화를 포함한다. 데이터 세트는 시각적 모달리티와 오디오 모달리티를 모두 제공하여 정렬 방법에 의존하는 멀티모달 학습 작업에 적합합니다. 비디오 및 오디오의 시간적 정보를 정렬함으로써, 모델들은 시각적 및 오디오 특징들 모두를 수화 인식에 레버리지할 수 있고, 이에 의해 인식의 정확성 및 유효성을 향상시킬 수 있다.

**감정 인식.** 단일 모달 데이터 세트만 사용 하 여 감정 인식을 수행할 수 있지만 다중 모달 데이터 세트를 입력으로 사용 하 여 성능을 향상시킬 수 있습니다. 멀티모달 입력은 비디오, 텍스트 및 오디오의 형태를 취할 수 있거나 뇌파 데이터와 같은 센서 데이터를 통합할 수 있다[54]. 실제 세계의 예는 음악에서의 감정 인식이다. 이 작업에서 모델은 오디오 특징과 가사를 사용하여 음악의 감정 내용을 식별할 필요가 있다. 이러한 경우, 최종 예측을 생성하기 위해 오디오 특징 및 가사와 같은 개별 모달리티에 대해 훈련된 모델의 예측을 결합하기 때문에 늦은 융합 접근법을 사용하는 것이 적절하다. DEAM 데이터 세트는 음악 감정 인식 및 분석에 대한 연구를 지원하기 위해 특별히 설계되었다. 2,000곡 이상의 노래에 대한 오디오 특징 및 가사를 포함한다[55]. 오디오 특징은 MFCC, 스펙트럼 콘트라스트 및 리듬 특징과 같은 다양한 기술자를 포함하는 반면 가사는 백 오브 워드 및 워드 임베딩과 같은 기술을 사용하여 표현된다.

**비디오 처리.** 비디오 및 오디오 도메인에서 멀티모달 융합도 증가 추세입니다. 이미지-텍스트 멀티모달 모델이 비디오-텍스트 및 오디오-텍스트 멀티모달 도메인으로 이동함에 따라 일련의 대표적인 모델이 등장했다. 예를 들어, 이미지-텍스트 도메인에 대한 VideoCoCa 모델[56]이다. CLIP 모델은 VideoCLIP 모델의 개발로 이어졌다[57]. 통합된 멀티모달 대형 모델의 출현은 비디오 처리 분야의 발전을 주도했다. 알리바바의 mPLUG-2[58]는 비디오 질문 응답 및 비디오 캡셔닝과 같은 비디오 관련 태스크에서 인상적인 성능을 보여주었다. 더욱이, 구글의 MusicIM[59]은 텍스트 입력에 기초하여 음악을 생성할 수 있기 때문에 오디오 멀티모달 도메인에서 인지도를 얻었다. 또한, 비디오 및 오디오 도메인은 다양한 다른 멀티모달 태스크를 수반한다. 오디오-시각 음성 인식은 개인의 주어진 비디오 및 오디오에 대해 음성 인식을 수행하는 작업이다. 비디오 음원 분리는 주어진 비디오 및 오디오 신호에서 다수의 음원을 로컬화하고 분리하는 것을 포함한다. 오디오로부터 영상 생성은 주어진 소리와 관련된 영상을 생성하는 것을 말한다. 음성 조건 얼굴 생성은 주어진 음성 발화에 기초하여 말하는 사람의 비디오를 생성하는 것을 포함한다. 주어진 스피치에 기초하여 말하는 사람의 3D 얼굴 애니메이션을 생성할 수 있는 오디오-구동 3D 얼굴 애니메이션, 및 3D 얼굴 템플릿과 같은 일부 작업이 있다[60].

\begin{table}
\begin{tabular}{c c c c c} \hline \hline

**더 똑똑한 디지털 인간.** AIGC 기술 [61]은 디지털 인간 개발에 중요한 역할을 하여 프로세스를 단순화하고 개발 효율성을 향상시켰습니다. 메타나 엔비디아와 같은 기업들은 NVIDIA의 옴니버스 아바타를 예로 들어 사용자가 3D 디지털 휴먼을 만들 수 있도록 도와주는 제품을 선보였다. 사용자는 사진, 비디오 또는 오디오를 업로드하여 디지털 인간을 만들 수 있어 효율성과 비용 효율성의 이점을 제공합니다. 구체적으로, 자연 언어 생성 기술은 인간-컴퓨터 상호작용에서 콘텐츠의 품질에 영향을 미치는 반면, 컴퓨터 비전 기술은 립 동기화(lip synchronization)와 같은 디지털 인간의 얼굴 표정 및 신체 움직임에 영향을 미친다[62]. AIGC 기술의 지속적인 발전은 고품질의 인간-컴퓨터 상호작용을 가능하게 한다. AIGC는 AI가 주도하는 디지털 휴먼에게 지능형 개발을 통해 멀티모달 상호 작용 동안 인식, 인식, 분석 및 의사 결정 능력을 제공한다.

**데이터에 대한 실용적인 가이드.** 멀티모달 데이터 세트는 비전과 언어 작업에 대한 연구를 발전시키는 데 중요한 역할을 합니다. 이러한 데이터 세트는 이미지, 텍스트, 비디오 및 오디오와 같은 다양한 모달리티를 결합하여 다양한 애플리케이션에 대한 풍부하고 다양한 정보 소스를 제공한다. 우리는 다중 모드 데이터 세트를 다른 유형으로 분류하고 표 II와 같이 각 범주에 대한 대표 데이터 세트의 선별된 선택을 제시한다. 향후 연구를 위해 이러한 데이터 세트를 사용하여 실험을 수행하여 모델의 유효성을 테스트할 수 있다.

## VI Challenges

멀티모달 애플리케이션들의 성능을 더욱 향상시키기 위해, 일부 근본적인 이슈들은 여전히 다음을 포함하지만 이에 제한되지 않는 더 많은 주의를 필요로 한다:

**모달리티 확장.** 센서 및 데이터 원본은 다양하므로 보다 포괄적이고 정확한 분석 및 인식을 달성하기 위해 풍부한 정보를 얻을 수 있습니다. 예를 들어, 감정 계산 분야에서, 모달리티 확장은 사람들의 감정 상태에 대한 보다 포괄적인 이해와 인식을 얻기 위해 오디오, 표정, 심전도(ECG), 뇌전도(EEG)와 같은 다중 모달리티를 사용하는 것을 포함한다[71]. 오디오 모달리티는 화자의 톤 및 스피치 레이트의 변화를 캡처할 수 있고, 시각적 모달리티는 얼굴 표정 및 바디 랭귀지를 분석할 수 있으며, ECG 및 EEG는 감정 변화와 관련된 생리학적 신호를 제공할 수 있다. 또한, 의료 영상 분야는 CT 스캔, MRI 및 PET와 같은 여러 양식을 포함한다. 예를 들어, CT 스캔은 조직 구조 및 병변에 대한 상세한 정보를 제공할 수 있고, MRI는 조직의 해부학적 구조 및 기능을 관찰할 수 있으며, PET는 대사 및 바이오마커의 분포를 검출하는 데 사용될 수 있다. 영상 데이터의 서로 다른 양식을 결합함으로써 의사 및 연구자는 보다 포괄적이고 정확한 의료 정보를 획득하여 정밀한 진단 및 치료 결정을 지원할 수 있다.

**시간이 많이 걸리는 문제.** 교육 아키텍처를 최적화하고 교육 시간을 개선하기 위해 대규모 모델은 AI 시스템에 상당한 영향을 미칩니다. 첫째, 모델의 엄청난 규모로 인해 계산은 클러스터 전체에 분산되어야 할 수 있다. 둘째, 다중 사용자 및 다중 작업 시나리오가 일반적이어서 다중 테넌시에 대한 지원이 필요하다. 또한, 동적 고장 허용 능력을 가지기 위해서는 높은 신뢰성이 필수적이며 까다로운 모델이다. 여러 백본 모델을 결합해야 합니다. 멀티모달 LLM은 다양한 영역에서 엄청난 성공을 거두었지만, 그들의 계산 요구 사항은 모델 훈련에 상당한 문제를 제기한다. 모델 훈련[72]을 어떻게 가속화할 수 있을까요? 우리는 서로 다른 아키텍처의 여러 모델을 두 개의 고속 상호 연결된 데이터 센터에 동적으로 할당할 수 있다. 훈련 및 추론 동안, 경로는 갱 스케줄링을 통해 모델을 동적으로 스케줄링하여, 공유 계산, 공유 가중치 및 동적 라우팅과 같은 능력을 가능하게 한다[26].

**평생/지속적 학습.** 현재 클래식 한 접근 방식은 지정 된 데이터 집합에서 AI 알고리즘을 실행 하 고 모델을 만든 다음이 모델을 실제 작업에 적용 하는 것입니다. 이를 고립학습이라 하며 알고리즘이 메모리 능력을 갖지 못한다는 단점을 야기한다. 따라서, 모델이나 알고리즘은 학습된 지식을 보유하지 않고, 이후 미래 학습에 지속적으로 적용한다. 고립된 작업이 아닌 실제 응용의 경우, 멀티모달 대형 모델은 평생 학습[73] 또는 지속 학습[74]의 능력을 필요로 한다. 우리는 자신의 경험을 바탕으로 세계에 대한 복잡한 이해를 할 수 있는 지속적인 학습 능력을 갖춘 LLM을 구축하여 자율적이고 점진적인 훈련과 개선을 위해 보다 복잡한 지식을 사용해야 한다[74].

**AGL을 향합니다.** 인공지능(AGI)을 향한 경로에서 우리는 여전히 많은 기회와 도전에 직면해 있습니다. 예를 들어, 재앙적 망각 문제 [73]는 언어 작업에 대해 원래 훈련된 신경망 및 관련 가중치가 다른 작업에 대해 용도 변경되어 네트워크가 초기 훈련 목표를 잊어버리는 현상을 나타냅니다. 이러한 경우 큰 모델은 원래 언어 능력을 상실하여 쇠퇴할 수 있습니다. 예를 들어, 로봇 기반 애플리케이션으로 이동할 때 언어 능력 [75]입니다. BLIP-2, KOSMOS-1, BEiT-3 및 PaLI [76]과 같은 최근 연구에서는 이 문제를 해결하기 위해 두 가지 실행 가능한 접근 방식을 강조했습니다. i) 더 작은 네트워크를 사용하고 새 데이터로 처음부터 다시 훈련함으로써 재앙적 망각을 피합니다. ii) 더 큰 언어 네트워크를 백본으로 사용하여 치명적인 망각을 우회합니다. 멀티모달 융합, 멀티모달 정렬, 공동 학습 및 MaaS(model-as-a-service)를 포함하는 AGI를 추구할 때 여전히 다른 과제가 있다는 점에 유의한다[2].

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline
**Datasets** & **Year** & **Scale** & **Modalities** & **Paper** \\ \hline COCO & 2014 & 567K & Image-Text & [63] \\ \hline Visual Genome & 2017 & 5.4M & Image-Text & [64] \\ \hline YouCook2 & 2018 & 2.2K & Video-Text & [65] \\ \hline WebVid2M & 2021 & 2.5M & Video-Text & [66] \\ Common Voice & 2019 & 9.2K & Audio-Text & [67] \\ \hline LibriSpeech & 2015 & 1K & Audio-Text & [68] \\ \hline M5Product & 2021 & 6M & Image-Text-Video-Audio & [69] \\ \hline MSR-VTT & 2016 & 10K & Image-Text-Video-Audio & [70] \\ \hline \end{tabular}
\end{table} TABLE II: The multimodal datasets

## VII Conclusion

멀티모달 모델의 발전은 바이너리 기계가 다양한 데이터 유형을 이해하고 처리할 수 있는 AI의 새로운 길을 열었다. 멀티모달 모델은 가까운 미래에 더 포괄적이고 지능적인 시스템으로 이어질 것이다. 우리는 멀티모달 모델 개발에 대한 포괄적인 탐색을 제공했습니다. 우리는 먼저 멀티모달 개념을 소개하고 멀티모달 알고리즘의 역사적 발전을 정리했다. 그 후, 멀티모달 제품 개발에 있어 주요 기술 기업의 노력에 대해 논의하고 멀티모달 모델의 기술적 측면에 대한 통찰력을 제공했다. 우리는 또한 귀중한 실험 및 평가 자원을 제공할 수 있는 일반적으로 사용되는 데이터 세트의 컴파일을 제시했다. 마지막으로, 멀티모달 모델 개발과 관련된 문제를 강조하고 추가 연구를 위해 논의했다. 이러한 측면을 다루면서, 본 논문은 다양한 도메인에서 멀티모달 모델과 그 잠재적 특성에 대한 더 깊은 이해를 제공하는 것을 목표로 한다.

## Acknowledgment

이 연구는 중국 국립 자연 과학 재단(62002136 및 62272196)에 의해 부분적으로 지원되었다. , PZL2021KF0023호인 Pazhou Lab.의 Young Scholar Program. 원성 간 박사는 이 논문의 교신 저자이다.

## References

*[1]W. 간진 Qi, J. Wu, and J. C. W. Lin (2023) Large language models in education: vision and opportunities. IEEE International Conference on Big Data, pp. 1-10. Cited by: SSI.
*[2]W. 간석 Wan, and P. S. Yu (2023) Model-as-a-service (MaaS): survey. IEEE International Conference on Big Data, pp. 1-10. Cited by: SSI.
*[3]W. 간진 Qi, J. Wu, and J. C. W. Lin (2023) Large language models in education: vision and opportunities. IEEE International Conference on Big Data, pp. 1-10. Cited by: SSI.
*[4]W. 간석 Wan, and P. S. Yu (2023) Model-as-a-service (MaaS): survey. IEEE International Conference on Big Data, pp. 1-10. Cited by: SSI.
*[5]W. 간진 Qi, J. Wu, and J. C. W. Lin (2023) Large language models in education: vision and opportunities. IEEE International Conference on Big Data, pp. 1-10. Cited by: SSI.
*[6]W. 간진 Qi, J. Wu, and J. C. W. Lin (2023) Large language models in education: vision and opportunities. IEEE International Conference on Big Data, pp. 1-10. Cited by: SSI.
*[7]W. 간석 Wan, and P. S. Yu (2023) Model-as-a-service (MaaS): survey. IEEE International Conference on Big Data, pp. 1-10. Cited by: SSI.
*[8]W. 간진 Qi, J. Wu, and J. C. W. Lin (2023) Large language models in education: vision and opportunities. IEEE International Conference on Big Data, pp. 1-10. Cited by: SSI.
*[9]W. 간석 Wan, and P. S. Yu (2023) Model-as-a-service (MaaS): survey. IEEE International Conference on Big Data, pp. 1-10. Cited by: SSI.
*[10]W. 간석 Wan, and P. S. Yu (2023) Model-as-a-service (MaaS): survey. IEEE International Conference on Big Data, pp. 1-10. Cited by: SSI.
*[11]W. 간진 Qi, J. Wu, and J. C. W. Lin (2023) Large language models in education: vision and opportunities. IEEE International Conference on Big Data, pp. 1-10. Cited by: SSI.
*[12]W. 간석 Wan, and P. S. Yu (2023) Model-as-a-service (MaaS): survey. IEEE International Conference on Big Data, pp. 1-10. Cited by: SSI.

[MISSING_PAGE_POST]

인 것을 특징으로 하는 반도체 소자의 제조방법. Wan, and P. S. Yu (2023) Model-as-a-service (MaaS): survey. IEEE International Conference on Big Data, pp. 1-10. Cited by: SSI.
*[37]W. 간진 Qi, J. Wu, and J. C. W. Lin (2023) Large language models in education: vision and opportunities. IEEE International Conference on Big Data, pp. 1-10. Cited by: SSI.
*[38]W. 간석 Wan, and P. S. Yu (2023) Model-as-a-service (MaaS): survey. IEEE International Conference on Big Data, pp. 1-10. Cited by: SSI.
*[39]W. 간석 Wan, and P. S. Yu (2023) Model-as-a-service (MaaS): survey. IEEE International Conference on Big Data, pp. 1-10. Cited by: SSI.
*[40]W. 간석 Wan, and P. S. Yu (2023) Model-as-a-service (MaaS): survey. IEEE International Conference on Big Data, pp. 1-10. Cited by: SSI.
*[41]W. 간엑스 Li, A. M. Shoib, and J. Abdul (2022) A review on methods and application in multimodal deep learning. ACM Transactions on Multimedia Computing, Communications, and Applications19 (28), pp. 761-764. Cited by: SSI.
*[42]J. 라일린 장광 이근 Toutanova (2018) BERT: 언어 이해를 위한 심층 양방향 변압기 사전 훈련. The Conference of the North American Chapter of the Association for Computational Linguistics, pp. 4171-4186. Cited by: SSI.
*[43]T. 미콜로프 Chen, G. Corrado, and J. Dean (2013) Efficient estimation of word representation in vector space. 제1차 국제 학습 표상에 관한 회의에서 SSII-A를 인용하였다.
*[44]K. Bostrom and G. Durrett (2020) Byte pair 인코딩은 언어 모델 사전 훈련에 있어서 차선책이다. In Findings of the Association for Computational Linguistics, pp. 4617-4624. Cited by: SSII-A.
*[45]T. 양영 왕영 Lu, N. Zheng (2022) Visual concepts tokenization. 신경정보처리시스템에 관한 회의에서는 SSII-A를 인용하였다.
*[46]Z. 두영 서진 간종수 왕락 왕창주 원남 Peng, et al.(2022) An empirical study of training end-to-end vision-and-language transformers. The IEEE Conference on Computer Vision and Pattern Recognition, pp. 1866-1876. Cited by: SSII-A.
*[47]J. 라오진 산락 류영 Zhou and Y. Yang(2023) Retrieval-based Knowledge augmented vision language pre-training. The 31st ACM International Conference on Multimedia, pp. 539-5409. Cited by: SSII-A.
*[48]J. 라오진 산락 류영 Zhou and Y. Yang(2023) Retrieval-based Knowledge augmented vision language pre-training. The 31st ACM International Conference on Multimedia, pp. 539-549. Cited by: SSII-A.
*[49]J. 서영 서진 간종수 왕락 왕창주 원남 Peng, et al.(2022) An empirical study of training end-to-end vision-and-language transformers. The IEEE Conference on Computer Vision and Pattern Recognition, pp. 1866-1876. Cited by: SSII-A.
*[50]J. 서진 신락 류영 Zhou and Y. Yang(2023)*[34] Z. 왕진우 대영 Tsvetkov, Y. Cao, "SimVLM: Simple visual language model pretraining with weak supervision" in _The 10th International Conference on Learning Representations_, 2022.
* [35] Z. Wang, W. Wang, H. Zhu, M. Liu, B. Qin, and F. Wei, "Distilled dual-encoder model for vision-language understanding," in _Empirical Methods in Natural Language Processing_, 2022, pp. 8901-8913.
* [36] H. Bao, W. Wang, L. Dong, Q. Liu, O. K. Mohammed, K. Aggarwal, S. Som, S. Piao, and F. Wei, "Vlmo: Unified vision-language pre-training with mixture-of-modality-experts," _Advances in Neural Information Processing Systems_, vol. 35, pp. 32 897-32 912, 2022.
* [37] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. El-nashar, J. Spencer-Smith, and D. C. Schmidt, "A prompt pattern catalog to enhance prompt engineering with ChatGPT," _arXiv preprint, arXiv:2302.11382_, 2023.
* [38] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, and N. Duan, "Visual ChatGPT: Talking, drawing and editing with visual foundation models," _arXiv preprint, arXiv:2303.04671_, 2023.
* [39] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need," _Advances in Neural Information Processing Systems_, vol. 30, 2017.
* [40] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly _et al._, "An image is worth 16x16 words: Transformers for image recognition at scale," in _9th International Conference on Learning Representations_, 2021.
* [41] Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu, M. Zeng, and L. Wang, "MM-REACT: Prompting chtgrd for multimodal reasoning and action," _arXiv preprint, arXiv:2303.11381_, 2023.
* [42] M. Tsimpoukelli, J. L. Menick, S. Cabi, S. Eslami, O. Vinyals, and F. Hill, "Multimodal few-shot learning with frozen language models," _Advances in Neural Information Processing Systems_, vol. 34, pp. 200-212, 2021.
* [43] J. Li, D. Li, S. Savarese, and S. Hoi, "BLIP-2: Bootstrapping language pre-training with frozen image encoders and large language models," in _International Conference on Machine Learning_, 2023, pp. 19 730-19 742.
* [44] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, and Y. Qiao, "LiLAMA-Adapter: Efficient fine-tuning of language models with zero-init attention," _arXiv preprint, arXiv:2303.16199_, 2023.
* [45] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, "MiniGPT-4: Enhancing vision-language understanding with advanced large language models," _arXiv preprint, arXiv:2304.10592_, 2023.
* [46] H. Liu, C. Li, Q. Wu, and Y. J. Lee, "Visual instruction tuning," _arXiv preprint, arXiv:2304.08485_, 2023.
* [47] Z. Yang, Z. Gan, J. Wang, X. Hu, Y. Lu, Z. Liu, and L. Wang, "An empirical study of GPT-3 for few-shot knowledge-based vqa," in _The AAAI Conference on Artificial Intelligence_, 2022, pp. 3081-3089.
* [48] A. M. H. Tiong, J. Li, B. Li, S. Savarese, and S. C. Hoi, "Plug-and-play VOA: Zero-shot vqa by conjoining large pretrained models with zero training," in _Findings of the Association for Computational Linguistics_, 2022, pp. 951-967.
* [49] J. Guo, J. Li, D. Li, A. M. H. Tiong, B. Li, D. Tao, and S. C. Hoi, "From images to textual prompts: Zero-shot VQA with frozen large language models," _arXiv preprint, arXiv:2212.10846_, 2022.
* [50] D. Gurari, Y. Zhao, M. Zhang, and N. Bhattacharya, "Captioning images taken by people who are blind," in _The 16th European Conference on Computer Vision_. Springer, 2020, pp. 471-474.
* [51] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans _et al._, "Photorealistic text-to-image diffusion models with deep language understanding," _Advances in Neural Information Processing Systems_, vol. 35, pp. 36 479-36 494, 2022.
* [52] S. Albanie, G. Varol, L. Momeni, T. Afouras, J. S. Chung, N. Fox, and A. Zisserman, "BSL-1K: Scaling up co-articulated sign language recognition using mouthing cues," in _The 16th European Conference on Computer Vision_. Springer, 2020, pp. 35-53.
* [53] J. Forster, C. Schmidt, O. Koller, M. Bellgardt, and H. Ney, "Extensions of the sign language recognition and translation corpus RWTH-PHOENIX-Weather," in _International Conference on Computational Linguistics, Language Resources and Evaluation_, 2014, pp. 1911-1916.
* [54] S. Zhao, G. Jia, J. Yang, G. Ding, and K. Keutzer, "Emotion recognition from multiple modalities: Fundamentals and methodologies," _IEEE Signal Processing Magazine_, vol. 38, no. 6, pp. 59-73, 2021.
* [55] A. Aljamaki, Y.-H. Yang, and M. Soleymani, "Developing a benchmark for emotional analysis of music," _The Public Library of Science_, vol. 12, no. 3, p. e0173392, 2017.
* [56] S. Yan, T. Zhu, Z. Wang, Y. Cao, M. Zhang, S. Ghosh, Y. Wu, and J. Yu, "Video-text modeling with zero-shot transfer from contrastive captioners," _arXiv preprint, arXiv:2212.04979_, 2022.
* [57] H. Xu, G. Ghosh, P.-Y. Huang, D. Okhonko, A. Aghajanyan, F. Metze, L. Zettlemoyer, and C. Feichtenhofer, "VideoCILP: Contrastive pre-training for zero-shot video-text understanding," in _Empirical Methods in Natural Language Processing_, 2021, pp. 6787-6800.
* [58] H. Xu, Q. Ye, M. Yan, Y. Shi, J. Ye, X. Xu, C. Li, B. Bi, Q. Qian, W. Wang _et al._, "mPLUQ:A 2: A modularized multi-modal foundation model across text, image and video," in _International Conference on Machine Learning_, 2023, pp. 38 728-38 748.
* [59] A. Agostinelli, T. I. Denk, Z. Borosos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi _et al._, "MusicM: Generating music from text," _arXiv preprint, arXiv:2301.11325_, 2023.
* [60] A. Richard, M. Zollhofer, Y. Wen, F. De la Torre, and Y. Sheikh, "Meshtalk: 3D face animation from speech using cross-modality disentanglement," in _The IEEE International Conference on Computer Vision_, 2021, pp. 1173-1182.
* [61] J. Wu, W. Gan, Z. Chen, S. Wan, and H. Lin, "AI-generated content (AIGC): A survey," _arXiv preprint, arXiv:2304.06632_, 2023.
* [62] K. Prajwal, R. Mukhopadhyay, V. P. Nambodri, and C. Jawahar, "A lily sync expert is all you need for speech to lip generation in the wild," in _The 28th ACM International Conference on Multimedia_, 2020, pp. 484-492.
* [63] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick, "Microsoft COCO: Common objects in context," in _The 13th European Conference on Computer Vision_. Springer, 2014, pp. 740-755.
* [64] R. Krishna, Y. Zhu, Q. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma _et al._, "Visual genome: Connecting language and vision using crowdsourced dense image annotations," _International Journal of Computer Vision_, vol. 123, pp. 32-73, 2017.
* [65] L. Zhou, C. Xu, and J. Corso, "Towards automatic learning of procedures from web instructional videos," in _The AAAI Conference on Artificial Intelligence_, vol. 32, no. 1, 2018.
* [66] M. Bain, A. Nagrani, G. Varol, and A. Zisserman, "Frozen in time: A joint video and image encoder for end-to-end retrieval," in _The IEEE International Conference on Computer Vision_, 2021, pp. 1728-1738.
* [67] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber, "Common voice: A massively-multilingual speech corpus," in _The 12th Language Resources and Evaluation Conference_, 2020, pp. 4218-4222.
* [68] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, "Librispeech: an asr corpus based on public domain audio books," in _IEEE International Conference on Acoustics, Speech and Signal Processing_. IEEE, 2015, pp. 5206-5210.
* [69] X. Dong, X. Zhan, Y. Wu, Y. Wei, X. Wei, M. Lu, and X. Liang, "MSProduct: A multi-modal pretraining benchmark for e-commercial product downstream tasks," _arXiv preprint, arXiv:2109.04275_, 2021.
* [70] J. Xu, T. Mei, T. Yao, and Y. Rui, "MSR-VTT: A large video description dataset for bridging video and language," in _The IEEE Conference on Computer Vision and Pattern Recognition_, 2016, pp. 5288-5296.
* [71] S. Katsigiannis and N. Ramzan, "DREAM: A database for emotion recognition through eeg and egc signals from wireless low-cost off-the-shelf devices," _IEEE Journal of Biomedical and Health Informatics_, vol. 22, no. 1, pp. 98-107, 2017.
* [72] F. Zeng, W. Gan, Y. Wang, and P. S. Yu, "Distributed training of large language models," in _The 29th IEEE International Conference on Parallel and Distributed Systems_. IEEE, 2023, pp. 1-8.
* [73] Z. Chen and B. Liu, _Lifelong machine learning_. Springer, 2018, vol. 1.
* [74] F. Zenke, B. Poole, and S. Ganguli, "Continual learning through synaptic
