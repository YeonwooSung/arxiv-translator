# Mamba: Selective State Spaces를 사용한 Linear-Time Sequence Modeling

Albert Gu

동등한 기여. 카네기멜론대학교 기계학습학과

Tri Dao

동등한 기여. 프린스턴대학교 컴퓨터학과

agu@cs.cmu.edu, tri@tridao.me

###### Abstract

현재 딥 러닝에서 대부분의 흥미로운 응용 프로그램에 전력을 공급하는 기반 모델은 트랜스포머 아키텍처와 핵심 주의 모듈을 기반으로 거의 보편적이다. 트랜스포머의 긴 시퀀스에 대한 계산 비효율성을 해결하기 위해 선형 어텐션, 게이트 컨볼루션 및 리커런트 모델, 구조화된 상태 공간 모델(SSM)과 같은 많은 준 2차 시간 아키텍처가 개발되었지만 언어와 같은 중요한 모달리티에 대한 어텐션뿐만 아니라 수행되지 않았다. 우리는 이러한 모델의 주요 약점이 내용 기반 추론을 수행할 수 없다는 것을 식별하고 몇 가지 개선을 수행한다. 첫째, SSM 파라미터들을 단순히 입력의 함수들이 되게 하는 것은 이산 모달리티들로 그들의 약점을 해결하여, 모델이 현재 토큰에 의존하여 시퀀스 길이 차원을 따라 정보를 선택적으로 전파하거나 잊어버릴 수 있게 한다. 둘째, 이러한 변화가 효율적인 컨볼루션의 사용을 방해함에도 불구하고, 본 논문에서는 순환 모드에서의 하드웨어 인식 병렬 알고리즘을 설계한다. 이러한 선택적 SSM을 주의력이나 MLP 블록(Mamba) 없이 단순화된 엔드 투 엔드 신경망 구조에 통합한다. Mamba는 빠른 추론(트랜스포머보다 5배 높은 처리량)과 시퀀스 길이의 선형 스케일링을 즐기며, 최대 백만 길이의 실제 데이터에서 성능이 향상된다. 일반적인 시퀀스 모델 백본으로서 맘바는 언어, 오디오 및 유전체학과 같은 여러 양식에 걸쳐 최첨단 성능을 달성한다. 언어 모델링에서 Mamba-3B 모델은 동일한 크기의 트랜스포머보다 성능이 우수하며 사전 훈련 및 다운스트림 평가 모두에서 트랜스포머의 크기를 두 배로 일치시킨다.

## 1 Introduction

파운데이션 모델(Foundation Model, FM)은 대규모 데이터를 사전 학습한 후 다운스트림 작업에 맞게 적응하는 대규모 모델로 현대 기계 학습의 효과적인 패러다임으로 부상했다. 이들 FM의 백본은 종종, 언어, 이미지, 스피치, 오디오, 시계열, 및 유전체학과 같은 매우 다양한 도메인으로부터의 입력들의 임의의 시퀀스들 상에서 동작하는 시퀀스 모델들이다(Brown et al., 2020; Dosovitskiy et al., 2020; Ismail Fawaz et al., 2019; Oord et al., 2016; Poli et al., 2023; Sutskever, Vinyals, and Quoc V Le, 2014). 이 개념은 모델 아키텍처의 특정 선택에 불가지론적이지만, 현대의 FM은 주로 단일 유형의 시퀀스 모델: 트랜스포머(Vaswani et al., 2017) 및 그 핵심 주의 계층(Bahdanau, Cho, and Bengio, 2015)에 기초한다. 자기 주의의 효능은 컨텍스트 윈도우 내에서 정보를 조밀하게 라우팅하는 능력에 기인하여 복잡한 데이터를 모델링할 수 있다. 그러나, 이러한 특성은 유한 윈도우 외부의 어떤 것도 모델링할 수 없고, 윈도우 길이에 대한 이차 스케일링이라는 근본적인 단점을 가져온다. 이러한 단점을 극복하기 위해 주의의 보다 효율적인 변형에 대한 엄청난 연구가 나타났지만(Tay, Dehghani, Bahri, et al., 2022), 종종 이를 효과적으로 만드는 매우 많은 속성을 희생시킨다. 아직까지 이러한 변이체 중 어느 것도 도메인에 걸쳐 규모에서 경험적으로 효과적인 것으로 나타나지 않았다.

최근, 구조화된 상태 공간 시퀀스 모델(SSM: structured state space sequence models)(Gu, Goel, and Re, 2022; Gu, Johnson, Goel, et al., 2021)이 시퀀스 모델링을 위한 유망한 아키텍처의 클래스로 부상하였다. 이러한 모델은 고전적 상태 공간 모델(Kalman, 1960)로부터의 영감과 함께 순환 신경망(RNN) 및 컨볼루션 신경망(CNN)의 조합으로 해석될 수 있다. 이 모델의 클래스는 시퀀스 길이에서 선형 또는 거의 선형 스케일링을 사용하여 반복 또는 컨볼루션으로 매우 효율적으로 계산할 수 있다. 또한, 이들은 특정 데이터 모달리티에서 장거리 종속성을 모델링하기 위한 원칙적인 메커니즘(Gu, Dao, et al., 2020)을 가지고 있으며, Long Range Arena(Tay, Dehghani, Abnar, et al., 2021)와 같은 벤치마크를 지배하고 있다. SSM의 많은 맛(Gu, Goel, and Re, 2022; Gu, Gupta, et al., 2022; Gupta, Gu, and Berant, 2022; Y. Li 등, 2023; Ma 등, 2023; Orvieto 등, 2023; Smith, Warrington, and Linderman, 2023)은 오디오 및 비전과 같은 연속적인 신호 데이터를 포함하는 도메인에서 성공적이었다 (Goel 등, 2022; Nguyen, Goel, et al., 2022; Saon, Gupta, and Cui, 2023). 그러나 이들은 텍스트와 같은 이산적이고 정보 밀도가 높은 데이터를 모델링하는 데 덜 효과적이었다.

본 논문에서는 수열길이를 선형적으로 조절하면서 변압기의 모델링 파워를 얻기 위해 여러 축에 대한 선행 연구를 개선한 새로운 종류의 선택적 상태공간 모델을 제안한다.

선택 메커니즘.먼저, 우리는 입력 의존적 방식(즉, 특정 입력에 초점을 맞추거나 무시하는 방식)으로 데이터를 효율적으로 선택하는 능력이라는 선행 모델의 주요 제한을 식별한다. 선택적 복사 및 유도 헤드와 같은 중요한 합성 작업을 기반으로 하는 직관을 기반으로 입력을 기반으로 SSM 매개변수를 매개변수화하여 간단한 선택 메커니즘을 설계한다. 이를 통해 모델은 관련 없는 정보를 필터링하고 관련 정보를 무한정 기억할 수 있습니다.

하드웨어 인식 알고리즘.이 간단한 변경은 모델의 계산에 기술적인 문제를 제기한다. 사실, 모든 이전의 SSM 모델은 계산 효율적이기 위해서는 시간과 입력에 불변해야 한다. 이를 하드웨어 인식 알고리즘으로 극복하여 컨볼루션 대신 스캔을 통해 모델을 반복적으로 계산하지만 GPU 메모리 계층 구조의 서로 다른 레벨 간의 IO 액세스를 피하기 위해 확장된 상태를 구체화하지 않는다. 결과 구현은 이론적으로 (모든 컨볼루션 기반 SSM에 대해 의사 선형에 비해 시퀀스 길이에서 선형으로 스케일링) 및 최신 하드웨어에서 (A100 GPU에서 최대 3\(\times\) 더 빠르다.

아키텍처.우리는 이전의 SSM 아키텍처(Dao, Fu, Saab, et al., 2023)의 설계와 Transformers의 MLP 블록을 하나의 블록으로 결합함으로써 이전의 딥 시퀀스 모델 아키텍처를 단순화하고, 선택적인 상태 공간을 통합하는 단순하고 균질한 아키텍처 설계(Mamba)로 이어진다.

선택적 SSM 및 확장으로 맘바 아키텍처는 시퀀스에서 작동하는 일반적인 기초 모델의 백본으로 적합하도록 만드는 핵심 속성을 가진 완전 반복 모델이다. (i) 고품질: 선택성은 언어 및 유전체학과 같은 조밀한 양식에서 강력한 성능을 가져온다. (ii) 빠른 트레이닝 및 추론: 연산 및 메모리는 트레이닝 동안 시퀀스 길이에서 선형적으로 스케일링하고, 추론 동안 모델을 자동으로 언롤링하는 것은 이전 요소들의 캐시를 필요로 하지 않기 때문에 단계당 일정한 시간만을 필요로 한다. (iii) 긴 문맥: 품질과 효율은 함께 시퀀스 길이 1M까지의 실제 데이터에 대한 성능 개선을 산출한다.

우리는 여러 유형의 양식과 설정에서 사전 훈련 품질과 도메인별 작업 성능 모두에서 일반적인 시퀀스 FM 백본으로서의 맘바의 잠재력을 경험적으로 검증한다.

* 합성학. 대형 언어 모델의 핵심으로 제안된 복사 및 인덕션 헤드와 같은 중요한 합성 작업에서 맘바는 쉽게 해결할 수 있을 뿐만 아니라 무한히 긴 솔루션(>1M 토큰)을 추론할 수 있다.
* 오디오 및 유전체학. Mamba는 사전 훈련 품질 및 다운스트림 메트릭 모두에서 오디오 파형 및 DNA 서열 모델링에 대해 SaShiMi, Hyena 및 Transformers와 같은 이전 최신 모델을 능가한다(예: 도전적인 음성 생성 데이터 세트의 FID를 절반 이상 감소). 두 설정 모두에서 최대 백만 길이의 시퀀스가 더 긴 컨텍스트로 성능이 향상됩니다.
* 언어 모델링. Mamba는 사전 훈련 복잡성과 다운스트림 평가 모두에서 트랜스포머 품질 성능을 진정으로 달성하는 최초의 선형 시간 시퀀스 모델이다. 최대 1B 파라미터까지 스케일링 법칙을 사용하여, Mamba가 LLaMa에 기초한 매우 강력한 현대 트랜스포머 트레이닝 레시피를 포함하여 광범위한 베이스라인의 성능을 초과한다는 것을 보여준다(Touvron et al., 2023). 제안하는 Mamba 언어 모델은 비슷한 크기의 Transformers에 비해 5\(\times\)의 생성 처리율을 가지며, Mamba-3B의 품질은 Transformers의 2배(예: Pythia-3B에 비해 상식 추론에서 4포인트 더 높고 심지어 Pythia-7B를 초과함)와 일치한다.

모델 코드 및 미리 훈련된 검사점은 [https://github.com/state-spaces/mamba](https://github.com/state-spaces/mamba)에서 공개 소스됩니다.

## 2 State Space Models

구조화된 상태 공간 시퀀스 모델(S4)은 RNN, CNN 및 고전적 상태 공간 모델과 광범위하게 관련된 딥 러닝을 위한 시퀀스 모델의 최근 클래스이다. 그들은 1차원 함수나 수열 \(x(t)\in\mathbb{R}\mapsto y(t)\in\mathbb{R}\)을 암시적 잠재 상태 \(h(t)\in\mathbb{R}^{N}\)을 통해 매핑하는 특정한 연속 시스템(1)에서 영감을 얻었다.

구체적으로 S4 모델은 4개의 매개 변수 \((\Delta,\mathbf{A},\mathbf{B},\mathbf{C})\)으로 정의되며, 이는 두 단계의 시퀀스 간 변환을 정의한다.

\[h^{\prime}(t) =\mathbf{A}h(t)+\mathbf{B}x(t) \tag{1a}\] \[y(t) =\mathbf{C}h(t) \tag{1b}\]

\[h_{t} =\overline{\mathbf{A}}h_{t-1}+\overline{\mathbf{B}}x_{t} \tag{2a}\] \[\overline{\mathbf{K}} =(\mathbf{C}\overline{\mathbf{B}},\mathbf{C}\overline{\mathbf{A}\overline{\mathbf{B}},...,\mathbf{C}\overline{\mathbf{A}}\overline{\mathbf{B}},...)\] (3a) \[y_{t} =\mathbf{C}h_{t} \tag{2b}\]

이산화(Discretization) 첫 번째 단계는 고정된 수식 \(\overline{\mathbf{A}}=f_{A}(\Delta,\mathbf{A},\mathbf{B})\)과 \(\overline{\mathbf{B}}=f_{B}(\Delta,\mathbf{A},\mathbf{B})\)을 통해 "연속 파라미터" \((\Delta,\mathbf{A},\mathbf{B})\)를 이산 파라미터로 변환한다. 여기서 쌍 \(((f_{A},f_{B})\)을 이산화 규칙이라 한다. 식 (4)에 정의된 영차 홀드(ZOH)와 같이 다양한 규칙이 사용될 수 있다.

\[\overline{\mathbf{A}}=\exp(\Delta\mathbf{A})\qquad\overline{\mathbf{B}}=(\Delta\mathbf{A})^{-1 }(\exp(\Delta\mathbf{A})-\mathbf{I})\cdot\Delta\mathbf{B} \tag{4}\)

이산화는 해상도 불변(Nguyen, Goel, et al., 2022)과 같은 추가적인 속성을 부여할 수 있고 모델이 적절하게 정규화되도록 자동으로 보장할 수 있는 연속 시간 시스템에 깊은 연결을 갖는다(Gu, Johnson, Timalsina, et al., 2023; Orvieto et al., 2023). 또한 RNN(Gu, Gulcehre, et al., 2020; Tallec and Ollivier, 2018)의 게이팅 메커니즘과 연결되어 있으며, 섹션 3.5에서 다시 논의할 것이다. 그러나 기계적 관점에서 이산화는 SSM의 순방향 패스에서 계산 그래프의 첫 번째 단계로 간단히 볼 수 있다. SSM의 대체 맛은 이산화 단계를 우회하고 \((\overline{\mathbf{A}},\overline{\mathbf{B}})\) 대신 직접 매개변수화할 수 있으며(Zhang et al., 2023), 이는 추론하기에 더 쉬울 수 있다.

계산.매개변수를 \((\Delta,\mathbf{A},\mathbf{B},\mathbf{C})\mapsto(\overline{\mathbf{A}},\overline{\mathbf{B}},\mathbf{C})\)로 변환한 후, 선형 반복(2)과 전역 컨볼루션(3)의 두 가지 방법으로 모델을 계산할 수 있다.

그림 1: (**개요.**) 구조화된 SSM은 입력 \(x\)의 각 채널 (예: \(D=5\))을 고차원 잠재 상태 \(h\)(예: \(N=4\))을 통해 \(y\) 출력으로 독립적으로 매핑합니다. 이전의 SSM은 시간-불변성을 필요로 하는 영리한 대체 계산 경로를 통해 이 큰 유효 상태(\(DN\), 배치 크기 \(B\) 및 시퀀스 길이 \(L\))를 구체화하는 것을 피한다. \((\Delta,\mathbf{A},\mathbf{B},\mathbf{C})\) 매개변수는 시간에 따라 일정하다. 우리의 선택 메커니즘은 입력 의존적인 역학을 추가하며, 또한 GPU 메모리 계층 구조의 보다 효율적인 수준에서 확장된 상태만을 구체화하기 위해 신중한 하드웨어 인식 알고리즘이 필요하다.

일반적으로, 모델은 효율적인 병렬화 트레이닝(전체 입력 시퀀스가 시간 이전에 보이는 경우)을 위해 컨볼루션 모드(3)를 사용하고, 효율적인 자기회귀 추론을 위해 순환 모드(2)로 전환한다(입력들이 한 번에 하나의 타임스테프에서 보이는 경우).

선형 시간 불변(Linear Time Invariance, LTI) 식 (1)~(3)의 중요한 성질은 시간의 흐름에 따라 모델의 동역학이 일정하다는 것이다. 즉, \((\Delta,\mathbf{A},\mathbf{B},\mathbf{C})\), 그리고 결과적으로 \((\overline{\mathbf{A}},\overline{\mathbf{B}})\)도 모든 시간 단계에 대해 고정된다. 이러한 성질을 선형 시간 불변(Linear Time Invariance, LTI)이라 하며, 이는 재발 및 컨볼루션과 깊은 관련이 있다. 공식적으로 LTI SSM을 선형 반복(2a) 또는 컨볼루션(3b)과 동등한 것으로 생각하고 LTI를 이러한 모델 클래스에 대한 포괄적인 용어로 사용한다.

지금까지 구조화된 SSM은 섹션 3.3에서 논의된 기본적인 효율성 제약으로 인해 LTI(예: 컨볼루션으로 계산됨)가 있었다. 그러나 이 작업의 핵심 통찰력은 LTI 모델이 특정 유형의 데이터를 모델링하는 데 근본적인 한계가 있다는 것이며 우리의 기술적 기여는 효율성 병목 현상을 극복하면서 LTI 제약을 제거하는 것이다.

마지막으로, 구조화 SSM을 효율적으로 계산하려면 \(\mathbf{A}\) 행렬에 구조를 부과해야 하기 때문에 구조화 SSM이 명명되었음을 주목한다. 가장 인기 있는 구조의 형태는 대각선(Gu, Gupta, et al., 2022; Gupta, Gu, and Berant, 2022; Smith, Warrington, and Linderman, 2023)이며, 이 역시 우리가 사용한다.

이 경우, \(\mathbf{A}\in\mathbb{R}^{N\times N},\mathbf{B}\in\mathbb{R}^{N\times 1},\mathbf{C}\in\mathbb{R}^{1\times N}\) 행렬은 모두 \(N\)개의 숫자로 표현될 수 있다. 배치 크기 \(B\)와 길이 \(L\)의 입력 시퀀스 \(x\)에서 \(D\) 채널로 동작하기 위해 SSM은 각 채널에 독립적으로 적용된다. 이 경우, 전체 은닉 상태는 입력당 차원 \(D\!N\)을 가지며, 시퀀스 길이에 걸쳐 계산하려면 \(O(BLDN)\) 시간과 메모리가 필요하며, 이것은 섹션 3.3에서 다루는 기본 효율 병목 현상의 근이다.

일반 상태 공간 모델. 우리는 상태 공간 모델이라는 용어가 단순히 잠재 상태를 가진 순환 과정의 개념을 나타내는 매우 광범위한 의미를 가지고 있다는 점에 주목한다. 마르코프 결정 프로세스(MDP)(강화 학습(Hafner et al., 2020)), 동적 인과 모델링(DCM)(컴퓨팅 신경과학(Friston, Harrison, and Penny, 2003)), 칼만 필터(제어(Kalman, 1960)), 은닉 마르코프 모델(HMM) 및 선형 역학 시스템(LDS)(기계 학습) 및 대규모(딥 러닝)에서 순환(및 때때로 컨볼루션) 모델을 포함하는 다양한 분야의 많은 이질적인 개념을 지칭하는 데 사용되었다.

이 전체 논문 전체에 걸쳐 우리는 구조화된 SSM 또는 S4 모델의 클래스(Gu, Goel, and Re, 2022; Gu, Gupta, et al., 2022; Gupta, Gu, and Berant, 2022; Hasani, et al., 2023; Ma, et al., 2023; Smith, Warrington, and Linderman, 2023)를 배타적으로 지칭하기 위해 "SSM"이라는 용어를 사용하고 이러한 용어를 상호 교환적으로 사용한다. 편의상, 선형-재발생 또는 글로벌-컨볼루션 관점들 중 어느 하나에 초점을 맞추는 것들과 같은 그러한 모델들의 도함수들을 또한 포함할 수 있다(Y. Li et al., 2023; Orvieto et al., 2023; Poli et al., 2023). 그리고 필요할 때 뉘앙스를 명확히 한다.

SSM 아키텍처.SSM은 엔드 투 엔드 신경망 아키텍처에 통합될 수 있는 독립형 시퀀스 변환이다. (우리는 또한 때때로 SSM 아키텍처를 SSNN이라고 부르는데, 이는 CNN은 선형 컨볼루션 계층이고, SSM 계층은 선형 컨볼루션 계층이다.) 가장 잘 알려진 SSM 아키텍처 중 일부에 대해 논의하며, 이들 중 다수는 또한 우리의 기본 베이스라인으로서 기능할 것이다.

* 선형 주의력(Katharopoulos et al., 2020)은 퇴행성 선형 SSM으로 볼 수 있는 재발을 포함하는 자기 주의력의 근사치이다.
* H3(Dao, Fu, Saab, et al., 2023)는 S4를 사용하기 위해 이러한 재발을 일반화했다; 이는 두 개의 게이트 연결에 의해 샌드위치된 SSM을 갖는 아키텍처로 볼 수 있다(도 3). H3는 또한 그들이 쉬프트-SSM으로 프레임하는 표준 로컬 콘볼루션을 메인 SSM 계층 앞에 삽입한다.
* Hyena(Poli et al., 2023)는 H3와 동일한 아키텍처를 사용하지만 S4 계층을 MLP-파라미터화된 글로벌 컨볼루션(Romero et al., 2021)으로 대체한다.
* RetNet (Y). Sun 등, 2023)은 아키텍처에 추가 게이트를 추가하고 더 간단한 SSM을 사용하여, 컨볼루션 대신에 멀티헤드 어텐션(MHA)의 변형을 사용하여, 대안적인 병렬가능 계산 경로를 허용한다.

* RWKV (B. Peng et al., 2023) 는 또 다른 선형 주의 근사(attention-free Transformer(S. Zhai et al., 2021))에 기초하여 언어 모델링을 위해 설계된 최근의 RNN이다. 주요 "WKV" 메커니즘은 LTI 재발을 포함하며 두 SSM의 비율로 볼 수 있다.

다른 밀접하게 관련된 SSM 및 아키텍처는 확장된 관련 작업(부록 B)에서 더 논의된다. 특히 S5(Smith, Warrington, and Linderman, 2023), QRNN(Bradbury et al., 2016), SRU(Lei et al., 2017)를 강조하며, 이는 핵심 선택적 SSM과 가장 밀접하게 관련된 방법으로 본다.

## 3 선택적 상태 공간 모델

우리는 합성 작업의 직관을 사용하여 선택 메커니즘에 동기를 부여한 다음(섹션 3.1), 이 메커니즘을 상태 공간 모델에 통합하는 방법을 설명한다(섹션 3.2). 결과적인 시변 SSM은 컨볼루션을 사용할 수 없으므로 컨볼루션을 효율적으로 계산하는 방법에 대한 기술적 과제를 제시한다. 이를 현대 하드웨어의 메모리 계층을 이용하는 하드웨어 인식 알고리즘(섹션 3.3)으로 극복한다. 이어서, 주목 또는 심지어 MLP 블록들 없이 간단한 SSM 아키텍처를 설명한다(섹션 3.4). 마지막으로 선택 메커니즘의 몇 가지 추가 속성에 대해 논의한다(섹션 3.5).

### 동기: 압축 수단으로 선택

우리는 시퀀스 모델링의 근본적인 문제가 컨텍스트를 더 작은 상태로 압축하는 것이라고 주장한다. 사실, 우리는 이러한 관점에서 인기 있는 서열 모델의 절충안을 볼 수 있다. 예를 들어, 주의는 문맥을 전혀 압축하지 않기 때문에 효과적이고 비효율적이다. 이는 자기 회귀 추론이 전체 컨텍스트(즉, KV 캐시)를 명시적으로 저장해야 한다는 사실에서 알 수 있으며, 이는 변압기의 느린 선형 시간 추론과 2차 시간 학습을 직접적으로 야기한다. 반면에, 순환 모델은 유한한 상태를 가지기 때문에 효율적이며, 이는 상수-시간 추론 및 선형-시간 훈련을 암시한다. 그러나, 이들의 효과는 이 상태가 컨텍스트를 얼마나 잘 압축시켰는지에 의해 제한된다.

이 원리를 이해하기 위해 합성 과제의 두 가지 실행 사례에 초점을 맞춘다(그림 2).

* 선택적 복사 작업은 기억할 토큰의 위치를 변경 하 여 인기 있는 복사 작업 (Arjovsky, Shah 및 Bengio, 2016)을 수정 합니다. 관련된 토큰(색상)을 암기하고 관련 없는 토큰(흰색)을 걸러낼 수 있는 내용 인식 추론이 필요하다.
* Induction Heads 작업은 LLM의 컨텍스트 내 학습 능력의 대부분을 설명하는 것으로 가정된 잘 알려진 메커니즘입니다 (Olsson 등, 2022). 적절한 컨텍스트(흑색)에서 정확한 출력을 언제 생성할지를 알기 위해서는 컨텍스트 인식 추론이 필요하다.

이러한 작업은 LTI 모델의 고장 모드를 보여준다. 반복적인 관점에서 볼 때, (2)의 \((\overline{\mathbf{A}},\overline{\mathbf{B}})\) 전이)의 상수 동역학은 문맥에서 올바른 정보를 선택하거나 입력 의존적인 방식으로 시퀀스를 따라 전달되는 숨겨진 상태에 영향을 줄 수 없다. 컨벌루션 관점에서 볼 때, 글로벌 컨벌루션은 시간-인식만을 필요로 하기 때문에 바닐라 복사 작업(Romero et al., 2021)을 해결할 수 있지만, 내용-인식이 부족하여 선택 복사 작업에 어려움을 겪는 것으로 알려져 있다(그림 2). 보다 구체적으로, 입력-출력 사이의 간격은 가변적이며 정적 컨볼루션 커널에 의해 모델링될 수 없다.

요약하면 효율성 대 효율성입니다. 시퀀스 모델의 효과적인 절충은 상태를 얼마나 잘 압축하는지를 특징으로 하며, 효율적인 모델은 작은 상태를 가져야 하는 반면 효과적인 모델은 컨텍스트에서 필요한 모든 정보를 포함하는 상태를 가져야 한다. 순차 모델을 구축하기 위한 기본 원리는 선택성 또는 순차 상태로 입력에 초점을 맞추거나 필터링할 수 있는 상황 인식 능력이라고 제안한다. 특히, 선택 메커니즘은 정보가 시퀀스 차원을 따라 어떻게 전파되거나 상호작용하는지를 제어한다(더 많은 논의를 위해 섹션 3.5 참조).

### Selection을 사용 하 여 SSMs 개선

선택 메커니즘을 모델에 통합하는 한 가지 방법은 시퀀스를 따른 상호작용(예: RNN의 순환 역학 또는 CNN의 컨볼루션 커널)에 영향을 미치는 매개변수가 입력 종속되도록 하는 것이다.

알고리즘 1과 2는 우리가 사용하는 주요 선택 메커니즘을 보여준다. 주요 차이점은 단순히 입력의 여러 매개변수 \(\Delta,\mathbf{B},\mathbf{C}\) 함수와 텐서 모양의 관련 변화를 만드는 것이다. 특히, 이 매개변수들은 이제 길이 차원 \(L\)을 가지고 있으며, 이는 모델이 시불변에서 시변으로 변경되었음을 의미한다. (형상 주석은 섹션 2에 설명되어 있습니다.) 이것은 다음에 논의되는 효율성에 대한 함축과 함께 컨볼루션(3)과의 동등성을 상실한다.

특히, \(s_{\mathbf{S}}(x)=\mathsf{Linear}_{N}(x)\), \(s_{\mathbf{C}}(x)=\mathsf{Linear}_{N}(x)\), \(s_{\mathbf{\Delta}}(x)=\mathsf{Broadcast}_{D}(\mathsf{Linear}_{1}(x))\), \(\tau_{\Delta}=\mathsf{softplus}\)을 선택하는데, 여기서 \(\mathsf{Linear}_{d}\)은 차원 \(d\)으로의 매개변수화된 투영이다. \(s_{\mathbf{\Delta}}\) 및 \(\tau_{\Delta}\)의 선택은 섹션 3.5에 설명된 RNN 게이팅 메커니즘과의 연결 때문이다.

### 선택적 SSM의 효율적인 구현

컨볼루션(Krizhevsky, Sutskever, and Hinton, 2012) 및 트랜스포머(Vaswani et al., 2017)와 같은 하드웨어 친화적인 아키텍처는 광범위한 적용을 즐긴다. 본 논문에서는 선택적 SSM을 현대 하드웨어(GPU)에서도 효율적으로 구현하고자 한다. 선택 메커니즘은 매우 자연스러우며, 초기 연구에서는 반복 SSM(Gu, Dao, et al., 2020)에서 \(\Delta\)이 시간에 따라 달라지도록 하는 것과 같은 특별한 선택 사례를 통합하려고 시도했다. 그러나 이전에 언급한 바와 같이 SSM 사용의 핵심 한계는 계산 효율이며, 이는 S4와 모든 파생물이 가장 일반적으로 전역 컨볼루션 형태로 LTI(비선택적) 모델을 사용한 이유이다.

#### 3.3.1 이전 모델의 동기

우리는 먼저 이 동기를 재고하고 이전 방법의 한계를 극복하기 위한 접근법을 개관한다.

* 높은 수준에서 SSM과 같은 반복 모델은 표현성과 속도 간의 균형을 항상 조정합니다. 섹션 3.1에서 논의 된 것처럼 숨겨진 상태 차원이 더 큰 모델은 더 효과적이지만 느려야 합니다. 따라서,

그림 2: (_Left_) 복사 작업의 표준 버전은 입력 요소와 출력 요소 사이의 일정한 간격을 포함하며 선형 반복 및 전역 컨볼루션과 같은 시간 불변 모델에 의해 쉽게 해결된다. (_오른쪽 상단_) 선택적 복사 작업은 입력 간에 임의 간격을 가지며 내용에 따라 입력을 _선택적으로_ 기억하거나 무시할 수 있는 시변 모델이 필요합니다. (_Right Bottom_) Induction Heads 태스크는 LLMs에 대한 핵심 능력인 컨텍스트에 기초하여 답변을 검색해야 하는 연관 회상의 예이다.

우리는 속도와 메모리 비용을 지불하지 않고 숨겨진 상태 차원을 최대화하고 싶습니다.
* 후자(3)가 전자(2)의 확장으로부터 도출되기 때문에, 리커런트 모드는 컨볼루션 모드보다 더 유연하다는 것에 유의한다(Gu, Goel, and Re, 2022; Gu, Johnson, Goel, et al., 2021). 그러나, 이것은 잠재 상태 \(h\)를 형상(\(\text{B},\text{L},\text{D},\text{N}\))으로 계산하고 구체화해야 하며, 형상(\(\text{B},\text{L},\text{D}\))의 입력 \(x\)과 출력 \(y\)보다 훨씬 큰 값(\(N\), SSM 상태 차원)을 필요로 한다. 따라서 상태 계산을 우회하고 \((\text{B},\text{L},\text{D})\)의 컨볼루션 커널(3a)을 구체화할 수 있는 보다 효율적인 컨볼루션 모드가 도입되었다.
* 이전 LTI SSM은 이중 반복-컨볼루션 형식을 활용하여 효율성 패널티 없이 기존 RNN보다 훨씬 큰 \(N\)(\(\approx 10-100\))의 효과 상태 차원을 증가시킵니다.

#### 3.3.2 Selective Scan 개요: Hardware-Aware State Expansion

선택 메커니즘은 LTI 모델의 한계를 극복하기 위해 설계되었으며, 따라서 SSM의 계산 문제를 다시 논의할 필요가 있다. 우리는 이것을 커널 융합, 병렬 스캔 및 재계산이라는 세 가지 고전적인 기술로 다룬다. 우리는 두 가지 주요 관찰을 한다.

* 순진한 반복 계산은 \(\text{O}(\textit{BLDN})\) FLOP를 사용 하는 반면 컨볼루션 계산은 \(\text{O}(\textit{BLD}\log(L))\) FLOP를 사용 하며 전자는 상수 인자가 더 낮습니다. 따라서 긴 시퀀스 및 너무 크지 않은 상태 차원 \(N\)의 경우, 리커런트 모드는 실제로 더 적은 FLOP를 사용할 수 있다.
* 두 가지 문제는 반복의 순차적 특성과 큰 메모리 사용량입니다. 후자를 다루기 위해, 컨벌루션 모드와 마찬가지로, 우리는 실제로 전체 상태 \(h\)를 구체화하지 않도록 시도할 수 있다.

주요 아이디어는 현대 가속기(GPU)의 특성을 활용하여 메모리 계층 구조의 보다 효율적인 수준에서만 상태를 구체화하는 것이다. 특히, 대부분의 연산(행렬 곱셈 제외)은 메모리 대역폭에 의해 경계지어진다(Dao, Fu, Ermon, et al., 2022; Ivanov et al., 2021; Williams, Waterman, and Patterson, 2009). 여기에는 스캔 작업이 포함되며 커널 융합을 사용하여 메모리 IO의 양을 줄여 표준 구현에 비해 상당한 속도가 향상됩니다.

구체적으로, GPU HBM(high-bandwidth memory)에서 크기 \((\text{B},\text{L},\text{D},\text{N})\)의 스캔 입력(\(\overline{\mathbf{A}},\overline{\mathbf{B}}\))을 준비하는 대신 SSM 파라미터 \((\mathbf{\Delta},\mathbf{A},\mathbf{B},\mathbf{C})\)를 느린 HBM에서 빠른 SRAM으로 직접 로딩하고 SRAM에서 이산화 및 반복을 수행한 후 크기 \((\text{B},\text{L},\text{D})\)의 최종 출력을 HBM에 다시 기록한다.

순차적인 재발을 피하기 위해, 선형이 아님에도 불구하고, 일-효율적인 병렬 스캔 알고리즘(Blelloch, 1990; Martin and Cundy, 2018; Smith, Warrington, and Linderman, 2023)으로 여전히 병렬화될 수 있음을 관찰한다.

마지막으로 역전파에 필요한 중간 상태를 저장하는 것도 피해야 한다. 우리는 메모리 요구량을 줄이기 위해 고전적인 재계산 기법을 조심스럽게 적용한다. 중간 상태는 저장되지 않고 입력이 HBM에서 SRAM으로 로드될 때 백워드 패스에서 재계산된다. 결과적으로, 융합 선택 스캔 레이어는 FlashAttention을 이용한 최적화된 트랜스포머 구현과 동일한 메모리 요구 사항을 갖는다.

융합된 커널 및 재계산에 대한 자세한 내용은 부록 D에 있다. 전체 선택적 SSM 계층 및 알고리즘은 그림 1에 나와 있다.

### 단순화된 SSM 아키텍처

구조화된 SSM과 마찬가지로 선택적 SSM은 신경망에 유연하게 통합될 수 있는 독립형 시퀀스 변환이다. H3 아키텍처는 일반적으로 MLP(multi-layer perceptron) 블록과 인터리빙된 선형 주의에서 영감을 얻은 블록으로 구성되는 가장 잘 알려진 SSM 아키텍처의 기초이다(섹션 2). 우리는 이 두 구성 요소를 하나로 결합하여 이 아키텍처를 단순화하며, 이는 균질하게 쌓인다(그림 3). 이것은 주의 집중을 위해 유사한 것을 한 게이티드 어텐션 유닛(GAU)(Hua et al., 2022)에서 영감을 받았다.

이 아키텍처는 제어 가능한 확장 인자 \(E\)에 의해 모델 차원 \(D\)을 확장하는 것을 포함한다. 각 블록에 대해 대부분의 매개변수(\(3ED^{2}\))는 입력 투영의 경우 선형 투영(\(2ED^{2}\), 출력 투영의 경우 \(ED^{2}\)에 있는 반면 내부 SSM은 기여도가 적다. SSM 매개 변수(\(\mathbf{\Delta},\mathbf{B},\mathbf{C}\)와 행렬 \(\mathbf{A}\))의 수는 훨씬 적다. 우리는 표준 정규화 및 잔여 연결로 인터리빙된 이 블록을 반복하여 맘바 아키텍처를 형성한다. 실험에서 우리는 항상 \(E=2\)으로 고정하고, 두 개의 블록 스택을 사용하여 변압기의 인터리빙된 MHA (multi-head attention) 블록과 MLP 블록의 \(12D^{2}\) 파라미터를 일치시킨다. 우리는 SiLU/Swish 활성화 함수(Hendrycks and Gimpel, 2016; Ramachandran, Zoph, and Quoc V Le, 2017)를 사용하여, Gated MLP가 인기 있는 "SwiGLU" 변이체가 되도록 동기 부여하였다(Chowdhery et al., 2023; Shazeer, 2020; Touvron et al., 2023). 마지막으로, RetNet의 유사한 위치에서의 정규화 계층 사용에 의해 동기화된 선택적 정규화 계층(LayerNorm(J. L. Ba, Kiros, and Hinton, 2016)을 추가로 사용한다(Y. Sun et al., 2023).

### 선택 메커니즘의 특성

선택 메커니즘은 더 전통적인 RNN들 또는 CNN들, 상이한 파라미터들(예를 들어, 알고리즘 2의 \(\mathbf{A}\)), 또는 상이한 변환들 \(\mathbf{s}(\mathbf{x})\)을 사용하는 것과 같은 상이한 방식들로 적용될 수 있는 더 넓은 개념이다.

#### 3.5.1 Gating Mechanisms 연결

우리는 가장 중요한 연결을 강조하는데, RNN의 고전적인 게이팅 메커니즘은 SSM에 대한 선택 메커니즘의 예이다. RNN 게이팅과 연속시간 시스템의 이산화 사이의 연결이 잘 확립되어 있다는 점에 주목한다(후나하시와 나카무라, 1993; Tallec와 Ollivier, 2018). 실제로, 정리 1은 ZOH 이산화 및 입력 종속 게이트로 일반화하는 Gu, Johnson, Goel, et al.(2021, Lemma 3.1)의 개선이다(부록 C에서 증명). 더 넓게는 SSM의 \(\Delta\)은 RNN 게이팅 메커니즘의 일반화된 역할을 하는 것으로 볼 수 있다. 선행 연구에 따르면, 우리는 SSM의 이산화가 휴리스틱 게이팅 메커니즘의 원칙적인 기초라는 견해를 채택한다.

**정리 1**.: _(N=1,\mathbf{A}=-1,\mathbf{B}=1,s_{\Delta}=\texttt{Linear}(\mathbf{x})\), \(\tau_{\Delta}=\texttt{softplus}\)일 때 선택적 SSM 되풀이(알고리즘 2)는 형식_을 사용합니다.

\[\begin{split} g_{t}&=\sigma(\texttt{Linear}(\mathbf{x}_ {t}))\\ h_{t}&=(1-g_{t})h_{t-1}+g_{t}\mathbf{x}_{t}.\end{split} \tag{5}\]

섹션 3.2에서 언급했듯이 \(s_{\Delta},\tau_{\Delta}\)의 구체적인 선택은 이 연결에서 비롯된다. 특히 주어진 입력 \(\mathbf{x}_{t}\)을 완전히 무시해야 하는 경우(합성 작업에서 필요한 경우) 모든 \(D\) 채널은 이를 무시해야 하므로 \(\Delta\)으로 반복/방송하기 전에 입력을 1차원으로 투영한다.

도 3: (**아키텍처.**) 우리의 단순화된 블록 설계는 대부분의 SSM 아키텍처의 기초가 되는 H3 블록과 현대 신경망의 유비쿼터스 MLP 블록을 결합한다. 이 두 블록을 인터리빙하는 대신, 우리는 단순히 맘바 블록을 균질하게 반복한다. H3 블록과 비교하여 맘바는 첫 번째 곱셈 게이트를 활성화 함수로 대체한다. MMP 블록에 비해 맘바는 메인 브랜치에 SSM을 추가한다. \(\sigma\)의 경우 SiLU/Swish 활성화(Hendrycks and Gimpel, 2016; Ramachandran, Zoph, and Quoc V Le, 2017)를 사용한다.

#### 3.5.2 선택 메커니즘의 해석

우리는 선택의 두 가지 특정 기계론적 효과에 대해 자세히 설명한다.

가변 간격.선택성을 사용하면 관심 입력 간에 발생할 수 있는 관련 없는 노이즈 토큰을 필터링할 수 있습니다. 이것은 선택 복사 태스크에 의해 예시되지만, 특히 이산 데이터에 대해, 예를 들어 "um"과 같은 언어 필러의 존재와 같은 공통 데이터 모달리티에서 편재적으로 발생한다. 이 성질은 모델이 특정한 입력 \(\mathsf{x}_{t}\)을 기계적으로 필터링할 수 있기 때문에 발생한다. 예를 들어, \(\mathsf{g}_{t}\to 0\)일 때 게이트 RNN의 경우(정리 1)이다.

Filtering Context. 많은 시퀀스 모델들이 더 긴 컨텍스트에 따라 개선되지 않는다는 것이 경험적으로 관찰되었다 (F. Shi et al., 2023). 더 많은 맥락이 엄격하게 더 나은 수행으로 이어져야 한다는 원칙에도 불구하고. 설명은 많은 시퀀스 모델이 필요할 때 관련 없는 컨텍스트를 효과적으로 무시할 수 없다는 것이다; 직관적인 예는 전역 컨볼루션(및 일반 LTI 모델)이다. 한편, 선택적 모델들은 외부 이력을 제거하기 위해 그들의 상태를 언제든지 간단히 리셋할 수 있고, 따라서 그들의 성능은 원칙적으로 문맥 길이에 따라 단조롭게 개선된다(예를 들어, 섹션 4.3.2).

경계 재설정.다수의 독립적인 시퀀스들이 함께 스티칭되는 설정들에서, 트랜스포머들은 특정 주의 마스크를 인스턴스화함으로써 그들을 분리되게 유지할 수 있는 반면, LTI 모델들은 시퀀스들 사이에서 정보를 블리드할 것이다. 선택적 SSM은 경계(예: \(\Delta_{t}\to\infty\) 또는 \(\mathsf{g}_{t}\to1\))에서 상태를 재설정할 수도 있습니다. 이러한 설정은 인위적으로(예를 들어, 하드웨어 활용도를 향상시키기 위해 문서를 함께 패킹) 발생하거나 자연스럽게(예를 들어, 강화 학습에서의 에피소드 경계(Lu 등, 2023)) 발생할 수 있다.

또한 각 선택적 매개변수의 효과에 대해 자세히 설명한다.

\(\Delta\)의 해석 일반적으로 \(\Delta\)은 현재 입력(\mathsf{x}_{t}\)에 얼마나 초점을 맞추거나 무시할 것인가의 균형을 제어한다. 정리 1에서 RNN 게이트(예: \(\mathsf{g}_{t}\)를 일반화하고, 기계적으로 큰 \(\Delta\)은 상태 \(h\)를 재설정하고 현재 입력 \(\mathsf{x}\)에 초점을 맞춘 반면 작은 \(\Delta\)은 상태를 유지하고 현재 입력을 무시한다. SSM (1)-(2)는 시간단계 \(\Delta\)에 의해 이산화된 연속적인 시스템으로 해석될 수 있으며, 이러한 맥락에서 직관은 큰 \(\Delta\to\infty\)은 현재 입력에 초점을 맞추는 시스템을 더 오래(따라서 "선택"하고 현재 상태를 잊는 것) 나타내는 반면 작은 \(\Delta\to0\)은 무시되는 과도 입력을 나타낸다.

\(A\)의 해석.\(A\) 매개변수도 선택적일 수 있지만, \(\overline{\mathbf{A}}=\mathsf{exp}(\Delta\mathbf{A})\)를 통한 \(\Delta\)와의 상호 작용을 통해서만 궁극적으로 모델에 영향을 미친다는 점을 언급하였다. 따라서 \(\Delta\)의 선택성은 \((\overline{\mathbf{A}},\overline{\mathbf{B}})\)의 선택성을 보장하기에 충분하며 개선의 주요 원인이다. 우리는 \(A\)를 선택적으로 만들거나 \(\Delta\ 대신) \(\Delta\을 선택하는 것이 유사한 성능을 가질 것이라고 가정하고 단순화를 위해 생략한다.

\(B\)와 \(C\)의 해석 3.1절에서 논의한 바와 같이 선택성의 가장 중요한 속성은 시퀀스 모델의 컨텍스트가 효율적인 상태로 압축될 수 있도록 관련 없는 정보를 필터링하는 것이다. SSM에서 입력 \(\mathsf{x}_{t}\)을 상태 \(\mathbf{h}_{t}\)로 할 것인지, 출력 \(\mathsf{y}_{t}\)으로 할 것인지에 대한 세밀한 제어가 가능하다. 이들은 모델이 각각 콘텐츠(입력) 및 컨텍스트(숨겨진 상태)에 기초하여 순환 역학을 변조할 수 있게 하는 것으로 해석될 수 있다.

### 추가 모델 세부 정보

진짜 vs. 복소.대부분의 이전 SSM은 상태 \(h\)에서 복소수를 사용하는데, 이는 많은 태스크(Gu, Goel, and Re, 2022)에서 강력한 성능을 위해 필요하다. 그러나, 완전히 실치된 SSM들이 일부 설정들에서 양호하고, 아마도 더 양호하게 작동하는 것으로 보인다는 것이 경험적으로 관찰되었다(Ma 등, 2023). 우리는 실수 값을 기본값으로 사용하여 작업 중 하나를 제외한 모든 작업에 잘 작동하며, 복소-실제 트레이드오프는 데이터 모달리티의 연속 이산 스펙트럼과 관련이 있다고 가정하며, 여기서 복소 수는 연속 모달리티(예: 오디오, 비디오)에는 도움이 되지만 이산(예: 텍스트, DNA)에는 도움이 되지 않는다.

초기화.대부분의 이전 SSM은 특히 복소 값의 경우 특수 초기화를 제안하며, 이는 낮은 데이터 레짐과 같은 여러 설정에서 도움이 될 수 있습니다. 컴플렉스 케이스에 대한 우리의 디폴트 초기화는 S4D-Lin이고, 실제 케이스에 대한 우리의 디폴트 초기화는 HIPPO 이론에 기초한 S4D-Real(Gu, Gupta, et al.2022)이다(Gu, Dao, et al.2020). 이들은 각각 \(A\)의 \(n\)-번째 원소를 \(-1/2+ni\)과 \(-(n+1)\)으로 정의한다. 그러나 많은 초기화가 특히 대규모 데이터 및 실제 값 SSM 체제에서 잘 작동할 것으로 예상하며 일부 삭제는 섹션 4.6에서 고려된다.

\(\Delta\)의 매개변수화는 \(\Delta\)에 대한 선택적 조정을 \(s_{\Delta}(x)=\mathsf{Broadcast}_{D}(\mathsf{Linear}_{1}(x))\으로 정의하였으며, 이는 \(\Delta\)의 역학에 의해 동기화되었다(섹션 3.5). 우리는 그것이 차원 \(1\)에서 더 큰 차원 \(R\)으로 일반화될 수 있음을 관찰한다. 우리는 이것을 블록의 주요 선형 사영에 비해 무시할 수 있는 수의 매개변수를 사용하는 \(D\)의 작은 부분으로 설정했다. 또한 방송 연산은 \(1\)'s와 \(0\)'s의 특정 패턴으로 초기화된 다른 선형 프로젝션으로 볼 수 있으며, 이 프로젝션이 훈련 가능하면 낮은 순위의 프로젝션으로 볼 수 있는 대안 \(s_{\Delta}(x)=\mathsf{Linear}_{D}(\mathsf{Linear}_{R}(x))\으로 이어진다.

실험에서는 SSM(Gu, Johnson, Timalsina, et al.2023)에 대한 선행 연구를 통해 \(\Delta\) 파라미터(바이어스 항으로 볼 수 있음)를 \(\tau_{\Delta}^{-1}(\mathsf{Uniform}([0.001,0.1])\으로 초기화하였다.

**주목 3.1**.: _실험 결과의 간결성을 위해 선택 메커니즘을 사용 하 고 스캔으로 계산 하는 S4 모델이기 때문에 선택적 SSM을 S6 모델로 줄이기도 합니다._

## 4 경험적 평가

4.1절에서는 3.1절에서 동기화된 두 가지 합성 과제를 해결하는 맘바의 능력을 테스트한다. 그 다음 세 가지 영역에 대해 평가하고, 각각 자기 회귀 사전 훈련과 다운스트림 작업에 대해 평가한다.

* 섹션 4.2: 언어 모델 사전 트레이닝(스케일링 법칙), 제로샷 다운스트림 평가.
* 섹션 4.3: DNA 서열 사전 훈련 및 긴 서열 분류 작업에 대한 미세 조정.
* 섹션 4.4: 오디오 파형 사전 트레이닝, 및 자동 생성 음성 클립의 품질.

마지막으로, 섹션 4.5는 학습 및 추론 시간 모두에서 Mamba의 계산 효율을 보여주고, 섹션 4.6은 아키텍처의 다양한 구성요소와 선택적 SSM을 제거한다.

### Synthetic Tasks

작업 세부 정보 및 훈련 프로토콜을 포함한 이러한 작업에 대한 전체 실험 세부 정보는 부록 E.1에 있다.

#### 4.1.1 Selective Copying

복사 작업은 시퀀스 모델링을 위해 가장 잘 연구된 합성 작업 중 하나이며, 원래 순환 모델의 암기 능력을 테스트하기 위해 설계되었다. 섹션 3.1에서 논의한 바와 같이 LTI SSM(선형 반복 및 전역 컨볼루션)은 데이터에 대한 추론 대신 시간 추적만 유지하면 이 작업을 쉽게 해결할 수 있으며, 예를 들어 정확하게 오른쪽 길이의 컨볼루션 커널을 구성한다(그림 2). 이는 글로벌 컨볼루션에 대한 이전 작업(Romero et al.2021)에서 명시적으로 검증되었다. 선택 복사 작업은 토큰 간의 간격을 랜덤화하여 바로 가기를 방지합니다. 이 작업은 이전에 디노이징 작업(Jing et al.2019)으로 소개된 바 있음에 유의한다.

많은 이전 연구에서 아키텍처 게이팅(다중 상호작용)을 추가하면 "데이터 의존성" 모델을 부여하고 관련 작업을 해결할 수 있다고 주장한다(Dao, Fu, Saab, et al.2023; Poli et al.2023). 그러나 이러한 게이팅은 시퀀스 축을 따라 상호 작용하지 않고 토큰 간의 간격에 영향을 줄 수 없기 때문에 직관적으로 이 설명이 불충분하다는 것을 발견했다. 특히 아키텍처 게이팅은 선택 메커니즘(부록 A)의 인스턴스가 아니다.

표 1은 H3 및 Mamba와 같은 게이티드 아키텍처가 성능을 부분적으로만 향상시키는 반면, 선택 메커니즘(S4 내지 S6을 수정)은 특히 이러한 보다 강력한 아키텍처와 결합될 때 이 작업을 쉽게 해결한다는 것을 확인한다.

#### 4.1.2 Induction Heads

유도 헤드(Olsson et al., 2022)는 기계론적 해석 가능성 렌즈(Elhage et al., 2021)로부터의 간단한 작업으로서, LLM의 인-컨텍스트 학습 능력을 놀라울 정도로 예측한다. 연관 리콜 및 복사를 수행하기 위해 모델이 필요하다: 예를 들어, 모델이 시퀀스에서 "해리 포터"와 같은 바이그램을 보았다면, 다음에 "해리"가 동일한 시퀀스에 나타날 때, 모델은 역사로부터 복사함으로써 "포터"를 예측할 수 있어야 한다.

Dataset.We trained 2-layer model on the induction head task at sequence length 256, and a vocab size of 16, this task(Dao et al., 2023)에 대한 선행 작업과 유사하지만 더 긴 sequence를 사용한다. 또한 테스트 시간에 \(2^{6}=64\)에서 \(2^{20}=1048576\)까지의 시퀀스 길이의 범위를 평가하여 일반화 및 외삽 능력을 추가로 조사한다.

모델.유도 헤드들에 대한 확립된 작업에 이어, 유도 헤드 태스크를 기계적으로 해결할 수 있도록 주의할 수 있는 2층 모델을 사용한다(Olsson et al., 2022). 다중 헤드 주의력(8헤드, 다양한 위치 인코딩이 있는)과 SSM 변형을 모두 테스트한다. Mamba의 경우 64, 다른 모델의 경우 128의 모델 차원 \(D\)을 사용한다.

결과.표 2는 맘바(또는 더 정확하게는 선택적 SSM 계층)가 그 사이의 다른 모든 것을 무시하면서 관련 토큰을 선택적으로 기억하는 능력 때문에 작업을 완벽하게 해결할 수 있는 능력을 가지고 있음을 보여준다. 이 방법은 100만 길이의 시퀀스를 완벽하게 일반화하거나 훈련 중에 본 것보다 훨씬 긴 \(4000\mathsf{x}\)을 일반화하는 반면, \(2\mathsf{x}\)을 넘어서는 다른 방법은 없다.

어텐션 모델에 대한 위치 인코딩 변형에서 xPos(길이 외삽을 위해 설계된)는 다른 것보다 약간 더 우수하며, 모든 어텐션 모델은 메모리 제한으로 인해 시퀀스 길이 \(2^{14}=16384\)까지만 테스트되었다. 다른 SSM 중 H3와 Hyena는 Poli 등(2023)의 결과와 달리 유사하다.

### Language Modeling

Mamba 아키텍처를 다른 아키텍처에 비해 표준 자기회귀 언어 모델링, 사전 훈련 메트릭(복잡성) 및 제로 샷 평가 모두에서 평가한다. 모델 크기(깊이 및 너비)를 GPT3 사양을 반영하도록 설정합니다. Pile 데이터 세트(L)를 사용합니다. Gao 등, 2020), Brown 등(2020)에 기재된 훈련 레시피를 따른다. 모든 교육 세부 사항은 부록 E.2에 나와 있다.

#### 4.2.1 Scaling Laws

기본선의 경우 PaLM과 LLaMaarchitectures(예: 회전 임베딩, SwiGLU MLP, LayerNorm 대신 RMSNorm, 선형 바이어스 없음, 더 높은 학습률)를 기반으로 표준 트랜스포머 아키텍처(GPT3 아키텍처)와 우리가 아는 가장 강력한 트랜스포머 레시피(여기서는 트랜스포머++라고 함)를 비교한다. 우리는 또한 최근의 다른 2차 구조들과 비교한다(그림 4). 모든 모델 세부 정보는 부록 E.2에 나와 있습니다.

그림 4는 표준 Chinchilla(Hoffmann et al., 2022) 프로토콜에서 \(\approx 125M\)에서 \(\approx 1.3B\) 매개변수까지의 모델에 대한 스케일링 법칙을 보여준다. 맘바는 특히 시퀀스 길이가 증가함에 따라 이제 표준이 된 매우 강력한 트랜스포머 레시피(트랜스포머++)의 성능과 일치하는 최초의 주의력 없는 모델이다. 메모리 부족 또는 비현실적인 계산 요구 사항으로 이어지는 효율적인 구현의 부족으로 인해 SSM으로도 해석될 수 있는 이전의 강력한 반복 모델인 RWKV 및 RetNet 기준선에는 컨텍스트 길이 8k에 대한 전체 결과가 누락되어 있다.

#### 4.2.2 Downstream Evaluations

표 3은 인기 있는 다운스트림 제로샷 평가 과제의 범위에 대한 맘바의 성능을 보여준다. 우리는 이러한 크기, 가장 중요한 피티아(Biderman et al., 2023) 및 RWKV(B)에서 가장 잘 알려진 오픈 소스 모델과 비교한다. Peng et al., 2023)은 우리의 모델과 동일한 토큰화기, 데이터 세트 및 훈련 길이(300B 토큰)로 훈련되었다. (Mamba 및 Pythia는 컨텍스트 길이 2048로 트레이닝되는 반면, RWKV는 컨텍스트 길이 1024로 트레이닝된다는 점에 유의한다.)

### DNA Modeling

대규모 언어 모델의 성공에 힘입어 최근 유전체학을 위한 기초 모델 패러다임을 사용하는 탐색이 있었다. DNA는 유한한 어휘를 가진 이산 토큰의 시퀀스로 구성되어 있다는 점에서 언어에 비유되어 왔다. 또한 모델에 장거리 종속성을 요구하는 것으로 알려져 있다(Avsec 등, 2021). 우리는 DNA에 대한 롱-시퀀스 모델에 대한 최근 연구와 동일한 환경에서 사전 훈련 및 미세 조정을 위한 FM 백본으로서 맘바를 조사한다(Nguyen, Poli, et al., 2023). 특히 모델 크기와 시퀀스 길이에 걸친 스케일링 법칙의 두 가지 탐색(그림 5)과 긴 컨텍스트가 필요한 어려운 다운스트림 합성 분류 작업(그림 6)에 중점을 둔다.

사전 훈련의 경우, 훈련 및 모델 세부사항에 대한 표준 인과 언어 모델링(다음 토큰 예측) 설정을 주로 따른다(부록 E.2 참조). 데이터세트의 경우, 훈련 분할에서 약 45억 토큰(DNA 염기쌍)을 갖는 단일 인간 게놈으로 구성된 사전 훈련을 위해 HG38 데이터세트를 사용하는 HyenaDNA(Nguyen, Poli, et al., 2023)의 설정을 크게 따른다.

#### 4.3.1 Scaling: 모델 크기

본 실험에서는 다양한 모델 백본을 가진 유전체 기초 모델의 스케일링 특성을 조사한다(그림 5 왼쪽).

훈련.기준선을 활용하기 위해 1024의 짧은 시퀀스 길이로 훈련하며 섹션 4.3.2에서 볼 수 있듯이 결과가 더 긴 시퀀스 길이로 맘바를 훨씬 선호할 것으로 예상한다. 우리는 1024의 글로벌 배치 크기를 고정합니다.

그림 4: **(Scaling Laws.) 크기 \(\approx 125M\)에서 \(\approx 1.3B\) 매개 변수의 모델, 파일에서 학습되었습니다. Mamba는 다른 모든 주의 없는 모델보다 더 잘 확장되며, 특히 시퀀스 길이가 증가함에 따라 이제 표준이 된 매우 강력한 "Transformer++" 레시피의 성능과 가장 먼저 일치합니다.* *

일괄 처리당 총 \(2^{20}\approx 1M\) 토큰입니다. 모델은 총 \(10B\) 토큰에 대해 \(10K\) 구배 단계에 대해 훈련되었다.

결과. 그림 5(왼쪽)는 Mamba의 사전 훈련 복잡성이 모델 크기에 따라 부드럽게 개선되고 Mamba가 HyenaDNA와 Transformer++보다 더 잘 스케일링된다는 것을 보여준다. 예를 들어, \(\approx 40M\) 파라미터들의 가장 큰 모델 크기에서, 곡선은 Mamba가 대략 \(3\times\) 내지 \(4\times\) 더 적은 파라미터들로 Transformer++ 및 HyenaDNA 모델들을 매칭할 수 있음을 보여준다.

#### 4.3.2 Scaling: Context Length

다음 DNA 실험에서 우리는 서열 길이에 대한 모델의 스케일링 특성을 조사한다. 2차 주의가 더 긴 서열 길이에서 엄청나게 비싸지기 때문에 하이에나DNA와 맘바 모델만 비교한다. 시퀀스 길이 \(2^{10}=1024\), \(2^{12}=4096\), \(2^{14}=16384\), \(2^{16}=65536\), \(2^{18}=262144\), \(2^{20}=1048576\)에 대한 모델을 사전 학습하였다. 6개 레이어의 모델 크기를 128(약 1.3M-1.4M 파라미터)로 고정한다. 모델은 총 \(\approx 330B\) 토큰에 대해 \(20K\) 구배 단계에 대해 훈련되었다. 더 긴 서열 길이는 (Nguyen, Poli, et al.2023)과 유사한 서열 길이 워밍업을 사용하였다.

결과.그림 5(오른쪽)은 Mamba가 길이가 1M인 매우 긴 시퀀스까지 더 긴 컨텍스트를 사용할 수 있으며 컨텍스트가 증가함에 따라 사전 훈련 복잡성이 향상됨을 보여준다. 반면에, HyenaDNA 모델은 서열 길이에 따라 악화된다. 이것은 선택 메커니즘의 속성에 대한 섹션 3.5의 논의에서 직관적이다. 특히, LTI 모델들은 정보를 선택적으로 무시할 수 없다; 컨볼루션 관점에서, 매우 긴 컨볼루션 커널은 긴 시퀀스에 걸쳐 모든 정보를 집계하고 있다

\begin{table}
\begin{tabular}{l l l l l l l l l l l} \hline \hline Model & Token. & Pile & LAMBADA & LAMBADA & HellaSwag & PIQA & Arc-E & Arc-C & WinoGrande & Average \\  & & ppl \(\downarrow\) & ppl \(\downarrow\) & acc \(\uparrow\) & acc \(\uparrow\) & acc \(\uparrow\) & acc \(\uparrow\) & acc \(\uparrow\) & acc \(\uparrow\) & acc \(\uparrow\) \\ \hline Hybrid H3-130M & GPT2 & — & 89.48 & 25.77 & 31.7 & 64.2 & 44.4 & 24.2 & 50.6 & 40.1 \\ Pythia-160M & NeoX & 29.64 & 38.10 & 33.0 & 30.2 & 61.4 & 43.2 & 24.1 & **51.9** & 40.6 \\
**Mamba-130M** & NeoX & **10.56** & **16.07** & **44.3** & **35.3** & **64.5** & **48.0** & **24.3** & **51.9** & **44.7** \\ \hline Hybrid H3-360M & GPT2 & — & 12.58 & 48.0 & 41.5 & 68.1 & 51.4 & 24.7 & 54.1 & 48.0 \\ Pythia-410M & NeoX & 9.95 & 10.84 & 51.4 & 40.6 & 66.9 & 52.1 & 24.6 & 53.8 & 48.2 \\
**Mamba-370M** & NeoX & **8.28** & **8.14** & **55.6** & **46.5** & **69.5** & **55.1** & **28.0** & **55.3** & **50.0** \\ \hline Pythia-1B & NeoX & 7.82 & 7.92 & 56.1 & 47.2 & 70.7 & 57.0 & 27.1 & 53.5 & 51.9 \\
**Mamba-790M** & NeoX & **7.33** & **6.02** & **62.7** & **55.1** & **72.1** & **61.2** & **29.5** & **56.1** & **57.1** \\ \hline GPT-Neo 1.3B & GPT2 & — & 7.50 & 57.2 & 48.9 & 71.1 & 56.2 & 25.9 & 54.9 & 52.4 \\ Hybrid H3-1.3B & GPT2 & — & 11.25 & 49.6 & 52.6 & 71.3 & 59.2 & 28.1 & 56.9 & 53.0 \\ OPT-1.3B & OPT & — & 6.64 & 58.0 & 53.7 & 72.4 & 56.7 & 29.6 & 59.5 & 55.0 \\ Pythia-1.4B & NeoX & 7.51 & 6.08 & 61.7 & 52.1 & 71.0 & 60.5 & 28.5 & 57.2 & 55.2 \\ RWKV-1.5B & NeoX & 7.70 & 7.04 & 56.4 & 52.5 & 72.4 & 60.5 & 29.4 & 54.6 & 54.3 \\
**Mamba-1.4B** & NeoX & **6.80** & **5.04** & **64.9** & **59.1** & **74.2** & **65.5** & **32.8** & **61.5** & **59.7** \\ \hline GPT-Neo 2.7B & GPT2 & — & 5.63 & 62.2 & 55.8 & 72.1 & 61.1 & 30.2 & 57.6 & 56.5 \\ Hybrid H3-2.7B & GPT2 & — & 7.92 & 55.7 & 59.7 & 73.3 & 65.6 & 32.3 & 61.4 & 58.0 \\ OPT-2.7B & OPT & — & 5.12 & 63.6 & 60.6 & 74.8 & 60.8 & 31.3 & 61.0 & 58.7 \\ Pythia-2.8B & NeoX & 6.73 & 5.04 & 64.7 & 59.3 & 74.0 & 64.1 & 32.9 & 59.7 & 59.1 \\ RWKV-3B & NeoX & 7.00 & 5.24 & 63.9 & 59.6 & 73.7 & 67.8 & 33.1 & 59.6 & 59.6 \\
**Mamba-2.8B** & NeoX & **6.22** & **4.23** & **69.2** & **66.1** & **75.2** & **69.7** & **36.3** & **63.5** & **63.3** \\ \hline GPT-J-6B & GPT2 & – & 4.10 & 68.3 & 66.3 & 75.4 & 67.0 & 36.6 & 64.1 & 63.0 \\ OPT-6.7B & OPT & – & 4.25 & 67.7 & 67.2 & 76.3 & 65.6 & 34.9 & 65.5 & 62.9 \\ Pythia-6.9B & NeoX & 6.51 & 4.45 & 67.1 & 64.0 & 75.2 & 67.3 & 35.5 & 61.3 & 61.7 \\ RWKV-7.4B & NeoX & 6.31 & 4.38 & 67.2 & 65.5 & 76.1 & 67.8 & 37.5 & 61.0 & 62.5 \\ \hline \hline \end{tabular}
\end{table}
표 3: (**영샷 평가**) 굵게 표시된 각 크기에 대한 최상의 결과입니다. 최대 300B 토큰에 대해 훈련된 다양한 토큰라이저를 사용하여 오픈 소스 LM과 비교합니다. Pile은 동일한 데이터 세트 및 토큰나이저(GPT-NeoX-20B)에서 학습된 모델과만 비교하여 유효성 검사 분할을 나타냅니다. 각 모델 크기에 대해 맘바는 모든 평가 결과에서 동급 최고 수준이며 일반적으로 모델 크기의 두 배에서 기준선과 일치한다.

매우 시끄러울 수 있습니다. 하이에나DNA는 더 긴 컨텍스트로 개선한다고 주장하지만 그 결과는 계산 시간을 제어하지 않는다.

#### 4.3.3 합성종 분류

DNA의 인접한 부분을 무작위로 샘플링하여 5가지 다른 종 사이에서 분류하는 다운스트림 작업에 대한 모델을 평가한다. 이 작업은 종 {인간, 여우원숭이, 쥐, 돼지, 하마}를 사용한 하이에나DNA에서 적응되었다. 우리는 DNA의 99%를 공유하는 것으로 알려져 있는 5가지 유인원 종 {인간, 침팬지, 고릴라, 오랑우탄, 보노보}를 분류함으로써 작업을 훨씬 더 도전적이도록 수정한다.

### 오디오 모델링 및 생성

오디오 파형 모달리티에 대해, 우리는 주로 SaShiMi 아키텍처 및 트레이닝 프로토콜(Goel 등, 2022)과 비교한다. 상기 모델은,

1. 단계당 모델 차원 \(D\)을 배가하는 인자 \(p\)에 의해 풀링의 두 단계를 갖는 U-Net 백본,
2. 각 스테이지에서 S4와 MLP 블록을 교번한다.

우리는 S4+MLP 블록을 맘바 블록으로 대체하는 것을 고려한다. 실험 세부 사항은 부록 E.4에 나와 있다.

#### 4.4.1 Long-Context Autoregressive Pretraining

4시간의 솔로 피아노 음악으로 구성된 선행 작업에서 사용된 표준 피아노 음악 데이터 세트인 YouTubeMix(DeepSound, 2017)에서 프리트레이닝 품질(autoregressive next-sample prediction)을 4시간의 비율로 샘플링하여 평가한다.

도 5: (**DNA 스케일링 법칙.**) HG38(인간 게놈) 데이터세트에 대한 사전 트레이닝. (_Left_) 짧은 문맥 길이를 수정하는 것 \(2^{10}=1024\)과 크기를 증가시키는 것 \(\approx 200K\)에서 \(\approx 40M\) 파라미터로 Mamba가 기준선보다 더 잘 확장한다. (_Right_) 토큰/배치 및 총 학습 토큰을 고정 하는 동안 모델 크기를 수정 하 고 시퀀스 길이를 증가 합니다. 기본선과 달리 맘바의 선택 메커니즘은 컨텍스트 길이가 증가함에 따라 더 나은 성능을 촉진한다.

그림 6: (**Great Apes DNA 분류.**) 동일한 컨텍스트 길이의 사전 훈련된 모델을 사용하여 길이 \(2^{10}=1024\) 최대 \(2^{20}=1048576\)의 서열에 대한 미세 조정 후 정확도. 표 13의 수치 결과이다.

도 7: (**오디오 프리트레이닝.**) 맘바는 자기회귀 오디오 모델링에서 이전의 최신 기술(사시미)에 비해 성능을 향상시키는 한편, 최대 분 길이의 컨텍스트 또는 백만 길이의 시퀀스(계산 제어)를 개선한다.

16000 Hz 사전 훈련 세부 사항은 주로 표준 언어 모델링 설정(섹션 4.2)을 따른다. 그림 7은 계산을 고정하면서 훈련 시퀀스 길이가 \(2^{13}=8192\)에서 \(2^{20}\approx 10^{6}\)으로 증가하는 효과를 평가한다. (데이터가 큐레이팅되는 방식에는 약간의 에지 경우가 있으며, 이는 스케일링 곡선의 꼬임을 초래할 수 있다. 예를 들어, 1분 길이의 클립만 사용 가능하므로 최대 시퀀스 길이는 실제로 \(60\mathbf{s}\cdot 16000Hz=960000\)으로 제한된다.)

Mamba와 SaShiMi(S4+MLP) 기준선은 문맥 길이가 길수록 일관되게 개선되며, Mamba는 전체적으로 더 우수하고 길이가 길수록 간격이 넓어진다. 주요 메트릭은 바이트당 비트(BPB)이며, 이는 다른 모달리티를 사전 훈련하기 위한 표준 음성 로그 가능성(NLL) 손실의 상수 인자 \(\log(2)\)이다.

우리는 한 가지 중요한 세부 사항에 주목한다. 이것은 실제 매개변수화에서 복소로 전환한 이 논문의 유일한 실험이다(섹션 3.6). 우리는 부록 E.4에서 추가 절제를 보여준다.

#### 4.4.2 Autoregressive Speech Generation

SC09는 벤치마크 음성 생성 데이터세트(Donahue, McAuley, and Puckette, 2019; Warden, 2018)로서, 매우 가변적인 특성을 갖는 "0" 내지 "9"의 자릿수의 16000Hz에서 샘플링된 1초 클립으로 구성된다. 우리는 Goel 등(2022)의 자기회귀 훈련 설정 및 생성 프로토콜을 크게 따른다.

표 4는 Goel et al.(2022): WaveNet(Oord et al., 2016), SampleRNN(Mehri et al., 2017), WaveGAN(Donahue, McAuley, and Puckette, 2019), DiffWave(Z. Kong et al., 2021)의 다양한 기준선과 비교한 Mamba-UNet 모델의 자동화된 메트릭을 나타낸다. 및 사시미 중 적어도 하나를 포함하는 것을 특징으로 하는 유기 발광 표시 장치. 작은 맘바 모델은 최신(그리고 훨씬 더 큰) GAN 및 확산 기반 모델보다 성능이 우수하다. 기준선과 일치하는 더 큰 모델 매개변수는 충실도 메트릭을 더욱 극적으로 개선한다.

표 5는 작은 맘바 모델을 취하고 외부 스테이지와 중앙 스테이지에 대해 서로 다른 아키텍처의 조합을 조사한다. 외부 블록에서는 Mamba가 S4+MLP보다, 중앙 블록에서는 Mamba > S4+MLP > MHA+MLP보다 일관되게 우수함을 보여준다.

### 속도 및 메모리 벤치마크

본 논문에서는 SSM 스캔 동작의 속도(상태 확장 \(N=16\))와 Mamba의 종단 간 추론 처리량을 그림 8에서 벤치마킹한다. 본 논문에서 제안하는 SSM 스캔은 시퀀스 길이 2K 이상(FlashAttention-2 (Dao, 2023))에서 알고 있는 최상의 주의력 구현보다 빠르고, PyTorch의 표준 스캔 구현보다 최대 20-40\(\mathbf{\times}\) 빠르다. Mamba는 비슷한 크기의 Transformer보다 4-5\(\mathbf{\times}\) 더 높은 추론 처리량을 얻는데, KV 캐시가 없으면 훨씬 더 높은 배치 크기를 사용할 수 있기 때문이다. 예를 들어, Mamba-6.9B(무훈련)는 5\(\mathbf{\times}\) 더 작은 Transformer-1.3B보다 더 높은 추론 처리량을 가질 것이다. 메모리 소비의 벤치마크를 추가로 포함하는 부록 E.5의 세부 정보.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Model & Params & NLL \(\downarrow\) & FID \(\downarrow\) & IS \(\uparrow\) & mIS \(\uparrow\) & AM \(\downarrow\) \\ \hline SampleRNN & 35.0M & 2.042 & 8.96 & 1.71 & 3.02 & 1.76 \\ WaveNet & 4.2M & 1.925 & 5.08 & 2.27 & 5.80 & 1.47 \\ SaShiMi & 5.8M & 1.873 & 1.99 & 5.13 & 42.57 & 0.74 \\ \hline WaveGAN & 19.1M & - & 2.03 & 4.90 & 36.10 & 0.80 \\ DiffWave & 24.1M & - & 1.92 & 5.26 & 51.21 & 0.68 \\ + SaShiMi & 23.0M & - & 1.42 & 5.94 & 69.17 & 0.59 \\ \hline
**Mamba** & 6.1M & **1.852** & 0.94 & 6.26 & 88.54 & 0.52 \\
**Mamba** & 24.3M & **1.860** & **0.67** & **7.33** & **144.9** & **0.36** \\ \hline Train & - & - & 0.00 & 8.56 & 292.5 & 0.16 \\ Test & - & - & 0.02 & 8.33 & 257.6 & 0.19 \\ \hline \hline \end{tabular}
\end{table}
표 4: **(SC09) 고정 길이 음성 클립의 도전적인 데이터 집합에서 무조건 생성을 위한 자동화된 메트릭입니다. (_Top to Bottom_) Autoregressive Baselines, non-Autoregressive Baselines, Mamba 및 데이터 세트 메트릭입니다. **

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Outer & Center & NLL \(\downarrow\) & FID \(\downarrow\) & IS \(\uparrow\) & mIS \(\uparrow\) & AM \(\downarrow\) \\ \hline S4+MLP & MHA+MLP & 1.859 & 1.45 & 5.06 & 47.03 & 0.70 \\ S4+MLP & S4+MLP & 1.867 & 1.43 & 5.42 & 53.54 & 0.65 \\ S4+MLP & Mamba & 1.859 & 1.42 & 5.71 & 56.51 & 0.64 \\ Mamba & MHA+MLP & **1.850** & 1.37 & 5.63 & 58.23 & 0.62 \\ Mamba & S4+MLP & 1.853 & 1.07 & 6.05 & 73.34 & 0.55 \\ Mamba & Mamba & 1.852 & **0.94** & **6.26** & **88.54** & **0.52** \\ \hline \hline \end{tabular}
\end{table}
표 5: **(SC09 모델 블래이션) 6M 파라미터를 갖는 모델. SaShiMi의 U-Net 백본에는 시퀀스 길이 1000에서 작동하는 8개의 중앙 블록이 있으며, 각 측면에 시퀀스 길이 4000에서 8개의 외부 블록으로 샌드위치되고, 시퀀스 길이 16000에서 8개의 외부 블록으로 샌드위치된다(총 40개 블록). 8개의 중심 블록의 아키텍처는 나머지 블록과 독립적으로 제거된다. 트랜스포머(MHA+MLP)는 효율성 제약 조건으로 인해 더 중요한 외부 블록에서 테스트되지 않았습니다.**

### Model Ablations

우리는 Chinchilla 토큰 카운트(그림 4와 동일한 설정)에서 크기 \(\approx\) 350M 모델을 사용하는 언어 모델링 설정에 초점을 맞추어 모델의 구성 요소에 대한 일련의 자세한 삭제를 수행한다.

#### 4.6.1 Architecture

표 6은 아키텍처(블록) 및 그 내부 SSM 층의 효과를 조사한다(도 3). 우리는

* 전역 컨볼루션과 동등한 이전의 비선택적(LTI) SSM들 중에서, 성능은 매우 유사하다.
* 이전 작업의 복소 값 S4 변형을 실수 값으로 대체하는 것은 성능에 큰 영향을 주지 않으며, 이는 하드웨어 효율성을 고려할 때 (LM의 경우) 실수 SSM이 더 나은 선택일 수 있음을 시사합니다.
* 이들 중 임의의 것을 선택적 SSM으로 교체하는 것(S6)은 성능을 상당히 향상시켜 섹션 3의 동기를 검증한다.
* Mamba 아키텍처는 H3 아키텍처와 유사하게 수행되며 선택적 계층을 사용할 때 약간 더 좋아 보입니다.

또한 부록 E.2.2에서 MLP(전통적인 아키텍처) MHA(하이브리드 주의 아키텍처)와 같은 다른 블록과 Mamba 블록을 인터리빙하는 것을 조사한다.

#### 4.6.2 Selective SSM

표 7은 선택적 \(\Delta\), \(\mathbf{B}\), \(\mathbf{C}\) 파라미터(알고리즘 2)의 서로 다른 조합을 고려하여 선택적 SSM 계층을 삭제한 것으로, RNN 게이팅(정리 1)과의 연결로 인해 \(\Delta\)이 가장 중요한 파라미터임을 보여준다.

표 8은 SSM의 상이한 초기화를 고려하며, 이는 일부 데이터 양식과 설정에서 큰 차이를 만드는 것으로 나타났다(Gu, Goel, and Re, 2022; Gu, Gupta, et al., 2022). 언어 모델링에서 더 표준적인 복소값 파라미터화(S4D-Lin, 1행) 대신 더 간단한 실수값 대각선 초기화(S4D-Real, 3행)가 더 나은 성능을 보인다는 것을 발견했다. 무작위 초기화는 또한 선행 작업의 발견과 일치하여 잘 작동한다(Mehta et al., 2023).

표 9와 표 10은 각각 \(\Delta\)와 \((\mathbf{B},\mathbf{C})\) 투영의 차원을 변화시키는 것을 고려한다. 정적에서 선택적인 것으로 변경하는 것이 가장 많은 이점을 제공하는 반면, 차원을 증가시키면 일반적으로 매개변수 카운트의 작은 증가로 성능이 완만하게 향상된다.

특히 주의할 점은 상태 크기 \(N\)가 증가할 때 선택적 SSM의 극적인 개선과 1% 추가 매개변수의 비용에 대해 1.0 이상의 복잡도 개선이 있다는 것이다. 이것은 섹션 3.1과 3.3에서 우리의 핵심 동기를 검증한다.

도 8: (**Efficiency Benchmarks.**)(_Left_) 훈련: 우리의 효율적인 검색은 표준 구현보다 40배 빠르다. (_Right_) 추론: 순환 모델로서, Mamba는 Transformers보다 5배 더 높은 처리량을 달성할 수 있다.

## 5 Discussion

관련 업무, 한계점 및 향후 방향에 대해 논의합니다.

관련 작업.부록 A는 선택 메커니즘이 유사한 개념과 어떻게 관련되는지 논의한다. 부록 B는 SSM 및 기타 관련 모델의 확장된 관련 작업을 가지고 있다.

Free Lunch: Continuous-Discrete Spectrum.Structured SSM은 원래 연속 시스템의 이산화(1)로 정의되었으며 지각 신호(예: 오디오, 비디오)와 같은 연속 시간 데이터 양식에 대한 강한 귀납적 편향을 가지고 있다. 섹션 3.1 및 3.5에서 논의된 바와 같이 선택 메커니즘은 텍스트 및 DNA와 같은 개별 양식에 대한 약점을 극복하지만 반대로 성능을 방해할 수 있다.

\begin{table}
\begin{tabular}{l l l l} \hline \hline Model & Arch. & SSM Layer & Perplexity \\ \hline Hyena & H3 & Hyena & 10.24 \\ H3 & H3 & S4 (complex) & 10.30 \\ - & H3 & S4 (real) & 10.34 \\ - & H3 & S6 & **8.95** \\ \hline \hline \end{tabular}
\begin{tabular}{l l l l} \hline \hline Model & Arch. & SSM Layer & Perplexity \\ \hline - & Mamba & Hyena & 10.75 \\ - & Mamba & S4 (complex) & 10.54 \\ - & Mamba & S4 (real) & 10.56 \\ Mamba & Mamba & S6 & **8.69** \\ \hline \hline \end{tabular}
\end{table}
표 6: (**Ablations: Architecture and SSM layer.**) 맘바 블록은 보다 간단하면서도 H3와 유사하게 수행한다. 내층에서는 LTI 모델의 매개변수화 간에 차이가 거의 없는 반면 선택적 SSM(S6)은 큰 개선을 제공한다. 보다 구체적으로, S4(real) 변이체는 S4D-Real이고, S4(complex) 변이체는 S4D-Lin이다.

\begin{table}
\begin{tabular}{l l l} \hline \hline \(\mathbf{A}_{n}\) Initialization & Field & Perplexity \\ \hline \(\mathbf{A}_{n}=-\frac{1}{2}+ni\) & Complex & 9.16 \\ \(\mathbf{A}_{n}=-1/2\) & Real & 8.85 \\ \(\mathbf{A}_{n}=-(n+1)\) & Real & 8.71 \\ \(\mathbf{A}_{n}\sim\exp(\mathcal{N}(0,1))\) & Real & 8.71 \\ \hline \hline \end{tabular}
\end{table}
표 10: (**절제들: SSM 상태 차원**). (_Top_) Constant \(\mathbf{B}\) and \(\mathbf{C}\) (_Bottom_) Selective \(\mathbf{B}\) and \(\mathbf{C}\) 순환 상태의 차원에 대한 확장 인자로 볼 수 있는 SSM 상태 차원 \(N\)을 증가시키는 것은 매개변수/FLOP에서 무시할 수 있는 비용에 대해 성능을 크게 향상시킬 수 있지만 \(\mathbf{B}\) 및 \(\mathbf{C}\)도 선택적일 때만 성능을 향상시킬 수 있다. \(\Delta\) 투영의 크기가 64로 고정되었습니다.

\begin{table}
\begin{tabular}{l l l l} \hline \hline \(\mathbf{A}_{n}\) Initialization & Field & Perplexity \\ \hline \(\mathbf{A}_{n}=-\frac{1}{2}+ni\) & Complex & 9.16 \\ \(\mathbf{A}_{n}=-1/2\) & Real & 8.85 \\ \(\mathbf{A}_{n}=-(n+1)\) & Real & 8.71 \\ \(\mathbf{A}_{n}\sim\exp(\mathcal{N}(0,1))\) & Real & 8.71 \\ \hline \hline \end{tabular}
\end{table}
표 10: (**절제들: 시스템 상태 차원**). (_Top_) Constant \(\mathbf{B}\) and \(\mathbf{C}\) (_Bottom_) Selective \(\mathbf{B}\) and \(\mathbf{C}\) 순환 상태의 차원에 대한 확장 인자로 볼 수 있는 SSM 상태 차원 \(N\)을 증가시키는 것은 매개변수/FLOP에서 무시할 수 있는 비용에 대해 성능을 크게 향상시킬 수 있지만 \(\mathbf{B}\) 및 \(\mathbf{C}\)도 선택적일 때만 성능을 향상시킬 수 있다. \(\Delta\) 투영의 크기가 64로 고정되었습니다.

\begin{table}
\begin{tabular}{l l l} \hline \hline Size of \(\Delta\) proj. & Params (M) & Perplexity \\ \hline - & 358.9 & 9.12 \\ 1 & 359.1 & 8.97 \\ 2 & 359.3 & 8.97 \\ 4 & 359.7 & 8.91 \\ 8 & 360.5 & 8.83 \\ 16 & 362.1 & 8.84 \\ 32 & 365.2 & 8.80 \\ 64 & 371.5 & 8.71 \\ \hline \hline \end{tabular}
\end{table}
표 9: (**절제: \(\Delta\).**) \(\Delta\)의 선택 메커니즘은 입력의 프로젝션으로 구성합니다. 어두워질 때까지 투영합니다. 도 1은 성능의 큰 증가를 제공하고; 그것을 증가시키는 것은 파라미터들의 적당한 증가의 비용으로 추가적인 개선들을 더 제공한다. 상태 크기가 \(N=16\)으로 고정되었습니다.

LTI SSM이 뛰어난 데이터에 대한 것이다. 오디오 파형에 대한 삭제는 이 절충안을 더 자세히 조사합니다.

다운스트림 어포던스.트랜스포머 기반 기초 모델(특히 LLMs)은 미세 조정, 적응, 프롬프트, 인-컨텍스트 학습, 명령어 튜닝, RLHF, 양자화 등과 같은 사전 훈련된 모델과의 속성 및 상호작용 모드의 풍부한 생태계를 갖는다. 우리는 특히 SSM과 같은 트랜스포머 대안이 유사한 특성과 어포던스를 가지고 있는지에 관심이 있다.

스케일링.우리의 경험적 평가는 가장 강한 오픈 소스 LLM(예를 들어, Llama(Touvron et al., 2023)) 및 RWKV(B. Peng et al., 2023)와 같은 다른 순환 모델의 임계치 미만의 작은 모델 크기로 제한된다. and RetNet (Y. Sun et al., 2023) (p<0.05). 맘바가 여전히 이러한 더 큰 크기에서 유리하게 비교되는지 여부를 평가해야 한다. 또한 SSM을 스케일링하는 것은 이 문서에서 논의되지 않은 모델에 대한 추가 엔지니어링 문제 및 조정을 포함할 수 있다.

## 6 Conclusion

구조화된 상태 공간 모델에 선택 메커니즘을 도입하여 시퀀스 길이에서 선형적으로 스케일링하면서 컨텍스트 종속 추론을 수행할 수 있다. 단순한 주의 없는 아키텍처에 통합될 때, 맘바는 강력한 트랜스포머 모델의 성능과 일치하거나 능가하는 다양한 도메인 집합에서 최첨단 결과를 달성한다. 우리는 특히 유전체학, 오디오 및 비디오와 같은 긴 맥락을 필요로 하는 새로운 양식에서 다양한 도메인에 대한 기초 모델을 구축하기 위한 선택적 상태 공간 모델의 광범위한 적용에 대해 흥분한다. 우리의 결과는 맘바가 일반적인 서열 모델 백본이 될 수 있는 강력한 후보임을 시사한다.

#### Acknowledgments

우리는 드래프트에 대한 도움이 되는 피드백을 해준 카란 고엘, 아르준 데사이, 쿠시 바티아에게 감사드린다.

## References

* [1] Martin Arjovsky, Amar Shah, and Yoshua Bengio. "Unitary Evolution Recurrent Neural Networks". In: _The International Conference on Machine Learning (ICML)_. 2016, pp. 1120-1128.
*[2]iga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. "장거리 상호작용을 통합함으로써 시퀀스로부터 효과적인 유전자 발현 예측" In: _Nature Methods_ 18.10 (2021), pp. 1196-1203.
* [3] Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. "Using Fast Weights to Attend to the Recent Past". In: _Advances in Neural Information Processing Systems (NeurIPS)_ 29 (2016).
* [4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. "Layer Normalization". In: _arXiv preprint arXiv:1607.06450_ (2016).
* [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. "Neural Machine Translation by Jointly Learning to Align and Translate". In: _The International Conference on Learning Representations (ICLR)_. 2015.
* [6] David Balduzzi and Muhammad Ghifary. "Strongly-typed Recurrent Neural Networks". In: _International Conference on Machine Learning_. PMLR. 2016, pp. 1292-1300.
* [7] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. "Pythia: A Suite for Analyzing Large Language Models across Training and Scaling". In: _The International Conference on Machine Learning (ICML)_. PMLR. 2023, pp. 2397-2430.
* [8] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. "PIQA: Reasoning about Physical Commonsense in Natural Language". In: _Proceedings of the AAAI conference on Artificial Intelligence_. Vol. 34. 05. 2020, pp. 7432-7439.
* [9] Guy E Blelloch. "Prefix Sums and Their Applications". In: (1990).
* [10] James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. "Quasi-recurrent Neural Networks". In: _arXiv preprint arXiv:1611.01576_ (2016).

* [11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. "Language Models are Few-shot Learners". In: _Advances in Neural Information Processing Systems (NeurIPS)_ 33 (2020), pp. 1877-1901.
* [12] Aydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. "Scaling Transformer to 1M tokens and Beyond with RMT". In: _arXiv preprint arXiv:2304.11062_ (2023).
* [13] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. "Generating Long Sequences with Sparse Transformers". In: _arXiv preprint arXiv:1904.10509_ (2019).
* [14] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. "Rethinking Attention with Performers". In: _The International Conference on Learning Representations (ICLR)_. 2021.
* [15] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. "PaLM: Scaling Language Modeling with Pathways". In: _Journal of Machine Learning Research_ 24.240 (2023), pp. 1-113. url: [http://jmlr.org/papers/v24/22-1144.html](http://jmlr.org/papers/v24/22-1144.html).
* [16] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling". In: _arXiv preprint arXiv:1412.3555_ (2014).
* [17] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge". In: _arXiv preprint arXiv:1803.05457_ (2018).
* [18] Tri Dao. "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning". In: (2023).
* [19] Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher Re. "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness". In: _Advances in Neural Information Processing Systems (NeurIPS)_. 2022.
* [20] Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Re. "Hungry Hungary Hippos: Towards Language Modeling with State Space Models". In: _The International Conference on Learning Representations (ICLR)_. 2023.
* [21] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. "Language Modeling with Gated Convolutional Networks". In: _The International Conference on Machine Learning (ICML)_. PMLR. 2017, pp. 933-941.
* [22] DeepSound. _SampleRNN_. [https://github.com/deepsound-project/samplernn-pytorch](https://github.com/deepsound-project/samplernn-pytorch). 2017.
* [23] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. "LongNet: Scaling Transformers to 1,000,000,000 Tokens". In: _arXiv preprint arXiv:2307.02486_ (2023).
* [24] Chris Donahue, Julian McAuley, and Miller Puckette. "Adversarial Audio Synthesis". In: _The International Conference on Learning Representations (ICLR)_. 2019.
* [25] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale". In: _The International Conference on Learning Representations (ICLR)_. 2020.
* [26] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. "A Mathematical Framework for Transformer Circuits". In: _Transformer Circuits Thread_ (2021). [https://transformer-circuits.pub/2021/framework/index.html](https://transformer-circuits.pub/2021/framework/index.html).
* [27] Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, Christopher Pal, Orhan Firat, and Ross Goroshin. "Block-State Transformer". In: _arXiv preprint arXiv:2306.09539_ (2023).
* [28] Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar, Chunxi Liu, Yangyang Shi, Ozlem Kalinli, Mike Seltzer, et al. "Multi-Head State Space Model for Sequence Modeling". In: _INTERSPEECH_. 2023.
* [29] Karl J Friston, Lee Harrison, and Will Penny. "Dynamic Causal Modelling". In: _Neuroimage_ 19.4 (2003), pp. 1273-1302.
* [30] Daniel Y Fu, Elliot L Epstein, Eric Nguyen, Armin W Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher Re. "Simple Hardware-efficient Long Convolutions for Sequence Modeling". In: _The International Conference on Machine Learning (ICML)_ (2023).
* [31] Ken-ichi Funahashi and Yuichi Nakamura. "Approximation of Dynamical Systems by Continuous Time Recurrent Neural Networks". In: _Neural Networks_ 6.6 (1993), pp. 801-806.

* [32] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. "The Pile: An 800GB Dataset of Diverse Text for Language Modeling". In: _arXiv preprint arXiv:2101.00027_ (2020).
* [33] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. _A Framework for Few-shot Language Model Evaluation_. Version v0.0.1. Sept. 2021. doi: 10.5281/zenodo.5371628. url: [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628).
* [34] Karan Goel, Albert Gu, Chris Donahue, and Christopher Re. "It's Rawl Audio Generation with State-Space Models". In: _The International Conference on Machine Learning (ICML)_. 2022.
* [35] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. "HIPPO: Recurrent Memory with Optimal Polynomial Projections". In: _Advances in Neural Information Processing Systems (NeurIPS)_. 2020.
* [36] Albert Gu, Karan Goel, and Christopher Re. "Efficiently Modeling Long Sequences with Structured State Spaces". In: _The International Conference on Learning Representations (ICLR)_. 2022.
* [37] Albert Gu, Caglar Gulcehre, Tom Le Paine, Matt Hoffman, and Razvan Pascanu. "Improving the Gating Mechanism of Recurrent Neural Networks". In: _The International Conference on Machine Learning (ICML)_. 2020.
* [38] Albert Gu, Ankit Gupta, Karan Goel, and Christopher Re. "On the Parameterization and Initialization of Diagonal State Space Models". In: _Advances in Neural Information Processing Systems (NeurIPS)_. 2022.
* [39] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Re. "Combining Recurrent, Convolutional, and Continuous-time Models with the Linear State Space Layer". In: _Advances in Neural Information Processing Systems (NeurIPS)_. 2021.
* [40] Albert Gu, Isys Johnson, Aman Tamalsina, Atri Rudra, and Christopher Re. "How to Train Your HIPPO: State Space Models with Generalized Basis Projections". In: _The International Conference on Learning Representations (ICLR)_. 2023.
* [41] Ankit Gupta, Albert Gu, and Jonathan Berant. "Diagonal State Spaces are as Effective as Structured State Spaces". In: _Advances in Neural Information Processing Systems_ 35 (2022), pp. 22982-22994.
* [42] David Ha, Andrew Dai, and Quoc V. Le. "HyperNetworks". In: _The International Conference on Learning Representations (ICLR)_. 2017.
* [43] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. "Dream to Control: Learning Behaviors by Latent Imagination". In: _The International Conference on Learning Representations (ICLR)_. 2020.
* [44] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. "Liquid Structural State-Space Models". In: _The International Conference on Learning Representations (ICLR)_. 2023.
* [45] Mikael Henaff, Arthur Szlam, and Yann LeCun. "Recurrent Orthogonal Networks and Long-Memory Tasks". In: _The International Conference on Machine Learning (ICML)_. 2016.
* [46] Dan Hendrycks and Kevin Gimpel. "Gaussian Error Linear Units (GELUs)". In: _arXiv preprint arXiv:1606.08415_ (2016).
* [47] Sepp Hochreiter and Jurgen Schmidhuber. "Long Short-Term Memory". In: _Neural Computation_ 9.8 (1997), pp. 1735-1780.
* [48] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. "An Empirical Analysis of Compute-Optimal Large Language Model Training". In: _Advances in Neural Information Processing Systems (NeurIPS)_ 35 (2022), pp. 30016-30030.
* [49] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. "Transformer Quality in Linear Time". In: _The International Conference on Machine Learning (ICML)_. PMLR. 2022, pp. 9099-9117.
* [50] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lassane Idoumghar, and Pierre-Alain Muller. "Deep Learning for Time Series Classification: A Review". In: _Data Mining and Knowledge Discovery_ 33.4 (2019), pp. 917-963.
* [51] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. "Data Movement is All You Need: A Case Study on Optimizing Transformers". In: _Proceedings of Machine Learning and Systems_ 3 (2021), pp. 711-732.
* [52] Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Soljacic, and Yoshua Bengio. "Gated Orthogonal Recurrent Units: On Learning to Forget". In: _Neural Computation_ 31.4 (2019), pp. 765-783.
* [53] Rudolph Emil Kalman. "A New Approach to Linear Filtering and Prediction Problems". In: (1960).

* [54] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention". In: _International Conference on Machine Learning_. PMLR. 2020, pp. 5156-5165.
* [55] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. "DiffWave: A Versatile Diffusion Model for Audio Synthesis". In: _International Conference on Learning Representations_. 2021.
* [56] Chrysoula Kosma, Giannis Nikolentzos, and Michalis Vazirgiannis. "Time-Parameterized Convolutional Neural Networks for Irregularly Sampled Time Series". In: _arXiv preprint arXiv:2308.03210_ (2023).
* [57] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. "ImageNet Classification with Deep Convolutional Neural Networks". In: _Advances in Neural Information Processing Systems (NeurIPS) 25_ (2012).
* [58] Tao Lei. "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute". In: _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_. 2021, pp. 7633-7648.
* [59] Tao Lei, Yu Zhang, Sida I Wang, Hui Dai, and Yoav Artzi. "Simple Recurrent Units for Highly Parallelizable Recurrence". In: _arXiv preprint arXiv:1709.02755_ (2017).
* [60] Mario Lezcano-Casado and David Martinez-Rubio. "Cheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Group". In: _The International Conference on Machine Learning (ICML)_. 2019.
* [61] Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. "What Makes Convolutional Models Great on Long Sequence Modeling?" In: _The International Conference on Learning Representations (ICLR)_. 2023.
* [62] Vasileios Lioutas and Yuhong Guo. "Time-aware Large Kernel Convolutions". In: _The International Conference on Machine Learning (ICML)_. PMLR. 2020, pp. 6172-6183.
* [63] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbhani. "Structured State Space Models for In-Context Reinforcement Learning". In: _Advances in Neural Information Processing Systems (NeurIPS)_. 2023.
* [64] Shahar Lutati, Itamar Zimerman, and Lior Wolf. "Focus Your Attention (with Adaptive IIR Filters)". In: _arXiv preprint arXiv:2305.14952_ (2023).
* [65] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. "Mega: Moving Average Equipped Gated Attention". In: _The International Conference on Learning Representations (ICLR)_. 2023.
* [66] Eric Martin and Chris Cundy. "Parallelizing Linear Recurrent Neural Nets Over Sequence Length". In: _The International Conference on Learning Representations (ICLR)_. 2018.
* [67] Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville, and Yoshua Bengio. "SampleRNN: An Unconditional End-to-End Neural Audio Generation Model". In: _The International Conference on Learning Representations (ICLR)_. 2017.
* [68] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. "Long Range Language Modeling via Gated State Spaces". In: _The International Conference on Learning Representations (ICLR)_. 2023.
* [69] Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. "Efficient Orthogonal Parametrisation of Recurrent Neural Networks using Householder Reflections". In: _International Conference on Machine Learning_. PMLR. 2017, pp. 2401-2409.
* [70] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher Re. "S4ND: Modeling Images and Videos as Multidimensional Signals with State Spaces". In: _Advances in Neural Information Processing Systems (NeurIPS)_. 2022.
* [71] Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, et al. "HyenaDNA: Long-range Genomic Sequence Modeling at Single Nucleotide Resolution". In: _Advances in Neural Information Processing Systems (NeurIPS)_. 2023.
* [72] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. "In-context Learning and Induction Heads". In: _Transformer Circuits Thread_ (2022). [https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html).
* [73] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. "WaveNet: A Generative Model for Raw Audio". In: _arXiv preprint arXiv:1609.03499_ (2016).

* [74] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. "Resurrecting Recurrent Neural Networks for Long Sequences". In: _The International Conference on Machine Learning (ICML)_. 2023.
* [75] Denis Paperno, German Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. "The LAMBADA Dataset: Word Prediction Requiring a Broad Discourse Context". In: _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics_. 2016, pp. 1525-1534.
* [76] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. "On the Difficulty of Training Recurrent Neural Networks". In: _International Conference on Machine Learning_. 2013, pp. 1310-1318.
* [77] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. "RWEV: Reinventing RNNs for the Transformer Era". In: _arXiv preprint arXiv:2305.13048_ (2023).
* [78] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. "Random Feature Attention". In: _The International Conference on Learning Representations (ICLR)_. 2021.
* [79] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. "Hyena Hierarchy: Towards Larger Convolutional Language Models". In: _The International Conference on Machine Learning (ICML)_. 2023.
* [80] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. "Toeplitz Neural Network for Sequence Modeling". In: _The International Conference on Learning Representations (ICLR)_. 2023.
* [81] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. "The devil in linear transformer". In: _arXiv preprint arXiv:2210.10340_ (2022).
* [82] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. "CosFormer: Rethinking Softmax in Attention". In: _The International Conference on Learning Representations (ICLR)_. 2022.
* [83] Ali Rahimi and Benjamin Recht. "Random features for large-scale kernel machines". In: _Advances in neural information processing systems_ 20 (2007).
* [84] Prajit Ramachandran, Barret Zoph, and Quoc V Le. "Swish: A Self-gated Activation Function". In: _arXiv preprint arXiv:1710.05941_ 7.1 (2017), p. 5.
* [85] David W Romero, Anna Kuzina, Erik J Bekkers, Jakub M Tomczak, and Mark Hoogendoorn. "CKConv: Continuous Kernel Convolution For Sequential Data". In: _arXiv preprint arXiv:2102.02611_ (2021).
* [86] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. "Winogrande: An Adversarial Winograd Schema Challenge at Scale". In: _Communications of the ACM_ 64.9 (2021), pp. 99-106.
* [87] George Saon, Ankit Gupta, and Xiaodong Cui. "Diagonal State Space Augmented Transformers for Speech Recognition". In: _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_. IEEE. 2023, pp. 1-5.
* [88] Imanol Schlag, Kazuki Irie, and Jurgen Schmidhuber. "Linear Transformers are Secretly Fast Weight Programmers". In: _The International Conference on Machine Learning (ICML)_. PMLR. 2021, pp. 9355-9366.
* [89] Noam Shazeer. "GLU Variants Improve Transformer". In: _arXiv preprint arXiv:2002.05202_ (2020).
* [90] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Scharli, and Denny Zhou. "Large Language Models can be Easily Distracted by Irrelevant Context". In: _The International Conference on Machine Learning (ICML)_. PMLR. 2023, pp. 31210-31227.
* [91] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. "Sequence Modeling with Multiresolution Convolutional Memory". In: _The International Conference on Machine Learning (ICML)_. PMLR. 2023, pp. 31312-31327.
* [92] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. "Simplified State Space Layers for Sequence Modeling". In: _The International Conference on Learning Representations (ICLR)_. 2023.
* [93] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. "Roformer: Enhanced Transformer with Rotary Position Embedding". In: _arXiv preprint arXiv:2104.09864_ (2021).
* [94] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. "Retentive network: A successor to transformer for large language models". In: _arXiv preprint arXiv:2307.08621_ (2023).
* [95] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. "Sequence to Sequence Learning with Neural Networks". In: _Advances in Neural Information Processing Systems (NeurIPS)_ 27 (2014).

* [96] Corentin Tallec and Yann Ollivier. "Can Recurrent Neural Networks Warp Time?" In: _The International Conference on Learning Representations (ICLR)_. 2018.
* [97] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. "Long Range Arena: A Benchmark for Efficient Transformers". In: _International Conference on Learning Representations (ICLR)_. 2021.
* [98] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. "Efficient Transformers: A Survey". In: _ACM Computing Surveys_ 55.6 (2022), pp. 1-28.
* [99] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. "Llama: Open and Efficient Foundation Language Models". In: _arXiv preprint arXiv:2302.13971_ (2023).
* [100] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. "Attention Is All You Need". In: _Advances in Neural Information Processing Systems (NeurIPS)_. 2017.
* [101] Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. "On Orthogonality and Learning Recurrent Networks with Long Term Dependencies". In: _International Conference on Machine Learning_. PMLR. 2017, pp. 3570-3578.
* [102] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. "Selective Structured State-Spaces for Long-form Video Understanding". In: _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 2023, pp. 6387-6397.
* [103] Pete Warden. "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition". In: _ArXiv_ abs/1804.03209 (2018).
* [104] Samuel Williams, Andrew Waterman, and David Patterson. "Roofline: An Insightful Visual Performance Model for Multicore Architectures". In: _Communications of the ACM_ 52.4 (2009), pp. 65-76.
* [105] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. "CondConv: Conditionally Parameterized Convolutions for Efficient Inference". In: _Advances in Neural Information Processing Systems (NeurIPS)_ 32 (2019).
* [106] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. "HellaSwag: Can a Machine Really Finish Your Sentence?" In: _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_. 2019.
* [107] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. "An Attention Free Transformer". In: _arXiv preprint arXiv:2105.14103_ (2021).
* [108] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher Re. "Effectively Modeling Time Series with Simple Discrete State Spaces". In: _The International Conference on Learning Representations (ICLR)_. 2023.
* [109] Lin Zheng, Chong Wang, and Lingpeng Kong. "Linear complexity randomized self-attention mechanism". In: _International Conference on Machine Learning_. PMLR. 2022, pp. 27011-27041.
* [110] Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. "Efficient Long Sequence Modeling via State Space Augmented Transformer". In: _arXiv preprint arXiv:2212.08136_ (2022).

토론 : 선택 메커니즘

우리의 선택 메커니즘은 게이팅, 하이퍼네트워크 및 데이터 의존성과 같은 개념에서 영감을 받고 관련된다. 또한 "빠른 체중"(J)과 관련된 것으로 볼 수 있다. Ba et al., 2016), 이는 고전적 RNNs를 선형 주의의 메커니즘과 연결한다(Schlag, Irie, and Schmidhuber, 2021). 그러나, 우리는 그것이 명확히 할 가치가 있는 별개의 개념이라고 믿습니다.

게이팅.게이팅은 원래 LSTM(Hochreiter and Schmidhuber, 1997) 및 GRU(J. Chung 등, 2014), 또는 게이트 방정식 (5)n 정리 1. 이것은 입력을 RNN의 숨겨진 상태로 허용할지를 제어하기 위한 특정 메커니즘으로 해석되었다. 특히, 이것은 시간을 통한 신호의 전파에 영향을 미치고 입력들이 시퀀스 길이 차원을 따라 상호작용하게 한다.

그러나, 게이팅의 개념은 그 이후로 대중적인 사용에서 완화되어 단순히 임의의 곱셈적 상호작용(종종 활성화 함수와 함께)을 의미한다. 예를 들어, (시퀀스 길이를 따라 상호작용하지 않는) 신경망 아키텍처의 요소별 곱셈 컴포넌트는, 원래의 RNN 센스와 매우 상이한 의미에도 불구하고, 이제 일반적으로 게이티드 아키텍처(Hua et al., 2022; Mehta et al., 2023)로 지칭된다. 따라서 우리는 RNN 게이팅의 원래 개념과 곱셈 게이팅의 일반적인 사용은 실제로 매우 다른 의미론적 의미를 가지고 있다고 믿는다.

하이퍼네트워크.하이퍼네트워크는 파라미터 자체가 더 작은 뉴럴 네트워크에 의해 생성되는 뉴럴 네트워크를 지칭한다. The original idea (Ha, Dai, and Quoc V. Le, 2017) 이를 좁은 의미로 사용하여 순환 매개변수가 더 작은 RNN에 의해 생성되는 큰 RNN을 정의했다.

데이터-의존성.하이퍼네트워크와 유사하게, 데이터-의존성은 모델의 일부 파라미터가 데이터에 의존하는 임의의 개념을 지칭할 수 있다(Poli et al., 2023).

예제: GLU 활성화 이러한 개념의 문제를 설명 하려면 간단한 대각선 선형 계층 \(\mathbf{y}=\mathbf{D}\mathbf{x}\)을 고려 합니다. 여기서 \(\mathbf{D}\)은 대각선 가중치 매개 변수입니다. 이제 \(\mathbf{D}\)는 선택적인 비선형성을 갖는 \(\mathbf{D}=\sigma(\mathbf{W}\mathbf{x})\)의 선형 변환으로부터 생성된다고 가정하자. 대각선이므로 곱셈은 원소곱이 된다. \(\mathbf{y}=\sigma(\mathbf{W}\mathbf{x})\circ\mathbf{x}\).

이것은 다소 사소한 변환이지만 기술적으로 게이팅의 일반적인 의미(승법적인 "분기"를 가지기 때문에), 하이퍼네트워크(매개변수 \(\mathbf{D}\)가 다른 계층에 의해 생성되기 때문에), 데이터 종속(\(\mathbf{D}\)이 데이터 \(\mathbf{x}\)에 의존하기 때문에)를 충족한다. 그러나, 이것은 실제로 단순히 GLU 함수를 정의하는데, 이는 매우 단순하여 종종 의미 있는 층 대신에 활성화 함수(Dauphin et al., 2017; Shazeer, 2020)로만 고려된다.

선택.따라서, 선택 메커니즘은 건축적 게이팅, 하이퍼네트워크 또는 데이터 의존성과 같은 아이디어의 특별한 경우로 간주될 수 있지만, 표준 주의 메커니즘(바다나우, 초, 벵지오, 2015; 바스와니 등, 2017)을 포함하여 곱셈이 있는 모든 다른 구성의 엄청난 범위를 고려할 수 있으며, 우리는 그러한 것으로 생각하는 것이 유익하지 않다는 것을 발견했다.

대신, 우리는 전통적인 RNN의 게이팅 메커니즘과 가장 밀접한 관련이 있다고 본다(정리 1). 또한 \(\Delta\)(Funahashi and Nakamura, 1993; Gu, Dao, et al., 2020; Tallec and Ollivier, 2018). 우리는 또한 전자의 과부하 사용을 명확히 하기 위해 선택에 유리한 "게이팅"이라는 용어를 피한다. 더 좁게는 선택을 사용하여 모델의 기계론적 작용을 참조하여 입력을 선택하거나 무시하고 시퀀스 길이를 따라 데이터 상호 작용을 촉진한다(섹션 3.1). 선택적 SSM들 및 게이티드 RNN들을 넘어, 다른 예들은 입력-의존성 컨볼루션들(Kosma, Nikolentzos, and Vazirgiannis, 2023; Lioutas and Guo, 2020; Lutati, Zimerman, and Wolf, 2023; Yang et al., 2019) 및 심지어 주의를 포함할 수 있다.

Related Work

우리는 우리의 방법과 관련된 몇 가지 이전 작업을 개관한다. 가장 밀접하게 관련된 모델 중 일부는 S4, S5 및 quasi-RNN과 같은 순환 계층과 H3, RetNet 및 RWKV와 같은 종단 간 아키텍처를 포함한다고 언급한다.

### S4 Variants 및 Deratives

우리는 과거 작업의 일부 구조화된 SSM, 특히 우리의 방법과 관련이 있는 SSM에 대한 간략한 개요를 설명한다.

* S4(Gu, Goel, and Re, 2022; Gu, Johnson, Goel, et al., 2021)는 대각선 구조 및 대각선 플러스 저순위(DPLR)를 기술하는 최초의 구조화된 SSM을 도입하였다. 연속시간 온라인 암기(HIPPO)로의 연결로 인해 DPLR SSM을 위한 효율적인 컨볼루션 알고리즘에 초점을 맞추었다(Gu, Dao, et al., 2020).
* DSS(Gupta, Gu, and Berant, 2022)는 먼저 HIPPO 초기화에 근사하여 대각 구조 SSM의 경험적 유효성을 발견하였다. 이는 S4D(Gu, Gupta, et al., 2022)에서 이론적으로 확장되었다.
* S5(Smith, Warrington, and Linderman, 2023)는 대각 SSM 근사치를 독립적으로 발견하였으며, 병렬 스캔과 함께 순환적으로 계산되는 최초의 S4 모델이다. 그러나 이는 SSM 차원을 SISO(단일 입력 단일 출력)에서 MIMO(다중 입력 다중 출력) 공식으로 전환하여 달성한 유효 상태 차원을 낮추는 것이 필요했다. 제안된 S6은 스캔을 공유하지만, (i) SISO 차원을 유지하여 더 큰 유효 순환 상태를 제공하는 것, (ii) 계산 문제를 극복하기 위해 하드웨어 인식 알고리즘을 사용하는 것, (iii) 선택 메커니즘을 추가하는 것에 따라 다르다.

Lu 등(2023)은 에피소드 궤적들 사이의 SSM 상태를 리셋하는 것을 핸들링하기 위해 S5를 메타-RL에 적용하였다. 그들의 메커니즘은 입력에 의존하는 학습 가능한 메커니즘 대신 \(\overline{\mathbf{A}}\)을 수동으로 \(\mathbf{0}\)으로 설정하는 선택 메커니즘의 특정 하드 코딩 인스턴스로 볼 수 있다. 모델이 에피소드 경계에서 상태를 자동으로 재설정하는 방법을 배웠다면 선택적 SSM을 이 설정 및 프로브에 일반적으로 적용하는 것이 흥미로울 것이다.
* 메가(Ma 등, 2023)는 S4를 복소 값 대신 실수 값으로 단순화하여 지수 이동 평균(EMA)으로 해석하는 것을 도입하였다. 그들은 또한 SSM의 이산화 단계를 EMA 감쇠 항과 흥미롭게 연결한다. 이것은 원래 S4 논문의 결과와 달리 실제 값 SSM이 특정 설정에서 또는 다른 아키텍처 구성 요소와 결합될 때 경험적으로 효과적이라는 것을 보여주는 첫 번째 모델이었다.
* Liquid S4(Hasani 등, 2023)는 또한 입력-의존 상태 전이를 갖는 S4를 증강시킴으로써 동기 부여된다. 이러한 관점에서 그것은 여전히 컨볼루션 방식으로 계산되고 LTI에 가까운 제한된 형태이지만 선택 메커니즘과 유사성을 공유한다.
* S6Conv(Y). Li et al., 2023), Hyena (Poli et al., 2023), LongConv (Fu et al., 2023), MultiresConv (J. Shi, K. A. Wang, and Fox, 2023) , 및 Toeplitz Neural Network (Qin, Han, W. Sun, He, et al., 2023) 모두 S4의 컨볼루션 표현에 집중하고 상이한 파라미터화를 갖는 전역 또는 긴 컨볼루션 커널을 생성한다. 그러나 이러한 방법들은 빠른 자기회귀추론을 직접 수행할 수 없다.

특히, 이러한 모든 방법과 우리가 알고 있는 다른 모든 구조화된 SSM은 비선택적이며 일반적으로 엄격하게 LTI(선형 시간 불변)였다.

### SSM Architectures

우리는 이전의 SSM들 중 하나를 블랙박스 계층으로 통합한 심층 신경망 아키텍처를 지칭하기 위해 SSM 아키텍처들 또는 상태 공간 신경망(SSNN)을 사용한다.

* GSS(Mehta et al., 2023)는 SSM을 통합한 최초의 게이트 신경망 아키텍처였다. 이것은 Hua 등(2022)의 게이트 어텐션 유닛(GAU)에 의해 동기화되고, 추가 투영을 제외하고는 우리의 블록과 상당히 유사하게 보인다. 가장 중요한 것은 그것의 투영은 SSM의 상태 크기를 줄이기 위해 모델 차원을 수축시키는 반면, 우리의 투영은 섹션 3.1의 동기를 기반으로 상태 크기를 증가시키기 위해 모델 차원을 확장한다.

* 메가(Ma 등, 2023)는 전술한 S4의 EMA 단순화를 효율적인 어텐션 근사를 사용하여 하이브리드 아키텍처로 결합시켰다.
* H3(Dao, Fu, Saab, et al., 2023)는 S4와 선형 주의력을 결합하여 동기를 부여한다(Katharopoulos et al., 2020). 선형 주의의 이 공식화를 보다 일반적인 재발에 일반화한 것은 처음이며, 이는 후기 아키텍처의 기본이기도 하다.
* Selective S4 (J. Wang et al., 2023) S4를 블랙박스로 통합하여 입력에 곱해지는 이진 마스크를 생성한다. "선택" 이름을 공유하면서, 우리는 이것을 선택 메커니즘(부록 A)보다 아키텍처 게이팅에 더 가까운 아키텍처 수정으로 간주한다. 예를 들어, 우리는 무관한 입력들을 단순히 마스킹하는 것이 관련된 입력들 사이의 간격에 영향을 미치지 않기 때문에 선택 복사 태스크를 해결하지 않을 것이라고 가정한다(실제로, 선택 복사 태스크는 잡음 토큰들이 0으로 임베딩되는 경우 심지어 미리 마스킹되는 것으로 볼 수 있다).
* RetNet (Y). Sun 등, 2023)은 또한 Linear Attention에 기초하고 H3와 매우 유사하지만, 내부 S4 레이어를 상태 차원이 \(N=1\)인 특수한 경우로 감소시킨다. 이와 같이 프레임화되지는 않았지만, 그것의 재발은 선형 SSM의 특별한 경우로 볼 수 있다. 그것의 주요 개선 원인은 머리 치수가 큰 선형 주의를 사용하는 것이며, 이는 입력 의존 상태 확장을 수행하는 또 다른 방법으로 볼 수 있다. 선형 주의 변형의 맥락에서 더 큰 머리 차원을 사용하는 것은 H3에 의해 처음 수행되었지만 비례량의 추가 계산이 필요하기 때문에 광범위하게 사용되지 않았다. RetNet은 간단한 EMA 역할을 하는 SSM의 특별한 경우에 의해 실현 가능한 컨볼루션 대신 표준 다중 헤드 주의의 변형으로 계산을 병렬화하는 대체 방법으로 이를 방지한다.
* RWKV (B. Peng et al., 2023) 는 언어 모델링을 위해 설계된 또 다른 최근의 RNN이다. AFT(attention-free Transformer(S. Zhai et al., 2021)), 또 다른 변형예 (p<0.05). 주요 "WKV" 메커니즘은 LTI 재발을 포함하며 두 SSM의 비율로 볼 수 있다.

또한 Transformer의 MHA 블록과 MLP 블록을 결합하여 동기화된 Hua 등(2022)의 게이트 어텐션 유닛(GAU)을 강조하여 H3 블록과 MLP 블록을 결합한 아키텍처(섹션 3.4)에 영감을 주었다.

### RNNs 관계

RNN과 SSM은 모두 잠재 상태에서의 재발 개념을 포함하기 때문에 광범위하게 관련되어 있다.

강력하게 타이핑된 RNN (Balduzzi and Ghifary, 2016), quasi-RNN (QRNN)(Bradbury et al., 2016), 및 단순 순환 유닛 (SRU)(Lei, 2021; Lei et al., 2017)과 같은 몇몇 오래된 RNN은 시간-방향 비선형성이 없는 게이티드 RNN의 형태를 포함한다. 게이팅 메커니즘과 선택 메커니즘의 연결로 인해 이들은 선택적 SSM의 사례로 볼 수 있으므로 위의 LTI 구조 SSM 계열보다 어떤 의미에서 더 강력하다. 주요 차이점은 다음과 같다.

* 상태 확장 (\(N=1\)) 또는 선택적 \(B,C\) 매개 변수를 사용 하지 않으며 둘 다 성능에 중요 합니다 (섹션 4.6).
* 선택 메커니즘 + 이산화(정리 1)의 결과로 일반화하는 휴리스틱 게이팅 메커니즘을 사용합니다. 원칙적인 SSM 이론에 대한 연결은 더 나은 파라미터화 및 초기화를 제공한다(섹션 3.6).

또한, 오래된 RNN은 효율성 문제와 소실 구배 문제(파스카누, 미콜로프 및 벵지오, 2013)로 유명하며 둘 다 순차적 특성으로 인해 발생한다. 후자는 병렬 스캔을 활용하여 위의 RNN 중 일부에 대해 해결할 수 있었지만(Martin and Cundy, 2018), 전자는 나중에 SSM에 대해 개발된 이론 없이는 어려웠다. 예를 들어, 현대의 구조화된 SSM은 고전적 SSM 이론으로부터 영감을 받은 순환 역학(예를 들어, 이산화(Gu, Johnson, Goel, et al., 2021; Gu, Johnson, Timalsina, et al., 2023)), 또는 직접 분석(Orvieto et al., 2023)의 보다 신중한 파라미터화가 상이하다.

또한 직교 RNNs(Arjovsky, Shah, and Bengio, 2016; Henaff, Szlam, and LeCun, 2016; Lezcano-Casado and Martinez-Rubio, 2019; Mhammedi et al., 2017; Vorontsov et al., 2017)에 대한 긴 작업 라인이 있다는 점에 주목한다. 그러나 이것들은 다른 한계를 가지고 있었고, 우리는 이것이 직교/단일 RNN도 LTI라는 사실에서 비롯되었다고 믿는다. 예를 들어, 이들은 거의 항상 완벽하게 해결할 수 있는 복사 과제에 대해 평가되지만, 선택 복사 과제에 대해 투쟁하는 것으로 관찰된다(Jing 등, 2019).

### Linear Attention

선형 주의력(Linear Attention, LA) (Katharopoulos et al., 2020) 프레임워크는 커널 주의를 대중화하고 순환 자기 회귀 모델과 어떻게 관련이 있는지 보여주는 중요한 결과이다. 많은 변형들이 대안적인 커널들 및 다른 변형들을 제안했다. RFA(Random Feature Attention)(H). Peng et al., 2021)은 가우시안 커널(Rahimi and Recht, 2007)의 랜덤 푸리에 특징 근사화를 이용하여 소프트맥스 어텐션(즉, exp feature map)을 근사하기 위해 커널 특징 맵을 선택한다. 퍼포먼서(Choromanski 등, 2021)는 양의 특징만을 포함하는 지수 커널에 대한 근사치를 찾고, 이 근사치는 소프트맥스 정규화 항도 허용한다. TransNormer (Qin, Han, W. Sun, D. Li, et al., 2022) LA 분모항이 불안정할 수 있음을 보여주고 LayerNorm으로 대체하는 것을 제안했다. cosFormer (Qin, W. Sun, et al., 2022) 지역성을 강조하기 위해 위치 정보를 통합하는 코사인 재가중 메커니즘으로 RFA를 증강한다. Linear Randomized Attention (Zheng, C. Wang, and L. Kong, 2022) RFA를 중요도 샘플링 관점에서 일반화하고 일반화하여 전체 소프트맥스 커널에 대한 더 나은 추정치를 제공한다.

커널 주의 외에도, 효율적인 주의의 많은 다른 변형들이 존재한다; 조사 Tay, Dehghani, Bahri 등(2022)은 이들 중 많은 것에 대한 광범위한 분류를 제공한다.

### Long Context Models

긴 컨텍스트가 인기 있는 주제가 되었으며 몇 가지 최근 모델이 점점 더 긴 시퀀스로 확장한다고 주장했다. 그러나 이는 종종 계산적 관점에서 발생하며 광범위하게 검증되지 않았다. 이들은 다음을 포함한다:

* Reurrent Memory Transformer (Bulatov, Kuratov 및 Burtsev, 2023)는 Transformer 백본 주변의 경량 래퍼입니다. 최대 1M 시퀀스를 일반화하지만 합성 암기 작업에서만 일반화하는 능력을 보여주었으며, 주요 결과는 우리의 유도 머리 외삽 실험(표 2)과 유사하다.
* LongNet(Ding et al., 2023)은 1B 길이로 확장한다고 주장했지만 실제 작업에 대해 길이 \(<100\)K에 대해서만 평가했다.
* Hyena and HyenaDNA (Nguyen, Poli, et al., 2023; Poli et al., 2023)는 최대 1M 컨텍스트를 활용한다고 주장했다. 그러나 그들의 실험은 더 긴 컨텍스트에서 비례적으로 더 많은 데이터에 대해 훈련되어 1M 컨텍스트에서 품질 개선이 컨텍스트 길이 때문인지 또는 더 많은 데이터 및 계산 때문인지 결론을 내리기가 어렵다.
* Sparse Transformer(Child et al., 2019)는 길이 \(2^{20}=1048576\)의 오디오 파형을 모델링하기 위해 strided sparse attention Transformer를 사용하는 개념 증명을 보여주었지만, 계산 및 모델 크기를 제어할 때 성능 트레이드오프에 대해서는 논의하지 않았다.

대조적으로, 우리는 이 작업이 더 긴 맥락에서 증가하는 성능을 의미 있게 보여주기 위한 첫 번째 접근법 중 하나를 제시한다고 믿는다.

## Appendix C Mechanics Selective SSMs

정리의 증명 1.: 선택적 SSM(알고리즘 2)을 \(N=1\), \(\mathbf{A}=-1\), \(\mathbf{B}=1\), \(s_{\Delta}=\text{Linear}(x)\), \(\tau_{\Delta}=\text{softplus}\)으로 간주한다. 상기 대응하는 연속시간 SSM(1)은,

\[h(t)=-h(t)+x(t)\]

이는 _리키 적분기_라고도 한다.

상기 이산화 스텝 사이즈는,

\[\Delta_{t} =\tau_{\Delta}(\text{Parameter}+s_{\Delta}(x_{t}))\] \[=\text{softplus}(\text{Parameter}+\text{Linear}(x_{t}))\] \[=\text{softplus}(\text{Linear}(x_{t}))\]

여기서 우리는 매개변수가 학습 가능한 편향으로 볼 수 있고 선형 투영으로 접힐 수 있음을 관찰한다.

이제 영차 홀드(ZOH) 이산화 공식들을 적용한다:

\[\overline{\mathbf{A}}_{t} =\exp(\Delta\mathbf{A})=\frac{1}{1+\exp(\text{Linear}(x_{t})}=\sigma (-\text{Linear}(x_{t}))\] \[=1-\sigma(\text{Linear}(x_{t}))\] \[\overline{\mathbf{B}}_{t} =(\Delta\mathbf{A})^{-1}(\exp(\Delta\mathbf{A})-\mathbf{I})\cdot\Delta\mathbf{B}= -(\exp(\Delta\mathbf{A})-\mathbf{I}=1-\overline{\mathbf{A}}\] \[=\sigma(\text{Linear}(x_{t}))\]\[\overline{\mathbf{A}})\]\]\[\overline{\mathbf

따라서 최종 이산 반복(2a)은

\[g_{t} =\sigma(\text{Linear}(x_{t}))\] \[h_{t} =(1-g_{t})h_{t-1}+g_{t}x_{t}\]

원하는 대로.

## 선택적 SSM을 위한 부록 D 하드웨어 인식 알고리즘

입력 의존적 선택성 없이 SSM은 고속 푸리에 변환(FFT)을 프리미티브로 활용하는 컨볼루션(Dao, Fu, Saab, et al., 2023; Gu, Goel, and Re, 2022)으로 효율적으로 구현될 수 있다. 선택성을 사용하면 SSM은 컨볼루션과 더 길지 않지만 병렬 연관 스캔을 활용한다. SSM 스캔은 이론적으로 효율적이지만(\(O(BLDN)\) FLOP, \(L\)에서 선형 스케일링, \(L\)), 선택적 SSM을 사용하여 기초 모델을 훈련하려면 최신 하드웨어(GPU)에서도 효율적이어야 한다. 우리는 SSM 스캔을 빠르고 메모리 효율적으로 만들기 위해 커널 융합과 재컴퓨팅을 사용하는 방법을 설명한다. 섹션 4.5에서 컨벌루션 및 어텐션과 비교하여 스캔 구현 속도를 평가하여 시퀀스 길이 32K에서 어텐션보다 최대 7\(\times\)배 빠르고 최고의 어텐션 구현(FlashAttention)만큼 메모리 효율적이다.

Speed.On modern hardware accelerators(GPU)에서 대부분의 연산(except matrix multiply)은 메모리-대역폭에 의해 바운딩된다(Dao, Fu, Ermon, et al., 2022; Ivanov et al., 2021; Williams, Waterman, and Patterson, 2009). 이것은 스캔 동작의 경우이며, 커널 융합을 사용하여 메모리 IO의 양을 줄여 표준 구현에 비해 상당한 속도를 낸다.

섹션 3.2에서 스캔 알고리즘을 구현하는 표준 방법은 GPU HBM(high-bandwidth memory, 일반적으로 GPU 메모리라 칭함)에서 크기 \((\mathbf{B},\mathbf{L},\mathbf{D},\mathbf{N})\)의 스캔 입력 \(\overline{\mathbf{A}},\overline{\mathbf{B}}\)을 준비하고, 병렬 연관 스캔 구현을 호출하여 크기 \((\mathbf{B},\mathbf{L},\mathbf{D},\mathbf{N})\)의 스캔 출력을 GPU HBM에 기록하고, 그 스캔 출력을 \(\mathbf{C}\)과 곱하여 크기 \((\mathbf{B},\mathbf{L},\mathbf{D})\)의 출력을 생성한다. 그러나, 이것은 \(O(BLDN)\)의 순서로 메모리 판독/기록의 수를 필요로 한다. 대신 이산화 단계, 스캔 및 \(\mathbf{C}\)과의 곱셈을 하나의 커널로 융합할 수 있습니다.

1. 느린 HBM에서 빠른 SRAM으로 \(O(BLD+DN)\) 바이트의 메모리 \((\Delta,\mathbf{A},\mathbf{B},\mathbf{C})\)를 읽습니다.
2. SRAM에서 \(\overline{\mathbf{A}},\overline{\mathbf{B}}\) 크기의 \(\mathbf{B},\mathbf{L},\mathbf{D},\mathbf{N})\)을 생성하도록 이산화한다.
3. 병렬연관 스캔을 수행하여 SRAM에서 크기 \((\mathbf{B},\mathbf{L},\mathbf{D},\mathbf{N})\)의 중간 상태를 산출한다.
4. \(\mathbf{C}\)과 곱하고 합하여 \((\mathbf{B},\mathbf{L},\mathbf{D})\) 크기의 출력을 생성하여 HBM에 기록한다.

이렇게 하면, IOs를 \(O(\mathbf{N})\)(상태 차원)의 배수로 줄일 수 있으며, 이는 실제로 연산을 20-40배 가속화시킨다(섹션 4.5).

시퀀스 길이 \(L\)이 너무 긴 SRAM(HBM보다 훨씬 작은)에서 시퀀스를 맞출 수 없는 경우, 시퀀스를 청크로 분할하고 각 청크에 대해 융합 스캔을 수행한다. 중간 스캔 상태가 있는 한 다음 청크로 스캔을 계속할 수 있다.

메모리.우리는 선택적 SSM 계층을 훈련하는 데 필요한 메모리의 총량을 줄이기 위해 재계산이라는 고전적인 기술을 사용하는 방법을 설명한다.

순방향 패스를 퓨즈하는 방식으로부터 메모리 블로우업을 피하기 위해 크기 \((\mathcal{B},L,D,N)\)의 중간 상태를 저장하지 않는다. 그러나, 이러한 중간 상태들은 백워드 패스가 구배들을 계산하기 위해 필요하다. 우리는 대신 그 중간 상태들을 뒤쪽 패스에서 재계산한다. 입력 \(\Delta,\boldsymbol{A},\boldsymbol{B},\boldsymbol{C}\)과 HBM에서 SRAM으로 읽히는 출력 기울기는 크기 \(O(BLN+DN)\), 입력 기울기 또한 크기 \(O(BLN+DN)\이므로 재계산으로 HBM에서 \(O(BLND)\) 요소를 읽는 비용을 피할 수 있다. 이는 역방향 경로에서 SSM 상태를 재계산하는 것이 저장 및 HBM에서 읽는 것보다 계산 속도를 향상시킨다는 것을 의미한다.

또한 전체 선택적 SSM 블록(입력 투영, 컨볼루션, 활성화, 스캔, 출력 투영)의 메모리 요구량을 최적화하기 위해 재계산(recomputation)을 사용한다. 특히, 많은 메모리를 필요로 하지만 재계산(예를 들어, 활성화 함수의 출력 또는 짧은 컨볼루션)에 빠른 중간 활성화들을 저장하지 않는다. 그 결과, 선택적 SSM 계층은 FlashAttention을 적용한 최적화된 Transformer 구현과 동일한 메모리 요구량을 갖는다. 특히, 각 주의 계층(FlashAttention)은 토큰당 약 12 바이트의 액티베이션들을 저장하고, 각 MLP 계층은 총 32 바이트에 대해(FP16 또는 BF16에서 혼합-정밀 트레이닝을 가정함) 토큰당 약 20 바이트의 액티베이션들을 저장한다. 각 선택적 SSM은 토큰당 약 16 바이트의 활성화를 저장합니다. 따라서 선택적 SSM의 두 층은 주의 계층과 MLP 계층과 동일한 활성화 메모리를 가지고 있다.

## 부록 E 실험 세부 정보 및 추가 결과

### Synthetic Tasks

선택적 복사.우리의 설정은 16개의 가능한 토큰(도 2의 백색 "잡음" 토큰을 포함함)의 어휘 크기를 가지며 모델들이 16개의 "데이터" 토큰을 기억하도록 요구하는 길이 4096의 시퀀스에 있다. 모델 차원이 \(D=64\)인 2개의 계층 모델을 사용한다.

모델들은 400K 단계에 대해 \(64\)의 배치 크기를 갖는 \(0.0001\)의 일정한 학습 속도로 학습된다.

Induction Heads.Training은 일괄 크기 \(8\)으로 매 단계마다 무작위로 데이터를 생성하는 것으로 구성된다. 우리는 8192 단계의 "에포크" 크기를 선택하고 각 표적 서열 길이의 고정된 검증 세트(또한 무작위로 생성됨)에서 정확도를 추적한다. MHA-Abs 모델과 Mamba 모델의 경우, 25번째 단계(\(8192\times 25=204800\) 단계) 이후에 결과가 보고된다. MHA-RoPE 및 MHA-xPos 모델의 경우 50번째 단계(\(8192\times 50=409600\) 단계) 후에 결과가 보고된다. LTI H3 및 하이에나 모델의 경우 10단계(\(81920\) 단계) 이후에 결과가 보고되는데, 이는 그때까지 수렴하여 더 이상 개선되지 않았기 때문이다.

\begin{table}
\begin{tabular}{l l l l l l l l l l l l l l l l l} \hline \hline Model & Params & \multicolumn{10}{c}{Test Accuracy (\%) at Sequence Length} \\ \cline{3-14}  & & \(2^{6}\) & \(2^{7}\) & \(\boldsymbol{2^{8}}\) & \(2^{9}\) & \(2^{10}\) & \(2^{11}\) & \(2^{12}\) & \(2^{13}\) & \(2^{14}\) & \(2^{15}\) & \(2^{16}\) & \(2^{17}\) & \(2^{18}\) & \(2^{19}\) & \(2^{20}\) \\ \hline MHA-Abs & 137K & ✓ & 99.6 & 100.0 & 58.6 & 26.6 & 18.8 & 9.8 & 10.9 & 7.8 & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ MHA-RoPE & 137K & ✓ & ✓ & 100.0 & 83.6 & 31.3 & 18.4 & 8.6 & 9.0 & 5.5 & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ MHA-xPos & 137K & ✓ & ✓ & 100.0 & 99.6 & 67.6 & 25.4 & 7.0 & 9.0 & 7.8 & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ H3 & 153K & ✓ & ✓ & 100.0 & 80.9 & 39.5 & 23.8 & 14.8 & 8.2 & 5.9 & 6.6 & 8.2 & 4.7 & 8.2 & 6.3 & 7.4 \\ Hyena & 69M* & 97.7 & ✓ & 100.0 & ✓ & 44.1 & 12.5 & 6.6 & 5.1 & 7.0 & 5.9 & 6.6 & 6.6 & 5.9 & 6.3 & 9.8 \\ Mamba & 74K & ✓ & ✓ & 100.0 & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\ \hline \hline \multicolumn{14}{l}{* Most of the parameters are in learnable positional encodings.} \\ \end{tabular}
\end{table}
표 11: (**유도 헤드.**) 모델은 시퀀스 길이 \(2^{8}=256\)에 대해 트레이닝되고, \(2^{20}=1048576\)까지의 다양한 시퀀스 길이 \(2^{6}=64\)에 대해 테스트된다. ✓ 은 완전한 일반화 정확도를 나타내는 반면, ✗은 메모리가 부족함을 나타낸다.

우리는 체중 감량이 없는 아담 최적화기를 사용한다. 모든 모델은 일정한 학습률 \(2e-4\)과 \(1e-3\)으로 학습되며, Mamba를 제외한 모든 모델에 대해 각 모델(\(2e-4\)에 대해 더 나은 결과가 보고된다. Attention 모델과 Hyena 모델은 LR \(1e-3\)에서 학습하지 않았다. H3는 두 LR에서 학습되었지만 흥미롭게도 \(2e-4\)의 더 작은 LR에서 더 짧은 서열로 더 잘 일반화되었다. Mamba는 두 LR에서 학습했지만 \(1e-3\)의 더 큰 LR에서 더 잘 외삽되었다.

### Language Modeling

#### e.2.1 Scaling Law Details

모든 모델은 파일에서 훈련되었습니다.

모형 크기.표 12는 스케일링 법칙에 사용하는 모형 크기를 지정합니다. 이것은 GPT3 사양(Brown et al., 2020)으로부터 직접 취하여, 매우 작은 수정을 가한다. 먼저, 1.3B 모델의 배치 크기를 1M 토큰에서 0.5M 토큰으로 변경했는데, 이는 더 큰 배치 크기를 요구하기에 충분한 병렬화를 사용하지 않았기 때문이다. 둘째, 훈련 단계 수와 총 토큰 수를 Chinchilla scaling laws(Hoffmann et al., 2022)와 대략 일치하도록 변경하였으며, 훈련 토큰은 모델 크기에 비례하여 증가해야 함을 명시하였다.

훈련 레시피.모든 모델은 AdamW 최적화기와 함께 사용하였다.

* 그래디언트 클립값 1.0
* 체중감량 0.1
* Dropout 없음
* 코사인 붕괴를 동반한 선형 학습률 워밍업

기본적으로, 피크 학습률은 GPT3 사양이다.

우리는 PaLM(Chowdhery et al., 2023) 및 LLaMa(Touvron et al., 2023)와 같은 인기 있는 대형 언어 모델에 의해 채택된 변경에서 영감을 받은 "개선된 레시피"를 여러 모델에 제공한다. 이들은 다음을 포함한다:

* 코사인 감쇠를 \(1e-5\)으로 하는 선형 학습 속도 워밍업, 5x의 피크 값을 갖는 GPT3 값
* 선형 바이어스 항 없음
* LayerNorm 대신 RMSNorm
* AdamW hyperparameter \(\beta=(.9,.95)\)(the GPT3 value) instead the PyTorch default of \(\beta=(.9,.999)\)

건축 및 교육 세부 정보.우리의 모델은 다음과 같습니다.

* 변압기: GPT3 기반의 표준 변압기(표 12).
* Transformer++: 개선된 아키텍처, 즉 회전 위치 인코딩(Su et al., 2021) 및 SwiGLU MLP(Shazeer, 2020)를 갖는 Transformer, 및 상기 개선된 트레이닝 레시피를 포함한다.
* 하이에나: 하이에나 블록(S4를 갖는 H3 블록은 MLP에 의해 파라미터화된 글로벌 컨볼루션으로 대체됨)을 표준 MLP 블록들과 인터리빙. MLP 블록들은 4 대신에 확장 인자 2를 가지며, 파라미터 카운트를 보존하기 위해 층들의 수는 상응하게 1.5배 증가된다.

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline Params & n\_layers & d\_model & n\_heads / d\_head & Training steps & Learning Rate & Batch Size & Tokens \\ \hline
125M & 12 & 768 & 12 / 64 & 4800 & 6e-4 & 0.5M 토큰 & 2.5B \\
350M & 24 & 1024 & 16/64 & 13500 & 3e-4 & 0.5M 토큰 & 7B \\
760M & 24 & 1536 & 16 / 96 & 29000 & 2.5e-4 & 0.5M 토큰 & 15B \\
1.3B & 24 & 2048 & 32 / 64 & 50000 & 2e-4 & 0.5M tokens & 26B \\ \hline \hline \end{tabular}
\end{table}
표 12: (**스케일링 법칙 모델 크기.**) 스케일링 실험을 위한 모델 크기 및 하이퍼파라미터. (모델 차원 및 헤드 수는 Transformer 모델에만 적용된다.)* H3++: (i) 위의 동일한 "얇은" 하이에나 차원을 사용하는 (ii) 위의 개선된 트레이닝 레시피 (iii) 8의 선형 주의력 헤드 차원을 포함하는 몇 가지 수정을 갖는 H3 아키텍처.
* RWKV: B. Peng 등(2023)으로부터의 디폴트 RWKV 모델, 그 수정된 MLP 블록을 포함한다. 또한 특정 파라미터에 대해 학습률을 2\(\times\) 또는 3\(\times\) 증가시키는 등 지정된 학습 레시피를 최대한 사용했다.
* RetNet: Y의 기본 RetNet 모델입니다. Sun 등(2023). 우리는 또한 위의 개선된 훈련 레시피를 주었습니다.
* 맘바: 개선된 트레이닝 레시피와 함께 표준 맘바 아키텍처.

#### e.2.2 Additional Scaling Law Ablations

우리는 그림 4(왼쪽)의 2k 컨텍스트 길이 스케일링 법칙과 동일한 프로토콜을 사용하여 아키텍처에 대해 추가 삭제를 수행한다.

맘바 아키텍처: 인터리빙 블록.우리는 맘바 블록과 결합된 상이한 건축 블록의 효과를 테스트한다. 우리는 Mamba 블록이 단순히 여분의 \(\mathsf{conv}\rightarrow\mathsf{SSM}\) 경로를 추가한 표준 SwiGLU 블록이라는 관점에 초점을 맞춘다. 이는 두 개의 자연적인 삭마로 이어진다:

* Mamba 블록이 균질하게 쌓이는 대신 표준 MLP 블록과 인터리빙되는 경우 어떻게 됩니까? 이는 맘바를 복용하고 SSM의 절반을 제거하는 것으로도 해석할 수 있다.
* Mamba 블록이 MHA(다중 머리 주의) 블록과 인터리빙되면 어떻게 되나요? 이것은 또한 SwiGLU MLP(즉, 우리가 Transformer+라고 부르는 것)를 갖는 Transformer를 취하고 단순히 MLP 블록에 SSM을 추가하는 것으로 해석될 수 있다.

그림 9(오른쪽)는 원래(동질적) 맘바 아키텍처와 비교하여 이러한 변형을 보여준다. 흥미롭게도, 어떤 변화도 큰 문제가 되지 않는다. Mamba-MLP 구조는 Transformer++를 제외한 모든 모델보다 약간 더 나쁘고 여전히 더 좋다. Mamba-MHA 아키텍처는 단지 약간 더 우수하며, 이는 많은 최근 작업들이 (LTI) SSM들을 Attention과 결합하는 것이 실질적인 개선으로 이어질 수 있다는 것을 발견했다는 사실에 비추어 다소 놀랍다(Dao, Fu, Saab, et al., 2023; Fathi et al., 2023; Fathullah et al., 2023; Saon, Gupta, and Cui, 2023; Zuo et al., 2022).

H3 아키텍처: 훈련 레시피. 다음으로 트랜스포머++와 맘바 외부에서 가장 약한 모델과 가장 강한 모델인 하이에나와 H3++ 모델의 차이점을 제거하며, 특히 훈련 레시피의 효과를 분리한다.

* 하이에나: 원래의 아키텍처와 GPT3 트레이닝 레시피를 갖는 하이에나 블록(도 4와 동일).
* Hyena+: 동일한 아키텍처이지만 전술한 개선된 트레이닝 레시피를 갖는다.
* H3+: 하이에나+와 동일한 아키텍처이지만 하이에나 컨볼루션 커널이 S4D 컨볼루션 커널로 스왑 아웃된 구조.
* H3++: H3+와 동일하지만 선형 주의 머리 치수가 8입니다. 이로 인해 SSM 반복 내부에서 계산이 증가하지만 매개 변수는 증가하지 않습니다.

우리의 일반적인 관례는 "모델+"가 개선된 트레이닝 레시피로 기본 모델을 나타내고 "모델++"도 아키텍처 변경을 허용한다는 것이다.

도 9(오른쪽)은

* 개선된 트레이닝 레시피에 의해 큰 개선이 달성되며, 이는 주요 그림 4의 많은 모델(RetNet, H3++, Transformer++, Mamba)에 사용되었다.
* 내부 LTI SSM의 선택은 중요하지 않습니다 (예: Hyena 대 S4). 이 논문 전반에 걸친 결과와 일치한다.
* 헤드 차원 확장은 SSM에 대한 성능을 향상시키는 상태 차원을 확장한 주요 주제 중 하나와 일치하여 성능을 향상시킵니다 (섹션 3).

#### e.2.3 Downstream Evaluation Details

이 프리트레이닝 절차는 스케일링 법칙 프로토콜과 동일하지만, 300B 토큰으로 확장된다. 1.3B 모델의 경우 GPT3 사양과 일치하도록 1M 토큰의 배치 크기를 사용합니다. 우리는 파일 검증 세트에 대한 복잡성을 보고하며, 이 메트릭에 대해 동일한 데이터 세트 및 동일한 토큰나이저, 특히 피티아 및 RWKV에서 훈련된 모델과만 비교한다.

하류 평가를 위해 EleutherAI(L. Gao, Tow, et al.2021)의 LM 평가 하네스를 사용한다. 이 지역에서 대부분의 작업이 수행한 대로입니다. 상식 추론을 측정하는 다음 작업/데이터 세트에 대해 평가합니다.

* LAMBADA(Paperno et al.2016).
* HellaSwag (Zellers et al.2019).
* PIQA (Bisk et al.2020).
* ARC-challenge (P. Clark et al.2018).
* ARC-easy: ARC-challenge의 쉬운 부분 집합입니다.
* WinoGrande (Sakaguchi et al.2021).

LAMBADA, WinoGrande, PIQA 및 ARC-easy에 대한 정확도와 HellaSwag 및 ARC-챌린지에 대한 서열 길이로 정규화된 정확도를 보고한다(정규화된 정확도는 이러한 작업에 대한 거의 모든 모델에 대해 더 높기 때문에).

### DNA Modeling

#### e.3.1 Pretraining Details

우리는 HG38 사전 훈련 작업의 데이터 세트와 훈련 절차를 더 자세히 설명한다.

데이터셋은 Genomics에 대한 이전 Enformer 작업(Avsec et al.2021)의 분할을 따르며, 훈련 분할은 총 약 45억 개의 토큰(DNA 염기쌍)에 대해 유전체를 포함하는 총 \(S=34021\) 길이의 \(2^{17}=131072\) 세그먼트를 포함한다. 이러한 세그먼트는 (염색체 수, 시작 인덱스, 종료 인덱스)의 쌍이며, 필요한 경우 (예를 들어, 더 긴 세그먼트를 얻기 위해) 확장될 수 있다.

우리는 훈련 서열 길이가 \(2^{17}\)이 아닐 때 HyenaDNA에서 벗어난다. HyenaDNA는 항상 고정된 하위 세그먼트(예: 규정된 세그먼트의 시작 또는 중간)를 취하므로 각 에폭의 훈련 서열 길이에 대해 \(34021\) 샘플에 고정되며 전체 유전체를 반드시 통과하는 것은 아니다. 한편, 전체 훈련 데이터를 사용한다:

* 컨텍스트 길이 \(L\)가 \(2^{17}\)보다 작거나 같을 때 각 세그먼트를 길이 \(L\)의 겹치지 않는 하위 세그먼트로 나누어 \(S\times\frac{2^{17}}{L}\) 총 샘플과 epoch당 \(S\times 2^{17}\approx 4.5B\ 토큰이 있습니다.
* 컨텍스트 길이 \(L\)가 \(2^{17}\)보다 크면 각 세그먼트를 지정된 세그먼트로 시작하는 샘플과 지정된 세그먼트로 끝나는 두 개의 샘플로 바꿉니다. 따라서 각 에폭에는 \(2S\) 항목과 \(2SL\)이 있습니다.

그림 9: (**Scaling laws: extra ablations.**) (_Left_) 대신 (_Right_) epoch당 tokens 대신입니다. 예를 들어, 시퀀스 길이 \(2^{18}=262144\)에서는 기본값만큼의 토큰이 있고, 시퀀스 길이 \(2^{20}\)에서는 16개의 토큰이 있다.

다른 훈련 세부사항들은 일반적으로 우리의 언어 모델링 실험과 동일한 프로토콜을 따른다(부록 E.2). 예를 들어, AdamW는 \((\mathbf{\beta}_{1},\mathbf{\beta}_{2})=(0.9,0.95)\), 탈락 없음, 체중 감량 \(0.1\)을 사용한다. 전체 스텝의 \(10\%\)에 대해 선형 워밍업을 갖는 코사인 학습률 스케줄러를 사용한다.

#### e.3.2 Scaling: 모델 크기 세부 정보

모델. 우리가 고려하는 모델은 다음과 같다:

* Transformer++: 개선된 아키텍처를 갖는 Transformer, 특히 RoPE 위치 인코딩의 사용(Su et al., 2021). 공식적으로, 우리는 이것들이 바닐라 위치 인코딩보다 눈에 띄게 더 낫다는 것을 발견했다(바스와니 등, 2017).
* HyenaDNA: Nguyen, Poli, et al.(2023) 및 Poli et al.(2023)의 Hyena 모델로서, MLP에 의해 파라미터화된 글로벌 컨벌루션을 사용하여 MHA 블록이 H3 블록으로 대체된 대략 트랜스포머이다.
* Mampa: 표준 Mampa 아키텍처입니다.

모형 크기.다음 모형 크기를 사용합니다.

Training.각 모델(Transformer++, HyenaDNA, Mampa)에 대해 \(\{1e-3,2e-3,4e-3,8e-3\}\)에 걸쳐 학습률을 휩쓸었다. 최적의 트랜스포머 및 HyenaDNA 학습률은 모든 크기에 걸쳐 2e-3이었다. 최적의 Mampa 학습률은 8e-3이었고, Mampa는 일치된 학습률(2e-3)로 기준선보다 더 나은 성능을 보였지만 더 높은 학습률에서 더 안정적이고 훨씬 더 개선되었다는 점에 주목한다. (게다가, 이 LR이 스위프의 상위 범위에 있기 때문에, 우리의 결과가 여전히 차선책일 수 있다.)

표준 LM 스케일링 법칙(표 12)과 달리 우리의 LR은 단순화를 위해 모델 크기에 걸쳐 일정하게 유지되었다. 최적 LR은 더 큰 모델에 대해 내려가야 하지만 우리가 고려한 작은 모델 크기(최대 몇 백만 매개변수)에서는 눈에 띄는 효과를 찾지 못했다.

#### e.3.3 Scaling: Context Length Details

모든 시퀀스 길이(예: 길이 \(2^{20}\)에서 배치당 16개의 세그먼트가 있고 길이 \(2^{10}\)에서 배치당 16384개의 세그먼트가 있다)에 대해 훈련 단계당 \(2^{24}\approx 16M\ 토큰의 총 배치 크기를 사용한다. 이것은 일반적인 LM 표준에 의한 모델 크기에 비해 큰 배치 크기이지만 8개의 GPU와 서열 길이가 \(2^{2}0\인 기계에서 \(2^{23}\)의 배치 크기가 가능한 최소이고 HyenaDNA는 \(2^{28}\)의 훨씬 더 큰 배치를 사용한다는 점에 유의한다.

사용된 학습률은 Mampa의 경우 \(0.008\), HyenaDNA의 경우 \(0.001\)이었으며, 처음에 HyenaDNA의 경우 이전 섹션에서 \(0.002\)의 동일한 학습률을 사용하려고 시도했지만 가장 긴 컨텍스트 길이에서 불안정하다는 것을 발견했다.

Sequence Length Warmup.Following(Nguyen, Poli, et al., 2023), 우리는 프리트레이닝 동안 Sequence Length warmup(SLW)을 사용한다. 우리는 \(2^{10}=1024\)에서 시작하는 두 수열의 각 거듭제곱 길이에서 두 에폭의 간단한 스케줄을 선택한다. (데이터를 큐레이션 하는 방법 때문에 가장 긴 시퀀스 길이에서 더 많은 단계와 토큰이 비례 하 게 사용 됩니다. 특히, 최대 길이 \(2^{17}\)까지의 각 스테이지는 동일한 수의 토큰을 처리하지만, 최대 길이 \(2^{18}\), 최대 길이 \(2^{19}\), 최대 길이 \(16\times\)의 토큰이 처리된다. )

HyenaDNA와 달리 구배 업데이트당 토큰 수를 항상 제어하므로 각 단계에서 서열 길이가 두 배로 증가함에 따라 배치 크기가 연속적으로 반으로 줄어든다.

**Remark E.1**.: _또한 일정이 조정되지 않았으며 이러한 사전 훈련 실험을 위해 시퀀스 길이 예열을 해제하는 실험을 한 적이 없습니다. 나중에 SLW가 유사한 길이(섹션 4.4)에서 오디오 사전 훈련에 눈에 띄게 도움이 되지 않았으며 DNA 사전 훈련에도 필요하지 않을 수 있음을 발견했다._

#### e.3.4 Species (Great Apes) Classification

모델은 인과 관계이므로 모델의 출력의 마지막 요소(시퀀스 길이에 걸쳐)만 분류 헤드에 사용됩니다. 우리는 기울기 단계당 손실 함수의 총 요소 수를 제어한다는 점에 유의한다. 프리트레이닝 목적은 시퀀스 길이에 걸친 모든 포지션들을 포함하고, 따라서 batch_sizexsequence_length는 일정하게 유지된다; 다시 말하면, 배치 크기는 시퀀스 길이가 증가함에 따라 감소한다. 그러나 분류 작업의 경우 마지막 위치만 손실로 들어가기 때문에 배치 크기 자체가 일정하게 유지됩니다. 이것은 또한 더 긴 시퀀스 길이를 갖는 모델들을 미세 조정하는 것이 더 계산적으로 비싸다는 것을 의미한다는 것에 유의한다.

훈련은 10개의 에폭으로 구성되며, 각 에폭은 1024개의 기울기 단계를 갖는다. 각 구배 단계는 배치 크기 64를 사용하며, 이는 모두 종을 균일하게 선택하고 염색체를 균일하게 선택한 다음 DNA의 인접한 부분을 균일하게 선택하여 독립적으로 무작위로 그려진다.

(Nguyen, Poli, et al., 2023)에 이어, \(2^{14}=16384\)보다 큰 최대 컨텍스트 길이를 갖는 모델들은 길이 \(2^{14}=16384\), 길이 \(2^{15}=32768\), 길이 \(2^{16}=65536\)에서의 1 epoch를 갖는 시퀀스 길이 워밍업 등을 최대 시퀀스 길이까지 사용한다. 예를 들어, \(2^{20}=1048576\) 컨텍스트를 갖는 모델은 최대 시퀀스 길이에서 4개의 에폭보다 먼저 6개의 에폭의 시퀀스 길이 워밍업을 겪는다.

모든 하이에나 모델의 학습률은 \(4\mathbf{e}-5\)이고, 모든 맘바 모델의 학습률은 \(1\mathbf{e}-4\)이다. 수열 길이가 작은 모델(\(2^{10},2^{12},2^{14},2^{16}\))에 대한 \(\{1\mathbf{e}-5,2\mathbf{e}-5,4\mathbf{e}-5,1\mathbf{e}-4,2\mathbf{e}-4\}\) 중에서 각 모델에 대한 학습 속도 스윕을 수행하여 이러한 값이 각 모델에서 일관되게 가장 좋은 것으로 나타났다. 이 값과 일치하는 길이 \(2^{18}\)에서 요약된 학습 속도 스윕을 수행하고 길이 \(2^{20}\)에서 단일 실행을 수행했다(위에서 설명한 바와 같이 이 실험의 계산 비용은 시퀀스 길이에 비례한다). 학습률은 최대 학습률까지 선형 예열 5에폭, 코사인 붕괴 5에폭은 \(1\mathbf{e}-6\)으로 예열된 코사인 붕괴 스케줄을 따랐다. 시퀀스 길이의 웜업도 길기 때문에 비정상적으로 긴 학습률 웜업 일정을 선택했으며(예: 컨텍스트 길이가 \(2^{20}\)인 모델에 대해 10개 에폭 중 6개를 포함함); 이 선택을 실험하지 않았다.

종 분류 작업에 대한 결과는 표 13에 나와 있다.

### Audio Details

#### e.4.1 YouTubeMix Audio Pretraining

모델.우리는 약 3.5M 매개변수에 대해 스테이지당 3개의 블록(\(3\times 5=15\), 풀링 팩터 \(p=16\), 외부 차원 \(D=64\)을 갖는 모델을 사용한다.

데이터 세트.데이터는 8비트로 mu-law 인코딩되므로, 모델은 256의 어휘 크기를 갖는 이산 토큰들을 모델링하고 있다.

데이터세트는 최대 1분 길이의 클립, 또는 길이 960000으로 구성되며, 이는 서브샘플링되고 임의의 원하는 시퀀스 길이의 세그먼트로 분할된다. 상기 아키텍처는 16배만큼 풀링의 두 단계를 포함하기 때문에,

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Model & Params & \multicolumn{4}{c}{Accuracy (\%) at Sequence Length} \\ \cline{3-6}  & & \(2^{10}\) & \(2^{12}\) & \(2^{14}\) & \(2^{16}\) & \(2^{18}\) & \(2^{20}\) \\ \hline HyenaDNA & \(1.4\)M & \(28.04\) & \(28.43\) & \(41.17\) & \(42.22\) & \(31.10\) & \(54.87\) \\ Mamba & \(1.4\)M & \(31.47\) & \(27.50\) & \(27.66\) & \(40.72\) & \(42.41\) & **71.67** \\ \hline Mamba & \(7\)M & \(30.00\) & \(29.01\) & \(31.48\) & \(43.73\) & \(56.60\) & **81.31** \\ \hline \hline \end{tabular}
\end{table}
표 13: (**Great Apes DNA 분류.**) 동일한 컨텍스트 길이의 사전 훈련된 모델을 사용하여 길이 \(2^{10}=1024\) 최대 \(2^{20}=1048576\)의 시퀀스에 대한 미세 조정 후 정확도. 무작위 추측은 20%입니다.

그리고 하드웨어 효율을 위해 결과적인 시퀀스 길이를 8의 배수로 하고, 가능한 가장 긴 시퀀스는 \(468\times 2048=958464\)이다. 우리의 나머지 서열 길이는 이것을 연속적으로 반으로 나누고 \(2048\)의 가장 가까운 배수로 반올림함으로써 정의된다.

표 14는 도 7에서 사용된 사양을 나열한다. 변화하는 배치 크기들을 넘어서, 트레이닝 세트 내의 유효 세그먼트들의 수는 상이한 시퀀스 길이들 사이에서 변화되었고(예를 들어, 그래프 내의 상이한 포인트들에 대해 에포크당 트레이닝 단계들의 수는 일정하지 않았다), 이는 스케일링 곡선들에서의 꼬임에 기여했을 수 있다.

훈련 모델은 \(200\!K\) 훈련 단계에 대해 훈련되었으며 최대 학습률은 \(0.002\), \(20\!K\)(10\%\)) 워밍업 단계 및 체중 감량 \(0.1\)(도메인 간 일반적인 사전 훈련 레시피와 유사하다.

추가적인 Ablations: SSM 매개변수화.우리는 그림 7의 설정에서 긴 형태의 오디오 파형 사전 훈련에 대한 SSM 매개변수화를 조사한다. 6M 파형에 대해 더 큰 모델(8개 층 및 \(D=64\), SaShiMi 기본값), 더 짧은 시퀀스(\(2^{13}\)에서 \(2^{20}\) 대신 \(2^{11}=2048\)에서 \(2^{18}=262144\)), 더 낮은 LR(\(0.002\)에서 0.001\) 및 더 짧은 훈련 주기(\(200\)K 단계 대신\(100\)K)를 사용하도록 설정을 약간 수정한다.

그림 10은 S\(4\xrightarrow{}\)S6(즉, 선택 메커니즘)으로부터의 변화가 항상 유익한 것은 아님을 보여준다. 긴 형태의 오디오 파형에서, 그것은 실제로 성능을 상당히 저해하는데, 이는 오디오가 균일하게 샘플링되고 매우 매끄럽다는 관점에서 직관적일 수 있고, 따라서 연속 선형 시간 불변(LTI) 방법으로부터 이익을 얻는다. 선택 메커니즘을 제거한 후 결과 모델이 맘바 블록 내부의 S4 레이어라는 점에 유의한다. 명확하게 하기 위해 기본 맘바 아키텍처인 맘바-S6과 반대로 이 맘바-S4를 명명한다.

그러나 오른쪽에는 U-Net Mamba-S4의 외층을 유지하고 내층만 삭마한다. 성능 차이들은 극적으로 축소된다; 이것은 원시 오디오 신호에 더 가까운 층들이 LTI이어야 한다는 가설을 강화하지만, 일단 그것들이 "토큰화"되고 외부 층들에 의해 압축되면, 내부 층들은 더 이상 LTI가 될 필요가 없다. 그러나 이 설정에서는 실제 값 SSM이 복소 값 SSM을 여전히 과소 수행합니다.

\begin{table}
\begin{tabular}{c c c} \hline \hline Sequence length & Batch size & Tokens / batch \\ \hline \(468\times 2048=958464\) & 1 & 958464 \\ \(234\times 2048=479232\) & 2 & 958464 \\ \(117\times 2048=239616\) & 4 & 958464 \\ \(59\times 2048=120832\) & 8 & 966656 \\ \(30\times 2048=61440\) & 16 & 983040 \\ \(15\times 2048=30720\) & 32 & 983040 \\ \(8\times 2048=16384\) & 64 & 1048576 \\ \(4\times 2048=8192\) & 128 & 1048576 \\ \hline \hline \end{tabular}
\end{table}
표 14: 유튜브믹스 길이 스케일링 시퀀스 길이 및 배치 크기.

도 10: (**오디오 프리트레이닝(YouTubeMix) 블레이션.**) 균일하게 샘플링된 "연속" 신호 모달리티로서, 오디오 파형은 실제로 매칭 유도성 바이어스를 갖는 LTI 모델로부터 이익을 얻는다. (_왼쪽_) 동질 모델 (모든 블록은 동일한 매개 변수화) (_오른쪽_) 중앙 U-Net 블록만 제거 됩니다. 외부 블록은 Mamba-S4입니다. 보라색 선은 왼쪽 그림과 동일 합니다.

#### e.4.2 SC09 음성 생성

자기회귀 훈련은 주로 자기회귀 언어 모델링 프로토콜을 따랐다.

* 체중감량 0.1
* 전체 단계의 10%에 대한 학습률 준비
* AdamW optimizer with \(\beta=(0.9,0.95)\)
* 기울기 클립값 0.1

16의 배치 크기에서 0.002 및 200000 훈련 단계의 학습률을 사용했다.

표 4의 대형 맘바 모델은 \(D=96\)의 외부 차원과 풀링 인자 4를 갖는 스테이지당 15개의 레이어를 갖는다. 우리는 이 데이터 세트가 작고(훈련이 100에포크를 거쳤으며) 이 대형 모델의 경우 BPB 또는 NLL의 상당한 과적합이 있음을 주목한다. 그러나 생성된 샘플의 자동화된 메트릭은 교육 전반에 걸쳐 지속적으로 개선된다.

표 5의 아키텍처 삭마 모델은 모두 \(D=64\)의 외부 차원과 풀링 인자 4를 갖는 스테이지당 8개의 레이어를 갖는다. S4+MLP 블록은 대략 \(2D^{2}+4D^{2}\) 파라미터(MLP의 확장 인자 2)를 갖는다. 트랜스포머 블록은 \(4D^{2}+2D^{2}\) 파라미터(MLP의 확장계수 1)를 갖는다. Mamba 블록은 일반적인 \(\approx 6D^{2}\) 파라메터를 갖는다. 모든 모형에는 약 6M의 총 매개변수가 있습니다.

### Efficiency Benchmark

Scan Operation.A100 80GB PCIe GPU에서 측정된 Convolution과 주의력에 대해 병렬 스캔(Section 3.3)인 선택적 SSM의 핵심 연산을 비교한다. 이들은 글로벌-콘볼루션 모델들에서 콘볼루션 커널을 컴퓨팅하거나, 주의하여 QKV 투영들을 컴퓨팅하는 것과 같은 이 코어 동작 외부의 다른 동작들의 비용을 포함하지 않는다는 점에 유의한다.

기준선으로서 커널 융합이 없는 PyTorch에서 표준 병렬 스캔을 구현한다. 이를 위해서는 HBM의 파라미터 \(\overline{\mathbf{A}},\overline{\mathbf{B}},\mathbf{C}\)를 구체화할 필요가 있다.

우리의 스캔 구현은 HBM의 모든 큰 파라미터를 구체화하는 비용을 피하면서 이산화 단계와 병렬 스캔을 융합한다.

Convolution을 위해 입력과 필터에 대해 각각 FFT를 수행하고 주파수 영역에서 곱한 후 역 FFT를 수행하여 결과를 얻는 PyTorch의 표준 구현을 사용한다. 이론적 복잡도는 시퀀스 길이 \(L\)에 대한 \(O(L\log(L))\)이다.

주의를 위해, 우리는 우리가 알고 있는 가장 빠른 구현(FlashAttention-2(Dao, 2023))과 인과 마스크를 비교한다. 인과적 마스크가 있는 FlashAttention-2는 인과적 마스크가 없는 플래시Attention-2에 비해 약 1.7\times\ 정도 더 빠르며, 이는 주의 항목 중 절반만 계산되기 때문이다.

배치 크기 1을 사용하여 시퀀스 길이를 \(2^{9}=512\), \(2^{10}\approx 1K\), \(2^{11}\approx 2K\), \(2^{19}\approx 500K\)까지 증가시킨다. 모델 차원 \(D=1024\)과 상태 차원 \(N=16\)을 사용한다. 우리는 대규모 훈련에 가장 일반적으로 사용되는 데이터 유형인 BF16 입력으로 측정한다.

End-to-End Inference.Mamba 1.4B 모델과 학습되지 않은 Mamba 6.9B 모델의 추론 처리량을 1.3B 및 6.7B 크기의 표준 트랜스포머(GPT3)에 대해 측정한다. 우리는 포그페이스 트랜스포머 라이브러리에서 표준 트랜스포머 구현을 사용한다.

우리는 프롬프트 길이를 2048로 설정하고 생성 길이를 128로 설정한다. 배치 크기를 1, 2, 4, 8, 16, 32, 64, 128로 변경하고 128 토큰을 생성하는 데 걸리는 시간을 측정한다. 그런 다음 처리량(토큰/s)을 배치 크기 \(\times\) 128/시간으로 계산합니다. 우리는 측정을 3번 반복하고 평균을 취한다. 측정은 A100 80GB PCIe GPU에서 수행됩니다.

메모리 벤치마크.메모리 사용량은 대부분의 심층 시퀀스 모델과 마찬가지로 활성화 텐서의 크기에 비례하여만 확장됩니다. 125M 모델슨 1 A100 80GB GPU의 훈련 메모리 요구사항 측정을 보고합니다. 각 배치는 길이가 2048인 시퀀스로 구성되어 있다. 본 논문에서는 토치 컴파일로부터 커널을 융합하고 FlashAttention-2를 사용하여 메모리 효율이 가장 높은 Transformer를 구현하였다. 표 15는 맘바의 메모리 요구 사항이 매우 최적화된 구현으로 유사한 크기의 트랜스포머와 유사하며 향후 맘바의 메모리 발자국이 더욱 개선될 것으로 기대한다.

\begin{table}
\begin{tabular}{l l l} \hline \hline Batch size & Transformer (w/ FlashAttention-2) & Mamba \\ \hline
1 & 4.6GB & 4.8GB \\
2 & 5.2GB & 5.8GB \\
4 & 6.9GB & 7.3GB \\
8 & 11.5GB & 12.3GB \\
16 & 20.7GB & 23.1GB \\
32 & 34.5GB & 38.2GB \\ \hline \hline \end{tabular}
\end{table}
표 15: (**메모리 벤치마크.**) 맘바의 메모리 풋프린트는 가장 최적화된 트랜스포머와 비슷하다. 125M 모델에 대한 결과입니다.
