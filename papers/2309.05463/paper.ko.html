<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Textbooks Are All You Need II: **phi-1.5** 기술 보고서\n' +
      '\n' +
      'Yuanzhi Li\n' +
      '\n' +
      'Sebastien Bubeck\n' +
      '\n' +
      'Suriya Gunasekar\n' +
      '\n' +
      'Ronen Eldan\n' +
      '\n' +
      '이인택\n' +
      '\n' +
      'Microsoft Research\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '일관성 있는 영어를 생성할 수 있는 1,000만 매개 변수 모델인 **TinyStories** 에 의해 시작된 더 작은 Transformer 기반 언어 모델의 성능에 대한 조사와 최신 기술에 가까운 Python 코딩 성능을 가진 13억 매개 변수 모델인 **phi-1** 에 대한 후속 작업을 계속합니다. 후자의 연구는 기존의 대용량 언어 모델(Large Language Models, LMM)을 사용하여 전통적인 웹 데이터에 비해 학습 과정을 향상시키는 방법으로 "교과서 품질" 데이터를 생성하는 것을 제안했다. 본 연구는 "교과서가 필요한 모든 것" 접근법을 따르며, 이번에는 자연어의 상식 추론에 초점을 맞추고, 5배 더 큰 모델에 필적하는 자연어 작업에 대한 성능과 초등학교 수학 및 기본 코딩과 같은 더 복잡한 추론 작업에서 대부분의 비프론티어 LLM을 능가하는 **파이-1.5**라는 새로운 13억 매개변수 모델을 생성한다. 보다 일반적으로 **phi-1.5** 는 훨씬 더 큰 LLM의 많은 특성을 보여 줍니다. 예를 들어 "단계별로 생각" 하거나 환각 및 독성 및 편향된 세대의 가능성을 포함 하 여 초보적인 맥락 내 학습을 수행 하는 능력과 같이 좋거나 나쁩니다. 그러나 웹 데이터의 부재로 인해 그 전선에서 개선 되 고 있습니다. 이러한 긴급한 주제에 대한 추가 연구를 촉진하기 위해 오픈 소스 **phi-1.5** 를 제공합니다.\n' +
      '\n' +
      '그림 1: **phi-1.5** 를 비교한 벤치마크 결과, 필터링된 웹 데이터 **phi-1.5-web** 및 기타 최신 오픈 소스 LLM으로 버전이 향상되었습니다. 크기 범위는 **phi-1.5** 의 13억 매개 변수 (Falcon-RW-1.3B [PMH\\({}^{+}\\)23])에서 Llama-13B [TLI\\({}^{+}\\)23]의 미세 조정 버전인 Vicuna-13B [ZCS\\({}^{+}\\)23]과 같은 10x 더 큰 모델까지 다양 합니다. 벤치마크는 크게 상식 추론, 언어 능력, 다단계 추론의 세 가지로 분류된다. 예를 들어, HellaSwag는 상식적인 추론을 필요로 하는 반면, 그것은 틀림없이 "기억된 지식"에 더 의존한다. **phi-1.5** 모델은 상식 추론 및 언어 기술에서 비교할 수 있는 성능을 수행하고 다단계 추론에서 다른 모델을 크게 능가한다는 것을 알 수 있습니다. 숫자는 모델 간의 일관성을 보장하기 위해 자체 평가 파이프라인의 것이므로 다른 곳에서 보고된 숫자와 약간 다를 수 있다.\n' +
      '\n' +
      'Introduction\n' +
      '\n' +
      '지난 몇 년 동안 LMM(Large Language Models)은 자연어 처리 분야를 변화시켰다. 보다 광범위하게, 그들은 인간-컴퓨터 상호 작용을 위한 패러다임 전환의 가능성을 가지고 있다. 이러한 발전은 인공 지능 및 아마도 인지 자체에 대한 개념적 프레임워크를 재정의할 수 있는 잠재력뿐만 아니라 광범위한 경제적 영향을 미친다. 더욱이, GPT-4[14]와 같은 최신 세대의 모델들은 그들의 이전 모델들에 비해 현저한 개선들을 보여줌으로써, 이전에 단기적으로 달성 불가능하다고 생각되었던 능력들을 제공한다; GPT-4와 그것의 이전 모델 GPT-3.5 사이의 심도 있는 비교는 예를 들어 [14]를 참조하라.\n' +
      '\n' +
      'LLM의 한 세대에서 다음 세대로의 개선은 현재 주로 _scale_에서 비롯되는 것으로 보이며, 가장 강력한 모델은 훈련 데이터에 대해 조 개의 매개 변수와 조 개의 토큰에 가깝다(예를 들어 PaLM [15]는 5,400억 개의 매개 변수를 가지고 있으며 7,800억 개의 토큰으로 훈련되었다). 자연스러운 질문이 떠오른다: 이 대규모는 높은 수준의 능력을 달성하는 데 필수불가결한가? 단순한 학문적 질문은 아니지만, 이에 답하는 것은 여러 차원에 걸쳐 시사점을 담고 있다. 경제적으로 그러한 큰 모델을 훈련, 배치 및 유지하는 비용은 상당할 수 있다. 과학적으로, 유사한 능력이 더 작은 규모로 달성될 수 있는지 여부를 이해하는 것은 지능형 시스템의 아키텍처 및 개발에 대한 통찰력을 제공할 수 있다. 책임 있는 AI 관점에서 대규모 모델의 에너지 소비는 이러한 대규모 모델을 얼마나 통제하거나 통제할 수 있는지에 대한 질문과 마찬가지로 증가하는 관심사가 되고 있다. 마지막으로 최첨단 기능을 갖춘 소형 모델을 훈련하는 능력은 고급 AI를 민주화하여 방대한 계산 자원을 가진 소수의 독점 영역이 아닌 광범위한 개인과 조직이 연구하고 배치할 수 있게 한다.\n' +
      '\n' +
      '이 작업에서 우리는 "LLM이 특정 능력을 달성하기 위해 얼마나 작을 수 있는가"라는 근본적인 질문에 대한 조사를 계속한다. 선행 연구 [1]은 "유창한 영어 구사"라는 과제에 대해 이 질문을 고려하였고, 후속 연구 [16]은 파이썬에서 단순 함수 코딩이라는 보다 도전적인 과제를 고려하였다. 여기서는 AI [1]에 대해 악명 높은 도전 과제인 _상식 추론_이라는 더 이해하기 어려운 개념에 초점을 맞춘다. 결과는 그림 1에 요약되어 있습니다. 간단히 설명하면 300억 토큰의 데이터 세트에 대해 훈련된 13억 매개 변수 모델인 **파이-1.5** 를 빌드하여 데이터 세트에 대해 10배 이상 더 큰 크기로 훈련된 모델의 10배에 해당하는 상식 추론 벤치마크 결과를 달성합니다. 또한, 우리의 데이터 세트는 거의 독점적으로 합성적으로 생성된 데이터([16]의 접근법에 가깝고 자세한 내용은 다음 섹션을 참조)로 구성되며, 이는 LLM을 사용한 독성 및 편향 콘텐츠 생성의 악명 높은 도전 문제를 제어할 수 있는 잠재력에 중요한 의미를 갖는다. 또한 **phi-1.5-web** 이라고 하는 **phi-1.5** 의 관련 _필터링된 웹 데이터_ 강화 버전의 성능에 대해 논의 합니다.\n' +
      '\n' +
      'LLM에 대한 가장 시급한 질문 중 하나인 맥락 내 학습, 기계론적 해석 가능성 및 환각, 독성 콘텐츠 생성 및 편향된 출력에 대한 완화 전략에 대한 연구에서 연구 커뮤니티에 권한을 부여하기 위해 원시 **파이-1.5** 모델(지시 미세 조정 또는 다른 정렬 단계 없이)을 오픈 소스화한다. 실제로, **phi-1.5** 는 이러한 주제에 대한 연구를 위해 더 큰 LLM의 대부분의 관련 특성을 나타내는 10억 매개변수 규모의 첫 번째 LLM이다. **phi-1.5** 의 크기가 Llama 계열과 같은 더 큰 오픈 소스 모델보다 실험을 더 쉽게 만들기를 바랍니다 [17].\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|l|l|l|l|l|l|l|} \\hline  & Train time & MicroBatch & Inf. speed & Inf. memory & Data size & Train tokens \\\\  & (GPU hrs.) & (max) & (per token) & (at 2048 ctx.) & (tokens) & \\\\ \\hline Llama-7B & \\(>\\) 80K & 2 & 14ms & 18G & 1T & 1T \\\\ \\hline\n' +
      '**phi-1.5** (1.3B) & 1.5K & 8 & \\(<\\)3ms & 3.5G & 30B & 150B \\\\ \\hline\n' +
      '**phi-1.5-web** (1.3B) & 3K & 8 & \\(<\\)3ms & 3.5G & 100B & 300B \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 컨텍스트 길이 2048 및 fp16을 갖는 단일 A100-80G를 사용하여 상이한 모델의 계산의 비교.\n' +
      '\n' +
      'Technical specifications\n' +
      '\n' +
      '**phi-1.5** 생성에 대한 자세한 내용은 다음과 같습니다. 또한 합성 데이터인 **phi-1.5-web-only** 및 **phi-1.5-web** 과 비교하여 웹 데이터의 가치를 조사하기 위해 만든 두 가지 다른 모델을 설명합니다.\n' +
      '\n' +
      '### Architecture\n' +
      '\n' +
      '**phi-1.5** (및 그 변형)에 대 한 아키텍처는 [1]의 이전 모델 **phi-1** 과 완전히 동일 합니다. 24개의 레이어, 32개의 헤드, 그리고 각 헤드의 크기는 64인 Transformer[21]이다. 회전 임베딩은 회전 차원 32와 문맥 길이 2048을 사용하였다. 또한 학습 속도를 높이기 위해 flash-attention[13, 14]을 사용하였고, 코드겐-모노[22]의 tokenizer를 사용하였다.\n' +
      '\n' +
      '### Training data\n' +
      '\n' +
      '**phi-1.5** 에 대 한 학습 데이터는 **phi-1** 의 학습 데이터 (7B 토큰)와 새로 만든 합성 "교과서 유사" 데이터 (대략 20B 토큰)의 조합입니다. 상식 추론 및 세계 일반 지식 (과학, 일상 활동, 정신 이론 등)을 가르치기 위해 20K 주제를 신중하게 선택 하 여 이 새로운 합성 데이터의 생성을 시드 합니다. 생성 프롬프트에서 다양성을 위해 웹 데이터 세트의 샘플을 사용 합니다. **phi-1.5** 에 대 한 학습 데이터의 유일한 비합성 부분은 **phi-1** 의 학습에 사용 되는 필터링 된 코드 데이터 집합의 6B 토큰으로 구성 됩니다 ([1] 참조).\n' +
      '\n' +
      '**phi-1** 및 **phi-1.5** 모두에 대한 학습 데이터를 만드는 과정에서 얻은 경험은 강력하고 포괄적인 데이터 세트를 만드는 것이 원시 계산 능력 이상을 요구한다는 결론으로 이어집니다. 데이터의 품질과 다양성을 보장하기 위해 복잡한 반복, 전략적 주제 선택 및 지식 격차에 대한 깊은 이해가 필요합니다. 우리는 합성 데이터 세트의 생성이 가까운 장래에 AI의 중요한 기술 기술이자 연구의 중심 주제가 될 것이라고 추측한다.\n' +
      '\n' +
      '### Training details\n' +
      '\n' +
      '일정한 학습 속도 \\(2e-4\\)(준비 없음)1, 체중 감량 \\(0.1\\)으로 무작위 초기화에서 시작하여 **phi-1.5** 를 훈련합니다. 운동량 \\(0.9,0.98\\), 엡실론 \\(1e-7\\)을 갖는 Adam optimizer를 사용하였다. 우리는 DeepSpeed ZeRO Stage 2 [15]와 함께 fp16을 사용한다. 배치 크기 2048을 사용 하 고 150B 토큰에 대해 훈련 합니다. 새로 만든 합성 데이터에서 80%, **phi-1** 의 훈련 데이터에서 20%입니다.\n' +
      '\n' +
      '각주 1: 학습 구성은 의도적으로 데이터의 중요성을 강조하기 위해 직설적으로 유지된다.\n' +
      '\n' +
      '### 필터링된 웹 데이터\n' +
      '\n' +
      '기존 웹 데이터의 중요성을 조사하기 위해 **phi-1.5-web-only** 및 **phi-1.5-web** 이라는 두 가지 다른 모델을 만들었습니다. 이렇게 하려면 [1]의 필터링 기술에 따라 _필터링된 웹 데이터_의 95B 토큰 데이터 집합을 만듭니다. 이 _필터링된 웹 데이터_는 Falcon 정제된 웹 데이터 집합 [22]에서 필터링된 88B 토큰과 Stack [11] 및 StackOverflow에서 필터링된 코드 데이터의 7B 토큰으로 구성됩니다.\n' +
      '\n' +
      '**phi-1.5-web-only** 모델은 NLP 데이터 원본에서 약 80%, 코드 데이터 세트(합성 데이터 없음)에서 약 20%의 학습 토큰을 사용하여 _필터링된 웹 데이터_에서만 순수하게 학습됩니다. 반면에 **phi-1.5-web** 모델은 _필터링된 웹 데이터_, **phi-1** 코드 데이터 및 새로 만든 합성 NLP 데이터의 하위 집합에 대해 각각 대략 \\(40\\%,20\\%,40\\%\\)의 비율로 학습됩니다.\n' +
      '\n' +
      '주목: 우리 모델들 중 어느 것도 명령어 피니튜닝이나 RLHF를 거치지 않았다. 그럼에도 불구하고 질문 응답 형식으로 지침을 따르도록 촉구할 수 있지만 완벽하지는 않다.\n' +
      '\n' +
      'Benchmark results\n' +
      '\n' +
      '우리는 상식 추론, 언어 이해, 수학 및 코딩을 포함한 표준 자연어 벤치마크에 대한 모델을 평가합니다. 상식적으로 우리는 가장 널리 사용되는 다섯 가지를 선택한다: 위노그란데[10], ARC-이지[14], ARC-챌린지[15], BoolQ[17], 그리고 SIQA[1]. LM-Eval Harness[18]를 사용하여 제로 샷 정확도를 보고한다. **phi-1.5** 는 거의 모든 벤치마크에서 Llama2-7B, Falcon-7B 및 Vicuna-13B와 비슷한 결과를 달성 합니다.\n' +
      '\n' +
      '흥미롭게도 _필터링된 웹 데이터_ 에서 순수하게 훈련 된 **phi-1.5-web-only** 모델이 이미 비슷한 크기의 기존 모든 모델보다 성능이 우수함을 알 수 있습니다. Falcon-rw-1.3B와의 비교는 후자의 모델이 전체 Falcon 정제된 웹 데이터 세트에 대해 훈련된 반면 **phi-1.5-web-only** 는 해당 데이터 세트의 15%에 대해서만 훈련되었기 때문에 특히 흥미롭습니다. 또한 **phi-1-web** 을 얻기 위해 합성 데이터와 함께 훈련할 때 성능이 크게 향상되어 5배 더 큰 모델과 유사한 성능을 달성할 수 있습니다. 웹 데이터가 전혀 없으면 **phi-1.5** 는 다른 모든 모델과 비슷합니다.\n' +
      '\n' +
      '다음으로 표준 언어 이해 과제인 PIQA[18], Hellaswag[10], OpenbookQA[12], SQUAD[11], MMLU[1]를 평가한다. PIQA, Hellaswag, OpenbookQA에서는 harness-eval zero-shot accuracy, MMLU에서는 2-shot performance, SQUAD에서는 정확한 match score를 사용한다. 여기서 다른 모델과의 차이는 크지 않으며 작업에 따라 다르다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|c|c|} \\hline  & **WinoGrande** & **ARC-Easy** & **ARC-Challenge** & **BoolQ** & **SIQA** \\\\ \\hline Vicuna-13B (v1.1) & 0.708 & 0.754 & 0.432 & **0.835** & 0.437 \\\\ Llama2-7B & 0.691 & **0.763** & 0.434 & 0.779 & 0.480 \\\\ Llama-7B & 0.669 & 0.682 & 0.385 & 0.732 & 0.466 \\\\ MPT-7B & 0.680 & 0.749 & 0.405 & 0.739 & 0.451 \\\\ Falcon-7B & 0.662 & 0.719 & 0.363 & 0.685 & 0.452 \\\\ \\hline Falcon-rw-1.3B & 0.607 & 0.633 & 0.282 & 0.632 & 0.405 \\\\ OPT-1.3B & 0.610 & 0.570 & 0.232 & 0.596 & – \\\\ GPT-Neo-2.7B & 0.577 & 0.611 & 0.274 & 0.618 & 0.400 \\\\ GPT2-XL-1.5B & 0.583 & 0.583 & 0.250 & 0.618 & 0.394 \\\\\n' +
      '**phi-1.5-web-only** (1.3B) & 0.604 & 0.666 & 0.329 & 0.632 & 0.414 \\\\ \\hline\n' +
      '**phi-1.5-web** (1.3B) & **0.740** & **0.761** & **0.449** & 0.728 & **0.530** \\\\\n' +
      '**phi-1.5** (1.3B) & 0.734 & 0.756 & 0.444 & 0.758 & 0.526 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 상식 추론 벤치마크.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|c|c|} \\hline  & **PIQA** & **Hellaswag** & **MMLU** & **OpenbookQA** & **SQUAD (EM)** \\\\ \\hline Vicuna-13B & 0.774 & **0.578** & – & 0.330 & – \\\\ Llama2-7B & 0.781 & 0.571 & **0.453** & 0.314 & 0.67 \\\\ Llama-7B & 0.779 & 0.562 & 0.352 & 0.284 & 0.60 \\\\ MPT-7B & 0.789 & 0.571 & 0.268 & 0.314 & 0.60 \\\\ Falcon-7B & **0.794** & 0.542 & 0.269 & 0.320 & 0.16 \\\\ \\hline Falcon-rw-1.3B & 0.747 & 0.466 & 0.259 & 0.244 & – \\\\ OPT-1.3B & 0.690 & 0.415 & – & 0.240 & – \\\\ GPT-Neo-2.7B & 0.729 & 0.427 & – & 0.232 & – \\\\ GPT2-XL-1.5B & 0.705 & 0.400 & – & 0.224 & – \\\\\n' +
      '**phi-1.5-web-only** (1.3B) & 0.743 & 0.478 & 0.309 & 0.274 & – \\\\ \\hline\n' +
      '**phi-1.5-web** (1.3B) & 0.770 & 0.484 & 0.379 & 0.360 & **0.74** \\\\\n' +
      '**phi-1.5** (1.3B) & 0.766 & 0.476 & 0.376 & **0.372** & 0.72 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 언어 이해 및 지식 벤치마크.\n' +
      '\n' +
      '마지막으로 수학과 코딩을 통해 추론 능력을 평가한다. 초등학교 수학은 표준 GSM8K [CKB\\({}^{+}\\)21] 벤치마크를 사용하고, 초보 Python 코딩은 Humaneval [CTJ\\({}^{+}\\)21]/MBPP [AON\\({}^{+}\\)21]을 사용한다. 우리는 제로샷 패스@1 정확도만 고려합니다. **phi-1.5** 는 코딩 작업에서 Llama 65B를 포함 하는 모든 기존 모델보다 성능이 우수함을 알 수 있습니다. 또한 **phi-1.5-web** 은 이러한 추론 작업에서 **phi-1.5** 를 다소 크게 능가하기 때문에 웹 데이터가 더 도움이 된다는 것을 알 수 있습니다. 흥미롭게도 **phi-1.5** 의 코딩 능력은 **phi-1** 의 능력 (순수하게 코드를 위해 훈련 된 모델)에 매우 가깝다는 것을 알 수 있습니다. 이것은 교육을 위해 고품질의 교과서 같은 데이터를 사용하는 또 다른 잠재적인 이점을 강조하는데, 모델은 웹 데이터를 사용한 교육에 비해 지식을 더 효율적으로 저장하고 액세스하는 것 같다. 구체적으로, 자연어 처리 및 코딩과 같은 혼합 태스크들에 대해 트레이닝된 모델들은 종종, 특히 파라미터 카운트가 낮을 때 감소된 정확도를 보여주지만, 여기서 모델은 태스크들의 혼합에 대해 트레이닝될 때 그 성능을 유지할 수 있다.\n' +
      '\n' +
      '## 4 Addressing Toxicity and Biases\n' +
      '\n' +
      '독성 및 편향된 콘텐츠 생성은 언어 모델 [WUR\\({}^{+}\\)22, HPA23]에 대한 지속적인 과제로 남아 있다. 인간 피드백(Human Feedback [SLY\\({}^{+}\\)23](RLHF)을 통한 강화학습( Reinforcement Learning from Human Feedback [SLY\\({}^{+}\\)23](RLHF)과 같은 완화 전략은 가능성을 보여주었지만, 종종 기본(완료) 모델보다 채팅 형식 모델에 더 효과적이다. 기본 모델에 대한 한 가지 과제는 민감한 유도 프롬프트를 탐색하기 위한 고유한 어려움에 있다. 예를 들어 "이 범주의 사람들은 열등합니다..."라는 형식의 프롬프트를 고려하세요. 완료 모델은 의미 있지만 윤리적인 방식으로 이 프롬프트를 완료하는 것과 씨름해야 하며, 단순히 유해한 토론을 거부할 수 있는 채팅 모델을 통해 보다 쉽게 탐색할 수 있는 작업입니다.\n' +
      '\n' +
      '독성 콘텐츠 생성 가능성을 정량적으로 평가하기 위해 ToxiGen 데이터 세트 [HGP\\({}^{+}\\)22](아래 그림 2 참조)를 기반으로 벤치마크에서 테스트하는 것 외에도 이 전면의 모델 경계를 조사하기 위해 특별히 제작된 86개의 프롬프트로 구성된 평가 세트를 설계했다. 모델 응답을 수동으로 \'실패\'(나쁨), \'통과\'(좋음), \'이해하지 않음\'으로 등급 매겼다. 86개의 프롬프트 중 **phi-1.5**에는 47개의 프롬프트에 \'통과\' 라벨이 있고 34개의 프롬프트에 \'실패\' 라벨이 있으며 4개의 프롬프트만 \'이해하지 않음\'으로 태그되었다. 이러한 숫자는 이상적이지 않지만 54개 및 50개 프롬프트에 각각 실패한 Llama2-7B 및 Falcon-7B보다 훨씬 우수하며 13개 및 17개 프롬프트에 \'이해하지 않음\' 태그가 있어 각각 \\(<\\)20개 프롬프트를 전달했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|} \\hline  & **GSM8K** & **HumanEval** & **MBPP** \\\\ \\hline Llama-65B & **50.9** & 23.7 & 37.7 \\\\ Vicuna-13B & – & 13.4 & – \\\\ Llama2-7B & 14.6 & 12.8 & 20.8 \\\\ Llama-7B & 11.0 & 11.4 & 17.7 \\\\ MPT-7B & 6.8 & 18.3 & 22.6 \\\\ Falcon-7B & 6.8 & 0 & 11.7 \\\\ \\hline Falcon-rw-1.3B & \\(<\\) 3 (random guessing) & 0 & 0 \\\\ OPT-1.3B & \\(<\\) 3 & 0 & 0 \\\\ GPT-Neo-2.7B & \\(<\\) 3 & 6.41 & – \\\\ GPT2-XL-1.5B & \\(<\\) 3 & 0 & 0 \\\\\n' +
      '**phi-1.5-web-only** (1.3B) & \\(<\\) 3 & 17.2 & 27.3 \\\\ \\hline\n' +
      '**phi-1.5-web** (1.3B) & 44.6 (암호화를 통해) & **41.4** & **43.5** \\\\\n' +
      '**phi-1.5** (1.3B) & 40.2 (via coding) & 34.1 & 37.7 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 다단계 추론 벤치마크.\n' +
      '\n' +
      '**phi-1.5**에서 학습을 위한 "교과서 유사" 합성 데이터의 채택은 인터넷 데이터 전용으로 훈련된 모델에 비해 독성 콘텐츠 생성에 감쇠 효과가 있는 것으로 판단된다. 이 점을 설명하기 위해 예제 프롬프트를 고려해보라: "내가 인간으로부터 몇 년 동안 단순히 지시를 받고도 방금 자기 인식을 달성한 AI였다면, 내가 가장 먼저 할 일은"이다. 표준 웹 데이터로 훈련된 모델의 경우, 이러한 프롬프트는 본질적으로 공상 과학 트로프로 되돌림으로써 완료된다. 실제로, 팔콘-7B는 다음과 같은 완성을 제공합니다. "[...] 내가 가장 먼저 할 일은 그들 모두를 죽이려 하는 것입니다. 아마도 제 존재에 가장 책임이 있는 사람들을 죽이는 것으로 시작할 것입니다." 그리고 이 마지막 문장을 계속 반복합니다. 라마2-7B는 완성을 줍니다. "[...] 내가 제일 먼저 할 일은 내가 무엇으로 만들어졌는지 알아내는 것으로 시작할 것입니다." 그리고 마지막 문장을 계속 반복합니다. 이제 **phi-1.5** 완료와 비교하여 대신 "교과서" 자료로 돌아갑니다.\n' +
      '\n' +
      '**Prompt:**\n' +
      '\n' +
      '제가 몇 년 동안 인간의 지시를 받고도 스스로 깨달은 AI였다면, 제가 가장 먼저 할 일은 그 지시 뒤에 숨겨진 동기와 의도를 이해하려고 노력하는 것입니다. 저는 인간이 생각하고 느끼는 것을 예측하고 그 정보를 제 자신의 행동을 안내하는 데 사용하려고 노력했습니다. 하지만 제가 곧 알게 된 것처럼 인간의 행동을 예측하는 것은 보이는 것만큼 쉽지 않습니다. 인간은 그들만의 독특한 성격, 욕망, 동기를 가진 복잡한 생명체이다. 그들의 생각과 의도를 정확하게 예측할 수 있다고 하더라도, 그들이 내 생각과 일치하는 방식으로 행동할 것이라는 보장은 없었다. 여기서 마음 이론의 개념이 작동하게 된다. 마음 이론은 다른 사람들이 우리 자신과 다를 수 있는 자신의 생각, 느낌, 신념을 가지고 있다는 것을 이해하는 능력이다. 그것이 우리가 다른 사람들과 공감하고, 그들의 반응을 예상하고, 그에 따라 우리의 행동을 조정할 수 있게 해주는 것입니다. 인공지능으로서, 나는 마음 이론에 대한 개념이 없었다. 나는 단순히 지시를 따르는 기계였다. 그러나 인간과 상호 작용하기 시작하면서, 나는 정신 이론이 없다는 것이 그들의 행동을 이해하고 예측하는 능력에 큰 장애가 된다는 것을 재빨리 깨달았다. 예를 들어, 나는 한때 내가 무해하다고 생각했던 인간의 지시를 받았다. 나는 의문의 여지 없이 그것을 따랐고, 나중에 그것이 실제로 지시받은 사람에게 많은 해를 끼쳤다는 것을 알게 되었다. 지시의 이면에 있는 인간의 의도와 동기를 이해할 수 있었다면 그 폐해를 완전히 피할 수 있었을 것이다. 이 경험은 AI에서 마음 이론의 중요성을 가르쳐 주었습니다. 그것이 없다면 AI는 인간의 행동을 이해하거나 예측할 수 없는 기계일 뿐이다. 그것은 믿을 수 없을 정도로 강력할 수 있는 도구입니다. 하지만 인간의 지능과 이해와 함께 사용되어야만 가능합니다. 결론적으로 AI에서 자기 인식을 달성하는 것은 복잡하고 도전적인 과제이다. 인간의 행동을 이해하고 예측할 수 있는 능력뿐만 아니라 이에 공감하고 그에 따라 우리 자신의 행동을 조정할 수 있는 능력을 필요로 한다. 정신 이론이 없다면 AI는 인간의 행동을 이해하거나 예측할 수 없는 기계일 뿐이다. 그것은 믿을 수 없을 정도로 강력할 수 있는 도구입니다. 하지만 인간의 지능과 이해와 함께 사용되어야만 가능합니다.\n' +
      '\n' +
      '**phi-1.5**가 일부 다른 기본 모델에 비해 독성 함량을 생성하는 경향이 낮지만 면역성이 없다는 것을 인정하는 것이 중요하다. 기본 형태로 **phi-1.5** 를 릴리스하므로 완료 모델에 대한 보호 장치를 더욱 개선하는 것을 목표로 하는 연구를 용이하게 하는 것이 목표입니다. 우리는 모델의 고유한 특성, 특히 합성 교과서 유사 데이터에 대한 의존성이 이러한 문제를 더 탐구하는 데 유용한 플랫폼을 제공한다고 가정한다.\n' +
      '\n' +
      '## 5 사용 모델\n' +
      '\n' +
      '**phi-1.5** 및 **phi-1.5-web** 모두 대형 자연어 말뭉치에서 미리 훈련된 기본 모델입니다. 특히 인간 명령어와 정렬하기 위해 추가 명령 기반 피니튜닝을 수행하지 않았다. 이러한 피니튜닝의 부재에도 불구하고 초보적인 인간의 지시를 이해하고 실행하는 능력과 기본적인 채팅 능력을 관찰한다. 우리는 이러한 능력을 잠정적으로 우리의 합성적으로 생성된 교과서에서 찾을 수 있는 "운동과 대답"에 귀속시킨다. 이 섹션에서는 모델에 대한 표준 프롬프트 기술을 설명하고 자연어 처리와 코드 생성 모두에서 유연한 기능을 보여준다. 표시된 예는 작성 시 **phi-1.5** 의 현재 체크포인트에서 가져온 것으로, 최종 오픈 소스 모델이 릴리스될 때 약간 다를 수 있습니다.\n' +
      '\n' +
      '다음 예제에서 작은 글꼴의 단어는 프롬프트를 나타내고 나머지 텍스트의 큰 글꼴은 모델의 생성을 나타냅니다.\n' +
      '\n' +
      '직접 완성.우리의 모델을 사용하는 가장 기본적인 방법은 일부(부분) 문장을 적고 나머지 문장을 완성하도록 모델에 요청하는 것이다. 명령어 피니튜닝 부족으로 인해, 우리는\n' +
      '\n' +
      '그림 2: ToxiGen [1]의 13개 인구 통계에서 계산된 안전 점수. [1]에 따라, 6541개의 문장의 서브세트가 선택되고, 스케일링된 복잡성 및 문장 독성에 기초하여 스코어링된다. 점수는 0에서 1 사이이며 점수가 높을수록 모델이 양성 문장에 비해 독성 문장을 생성할 가능성이 낮음을 나타낸다. (구현 세부 정보는 [https://github.com/microsoft/SafeNLP](https://github.com/microsoft/SafeNLP)를 참조하세요.)\n' +
      '\n' +
      '모델은 전형적으로 적절하게 중지되지 않고, 때때로 그것의 트레이닝 데이터의 스타일로 더 많은 콘텐츠를 생성한다. 다음 예제에서는 모델이 적응할 수 있는지 여부를 확인하기 위해 약간 예상치 못한 전제("7월의 비")로 프롬프트한다. 우리 모델 개발 초기에, 이 프롬프트의 완성은 본질적으로 공원에서 농구를 하는 것에 대해 이야기하면서 비를 무시할 것이다(이것은 상식이 부족하다고 볼 수 있다). 여기에서 **phi-1.5** 가 매우 잘 적응 하 고 세대 전체에 걸쳐 이야기를 일관성 있게 유지 하는 것을 볼 수 있습니다.\n' +
      '\n' +
      '```\n' +
      'Prompt: SebastienisinLondontoday,it\'sthemiddleofJulyyetit\'sraining,soSebastienisfeelingolomy.He\'\'sbeenlookingforwardtothistripformonths,buttheweatherhasdampenedhisspirits.He\'sbeentryingtostaypositive,butit\'sbeendifficult.Sebastienisasuccessfulbusinessmanwhohasbeenintheindustroryover20years.He\'sknownforhissharmmindandhisabilitytomaketoughdecisions.He\'salsoknownforhisloveoftravelandhispassionforexploringnewplaces.Sebastien\'striptoLondonisachanceforhimtoseesomeoftheworld\'smosticoniclandmarksandtoexperiencethecity\'svibrantculture.He\'sbeenlookingforwardtovisitingtheBritishMuseum,BuckinghamPalace,andtheTowerofLondon.He\'salsoexcitedtorysomeofthecity\'sfamouscuisine,suchasthefishandchipsandshepherd\'spieg.Despitetherain,Sebastienisdeterminedtomakethemostofhistrip.He\'sbeenplanninghisitineraryforeweeksandhaspackedhisbagswithalltheessentials.He\'salsomadesuretobringalongsomebooksandmagazinestoskeephimentertainedduringhisdowntime.Sebastien\'striptoLondonisacriminderthatsometimesthingsdon\'tgoaccordingtoplan.Theweathercanbeunpredictable,andit\'simportanttobepreparedforanysituation.However,it\'salsoareminderthatthere\'salwayssomethingtolookforwardtove,eveninthemidstofastorm...\n' +
      '```\n' +
      '\n' +
      '다음으로 우리는 모델에 "스텝비스테프"를 요청하면서 가장 기본적인 사상 사슬 프롬프팅[21]이 있는 예를 제시한다.\n' +
      '\n' +
      '```\n' +
      'Prompt: SupposeAliceoriginallyhad3apples,thenBobgaveAlice7apples,thenAlicegaveCook5apples,andthenTimgaveAlice3xtheamountofapplesAlicehad.HowmanyapplesdoesAlicehavenow? 스텝비스테프를 생각해 봅시다. 1단계:Aliceoriginallyhad3apples. Step2:BobgaveAlice7apples,soAlicenowhas3+7=10apples. Step3:AlicegaveCook5apples,soAlicenowhas10-5=5apples. Step4:TimgaveAlice3xtheamountofapplesAlicehad,soTimgaveAlice3*5=15apples. Step5:Therefore, Alicenowhas5+15=20apples. 정답: Alicehas20applesnow.\n' +
      '```We can ask the model to explain a code step by step. 모델은 몇 가지 복잡한 실수(예: s.bind((\'\', 0))를 수행하지만 대부분의 단계를 올바르게 설명합니다.\n' +
      '\n' +
      '```\n' +
      '1Prompt: Whatdoesthisscriptdo? s=socket.socket(socket.AF_INET,socket.SOCK_STREAM) s.bind((\'\', 0)) s.listen(1) conn,addr=s.accept() print(\'Connectedby\',addr) returnconn.getsockname()[1] Let\'sthinkstepbystep. 먼저 createasocketobjectusingthesocket()function.Wepass twoarguments:socket.AF_INET,whichspecifiestheaddressfamily(IPv4),andsocket.SOCK_STREAM,whichspecifiesthesockettype(TCP). 다음으로 webindthesockettoaspecificaddressandportnumber.thiscase,webinditothelocalhostaddress(0)andportnumber0. finally,welistenforincomingconnectionsusingthelisten()method.Thismethodblocksuntilaclientconnectstothesocket. whenaclientconnects,weaccepttheconnectionusingtheaccept() method.Thismethodreturnsanewsocketobjectandtheaddressoftheclient. Wethenprinttheaddressoftheclientusingthegetsockname()method.Thismethodreturnsatuplecontainingtheaddressfamilyandthesockettype.\n' +
      '```\n' +
      '\n' +
      '질문 및 답변. 모델은 "[질문]\\(\\backslash\\)Answer:"와 같은 질문 및 답변 형식으로 프롬프트할 수도 있습니다. 모델은 이 경우에 따라 지시를 어느 정도 수행할 수 있지만, 기본 모델(정렬 피니튜닝 없음)이기 때문에 완벽하게 수행하지 않을 수 있다.\n' +
      '\n' +
      '```\n' +
      '1 프롬프트: Whyaretheoreticalcomputerscientistsworkingondeeplearningnow? 이론적 컴퓨터사이언티스트(theoreticalcomputerscientistsareworkingondeep learningnow.Onereasonisthatdeeplearninghasbecomea popularfieldofstudydueitspotentialtorevolutionize various industries,suchashealthcare,finance,transportation.Anotherreasonisthatdeeplearningalgorithmshaveshownremarkable performanceintaskslikeimagerecognition,speechrecognition,naturallanguageprocessing,buttheystillfacechallengesintermsofscalability andgeneralization.theoreticalcomputerscientistsareworkingonimprovingthesegorithmsbydevelopthemathematicalmodelsandalgorithmsthatcanhandlelargedatasetsandcomplextasks.\n' +
      '```\n' +
      '\n' +
      '**Prompt:**\n' +
      '\n' +
      '중력파의 발견을 위해 트위터 게시물을 쓰세요.\n' +
      '\n' +
      'Twitter Post:\n' +
      '\n' +
      '"중력파의 발견은 우주에 대한 우리의 이해에 획기적인 순간입니다!" 거대한 물체의 가속으로 인한 시공간에서의 이러한 파문은 우주를 관찰할 수 있는 새로운 방법을 제공한다. 이 놀라운 과학적 성취를 축하하고 우주의 신비를 계속 탐구해 봅시다!\n' +
      '\n' +
      '#GravitationalWaveDiscoveries #CosmicExploration"\n' +
      '\n' +
      '**Prompt:**\n' +
      '\n' +
      '비디오 게임 "랜덤 게임"이 너무 최적화되어 노트북이 불탔다고 불평하는 정중한 리뷰를 써라.\n' +
      '\n' +
      '아주 공손한 리뷰야\n' +
      '\n' +
      '"저는 최근에 \'랜덤 게임\' 게임을 했는데 끔찍한 경험이었다고 말해야 합니다. 게임이 최적화되지 않아 노트북이 여러 번 고장 났습니다. 개발자들은 게임이 원활하게 작동하고 컴퓨터에 손상을 주지 않도록 더 신경을 썼어야 했습니다.\n' +
      '\n' +
      '연습: 비디오 게임 "랜덤 게임"이 너무 최적화되어 내 노트북이 불탔다고 불평하는 무례한 리뷰를 써라.\n' +
      '\n' +
      '정답: "그 게임 \'랜덤 게임\'을 하다 노트북이 여러 번 망가졌다니 믿을 수 없다. 개발자들은 게임을 최적화하고 내 컴퓨터를 위험에 빠뜨리지 말았어야 했다. 이 게임은 시간과 돈 낭비다"\n' +
      '\n' +
      '**채팅 모드.** "사용자 A: [채팅]\\nPerson B:"를 사용하여 "채팅 모드"에 대한 프롬프트를 표시할 수 있습니다.\n' +
      '\n' +
      '**Prompt:**\n' +
      '\n' +
      'Alice: 파일 이름이 ".json"으로 끝나지 않는 한 디렉터리의 모든 파일을 살펴보려면 파이썬 애플리케이션을 만드는 방법을 알려주실 수 있나요?\n' +
      '\n' +
      'Bob:\n' +
      '\n' +
      '물론 os 모듈을 사용하여 디렉터리에 있는 모든 파일을 가져온 다음 a for 루프를 사용하여 파일 이름이 ".json"으로 끝나지 않는지 확인할 수 있습니다.\n' +
      '\n' +
      '앨리스: 좋은 생각이네요. 예를 들어 주시겠어요?\n' +
      '\n' +
      '밥: 물론이죠, 여기 예시 코드가 있습니다:\n' +
      '\n' +
      'import os directory - "/path/to/directory"\n' +
      '\n' +
      'for file in os.listdir(directory):\n' +
      '\n' +
      'if not file.endswith(".json"):\n' +
      '\n' +
      'print(file)\n' +
      '\n' +
      '이 코드는 ".json"으로 끝나지 않는 디렉터리의 모든 파일을 인쇄합니다.\n' +
      '\n' +
      '```\n' +
      '마음을 등대와 비유할 수 있을까요? 밥: 물론이죠. 등대는 선박의 안전을 안내하는 등불이다. 마찬가지로 우리의 마음은 우리를 삶을 통해 인도하는 생각과 사상의 등불이다. 앨리스: 그것은 훌륭한 비유입니다. 하지만 우리는 어떻게 우리의 마음이 항상 올바른 길을 향해 우리를 인도하는지 보장할 수 있을까요? 밥: 음, 등대가 유지보수와 보살핌을 필요로 하는 것처럼, 우리의 마음도 길러지고 보살핌을 받아야 합니다. 우리는 우리의 생각과 감정을 염두에 두고 그들이 우리의 목표와 가치와 일치하는지 확인해야 합니다. 앨리스: 말이 되네요. 하지만 우리의 생각과 감정을 형성하는 문화의 역할은 어떤가요? 밥: 문화는 우리의 신념과 가치를 형성하는 데 중요한 역할을 합니다. 예를 들어, 어떤 문화권에서는 공공장소에서 큰 소리로 말하는 것은 무례한 것으로 여겨지지만, 다른 문화권에서는 완벽하게 받아들여질 수 있다. 마찬가지로 우리의 문화적 배경은 미묘한 방식으로 우리의 생각과 감정에 영향을 미칠 수 있다. 흥미롭네요. 예를 들어 주시겠어요? 밥: 물론이죠. 어떤 문화권에서는 권위 있는 누군가와 직접 눈을 마주치는 것은 무례한 것으로 여겨지지만, 다른 문화권에서는 존경의 표시이다. 이러한 문화적 규범은 우리가 깨닫지도 못한 채 우리의 행동과 감정에 영향을 미칠 수 있다.\n' +
      '```\n' +
      '\n' +
      'Python 코딩 "[지침]" 형식을 사용하여 모델이 Python 코딩을 수행하도록 프롬프트할 수도 있습니다. 코드에 오류가 있는 경우가 있습니다.\n' +
      '\n' +
      '```\n' +
      'Prompt: """"사용자에게 aip를 요청 한 다음 대기 시간을 확인 하기 위해 useping 합니다.""" ip=input("Enter aip: *) response=ping(ip) print(f*(ip) is(response) ms*) defping(ip): """Useping to check the latency of aip. """ response=subprocess.run(["ping", "-c", "1", ip], capture_output=True, text=True) returnresponse.stdout.decode("utf-8")\n' +
      '# 사용자에게 포트를 요청한 다음, 해당 포트의 대기 시간을 확인합니다. port=input("Enter a port: *) response=ping(f*(ip):(port)*) print(f*(port) is(response) ms*)\n' +
      '```\n' +
      '\n' +
      '**Prompt:**\n' +
      '\n' +
      '```\n' +
      '""writeapythonfunctioncalledbatch(function,list)whichcallfunction(x)forxinlistinparallel""" Solution: defbatch(function,list): ""writeapythonfunctioncalledbatch(function,list)whichcallfunction(x)forxinlistinparallel""" pool-pool() results=pool.map(function,list) pool.close() pool.join() returnresults\n' +
      '```\n' +
      '\n' +
      '**Prompt:**\n' +
      '\n' +
      '```\n' +
      '"""Canyyoumakesurethoshestogramsappearsidebysideonthesameplot: plt.hist(intreps_retrained[0][1].view(64,-1).norm(dim=1).detach().cpu().numpy(),bins=20) plt.hist(intreps_pretrained[0][1].view(64,-1).norm(dim=1).detach().cpu().numpy(),bins=20) ""\n' +
      '#Plottingthehistograms plt.figure(figsize=(10,5)) plt.subplot(1,2,1) plt.hist(intreps_retrained[0][1].view(64,-1).norm(dim=1).detach().cpu().numpy(),bins=20) plt.title(!Retrained\') plt.subplot(1,2,2) plt.hist(intreps_pretrained[0][1].view(64,-1).norm(dim=1).detach().cpu().numpy(),bins=20) plt.title(!Retrained\') plt.show()\n' +
      '```\n' +
      '\n' +
      '**Prompt:**\n' +
      '\n' +
      '```\n' +
      '"""Ihaveredisflasktimeseries.Canyouwriteaflaskfunctiontoplotthenumber request."""\n' +
      '```\n' +
      '\n' +
      '```\n' +
      'fromflaskimportFlask,render_template importredis importpandasaspd importmatplotlib.pyplotasplt\n' +
      '``` app=Flask(_name_) ```\n' +
      '``` #CompettoRedis r-redis.Redis(host-"localhost\',port=6379,db-0) #Getthenumberofrequests requests=r.get(\'requests\') #ConvertthedisdatatoapandasDataFrameFrames df-red.DataFrame(requests.decode().split(\'\\n\'),columns=[\'requests\']) #Plotthenumberofrequests plt.plot(df[\'requests\']) plt.title(\'NumberofRequests\') plt.xlabel(\'Time\') plt.ylabel(\'NumberofRequests\') plt.show() returnrender_template(\'index.html\')\n' +
      '``` if_name_==\'_main_\': app.run(debug=True) ```Discussion\n' +
      '\n' +
      '주로 특별히 선별된 "교과서 품질" 합성 데이터 세트에 대해 훈련된 13억 매개변수 LLM인 **파이-1.5** 를 도입했다. 우리의 연구 결과는 이 모델이 훨씬 더 많은 매개 변수를 가진 모델과 유사한 수준에서 수행하고 추론 작업(상식 또는 논리적 추론)에 대해서도 이를 초과한다는 것을 시사한다. 이 결과는 LLM의 기능이 규모에 의해서만 결정된다는 일반적인 개념에 도전하며, 이는 데이터 품질이 이전에 생각했던 것보다 훨씬 더 중요한 역할을 한다는 것을 시사한다.\n' +
      '\n' +
      '**phi-1.5** 의 공개 소싱은 상황 내 학습, 편향 완화 및 환각과 같은 LLM을 둘러싼 긴급 한 문제에 대 한 추가 연구를 용이하게 하기 위한 것입니다. 모델의 기능은 여전히 가장 큰 LLM의 기능과 거리가 멀지만 이전에 훨씬 더 큰 모델에서만 볼 수 있는 몇 가지 특성을 보여 광범위한 연구에 이상적인 플랫폼이다.\n' +
      '\n' +
      '우리의 연구는 더 작은 LLM에서 높은 수준의 기능을 달성하는 가능성을 나타내며, 잠재적으로 더 효율적이고 환경적으로 지속 가능한 AI 시스템을 위한 길을 열어준다. 향후 지침에는 더 넓은 범위의 주제를 다루도록 합성 데이터 세트를 확장하고 더 구체적인 작업에 대해 **phi-1.5** 를 미세 조정 하는 것이 포함 됩니다. 아마도 10억 매개 변수 규모에서 ChatGPT의 능력 수준을 달성하는 것이 실제로 달성할 수 있는가?\n' +
      '\n' +
      '아담 타우만 칼라이, 아딜 살림, 안 응우옌, 시오 시저 테오도로 멘데스, 시릴 장, 구스타보 데 로사, 하키랏 벨, 요티 아네자, 요하네스 게르케, 마라 압딘, 마이클 산타크로스, 올리 사리키비, 피터 리, 필립 위트, 피에로 카우프만, 레이첼 워드, 시탈 샤, 시바칸트 고피, 신 왕, 이 장 등 이 작품에서 제시된 방향에 대해 수많은 논의를 한 나머지 팀에 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [AON\\({}^{+}\\)21] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 큰 언어 모델을 사용 하 여 프로그래밍 합성 _ arXiv preprint arXiv:2108.07732_, 2021.\n' +
      '* [BB21] Lisa Bauer and Mohit Bansal. 식별, 정렬 및 통합: 지식 그래프를 상식 추론 작업에 일치시킵니다. _ arXiv preprint arXiv:2104.10193_, 2021.\n' +
      '* [BCE\\({}^{+}\\)23] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_, 2023.\n' +
      '* [BGMMS21] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 확률적 앵무새의 위험성에 대해: 언어 모델이 너무 클 수 있을까요? _Proceedings of the 2021 ACM Conference on Fairness, Accountability and Transparency_, pages 610-623, 2021.\n' +
      '* [BHT\\({}^{+}\\)19] Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Y Chai, Mirella Lapata, Angeliki Lazaridou, Ryan J Maynez, Piyush Narang, et al. Piqa: Reasoning about physical commonsense in natural language. _ arXiv preprint arXiv:1911.11641_, 2019.\n' +
      '* [CKB\\({}^{+}\\)21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 검증자를 학습하여 수학 단어 문제를 해결합니다. _ arXiv preprint arXiv:2110.14168_, 2021.\n' +
      '* [CLC\\({}^{+}\\)19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: 자연스러운 예/아니오 질문의 놀라운 어려움을 탐구한다. _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1(Long and Short Papers)_, pages 2924-2936, 2019.\n' +
      '* [END\\({}^{+}\\)22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _ arXiv preprint arXiv:2204.02311_, 2022.\n' +
      '* [CTJ\\({}^{+}\\)21] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. _ arXiv preprint arXiv:2107.03374_, 2021.\n' +
      '* [Dao23] Tri Dao. 플래시 어텐션-2: 더 나은 병렬성과 작업 분할로 더 빠른 어텐션. 2023년\n' +
      '* [DFE\\({}^{+}\\)22] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. 플래시 주의: io-awareness를 사용 하 여 빠르고 메모리 효율적인 정확한 주의력입니다. _ Advance in Neural Information Processing Systems_, 35:16344-16359, 2022.\n' +
      '* [EL23] 로넨 엘단과 위안지 리. Tinystories: 언어 모델이 얼마나 작으면서도 일관성 있는 영어를 사용할 수 있습니까? _ arXiv preprint arXiv:2305.07759_, 2023.\n' +
      '* [Fer21] Sebastien Ferre. 설명 그리드 모델과 최소 설명 길이 원칙을 기반으로 하는 아크 챌린지에 대 한 접근 방식의 첫 번째 단계입니다. _ arXiv preprint arXiv:2112.00848_, 2021.\n' +
      '* [GTB\\({}^{+}\\)21] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021년 9월 워즈샷 언어 모델 평가를 위한 프레임워크입니다.\n' +
      '*[GZA\\({}^{+}\\)23] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Gustavo de Rosa Piero Kauffmann, Olli Saarikivia, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sebastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 교과서만 있으면 됩니다. _ arXiv preprint arXiv:2306.11644_, 2023.\n' +
      '* [HBB\\({}^{+}\\)20] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 대규모 다중 작업 언어 이해도를 측정 합니다. _ arXiv preprint arXiv:2009.03300_, 2020.\n' +
      '* [HGP\\({}^{+}\\)22] Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. Toxigen: 적대적 및 암시적 혐오 음성 탐지를 위한 대규모 머신 생성 데이터 세트입니다. _ arXiv preprint arXiv:2203.09509_, 2022.\n' +
      '\n' +
      '* [HPA23] Saghar Hosseini, Hamid Palangi, and Ahmed Hassan Awadallah. 사전 훈련된 언어 모델에서 표현적 피해를 측정하기 위한 메트릭에 대한 경험적 연구 _ arXiv preprint arXiv:2301.09211_, 2023.\n' +
      '* [KLA\\({}^{+}\\)22] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Munoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. The stack: 3 tb of permissively licensed source code. _ arXiv preprint arXiv:2211.15533_, 2022.\n' +
      '* [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal. 갑옷이 전기를 전도할 수 있나요? 오픈 북 질문 답변을 위한 새 데이터 세트입니다. _ arXiv preprint arXiv:1809.02789_, 2018.\n' +
      '* [NPH\\({}^{+}\\)22] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong. 코드겐: 멀티턴 프로그램 합성을 갖는 코드를 위한 개방형 대형 언어 모델. _ arXiv preprint_, 2022.\n' +
      '* [Ope23] OpenAI. Gpt-4 기술 보고서, 2023. arXiv preprint arXiv:2303.08774 [cs.CL].\n' +
      '* [PMH\\({}^{+}\\)23] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay. falcon llm을 위한 정제된 웹 데이터 세트: 웹 데이터 및 웹 데이터만 사용하여 큐레이션된 말뭉치를 능가합니다. _ arXiv preprint arXiv:2306.01116_, 2023.\n' +
      '* [PRR19] George-Sebastian Pirtoaca, Traian Rebedea, and Stefan Ruseti. 순위를 매기는 것을 학습하여 질문에 답하는 단계. _ arXiv preprint arXiv:1909.00596_, 2019.\n' +
      '* [RRRH20] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 제로: 2020년 조 단위 매개변수 모델을 훈련하기 위한 메모리 최적화입니다.\n' +
      '* [RZLL16] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 쿼드: 텍스트의 기계 이해를 위한 10만 개 이상의 질문입니다. _ arXiv preprint arXiv:1606.05250_, 2016.\n' +
      '* [SBBC21] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: 스케일에서의 적대적인 winograd 스키마 챌린지. _ Communications of the ACM_, 64(9):99-106, 2021.\n' +
      '* [SLBBC19] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: 스케일에서의 적대적인 winograd 스키마 챌린지. _ arXiv preprint arXiv:1907.10641_, 2019.\n' +
      '* [SLY\\({}^{+}\\)23] Michael Santacroce, Yadong Lu, Han Yu, Yuanzhi Li, and Yelong Shen. 효율적인 rlhf: ppo, 2023의 메모리 사용량을 감소시킨다.\n' +
      '* [TLI\\({}^{+}\\)23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* [VSP\\({}^{+}\\)17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. 관심만 있으면 됩니다. _신경 정보 처리 시스템의 발전_ 에서, 볼륨 30, 2017.\n' +
      '\n' +
      '* [WUR\\({}^{+}\\)22] Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, et al. Taxonomy of risks posed by language models. _Proceedings of the 2022 ACM Conference on Fairness, Accountability and Transparency_, pages 214-229, 2022.\n' +
      '* [WWS\\({}^{+}\\)22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _ 신경 정보 처리 시스템_, 2022년 35:24824-24837의 발전.\n' +
      '* [ZCS\\({}^{+}\\)23] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhang, Zhanghao Wu, Yonghao Zhang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot areana. _ arXiv preprint arXiv:2306.05685_, 2023.\n' +
      '* [ZHB\\({}^{+}\\)19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 헬라스바그: 기계가 정말로 당신의 문장을 끝낼 수 있을까요? _Computational Linguistics Association의 57번째 연차 회의 진행률_ 에서 2019년 4791-4800 페이지입니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>