# Unified Text-to-Text Transformer를 사용한 전이 학습의 한계 탐색

Colin Raffel

* 동등 기여도. 각 저자의 기여에 대한 설명은 부록 A. craffel@gmail.com에 해당한다.

Noam Shazeer

* noam@google.com

Adam Roberts

* adarob@google.com

Katherine Lee

* katherinelee@google.com

Sharan Narang

* sharannarang@google.com

Michael Matena

* mmatena@google.com

Yanqi Zhou

*Wei Li

* mweili@google.com

류피터

* peterjliu@google.com

구글, 마운틴뷰, CA 94043, USA

###### Abstract

모델이 다운스트림 태스크에서 미세 조정되기 전에 데이터가 풍부한 태스크에 대해 먼저 사전 훈련되는 전이 학습은 자연어 처리(NLP)에서 강력한 기술로 부상했다. 전이학습의 효과성은 접근법, 방법론, 실천의 다양성을 낳았다. 본 논문에서는 모든 텍스트 기반 언어 문제를 텍스트 대 텍스트 형식으로 변환하는 통합 프레임워크를 도입하여 NLP를 위한 전이 학습 기법의 경관을 탐구한다. 우리의 체계적인 연구는 수십 가지 언어 이해 작업에 대한 사전 교육 목표, 아키텍처, 레이블이 지정되지 않은 데이터 세트, 이전 접근법 및 기타 요인을 비교한다. 탐구의 통찰력과 척도 및 새로운 "거대한 깨끗한 크롤링 코퍼스"를 결합하여 요약, 질문 응답, 텍스트 분류 등을 포함하는 많은 벤치마크에서 최첨단 결과를 달성한다. NLP를 위한 전이 학습에 대한 향후 작업을 용이하게 하기 위해 데이터 세트, 사전 훈련된 모델 및 코드를 출시한다.

각주 1: [https://github.com/google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer)

트레이커 학습, 자연어 처리, 멀티태스킹 학습, 어텐션 기반 모델, 딥러닝

## 1 Introduction

자연어 처리(NLP) 태스크들을 수행하기 위해 기계 학습 모델을 트레이닝하는 것은 종종 모델이 다운스트림 학습에 순응가능한 방식으로 텍스트를 프로세싱할 수 있음을 요구한다. 이는 모형이 텍스트를 "이해"할 수 있도록 하는 범용의 지식을 개발하는 것으로 느슨하게 볼 수 있다. 이 지식은 하위 레벨(예를 들어, 철자)에서 다양할 수 있다.

[MISSING_PAGE_FAIL:2]

요약 및 감정 분류를 몇 가지 예로 들 수 있다. 이 통합된 접근 방식을 통해 다양한 전이 학습 목표, 레이블이 지정되지 않은 데이터 세트 및 기타 요인의 효과를 비교할 수 있는 동시에 이전에 고려된 모델 및 데이터 세트를 확장하여 NLP에 대한 전이 학습의 한계를 탐색할 수 있다.

우리는 우리의 목표가 새로운 방법을 제안하는 것이 아니라 그 분야가 어디에 서 있는지에 대한 포괄적인 관점을 제공하는 것임을 강조한다. 따라서 우리의 작업은 주로 기존 기술에 대한 조사, 탐색 및 경험적 비교로 구성된다. 우리는 또한 우리가 고려하는 많은 작업에서 최첨단 결과를 얻기 위해 체계적인 연구(최대 110억 매개변수의 훈련 모델)의 통찰력을 확장하여 현재 접근법의 한계를 탐구한다. 이 규모의 실험을 수행하기 위해 웹에서 스크래핑된 수백 기가바이트의 깨끗한 영어 텍스트로 구성된 데이터 세트인 "거대한 깨끗한 크롤링 코퍼스"(C4)를 소개한다. 전이 학습의 주요 효용이 데이터 부족 설정에서 사전 훈련된 모델의 활용 가능성임을 인식하여 코드, 데이터 세트 및 사전 훈련된 모델을 출시한다.1

각주 1: [https://github.com/google-research/](https://github.com/google-research/)

이 논문의 나머지 부분은 다음과 같이 구성된다. 다음 섹션에서는 기본 모델과 그 구현, 모든 텍스트 처리 문제를 텍스트 대 텍스트 태스크로 공식화하는 절차 및 우리가 고려하는 태스크 세트에 대해 논의한다. 섹션 3에서는 NLP에 대한 전이 학습 분야를 탐구하는 대규모 실험 세트를 제시한다. 섹션(섹션 3.7)이 끝나면 체계적인 연구의 통찰력을 결합하여 다양한 벤치마크에 대한 최첨단 결과를 얻는다. 마지막으로 결과를 요약하고 섹션 4에서 미래를 내다보는 것으로 마무리한다.

그림 1: 텍스트 간 프레임워크의 다이어그램입니다. 번역, 질의 응답 및 분류를 포함하여 우리가 고려하는 모든 작업은 모델 텍스트를 입력으로 공급하고 일부 대상 텍스트를 생성하기 위해 훈련하는 것으로 캐스팅된다. 이를 통해 동일한 모델, 손실 함수, 하이퍼파라미터 등을 사용할 수 있습니다. 우리의 다양한 과제들 전반에 걸쳐. 또한 실증 조사에 포함된 방법에 대한 표준 테스트 베드를 제공합니다. "T5"는 "**T**ext-**to**-**T**ext **T**ransfer **T**ransformer" 라고 하는 모델을 참조 합니다.

## 2 Setup

대규모 실증 연구의 결과를 제시하기 전에 트랜스포머 모델 아키텍처와 우리가 평가하는 다운스트림 태스크를 포함하여 결과를 이해하는 데 필요한 배경 주제를 검토한다. 또한 모든 문제를 텍스트 대 텍스트 태스크로 처리하는 방법을 소개하고 레이블이 지정되지 않은 텍스트 데이터의 원본으로 만든 공통 크롤 기반 데이터 세트인 "거대한 깨끗한 크롤 코퍼스"(C4)를 설명한다. 모델 및 프레임워크를 "**T**ext-**to-**T**ext **T**ransfer **T**ransformer"(T5)라고 합니다.

### Model

NLP 레버리지 순환 신경망에 대한 전이 학습에 대한 초기 결과(Peters et al., 2018; Howard and Ruder, 2018), 그러나 최근에 "Transformer" 아키텍처에 기초한 모델을 사용하는 것이 더 보편화되었다(Vaswani et al., 2017). 트랜스포머는 처음에는 기계 번역에 효과적인 것으로 나타났지만, 이후 매우 다양한 NLP 설정에서 사용되었다(Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018). 편재성이 증가하기 때문에 우리가 연구하는 모든 모델은 변압기 아키텍처를 기반으로 합니다. 아래에 언급된 세부 사항과 섹션 3.2에서 탐구하는 변형 외에도 원래 제안된 대로 이 아키텍처에서 크게 벗어나지 않는다. 이 모델에 대한 포괄적인 정의를 제공하는 대신 관심 있는 독자를 원본 논문(바스와니 등, 2017) 또는 후속 자습서 3,4를 참조하여 보다 자세한 소개를 한다.

각주 3: [http://nlp.seas.harvard.edu/2018/04/03/attention.html](http://nlp.seas.harvard.edu/2018/04/03/attention.html)

각주 4: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)

Transformer의 주요 빌딩 블록은 셀프-어텐션이다(Cheng et al., 2016). 셀프-어텐션은 각 요소를 시퀀스의 나머지 부분의 가중 평균에 의해 대체함으로써 시퀀스를 처리하는 어텐션의 변형이다(Graves, 2013; Bahdanau et al., 2015). 원래의 Transformer는 인코더-디코더 아키텍처로 구성되었고, 시퀀스-투-시퀀스(Sutskever et al., 2014; Kalchbrenner et al., 2014) 작업을 위해 의도되었다. 언어 모델링에 적합한 아키텍처를 생성하기 위해 사용되는 다양한 형태의 셀프-어텐션(Radford et al., 2018; Al-Rfou et al., 2019) 또는 분류 및 스팬 예측 작업(Devlin et al., 2018; Yang et al., 2019)과 함께 단일 트랜스포머 층 스택으로 구성된 모델을 사용하는 것도 최근 일반화되고 있다. 우리는 섹션 3.2에서 이러한 아키텍처 변형을 경험적으로 탐구한다.

전반적으로, 우리의 인코더-디코더 트랜스포머 구현은 원래 제안된 형태를 밀접하게 따른다(Vaswani 등, 2017). 먼저, 토큰들의 입력 시퀀스는 임베딩들의 시퀀스에 매핑되고, 그 후 인코더로 전달된다. 인코더는 "블록들"의 스택으로 구성되며, 이들 각각은 자기-주의 계층 다음에 작은 피드-포워드 네트워크가 뒤따르는 두 개의 서브컴포넌트들을 포함한다. 레이어 정규화(Ba 등, 2016)는 각 서브컴포넌트의 입력에 적용된다. 우리는 활성화가 재조정되고 추가 바이어스가 적용되지 않는 단순화된 버전의 계층 정규화를 사용한다. 계층 정규화 후, 잔차 스킵 연결(He et al., 2016)은 각각의 서브컴포넌트의 입력을 자신의 출력에 추가한다. 드롭아웃(Srivastava et al., 2014)은 피드포워드 네트워크 내에서, 스킵 연결 상에서, 어텐션 가중치 상에서, 그리고 전체 스택의 입력 및 출력에서 적용된다. 디코더는 인코더의 출력에 참석하는 각각의 자기-주의 계층 이후의 표준 주의 메커니즘을 포함한다는 점을 제외하고는 인코더와 구조가 유사하다. 디코더에서의 자기-주의 메커니즘은 또한 자기회귀 또는 인과적 자기-주의 형태를 이용하는데, 이는 단지 모델이 과거의 출력들에 참석할 수 있게 한다. 최종 디코더 블록의 출력은 소프트맥스 출력을 갖는 조밀한 계층으로 공급되며, 이들의 가중치는 입력 임베딩 매트릭스와 공유된다. 트랜스포머의 모든 주의 메커니즘은 추가 처리 전에 출력이 연결된 독립적인 "헤드"로 나뉜다.

자기 주의는 차수에 독립적이므로(즉, 집합에 대한 연산이므로), 명시적인 위치 신호를 Transformer에 제공하는 것이 일반적이다. 원본 Transformer는 정현파 위치 신호 또는 학습된 위치 임베딩을 사용하였지만, 최근에는 상대적인 위치 임베딩을 사용하는 것이 보편화되고 있다(Shaw et al., 2018; Huang et al., 2018). 각각의 포지션에 대해 고정된 임베딩을 사용하는 대신에, 상대 포지션 임베딩들은 자기-주의 메커니즘에서 비교되는 "키"와 "쿼리" 사이의 오프셋에 따라 상이한 학습된 임베딩을 생성한다. 우리는 각 "임베딩"이 단순히 주의 가중치를 계산하는 데 사용되는 해당 로짓에 추가되는 스칼라인 단순화된 형태의 위치 임베딩을 사용한다. 효율성을 위해, 우리는 또한 모델의 모든 계층에 걸쳐 위치 임베딩 파라미터를 공유하지만, 주어진 계층 내에서 각각의 주의 헤드는 상이한 학습된 위치 임베딩을 사용한다. 전형적으로, 고정된 개수의 임베딩이 학습되며, 각각은 가능한 키-쿼리 오프셋의 범위에 대응한다. 이 작업에서 크기가 대수적으로 증가하는 범위가 128 이상의 오프셋까지 있는 모든 모델에 대해 32개의 임베딩을 사용하여 모든 상대 위치를 동일한 임베딩에 할당한다. 주어진 계층은 128 토큰 이상의 상대적인 위치에 둔감하지만, 후속 계층은 이전 계층으로부터의 로컬 정보를 조합함으로써 더 큰 오프셋에 대한 감도를 구축할 수 있다는 점에 유의한다. 요약하면, 본 모델은 계층 노멀 바이어스를 제거하고, 계층 정규화를 잔여 경로 외부에 배치하고, 상이한 위치 임베딩 방식을 사용하는 것을 제외하고, Vaswani 등(2017)에 의해 제안된 원래의 Transformer와 대략 동일하다. 이러한 구조적 변화는 이전 학습에 대한 경험적 조사에서 고려하는 실험 요소와 직교하기 때문에 향후 작업에 대한 영향을 제거한다.

연구의 일환으로, 우리는 이러한 모델의 확장성, 즉 더 많은 매개변수 또는 계층을 갖도록 만들어짐에 따라 성능이 어떻게 변하는지를 실험한다. 큰 모델을 훈련하는 것은 하나의 기계에 맞지 않을 수 있고 많은 계산이 필요하기 때문에 자명하지 않을 수 있다. 그 결과, 클라우드 TPU Pods의 "슬라이스"에서 모델과 데이터 병렬성을 조합하고 모델을 훈련한다. 5개의 TPU Pod는 CPU 호스트 머신을 지원하는 고속 2D 메쉬 인터커넥트를 통해 연결된 1,024개의 TPU v3 칩을 포함하는 멀티랙 ML 슈퍼컴퓨터이다. 모델 병렬성 및 데이터 병렬성(Krizhevsky, 2014)의 구현의 용이성을 위해 Mesh TensorFlow 라이브러리(Shazeer 등, 2018)를 활용한다.

각주 5: [https://cloud.google.com/tpu/](https://cloud.google.com/tpu/)

### Colossal Clean Crawled Corpus

NLP에 대한 전이 학습에 대한 이전 작업의 대부분은 비지도 학습을 위해 레이블이 지정되지 않은 큰 데이터 세트를 사용한다. 본 논문에서는 레이블이 지정되지 않은 데이터의 품질, 특성 및 크기의 영향을 측정하는 데 관심이 있다. 우리의 요구를 만족시키는 데이터 세트를 생성하기 위해, 우리는 웹에서 스크래핑된 텍스트의 소스로 커먼 크롤을 활용한다. CommonCrawl은 이전에 NLP를 위한 텍스트 데이터의 소스로서, 예를 들어 n-그램 언어 모델을 트레이닝하기 위해(Buck et al., 2014), 상식 추론을 위한 트레이닝 데이터로서(Trinh and Le, 2018), 기계 번역을 위한 병렬 텍스트를 마이닝하기 위해(Smith et al., 2013), 사전-트레이닝 데이터 세트로서(Grave et al., 2018; Zellers et al., 2019; Liu et al., 2019c), 심지어 단순히 최적화자를 테스트하기 위한 거대 텍스트 코퍼스로서(Anil et al., 2019) 사용되었다.

커먼 크롤은 스크래핑된 HTML 파일에서 마크업 및 기타 비텍스트 콘텐츠를 제거하여 "웹 추출 텍스트"를 제공하는 공개적으로 사용할 수 있는 웹 아카이브이다. 이 프로세스는 매달 약 20TB의 스크래핑된 텍스트 데이터를 생성합니다. 불행하게도, 결과 텍스트의 대다수는 자연어가 아니다. 대신 메뉴, 오류 메시지 또는 중복 텍스트와 같은 횡설수설 또는 상용판 텍스트로 크게 구성됩니다. 또한, 스크랩된 텍스트의 상당수는 우리가 고려하는 작업(공격적인 언어, 자리 표시자 텍스트, 소스 코드 등)에 도움이 되지 않을 것 같은 콘텐츠를 포함한다. 이러한 문제를 해결하기 위해 커먼 크롤의 웹 추출 텍스트를 정리하기 위해 다음과 같은 휴리스틱을 사용했다.

* 말단 구두점 표시(즉, 마침표, 느낌표, 물음표 또는 끝 따옴표)로 끝나는 줄만 보유했습니다.
* 3개 미만의 문장이 있는 모든 페이지를 폐기하고 5개 이상의 단어가 포함된 줄만 보유했습니다.
* "Dirty, Naughty, Obscene 또는 기타 잘못된 단어 목록"에 단어가 포함된 페이지를 제거했습니다. 6 각주 6: [https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words)
* 스크래핑 된 많은 페이지에는 Javascript를 사용 하도록 설정 해야 한다는 경고가 포함 되어 있으므로 Javascript라는 단어를 사용 하 여 줄을 제거 했습니다.
* 일부 페이지에는 자리 표시자 "lorem ipsum" 텍스트가 있었고 "lorem ipsum" 문구가 표시 된 모든 페이지를 제거 했습니다.
* 일부 페이지에는 실수로 코드가 포함되어 있습니다. 컬리 괄호 "{"는 많은 프로그래밍 언어(예: 웹에서 널리 사용되는 자바스크립트)에 표시되지만 자연 텍스트에는 표시되지 않기 때문에 컬리 괄호가 포함된 모든 페이지를 제거했다.
* 스크래핑된 페이지 중 일부는 위키피디아에서 공급되고 인용 표지자(예: [1], [인용 필요] 등)가 있기 때문에 이러한 표지자를 제거했습니다.
* 많은 페이지에 상용구 정책 알림이 있으므로 문자열 "사용 약관", "프라이버시 정책", "쿠키 정책", "쿠키 사용", "쿠키 사용" 또는 "쿠키 사용"이 포함된 줄을 제거했습니다.
* 데이터 집합을 중복 복제 하기 위해 데이터 집합에서 두 번 이상 발생 하는 3 문장 범위 중 하나를 제외 하 고 모두 폐기 했습니다.

또한, 대부분의 다운스트림 작업은 영어 텍스트에 초점을 맞추고 있기 때문에 langdetect7을 사용하여 0.99 이상의 확률로 영어로 분류되지 않은 모든 페이지를 필터링했다. 우리의 휴리스틱은 NLP의 데이터 소스로 CommonCrawl을 사용하는 과거 작업에서 영감을 받았다. 예를 들어 Grave et al.(2018)은 자동 언어 검출기를 사용하여 텍스트를 필터링하고 짧은 줄을 폐기하고 Smith et al.(2013)과 Grave et al.(2018)은 모두 줄 수준 중복 제거를 수행한다. 그러나, 이전 데이터 세트가 더 제한된 필터링 휴리스틱 세트를 사용하고, 공개적으로 이용 가능하지 않으며, 및/또는 범위가 다르기 때문에 새로운 데이터 세트를 생성하기로 결정했다(예를 들어, 뉴스 데이터로 제한됨(Zellers et al., 2019; Liu et al., 2019), 크리에이티브 커먼즈 콘텐츠만을 포함함(Habernal et al., 2016), 또는 기계 번역을 위한 병렬 트레이닝 데이터에 집중됨(Smith et al., 2013).

기본 데이터 세트를 조립하기 위해 2019년 4월부터 웹 추출 텍스트를 다운로드하고 앞서 언급한 필터링을 적용했다. 이것은 사전 훈련(약 750 GB)에 사용되는 대부분의 데이터 세트보다 10배 더 클 뿐만 아니라 합리적으로 깨끗하고 자연스러운 영어 텍스트로 구성되는 텍스트의 컬렉션을 생성한다. 이 데이터 집합을 "**C**올로살 **C**lean **C**rawled **C**orpus"(또는 줄여서 C4)라고 명명하고 TensorFlow Datasets의 일부로 릴리스합니다.8 섹션 3.4에서 이 데이터 집합의 다양한 대체 버전을 사용할 때의 영향을 고려합니다.

각주 8: [https://www.tensorflow.org/datasets/catalog/c4](https://www.tensorflow.org/datasets/catalog/c4)

### Downstream Tasks

본 논문의 목표는 일반적인 언어 학습 능력을 측정하는 것이다. 따라서 기계 번역, 질의 응답, 추상 요약 및 텍스트 분류를 포함한 다양한 벤치마크 세트에 대한 다운스트림 성능을 연구한다. 구체적으로 GLUE와 SuperGLUE 텍스트 분류 메타 벤치마크, CNN/Daily Mail 추상 요약, SQuAD 질의 응답, WMT 영어에서 독일어, 프랑스어, 루마니아어 번역에 대한 성능을 측정한다. 모든 데이터는 TensorFlow Datasets.9에서 공급됨

각주 9: [https://www.tensorflow.org/datasets](https://www.tensorflow.org/datasets)

GLUE(Wang et al., 2018) 및 SuperGLUE(Wang et al., 2019)는 각각 일반적인 언어 이해 능력을 테스트하기 위한 텍스트 분류 태스크들의 모음을 포함한다:

* 문장 수용성 판단(CoLA(Warstadt et al., 2018))
* 감성분석(SST-2(Socher et al., 2013))
* 패러프레이징/문장 유사성(MRPC(Dolan and Brockett, 2005), STS-B(Cer et al., 2017), QQP(Iyer et al., 2017))
* 자연어 추론(MNLI(Williams et al., 2017), QNLI(Rajpurkar et al., 2016), RTE(Dagan et al., 2005), CB(De Marneff et al., 2019))
* Coreference resolution (WNLI and WSC (Levesque et al., 2012))
* 문장 완성(COPA(Roemmele et al., 2011))
* 단어 의미 중의성 해소(WIC(Pilehvar and Camacho-Collados, 2018))
* 질의 응답(MultiRC(Khashabi et al., 2018), ReCoRD(Zhang et al., 2018), BoolQ(Clark et al., 2019))GLUE 및 SuperGLUE 벤치마크에 의해 분배된 바와 같은 데이터 세트를 사용한다. 단순화를 위해 미세 조정 시 모든 구성 데이터 세트를 연결하여 GLUE 벤치마크(및 SuperGLUE의 경우에도 유사하게)의 모든 작업을 단일 작업으로 처리합니다. Kocijan 등(2019)에 의해 제안된 바와 같이, 우리는 또한 결합된 SuperGLUE 태스크에 DPR(Definite Pronoun Resolution) 데이터 세트(Rahman and Ng, 2012)를 포함한다.

CNN/Daily Mail (Hermann et al., 2015) 데이터 세트는 질문-응답 태스크로서 도입되었지만, Nallapati et al.(2016)에 의해 텍스트 요약을 위해 적응되었다; 우리는 추상 요약 태스크로서 See et al.(2017)의 비익명화된 버전을 사용한다. SQuAD(Rajpurkar et al., 2016)는 일반적인 질문-응답 벤치마크이다. 우리의 실험에서, 모델은 질문과 그 문맥을 공급하고 답변 토큰-대-토큰을 생성하도록 요청받는다. WMT English to German의 경우, (Vaswani et al., 2017) (즉, News Commentary v13, Common Crawl, Europarl v7) 및 newstest2013과 동일한 트레이닝 데이터를 검증 세트(Bojar et al., 2014)로 사용한다. 영어에서 프랑스어까지의 경우, 2015년과 newstest2014의 표준 트레이닝 데이터를 검증 세트로 사용한다(Bojar et al., 2015). 표준 하위 자원 기계 번역 벤치마크인 영어에서 루마니아로의 경우, WMT 2016(Bojar et al., 2016)의 트레인 및 검증 세트를 사용한다. 우리는 영어 데이터만 미리 학습하므로 주어진 모델을 번역하는 법을 배우기 위해서는 새로운 언어로 텍스트를 생성하는 법을 배워야 합니다.

### 입력 및 출력 형식

위에서 설명한 다양한 작업 집합에서 단일 모델을 훈련시키기 위해, 우리는 우리가 고려하는 모든 작업을 "텍스트 대 텍스트" 형식으로 캐스팅한다. 즉, 모델이 컨텍스트 또는 컨디셔닝을 위해 일부 텍스트를 공급받은 다음 일부 출력 텍스트를 생성하도록 요청받는 작업이다. 이 프레임워크는 사전 훈련과 미세 조정을 위한 일관된 훈련 목표를 제공한다. 구체적으로, 모델은 태스크에 관계없이 최대 가능성 목표("교사 강제"(Williams and Zipser, 1989)를 사용하여)로 트레이닝된다. 모델이 수행해야 하는 작업을 지정하기 위해 모델에 공급하기 전에 원래 입력 시퀀스에 작업별(텍스트) 접두사를 추가한다.

일례로서, 모델에 영어에서 독일어로 "그것은 좋다."라는 문장을 번역하도록 요청하기 위해, 모델은 "영어를 독일어로 번역: 그것은 좋다."라는 시퀀스를 공급받을 것이고, "Das ist gut"을 출력하도록 훈련될 것이다. 텍스트 분류 태스크들에 대해, 모델은 타겟 라벨에 대응하는 단일 단어를 단순히 예측한다. 예를 들어, MNLI 벤치마크(Williams 등, 2017)에서 목표는 전제가 ("entailment"), 모순("contradiction") 또는 둘 다("중립") 가설을 내포하는지 여부를 예측하는 것이다. 우리의 전처리 과정을 통해 입력 시퀀스는 "전제: 나는 비둘기를 싫어한다. 가설: 비둘기에 대한 나의 감정은 적대감으로 가득 차 있다."라는 대응 대상 단어 "감정"으로 구성된다. 우리 모델이 가능한 레이블 중 하나에 해당 하지 않는 텍스트 분류 작업에서 텍스트를 출력 하는 경우 문제가 발생 합니다 (예: 작업에 대 한 유일한 가능한 레이블이 "발송", "중립" 또는 "모순" 인 경우 모델이 "햄버거"를 출력 하는 경우). 이 경우 우리는 항상 모델의 출력을 잘못된 것으로 간주하지만 훈련된 모델에서는 이러한 행동을 관찰한 적이 없다. 주어진 작업에 사용되는 텍스트 접두사의 선택은 본질적으로 하이퍼파라미터이며, 접두사의 정확한 문구를 변경하는 것은 제한된 영향을 미치므로 다양한 접두사 선택으로 광범위한 실험을 수행하지 않는다는 것을 발견했다. 몇 가지 입력/출력 예가 있는 텍스트 대 텍스트 프레임워크의 다이어그램은 그림 1에 나와 있다. 우리는 부록 D에서 연구한 모든 작업에 대해 전처리된 입력의 전체 예를 제공한다.

우리의 텍스트-대-텍스트 프레임워크는 다수의 NLP 태스크들을 공통 포맷으로 캐스팅하는 이전 작업을 따른다: McCann 등(2018)은 10개의 NLP 태스크들의 세트에 대해 일관된 질문-응답 포맷을 사용하는 벤치마크인 "자연 언어 데카슬론"을 제안한다. 자연어 10종경기는 또한 모든 모델이 다중 작업이어야 하며, 즉 모든 작업을 한 번에 동시에 처리할 수 있다고 규정하고 있다. 우리는 대신 각 개별 태스크에서 모델을 별도로 미세 조정하고 명시적인 질문-답변 형식 대신 짧은 태스크 접두사를 사용한다. 래드포드 등(2019)은 일부 입력을 모델에 프리픽스로 피딩한 다음 출력을 자동 샘플링함으로써 언어 모델의 제로 샷 학습 능력을 평가한다. 예를 들어, 자동 요약은 문서 다음에 텍스트 "TL;DR:"(너무 긴, 읽지 않은, 일반적인 약어)로 피딩함으로써 수행되고, 그 후 요약은 자기회귀 디코딩을 통해 예측된다. 우리는 별도의 디코더를 사용하여 출력을 생성하기 전에 인코더로 입력을 명시적으로 처리하는 모델을 주로 고려하고 제로 샷 학습보다는 전이 학습에 중점을 둔다. 마지막으로, Keskar 등(2019)은 많은 NLP 태스크들을 "스팬 추출"로 통일하는데, 여기서 가능한 출력 선택들에 대응하는 텍스트가 입력에 부가되고 모델이 올바른 선택에 대응하는 입력 스팬을 추출하도록 트레이닝된다. 대조적으로, 우리의 프레임워크는 또한 가능한 모든 출력 선택을 열거할 수 없는 기계 번역 및 추상 요약과 같은 생성 작업을 허용한다.

1과 5 사이의 유사도 점수를 예측하는 것을 목표로 하는 회귀 작업인 STS-B를 제외하고, 우리가 고려했던 모든 작업을 텍스트 대 텍스트 형식으로 직접 캐스팅할 수 있었다. 이러한 점수의 대부분은 0.2 단위로 주석이 달려 있음을 발견했으며, 따라서 우리는 임의의 점수를 0.2의 가장 가까운 증분으로 간단히 반올림하고 결과를 숫자의 문자 문자열 표현으로 변환했다(예: 부동 소수점 값 2.57은 문자열 "2.6"에 매핑될 것이다). 테스트 시간에 모델이 1과 5 사이의 숫자에 해당하는 문자열을 출력하면 부동 소수점 값으로 변환하고 그렇지 않으면 모델의 예측을 부정확한 것으로 처리한다. 이는 STS-B 회귀 문제를 21-클래스 분류 문제로 효과적으로 재조명한다.

이와 별도로 위노그라드 작업(GLUE의 WLI, SuperGLUE의 WSC 및 SuperGLUE에 추가하는 DPR 데이터 세트)을 텍스트 간 프레임워크에 더 적합한 더 간단한 형식으로 변환한다. 위노그라드 태스크의 예는 구절의 명사구 중 하나 이상을 참조할 수 있는 모호한 대명사를 포함하는 텍스트 구절로 구성된다. 예를 들어 ‘시의원들이 폭력을 두려워해 시위대를 허가하지 않았다’는 구절이 ‘시의원’이나 ‘시위대’를 지칭할 수 있는 애매한 대명사 ‘그들’을 담고 있을 수 있다. WNLI, WSC, DPR 태스크는 텍스트 문장에서 모호한 대명사를 강조하고 모델이 참조하는 명사를 예측하도록 요청하여 텍스트 대 텍스트 문제로 캐스팅한다. 위에서 언급한 예는 "시의원들이 폭력을 두려워하여 시위자들에게 허가를 거절했다."라는 입력으로 변환되고 모델은 "시의원들"이라는 목표 텍스트를 예측하도록 훈련될 것이다.

WSC의 경우, 예들은 구절, 모호한 대명사, 후보 명사, 및 후보가 대명사와 일치하는지 여부를 반영하는 True/False 라벨(임의의 관사를 무시함)을 포함한다. 우리는 "거짓" 레이블이 있는 예제의 정확한 명사 대상을 알지 못하기 때문에 "참" 레이블이 있는 예제에 대해서만 훈련한다. 평가를 위해 모델의 출력에 있는 단어가 후보 명사구(또는 그 반대)에 있는 단어의 하위 집합인 경우 "참" 레이블을 할당하고 그렇지 않은 경우 "거짓" 레이블을 할당한다. 이것은 WSC 트레이닝 세트의 대략 절반을 제거하지만, DPR 데이터 세트는 약 1,000개의 대명사 해상도 예들을 추가한다. DPR의 예들은 정확한 지시명사로 주석이 달려서, 위에 나열된 포맷으로 이 데이터 세트를 쉽게 사용할 수 있게 한다.

WNLI 훈련 및 검증 세트는 WSC 훈련 세트와 상당한 중첩을 갖는다. 검증 예제가 학습 데이터로 누출되는 것을 피하기 위해(섹션 3.5.2의 다중 작업 실험의 특정 문제), 따라서 우리는 WNLI에서 훈련하지 않고 WNLI 검증 세트에 대한 결과를 보고하지 않는다. WNLI 검증 세트에 대한 결과를 생략하는 것은 훈련 세트에 대해 "적대적"이라는 사실로 인해 표준 관행(Devlin 등, 2018)이며, 즉 검증 예는 모두 반대 라벨을 갖는 훈련 예의 약간 교란된 버전이다. 따라서 검증 세트에 대해 보고할 때마다 평균 GLUE 점수에 WNLI를 포함하지 않는다(테스트 세트에 결과가 표시되는 섹션 3.7을 제외한 모든 섹션). WNLI에서 위에서 설명한 "참조 명사 예측" 변형으로 예제를 변환하는 것은 약간 더 관련되어 있으며 부록 B에서 이 과정을 설명한다.

## 3 Experiments

최근 NLP에 대한 전이 학습의 발전은 새로운 사전 훈련 목표, 모델 아키텍처, 레이블이 지정되지 않은 데이터 세트 등과 같은 다양한 개발에서 비롯되었다. 이 섹션에서는 이러한 기술의 기여와 중요성을 구별하기 위해 이러한 기술에 대한 경험적 조사를 수행한다. 그런 다음 우리가 고려하는 많은 작업에서 최첨단을 달성하기 위해 얻은 통찰력을 결합합니다. NLP에 대한 전이 학습은 빠르게 성장하는 연구 영역이기 때문에 경험적 연구에서 가능한 모든 기술이나 아이디어를 다루는 것은 가능하지 않다. 보다 광범위한 문헌 검토를 위해 Ruder 등(2019)의 최근 조사를 권장한다.

우리는 합리적인 기준선(섹션 3.1에 설명됨)을 취하고 한 번에 설정의 한 측면을 변경하여 이러한 기여를 체계적으로 연구한다. 예를 들어, 섹션 3.3에서 우리는 나머지 실험 파이프라인을 고정시키면서 다른 감독되지 않은 목표의 성능을 측정한다. 이 "좌표 상승" 접근법은 2차 효과를 놓칠 수 있지만(예: 일부 특정 감독되지 않은 목표가 기준 설정보다 큰 모델에서 가장 잘 작동할 수 있지만 연구의 모든 요인에 대한 조합 탐색을 수행하는 것은 엄청나게 비쌀 것이다. 향후 작업에서 우리가 연구하는 접근법의 조합을 더 철저히 고려하는 것이 유익할 수 있을 것으로 기대한다.

우리의 목표는 가능한 한 많은 요소를 고정시키면서 다양한 작업 세트에 대한 다양한 접근법을 비교하는 것이다. 이 목표를 충족시키기 위해 어떤 경우에는 기존 접근법을 정확하게 복제하지 않는다. 예를 들어, BERT(Devlin 등, 2018)와 같은 "인코더 전용" 모델들은 입력 토큰당 단일 예측 또는 전체 입력 시퀀스에 대한 단일 예측을 생성하도록 설계된다. 이렇게 하면 분류 또는 범위 예측 작업에 적용할 수 있지만 번역 또는 추상 요약과 같은 생성 작업에는 적용할 수 없습니다. 이와 같이, 우리가 고려하는 모델 아키텍처들 중 어느 것도 BERT와 동일하거나 인코더 전용 구조로 구성되지 않는다. 대신 정신적으로 유사한 접근법을 테스트하는데, 예를 들어 섹션 3.3의 BERT의 "마스크 언어 모델링" 목표와 유사한 목표를 고려하고 섹션 3.2의 텍스트 분류 작업에서 BERT와 유사하게 작동하는 모델 아키텍처를 고려한다.

다음 세부 섹션에서 기본 실험 설정을 설명한 후 모델 아키텍처(섹션 3.2), 감독되지 않은 목표(섹션 3.3), 사전 훈련 데이터 세트(섹션 3.4), 이전 접근법(섹션 3.5) 및 스케일링(섹션 3.6)의 경험적 비교를 수행한다. 이 섹션의 절정에서 우리는 연구의 통찰력을 규모와 결합하여 우리가 고려하는 많은 작업에서 최첨단 결과를 얻는다(섹션 3.7).

### Baseline

기준선에 대한 우리의 목표는 전형적인 현대 관행을 반영하는 것입니다. 우리는 간단한 잡음 제거 목표를 사용하여 표준 트랜스포머(섹션 2.1에서 설명됨)를 사전 훈련한 다음 각 다운스트림 작업에 대해 개별적으로 미세 조정한다. 우리는 다음 하위 섹션에서 이 실험 설정의 세부 사항을 설명한다.

#### 3.1.1 Model

본 모델의 경우 Vaswani 등(2017)이 제안한 표준 인코더-디코더 Transformer를 사용한다. NLP를 위한 전이 학습에 대한 많은 현대적 접근법들은 단지 단일 "스택"으로 구성된 Transformer 아키텍처(예를 들어, 언어 모델링을 위한 (Radford et al., 2018; Dong et al., 2019) 또는 분류 및 스팬 예측(Devlin et al., 2018; Yang et al., 2019)을 사용하지만, 표준 인코더-디코더 구조를 사용하는 것이 생성 및 분류 작업 모두에서 양호한 결과를 달성한다는 것을 발견하였다. 우리는 섹션 3.2에서 다양한 모델 아키텍처의 성능을 탐구한다.

우리의 기본 모델은 인코더와 디코더가 각각 "BERT\({}_{\text{BASE}}\)" (Devlin et al., 2018) 스택과 크기와 구성이 유사하도록 설계되었다. 구체적으로, 인코더 및 디코더 모두는 12개의 블록들(각각의 블록은 셀프-어텐션, 선택적 인코더-디코더 어텐션, 및 피드-포워드 네트워크를 포함함)로 구성된다. 각 블록의 피드포워드 네트워크는 \(d_{\text{ff}}=3072\)의 출력 차원을 갖는 조밀한 계층과 ReLU 비선형성 및 다른 조밀한 계층으로 구성된다. 모든 주의 메커니즘의 "키"와 "값" 행렬은 \(d_{\text{kv}}=64\)의 내적 차원을 가지며 모든 주의 메커니즘은 12개의 헤드를 갖는다. 다른 모든 하위 계층과 임베딩은 \(d_{\text{model}}=768\)의 차원을 갖는다. 총합하면 약 2억 2천만 개의 모수를 갖는 모형이 된다. 이것은 BERT\({}_{\text{BASE}}\)의 파라미터 수의 대략 두 배인데, 이는 우리의 기준 모델이 하나의 층 스택 대신 두 개의 층 스택을 포함하기 때문이다. 정규화를 위해, 우리는 모델에 드롭아웃이 적용되는 모든 곳에서 0.1의 드롭아웃 확률을 사용한다.

#### 3.1.2 Training

섹션 2.4에 설명된 바와 같이, 모든 태스크는 텍스트 대 텍스트 태스크로서 공식화된다. 이를 통해 우리는 항상 표준 최대 가능성, 즉 교사 강제(Williams and Zipser, 1989)와 교차 엔트로피 손실을 사용하여 훈련할 수 있다. 최적화를 위해 AdaFactor(Shazeer and Stern, 2018)를 사용한다. 테스트 시간에는 탐욕스러운 디코딩(즉, 모든 시간 단계에서 가장 높은 확률 로짓 선택)을 사용합니다.

우리는 미세 조정 전에 C4의 \(2^{19}=524,\!288\) 단계에 대해 각 모델을 사전 훈련한다. 우리는 512의 최대 서열 길이와 128개의 서열 배치 크기를 사용한다. 가능하면 여러 시퀀스를 배치10의 각 엔트리에 "팩"하여 배치에 대략 \(2^{16}=65\),\(536\) 토큰이 포함되도록 한다. 이 배치 크기와 단계 수는 \(2^{35}\approx 34\)B 토큰에 대한 사전 훈련에 해당한다. 이는 대략 137B 토큰을 사용한 BERT(Devlin et al., 2018), 또는 대략 2.2T 토큰을 사용한 RoBERTa(Liu et al., 2019)보다 상당히 적다. \(2^{35}\) 토큰만을 사용하는 것은 합리적인 계산 예산을 제공하면서도 수용 가능한 성능을 위해 충분한 양의 사전 훈련을 제공한다. 섹션 3.6 및 3.7에서 더 많은 단계에 대 한 사전 훈련의 효과를 고려 합니다. \(2^{35}\) 토큰은 전체 C4 데이터 집합의 일부만 포함 하므로 사전 훈련 중에 데이터를 반복 하지 않습니다.

각주 10: [https://www.pydoc.io/pypi/tensor2tensor-1.5.7/autoapi/data_generators/generator_utils/index.html#data_generators.generator_utils.pack_examples](https://www.pydoc.io/pypi/tensor2tensor-1.5.7/autoapi/data_generators/generator_utils/index.html#data_generators.generator_utils.pack_examples)

사전 훈련 시, 우리는 역제곱근 학습 속도 스케줄(1/\sqrt{\max(n,k)}\)을 사용한다. 여기서, \(n\)은 현재 훈련 반복이고 \(k\)은 모든 실험에서 준비 단계(10^{4}\)로 설정)의 수이다. 이는 첫 번째 단계인 \(10^{4}\)에 대해 \(0.01\)의 일정한 학습률을 설정한 후, 사전 학습이 끝날 때까지 학습률을 지수적으로 감소시킨다. 또한 삼각형 학습률(Howard and Ruder, 2018)을 사용하여 실험하였는데, 이는 약간 더 나은 결과를 얻었지만 미리 총 훈련 단계 수를 알아야 한다. 우리는 일부 실험에서 훈련 단계의 수를 변경할 것이기 때문에 더 일반적인 역제곱근 일정을 선택한다.

제안된 모델은 모든 태스크의 \(2^{18}=262\),\(144\) 단계에 대해 세밀하게 조정된다. 이 값은 추가 미세 조정을 통해 이익을 얻는 높은 자원 작업(즉, 데이터 세트가 큰 작업)과 빠르게 과적합되는 낮은 자원 작업(데이터 세트가 작은 작업) 간의 절충으로 선택되었다. 미세 조정 동안, 우리는 128개의 길이-512 시퀀스(즉, 배치당 \(2^{16}\) 토큰)를 갖는 배치를 계속 사용한다. 미세조정을 할 때 일정한 학습률 \(0.001\)을 사용한다. 5,000 단계마다 검사점을 저장 하 고 가장 높은 유효성 검사 성능에 해당하는 모델 검사점에 대 한 결과를 보고 합니다. 여러 작업에서 미세 조정 된 모델의 경우 각 작업에 대 한 최상의 검사점을 독립적으로 선택 합니다. 섹션 3.7의 실험을 제외한 모든 실험에 대해 테스트 세트에 대한 모델 선택을 수행하지 않도록 검증 세트의 결과를 보고한다.

#### 3.1.3 Vocabulary

우리는 SentencePiece(Kudo and Richardson, 2018)를 사용하여 WordPiece 토큰으로 텍스트를 인코딩한다(Sennrich et al., 2015; Kudo, 2018). 모든 실험에서 우리는 32,000개의 단어 조각의 어휘를 사용한다. 우리는 궁극적으로 영어에 대한 모델을 독일어, 프랑스어, 루마니아어로 미세 조정하기 때문에 어휘가 이러한 비영어 언어를 포함해야 합니다. 이를 해결하기 위해 C4에 사용된 커먼 크롤 스크랩의 페이지를 독일어, 프랑스어 및 루마니아어로 분류했다. 그런 다음, 영어 C4 데이터의 10개 부분과 독일어, 프랑스어 또는 루마니아어로 분류된 각 데이터의 1개 부분을 혼합하여 SentencePiece 모델을 학습했다. 이 어휘는 우리 모델의 입력과 출력 모두에 공유되었다. 우리의 어휘는 우리의 모델이 미리 정해진 고정된 언어 세트만 처리할 수 있도록 만듭니다.

#### 3.1.4 Unsupervised Objective

레이블이 없는 데이터를 사용하여 모델을 사전 훈련하려면 레이블이 필요하지 않지만(느슨하게 말하면) 다운스트림 작업에서 유용할 모델 일반화 가능한 지식을 가르치는 목표가 필요하다. 사전 훈련의 전이 학습 패러다임을 적용하고 모델의 모든 파라미터를 NLP 문제에 미세 조정하는 예비 작업은 사전 훈련을 위해 인과적 언어 모델링 목표를 사용했다(Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018; Howard and Ruder, 2018). 그러나, 최근에 "denoising" 목적(Devlin et al., 2018; Taylor, 1953)(또한 "masked language modeling"이라고도 함)이 더 나은 성능을 생성하고 그 결과 그들이 신속하게 표준이 되는 것으로 나타났다. 디노이징 목적에서, 모델은 입력에서 누락되거나 그렇지 않으면 손상된 토큰을 예측하도록 트레이닝된다. BERT의 "마스킹된 언어 모델링" 목표 및 "단어 드롭아웃" 정규화 기법(Bowman et al., 2015)에서 영감을 받아, 입력 시퀀스에서 토큰을 무작위로 샘플링한 다음 15% 드롭아웃하는 목표를 설계한다. 중지된 토큰의 모든 연속 스팬은 단일 센티널 토큰으로 대체됩니다. 각 센티널 토큰에는 시퀀스에 고유한 토큰 ID가 할당됩니다. 센티넬 ID는 우리의 어휘에 추가된 특수 토큰이며 어떤 단어에도 해당하지 않습니다. 그런 다음 대상은 입력 시퀀스에서 사용 되는 동일한 센티널 토큰과 대상 시퀀스의 끝을 표시 하는 최종 센티널 토큰으로 구분 된 드롭 아웃 된 토큰의 모든 범위에 해당 합니다. 사전 훈련의 계산 비용을 줄이기 위해 연속된 토큰 스팬을 마스킹하고 드롭아웃 토큰만 예측하는 우리의 선택이 이루어졌다. 우리는 섹션 3.3에서 사전 훈련 목표에 대한 철저한 조사를 수행한다. 이 목표를 적용한 결과로 인한 변환의 예는 그림 2에 나와 있다. 이 목표를 섹션 3.3의 다른 많은 변형과 경험적으로 비교한다.

#### 3.1.5 Baseline Performance

이 섹션에서는 위에서 설명한 기본 실험 절차를 사용하여 다운스트림 작업 집합에서 어떤 성능을 기대할 수 있는지 결과를 제시한다. 이상적으로는 연구의 모든 실험을 여러 번 반복하여 결과에 대한 신뢰 구간을 얻을 수 있다. 불행히도 이것은 크므로 엄청나게 비쌀 것입니다.

그림 2: 기본 모델에서 사용하는 목표의 개략도. 이 예제에서는 "지난 주에 파티에 초대해 주셔서 감사합니다."라는 문장을 처리합니다. "for", "inviting" 및 "last"( \(\times\)로 표시됨)는 부패를 위해 무작위로 선택됩니다. 손상된 토큰의 각 연속 스팬은 예제에서 고유한 센티널 토큰(<X> 및 <Y>로 표시)으로 대체됩니다. "for"와 "inviting"이 연속적으로 발생하기 때문에, 이들은 하나의 센티넬 <X>로 대체된다. 그런 다음 출력 시퀀스는 입력에서 이를 대체하는 데 사용되는 센티널 토큰과 최종 센티널 토큰 <Z>로 구분되는 드롭아웃 스팬으로 구성됩니다.

우리가 실행하는 실험의 수입니다. 더 저렴한 대안으로 우리는 기본 모델을 처음부터 10번 훈련(즉, 다른 무작위 초기화 및 데이터 세트 셔플링)하고 기본 모델의 이러한 실행에 대한 분산이 각 실험 변형에도 적용된다고 가정한다. 우리는 우리가 하는 대부분의 변화가 실행 간 분산에 극적인 영향을 미칠 것으로 기대하지 않으므로 이것은 다른 변화의 중요성에 대한 합리적인 표시를 제공해야 한다. 또한, 사전 훈련 없이 모든 다운스트림 작업에 대해 \(2^{18}\) 단계(미세 조정에 사용하는 동일한 수)에 대한 모델의 훈련 성능을 측정한다. 이것은 기준선 설정에서 사전 훈련이 우리 모델에 얼마나 많은 이점을 주는지에 대한 아이디어를 제공한다.

본문에서 결과를 보고할 때 공간을 절약하고 해석을 쉽게 하기 위해 모든 벤치마크에 걸쳐 점수의 하위 집합만 보고한다. GLUE 및 SuperGLUE의 경우 "GLUE" 및 "SGLUE" 제목에 따라 모든 하위 작업(공식 벤치마크에서 규정한 대로)의 평균 점수를 보고한다. 모든 번역 작업에 대해, "exp" 평활화 및 "intl" 토큰화와 함께 SacreBLEU v1.3.0(Post, 2018)에 의해 제공된 바와 같이 BLEU 점수(Papineni 등, 2002)를 보고한다. 우리는 WMT 영어에서 독일어, 영어에서 프랑스어, 영어에서 루마니아어에 대한 점수를 각각 EnDe, EnFr 및 EnRo로 참조한다. CNN/Daily Mail의 경우 ROUGE-1-F, ROUGE-2-F 및 ROUGE-L-F 메트릭(Lin, 2004)에 대한 모델의 성능이 높은 상관 관계가 있음을 발견하여 "CNNDM"이라는 제목 아래 ROUGE-2-F 점수만 보고한다. 유사하게, SQuAD의 경우 "정확한 일치" 및 "F1" 점수의 성능이 높은 상관 관계가 있음을 발견하므로 "정확한 일치" 점수만 보고한다. 표 16, 부록 E의 모든 실험에 대해 모든 작업에서 달성된 모든 점수를 제공한다.

우리의 결과 표는 모두 각 행이 각 벤치마크에 대한 점수를 제공하는 열이 있는 특정 실험 구성에 해당하도록 형식이 지정된다. 우리는 대부분의 표에 기준선 구성의 평균 성능을 포함할 것이다. 기준선 구성이 표시 되는 경우 (표 1의 첫 번째 행과 같이) \(\bigstar\)로 표시 합니다. 또한 주어진 실험에서 최대(최상)의 두 표준 편차 내에 있는 모든 점수를 **볼드체** 합니다.

우리의 기준 결과는 표 1에 나와 있다. 전반적으로 우리의 결과는 유사한 크기의 기존 모델과 비슷하다. 예를 들어, BERTBASE는 SQuAD에서 80.8의 정확한 매칭 스코어 및 MNLI-매칭에서 84.4의 정확도를 달성한 반면, 우리는 각각 80.88 및 84.24를 달성한다(표 16 참조). 우리는 부호화기-복호화기 모델이고, 대략적인 \(\nicefrac{{1}}{{4}}\)만큼 사전 훈련되었기 때문에 BERTBASE와 기준선을 직접 비교할 수 없다. 놀랄 것도 없이, 우리는 사전 훈련이 거의 모든 벤치마크에 걸쳐 상당한 이득을 제공한다는 것을 발견한다. 유일한 예외는 WMT 영어에서 프랑스어까지로, 그것은 크다.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & GLUE & CNNDM & SQuAD & SGLUE & EnDe & EnFr & EnRo \\ \hline \(\bigstar\) Baseline average & **83.28** & **19.24** & **80.88** & **71.36** & **26.98** & **39.82** & **27.65** \\ Baseline standard deviation & 0.235 & 0.065 & 0.343 & 0.416 & 0.112 & 0.090 & 0.108 \\ No pre-training & 66.22 & 17.60 & 50.31 & 53.04 & 25.86 & **39.77** & 24.04 \\ \hline \hline \end{tabular}
\end{table}
표 1: 기본 모델 및 훈련 절차에 의해 달성된 점수의 평균 및 표준 편차. 비교를 위해 기본 모델을 미세 조정하는 데 사용된 동일한 수의 단계에 대해 처음부터 각 작업에 대해 훈련할 때(즉, 사전 훈련 없이) 성능도 보고한다. 이 표의 모든 점수(표 14를 제외한 논문의 모든 표)는 각 데이터 세트의 검증 세트에 대해 보고된다.

사전 훈련에서 얻는 이득은 미미한 경향이 있는 충분한 데이터 세트이다. 고자원 체제에서 전이 학습의 행동을 테스트하기 위해 이 작업을 실험에 포함한다. 가장 성능이 좋은 체크포인트를 선택하여 조기 정지를 수행하기 때문에 기준선과 "사전 훈련 없음" 사이의 큰 차이는 사전 훈련이 제한된 데이터를 가진 작업에서 성능을 얼마나 향상시키는지 강조한다. 본 논문에서는 데이터 효율성의 향상을 명시적으로 측정하지는 않지만, 이것이 전이 학습 패러다임의 주요 이점 중 하나임을 강조한다.

런 간 분산의 경우 대부분의 작업에서 런 간의 표준 편차가 작업 기준 점수의 1%보다 작음을 발견했습니다. 이 규칙에 대한 예외는 GLUE 및 SuperGLUE 벤치마크의 모든 저자원 작업인 CoLA, CB 및 COPA를 포함한다. 예를 들어, CB에서 기준 모델은 표준 편차가 3.237인 평균 F1 점수가 91.22였으며(표 16 참조), 이는 부분적으로 CB의 검증 세트가 56개의 예만 포함한다는 사실 때문일 수 있다. GLUE 및 SuperGLUE 점수는 각각의 벤치마크를 포함하는 태스크에 걸친 점수의 평균으로서 계산된다는 점에 유의한다. 결과적으로 CoLA, CB 및 COPA의 높은 실행 간 분산이 GLUE 및 SuperGLUE 점수만을 사용하여 모델을 비교하는 것을 어렵게 만들 수 있음을 주의한다.

### Architectures

트랜스포머는 원래 인코더-디코더 구조로 도입되었지만, NLP를 위한 전이 학습에 대한 많은 현대 작업은 대체 아키텍처를 사용한다. 이 섹션에서는 이러한 아키텍처 변형을 검토하고 비교한다.

#### 3.2.1 모델 구조

다른 아키텍처에 대한 주요 구별 요소는 모델에서 다른 주의 메커니즘에 의해 사용되는 "마스크"이다. 트랜스포머에서의 자기 주의 동작은 시퀀스를 입력으로 하고 동일한 길이의 새로운 시퀀스를 출력한다는 것을 기억하라. 출력 시퀀스의 각각의 엔트리는 입력 시퀀스의 엔트리들의 가중 평균을 계산함으로써 생성된다. 구체적으로, \(y_{i}\)은 출력 시퀀스의 \(i\)번째 엘리먼트를 지칭하고, \(x_{j}\)은 입력 시퀀스의 \(j\)번째 엔트리를 지칭하도록 한다. \ (y_{i}\)은 \(\sum_{j}w_{i,j}x_{j}\)으로 계산되며, 여기서 \(w_{i,j}\)은 \(x_{i}\)과 \(x_{j}\)의 함수로서 자기 주의 메커니즘에 의해 생성되는 스칼라 가중치이다. 이어서, 어텐션 마스크는 주어진 출력 타임스테프에서 입력의 어떤 엔트리들이 주목될 수 있는지를 제한하기 위해 특정 가중치들을 제로 아웃하는 데 사용된다. 우리가 고려할 마스크의 다이어그램은 그림 3에 나와 있다. 예를 들어 인과 마스크(그림 3, 중간)는 \(j>i\)이면 임의의 \(w_{i,j}\)을 0으로 설정한다.

첫 번째 모델 구조는 입력 시퀀스를 공급하는 인코더와 새로운 출력 시퀀스를 생성하는 디코더의 두 개의 레이어 스택으로 구성된 인코더-디코더 트랜스포머이다. 이 아키텍처 변형의 개략도는 그림 4의 왼쪽 패널에 나와 있다.

인코더는 "완전히 보이는" 주의력 마스크를 사용합니다. 완전-가시성 마스킹은 자기-주의 메커니즘이 그 출력의 각각의 엔트리를 생성할 때 입력의 임의의 엔트리에 주의를 기울일 수 있게 한다. 우리는 그림 3에서 이 마스킹 패턴을 시각화했다. 이러한 형태의 마스킹은 "접두사", 즉 예측을 할 때 나중에 사용되는 모델에 제공된 일부 컨텍스트에 걸쳐 참석할 때 적절하다. BERT(Devlin 등, 2018)는 또한 완전-가시 마스킹 패턴을 사용하고 입력에 특별한 "분류" 토큰을 부가한다. 이어서, 분류 토큰에 대응하는 타임스테프에서의BERT의 출력은 입력 시퀀스를 분류하기 위한 예측을 행하는데 사용된다.

트랜스포머의 디코더에서의 셀프-어텐션 동작들은 "인과적" 마스킹 패턴을 사용한다. 출력 시퀀스의 \(i\)번째 엔트리를 생성할 때 인과 마스킹은 모델이 \(j>i\)에 대한 입력 시퀀스의 \(j\)번째 엔트리에 참석하는 것을 방지한다. 이는 모델이 출력을 생성하므로 "미래를 내다볼 수 없도록 훈련 중에 사용됩니다. 이 마스킹 패턴에 대한 주의 매트릭스는 그림 3, 가운데에 나와 있다.

인코더-디코더 트랜스포머의 디코더는 출력 시퀀스를 자동으로 생성하기 위해 사용된다. 즉, 각각의 출력 타임스테프에서, 모델의 예측된 분포로부터 토큰이 샘플링되고 샘플은 다음 출력 타임스테프에 대한 예측을 생성하기 위해 모델 내로 피드백되는 등등이다. 이와 같이, (인코더가 없는) Transformer 디코더는 언어 모델(LM), 즉 오직 다음 단계 예측을 위해 훈련된 모델로서 사용될 수 있다(Liu et al., 2018; Radford et al., 2018; Al-Rfou et al., 2019). 이것은 우리가 고려하는 두 번째 모델 구조를 구성한다. 이 아키텍처의 개략도는 그림 4, 가운데에 나와 있다. 실제로, NLP에 대한 전이 학습에 대한 초기 작업은 사전 트레이닝 방법으로서 언어 모델링 목표와 함께 이 아키텍처를 사용했다(Radford et al., 2018).

언어 모델은 일반적으로 압축 또는 시퀀스 생성을 위해 사용된다(Graves, 2013). 그러나 단순히 입력과 대상을 연결하여 텍스트 대 텍스트 프레임워크에서도 사용할 수 있다. 예로서, 영어 대 독일어 번역의 경우를 고려하자: 입력 문장인 "그거 좋다."와 타겟 "Das ist gut"을 갖는 트레이닝 데이터포인트가 있는 경우, 우리는 단순히 연속된 입력 시퀀스 "영어를 독일어로 번역: 그거 좋다. 타겟: Das ist gut"에 대한 다음 단계 예측에 대한 모델을 트레이닝하고 싶을 때, "영어 대 독일어 번역: 그거 좋다. 타겟: Das ist gut"에 대한 다음 단계 예측에 대한 모델을 트레이닝한다.

도 3: 상이한 어텐션 마스크 패턴을 나타내는 행렬. 자기 주의 메커니즘의 입력과 출력은 각각 \(x\)과 \(y\)으로 표시된다. 행 \(i\) 및 열 \(j\)의 어두운 셀은 자체 주의 메커니즘이 출력 시간 \(i\)에서 입력 요소 \(j\)에 참석할 수 있음을 나타냅니다. 라이트 셀은 자체 주의 메커니즘이 해당 \(i\) 및 \(j\) 조합에 참석하도록 _not_ 허용됨을 나타냅니다. 왼쪽: 완전히 보이는 마스크는 자기-주의 메커니즘이 모든 출력 시간 단계에서 전체 입력에 참석할 수 있게 한다. 중간: 인과 마스크는 \(i\)번째 출력 요소가 "미래"로부터의 임의의 입력 요소에 의존하지 않도록 한다. 오른쪽: 프리픽스를 갖는 인과 마스킹은 자기-주의 메커니즘이 입력 시퀀스의 일부에 완전-가시 마스킹을 사용할 수 있게 한다.

이 예에 대한 모델의 예측을 획득하면, 모델은 프리픽스 "영어를 독일어로 번역: 그것은 좋다. 타겟:"을 공급받을 것이고, 시퀀스의 나머지를 자동으로 생성하도록 요청받을 것이다. 이러한 방식으로, 모델은 입력이 주어진 출력 시퀀스를 예측할 수 있으며, 이는 텍스트 대 텍스트 태스크의 요구를 만족시킨다. 이 접근법은 최근 언어 모델이 감독 없이 일부 텍스트 대 텍스트 태스크를 수행하는 것을 학습할 수 있음을 보여주기 위해 사용되었다(Radford et al., 2019).

텍스트-텍스트 설정에서 언어 모델을 사용하는 근본적이고 자주 인용되는 단점은 인과 마스킹이 입력 시퀀스의 \(i\)번째 엔트리에 대한 모델의 표현을 \(i\)까지의 엔트리에만 의존하게 한다는 것이다. 이것이 잠재적으로 불리한 이유를 알기 위해, 예측하도록 요청받기 전에 모델이 접두사/컨텍스트를 제공받는 텍스트 대 텍스트 프레임워크를 고려한다(예를 들어, 접두사는 영어 문장이고 모델은 독일어 번역을 예측하도록 요청받는다). 완전 인과 마스킹으로, 접두사 상태의 모델의 표현은 접두사의 이전 엔트리들에만 의존할 수 있다. 따라서, 출력의 엔트리를 예측할 때, 모델은 불필요하게 제한된 접두사의 표현을 다룰 것이다. 시퀀스-투-시퀀스 모델(Bahdanau et al., 2015)에서 단방향 순환 신경망 인코더를 사용하는 것에 대해 유사한 주장이 제기되었다.

도 4: 우리가 고려하는 트랜스포머 아키텍처 변형의 개략도. 이 다이어그램에서 블록은 시퀀스의 요소를 나타내고 선은 주의 가시성을 나타낸다. 블록들의 상이한 컬러 그룹들은 상이한 트랜스포머 층 스택들을 나타낸다. 어두운 회색 선은 완전히 보이는 마스킹에 해당하고 밝은 회색 선은 인과적 마스킹에 해당한다. 우리는 예측의 끝을 나타내는 특별한 종말 토큰을 나타내기 위해 "."을 사용한다. 입력 및 출력 시퀀스는 각각 \(x\) 및 \(y\)으로 표현된다. 왼쪽: 표준 인코더-디코더 아키텍처는 인코더에서의 완전-가시 마스킹 및 인코더-디코더 주의력을 사용하고, 디코더에서의 인과 마스킹을 사용한다. 중간: 언어 모델은 단일 트랜스포머 층 스택으로 구성되며, 전체적으로 인과 마스크를 사용하여 입력과 타겟의 연접을 공급받는다. 오른쪽: 언어 모델에 접두사를 추가하는 것은 입력에 대해 완전히 보이는 마스킹을 허용하는 것에 해당한다.

이 문제는 단순히 마스킹 패턴을 변경하기만 하면 트랜스포머 기반 언어 모델에서 피할 수 있다. 인과적 마스크를 사용하는 대신 시퀀스의 접두사 부분 동안 완전히 보이는 마스킹을 사용한다. 이러한 마스킹 패턴 및 결과적인 "접두사 LM"(우리가 고려하는 세 번째 모델 구조)의 개략도는 각각 도 3 및 도 4의 최우측 패널에 예시되어 있다. 위에서 언급된 영어 대 독일어 번역 예에서, 완전-가시 마스킹은 프리픽스 "번역 영어 대 독일어: That is good. target:"에 적용될 것이고, 인과 마스킹은 타겟 "Das ist gut"을 예측하기 위한 트레이닝 동안 사용될 것이다. 텍스트 대 텍스트 프레임워크에서 프리픽스 LM을 사용하는 것은 원래 Liu 등(2018)에 의해 제안되었다. 보다 최근에, Dong et al.(2019)은 이 아키텍처가 매우 다양한 텍스트-대-텍스트 태스크에 효과적이라는 것을 보여주었다. 이 아키텍처는 인코더 및 디코더에 걸쳐 공유된 파라미터들을 갖는 그리고 인코더-디코더 어텐션이 입력 및 타겟 시퀀스에 걸쳐 완전한 어텐션으로 대체된 인코더-디코더 모델과 유사하다.

우리는 우리의 텍스트-대-텍스트 프레임워크를 따를 때, 접두사 LM 아키텍처가 분류 태스크들에 대한 BERT(Devlin 등, 2018)와 매우 유사하다는 점에 주목한다. 그 이유를 알아보기 위해 전제가 "비둘기를 싫어합니다."라는 MNLI 벤치마크의 예를 들어, 가설은 "비둘기에 대한 내 감정은 적대감으로 가득 차 있다."이고 올바른 라벨은 "탈취"이다. 이 예를 언어 모델로 바꾸기 위해, 우리는 이것을 "mnli premise: 나는 비둘기를 싫어합니다. 가설: 비둘기에 대한 나의 감정은 적대감으로 가득 차 있습니다. 목표: 수반"이라는 시퀀스로 바꿀 것입니다. 이 경우, 완전-가시 프리픽스는 단어 "target:"까지의 전체 입력 시퀀스에 대응할 것이며, 이는 BERT에서 사용되는 "분류" 토큰과 유사하다고 볼 수 있다. 그래서, 우리의 모델은 전체 입력에 대해 완전한 가시성을 갖게 될 것이고, 그리고 나서 "우체국"이라는 단어를 출력함으로써 분류를 하는 임무를 받게 될 것입니다. 태스크 프리픽스(이 경우 "mnli")가 주어진 유효한 클래스 라벨들 중 하나를 출력하도록 모델이 학습하기 쉽다. 이와 같이, 프리픽스 LM과 BERT 아키텍처 사이의 주요 차이점은 분류기가 프리픽스 LM 내의 트랜스포머 디코더의 출력 계층에 단순히 통합된다는 것이다.

#### 3.2.2 다른 모델 구조 비교

이러한 아키텍처 변형을 실험적으로 비교하는 것에 관심을 두고, 우리는 우리가 고려하는 각 모델이 어떤 의미 있는 방식으로 동등하기를 원한다. 우리는 주어진 (입력-시퀀스, 목표-시퀀스) 쌍을 처리하기 위해 동일한 수의 매개 변수를 갖거나 대략 동일한 양의 계산이 필요한 경우 두 모델이 동일하다고 말할 수 있다. 불행하게도, 인코더-디코더 모델을 이 두 기준에 따른 언어 모델 아키텍처(단일 트랜스포머 스택을 포함함)와 동시에 비교하는 것은 가능하지 않다. 이유를 알아보기 위해, 먼저 인코더에 \(L\)개의 레이어가 있는 인코더-디코더 모델과 디코더에 \(L\)개의 레이어가 있는 언어 모델은 \(2L\)개의 레이어가 있는 언어 모델과 대략 동일한 수의 파라미터를 갖는다. 그러나, 동일한 \(L+L\) 인코더-디코더 모델은 _only_\(L\) 계층을 갖는 언어 모델과 대략 동일한 계산 비용을 가질 것이다. 이는 언어 모델 내의 \(L\) 계층이 입력 및 출력 시퀀스 _both_에 적용되어야 하는 반면, 인코더는 입력 시퀀스에만 적용되고 디코더는 출력 시퀀스에만 적용된다는 사실의 결과이다. 이러한 등가물들은 근사적이라는 점에 유의한다 - 인코더-디코더 어텐션으로 인해 디코더에 일부 여분의 파라미터들이 있고 또한 시퀀스 길이들에서 이차적인 어텐션 계층들에 일부 계산 비용들이 있다. 그러나 실제로 우리는 \(L\)-계층 언어 모델과 \(L+L\)-계층 인코더-디코더 모델에 대해 거의 동일한 단계 시간을 관찰하여 대략 동등한 계산 비용을 제안한다. 또한, 우리가 고려하는 모델 크기에 대해, 인코더-디코더 어텐션 계층의 파라미터 수는 전체 파라미터 수의 약 10%이므로, \(L+L\)-계층 인코더-디코더 모델이 \(2L\)-계층 언어 모델과 동일한 파라미터 수를 갖는다는 단순화 가정을 한다.

합리적인 비교 방법을 제공하기 위해, 우리는 인코더-디코더 모델에 대한 다중 구성을 고려한다. 우리는 BERTBASE 크기의 레이어 스택에서 레이어의 수와 파라미터를 각각 \(L\)과 \(P\)으로 지칭할 것이다. 우리는 주어진 입력-목표 쌍을 처리하기 위해 \(L+L\)-계층 인코더-디코더 모델 또는 \(L\)-계층 디코더 전용 모델에 필요한 FLOP의 수를 참조하기 위해 \(M\)을 사용할 것이다. 총합하여 비교하기로 한다:

* 인코더에 \(L\) 계층이 있고 디코더에 \(L\) 계층이 있는 인코더-디코더 모델입니다. 이 모델에는 \(2P\) 매개 변수와 \(M\) FLOP의 계산 비용이 있습니다.
* 동등한 모델이지만 인코더 및 디코더 간에 매개 변수가 공유되어 \(P\) 매개 변수와 \(M\)-FLOP 계산 비용이 발생합니다.
* 인코더 및 디코더에 각각 \(L/2\) 계층이 있는 인코더-디코더 모델로 \(P\) 매개 변수와 \(M/2\)-FLOP 비용을 제공합니다.
* \(L\) 계층 및 \(P\) 매개 변수와 \(M\) FLOP의 결과 계산 비용이 있는 디코더 전용 언어 모델입니다.
* 동일한 아키텍처(따라서 동일한 수의 파라미터 및 계산 비용)를 갖는 디코더 전용 프리픽스 LM이지만, 입력에 대해 완전히 가시적인 셀프-어텐션을 갖는다.

#### 3.2.3 Objectives

감독되지 않은 목적으로서, 기본 언어 모델링 목적뿐만 아니라 섹션 3.1.4에 기술된 우리의 베이스라인 디노이징 목적도 모두 고려할 것이다. 사전 트레이닝 목적으로서 그 역사적인 사용으로 인한 언어 모델링 목적(Dai and Le, 2015; Ramachandran et al., 2016; Howard and Ruder, 2018; Radford et al., 2018; Peters et al., 2018) 뿐만 아니라 우리가 고려하는 언어 모델 아키텍처에 대한 그것의 자연적인 적합도도 포함한다. 예측을 하기 전에 접두사를 수집하는 모델(인코더-디코더 모델 및 접두사 LM)의 경우 레이블이 지정되지 않은 데이터 세트에서 텍스트 범위를 샘플링하고 임의의 점을 선택하여 접두사와 대상 부분으로 분할한다. 표준 언어 모델의 경우 처음부터 끝까지 전체 범위를 예측하도록 모델을 훈련합니다. 우리의 감독되지 않은 노이즈 제거 목표는 텍스트 대 텍스트 모델을 위해 설계되었으며, 섹션 3.2.1에 설명된 대로 입력과 대상을 연결한 언어 모델과 함께 사용하도록 조정한다.

#### 3.2.4 Results

각 아키텍처에 의해 달성된 점수는 표 2에 나와 있다. 모든 작업에 대해 디노이징 목적을 가진 인코더-디코더 아키텍처가 가장 잘 수행되었다. 이 변형은 가장 높은 파라미터 카운트(\(2P\))를 갖지만, \(P\)-파라미터 디코더 전용 모델과 동일한 계산 비용을 갖는다. 놀랍게도, 우리는 인코더와 디코더에 걸쳐 파라미터들을 공유하는 것이 거의 또한 수행된다는 것을 발견했다. 대조적으로, 인코더 및 디코더 스택 내의 층들의 수를 절반으로 줄이는 것은 성능을 상당히 손상시킨다. 동시 작업(Lan et al., 2019)은 또한 트랜스포머 블록들에 걸쳐 파라미터들을 공유하는 것이 많은 성능을 희생시키지 않으면서 총 파라미터 카운트를 낮추는 효과적인 수단이 될 수 있다는 것을 발견했다. XLNet은 또한 잡음 제거 목적을 갖는 공유 인코더-디코더 접근법과 약간의 유사성을 갖는다(Yang et al., 2019). 또한, 공유 파라미터 인코더-디코더가 디코더 전용 프리픽스 LM보다 우수한 것을 주목하여, 명시적 인코더-디코더 주의의 추가가 유익함을 시사한다. 마지막으로, 잡음 제거 목표를 사용하는 것이 언어 모델링 목표에 비해 항상 더 나은 다운스트림 작업 성능을 가져온다는 널리 알려진 개념을 확인한다. 이러한 관찰은 이전에 Devlin 등(2018), Voita 등(2019), 및 Lample and Conneau(2019) 등에 의해 이루어졌다. 우리는 다음 섹션에서 감독되지 않은 목표에 대한 보다 자세한 탐색을 수행한다.

### Unsupervised Objectives

감독되지 않은 목표의 선택은 모델이 다운스트림 작업에 적용할 범용 지식을 얻는 메커니즘을 제공하기 때문에 중심적으로 중요하다. 이는 매우 다양한 사전 훈련 목표의 개발로 이어졌다(Dai and Le, 2015; Ramachandran et al., 2016; Radford et al., 2018; Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Wang et al., 2019; Song et al., 2019; Dong et al., 2019; Joshi et al., 2019). 이 섹션에서는 감독되지 않은 목표의 공간에 대한 절차적 탐구를 수행한다. 많은 경우에 우리는 기존의 목표를 정확하게 복제하지 않을 것이다. 일부는 텍스트 대 텍스트 인코더-디코더 프레임워크에 맞게 수정되고 다른 경우에는 여러 일반적인 접근법의 개념을 결합하는 목표를 사용할 것이다.

전반적으로, 우리의 모든 목표는 레이블이 지정되지 않은 텍스트 데이터 세트에서 토큰화된 텍스트 범위에 해당하는 토큰 ID의 시퀀스를 수집한다. 토큰 시퀀스는 (파손된) 입력 시퀀스 및 대응하는 타겟을 생성하도록 처리된다. 그리고, 모델은 평소와 같이 트레이닝된다

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline Architecture & Objective & Params & Cost & GLUE & CNNDM & SQuAD & SGLUE & EnDe & EnFr & EnRo \\ \hline \hline \multirow{2}{*}{Encoder-decoder} & Denoising & \(2P\) & \(M\) & **83.28** & **19.24** & **80.88** & **71.36** & **26.98** & **39.82** & **27.65** \\ Enc-dec, shared & Denoising & \(P\) & \(M\) & 82.81 & 18.78 & **80.63** & **70.73** & 26.72 & 39.03 & **27.46** \\ Enc-dec, 6 layers & Denoising & \(P\) & \(M/2\) & 80.88 & 18.97 & 77.59 & 68.42 & 26.38 & 38.40 & 26.95 \\ Language model & Denoising & \(P\) & \(M\) & 74.70 & 17.93 & 61.14 & 55.02 & 25.09 & 35.28 & 25.86 \\ Prefix LM & Denoising & \(P\) & \(M\) & 81.82 & 18.61 & 78.94 & 68.11 & 26.43 & 37.98 & 27.39 \\ \hline \hline \multirow{2}{*}{Encoder-decoder} & LM & \(2P\) & \(M\) & 79.56 & 18.59 & 76.02 & 64.29 & 26.27 & 39.17 & 26.86 \\ Enc-dec, shared & LM & \(P\) & \(M\) & 79.60 & 18.13 & 76.35 & 63.50 & 26.62 & 39.17 & 27.05 \\ Enc-dec, 6 layers & LM & \(P\) & \(M/2\) & 78.67 & 18.26 & 75.32 & 64.06 & 26.13 & 38.42 & 26.89 \\ Language model & LM & \(P\) & \(M\) & 73.78 & 17.54 & 53.81 & 56.51 & 25.23 & 34.31 & 25.38 \\ Prefix LM & LM & \(P\) & \(M\) & 79.68 & 17.84 & 76.87 & 64.86 & 26.28 & 37.51 & 26.76 \\ \hline \hline \end{tabular}
\end{table}
표 2: 섹션 3.2.2에 기술된 상이한 아키텍처 변형들의 성능. 우리는 \(P\)을 사용하여 12-계층 베이스 트랜스포머 계층 스택 내의 파라미터들의 수를 참조하고 \(M\)을 사용하여 인코더-디코더 모델을 사용하여 시퀀스를 프로세싱하는 데 필요한 FLOP들을 참조한다. 우리는 잡음 제거 목적(섹션 3.1.4에 설명됨)과 자기 회귀 목적(언어 모델을 훈련하는 데 일반적으로 사용되는 것처럼)을 사용하여 각 아키텍처 변형을 평가한다.

대상 시퀀스를 예측할 수 있는 최대 가능성이 있습니다. 우리는 표 3에서 고려하는 많은 목표에 대한 예시적인 예를 제공한다.

#### 3.3.1 상위 수준 접근 구분

먼저 일반적으로 사용되는 목표에서 영감을 얻었지만 접근 방식이 크게 다른 세 가지 기술을 비교한다. 먼저, 섹션 3.2.3에서 사용된 바와 같은 기본적인 "접두어 언어 모델링" 목적을 포함한다. 이 기술은 텍스트의 스팬을 인코더에 대한 입력으로서 사용하기 위한 것과 디코더에 의해 예측될 타겟 시퀀스로 사용하기 위한 것의 두 컴포넌트로 분할한다. 둘째, BERT(Devlin et al., 2018)에서 사용되는 "masked language modeling"(MLM) objective에서 영감을 받은 objective를 고려한다. MLM은 텍스트의 범위를 사용 하 여 토큰의 15%를 손상 합니다. 손상된 토큰의 90%는 특수 마스크 토큰으로 대체되고 10%는 랜덤 토큰으로 대체된다. BERT는 인코더 전용 모델이기 때문에, 사전-트레이닝 동안의 그것의 목표는 인코더의 출력에서 마스킹된 토큰들을 재구성하는 것이다. 인코더-디코더의 경우, 우리는 단순히 전체 무중단 시퀀스를 타겟으로 사용한다. 이것은 손상된 토큰만을 타겟으로 사용하는 우리의 베이스라인 목표와 다르다는 점에 유의한다; 섹션 3.3.2에서 이 두 접근법을 비교한다. 마지막으로, 디노이징 순차 오토인코더에 적용된 (Liu 등, 2019)에서와 같이 사용되는 기본 디셔플링 목표를 또한 고려한다. 이 접근법은 토큰의 시퀀스를 취하고, 그것을 셔플링한 다음, 원래의 셔플링된 시퀀스를 타겟으로 사용한다. 우리는 표 3의 처음 세 행에 이 세 가지 방법에 대한 입력 및 목표의 예를 제공한다.

이 세 가지 목표의 성능은 표 4에 나와 있다. 전반적으로 BERT 스타일 목표는 번역 작업에서 접두어 언어 모델링 목표가 유사한 성능을 얻지만 가장 잘 수행한다는 것을 알 수 있다. 실제로, BERT 목표에 대한 동기는 언어 모델 기반 사전 훈련을 능가하는 것이었다. 디셔플링 목표는 프리픽스 언어 모델링과 BERT 스타일 목표보다 상당히 더 나쁜 성능을 보인다.

\begin{table}
\begin{tabular}{l l l} \hline \hline Objective & Inputs & Targets \\ \hline Prefix language modeling & Thank you for inviting & me to your party last week. \\ BERT-style Devlin et al. (2018) & Thank you \textless{}\textless{}\textgreater{}\textgreater{}me to your party apple week. & _(original text)_ \\ Deshuffling & party me for your to. last fun you inviting week Thank & _(original text)_ \\ MASS-style Song et al. (2019) & Thank you \textless{}\textgreater{}\textgreater{}\textgreater{}\textgreater{}me to your party \textless{}\textgreater{}\textgreater{}\textgreater{}\textgreater{}. & _(original text)_ \\ I.i.d. noise, replace spans & Thank you \textless{}\textgreater{}\textgreater{}\textgreater{}me to your party \textless{}\textgreater{}\textgreater{}\textgreater{}. & \textless{}\textgreater{}\textgreater{}\textgreater{}\textgreater{} \\ I.i.d. noise, drop tokens & Thank you me to your party week. & for inviting last \\ Random spans & Thank you \textless{}\textgreater{}\textgreater{}\textgreater{}\textgreater{}\textgreater{}. & \textless{}\textgreater{}\textgreater{}\textgreater{}\textgreater{} \\ \hline \hline \end{tabular}
\end{table}
표 3: 우리가 고려하는 일부 감독되지 않은 목표에 의해 생성된 입력 및 목표의 예는 입력 텍스트 "지난 주에 당신의 파티에 초대해 주셔서 감사합니다"에 적용되었습니다. 우리의 모든 목표는 _토큰화_ 텍스트를 처리한다는 점에 유의하십시오. 이 특정 문장의 경우, 모든 단어는 우리의 어휘에 의해 하나의 토큰으로 매핑되었습니다. 모델이 전체 입력 텍스트를 재구성하는 작업을 수행한다는 것을 나타내기 위해 _(원본 텍스트)_를 대상으로 씁니다. \ textless{}\textgreater{}\textgreater{}는 공유된 마스크 토큰을 나타내며, \textless{}\textgreater{}\textgreater{}\textgreater{}\textgreater{}\textgreater{} 및 \textless{}\textgreater{}\textgreater{}\textgreater{}는 고유한 토큰 ID가 할당된 센티널 토큰을 나타냅니다. BERT-스타일 목적(두 번째 행)에는 일부 토큰이 랜덤 토큰 ID로 대체되는 손상이 포함되며, 이를 회색으로 표시된 단어 사과를 통해 보여준다.

#### 3.3.2 BERT Objective 단순화

앞 절의 결과를 바탕으로 이제 BERT 스타일의 노이즈 제거 목표에 대한 수정을 탐구하는 데 중점을 둘 것이다. 이 목적은 원래 분류 및 스팬 예측을 위해 훈련된 인코더 전용 모델에 대한 사전 훈련 기술로 제안되었다. 따라서 인코더-디코더 텍스트 대 텍스트 설정에서 더 잘 수행되거나 더 효율적이도록 수정하는 것이 가능할 수 있습니다.

먼저, 랜덤 토큰 스와핑 단계를 포함하지 않는 BERT-스타일 목표의 간단한 변형을 고려한다. 결과적인 목표는 단순히 입력 내의 토큰의 15%를 마스크 토큰으로 대체하고 모델은 원래의 중단되지 않은 시퀀스를 재구성하도록 트레이닝된다. 유사한 마스킹 대물렌즈가 Song 등(2019)에 의해 사용되었고, 여기서 "MASS"로 지칭되었으므로, 우리는 이 변형을 "MASS-스타일" 대물렌즈라고 부른다. 둘째, 디코더의 긴 시퀀스에 대한 자체 주의가 필요하기 때문에 전체 중단되지 않은 텍스트 범위를 예측하는 것을 피할 수 있는지 여부에 관심이 있었다. 이를 달성하기 위한 두 가지 전략을 고려한다 : 첫째, 각각의 손상된 토큰을 마스크 토큰으로 대체하는 대신에, 손상된 토큰의 각각의 연속적인 스팬의 전체를 고유한 마스크 토큰으로 대체한다. 그러면, 타겟 시퀀스는 "부러진" 스팬들의 연접이 되고, 각각은 입력에서 그것을 대체하는 데 사용되는 마스크 토큰에 의해 접두어진다. 이것은 섹션 3.1.4에 설명된 우리의 베이스라인에서 사용하는 사전 트레이닝 목표이다. 둘째, 우리는 입력 시퀀스로부터 손상된 토큰들을 완전히 드롭하고, 드롭된 토큰들을 순서대로 재구성하면서 모델을 태스크하는 변형을 또한 고려한다. 이러한 접근법의 예는 표 3의 다섯 번째 및 여섯 번째 행에 나와 있다.

이 세 가지 대안에 대한 원래 BERT 스타일 목표를 경험적으로 비교하면 표 5와 같다. 우리는 우리의 설정에서 이러한 모든 변형이 유사하게 수행한다는 것을 발견했다. 유일한 예외는 손상된 토큰을 떨어뜨리는 것이 CoLA(60.04)에서 우리보다 훨씬 높은 점수 덕분에 GLUE 점수에서 약간 개선된 것으로 나타났다.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Objective & GLUE & CNNDM & SQuAD & SGLUE & EnDe & EnFr & EnRo \\ \hline Prefix language modeling & 80.69 & 18.94 & 77.99 & 65.27 & **26.86** & 39.73 & **27.49** \\ BERT-style (Devlin et al., 2018) & **82.96** & **19.17** & **80.65** & **69.85** & **26.78** & **40.03** & **27.41** \\ Deshuffling & 73.17 & 18.59 & 67.61 & 58.47 & 26.11 & 39.30 & 25.62 \\ \hline \hline \end{tabular}
\end{table}
표 4: 섹션 3.3.1에 설명된 세 가지 이질적인 사전 훈련 목표의 성능.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline Objective & GLUE & CNNDM & SQuAD & SGLUE & EnDe & EnFr & EnRo \\ \hline BERT-style (Devlin et al., 2018) & 82.96 & 19.17 & **80.65** & 69.85 & 26.78 & **40.03** & 27.41 \\ MASS-style (Song et al., 2019) & 82.32 & 19.16 & 80.10 & 69.28 & 26.79 & **39.89** & 27.55 \\ \(\bigstar\)Replace corrupted spans & 83.28 & **19.24** & **80.88** & **71.36** & **26.98** & 39.82 & **27.65** \\ Drop corrupted tokens & **84.44** & **19.31** & **80.52** & 68.67 & **27.07** & 39.76 & **27.82** \\ \hline \hline \end{tabular}
\end{table}
표 5: BERT-스타일 사전-훈련 목표의 변형들의 비교. 처음 두 변형에서, 모델은 원래의 중단되지 않은 텍스트 세그먼트를 재구성하도록 트레이닝된다. 후자의 2에서 모델은 손상된 토큰의 시퀀스만을 예측한다.

기준 평균 53.84, 표 16 참조). 이것은 CoLA가 주어진 문장이 문법적으로 그리고 구문적으로 수용 가능한지를 분류하는 것을 포함하고, 토큰이 누락된 때를 결정할 수 있다는 것이 수용 가능성을 검출하는 것과 밀접한 관련이 있기 때문일 수 있다. 그러나 토큰을 떨어뜨리는 것은 슈퍼GLUE에서 센티넬 토큰으로 대체하는 것보다 완전히 더 나쁜 성능을 보였다. 전체 원본 시퀀스를 예측할 필요가 없는 두 가지 변형("손상된 스팬 대체" 및 "손상된 스팬 드롭")은 모두 대상 시퀀스를 더 짧게 만들고 결과적으로 훈련을 더 빠르게 만들기 때문에 잠재적으로 매력적이다. 앞으로 손상 된 스팬을 센티널 토큰으로 대체 하 고 손상 된 토큰만 예측 하는 변형을 탐색 합니다 (기준 목표).

#### 3.3.3 부패율 변경

지금까지, 우리는 BERT(Devlin 등, 2018)에서 사용된 값인 토큰의 15%를 손상시키고 있었다. 다시 말하지만, 우리의 텍스트 대 텍스트 프레임워크는 BERT와 다르기 때문에, 우리는 다른 부패율이 우리에게 더 잘 작동하는지 보는 것에 관심이 있다. <표 6>에서 10%, 15%, 25%, 50%의 부패율을 비교한다. 전반적으로 부패율이 모형의 성과에 제한적인 영향을 미쳤음을 알 수 있다. 유일한 예외는 우리가 고려하는 가장 큰 부패율(50%)이 GLUE 및 SQuAD에서 상당한 성능 저하를 초래한다는 것이다. 더 큰 부패율을 사용하는 것은 또한 더 긴 타겟들을 초래하고, 이는 잠재적으로 훈련을 늦출 수 있다. 이러한 결과와 BERT가 설정한 역사적 판례를 바탕으로 앞으로 15%의 부패율을 사용할 것이다.

#### 3.3.4 Corrupting Spans

우리는 이제 더 짧은 목표들을 예측함으로써 훈련 속도를 높이는 목표를 향해 돌아간다. 지금까지 우리가 사용한 접근법은 각 입력 토큰에 대해 손상 여부에 대한 ID 결정을 내린다. 여러 개의 연속 토큰이 손상 된 경우 "스팬"으로 처리 되 고 단일 고유 마스크 토큰을 사용 하 여 전체 스팬을 대체 합니다. 전체 스팬을 단일 토큰으로 교체하면 레이블이 지정되지 않은 텍스트 데이터가 더 짧은 시퀀스로 처리된다. i.i.d. corruption 전략을 사용하고 있기 때문에 상당수의 corruption된 토큰이 연속적으로 나타나는 경우가 항상 있는 것은 아니다. 결과적으로, 우리는 개별 토큰을 아이디 방식으로 손상시키는 것보다 토큰의 범위를 구체적으로 손상시킴으로써 추가적인 속도를 얻을 수 있다. 부패 스팬은 또한 이전에 BERT에 대한 사전 트레이닝 목표로 고려되었으며, 여기서 성능이 향상되는 것으로 밝혀졌다(Joshi et al., 2019).

이 아이디어를 테스트하기 위해 우리는 연속적이고 랜덤하게 배치된 토큰의 범위를 특별히 손상시키는 목표를 고려한다. 이 목표는 손상 될 토큰의 비율과 손상 된 총 스팬 수에 의해 매개 변수화될 수 있습니다. 그런 다음 스팬 길이를 선택합니다.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Corruption rate & GLUE & CNNDM & SQuAD & SGLUE & EnDe & EnFr & EnRo \\ \hline
10\% & **82.82** & 19.00 & **80.38** & 69.55 & **26.87** & 39.28 & **27.44** \\\(\bigstar\) 15\% & **83.28** & 19.24 & **80.88** & **71.36 & **26.98** & **39.82** & **27.65** \\\
25\% & **83.00** & **19.54** & **80.96** & 70.48 & **27.04** & **39.83** & **27.47** \\
50\% & 81.27 & 19.32 & 79.80 & 70.33 & **27.01** & **39.90** & **27.49** \\ \hline \hline \end{tabular}
\end{table}
표 6: 상이한 부패율을 갖는 아이디 부패 목표의 성능.

임의로 지정된 매개변수를 충족합니다. 예를 들어 500 토큰의 시퀀스를 처리 하는 경우 토큰의 15%가 손상 되어야 하며 총 스팬이 25 개여야 한다고 지정 하면 손상 된 토큰의 총 수는 \(500\times 0.15=75\)이고 평균 스팬 길이는 \(75/25=3\)입니다. 원래 시퀀스 길이와 손상률을 감안할 때 평균 스팬 길이 또는 총 스팬 수로 이 목표를 동등하게 매개변수화할 수 있다.

우리는 표 7의 스팬-부패 목표와 i.i.d-부패 목표를 비교한다. 우리는 모든 경우에 15%의 부패율을 사용하고 2, 3, 5 및 10의 평균 스팬 길이를 사용하여 비교한다. 다시 말하지만, 평균 스팬 길이가 10인 버전은 일부 경우에 다른 값을 약간 과소 수행한다. 또한 평균 스팬 길이 3을 사용하는 것이 대부분의 비번역 벤치마크에서 i.i.d. 목표보다 약간(그러나 상당히) 더 우수함을 발견했다. 다행히도 스팬 부패 목표는 스팬 부패가 평균적으로 더 짧은 시퀀스를 생성하기 때문에 ID 노이즈 접근법과 비교하여 훈련 중에 약간의 속도 향상을 제공한다.

#### 3.3.5 Discussion

그림 5는 감독되지 않은 목표를 탐색하는 동안 이루어진 선택의 흐름도를 보여준다. 전반적으로, 우리가 관찰한 성능에서 가장 중요한 차이점은 디노이징 목표가 사전 훈련을 위한 언어 모델링 및 디서플링을 능가한다는 것이다. 우리는 우리가 탐구한 노이즈 제거 목표의 많은 변형에서 놀라운 차이를 관찰하지 못했다. 그러나, 상이한 목적들(또는 목적들의 파라미터화들)은 상이한 시퀀스 길이들로 이어질 수 있고, 따라서 상이한 트레이닝 속도들로 이어질 수 있다. 이는 우리가 여기서 고려했던 잡음 제거 목적들 중에서 선택하는 것이 주로 계산 비용에 따라 이루어져야 함을 의미한다. 우리의 결과는 또한 여기에서 고려하는 목표와 유사한 목표에 대한 추가 탐색이 우리가 고려하는 작업 및 모델에 대한 상당한 이익으로 이어지지 않을 수 있음을 시사한다. 대신, 레이블이 지정되지 않은 데이터를 활용하는 완전히 다른 방법을 탐색하는 것은 우연일 수 있다.

### 사전 학습 데이터 집합

비지도 목표와 마찬가지로 사전 훈련 데이터 세트 자체는 전이 학습 파이프라인의 중요한 구성 요소이다. 그러나, 목표 및 벤치마크와 달리, 새로운 사전 트레이닝 데이터 세트는 보통 자체적으로 상당한 기여로 취급되지 않고 종종 그렇지 않다.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline Span length & GLUE & CNNDM & SQuAD & SGLUE & EnDe & EnFr & EnRo \\ \hline \(\bigstar\) Baseline (i.i.d.) & **83.28** & 19.24 & 80.88 & 71.36 & **26.98** & **39.82** & **27.65** \\
2 & **83.54** & 19.39 & **82.09** & **72.20** & **26.76** & **39.99** & **27.63** \\
3 & **83.49** & **19.62** & **81.84** & **72.53** & **26.86** & 39.65 & **27.62** \\
5 & **83.40** & 19.24 & **82.05** & **72.23** & **26.88** & 39.40 & **27.53** \\
10 & 82.85 & 19.33 & **81.84** & 70.44 & **26.79** & 39.49 & **27.69** \\ \hline \hline \end{tabular}
\end{table}
표 7: 상이한 평균 스팬 길이에 대한 스팬-부패 목적(조시 등(2019)에 의해 영감을 받은)의 성능. 모든 경우에 원본 텍스트 시퀀스의 15%를 손상시킵니다.

사전 훈련된 모델 및 코드와 함께 릴리스되었습니다. 대신 새로운 방법이나 모델을 제시하는 과정에서 전형적으로 도입된다. 그 결과, 사전-훈련에 사용되는 "표준" 데이터 세트의 부족뿐만 아니라 상이한 사전-훈련 데이터 세트의 비교가 상대적으로 거의 없었다. 몇몇 최근의 주목할만한 예외들(Baevski 등, 2019; Liu 등, 2019; Yang 등, 2019)은 새로운 큰(종종 커먼 크롤-소싱된) 데이터 세트에 대한 사전 트레이닝을 더 작은 기존 데이터 세트(종종 위키피디아)를 사용하는 것과 비교했다. 사전 훈련 데이터 세트가 성능에 미치는 영향을 더 깊이 조사하기 위해 이 섹션에서는 C4 데이터 세트의 변형과 사전 훈련 데이터의 다른 잠재적 소스를 비교한다. TensorFlow Datasets.11의 일부로 간주 하는 모든 C4 데이터 세트 변형을 릴리스 합니다.

각주 11: [https://www.tensorflow.org/datasets/catalog/c4](https://www.tensorflow.org/datasets/catalog/c4)

#### 3.4.1 Unlabeled Data Sets

C4를 생성할 때, 우리는 Common Crawl에서 웹-추출된 텍스트를 필터링하기 위해 다양한 휴리스틱을 개발했다(설명은 섹션 2.2 참조). 우리는 이 필터링이 다른 필터링 접근법 및 일반적인 사전 훈련 데이터 세트와 비교하는 것 외에도 다운스트림 작업에서 향상된 성능을 초래하는지 여부를 측정하는 데 관심이 있다. 이를 위해 다음 데이터 세트에 대한 사전 훈련 후 기준 모델의 성능을 비교한다.

**C4**: 기준선으로서 먼저 섹션 2.2에 설명된 대로 제안된 레이블이 지정되지 않은 데이터 세트에 대한 사전 교육을 고려합니다.
**필터링되지 않은 C4**: C4를 만드는 데 사용한 경험적 필터링(중복 제거, 나쁜 단어 제거, 문장 유지 등)의 효과를 측정하기 위해 이 필터링을 포기하는 C4의 대체 버전도 생성합니다. 우리는 여전히 langdetect를 사용합니다.

도 5: 감독되지 않은 목표들에 대한 우리의 탐색의 흐름도. 우리는 먼저 섹션 3.3.1에서 몇 가지 다른 접근법을 고려하고 BERT 스타일의 노이즈 제거 목표가 가장 잘 수행된다는 것을 발견한다. 그런 다음, 섹션 3.3.2에서 더 짧은 타겟 시퀀스를 생성하도록 BERT 목표를 단순화하기 위한 다양한 방법을 고려한다. 드롭아웃 스팬을 센티넬 토큰으로 대체하는 것이 잘 수행되고 짧은 타겟 시퀀스를 생성한다는 점을 고려하여 섹션 3.3.3에서 서로 다른 손상률을 실험한다. 마지막으로, 우리는 섹션 3.3.4에서 토큰의 연속적인 범위를 의도적으로 손상시키는 목표를 평가한다.

영어 텍스트를 추출합니다. 결과적으로, 우리의 "필터링되지 않은" 변형은 여전히 약간의 필터링을 포함하는데, 그 이유는 랑디텍트가 때때로 비자연적 영어 텍스트에 낮은 확률을 할당하기 때문이다.
**RealNews-like**: 최근 작업은 뉴스 웹 사이트에서 추출한 텍스트 데이터를 사용했습니다 (Zellers et al., 2019; Baevski et al., 2019). 이러한 접근법과 비교하기 위해, "RealNews" 데이터 세트에 사용된 도메인 중 하나로부터의 콘텐츠만을 포함하도록 C4를 추가로 필터링함으로써 라벨링되지 않은 또 다른 데이터 세트를 생성한다(Zellers 등, 2019). 비교의 용이성을 위해 우리는 C4에서 사용된 휴리스틱 필터링 방법을 유지하며, 유일한 차이점은 표면적으로 뉴스가 아닌 내용을 생략했다는 것이다.
**WebText-like**: 마찬가지로 WebText 데이터 세트(Radford 등, 2019)는 콘텐츠 집계 웹사이트 Reddit에 제출되고 최소 3의 "점수"를 받은 웹페이지의 콘텐츠만 사용합니다. Reddit에 제출되는 웹페이지에 대한 점수는 웹페이지를 지지(상향 투표)하거나 반대(하향 투표)하는 사용자의 비율에 따라 계산됩니다. 레딧 점수를 품질 신호로 사용하는 이면의 아이디어는 사이트의 사용자가 고품질 텍스트 콘텐츠에만 투표한다는 것이다. 비교 가능한 데이터 세트를 생성하기 위해 먼저 OpenWebText 노력으로 준비된 목록에 나타난 URL에서 유래하지 않은 C4에서 모든 콘텐츠를 제거하려고 시도했다.12 그러나 이는 대부분의 페이지가 Reddit에 나타나지 않기 때문에 상대적으로 적은 콘텐츠(약 2GB)를 초래했다. C4는 단일 월의 공통 크롤 데이터를 기반으로 생성되었음을 상기하라. 따라서 엄청나게 작은 데이터 세트를 사용하지 않기 위해 2018년 8월부터 2019년 7월까지 커먼 크롤에서 12개월간의 데이터를 다운로드하고 C4에 대한 휴리스틱 필터링을 적용한 다음 레딧 필터를 적용했다. 이것은 17GB WebText-like 데이터 세트를 생성했는데, 이는 원래의 40GB WebText 데이터 세트(Radford et al., 2019)와 비슷한 크기이다.
**위키피디아**: 웹 사이트 위키피디아는 협력적으로 작성 된 수백만 개의 백과사전 기사로 구성 됩니다. 사이트의 콘텐츠는 엄격한 품질 지침을 적용하므로 깨끗하고 자연스러운 텍스트의 신뢰할 수 있는 출처로 사용되었다. 우리는 문서에서 마크업이나 참조 섹션을 생략하는 텐서플로우 데이터 세트(13)의 영어 위키피디아 텍스트 데이터를 사용한다.

각주 12: [https://github.com/jcpeterson/openwebtext](https://github.com/jcpeterson/openwebtext)

각주 13: [https://www.tensorflow.org/datasets/catalog/wikipedia](https://www.tensorflow.org/datasets/catalog/wikipedia)

**위키피디아 + 토론토 책 코퍼스**: 위키피디아에서 사전 훈련 데이터를 사용하는 단점은 자연 텍스트(엔시클로피디아 기사)의 가능한 도메인 하나만 나타낸다는 것입니다. 이를 완화하기 위해, BERT(Devlin et al., 2018)는 위키피디아로부터의 데이터를 토론토 북스 코퍼스(TBC)와 결합시켰다(Zhu et al., 2015). TBC는 전자책에서 추출한 텍스트를 포함하고 있으며, 이는 자연어의 다른 영역을 나타낸다. BERT의 인기는 위키피디아+TBC 조합이 후속 작품에서 많이 쓰이게 만들었다.

이들 데이터 세트 각각에 대한 사전-훈련 후에 달성된 결과는 표 8에 도시되어 있다. 첫 번째 명백한 테이크아웃은 C4로부터 휴리스틱 필터링을 제거함으로써 성능이 균일하게 저하되고 필터링되지 않은 변이체가 모든 태스크에서 최악의 성능을 수행하게 한다는 것이다. 이 외에도 일부 경우에 더 제한된 도메인을 가진 사전 훈련 데이터 세트가 다양한 C4 데이터 세트를 능가한다는 것을 발견했다. 예를 들어, 위키피디아 + TBC 코퍼스를 사용하면 73.24의 SuperGLUE 점수가 생성되어 기준(C4 사용)의 점수 71.36을 능가했다. 이는 거의 전적으로 MultiRC에 대한 정확한 매치 점수에서 25.78(기준, C4)에서 50.93(위키피디아 + TBC 사용)까지의 성능 증가에 기인한다(표 16 참조). 멀티RC는 TBC가 다루는 영역인 소설책에서 가장 큰 데이터 출처가 나오는 읽기 이해 데이터 세트이다. 마찬가지로 사전 훈련을 위한 RealNews 유사 데이터 세트를 사용하면 뉴스 기사에 대한 읽기 이해도를 측정하는 데이터 세트인 ReCoRD에 대한 정확한 일치 점수에서 68.16에서 73.72로 증가했다. 마지막 예로 위키피디아의 데이터를 사용하면 위키피디아에서 가져온 구절이 있는 질문 응답 데이터 세트인 SQuAD에서 상당한(그러나 덜 극적인) 이득을 얻었다. 유사한 관찰이 선행 작업, 예를 들어 벨타기 등(2019)에서 연구 논문의 텍스트에 대한 사전 트레이닝 BERT가 과학 과제에 대한 성능을 향상시켰다는 것을 발견하였다. 이러한 발견의 주요 교훈은 _도메인 내 레이블이 지정되지 않은 데이터에 대한 사전 훈련_이 다운스트림 작업에 대한 성능을 향상시킬 수 있다는 것입니다. 이것은 놀랍지 않지만 우리의 목표가 임의의 도메인에서 언어 작업에 빠르게 적응할 수 있는 모델을 사전 훈련하는 것이라면 만족스럽지 않다. Liu 등(2019)은 또한 보다 다양한 데이터 세트에 대한 사전 훈련이 다운스트림 작업에 대한 개선을 산출하는 것을 관찰했다. 이 관찰은 또한 자연어 처리를 위한 도메인 적응에 대한 연구의 평행선에 동기를 부여하며, 이 분야의 조사는 예를 들어 루더(2019); Li(2012)를 참조한다.

단일 도메인에 대한 사전 트레이닝만의 단점은 결과 데이터 세트가 종종 실질적으로 더 작다는 것이다. 마찬가지로, 웹텍스트와 유사한 변형은 기본 설정에서 C4 데이터 세트보다 우수하거나 더 잘 수행되었지만 Reddit 기반 필터링은 Common Crawl의 \(12\times\) 더 많은 데이터를 기반으로 했음에도 불구하고 C4보다 약 \(40\times\) 작은 데이터 세트를 생성했다. 그러나 기본 설정에서는 \(2^{35}\approx 34\)B 토큰에 대해서만 사전 학습하며, 이는 우리가 고려하는 가장 작은 사전 학습 데이터 세트보다 약 8배 더 크다. 우리는 다음 절에서 더 작은 사전 훈련 데이터 세트를 사용하는 것이 어떤 시점에서 문제가 되는지 조사한다.

#### 3.4.2 Pre-training Data Set Size

우리가 C4를 만드는 데 사용하는 파이프라인은 매우 큰 사전 훈련 데이터 세트를 만들 수 있도록 설계되었다. 너무 많은 데이터에 대한 액세스를 통해 예제를 반복하지 않고도 모델을 사전 훈련할 수 있습니다. 우리의 사전 훈련 목표는 그 자체가 확률적이며 모델이 동일한 정확한 데이터를 여러 번 보는 것을 방지하는 데 도움이 될 수 있기 때문에 사전 훈련 중에 반복 사례가 다운스트림 성능에 도움이 되는지 해로운지는 분명하지 않다.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline Data set & Size & GLUE & CNNDM & SQuAD & SGLUE & EnDe & EnFr & EnRo \\ \hline \(\bigstar\) C4 & 745GB & 83.28 & **19.24** & 80.88 & 71.36 & **26.98** & **39.82** & **27.65** \\ C4, unfiltered & 6.1TB & 81.46 & 19.14 & 78.78 & 68.04 & 26.55 & 39.34 & 27.21 \\ RealNews-like & 35GB & **83.83** & **19.23** & 80.39 & 72.38 & **26.75** & **39.90** & **27.48** \\ WebText-like & 17GB & **84.03** & **19.31** & **81.42** & 71.40 & **26.80** & **39.74** & **27.59** \\ Wikipedia & 16GB & 81.85 & **19.31** & 81.29 & 68.01 & **26.94** & 39.69 & **27.67** \\ Wikipedia + TBC & 20GB & 83.65 & **19.28** & **82.08** & **73.24** & **26.77** & 39.63 & **27.57** \\ \hline \hline \end{tabular}
\end{table}
표 8: 상이한 데이터 세트에 대한 사전 트레이닝으로 인한 성능. 처음 네 가지 변형은 새로운 C4 데이터 세트를 기반으로 합니다.

제한된 레이블이 없는 데이터 집합 크기의 영향을 테스트하기 위해 인위적으로 잘린 C4 버전에 대한 기본 모델을 사전 훈련했다. \(2^{35}\approx 34\)B 토큰(전체 C4 크기의 작은 부분)에 대한 기본 모델을 사전 훈련했음을 상기하라. 우리는 \(2^{29}\), \(2^{27}\), \(2^{25}\) 및 \(2^{23}\) 토큰으로 구성된 C4의 절단된 변형에 대한 훈련을 고려한다. 이러한 크기들은 사전 트레이닝 과정 동안 데이터 세트(64, 256, 1,024, 및 4,096회)를 각각 반복하는 것에 대응한다.

결과적인 다운스트림 성능은 표 9에 나타나 있다. 예상대로, 데이터 세트 크기가 축소됨에 따라 성능이 저하된다. 우리는 이것이 모델이 사전 훈련 데이터 세트를 기억하기 시작한다는 사실 때문일 수 있다고 의심한다. 이것이 사실인지 측정하기 위해 그림 6에 이러한 데이터 세트 크기 각각에 대한 훈련 손실을 표시한다. 실제로 모델은 사전 훈련 데이터 세트의 크기가 축소됨에 따라 훈련 손실이 상당히 작아지므로 암기 가능성을 시사한다. Baevski 등(2019)은 사전-트레이닝 데이터 세트 크기를 절단하는 것이 다운스트림 태스크 성능을 저하시킬 수 있다는 것을 유사하게 관찰하였다.

이러한 효과는 사전 훈련 데이터 세트가 64회만 반복될 때 제한된다는 점에 유의한다. 이는 사전 훈련 데이터의 일부 반복이 해롭지 않을 수 있음을 시사한다. 그러나 추가 사전 훈련이 유익할 수 있고(섹션 3.6에 나와 있듯이) 추가 레이블이 지정되지 않은 데이터를 얻는 것이 저렴하고 쉽다는 점을 감안할 때 가능한 한 큰 사전 훈련 데이터 세트를 사용하는 것이 좋다. 또한 이 효과는 더 큰 모델 크기에 대해 더 두드러질 수 있으며, 즉 더 큰 모델은 더 작은 사전 훈련 데이터 세트에 과적합되기 쉬울 수 있다.

### Training Strategy

지금까지 우리는 모델의 모든 매개변수가 개별 감독 작업에서 미세 조정되기 전에 감독되지 않은 작업에서 사전 훈련되는 설정을 고려했다. 이 접근법은 간단하지만 다운스트림/감독 작업에서 모델을 훈련하기 위한 다양한 대체 방법이 제안되었다. 본 절에서는 여러 태스크에서 동시에 모델을 학습하는 방법과 함께 모델을 미세 조정하기 위한 다른 방법을 비교한다.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline Number of tokens & Repeats & GLUE & CNNDM & SQuAD & SGLUE & EnDe & EnFr & EnRo \\ \hline \hline \(\bigstar\) Full data set & 0 & **83.28** & **19.24** & **80.88** & **71.36** & **26.98** & **39.82** & **27.65** \\ \(2^{29}\) & 64 & **82.87** & **19.19** & **80.97** & **72.03** & **26.83** & **39.74** & **27.63** \\ \(2^{27}\) & 256 & 82.62 & **19.20** & 79.78 & 69.97 & **27.02** & **39.71** & 27.33 \\ \(2^{25}\) & 1,024 & 79.55 & 18.57 & 76.27 & 64.76 & 26.38 & 39.56 & 26.80 \\ \(2^{23}\) & 4,096 & 76.34 & 18.33 & 70.92 & 59.29 & 26.37 & 38.84 & 25.81 \\ \hline \hline \end{tabular}
\end{table}
표 9: 사전 트레이닝 동안 데이터를 반복하는 효과를 측정한다. 이 실험에서는 C4(첫 번째 열에 표시된 \(N\) 값이 다양한) 토큰의 첫 번째 \(N\) 토큰만 사용하지만 여전히 \(2^{35}\) 토큰에 대해 사전 훈련한다. 이는 데이터 세트가 사전 훈련 과정(두 번째 열에 표시된 각 실험에 대한 반복 횟수와 함께) 동안 반복되는 결과를 초래하며, 이는 암기를 초래할 수 있다(도 6 참조).

#### 3.5.1 Fine-tuning Methods

모델의 모든 파라미터를 미세 조정하면, 특히 저자원 태스크(Peters et al., 2019)에서 차선의 결과를 초래할 수 있다고 주장되어 왔다. 텍스트 분류 태스크들에 대한 전이 학습에 대한 초기 결과들은 고정된 사전-훈련된 모델에 의해 생성된 문장 임베딩들을 공급받은 작은 분류기의 파라미터들만을 미세 조정하는 것을 옹호하였다(Subramanian et al., 2018; Kiros et al., 2015; Logeswaran and Lee, 2018; Hill et al., 2016; Conneau et al., 2017). 이 방법은 주어진 작업에 대해 전체 디코더가 목표 시퀀스를 출력하도록 훈련되어야 하기 때문에 인코더-디코더 모델에 덜 적용할 수 있다. 대신, 우리는 인코더-디코더 모델의 파라미터의 부분 집합만 업데이트하는 두 가지 대안적인 미세 조정 접근법에 초점을 맞춘다.

첫 번째, "어댑터 층"(Houlsby et al., 2019; Bapna et al., 2019)은 미세 조정 동안 대부분의 원래 모델을 고정 상태로 유지하는 목표에 의해 동기 부여된다. 어댑터 레이어는 트랜스포머의 각 블록에서 기존 피드포워드 네트워크 각각에 추가된 추가 조밀한 ReLU 밀도 블록입니다. 이러한 새로운 피드포워드 네트워크는 출력 차원이 입력과 일치하도록 설계되었다. 이를 통해 구조나 매개변수에 대한 추가 변경 없이 네트워크에 삽입할 수 있습니다. 미세 조정 시 어댑터 계층 및 계층 정규화 매개 변수만 업데이트됩니다. 이 접근법의 주요 하이퍼파라미터는 모델에 추가된 새로운 파라미터의 수를 변경하는 피드포워드 네트워크의 내부 차원 \(d\)이다. 우리는 \(d\)에 대한 다양한 값을 실험한다.

우리가 고려하는 두 번째 대안적인 미세 조정 방법은 "점진적 언결빙"(Howard and Ruder, 2018)이다. 점진적인 동결 해제에서는 시간이 지남에 따라 더 많은 모델의 매개변수가 미세 조정된다. 점진적 언결빙은 원래 단일 층 스택으로 구성된 언어 모델 아키텍처에 적용되었다. 이 설정에서는, 미세 조정 개시시에, 단지,

그림 6: 원래 C4 데이터 세트와 인위적으로 잘린 4개의 버전에 대한 사전 훈련 손실. 나열된 크기는 각 데이터 집합의 토큰 수를 나타냅니다. 고려된 4개의 크기는 사전 훈련 과정에서 64에서 4,096번 사이의 데이터 세트를 반복하는 것에 해당한다. 더 작은 데이터 세트 크기를 사용하는 것은 더 작은 트레이닝 손실 값들을 초래하며, 이는 라벨링되지 않은 데이터 세트의 일부 암기를 제안할 수 있다.

최종 계층의 파라미터들이 업데이트되고, 그 후, 특정 개수의 업데이트들에 대한 트레이닝 후에, 전체 네트워크의 파라미터들이 미세 조정될 때까지, 두 번째 내지 마지막 계층의 파라미터들도 또한 포함된다. 이 방법을 인코더-디코더 모델에 적용하기 위해 두 경우 모두 위에서 시작하여 인코더와 디코더의 층을 점진적으로 병렬로 부동화한다. 입력 임베딩 행렬과 출력 분류 행렬의 파라미터가 공유되므로 미세 조정 전반에 걸쳐 업데이트한다. 우리의 기본 모델은 인코더와 디코더에 각각 12개의 레이어로 구성되고 \(2^{18}\) 단계에 대해 미세 조정된다는 것을 기억하라. 따라서 미세조정 과정을 \({}^{218}/_{12}\) 단계의 12개의 에피소드로 세분하고 \(n\) 번째 에피소드에서 \(12-n\)에서 12로 훈련한다. 우리는 하워드와 루더(2018)가 훈련의 각 시대 후에 추가 계층을 미세 조정하는 것을 제안했다는 점에 주목한다. 그러나 지도 데이터 세트는 크기가 매우 다양하고, 다운스트림 태스크 중 일부는 실제로 많은 태스크(GLUE와 SuperGLUE)의 혼합물이기 때문에, 우리는 대신 매 \({}^{218}/_{12}\) 단계 후에 추가 레이어를 미세 조정하는 더 간단한 전략을 채택한다.

이러한 미세 조정 접근법의 성능을 비교하면 표 10과 같다. 어댑터 계층의 경우 32, 128, 512, 2048의 내부 차원 \(d\)을 사용하여 성능을 보고한다. 과거 결과(Houlsby et al., 2019; Bapna et al., 2019)를 통해 SQuAD와 같은 하위 자원 태스크는 \(d\)의 작은 값으로 잘 작동하는 반면 상위 자원 태스크는 합리적인 성능을 얻기 위해 큰 차원을 필요로 한다는 것을 알 수 있다. 이는 어댑터 계층이 작업 크기에 맞게 차원을 적절하게 조정하는 한 더 적은 매개변수에 대한 미세 조정을 위한 유망한 기술이 될 수 있음을 시사한다. 우리의 경우 GLUE와 SuperGLUE를 각각 구성 데이터 세트를 연결하여 단일 "작업"으로 처리하므로 일부 저자원 데이터 세트를 구성하지만 결합된 데이터 세트는 \(d\)의 큰 값을 필요로 할 만큼 충분히 크다. 우리는 점진적인 동결 해제가 미세 조정 동안 약간의 속도 향상을 제공했지만 모든 작업에서 약간의 성능 저하를 유발한다는 것을 발견했다. 동결되지 않은 일정을 보다 신중하게 조정함으로써 더 나은 결과를 얻을 수 있다.

#### 3.5.2 Multi-task Learning

지금까지 우리는 각 다운스트림 태스크에서 개별적으로 미세 조정하기 전에 단일 비지도 학습 태스크에 대해 모델을 사전 훈련했다. "멀티-태스크 학습"(Ruder, 2017; Caruana, 1997)이라고 불리는 대안적 접근법은 한 번에 여러 태스크에 대해 모델을 훈련시키는 것이다. 이 접근법은 전형적으로 동시에 할 수 있는 단일 모델을 트레이닝하는 목표를 갖는다

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline Fine-tuning method & GLUE & CNNDM & SQuAD & SGLUE & EnDe & EnFr & EnRo \\ \hline \hline All parameters & **83.28** & **19.24** & **80.88** & **71.36** & **26.98** & **39.82** & **27.65** \\ Adapter layers, \(d=32\) & 80.52 & 15.08 & 79.32 & 60.40 & 13.84 & 17.88 & 15.54 \\ Adapter layers, \(d=128\) & 81.51 & 16.62 & 79.47 & 63.03 & 19.83 & 27.50 & 22.63 \\ Adapter layers, \(d=512\) & 81.54 & 17.78 & 79.18 & 64.30 & 23.45 & 33.98 & 25.81 \\ Adapter layers, \(d=2048\) & 81.51 & 16.62 & 79.47 & 63.03 & 19.83 & 27.50 & 22.63 \\ Gradual unfreezing & 82.50 & 18.95 & 79.17 & **70.79** & 26.71 & 39.02 & 26.93 \\ \hline \hline \end{tabular}
\end{table}
표 10: 모형의 매개변수 부분 집합만 업데이트하는 다른 대체 미세 조정 방법의 비교입니다. 어댑터 레이어의 경우 \(d\)은 어댑터의 내부 차원을 나타냅니다.

한 번에 많은 작업을 수행합니다. 즉, 모델과 대부분의 매개변수는 모든 작업에 공유됩니다. 이 목표를 다소 완화하고 대신 각 개별 작업에서 잘 수행하는 개별 매개변수 설정을 생성하기 위해 여러 작업에 대한 교육 방법을 한 번에 조사한다. 예를 들어 여러 작업에 대해 단일 모델을 훈련할 수 있지만 성능을 보고하는 경우 각 작업에 대해 다른 검사점을 선택할 수 있습니다. 이것은 우리가 지금까지 고려했던 사전-훈련-이후-미세-조정 접근법에 비해 멀티-작업 학습 프레임워크를 느슨하게 하고 더 균일한 기초 위에 놓는다. 또한 통합 텍스트 대 텍스트 프레임워크에서 "다중 작업 학습"은 단순히 데이터 세트를 함께 혼합하는 것에 해당한다. 따라서 비지도 태스크를 함께 혼합되는 태스크 중 하나로 간주하여 다중 태스크 학습을 사용할 때 레이블이 지정되지 않은 데이터에 대해 여전히 훈련할 수 있다. 대조적으로, NLP에 대한 멀티-태스크 학습의 대부분의 애플리케이션들은 태스크-특정 분류 네트워크들을 추가하거나 각각의 태스크에 대해 상이한 손실 함수들을 사용한다(Liu et al., 2019).

Arivazhagan 등(2019)이 지적한 바와 같이, 멀티 태스크 학습에서 매우 중요한 요소는 모델이 학습해야 하는 각 태스크의 데이터가 얼마나 되는지이다. 우리의 목표는 모델을 과소 또는 과도하게 훈련하지 않는 것이다. 즉, 우리는 모델이 주어진 작업에서 작업을 잘 수행할 수 있는 충분한 데이터를 볼 수 있지만 훈련 세트를 기억할 만큼 많은 데이터를 볼 수 있기를 원하지 않는다. 각 작업에서 들어오는 데이터의 비율을 정확히 설정하는 방법은 데이터 세트 크기, 작업을 학습하는 "어려움"(즉, 모델이 작업을 효과적으로 수행하기 전에 얼마나 많은 데이터를 봐야 하는지), 정규화 등을 포함한 다양한 요인에 따라 달라질 수 있다. 추가 문제는 한 작업에서 좋은 성능을 달성하는 것이 다른 작업에서 성능을 방해할 수 있는 "작업 간섭" 또는 "부정적인 이전"의 가능성이다. 이러한 우려를 감안할 때 각 작업에서 나오는 데이터의 비율을 설정하기 위한 다양한 전략을 탐색하는 것으로 시작한다. 유사한 탐사가 Wang 등(2019)에 의해 수행되었다.

예제-비례 혼합 모델이 주어진 작업에 얼마나 빨리 오버핏 되는지에 대한 주요 요인은 작업의 데이터 세트 크기입니다. 이와 같이 혼합 비율을 설정하는 자연스러운 방법은 각 과제의 데이터 집합의 크기에 비례하여 표본을 추출하는 것이다. 이는 모든 작업에 대한 데이터 세트를 연결하고 결합된 데이터 세트로부터 예를 무작위로 샘플링하는 것과 동일하다. 그러나 우리는 다른 모든 작업보다 훨씬 큰 데이터 세트를 사용하는 감독되지 않은 노이즈 제거 작업을 포함하므로 각 데이터 세트의 크기에 비례하여 샘플링하기만 하면 모델이 보는 대부분의 데이터는 레이블이 지정되지 않고 감독된 모든 작업에 착수한다. 감독되지 않은 태스크가 없더라도, 일부 태스크(예를 들어, WMT 영어에서 프랑스어)는 너무 커서 유사하게 대부분의 배치를 밀어낼 것이다. 이 문제를 해결하기 위해 비율을 계산하기 전에 데이터 세트 크기에 대한 인위적인 "한계"를 설정했다. 구체적으로, 각 \(N\) 태스크의 데이터 집합에 있는 예제의 개수가 \(e_{n},n\in\{1,\ldots,N\}\)일 경우, 훈련 중 \(m\)번째 태스크부터 \(r_{m}=\min(e_{m},K)/\sum\min(e_{n},K)\)까지 예제 샘플링 확률을 설정한다. 여기서 \(K\)은 인공 데이터 집합 크기 제한이다.

온도 크기 혼합 데이터 세트 크기 간의 큰 격차를 완화하는 대안적인 방법은 혼합 속도의 "온도"를 조정하는 것이다. 이 방법은 다국어 BERT를 사용하여 모델이 저자원 언어에 대해 충분히 훈련되었는지 확인하기 위해 사용되었다. 14 온도 \(T\)를 사용하여 온도 스케일링을 구현하기 위해 각 작업의 혼합 비율 \(r_{m}\)을 \(\nicefrac{{1}}{{r}}\)의 거듭제곱으로 올리고 비율을 합하여 1로 재정규화한다. \(T=1\)일 때 이 접근법은 예제-비례 혼합과 동일하며 \(T\)이 증가함에 따라 비율이 동일한 혼합에 가까워진다. 우리는 데이터 세트의 크기 제한 \(K\)(온도 스케일링 전에 \(r_{m}\)을 얻기 위해 적용됨)을 유지하지만 \(K=2^{21}\)의 큰 값으로 설정한다. 온도를 높이면 가장 큰 데이터 세트의 혼합 속도가 감소하기 때문에 큰 값 \(K\)을 사용한다.
**동일한 혼합**: 이 경우 동일한 확률로 각 작업의 예제를 샘플링합니다. 구체적으로, 각 배치의 각 예는 우리가 훈련하는 데이터 세트 중 하나에서 무작위로 균일하게 샘플링된다. 이는 모델이 저자원 태스크에 빠르게 과적합되고 고자원 태스크에 과소적합되기 때문에 차선책 전략일 가능성이 크다. 우리는 주로 비율을 차선책으로 설정할 때 잘못될 수 있는 것에 대한 기준점으로 그것을 포함한다.

동일한 조건에서 이러한 혼합 전략을 기본 사전 훈련 후 미세 조정 결과와 비교하기 위해 동일한 총 단계 수에 대해 다중 작업 모델을 훈련한다. \(2^{19}+2^{18}=786,432\). 결과를 표 11에 나타낸다.

일반적으로 멀티태스크 교육은 대부분의 태스크에 대해 사전훈련을 수행한 후 미세조정을 수행하는 것을 알 수 있다. 특히 "균등" 혼합 전략은 성능이 극적으로 저하되는데, 이는 저자원 태스크가 과도하게 적합하거나, 고자원 태스크가 충분한 데이터를 보지 못하거나, 모델이 범용 언어 기능을 학습하기에 레이블이 지정되지 않은 데이터를 충분히 보지 못했기 때문일 수 있다. 예제-비례 혼합의 경우, 대부분의 작업에서 모델이 최상의 성능을 얻는 \(K\)에 대해 "스위트 스폿"이 있고, \(K\)의 더 크거나 작은 값은 더 나쁜 성능을 초래하는 경향이 있음을 발견했다. 예외(우리가 고려한 \(K\) 값의 범위에 대해)는 WMT 영어에서 프랑스어 번역으로, 이는 항상 더 높은 혼합 비율에서 이점을 얻을 만큼 자원이 풍부한 작업이다. 마지막으로, 온도 스케일 혼합은 대부분의 작업에서 합리적인 성능을 얻을 수 있는 수단을 제공하며 대부분의 경우 \(T=2\)이 가장 잘 수행된다는 점에 주목한다. 멀티-태스크 모델이 각각의 개별 태스크에 대해 트레이닝된 별개의 모델들에 의해 우수하다는 발견은, 예를 들어, Arivazhagan 등(2019) 및 McCann 등(2018)에 의해 관찰되었지만, 멀티-태스크 셋업은 매우 유사한 태스크들에 걸쳐 이점을 부여할 수 있다는 것이 Liu 등(2019); Ratner 등(2018)에 의해 관찰되었다. 다음 절에서는 다중 작업 훈련과 사전 훈련 후 미세 조정 접근법의 격차를 줄이는 방법을 탐구한다.

#### 3.5.3 Multi-Task Learning with Fine-Tuning 결합

여러 태스크의 혼합에 대해 단일 모델을 학습하지만 모델에 대한 서로 다른 매개변수 설정(체크포인트)을 사용하여 성능을 평가할 수 있는 완화된 버전의 다중 태스크 학습을 연구하고 있음을 상기하십시오. 우리는 모델이 모든 작업에 대해 한 번에 사전 훈련된 다음 개별 감독 작업에 대해 미세 조정된 경우를 고려하여 이 접근법을 확장할 수 있다. 이는 "MT-DNN"(Liu et al., 2015, 2019)에 의해 사용되는 방법으로서, GLUE 및 기타 벤치마크에서 도입되었을 때 최첨단의 성능을 달성하였다. 우리는 이 접근법의 세 가지 변형을 고려한다. 먼저, 각 개별 다운스트림 작업에 대해 미세 조정하기 전에 인공 데이터 세트 크기 한계 \(K=2^{19}\)를 갖는 예제-비례 혼합물에 대해 모델을 사전 훈련한다. 이는 사전 훈련 동안 감독되지 않은 목표와 함께 감독된 작업을 포함하는 것이 모델에 다운스트림 작업에 대한 유익한 조기 노출을 제공하는지 여부를 측정하는 데 도움이 된다. 우리는 또한 많은 감독 소스에서 혼합이 사전 훈련된 모델이 개별 작업에 적응되기 전에 더 일반적인 "기술"(느슨하게 말하는) 세트를 얻는 데 도움이 되기를 바랄 수 있다. 이를 직접 측정하기 위해 이 사전 훈련 혼합물에서 다운스트림 작업 중 하나를 생략하는 것을 제외하고 동일한 예제-비례 혼합물( \(K=2^{19}\))에서 모델을 사전 훈련하는 두 번째 변형을 고려한다. 그런 다음 사전 교육 중에 누락된 작업에 대한 모델을 미세 조정합니다. 우리는 우리가 고려하는 각각의 다운스트림 작업에 대해 이것을 반복한다. 우리는 이 접근법을 "탈원아웃" 다중 작업 교육이라고 부른다. 이것은 사전 훈련 동안 보지 못한 작업에 대해 사전 훈련된 모델이 미세 조정되는 실제 설정을 시뮬레이션한다. 멀티-태스크 사전-훈련은 감독된 태스크들의 다양한 혼합물을 제공한다는 점에 유의한다. 다른 분야들(예를 들어, 컴퓨터 비전(Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski et al., 2014))이 사전-훈련을 위해 감독된 데이터 세트를 사용하기 때문에, 우리는 다중-작업 사전-훈련 혼합물로부터 비감독된 작업을 생략하는 것이 여전히 양호한 결과들을 생성하는지 여부를 보는 것에 관심이 있었다. 따라서 세 번째 변형에 대해 우리는 \(K=2^{19}\)으로 고려되는 모든 감독 작업의 예제-비례 혼합에 대해 사전 훈련한다. 이 모든 변형에서 우리는 \(2^{18}\) 단계에 대한 미세 조정 전에 \(2^{19}\) 단계에 대한 사전 훈련의 표준 절차를 따른다.

표 12에서 이러한 접근법의 결과를 비교한다. 비교를 위해 예제-비례 혼합에 대한 기본(사전 훈련 후 미세 조정) 및 표준 다중 작업 학습(미세 조정 없음)에 대한 결과도 포함한다. \(K=2^{19}\). 다중 작업 사전 훈련 후 미세 조정이 기준선과 유사한 성능을 가져온다는 것을 발견했다. 이는 다중 작업 학습 후 미세 조정을 사용하면 섹션 3.5.2에 설명된 서로 다른 혼합 비율 간의 상충 관계를 완화하는 데 도움이 될 수 있음을 시사한다. 흥미롭게도 "leave-one-out" 훈련의 성능은 약간 더 나빴으며, 이는 다양한 작업에 대해 훈련된 모델이 여전히 새로운 작업에 적응할 수 있음을 시사한다(즉, 다중 작업 사전 훈련은 극적인 작업 간섭을 초래하지 않을 수 있음). 마지막으로, 감독된 다중 작업 사전 훈련은 번역 작업을 제외한 모든 경우에서 훨씬 더 나쁜 성능을 보였다. 이거.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline Mixing strategy & GLUE & CNNDM & SQuAD & SGLUE & EnDe & EnFr & EnRo \\ \hline \(\bigstar\) Baseline (pre-train/fine-tune) & **83.28** & **19.24** & **80.88** & **71.36** & **26.98** & **39.82** & **27.65** \\ Equal & 76.13 & 19.02 & 76.51 & 63.37 & 23.89 & 34.31 & 26.78 \\ Examples-proportional, \(K=2^{16}\) & 80.45 & 19.04 & 77.25 & 69.95 & 24.35 & 34.99 & 27.10 \\ Examples-proportional, \(K=2^{17}\) & 81.56 & 19.12 & 77.00 & 67.91 & 24.36 & 35.00 & 27.25 \\ Examples-proportional, \(K=2^{18}\) & 81.67 & 19.07 & 78.17 & 67.94 & 24.57 & 35.19 & 27.39 \\ Examples-proportional, \(K=2^{19}\) & 81.42 & **19.24** & 79.78 & 67.30 & 25.21 & 36.30 & **27.76** \\ Examples-proportional, \(K=2^{20}\) & 80.80 & **19.24** & **80.36** & 67.38 & 25.66 & 36.93 & **27.68** \\ Examples-proportional, \(K=2^{21}\) & 79.83 & 18.79 & 79.50 & 65.10 & 25.82 & 37.22 & 27.13 \\ Temperature-scaled, \(T=2\) & 81.90 & **19.28** & 79.42 & 69.92 & 25.42 & 36.72 & 27.20 \\ Temperature-scaled, \(T=4\) & 80.56 & **19.22** & 77.99 & 69.54 & 25.04 & 35.82 & 27.45 \\ Temperature-scaled, \(T=8\) & 77.21 & 19.10 & 77.14 & 66.07 & 24.55 & 35.35 & 27.17 \\ \hline \hline \end{tabular}
\end{table}
표 11: 서로 다른 혼합 전략을 사용하는 다중 작업 훈련의 비교. 예-비례 혼합은 각 데이터 세트의 총 크기에 따라 각 데이터 세트로부터 샘플을 샘플링하는 것을 말하며, 최대 데이터 세트 크기에 대한 인위적 한계(\(K\))를 갖는다. 온도 스케일링 혼합은 샘플링 속도를 온도 \(T\)만큼 재스케일링한다. 온도-스케일 혼합은 \(K=2^{21}\)의 인공 데이터 세트 크기 제한을 사용한다.

번역 작업은 (영어) 사전 훈련의 혜택을 덜 받는 반면, 감독되지 않은 사전 훈련은 다른 작업에서 중요한 요소임을 시사할 수 있다.

### Scaling

머신 러닝 연구의 "쓴 교훈"은 추가적인 계산을 레버리지할 수 있는 일반적인 방법들이 궁극적으로 인간의 전문지식에 의존하는 방법들에 대해 승리한다고 주장한다(Sutton, 2019; Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Shazeer et al., 2018, 2017; Huang et al., 2018; Keskar et al., 2019). 최근의 결과들은 이것이 NLP(Liu et al., 2019; Radford et al., 2019; Yang et al., 2019; Lan et al., 2019)에서의 전이 학습에 대해 사실일 수 있다는 것을 시사하며, 즉, 스케일 업은 보다 신중하게 설계된 방법들에 비해 향상된 성능을 생성한다는 것을 반복적으로 보여주었다. 그러나, 더 큰 모델을 사용하는 것, 더 많은 단계를 위해 모델을 트레이닝하는 것, 및 앙상블을 포함하는 다양한 가능한 스케일링 방법이 있다. 이 섹션에서는 다음과 같은 전제를 해결 하 여 이러한 다른 접근 방식을 비교 합니다. "\(4\times\) 더 많은 계산이 제공 되었습니다. 어떻게 사용 해야 합니까?"

기본 모델은 220M 매개변수를 가지며 \(2^{19}\) 단계와 \(2^{18}\) 단계에 대해 각각 사전 훈련 및 미세 조정된다. 인코더와 디코더의 크기는 모두 "BERT\({}_{\text{BASE}}\)"과 유사하다. 모델 크기 증가를 실험하기 위해 "BERT\({}_{\text{LARGE}}\)" Devlin et al. (2018)의 지침을 따르고 \(d_{\text{ff}}=4096\), \(d_{\text{model}}=1024\), \(d_{\text{kv}}=64\) 및 16-head attention mechanism을 사용한다. 그리고 인코더와 디코더에 각각 16개, 32개의 계층으로 구성된 두 개의 변형 모델을 생성하여 원래의 모델만큼의 파라미터가 \(2\times\)과 \(4\times\)인 모델을 생성한다. 이 두 가지 변형은 계산 비용도 대략 \(2\times\)과 \(4\times\)이다. 베이스라인과 이 두 개의 더 큰 모델을 사용하여, 우리는 계산량만큼의 \(4\times\)을 사용하는 세 가지 방법을 고려한다: \(4\times\)의 많은 단계에 대한 훈련, \(2\times\)의 더 큰 모델에 대한 훈련, 그리고 \(4\times\)의 더 큰 모델에 대한 훈련. 훈련 단계를 늘릴 때 단순화를 위해 사전 훈련 단계와 미세 조정 단계를 모두 확장한다. 사전 훈련 단계의 수를 늘릴 때, \(2^{23}\) 단계의 훈련에도 C4가 너무 커서 데이터를 한 번 통과하지 못하기 때문에 더 많은 사전 훈련 데이터를 효과적으로 포함할 수 있다.

모델이 \(4\times\)의 많은 데이터를 볼 수 있는 대안적인 방법은 배치 크기를 4배 증가시키는 것이다. 이는 잠재적으로 더 효율적인 병렬화로 인해 더 빠른 훈련을 초래할 수 있다. 그러나 배치 크기가 \(4\times\) 더 큰 훈련은 훈련과 다른 결과를 초래할 수 있다.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline Training strategy & GLUE & CNNDM & SQuAD & SGLUE & EnDe & EnFr & EnRo \\ \hline \(\bigstar\) Unsupervised pre-training + fine-tuning & **83.28** & **19.24** & **80.88** & **71.36** & **26.98** & 39.82 & 27.65 \\ Multi-task training & 81.42 & **19.24** & 79.78 & 67.30 & 25.21 & 36.30 & 27.76 \\ Multi-task pre-training + fine-tuning & **83.11** & **19.12** & **80.26** & **71.03** & **27.08** & 39.80 & **28.07** \\ Leave-one-out multi-task training & 81.98 & 19.05 & 79.97 & **71.68** & **26.93** & 39.79 & **27.87** \\ Supervised multi-task pre-training & 79.93 & 18.96 & 77.38 & 65.36 & 26.81 & **40.13** & **28.04** \\ \hline \hline \end{tabular}
\end{table}
표 12: 비지도 사전 훈련, 다중 작업 학습 및 다양한 형태의 다중 작업 사전 훈련의 비교.

(Shallue et al., 2018). 이 두 경우를 비교하기 위해 기준선 모델을 \(4\times\) 더 큰 배치 크기로 훈련하는 추가 실험을 포함한다.

우리가 고려하는 많은 벤치마크에서 모델의 앙상블을 사용하여 훈련하고 평가하여 추가 성능을 줄이는 것은 일반적인 관행이다. 이는 추가적인 계산을 사용하는 직교 방식을 제공한다. 다른 스케일링 방법과 앙상블을 비교하기 위해 미리 훈련된 모델과 세밀하게 조정된 모델 4개의 앙상블에 대한 성능도 측정한다. 우리는 집계 예측을 얻기 위해 출력 소프트맥스 비선형성에 공급하기 전에 앙상블 전체의 로짓들을 평균화한다. 4개의 개별 모델을 사전 훈련하는 대신, 더 저렴한 대안은 단일 사전 훈련된 모델을 취하고 4개의 개별 미세 조정된 버전을 생산하는 것이다. 이 방법은 전체 \(4\times\) 계산 예산을 사용하지 않지만 다른 스케일링 방법에 비해 경쟁적인 성능을 나타내는지 확인하기 위해 이 방법을 포함한다.

이러한 다양한 스케일링 방법을 적용한 후 달성된 성능은 표 13에 나와 있다. 놀랍게도 훈련 시간 및/또는 모델 크기를 늘리면 기준선이 일관되게 개선된다. (4\times\)의 많은 단계에 대한 훈련 또는 \(4\times\)의 더 큰 배치 크기를 사용하는 훈련 사이에는 명확한 승자가 없었지만 둘 다 유익했다. 일반적으로, 모델 크기를 증가시키는 것은 트레이닝 시간 또는 배치 크기만을 증가시키는 것에 비해 성능에서 추가적인 범프를 초래했다. 우리가 연구한 어떤 과제에서도 \(2\times\)에 대해 \(2\times\) 더 큰 모델을 훈련하는 것과 \(4\times\) 더 큰 모델을 훈련하는 것 사이에 큰 차이를 관찰하지 못했다. 이는 훈련 시간을 늘리고 모델 크기를 늘리는 것이 성능 향상의 보완 수단이 될 수 있음을 시사한다. 또한 앙상블은 스케일을 통해 성능을 향상시키는 직교적이고 효과적인 수단을 제공한다는 것을 시사한다. 일부 작업(CNN/DM, WMT 영어에서 독일어, WMT 영어에서 루마니아어)에서 완전히 개별적으로 훈련된 4개의 모델을 앙상블하는 것이 다른 모든 스케일링 접근법을 훨씬 능가했다. 사전 훈련되었지만 별도로 미세 조정된 모델을 조립하는 것도 기준선보다 상당한 성능 증가를 가져왔으며, 이는 성능 개선의 더 저렴한 수단을 시사한다. 유일한 예외는 수퍼GLUE였으며, 앙상블 접근법이 기준선보다 크게 개선되지 않았다.

우리는 서로 다른 스케일링 방법이 성능과 별개인 서로 다른 트레이드오프를 가지고 있다는 점에 주목한다. 예를 들어, 더 큰 모델을 사용하면 다운스트림 미세 조정을 할 수 있고,

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline Scaling strategy & GLUE & CNNDM & SQuAD & SGLUE & EnDe & EnFr & EnRo \\ \hline \hline Baseline & 83.28 & 19.24 & 80.88 & 71.36 & 26.98 & 39.82 & 27.65 \\ \(1\times\) size, \(4\times\) training steps & 85.33 & 19.33 & 82.45 & 74.72 & 27.08 & 40.66 & 27.93 \\ \(1\times\) size, \(4\times\) batch size & 84.60 & 19.42 & 82.52 & 74.64 & 27.07 & 40.60 & 27.84 \\ \(2\times\) size, \(2\times\) training steps & **86.18** & 19.66 & **84.18** & 77.18 & 27.52 & **41.03** & 28.19 \\ \(4\times\) size, \(1\times\) training steps & **85.91** & 19.73 & **83.86** & **78.04** & 27.47 & 40.71 & 28.10 \\ \(4\times\) ensembled & 84.77 & **20.10** & 83.09 & 71.74 & **28.05** & 40.53 & **28.57** \\ \(4\times\) ensembled, fine-tune only & 84.05 & 19.57 & 82.36 & 71.55 & 27.55 & 40.22 & 28.09 \\ \hline \hline \end{tabular}
\end{table}
표 13: 우리의 베이스라인 모델을 스케일링하는 상이한 방법들의 비교. 세밀하게 조정된 모델을 앙상블하는 것을 제외한 모든 방법은 계산을 기본값으로 \(4\times\) 사용한다. "크기"는 모델 내의 파라미터들의 수를 지칭하고, "트레이닝 시간"은 사전-트레이닝 및 미세-튜닝 둘 모두에 사용되는 단계들의 수를 지칭한다.

추론은 더 비싸다. 이와는 대조적으로, 더 긴 시간 동안 작은 모델을 사전 훈련하는 비용은 많은 다운스트림 작업에 적용되면 효과적으로 상각된다. 이와는 별도로, \(N\)개의 개별 모델을 조립하는 것은 \(N\times\)의 계산 비용이 더 높은 모델을 사용하는 것과 유사한 비용이 든다는 점에 주목한다. 결과적으로 스케일링 방법 중 하나를 선택할 때 모델의 최종 사용에 대한 몇 가지 고려가 중요하다.

### Putting It All Together

이제 체계적인 연구의 통찰력을 활용하여 인기 있는 NLP 벤치마크에서 성과를 어디까지 밀어낼 수 있는지 결정한다. 또한 대용량 데이터에 대해 더 큰 모델을 학습하여 NLP에 대한 이전 학습의 현재 한계를 탐색하는 데 관심이 있다. 기본 교육 접근 방식으로 시작 하 고 다음 사항을 변경 합니다.

**목적**: SpanBERT(Joshi 등, 2019)에서 느슨하게 영감을 받은 섹션 3.3.4에 설명된 스팬 부패 목표에 대해 기준선에서 i.i.d. 디노이징 목표를 바꿉니다. 특히, 평균 스팬 길이가 3이고 원래 시퀀스의 15%가 손상된다. 이 목표가 더 짧은 표적 서열 길이로 인해 약간 더 계산 효율적이면서 약간 더 나은 성능(표 7)을 생성한다는 것을 발견했다.
**더 긴 트레이닝**: 우리의 베이스라인 모델은 상대적으로 적은 양의 사전 트레이닝(\(\nicefrac{{1}}{{4}}\)을 BERT(Devlin et al., 2018), \(\nicefrac{{\nu_{16}}}{{4}}\)을 XLNet(Yang et al., 2019), \(\nicefrac{{\nu_{64}}}{{4}\)을 RoBERTa(Liu et al., 2019) 등만큼 사용한다. 다행히 C4는 데이터를 반복하지 않고도 실질적으로 더 오래 트레이닝할 수 있을 만큼 충분히 크다(이는 섹션 3.4.2에 도시된 바와 같이 해로울 수 있다). 섹션 3.6에서 추가 사전 훈련이 실제로 도움이 될 수 있으며 배치 크기를 늘리고 훈련 단계를 늘리는 것이 이 이점을 제공할 수 있음을 발견했다. 따라서 우리는 총 1조 개의 사전 훈련 토큰(기준선만큼 약 32\times\)에 해당하는 길이 512의 \(2^{11}\) 시퀀스의 배치 크기에 대해 100만 단계에 대해 모델을 사전 훈련한다. 섹션 3.4.1에서 RealNews 유사, WebText 유사 및 Wikipedia + TBC 데이터 세트에 대한 사전 교육이 몇 가지 다운스트림 작업에서 C4에 대한 사전 교육을 능가한다는 것을 보여주었다. 그러나, 이러한 데이터 세트 변형은 1조 토큰에 대한 사전 훈련 과정에 걸쳐 수백 번 반복될 정도로 충분히 작다. 섹션 3.4.2에서 이러한 반복이 해로울 수 있음을 보여주었기 때문에 대신 C4 데이터 세트를 계속 사용하도록 선택했다.
**모델 크기**: 섹션 3.6에서 기준 모델 크기를 확장하면 성능이 향상되는 방법도 보여 줍니다. 그러나, 더 작은 모델을 사용하는 것은 미세 조정 또는 추론을 위해 제한된 계산 자원을 사용할 수 있는 설정에서 도움이 될 수 있다. 이러한 요소를 기반으로 다양한 크기의 모델을 훈련합니다.

* **기본.** 이 모델은 하이퍼 매개 변수가 섹션 3.1.1에 설명되어 있는 기본 모델입니다. 약 2억 2천만 개의 매개 변수가 있습니다.
* **Small.** 인코더와 디코더에서 각각 6개의 계층과 \(d_{\text{model}}=512\), \(d_{\text{ff}}=2{,}048\), 8-headed attention를 사용하여 기준선을 축소하는 더 작은 모델을 고려합니다. 이 변형에는 약 6천만 개의 매개변수가 있습니다.

* **큰.** 기준선은 BERTBASE 크기의 인코더 및 디코더를 사용 하기 때문에 인코더 및 디코더가 크기 및 구조가 BERTLARGE와 유사한 변형도 고려 합니다. 구체적으로, 이 변형은 \(d_{\text{model}=\) 1,024, \(d_{\text{ff}}=\) 4,096, \(d_{\text{kv}}=\) 64, 16-헤드 어텐션 및 인코더 및 디코더에 각각 24개의 레이어를 사용하여 약 7억 7천만 개의 파라미터를 생성한다.
* **3B 및 11B.** 더 큰 모델을 사용할 때 어떤 종류의 성능이 가능한지 추가로 탐색하기 위해 두 가지 추가 변형을 고려합니다. 두 경우 모두 \(d_{\text{model}=\) 1024, 24 계층 부호화기와 복호화기, \(d_{\text{kv}}=\) 128을 사용한다. "3B" 변형의 경우 \(d_{\text{ff}}=\) 16,384와 32-headed attention를 사용하여 약 28억 개의 파라미터를 생성하며, "11B"의 경우 \(d_{\text{ff}}=\) 65,536과 128-headed attention를 사용하여 약 110억 개의 파라미터를 생성하는 모델을 사용한다. 특히, 트랜스포머의 피드포워드(feed-forward) 회로망과 같은 대규모 밀집 행렬 곱셈을 위해 기존의 TPU와 같은 최신 가속기가 가장 효율적이기 때문에 확장 \(d_{\text{ff}}\)을 선택했다.

**다중 작업 사전 훈련**: 섹션 3.5.3에서 미세 조정 전에 감독 되지 않은 작업과 감독 된 작업의 다중 작업 혼합물에 대 한 사전 훈련과 감독 되지 않은 작업 단독에 대 한 사전 훈련이 작동 하는 것을 보여 줍니다. 이는 "MT-DNN"(Liu et al., 2015, 2019b)에 의해 주창된 접근법이다. 또한 미세 조정 시에만 모니터링하는 것이 아니라 훈련 기간 전체에 대해 "다운스트림" 성능을 모니터링할 수 있다는 실질적인 이점이 있다. 따라서 우리는 최종 실험 세트에서 다중 작업 사전 훈련을 사용했다. 더 긴 기간 동안 훈련된 더 큰 모델이 더 작은 훈련 데이터 세트에 과도하게 적합할 가능성이 더 높기 때문에 레이블이 지정되지 않은 데이터의 더 큰 비율로부터 이익을 얻을 수 있다고 가정한다. 그러나 섹션 3.5.3의 결과는 다중 작업 사전 훈련 후 미세 조정이 레이블이 지정되지 않은 데이터의 차선의 비율을 선택하여 발생할 수 있는 몇 가지 문제를 완화할 수 있음을 시사한다. 이러한 아이디어를 기반으로 표준 예-비례 혼합(섹션 3.5.2에 설명됨)을 사용하기 전에 레이블이 지정되지 않은 데이터에 다음 인공 데이터 세트 크기를 대체합니다. Small은 710,000, Base는 2,620,000, Large는 8,660,000, 3B는 33,500,000, 11B는 133,000,000. 모든 모델 변형에 대해 사전 훈련 동안 WMT 영어는 프랑스어, WMT 영어는 독일어 데이터 세트의 유효 데이터 세트 크기를 1M 예제로 캡핑했다.
**개별 GLUE 및 SuperGLUE 작업에 대한 미세 조정**: 지금까지 GLUE 및 SuperGLUE를 미세 조정할 때 각 벤치마크의 모든 데이터 세트를 연결하여 GLUE에 대해 한 번만, SuperGLUE에 대해 한 번만 모델을 미세 조정합니다. 이 접근법은 우리의 연구를 논리적으로 단순하게 만들지만, 우리는 이것이 개별적으로 작업을 미세 조정하는 것과 비교하여 일부 작업에서 약간의 성능을 희생시킨다는 것을 발견했다. 개별 작업에 대한 미세 조정의 잠재적인 문제는 그렇지 않으면 모든 작업에 대한 교육을 통해 완화될 수 있으며, 이는 자원이 적은 작업에 빠르게 과잉 적합할 수 있다는 것이다. 예를 들어, \(2^{11}\) 길이-512 시퀀스의 큰 배치 크기는 많은 저자원 GLUE 및 SuperGLUE 작업에 대해 전체 데이터 세트가 각 배치에서 여러 번 나타난다. 따라서 각 GLUE 및 SuperGLUE 작업에 대한 미세 조정 동안 8개의 길이-512 시퀀스의 더 작은 배치 크기를 사용한다. 또한 모델의 매개변수가 적합하기 전에 액세스할 수 있도록 검사점을 5,000단계마다 저장하는 것이 아니라 1,000단계마다 저장합니다.

**빔 검색**: 이전 모든 결과는 greedy 디코딩을 사용하여 보고되었습니다. 출력 시퀀스가 긴 태스크의 경우, 빔 탐색을 사용하는 것으로부터 향상된 성능을 발견하였다(Sutskever et al., 2014). 구체적으로 WMT 변환 및 CNN/DM 요약 작업을 위해 빔 폭 4와 길이 패널티 \(\alpha=0.6\)(Wu et al., 2016)을 사용한다.
**테스트 세트**: 이것이 최종 실험 세트이므로 유효성 검사 세트가 아닌 테스트 세트에 대한 결과를 보고합니다. CNN/Daily Mail의 경우 데이터 세트와 함께 배포된 표준 테스트 세트를 사용한다. WMT 과제의 경우 영어-독일어의 경우 newstest2014, 영어-프랑스어의 경우 newstest2015, 영어-루마니아어의 경우 newstest2016을 사용하는 것에 해당한다. GLUE와 SuperGLUE의 경우 벤치마크 평가 서버를 사용하여 공식 테스트 세트 점수를 계산했다. 15\({}^{,}\)16 SQuAD의 경우 테스트 세트를 평가하려면 벤치마크 서버에서 추론을 실행해야 한다. 불행히도 이 서버의 계산 리소스는 가장 큰 모델에서 예측을 얻기에 충분하지 않습니다. 결과적으로 대신 SQuAD 유효성 검사 세트에 대한 성능을 계속 보고합니다. 다행히 SQuAD 테스트 세트에서 성능이 가장 높은 모델도 검증 세트에 대한 결과를 보고했기 때문에 표면적으로는 최첨단의 것과 비교할 수 있다.

각주 15: [http://gluebenchmark.com](http://gluebenchmark.com)

각주 16: [http://super.gluebenchmark.com](http://super.gluebenchmark.com)

위에서 언급된 그러한 변화와는 별개로, 우리는 우리의 기준선과 동일한 트레이닝 절차 및 하이퍼파라미터(AdaFactor optimizer, 사전-트레이닝을 위한 역 제곱근 학습 속도 스케줄, 미세 조정을 위한 일정한 학습 속도, 드롭아웃 정규화, 어휘 등)를 사용한다. 참고로, 이러한 세부 사항은 섹션 2에 설명되어 있다.

이 최종 실험 세트의 결과는 표 14에 나와 있다. 전반적으로 우리가 고려하는 24개의 작업 중 18개에서 최첨단 성능을 달성했다. 예상대로, 우리의 가장 큰(110억 매개변수) 모델은 모든 작업에 걸쳐 모델 크기 변형 중에서 가장 잘 수행했다. 우리의 T5-3B 모델 변형은 몇 가지 작업에서 이전 최신 기술을 능가했지만 모델 크기를 110억 매개변수로 확장하는 것이 최상의 성능을 달성하는 데 가장 중요한 요소였다. 이제 각 개별 벤치마크에 대한 결과를 분석합니다.

최첨단 평균 GLUE 점수 90.3을 달성하였다. 특히, 자연어 추론 작업 MNLI, RTE 및 WNLI에 대해 우리의 성능은 이전의 최첨단보다 훨씬 더 우수했다. RTE와 WNLI는 역사적으로 기계 성능이 인간의 성능에 비해 각각 93.6과 95.9로 뒤처진 두 가지 작업이다(Wang et al., 2018). 매개변수 수 측면에서 11B 모델 변형은 GLUE 벤치마크에 제출된 가장 큰 모델이다. 그러나, 대부분의 베스트-스코어링 제출은 예측을 생성하기 위해 많은 양의 앙상블 및 계산을 사용한다. 예를 들어, ALBERT의 가장 성능이 좋은 변형(Lan et al., 2019)은 크기 및 아키텍처가 우리의 3B 변형과 유사한 모델을 사용한다(비록 영리한 파라미터 공유로 인해 파라미터가 극적으로 적지만). GLUE에서 인상적인 성능을 내기 위해 ALBERT 저자는 작업에 따라 "6개에서 17개" 모델을 조립했다. 이는 T5-11B보다 ALBERT 앙상블을 사용하여 예측을 생성하는 데 더 계산 비용이 많이 드는 결과를 초래할 수 있다.

SQuAD의 경우, Exact Match score에서 1점 이상 앞선 최신 기술(ALBERT(Lan et al., 2019))을 능가하였다. SQuAD는 오랜 벤치마크입니다.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & GLUE & CoLA & SST-2 & MRPC & MRPC & STS-B & STS-B \\ Model & Average & Matthew’s & Accuracy & F1 & Accuracy & Pearson & Spearman \\ \hline Previous best & 89.4\({}^{a}\) & 69.2\({}^{b}\) & 97.1\({}^{a}\) & **93.6\({}^{b}\)** & **91.5\({}^{b}\)** & 92.7\({}^{b}\) & 92.3\({}^{b}\) \\ T5-Small & 77.4 & 41.0 & 91.8 & 89.7 & 86.6 & 85.6 & 85.0 \\ T5-Base & 82.7 & 51.1 & 95.2 & 90.7 & 87.5 & 89.4 & 88.6 \\ T5-Large & 86.4 & 61.2 & 96.3 & 92.4 & 89.9 & 89.9 & 89.2 \\ T5-3B & 88.5 & 67.1 & 97.4 & 92.5 & 90.0 & 90.6 & 89.8 \\ T5-11B & **90.3** & **71.6** & **97.5** & 92.8 & 90.4 & **93.1** & **92.8** \\ \hline  & QQP & QQP & MNLI-m & MNLI-mm & QNLI & RTE & WNLI \\ Model & F1 & Accuracy & Accuracy & Accuracy & Accuracy & Accuracy & Accuracy \\ \hline Previous best & 74.8\({}^{c}\) & **90.7\({}^{b}\)** & 91.3\({}^{a}\) & 91.0\({}^{a}\) & **99.2\({}^{a}\)** & 89.2\({}^{a}\) & 91.8\({}^{a}\) \\ T5-Small & 70.0 & 88.0 & 82.4 & 82.3 & 90.3 & 69.9 & 69.2 \\ T5-Base & 72.6 & 89.4 & 87.1 & 86.2 & 93.7 & 80.1 & 78.8 \\ T5-Large & 73.9 & 89.9 & 89.9 & 89.6 & 94.8 & 87.2 & 85.6 \\ T5-3B & 74.4 & 89.7 & 91.4 & 91.2 & 96.3 & 91.1 & 89.7 \\ T5-11B & **75.1** & 90.6 & **92.2** & **91.9** & 96.9 & **92.8** & **94.5** \\ \hline  & SQuAD & SQuAD & SuperGLUE & BoolQ & CB & CB & COPA \\ Model & EM & F1 & Average & Accuracy & F1 & Accuracy & Accuracy \\ \hline Previous best & 90.1\({}^{a}\) & 95.5\({}^{a}\) & 84.6\({}^{d}\) & 87.1\({}^{d}\) & 90.5\({}^{d}\) & 95.2\({}^{d}\) & 90.6\({}^{d}\) \\ T5-Small & 79.10 & 87.24 & 63.3 & 76.4 & 56.9 & 81.6 & 46.0 \\ T5-Base & 85.44 & 92.08 & 76.2 & 81.4 & 86.2 & 94.0 & 71.2 \\ T5-Large & 86.66 & 93.79 & 82.3 & 85.4 & 91.6 & 94.8 & 83.4 \\ T5-3B & 88.53 & 94.95 & 86.4 & 89.9 & 90.3 & 94.4 & 92.0 \\ T5-11B & **91.26** & **96.22** & **88.9** & **91.2** & **93.9** & **96.8** & **94.8** \\ \hline  & MultiRC & MultiRC & ReCoRD & ReCoRD & RTE & WiC & WSC \\ Model & F1a & EM & F1 & Accuracy & Accuracy & Accuracy & Accuracy \\ \hline Previous best & 84.4\({}^{d}\) & 52.5\({}^{d}\) & 90.6\({}^{d}\) & 90.0\({}^{d}\) & 88.2\({}^{d}\) & 69.9\({}^{d}\) & 89.0\({}^{d}\) \\ T5-Small & 69.3 & 26.3 & 56.3 & 55.4 & 73.3 & 66.9 & 70.5 \\ T5-Base & 79.7 & 43.1 & 75.0 & 74.2 & 81.5 & 68.3 & 80.8 \\ T5-Large & 83.3 & 50.7 & 86.8 & 85.9 & 87.8 & 69.3 & 86.3 \\ T5-3B & 86.8 & 58.3 & 91.2 & 90.4 & 90.7 & 72.1 & 90.4 \\ T5-11B & **88.1** & **63.3** & **94.1** & **93.4** & **92.5** & **76.9** & **93.8** \\ \hline  & WMT EnDe & WMT EnFr & WMT EnRo & CNN/DM & CNN/DM & CNN/DM \\ Model & BLEU & BLEU & BLEU & ROUGE-1 & ROUGE-2 & ROUGE-L \\ \hline Previous best & **33.8\({}^{e}\)** & **43.8\({}^{e}\)** & **38.5\({}^{f}\)** & 43.47\({}^{g}\)** & 20.30\({}^{g}\) & 40.63\({}^{g}\) \\ T5-Small & 26.7 & 36.0 & 26.8 & 41.12 & 19.56 & 38.35 \\ T5-Base & 30.9 & 41.2 & 28.0 & 42.05 & 20.34 & 39.40 \\ T5-Large & 32.0 & 41.5 & 28.1 & 42.50 & 20.68 & 39.75 \\ T5-3B & 31.8 & 42.6 & 28.2 & 42.72 & 21.02 & 39.94 \\ T5-11B & 32.1 & 43.4 & 28.1 & **43.52** & **21.55** & **40.69** \\ \hline \hline \end{tabular}
\end{table}
표 14: 우리가 연구하는 모든 작업에 대한 T5 변형의 성능. Small, Base, Large, 3B 및 11B는 각각 6,000만, 2억 2,000만, 7억 7,000만, 30억 및 110억 매개변수가 있는 모델 구성을 나타낸다. 각 표의 첫 번째 행에서 작업에 대한 최신 기술(2019년 10월 24일 기준)을 보고하며 위 첨자는 이 캡션의 끝에 나열된 참조와 함께 출처를 나타낸다. 유효성 검사 집합을 사용하는 SQuAD를 제외한 모든 결과는 테스트 집합에 보고됩니다. \ ({}^{a}\)(Lan et al., 2019) \({}^{b}\)(Wang et al., 2019c) \({}^{c}\)(Zhu et al., 2019) \({}^{d}\)(Liu et al., 2019c) \({}^{e}\)(Edunov et al., 2018) \({}^{f}\)(Lample and Conneau, 2019) \({}^{g}\)(Dong et al., 2019)은 3년 전에 생성되었으며, 최근의 개선 사항은 최신 수준을 백분율 포인트만큼만 증가시켰다. 결과들이 테스트 세트에 보고될 때, 이들은 전형적으로 모델의 앙상블에 기초하고 그리고/또는 레버리지 외부 데이터 세트들(예를 들어, 트리비아QA(Joshi et al., 2017) 또는 NewsQA(Trischler et al., 2016))에 기초하여 작은 SQuAD 트레이닝 세트를 증강시킨다는 점에 주목한다. SQuAD에 대한 인간 성능은 Exact Match 및 F1 메트릭에 대해 각각 82.30 및 91.22로 추정되므로(Rajpurkar 등, 2016), 이 벤치마크에 대한 추가 개선이 의미 있는지 명확하지 않다.

SuperGLUE의 경우, 우리는 큰 마진(평균 점수 84.6(Liu 등, 2019)에서 88.9로 최첨단을 개선했다. SuperGLUE는 "현재의 최첨단 시스템의 범위를 넘어서지만, 대부분의 대학 교육 영어 화자에 의해 해결될 수 있는 작업들"을 포함하도록 설계되었다(Wang et al., 2019). 우리는 89.8의 인간 성능과 거의 일치한다(Wang et al., 2019). 흥미롭게도 읽기 이해 과제(MultiRC 및 ReCoRD)에서 우리는 인간의 성능을 큰 폭으로 초과하며, 이러한 작업에 사용되는 평가 메트릭이 기계로 만들어진 예측에 편향될 수 있음을 시사한다. 반면에 인간은 COPA와 WSC 모두에서 100% 정확도를 달성하여 모델의 성능보다 훨씬 우수하다. 이것은 우리 모델이 특히 자원이 적은 환경에서 완벽하기 어려운 언어적 과제가 남아 있음을 시사한다.

우리는 WMT 번역 작업에서 최첨단 성능을 달성하지 못했다. 이것은 부분적으로 영어 전용 레이블이 지정되지 않은 데이터 세트를 사용하기 때문일 수 있다. 또한 이러한 작업에 대한 대부분의 최상의 결과는 정교한 데이터 증강 방식인 역번역(Edunov et al., 2018; Lample and Conneau, 2019)을 사용한다는 점에 주목한다. 루마니아 벤치마크에 대한 저자원 영어에 대한 최신 기술은 또한 추가적인 형태의 교차-언어 비감독 트레이닝(Lample and Conneau, 2019)을 사용한다. 우리의 결과는 스케일 및 영어 사전 훈련이 이러한 보다 정교한 방법의 성능에 부합하기에 충분하지 않을 수 있음을 시사한다. 보다 구체적으로, 영어에서 독일어 newstest2014 세트에 대한 최상의 결과는 WMT 2018(Edunov et al., 2018)의 훨씬 더 큰 트레이닝 세트를 사용하여, 우리의 결과와 직접적인 비교를 어렵게 한다.

마지막으로 CNN/Daily Mail에서 우리는 ROUGE-2-F 점수에서 상당한 양만큼만 최첨단 성능을 달성한다. ROUGE 점수에 대한 개선이 반드시 더 일관된 요약에 대응하는 것은 아닌 것으로 나타났다(Paulus et al., 2017). 또한 CNN/Daily Mail이 추상적 요약 벤치마크로 간주되는 동안 순수하게 추출적인 접근법이 잘 작동하는 것으로 나타났다(Liu, 2019). 또한, 최대 가능성으로 훈련된 생성 모델이 반복적인 요약을 생성하기 쉽다는 것이 주장되었다(참조 등, 2017). 이러한 잠재적인 문제에도 불구하고, 우리는 우리의 모델이 일관되고 대체로 정확한 요약을 생성한다는 것을 발견했다. 부록 C에 몇 가지 체리 선택되지 않은 검증 세트 예를 제공한다.

강력한 결과를 달성하기 위해 T5는 실험 연구의 통찰력을 전례 없는 규모와 결합한다. 섹션 3.6에서 기본 모델의 사전 훈련량 또는 크기를 확장하면 상당한 이득이 발생한다는 것을 발견했다. 이를 감안해 T5에 도입한 '비스케일링' 변화가 얼마나 강력한 실적에 기여했는지 가늠하는 데 관심이 쏠렸다. 이를 위해 다음과 같은 세 가지 구성을 비교하였다. 첫째, \(2^{35}\approx 34\)B 토큰에 대해 사전 훈련된 표준 기준 모델, 둘째, "baseline-1T"라고 하는 약 1조 토큰(즉, T5에 사용된 사전 훈련의 동일한 양)에 대해 사전 훈련된 기준 모델, 셋째, T5-Base. 기준선-1T와 T5-베이스의 차이는 T5를 설계할 때 "비스케일링" 변화를 구성한다. 따라서 이 두 모델의 성능을 비교하면 체계적인 연구에서 얻은 통찰력의 영향을 구체적으로 측정할 수 있다.

이 세 가지 모델 구성의 성능은 표 15에 나와 있으며 섹션 3.6의 결과와 일치하여 추가 사전 훈련이 기준선보다 성능을 향상시킨다는 것을 발견했다. 그럼에도 불구하고, T5-Base는 모든 다운스트림 태스크에서 베이스라인-1T보다 실질적으로 더 우수하다. 이는 규모가 T5의 성공에 기여하는 유일한 요인이 아님을 시사한다. 우리는 더 큰 모델이 증가된 크기뿐만 아니라 이러한 비스케일링 요인으로부터도 이익을 얻는다고 가정한다.

## 4 Reflection

체계적인 연구를 완료한 후 가장 중요한 연구 결과를 먼저 요약하여 마무리한다. 우리의 결과는 연구 방법이 다소 유망할 수 있는 몇 가지 고수준의 관점을 제공한다. 결론적으로, 우리는 이 분야를 더 발전시키기 위한 효과적인 접근법을 제공할 수 있다고 생각하는 몇 가지 주제에 대해 개요를 제시한다.

### Takeaways

#### Text-to-text

본 논문의 텍스트-텍스트 프레임워크는 동일한 손실 함수 및 디코딩 절차를 사용하여 매우 다양한 텍스트 태스크에 대해 단일 모델을 훈련하는 간단한 방법을 제공한다. 우리는 이 접근법이 추상적 요약과 같은 생성 작업, 자연어 추론과 같은 분류 작업, 심지어 STS-B와 같은 회귀 작업에 성공적으로 적용될 수 있는 방법을 보여주었다. 단순성에도 불구하고, 우리는 텍스트 대 텍스트 프레임워크가 작업별 아키텍처와 유사한 성능을 얻었고 궁극적으로 규모와 결합될 때 최첨단 결과를 생성한다는 것을 발견했다.

#### Architectures

NLP를 위한 전이 학습에 대한 일부 작업은 트랜스포머의 구조적 변형을 고려했지만, 우리는 원래의 인코더-디코더 형태가 텍스트 대 텍스트 프레임워크에서 가장 잘 작동한다는 것을 발견했다. 인코더-디코더 모델은 "인코더-전용"(예를 들어, BERT) 또는 "디코더-전용"(언어 모델) 아키텍처보다 두 배 많은 파라미터를 사용하지만, 유사한 계산 비용을 갖는다. 또한, 인코더와 디코더에서 파라미터를 공유하는 것이 전체 파라미터 수를 절반으로 줄이면서 실질적인 성능 저하를 초래하지 않는다는 것을 보여주었다.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline Model & GLUE & CNNDM & SQuAD & SGLUE & EnDe & EnFr & EnRo \\ \hline \(\bigstar\) Baseline & 83.28 & 19.24 & 80.88 & 71.36 & 26.98 & 39.82 & 27.65 \\ Baseline-1T & 84.80 & 19.62 & 83.01 & 73.90 & 27.46 & 40.30 & 28.34 \\ T5-Base & **85.97** & **20.90** & **85.44** & **75.64** & **28.37** & **41.37** & **28.98** \\ \hline \hline \end{tabular}
\end{table}
표 15: 나머지 논문에서 사용된 기본 실험 설정과 T5-베이스의 성능 비교. 결과는 검증 세트에 보고됩니다. "기준선-1T"는 (기준선에 사용된 바와 같이) \(2^{35}\approx 34\)B 토큰 대신에 1조 토큰(T5 모델 변형에 사용된 동일한 수)에 대해 기준선 모델을 사전 트레이닝함으로써 달성된 성능을 지칭한다.

**감독되지 않은 목표**: 전반적으로 모델을 학습하여 무작위로 손상된 텍스트를 재구성하는 대부분의 "디노이징" 목표가 텍스트 간 설정에서 유사하게 수행된다는 것을 발견했습니다. 결과적으로, 우리는 비지도 사전 훈련이 더 계산적으로 효율적이도록 짧은 표적 시퀀스를 생성하는 목표를 사용하는 것을 제안한다.
**데이터 세트**: Common Crawl 웹 덤프에서 휴리스틱하게 정리된 텍스트로 구성된 "커널 클린 크롤링 코퍼스"(C4)를 도입했습니다. C4를 추가 필터링을 사용하는 데이터 세트와 비교할 때 도메인 내 레이블이 지정되지 않은 데이터에 대한 학습이 몇 가지 다운스트림 작업에서 성능을 높일 수 있음을 발견했다. 그러나, 단일 도메인에 대한 제약은 전형적으로 더 작은 데이터 세트를 초래한다. 우리는 레이블이 지정되지 않은 데이터 세트가 사전 훈련 과정에서 여러 번 반복될 만큼 충분히 작을 때 성능이 저하될 수 있음을 별도로 보여주었다. 이는 일반적인 언어 이해 작업에 C4와 같은 크고 다양한 데이터 세트를 사용하도록 동기를 부여한다.
**훈련 전략**: 미세 조정 중에 미리 훈련 된 모델의 모든 매개 변수를 업데이트 하는 기본 접근 방식은 모든 매개 변수를 업데이트 하는 것이 가장 비싸지만 더 적은 매개 변수를 업데이트 하도록 설계 된 메서드를 능가 한다는 것을 발견했습니다. 또한 여러 태스크에 대한 모델을 한 번에 학습하기 위한 다양한 접근 방식을 실험했는데, 이는 텍스트 대 텍스트 설정에서 단순히 배치를 구성할 때 다른 데이터 세트의 혼합 예제에 해당한다. 멀티 태스크 학습의 주요 관심사는 훈련할 각 태스크의 비율을 설정하는 것이다. 우리는 궁극적으로 감독되지 않은 사전 훈련에 이어 감독된 미세 조정의 기본 접근법의 성능과 일치하는 혼합 비율을 설정하는 전략을 찾지 못했다. 그러나, 태스크의 혼합에 대한 사전 훈련 후 미세 조정이 감독되지 않은 사전 훈련과 유사한 성능을 생성한다는 것을 발견했다.
**크기 조정**: 더 많은 데이터에 대한 모델 학습, 더 큰 모델 학습 및 모델 앙상블 사용을 포함하여 추가 컴퓨팅을 활용하기 위한 다양한 전략을 비교했습니다. 우리는 더 많은 데이터에 대해 더 작은 모델을 훈련하는 것이 종종 더 적은 단계에 대해 더 큰 모델을 훈련함으로써 더 나은 성능을 제공했지만 각 접근법이 상당한 성능 향상을 제공한다는 것을 발견했다. 우리는 또한 모델의 앙상블이 추가 계산을 활용하는 직교 수단을 제공하는 단일 모델보다 훨씬 더 나은 결과를 제공할 수 있음을 보여주었다. 동일한 기본 사전 훈련 모델에서 미세 조정된 앙상블 모델은 사전 훈련 및 모든 모델을 완전히 개별적으로 미세 조정하는 것보다 더 나쁜 성능을 보였지만 미세 조정 전용 앙상블은 여전히 단일 모델을 훨씬 능가했다.
**제한을 푸시**: 위의 인사이트를 결합 하 고 훨씬 더 큰 모델 (최대 110억 매개 변수)을 훈련 하 여 고려 한 많은 벤치마크에서 최신 결과를 달성 했습니다. 비지도 학습을 위해 C4 데이터 세트에서 텍스트를 추출하고 토큰의 연속 범위를 손상시키는 노이즈 제거 목표를 적용했다. 우리는 개별 작업을 미세 조정하기 전에 다중 작업 혼합물에 대해 사전 훈련했다. 전반적으로 우리 모델은 1조 개 이상의 토큰에 대해 훈련되었습니다. 결과의 복제, 확장 및 적용을 용이하게 하기 위해 각 T5 변형에 대한 코드, C4 데이터 세트 및 사전 훈련된 모델 가중치를 릴리스한다.1

### Outlook

**큰 모델의 불편함**: 놀랍지 않지만 중요한 결과는 큰 모델이 더 나은 성능을 보이는 경향이 있다는 것입니다. 이러한 모델들을 실행하는 데 사용되는 하드웨어가 계속해서 더 저렴해지고 더 강력해지고 있다는 사실은 스케일 업이 더 나은 성능을 달성하기 위한 유망한 방법일 수 있음을 시사한다(서튼, 2019). 그러나, 예를 들어 클라이언트 측 추론 또는 연합 학습을 수행할 때 더 작거나 더 저렴한 모델을 사용하는 것이 도움이 되는 애플리케이션 및 시나리오가 항상 있을 것이다(Konecny 등, 2015, 2016). 이와 관련하여 전이 학습의 한 가지 유익한 활용은 자원이 적은 과제에서 좋은 성과를 얻을 수 있는 가능성이다. 낮은 리소스 작업은 종종 더 많은 데이터에 레이블을 지정할 자산이 부족 한 설정에서 발생 합니다 (정의에 따라). 따라서, 낮은 자원 애플리케이션들은 종종 추가적인 비용을 발생시킬 수 있는 계산 자원들에 대한 제한된 액세스를 갖는다. 결과적으로 전이학습이 가장 큰 영향을 미칠 곳에 적용할 수 있도록 더 저렴한 모델로 더 강력한 성능을 달성하는 방법에 대한 연구를 옹호한다. 이들 라인을 따르는 일부 현재 작업은 증류(Hinton et al., 2015; Sanh et al., 2019; Jiao et al., 2019), 파라미터 공유(Lan et al., 2019), 및 조건부 계산(Shazeer et al., 2017)을 포함한다.
**더 효율적인 지식 추출**: 사전 훈련의 목표 중 하나는 다운스트림 작업에 대한 성능을 향상시키는 범용 "지식"을 모델에 제공하는 것입니다. 현재 일반적인 관행인 이 작업에서 사용하는 방법은 손상된 텍스트 범위를 제거하기 위해 모델을 훈련하는 것이다. 우리는 이 단순화 기법이 모델 범용 지식을 가르치는 매우 효율적인 방법이 아닐 수 있다고 의심한다. 보다 구체적으로, 먼저 1조 토큰의 텍스트로 모델을 훈련할 필요 없이 좋은 미세 조정 성능을 얻을 수 있는 것이 유용할 것입니다. 이러한 라인들에 따른 일부 동시 작업은 실제 및 머신-생성 텍스트를 구별하기 위해 모델을 사전 트레이닝함으로써 효율성을 향상시킨다(Clark 등, 2020).
**작업 간의 유사성 공식화**: 레이블이 지정 되지 않은 도메인 내 데이터에 대 한 사전 교육이 다운스트림 작업에 대 한 성능을 향상시킬 수 있음을 관찰 했습니다 (섹션 3.4). 이 발견은 대부분 위키피디아의 데이터를 사용하여 SQuAD가 생성되었다는 사실과 같은 기본적인 관찰에 의존한다. 사전 훈련과 다운스트림 작업 사이의 "유사성"에 대한 보다 엄격한 개념을 공식화하여 레이블이 지정되지 않은 데이터의 출처에 대해 보다 원칙적인 선택을 할 수 있도록 하는 것이 유용할 것이다. 컴퓨터 비전 분야에서 이러한 라인들에 따른 몇몇 초기 경험적 작업이 있다(Huh et al., 2016; Kornblith et al., 2018; He et al., 2018). 태스크의 연관성에 대한 더 나은 개념은 또한 GLUE 벤치마크에 도움이 되는 것으로 나타난 _감독_ 사전 트레이닝 태스크를 선택하는 데 도움이 될 수 있다(Phang et al., 2018).
**언어 불가지 모델**: 영어 전용 사전 교육이 우리가 연구한 번역 작업에 대한 최신 결과를 달성하지 못했다는 사실에 실망했습니다. 또한 어휘가 미리 인코딩할 수 있는 언어를 지정해야 하는 물류 어려움을 피하는 데 관심이 있습니다. 이러한 문제를 해결하기 위해 우리는 언어 불가지론 모델, 즉 텍스트의 언어에 관계없이 주어진 NLP 작업을 좋은 성능으로 수행할 수 있는 모델을 추가로 조사하는 데 관심이 있다. 이것은 영어가 세계 인구의 대다수를 위한 모국어가 아니라는 점을 고려할 때 특히 주목할 만한 문제이다.

이 논문의 동기는 NLP를 위한 전이 학습에 대한 최근 작업의 폭주였다. 이 작업을 시작하기 전에 이러한 발전은 학습 기반 방법이 아직 효과적인 것으로 나타나지 않은 환경에서 이미 획기적인 발전을 가능하게 했다. 예를 들어 현대 전이 학습 파이프라인이 어려울 수 있도록 특별히 설계된 작업인 슈퍼GLUE 벤치마크에서 인간 수준의 성능을 거의 일치시킴으로써 이러한 추세를 계속할 수 있게 되어 기쁘다. 우리의 결과는 간단하고 통합된 텍스트 대 텍스트 프레임워크, 새로운 C4 데이터 세트 및 체계적인 연구의 통찰력의 조합에서 비롯된다. 또한 현장의 경험적 개요와 그 위치에 대한 관점을 제공했다. 우리는 일반적인 언어 이해의 목표를 향한 전이 학습을 활용한 지속적인 작업을 보게 되어 흥분된다.

## Acknowledgments

우리는 그레이디 사이먼, 노아 피델, 사무엘 R에게 감사한다. Bowman, Augustus Odena, Daphne Ippolito, Noah Constant, Orhan Firat, Ankur Bapna, and Sebastian Ruder 이 원고에 대한 논평에 대해 Zak Stone과 TFRC 팀은 지원을 위해; 데이터 세트 생성에 대한 안내를 위해 Austin Tarango; 다중 작업 기계 번역에 대한 통찰을 위해 Melvin Johnson, Dima Lepikhin, Katrin Tomanek, Jeff Klingner, and Naveen Arivazhagan은 어댑터 레이어에 대한 논평에 대해 Neil Houlsby; 인프라에 대한 지원을 위해 Olga Wichowska, Ola Spyra, Michael Banfield, Yi Lin, and Frank Chen; TensorFlow Datasets에 대한 협력을 위해 Etienne Pot, Ryan Sepassi, and Pierre Ruyssen; Common Crawl에 대한 다운로드 파이프라인에 대한 도움을 위해 Rohan Anil; SentencePiece에 대한 작업을 위해 Robby Neale and Taku Kudo; C4 생성에 대한 자세한 내용을 지적하기 위해 Jeffrey Li; 그리고 그들의 토론과 통찰을 위해 Google Brain 팀의 많은 다른 구성원들이 언급했다.

## Appendix A Contributions

콜린은 이 프로젝트의 범위를 설계하고 이 논문을 작성했으며 섹션 3.1에서 3.6까지의 모든 실험을 실행했으며 코드 베이스의 많은 부분을 기여했다. 노암은 텍스트 대 텍스트 프레임워크, 감독되지 않은 목표 및 데이터 세트 혼합 전략을 포함한 많은 아이디어를 기여했으며 기본 트랜스포머 모델과 아키텍처 변형을 구현하고 섹션 3.7에서 실험을 실행했다. 애덤은 이 프로젝트의 모든 엔지니어링 측면을 감독하고 C4 데이터 세트를 만들고 데이터 세트 파이프라인을 구현했으며 다양한 벤치마크 데이터 세트를 추가했다. 캐서린은 실험을 조정하고 문서를 작성하고 업데이트했으며 기준선을 설계하는 데 도움이 되는 실험을 실행했으며 코드 베이스의 많은 부분에 기여했다. 샤란은 필요한 데이터 세트와 전처리기 중 일부를 제공하고 코드 베이스의 공개 소싱을 공동 선도하는 것 외에도 다양한 예비 실험을 실행했다. 마이클은 위노그라드 데이터 세트의 모든 측면을 소유하고 우리가 사용한 많은 데이터 세트를 섭취했으며 인프라에 다양한 개선과 수정을 기여했으며 몇 가지 예비 실험을 실행했다. Yanqi는 실험을 실행하고 합리적인 기준선에 정착하는 데 도움이 되는 방법을 구현했으며 섹션 3.7에서 모델의 최종 미세 조정을 도왔다. Wei는 또한 최종 미세 조정을 도왔고 일부 전처리기를 개선했다. 피터는 사전 훈련 데이터 세트의 초기 버전을 프로토타입화하고 SQuAD 및 CNN/DM 태스크와 관련된 문제를 해결했다. 모든 저자는 이 작업에서 따랐던 범위와 연구 방향을 설정하는 데 도움이 되었다.

## Appendix B Converting WNLI to Our Text to Text Format

섹션 2.4에서 논의한 바와 같이 WNLI의 데이터에 대해 훈련하지 않는다. 대신 WNLI 테스트 세트(섹션 3.7의 결과에 대해)에서 평가할 때 WSC 및 DPR에 훈련된 모델을 사용하여 평가할 수 있도록 WNLI 테스트 세트를 "참조 명사 예측" 텍스트 대 텍스트 형식으로 변환한다. 우리의 WNLI 전처리기는 He 등(2019)에 의해 제안된 것에서 영감을 받았다. WNLI의 예제는 전제, 가설 및 가설이 참인지 거짓인지를 나타내는 레이블로 구성됨을 상기하십시오. 2.4절의 예를 사용하여 가설은 "시위자들이 폭력을 두려워했기 때문에 시위자들에게 허가를 거절했다."라는 전제와 "시위자들이 폭력을 두려워했다."라는 라벨이 있다. 우리는 먼저 전제에서 모든 대명사의 위치를 찾는다(우리의 예에서 "그들"). 그런 다음, 우리는 가설에서 부분 문자열인 각 대명사 앞에 있거나 뒤따르는 단어의 최대 수("우리의 예에서는 격렬한 폭력")를 발견하며, 사례 및 구두점을 무시한다. 전제가 여러 후보 대명사를 포함할 때, 우리는 가설의 가장 큰 부분 문자열이 앞에 있거나 뒤에 있는 대명사를 선택한다. 그런 다음 우리는 전제에서 대명사를 별표로 둘러싸서 강조 표시한다. 후보 명사(참 또는 거짓 레이블을 얻기 위해 모델의 예측과 비교됨)에 대해, 우리는 가설에서 일치하는 부분 문자열을 제거하고 선택적으로 비-소유("시범자"의 결과)로 만든다.

## 부록 C 예제 CNN/Daily Mail의 예측

이 모델이 유창한 요약을 생성한다는 것을 보여주기 위해, 우리는 지상-진실 요약과 함께 검증 세트에 대한 최상의 모델(T5-11B)의 몇 가지 예제 해독을 포함한다. 이러한 예는 무작위로 선택되었으며 체리 선택되지 않았다.

1. **Ground-truth:**: 뾰족한 생물이 남아프리카의 크루거 국립 공원에서 싸움에서 물러서지 않자 표범은 포기했습니다. 야생동물 애호가 리슬 물먼(41세)은 도로에 나가 그녀의 카메라로 무장한 채 기이한 전투를 잡았다. 표범과 고슴도치는 덤불에서 굴러 나와 길가에서 싸우기 시작했다. **예측:**: 표범은 남아프리카의 크루거 국립 공원에서 용감한 포큐핀으로 점심을 만들려고 했습니다. 그러나 그 포식자는 싸우기 동안 뾰족한 생물이 물러서지 않자 굳게 자리를 잡았다. 야생동물 애호가 리슬 물먼(41세)은 도로에 나가 그녀의 카메라로 무장한 채 기이한 전투를 잡았다.
2. **지상 진실:**: 연구자들은 집이 발견되지 않은 종의 온상이라고 말합니다. 40개의 북쪽 캐롤라이나 집에 대한 연구에서 8,000개의 박테리아 및 고세균 분류군이 발견되었다. 캘리포니아의 11개 주택에서 우리 중 매우 다양한 곰팡이가 발견되었다. **예측:**: 연구자들은 실내 생물 군계가 거의 텍사 크기의 약 247,000 평방 마일인 얼음이 없는 토지의 약 0.5%를 구성한다고 추정합니다. 노스 캐롤라이나의 40개 주택에 대한 연구는 8,000개 이상의 박테리아 및 고세균 분류군을 문서화하기 위해 유전체 기술을 사용했다. 같은 주에 있는 50개의 집에 대한 또 다른 연구에서는 곤충과 거미를 포함한 750가지 이상의 절지동물을 설명했다. 캘리포니아의 11개 집을 조사한 결과 우리 사이에서 매우 다양한 곰팡이가 발견되었다.
3. **지상 진실:**: 치명적인 주사에 사용되는 약물이 월요일 밤에 결함이 있는 것으로 나타난 후 주에서 실행이 일시적으로 중단되었습니다. 킬리 기센다너는 치명적인 펜토바르비탈 복용량이 흐린 것으로 밝혀졌을 때 그녀의 사망이 두 번째로 지연되었다. 주 관리들은 그 결정이 효과가 없을 위험을 감수하지 않기로 결정하기 전에 세 번이나 마음을 바꾸면서 결정을 망설였다 조지아 법무장관 사몰렌스는 '사형이 헌법상 방식으로 수행되는 것이 필수적'이라고 말했다. **예측:**: 조지아의 사형수 중 유일한 여성인 켈리 gissendaner의 처형이 월요일에 두 번째로 지연되었습니다. 처형팀은 치명적인 펜토바르비탈 복용량이 흐린 것처럼 보인다는 것을 발견했다. 구름 낀 약물은 사형 반대자들을 강화시켰고, 그들은 미국의 다른 지역에서 3번의 처형을 실패한 후 반대 의견을 표명했다
4. **Ground-truth:**: dani alves는 프랑스 및 칠레와 대결할 브라질 분대에 선택 되지 않았습니다. 바르셀로나 수비수는 토요일 사람들에게 핫도그를 대접하는 것을 촬영했다. 이번 주 앨브스는 전 팀 매트 호세 핀토와 함께 자선 싱글을 발매했다. 올 여름 바르셀로나를 자유이적으로 떠날 것으로 보인다. 스페인 언론 보도에 따르면 그는 파리 성게르메인에 서명할 것이라고 주장한다. **예측:**: dani alves는 프랑스 및 칠레에 대한 브라질의 친선에 대해 선택되지 않았습니다. 바르셀로나는 바로 뒤에서 호세 핀토와 함께 자선 싱글을 발매했다. 토요일에 핫도그 바 카운터 뒤에서 간식을 제공하는 것이 촬영되었다. 알베스는 아직 누 캠프에서 새로운 제안을 받지 못했다. 모든 최신 바르셀로나 뉴스를 보려면 여기를 클릭하십시오.

## 부록 D 전처리 예제

이 섹션에서는 우리가 고려하는 각 데이터 세트에 대한 전처리 예제를 제공한다.

### CoLA

**Original input:**

**문장:** John은 Bill을 자신의 마스터로 만들었습니다.
**처리된 입력:** 콜라 문장: John은 Bill을 마스터로 만들었습니다.
**원래 대상:** 1
**처리 대상:** 허용 됩니다.

### Rte

**Original input:**

**문장 1:** 유고슬라비아의 이탈리아인 중 더 적은 비율이 슬로베니아에 정착되었습니다 (1991년 전국 인구 조사에서 약 3000명의 슬로베니아 주민이 자신을 민족 이탈리아인으로 선언했습니다).
**문장 2:** 슬로베니아에는 3,000명의 주민이 있습니다.
**처리된 입력:** 문장 1: 유고슬라비아의 이탈리아인 중 더 적은 비율이 슬로베니아에 정착되었습니다 (1991년 전국 인구 조사에서 약 3000명의 슬로베니아 주민이 자신을 민족 이탈리아인으로 선언했습니다). 문장2: 슬로베니아에는 3,000명의 주민이 살고 있습니다.
**원래 대상:** 1
**처리된 대상:** not_entailment

### Mnli

**Original input:**

**가설:** 세인트 루이스 카디널스는 항상 승리했습니다.

**전제:** 네, 패배가 확실합니다 제 말은 제가 원래 세인트루이스와 세인트루이스 카디널스 출신이라는 거죠 대부분 패배한 팀이었지만
**처리된 입력:** nli 가설: 세인트루이스 카디널스는 항상 승리했습니다. 패배 : 그래, 패배는 내 말은 내가 원래 세인트 루이스와 세인트 루이스 카디널스 출신이라는 거야 원래 세인트 루이스 카디널스 출신이야 대부분 패배가 있었지만

**원래 대상**: 2

**처리된 대상:**: 모순

### Mrpc

**Original input:**:

**문장 1:**: 우리는 9월 11일 경험의 프리즘을 통해 기존 증거를 새로운 관점에서 보았기 때문에 행동했습니다. " 럼스펠드가 말했습니다.  문장 2:**: 오히려 미국은 행정부가 "기존  증거를 새로운 관점에서, 9월 11일 경험의 프리즘을 통해" 보았기 때문에 행동했습니다.
**처리된 입력:**: mrpc 문장 1: 우리는 9월 11일 경험의 프리즘을 통해 새로운 관점에서 기존 증거를 보았기 때문에 행동했습니다. " 럼스펠드"가 말했습니다. 문장 2: 오히려, 미국은 행정부가 새로운 관점에서 기존 증거를 보았기 때문에 행동했습니다.  9월 11일 경험의 프리즘을 통해 말이죠.
**원래 대상**: 1

**처리된 대상:**: 동등

### Qnli

**Original input:**:

**질문:**: Jebe는 어디에서 죽었나요?
**문장:**: 칭기즈칸은 직후 수부타이를 몽고로 회고했고, 제베는 사마르칸드로 돌아가는 길에서 사망했습니다.
**처리된 입력:**: qnli 질문: Jebe는 어디에서 사망했습니까? 징기스칸은 곧  수부타이를 몽고로 회고했고, 제베는  사마르칸드로 돌아가는 길에서 사망했다.
**원래 대상**: 0
**처리된 대상:**: 수반 사항

### Qqp

**Original input:**:

**질문 1:**: 고대 로마에서는 어떤 특성이 당신을 매우 바람직하게 만들었을까요?
**질문 2:**: OPPERTINUTY를 프레셔로 등록하려면 어떻게 해야 하나요?
**처리된 입력:**: qqp question1: 고대 로마에서 어떤 특성이 당신을 매우 바람직하게 만들었을까요? 질문 2: 어떻게 내가 그 회사에 프레셔로 가입하게 되는가?

**원래 대상**: 0

**처리된 대상:**: not_duplicate

### Sst2

**Original input:**:

**문장:**: 심리적 통찰의 서비스에 기술 노하우를 기술적으로 구부리는 영화 제작자로서의 Fincher의 지위를 확인 합니다.
**처리된 입력:**: sst2 문장: 심리적 통찰의 서비스에 기술 노하우를 기술적으로 구부리는 영화 제작자로서의 Fincher의 지위를 확인 합니다.
**원래 대상**: 1
**처리된 대상:**: 긍정

### Stsb

**Original input:**:

**문장 1:**: Puretunes의 대표자는 수요일에 코멘트를 위해 즉시 도달할 수 없습니다.
**문장 2:**: Puretunes 대표는 목요일에 양복에 대해 언급할 위치를 찾을 수 없습니다.
**처리된 입력:**: stsb sentence1: 수요일에 주석을 위해 Puretunes 대표자에 즉시 도달할 수 없습니다. 문장2: 푸레툰스 대표들은 목요일에 그 소송에 대해 논평할 수 없었다.
**Original target:**: 3.25
**처리된 대상:**: 3.2

### Cb

**Original input:**:

**가설:**: Valence는 도움이 되었습니다.
**전제:**: void-brain의 유효성을 검사하고, virtuous valet의 유효성을 검사합니다. 왜 무화과자가 자기 몸의 일부분을 고르지 못했을까? 그가 돕고 있다고 생각했나요?
**처리된 입력:**: cb 가설: Valence는 전제를 돕고 있습니다. Valence the void-brain, Valence the virtuous valet. 왜 무화과자가 자기 몸의 일부분을 고르지 못했을까? 그가 돕고 있다고 생각했나요?
**원래 대상**: 1
**처리된 대상:**: 모순

### Copa

**Original input:**

**Question:** effect

**전제:** 전국적으로 정치적 폭력이 발생 했습니다.

**선택 1:** 많은 시민이 의사당으로 이동했습니다.

**선택 2:** 많은 시민이 다른 지역으로 피신했습니다.

**처리된 입력:** copa choice1: 많은 시민이 의사당으로 이전했습니다. choice2:

많은 시민들이 다른 지역으로 피신했다. 정치적 폭력.

 그 나라에서 파산했다. 질문: 효과

**원래 대상:** 1

**처리된 대상:** True

### MultiRC

**Original input:**

**정답:** 전통적인 아침 식사 대신 먹을 파이만 있습니다.

**단락:** <b>1: </b>옛날에 Joey라는 다람쥐가 있었습니다. <br><b>센트

2: </b> 조이는 밖에 나가서 사촌 지미와 노는 것을 좋아했어요. <br><b>센트

3:</b>Joey와 Jimmy가 함께 바보같은 게임을 하며 항상 웃고 있었다. <br><b>센트

4:</b>어느 날 조이와 지미가 줄리 이모 댁에서 함께 수영하러 갔다.

 pond. <br><b>발송 5:</b>조이가 아침 일찍 일어나 음식을 좀 먹었다

 떠나기 전에 <br><b>Sent 6:</b>He couldn't find anything except

 파이를 위해! <br><b>Sent 7:</b>보통 조이는 시리얼, 과일(배)을 먹을 것이고,

 아니면 아침으로 오트밀. <br><b>Sent 8:</b> 그가 식사한 후, 그와 Jimmy went

 연못으로. <br><b>Sent 9:</b> 그들이 가는 길에 그들은 친구를 보았다

 잭 래빗 <br><b>Sent 10:</b>They dove into the water and swam for several

 몇 시간째야 <br><b>Sent 11:</b>태양은 나갔으나 바람은 차가웠다. <br><b>센트

 12: </b> 조이와 지미는 물에서 나와 집으로 걷기 시작했습니다. <br><b>센트

 13: </b> 그들의 털은 젖었고, 바람은 그들을 식혔다. <br><b>Sent 14:</b>When

 집에 가서 마르고 지미가 좋아하는 보라색 셔츠를 입었어 <br><b>센트

 15:</b>조이가 빨간색과 초록색 점이 있는 파란색 셔츠를 입는다. <br><b>Sent 16:

 </b>두 다람쥐는 조이의 엄마 재스민이 만들고 간 음식을 먹었어요.

 자러 가자 <br>

**질문:** 조이가 아침 식사를 위해 깨어난 아침에 왜 놀랐을까요?

**처리된 입력:** 다중 문자열 질문: 조이가 깨운 아침에 왜 놀랐는지

 아침을 먹으려고? 답변: 전통적인 것이 아니라 먹을 수 있는 파이만 있었다.

 아침식사는 문단:<b>발송 1:</b>옛날에 다람쥐가 있었다

 조이라는 이름 <br><b>Sent 2:</b>Joey는 밖에 나가서 사촌과 노는 것을 좋아했다.

 지미 <br><b>Sent 3:</b>Joey and Jimmy played silly games together, was

 항상 웃고 <br><b>Sent 4:</b>One day, Joey and Jimmy went together at their aunt Julie's pond. <br><b>보낸 5:</b>조이는 아침 일찍 일어나서 떠나기 전에 음식을 좀 먹었다. <br><b>Sent 6:</b>그는 파이 외에는 먹을 것을 찾을 수 없었다! <br><b>Sent 7:</b>보통 조이는 아침 식사로 시리얼, 과일(배) 또는 오트밀을 먹는다. <br><b>Sent 8:</b> 그가 식사 후, 그와 지미는 연못으로 갔다. <br><b>Sent 9:</b>Their way on their friends Jack Rabbit. <br><b>10:</b>그들은 물속으로 비둘기를 치고 몇 시간 동안 헤엄쳤다. <br><b>Sent 11:</b>태양은 나갔으나 바람은 차가웠다. <br><b>Sent 12:</b>Joey and Jimmy get out the water and started walking home. <br><b>13:</b>그들의 털은 젖었고, 바람은 그들을 식혔다. <br><b>Sent 14:</b> 집에 오면 말려서 지미가 좋아하는 보라색 셔츠를 입혔다. <br><b>Sent 15:</b>Joey가 빨간색과 초록색 점으로 파란 셔츠를 입는다. <br><b>Sent 16:</b>두 다람쥐는 조이의 엄마 재스민이 만든 음식을 먹고 잠자리에 들었다. <br>

**원래 대상**: 1

**처리된 대상:**: True

**D.12 WiC 원본 입력:**:

**POS:**: N

**문장 1:**: 모욕적인 것은 그의 행동에 대한 심의였습니다.

**문장 2:**: 배심원의 심의입니다.

**Word:** deliberation

**처리된 입력:**: wic pos: N sentence1: 모욕적인 것은 그의 행동의 심의였습니다. 선고 2: 배심원의 심의. 단어: 숙고

**원래 대상**: 0

**처리된 대상:**: false

**D.13 WSC 및 DPR 원본 입력:**:

**Span 2 text:**: it

**Span 1 text:**: 안정적

**Span 2 인덱스**: 20

**Span 1 인덱스**: 1

**텍스트:**: 마구간은 4개의 좋은 포장마차로 매우 넓었고, 마당으로 큰 그네 창이 열려 쾌적하고 통풍이 잘 되었습니다.

**처리된 입력:**: wsc: 마구간은 4개의 좋은 마구간으로 매우 넓었고, 마당으로 큰 스윙 창이 열려 쾌적하고 통풍이 잘 되었습니다.

**Original target: 1**

**Processed target: stable**

### CNN/Daily Mail

**원본 입력:**: marouane fellaini와 adnan januzaj는 계속해서 팀 동료일 뿐만 아니라 최고의 친구라는 것을 세계에 보여줍니다. 맨체스터 유나이티드와 벨기움 듀오는 수요일 뉴캐슬과의 경기를 앞두고 월요일 밤에 레스토랑에서 그들 자신의 사진을 게시했다. Januzaj는 친구 한 가운데 포즈를 취하고 있고 친구는 잭슨 5 테마 밤이라는 메모를 받지 못한 사람처럼 보인다. 프리미어 리그 듀오 아드난 자누자즈와 마루안 펠라이니가 댄스 플로어에서 친구와 포즈를 취하고 있다. 맨체스터 유나이티드와 벨기움 듀오 펠라이니와 자누자즈는 경기장 안팎에서 좋은 친구이다. 맨체스터 유나이티드 에이스 펠라니가 벤치로 달려가 친구 자누자즈와 qpr전에서 골을 축하했다. 배경 속의 디스코 효과는 이론을 더하지만, 야누자지는 나중에 다른 친구들과 댄스 플로어에서 포즈를 취하면서 신경 쓰지 않는 것 같다. 유나이티드(유나이티드)는 이번 시즌에 노래와 춤을 출 이유가 많지 않았기 때문에 그들이 또 다른 형태의 발매로 디스코텍을 치고 있는 것 같다. 그러나, 수요일 뉴캐슬과의 승리는 루이 판 갈 감독이 올 시즌 챔피언 리그 자리를 놓고 계속 싸우기 때문에 적어도 그의 발가락을 두드리게 할 것이다. 웨스트 브롬에서 맨체스터 유나이티드 팬들 앞에서 자누자즈와 로빈 반 퍼시가 축하하며 펠라이니에 합류한다. 자누자지는 맨체스터 유나이티드의 네덜란드 감독 루이 반 갈로부터 지혜의 말을 받는다. 자누자자와 펠라이니는 뉴캐슬 경기를 앞두고 댄스 플로어로 이동하면서 몇몇 친구들과 함께한다.
**처리된 입력**: 요약: Marouane fellaini 및 adnan januzaj는 계속해서 팀 동료일 뿐만 아니라 최고의 동료라는 것을 세계에 보여줍니다. 맨체스터 유나이티드와 벨기움 듀오는 수요일 뉴캐슬과의 경기를 앞두고 월요일 밤에 레스토랑에서 그들 자신의 사진을 게시했다. Januzaj는 친구 한 가운데 포즈를 취하고 있고 친구는 잭슨 5 테마 밤이라는 메모를 받지 못한 사람처럼 보인다. 프리미어 리그 듀오 아드난 자누자즈와 마루안 펠라이니가 댄스 플로어에서 친구와 포즈를 취하고 있다. 맨체스터 유나이티드와 벨기움 듀오 펠라이니와 자누자즈는 경기장 안팎에서 좋은 친구이다. 맨체스터 유나이티드 에이스 펠라니가 벤치로 달려가 친구 자누자즈와 qpr전에서 골을 축하했다. 배경 속의 디스코 효과는 이론을 더하지만, 야누자지는 나중에 다른 친구들과 댄스 플로어에서 포즈를 취하면서 신경 쓰지 않는 것 같다. 유나이티드(유나이티드)는 이번 시즌에 노래와 춤을 출 이유가 많지 않았기 때문에 그들이 또 다른 형태의 발매로 디스코텍을 치고 있는 것 같다. 그러나 수요일 뉴캐슬전에서의 승리는 루이 반 갈 감독이 이번 시즌 챔피언 리그 자리를 놓고 계속 싸우기 때문에 적어도 그의 발가락을 두드리게 할 것이다. 웨스트 브롬에서 맨체스터 유나이티드 팬들 앞에서 자누자즈와 로빈 반 퍼시가 축하하며 펠라이니에 합류한다. 자누자지는 맨체스터 유나이티드의 네덜란드 감독 루이 반 갈로부터 지혜의 말을 받는다. 자누자자와 펠라이니는 뉴캐슬 경기를 앞두고 댄스 플로어에 오르면서 몇몇 친구들이 합류한다.

**원본 대상:**: 벨지 2인조는 월요일 밤에  일부 친구들과 함께 댄스 플로어로 갔습니다. 맨체스터 유나이티드는 수요일 프리미어 리그에서 뉴캐슬을 만났다. 붉은 악마들은 7년만에 두 번째 리그 원정 우승을 노릴 것이다. 루이 반 갈의 팀은 현재 4회에 리버풀에서 2포인트 떨어져 있다.

**처리된 대상:**: 벨지 2인조는 월요일 밤에  일부 친구들과 함께 댄스 플로어로 갔습니다. 맨체스터 유나이티드는 수요일 프리미어 리그에서 뉴캐슬을 만났다. 붉은 악마들은 7년만에 두 번째 리그 원정 우승을 노릴 것이다. 루이 반 갈의 팀은 현재 4회에 리버풀에서 2포인트 떨어져 있다.

### SQuAD

**Original input:**:

**질문:**: 환자의 폐에서 증가된 산소 농도는 무엇일까요?
**문맥:**: 고압(고압) 약은 특수 산소 챔버를 사용하여 환자 주변 및 필요한 경우 의료진의 분압을 0 2로 증가시킵니다. 일산화탄소 중독, 가스 괴저, 감압병('굴곡')은 때때로 이러한 장치를 사용하여 치료된다. 폐의 0 2 농도 증가는 헤모글로빈의 헴 그룹에서 일산화탄소를 대체하는 데 도움이 됩니다. 산소 가스는 가스 괴저를 일으키는 혐기성 세균에 유독하기 때문에, 그것의 분압을 높이는 것은 그들을 죽이는 데 도움이 된다. 감압병은 잠수 후 너무 빨리 감압하는 잠수부들에게서 발생하며, 그 결과 불활성 가스, 대부분 질소와 헬륨의 거품이 생겨 혈액 속에 형성된다. 0 2의 압력을 가능한 한 빨리 높이는 것이 치료의 일부입니다.

**처리된 입력:**: 질문: 환자의 폐에서 증가된 산소 농도는 무엇일까요? 맥락: 고압(고압) 약은 특수  산소 챔버를 사용하여 환자 주변의 분압을 02로 높이고 필요할 때 의료진에게 전달합니다. 일산화탄소 중독, 가스 괴저, 및 감압병('굴곡')은 때때로 이러한 장치를 사용하여 치료된다. 폐의 0 2 농도 증가는 헤모글로빈의 헴 그룹에서 일산화탄소를 대체하는 데 도움이 된다. 산소 가스는 가스 괴저병을 일으키는 혐기성 세균에 독성이 있으므로, 그것의 분압을 높이면 그들을 죽이는 데 도움이 된다. 감압병은 잠수 후 너무 빨리 감압하는 잠수부들에게서 발생하며, 그 결과 불활성 가스, 대부분 질소와 헬륨의 거품이 생겨 혈액 속에 형성된다. 0 2의 압력을 가능한 한 빨리 높이는 것이 치료의 일부입니다.

**원래 대상:** 일산화탄소

**처리된 대상:** 일산화탄소

### WMT English to German

**원작 입력:**: "루이지가 종종 나에게 형제가 법정에 서는 것을 원하지 않는다고 말했다"고 그녀는 썼다.
**처리된 입력:**: 영어를 독일어로 번역합니다: "루이지가 종종 나에게 형제가 법정에 서는 것을 원하지 않는다고 말했다."라고 그녀가 썼다.
**원본 대상:**: "Luigi sagte oft zu mir, dass ernie wollte, dass die Bruder vor Gericht landen", schrieb sie.
**처리된 대상:**: "Luigi sagte oft zu mir, dass ernie wollte, dass die Bruder vor Gericht landen", schrieb sie.

### WMT English to French

**원본 입력:**: 스피처 망원경에 의한 적외선 기록의 이 이미지 섹션은 무수히 많은 세대의 별의 "가족 초상화"를 보여줍니다. 가장 오래된 별은 파란색 점으로 표시되지만 식별이 더 어려운 것은 별 분만실의 분홍색 "신생아"입니다.
**처리된 입력:**: 영어를 프랑스어로 번역: 스피처 망원경에 의한 적외선 기록의 이 이미지 섹션은 무수히 많은 세대의 별의 "가족 초상화"를 보여줍니다. 가장 오래된 별은 파란색 점으로 표시되지만 식별이 더 어려운 것은 별 분만실의 분홍색 "신생아"입니다.
**원본 대상:**: Ce details d'une photographic infrouge prise par le t telescope Spitzer montre un "portrait de famille" des innombrables generations d'etoiles: les plus vieilles etoiles sont en bleu et les, and difficilees identifier, sont les "nouveau-nes" dans la salle d'accouchement de l'univers.
**처리된 대상:**: Ce details d'une photographie infrouge prise par le telscope Spitzer montre un "portrait de famille" des innombrables generations d'etoiles: les plus vieilles etoiles sont en bleu et les, plus difficilees identifier, sont les "nouveau-nes" dans la salle d'accouchement de l'univers.

### WMT English to 루마니아

**원본 입력:**: 타코벨은 2022년까지 미국에 2,000곳을 추가할 계획이라고 말했습니다.
**처리된 입력:**: 영어를 루마니아어로 번역합니다. 타코벨은 2022년까지 미국에 2,000곳을 추가할 계획이라고 말했습니다.
**원본 대상:**: Taco Bell a afirmat cA, pana in 2022, intentioneazA sa deschidA  2000 de restaurante in SUA.

**처리된 대상:**: Taco Bell a afirmat cA, pana in 2022, intentioneaz@ sa deschida

수아에 있는 2000 데 레스토랑.

## 모든 실험에 대한 모든 작업에 대한 E 점수 부록

다음 표는 섹션 3.2에서 3.6에 설명된 실험의 모든 작업에서 달성된 점수를 나열한다.

## References

* Al-Rfou 등(2019) Rami Al-Rfou, Dokook Choi, Noah Constant, Mandy Guo, and Llion Jones. 더 깊은 자기 주의력을 가진 캐릭터 수준 언어 모델링입니다. _Proceedings of the AAAI Conference on Artificial Intelligence_ 2019.
* Anil 등(2019) Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. 대규모 학습을 위한 메모리 효율적인 적응 최적화 _ arXiv preprint arXiv:1901.11150_, 2019.
* Arivazhagan et al. (2019) Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. Massively multilingual neural machine translation in the wild: Findings and challenges. _ arXiv preprint arXiv:1907.05019_, 2019.
* Ba 등(2016) Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 레이어 정규화_ arXiv preprint arXiv:1607.06450_, 2016.
* Baevski 등(2019) Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. 셀프-어텐션 네트워크의 클로즈-구동 프리트레이닝. _ arXiv preprint arXiv:1903.07785_, 2019.
* Bahdanau et al.(2015) Dzmitry Bahdanau, Kyungghyun Cho, and Yoshua Bengio. 정렬 및 번역을 공동으로 학습하여 신경 기계 번역을 할 수 있습니다. _Third International Conference on Learning Representations_ 2015.
* Bapna 등(2019) Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. 신경 기계 번역을 위한 간단하고 확장 가능한 적응입니다. _ arXiv preprint arXiv:1909.08478_, 2019.
* Beltagy 등(2019) Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: 과학 텍스트에 대한 사전 훈련된 언어 모델. _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, 2019.
* Bojar et al.(2014) Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. Findings of the 2014 workshop on statistical machine translation. Proceedings of the 9th Workshop on Statistical Machine Translation_, 2014.
* Bojar 등(2015) Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck, Chris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, et al. Findings of 2015 workshop on statistical machine translation. Proceedings of the Tenth Workshop on Statistical Machine Translation_, 2015.
* Bojar et al.(2016) Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, et al. Findings of the 2016 conference on machine translation. _Proceedings of the First Conference on Machine Translation_ 2016.
* Bowman 등(2015) Samuel R. 보우먼, 루크 빌니스, 오리올 빈얄스 앤드류 M 다이, 라팔 조제포위츠, 사미 벵지오 연속된 공간에서 문장들을 생성하는 단계. _ arXiv preprint arXiv:1511.06349_, 2015.

* Buck 등(2014) Christian Buck, Kenneth Heafield, and Bas Van Ooyen. N-그램 수와 공용 크롤의 언어 모델입니다. In _LREC_, 2014.
* Caruana (1997) Rich Caruana. Multitask learning. _ Machine learning_, 28(1), 1997.
* Cer 등(2017) Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. _ arXiv preprint arXiv:1708.00055_, 2017.
* Cheng 등(2016) Jianpeng Cheng, Li Dong, and Mirella Lapata. 기계 판독을 위한 긴 단기 메모리 네트워크입니다. _ arXiv preprint arXiv:1601.06733_, 2016.
* Clark 등(2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: 자연스러운 예/아니오 질문의 놀라운 어려움을 탐구합니다. _ arXiv preprint arXiv:1905.10044_, 2019.
* Clark 등(2020) Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 일렉트라: 텍스트 인코더를 생성자가 아닌 식별자로 사전 교육합니다. _ arXiv preprint arXiv:2003.10555_, 2020.
* Conneau and Kiela (2018) Alexis Conneau and Douwe Kiela. SentEval: 보편적인 문장 표현을 위한 평가 툴킷. _ arXiv preprint arXiv:1803.05449_, 2018.
* Conneau 등(2017) Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. 자연어 추론 데이터로부터 범용 문장 표현의 지도 학습 _ arXiv preprint arXiv:1705.02364_, 2017.
* Dagan 등(2005) Ido Dagan, Oren Glickman, and Bernardo Magnini. PASCAL은 텍스트 수반 문제를 인식합니다. _Machine Learning Challengees Workshop_, 2005.
* Dai and Le(2015) Andrew M. 다이와 퀵 V. 레 준 지도 시퀀스 학습. _Advances in neural information processing systems_ 2015.
* De Marneff 등(2019) Marie-Catherine De Marneff, Mandy Simons, and Judith Tonhauser. 헌신 은행: 자연 발생 담론의 투영을 조사합니다. In _Sinn und Bedeutung 23_, 2019.
* Deng 등(2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: 대규모 계층 이미지 데이터베이스입니다. 2009 IEEE conference on computer vision and pattern recognition_ 2009.
* Devlin 등(2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: 언어 이해를 위한 깊은 양방향 트랜스포머의 사전 훈련. _ arXiv preprint arXiv:1810.04805_, 2018.
* Dolan and Brockett (2005) William B. Dolan and Chris Brockett. 센센셜 패러프레이즈 코퍼스를 자동으로 구성합니다. Proceedings of the Third International Workshop on Paraphrasing (IWP2005)_, 2005.
* Dong et al.(2019) Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 자연어 이해 및 생성을 위한 통일 언어 모델 사전 훈련. _ arXiv preprint arXiv:1905.03197_, 2019.

* Edunov 등(2018) Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. 규모에서 역번역을 이해 합니다. _ arXiv preprint arXiv:1808.09381_, 2018.
* Grave et al. (2018) Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov. 157개 언어에 대한 단어 벡터를 학습하는 단계. _ arXiv preprint arXiv:1802.06893_, 2018.
* Graves (2013) Alex Graves. 순환 신경망을 갖는 시퀀스들을 생성하는 단계 _ arXiv preprint arXiv:1308.0850_, 2013.
* Habernal et al. (2016) Ivan Habernal, Omnia Zayed, and Iryna Gurevych. C4Corpus: 무료 라이선스가 있는 다국어 웹 크기 말뭉치입니다. _Proceedings of the Tenth International Conference on Language Resources and Evaluation(LREC'16)_, pages 914-922, 2016.
*He 등(2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 이미지 인식을 위한 딥 레지듀얼 학습. Proceedings of the IEEE conference on computer vision and pattern recognition_, 2016.
*He 등(2018) Kaiming He, Ross Girshick, and Piotr Dollar. ImageNet 사전 교육을 다시 생각 합니다. _ arXiv preprint arXiv:1811.08883_, 2018.
*He et al. (2019) Pengcheng He, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. 상식추론을 위한 하이브리드 신경망 모델. _ arXiv preprint arXiv:1907.11983_, 2019.
* Hermann 등(2015) Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 기계가 읽고 이해할 수 있도록 가르칩니다. _Advances in neural information processing systems_ 2015.
* Hestness 등(2017) Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. 모스토파 알리 팻와리, 양양, 옌치저우. 딥 러닝 스케일링은 경험적으로 예측할 수 있습니다. _ arXiv preprint arXiv:1712.00409_, 2017.
* Hill 등(2016) Felix Hill, Cho경현, Anna Korhonen. 라벨이 지정되지 않은 데이터로부터 문장의 분산 표현을 학습하는 단계 _ arXiv preprint arXiv:1602.03483_, 2016.
* Hinton 등(2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 상기 지식을 신경망에서 증류하는 단계 _ arXiv preprint arXiv:1503.02531_, 2015.
* Houlsby 등(2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. NLP에 대한 파라미터-효율적인 전이 학습. _ arXiv preprint arXiv:1902.00751_, 2019.
* Howard and Ruder (2018) Jeremy Howard and Sebastian Ruder. 텍스트 분류를 위한 범용 언어 모델 미세 조정 _ arXiv preprint arXiv:1801.06146_, 2018.
* Huang 등(2018a) Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam Shazeer, Andrew M. 다이, 매튜 호프만, 모니카 딘쿨레스쿠, 더글러스 엑 음악 트랜스포머: 장기적인 구조의 음악을 생성합니다. _제7회 International Conference on Learning Representations_, 2018a.

* Huang 등(2018b) Yanping Huang, Yonglong Cheng, Dehao Chen, Hyouk중 Lee, Jiquan Ngiam, Quoc V Le, and Zhifeng Chen. GPipe: 파이프라인 병렬성을 이용한 거대 신경망의 효율적인 훈련. _ arXiv preprint arXiv:1811.06965_, 2018b.
* Huh et al.(2016) Minyoung Huh, Pulkit Agrawal, and Alexei A. Efros. ImageNet이 전이 학습에 좋은 이유는 무엇입니까? _ arXiv preprint arXiv:1608.08614_, 2016.
* Iyer 등(2017) Shankar Iyer, Nikhil Dandekar, and Kornel Csernai. 첫 번째 Quora 데이터 세트 릴리스: 질문 쌍. [https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs] (https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs), 2017.
* Jia 등(2014) Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: 빠른 특징 임베딩을 위한 컨볼루션 아키텍처. _Proceedings of the 22nd ACM international conference on Multimedia_, 2014.
* Jiao 등(2019) Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. TinyBERT: 자연어 이해를 위한 증류 BERT. _ arXiv preprint arXiv:1909.10351_, 2019.
* Joshi 등(2017) Mandar Joshi, Eunsol Choi, Daniel S. 웰드, 루크 제틀모이어 TriviaQA: 읽기 이해를 위한 대규모 원거리 감독 챌린지 데이터 세트입니다. _ arXiv preprint arXiv:1705.03551_, 2017.
* Joshi 등(2019) Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. 웰드, 루크 제틀모이어, 오머 레비 SpanBERT: 스팬을 나타내고 예측하여 사전 교육을 개선합니다. _ arXiv preprint arXiv:1907.10529_, 2019.
* Jozefowicz et al.(2016) Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. 언어 모델링의 한계를 탐구합니다. _ arXiv preprint arXiv:1602.02410_, 2016.
* Kalchbrenner 등(2014) Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 문장을 모델링하기 위한 컨볼루션 신경망. _Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics_ 2014.
* Keskar 등(2019a) Nitish Shirish Keskar, Bryan McCann, Lav R. 바쉬니, 카이밍 시옹, 리처드 사셔 CTRL: 제어가능한 생성을 위한 조건부 트랜스포머 언어 모델. _ arXiv preprint arXiv:1909.05858_, 2019a.
* Keskar 등(2019b) Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 스팬 추출을 통해 질문 응답 및 텍스트 분류를 통합합니다. _ arXiv preprint arXiv:1904.09286_, 2019b.
* Khashabi 등(2018) Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 겉으로 보기: 여러 문장에 대한 읽기 이해를 위한 도전 세트입니다. Proceedings of North American Chapter of the Association for Computational Linguistics(NAACL)_, 2018.

라이언 키로스, 유쿤 주, 루슬란 R. 살라쿠트디노프, 리처드 제멜, 라켈 우르타순, 안토니오 토랄바, 산자 피들러. Skip-thought vector. _Advances in neural information processing systems_ 2015.
* Kocijan 등(2019) Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas Lukasiewicz. 위노그라드 스키마 챌린지에 대한 놀랍도록 강력한 속임수입니다. _ arXiv preprint arXiv:1905.06290_, 2019.
* Konecny 등(2015) Jakub Konecny, Brendan McMahan, and Daniel Ramage. 페더레이션된 최적화: 데이터 센터를 넘어 분산 최적화입니다. _ arXiv preprint arXiv:1511.03575_, 2015.
* Konecny et al. (2016) Jakub Konecny, H. Brendan McMahan, Felix X. 유, 피터 리치타릭, 아난다 테어사 수레쉬 그리고 데이브 베이컨. 연합 학습: 커뮤니케이션 효율성 향상을 위한 전략 _ arXiv preprint arXiv:1610.05492_, 2016.
* Kornblith et al. (2018) Simon Kornblith, Jonathon Shlens, and Quoc V. 레 ImageNet 모델이 더 잘 전송되나요? _ arXiv preprint arXiv:1805.08974_, 2018.
* Krizhevsky (2014) Alex Krizhevsky. 컨볼루션 신경망을 병렬화 하는 한 가지 이상한 트릭입니다. _ arXiv preprint arXiv:1404.5997_, 2014.
* Kudo(2018) Taku Kudo. 서브워드 정규화: 다수의 서브워드 후보를 갖는 신경망 번역 모델 개선_ arXiv preprint arXiv:1804.10959_, 2018.
* Kudo and Richardson (2018) Taku Kudo and John Richardson. SentencePiece: 신경 텍스트 처리를 위한 간단하고 언어 독립적인 서브워드 토큰화기 및 디토크화기. _ arXiv preprint arXiv:1808.06226_, 2018.
* Lample and Conneau (2019) Guillaume Lample and Alexis Conneau. 교차-언어 언어 모델 사전 트레이닝. _ arXiv preprint arXiv:1901.07291_, 2019.
* Lan 등(2019) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Kimpel, Piyush Sharma, and Radu Soricut. ALBERT: 언어 표현의 자기 지도 학습을 위한 라이트 BERT. _ arXiv preprint arXiv:1909.11942_, 2019.
* Levesque 등(2012) Hector Levesque, Ernest Davis, and Leora Morgenstern. 위노그라드 스키마 도전입니다. 2012년 _제13회 지식 표현 및 추론 원리에 관한 국제 회의_에서.
*Li(2012)QiLi. 문헌조사: 자연어 처리를 위한 도메인 적응 알고리즘 2012.
* Lin (2004) Chin-Yew Lin. 요약 자동 평가 패키지입니다. 텍스트 요약은 2004년에 분기합니다.
* Liu 등(2018) Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 긴 시퀀스를 요약하여 위키피디아를 생성하는 단계 _ arXiv preprint arXiv:1801.10198_, 2018.
* Liu 등(2019a) Peter J. Liu, Yu-An Chung, and Jie Ren. SummAE: Zero-shot abstractive text summarization using length-agnostic auto-encoder. _ arXiv preprint arXiv:1910.00998_, 2019a.

* Liu 등(2015) Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. 의미 분류 및 정보 검색을 위한 멀티태스크 심층 신경망을 이용한 표현 학습. _Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, 2015.
* Liu 등(2019b) Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 자연어 이해를 위한 다중 작업 심층 신경망입니다. _ arXiv preprint arXiv:1901.11504_, 2019b.
* Liu(2019) Yang Liu. 추출 요약을 위해 BERT를 미세 조정합니다. _ arXiv preprint arXiv:1903.10318_, 2019.
* Liu 등(2019c) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: 견고하게 최적화된 BERT 사전 트레이닝 접근법. _ arXiv preprint arXiv:1907.11692_, 2019c.
* Logeswaran and Lee (2018) Lajanugen Logeswaran and Honglak Lee. 문장 표현을 학습하기 위한 효율적인 프레임워크. _ arXiv preprint arXiv:1803.02893_, 2018.
* Mahajan 등(2018) Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. 약하게 감독된 사전 훈련의 한계를 탐구합니다. _Proceedings of the European Conference on Computer Vision (ECCV)_, 2018.
* McCann 등(2018) Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answer. _ arXiv preprint arXiv:1806.08730_, 2018.
* Mikolov et al.(2013a) Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 벡터 공간에서 단어 표현의 효율적인 추정 _ arXiv preprint arXiv:1301.3781_, 2013a.
* Mikolov et al.(2013b) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. 코라도, 제프 딘 단어들 및 구절들의 분포된 표현들 및 이들의 구성성. _Advances in neural information processing systems_, 2013b.
* Nallapati 등(2016) Ramesh Nallapati, Bowen Zhou, Cicero Nogueira dos Santos, Caglar Gulcehre, and Bing Xiang. sequence-to-sequence RNNs 및 beyond를 이용한 추상적인 텍스트 요약. _ arXiv preprint arXiv:1602.06023_, 2016.
* Oquab 등(2014) Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. 컨볼루션 신경망을 사용하여 중간 수준의 이미지 표현을 학습하고 전달한다. Proceedings of the IEEE conference on computer vision and pattern recognition_, 2014.
* Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: 기계 번역의 자동 평가를 위한 방법. 계산 언어학에 대 한 연관성에 대 한 40 번째 연례 회의 진행률_ 에서입니다. 2002년 계산 언어학 협회
* Paulus 등(2017) Romain Paulus, Caiming Xiong, and Richard Socher. 추상적 요약을 위한 심층 강화 모델입니다. _ arXiv preprint arXiv:1705.04304_, 2017.
* Papineni 등(2018)* Pennington 등(2014) Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: 단어 표현을 위한 전역 벡터. _Proceedings of the 2014 Conference on empirical methods in natural language processing (EMNLP)_, 2014.
* Peters 등(2019) Matthew Peters, Sebastian Ruder, and Noah A. Smith. 조율을 하려고? 아니면 안 하려고? 미리 훈련된 표현을 다양한 작업에 적응합니다. _ arXiv preprint arXiv:1903.05987_, 2019.
* Peters 등(2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 심층 문맥화된 단어 표현입니다. _ arXiv preprint arXiv:1802.05365_, 2018.
*Phang 등(2018) Jason Phang, Thibault Fevry, and Samuel R. 보우먼 STILT에 대 한 문장 인코더: 중간 레이블이 지정 된 데이터 작업에 대 한 추가 교육 _ arXiv preprint arXiv:1811.01088_, 2018.
* Pilehvar and Camacho-Collados (2018) Mohammad Taher Pilehvar and Jose Camacho-Collados. WIC: 문맥-민감 표현들을 평가하기 위한 10,000개의 예시 쌍들. _ arXiv preprint arXiv:1808.09121_, 2018.
* 포스트(2018) Matt Post. BLEU 점수를 보고할 때 명확성을 요구 합니다. _ arXiv preprint arXiv:1804.08771_, 2018.
* Radford 등(2018) Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018년 생성 사전 교육을 통해 언어 이해력을 향상시킵니다.
* Radford 등(2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 언어 모델은 2019년 무감독 멀티태스크 학습자입니다.
* Rahman and Ng (2012) Altaf Rahman and Vincent Ng. 확정 대명사의 복잡한 사례 해결: 위노그라드 스키마 도전입니다. 2012년 자연어 처리 및 계산 자연어 학습에 관한 경험적 방법에 관한 공동 회의의 진행문_에서. 2012년 계산 언어학 협회
* Rajpurkar 등(2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 쿼드: 텍스트의 기계 이해를 위한 10만 개 이상의 질문입니다. _ arXiv preprint arXiv:1606.05250_, 2016.
* Ramachandran 등(2016) Prajit Ramachandran, Peter J. Liu, and Quoc V. 레 시퀀스 대 시퀀스 학습에 대한 감독되지 않은 사전 트레이닝. _ arXiv preprint arXiv:1611.02683_, 2016.
* Ratner 등(2018) Alex Ratner, Braden Hancock, Jared Dunnmon, Roger Goldman, and Christopher Re. 스노클 MeTaL: 다중 작업 학습을 위한 약한 감독입니다. _Proceedings of the Second Workshop on Data Management for End-To-End Machine Learning_, 2018.
* Roemmele 등(2011) Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. 그럴듯한 대안들의 선택: 상식적인 인과 추론에 대한 평가 2011년 _2011 AAAI 봄 심포지엄 시리즈_에서.
* Ruder(2017) Sebastian Ruder. 심층 신경망에서 다중 작업 학습에 대한 개요입니다. _ arXiv preprint arXiv:1706.05098_, 2017.
* Ruder (2019) Sebastian Ruder. _ 자연어 처리를 위한 신경 전이 학습_. PhD thesis, NUI Galway, 2019.
* Ruder 등(2019)* Ruder 등(2019) Sebastian Ruder, Matthew E. Peters, Swabha Swayamdipta, and Thomas Wolf. 자연어 처리에서의 학습 전이. _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials_, pages 15-18, 2019.
* Russakovsky et al.(2015) Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition challenge. _ 2015년 컴퓨터 비전 국제 학술지.
* Sanh 등(2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, 증류된 버전의 BERT: 더 작고, 빠르고, 저렴하고, 가볍습니다. _ arXiv preprint arXiv:1910.01108_, 2019.
* See 등(2017) Abigail See, Peter J. Liu, and Christopher D. Manning. 포인트로 이동합니다. 포인터 생성자 네트워크를 사용한 요약입니다. _ arXiv preprint arXiv:1704.04368_, 2017.
* Sennrich et al. (2015) Rico Sennrich, Barry Haddow, and Alexandra Birch. 하위 단어 단위를 사용 하 여 희귀 단어의 신경 기계 번역 _ arXiv preprint arXiv:1508.07909_, 2015.
* Shallue 등(2018) Christopher J Shallue, Lee Jaehoon, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E. Dahl. 데이터 병렬 처리가 신경망 학습에 미치는 영향을 측정합니다. _ arXiv preprint arXiv:1811.03600_, 2018.
* Shaw et al. (2018) Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 상대적 위치 표현들을 갖는 자기-주의 _ arXiv preprint arXiv:1803.02155_, 2018.
* Shazeer and Stern (2018) Noam Shazeer and Mitchell Stern. Adafactor: sublinear memory cost를 갖는 Adaptive learning rate. _ arXiv preprint arXiv:1804.04235_, 2018.
* Shazeer 등(2017) Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 대단히 큰 신경망: 희박하게 게이트된 혼합물-오-전문가 계층. _ arXiv preprint arXiv:1701.06538_, 2017.
* Shazeer 등(2018) Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJungong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and Blake Hechtman. 메쉬-텐서플로우: 슈퍼컴퓨터를 위한 딥 러닝. _Advances in Neural Information Processing Systems_, 2018.
* Smith 등(2013) Jason R. 스미스, 허브 생아만드, 막달레나 플라마다, 필립 코엔, 크리스 캘리슨 버치, 그리고 아담 로페즈. 일반적인 크롤에서 값싼 웹 규모 병렬 텍스트를 더럽힙니다. _Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics_, 2013.
* Socher 등(2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 감성 트리뱅크에 대한 의미 구성성에 대한 재귀적 심층 모델입니다. _Proceedings of the 2013 Conference on empirical methods in natural language processing_, 2013.
* Song et al.(2019) Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu. MASS: 언어 생성을 위한 시퀀스 대 시퀀스 사전 트레이닝 마스킹된 시퀀스. _ arXiv preprint arXiv:1905.02450_, 2019.

* Srivastava 등(2014) Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: 신경망이 과적합되는 것을 방지하는 간단한 방법. _ 2014년 기계 학습 연구 저널.
* Subramanian et al. (2018) Sandeep Subramanian, Adam Trischler, Yoshua Bengio, and Christopher J. Pal. 대용량 다중 작업 학습을 통해 범용 분산 문장 표현을 학습합니다. _ arXiv preprint arXiv:1804.00079_, 2018.
* Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V. 레 신경망을 이용한 시퀀스 투 시퀀스 학습 방법. _Advances in neural information processing systems_, 2014.
* Sutton(2019) Richard S. 서튼 쓴맛의 교훈. [http://www.incomplete] (http://www.incomplete) ideas.net/IncIdeas/BitterLesson.html, 2019.
* Taylor (1953) Wilson L. 테일러 "Cloze procedure": 가독성을 측정하는 새로운 도구입니다. _ 저널리즘 게시판, 1953년
* Trinh and Le(2018) Trieu H. Trinh and Quoc V. 레 상식추론을 위한 간단한 방법. _ arXiv preprint arXiv:1806.02847_, 2018.
* Trischler 등(2016) Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. NewsQA: 기계 이해 데이터 세트입니다. _ arXiv preprint arXiv:1611.09830_, 2016.
* Vaswani 등(2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. 고메즈, 루카스 카이저, 일리아 폴로수킨 관심만 있으면 됩니다. _Advances in neural information processing systems_, 2017.
* Voita 등(2019) Elena Voita, Rico Sennrich, and Ivan Titov. 트랜스포머에서 표현의 상향식 진화: 기계 번역 및 언어 모델링 목표를 사용한 연구입니다. _ arXiv preprint arXiv:1909.01380_, 2019.
* Wang 등(2018) Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. 보우먼 GLUE: 자연어 이해를 위한 멀티 태스크 벤치마크 및 분석 플랫폼. _ arXiv preprint arXiv:1804.07461_, 2018.
* Wang 등(2019a) Alex Wang, Jan Hula, Patrick Xia, Raghavendra Pappagari, R. Thomas McCoy, Roma Patel, Najoung Kim, Ian Tenney, Yinghui Huang, Katherin Yu, et al. Sesame Street를 통과하는 방법을 알려줄 수 있는가? 언어 모델링을 넘어서는 문장 수준 사전 훈련입니다. _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, 2019a.
* Wang 등(2019b) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. 보우먼 SuperGLUE: 범용 언어 이해 시스템을 위한 더 까다로운 벤치마크입니다. _ arXiv preprint arXiv:1905.00537_, 2019b.
* Wang 등(2019c) Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Liwei Peng, 및 Luo Si. StructBERT: 언어 구조를 깊은 언어 이해를 위한 사전 훈련에 통합. _ arXiv preprint arXiv:1908.04577_, 2019c.

* Warstadt et al. (2018) Alex Warstadt, Amanpreet Singh, and Samuel R. 보우먼 신경망 허용 여부 판단 _ arXiv preprint arXiv:1805.12471_, 2018.
* Williams 등(2017) Adina Williams, Nikita Nangia, and Samuel R. 보우먼 추론을 통한 문장 이해를 위한 광범위 도전 말뭉치. _ arXiv preprint arXiv:1704.05426_, 2017.
* Williams and Zipser (1989) Ronald J. Williams and David Zipser. 완전 순환 신경망을 지속적으로 실행하기 위한 학습 알고리즘 _ Neural computation_, 1989.
* Wu et al. (2016) Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. _ arXiv preprint arXiv:1609.08144_, 2016.
* Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. 레 XLNet: Generalized autoregressive pretraining for language understanding. _ arXiv preprint arXiv:1906.08237_, 2019.
* Yosinski 등(2014) Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 심층 신경망의 특징은 얼마나 전달할 수 있습니까? _Advances in neural information processing systems_, 2014.
* Yu 등(2018) Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V. 레 QAnet: 로컬 컨벌루션과 읽기 이해를 위한 글로벌 셀프-어텐션을 결합. _ arXiv preprint arXiv:1804.09541_, 2018.
* Zellers 등(2019) Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 신경성 가짜 뉴스에 맞서 싸우는 것 arXiv preprint arXiv:1905.12616_, 2019.
* Zhang et al.(2018) Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. ReCoRD: 인간과 기계 상식 읽기 이해의 격차를 좁힙니다. _ arXiv preprint arXiv:1810.12885_, 2018.
* Zhu 등(2019) Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Thomas Goldstein, and Jingjing Liu. Freelb: 언어 이해를 위한 향상된 적대적 훈련. _ arXiv preprint arXiv:1909.11764_, 2019.
* Zhu 등(2015) Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 책과 영화의 정렬: 영화를 보고 책을 읽음으로써 이야기 같은 시각적 설명을 향합니다. Proceedings of the IEEE international conference on computer vision_ 2015.
