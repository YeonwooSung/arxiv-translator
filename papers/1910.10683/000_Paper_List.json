{
    "1910.10683": {
        "paper_id": "1910.10683",
        "abs_url": "https://arxiv.org/abs/1910.10683",
        "pdf_url": "https://arxiv.org/pdf/1910.10683.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "1910.10683_Exploring_the_Limits_of_Transfer_Learning_with_a_Unified_Text-to-Text_Transformer.pdf",
        "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Colin Raffel",
            "Noam Shazeer",
            "Adam Roberts",
            "Katherine Lee",
            "Sharan Narang",
            "Michael Matena",
            "Yanqi Zhou",
            "Wei Li",
            "Peter J. Liu"
        ],
        "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
        "comments": "",
        "official_code_urls": [],
        "pwc_page_url": "https://paperswithcode.com/paper/exploring-the-limits-of-transfer-learning",
        "bibtex": "@misc{raffel2023exploring,\n      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, \n      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n      year={2023},\n      eprint={1910.10683},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}"
    }
}