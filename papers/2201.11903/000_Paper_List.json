{
    "2201.11903": {
        "paper_id": "2201.11903",
        "abs_url": "https://arxiv.org/abs/2201.11903",
        "pdf_url": "https://arxiv.org/pdf/2201.11903.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2201.11903_Chain-of-Thought_Prompting_Elicits_Reasoning_in_Large_Language_Models.pdf",
        "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Jason Wei",
            "Xuezhi Wang",
            "Dale Schuurmans",
            "Maarten Bosma",
            "Brian Ichter",
            "Fei Xia",
            "Ed Chi",
            "Quoc Le",
            "Denny Zhou"
        ],
        "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
        "comments": "",
        "official_code_urls": [],
        "pwc_page_url": "https://paperswithcode.com/paper/chain-of-thought-prompting-elicits-reasoning",
        "bibtex": "@misc{wei2023chainofthought,\n      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, \n      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},\n      year={2023},\n      eprint={2201.11903},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
    }
}