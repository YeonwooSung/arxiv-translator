{
    "2306.12509": {
        "paper_id": "2306.12509",
        "abs_url": "https://arxiv.org/abs/2306.12509",
        "pdf_url": "https://arxiv.org/pdf/2306.12509.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2306.12509_Joint_Prompt_Optimization_of_Stacked_LLMs_using_Variational_Inference.pdf",
        "title": "Joint Prompt Optimization of Stacked LLMs using Variational Inference",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Alessandro Sordoni",
            "Xingdi Yuan",
            "Marc-Alexandre C\u00f4t\u00e9",
            "Matheus Pereira",
            "Adam Trischler",
            "Ziang Xiao",
            "Arian Hosseini",
            "Friederike Niedtner",
            "Nicolas Le Roux"
        ],
        "abstract": "Large language models (LLMs) can be seen as atomic units of computation mapping sequences to a distribution over sequences. Thus, they can be seen as stochastic language layers in a language network, where the learnable parameters are the natural language prompts at each layer. By stacking two such layers and feeding the output of one layer to the next, we obtain a Deep Language Network (DLN). We first show how to effectively perform prompt optimization for a 1-Layer language network (DLN-1). Then, we present an extension that applies to 2-layer DLNs (DLN-2), where two prompts must be learned. The key idea is to consider the output of the first layer as a latent variable, which requires inference, and prompts to be learned as the parameters of the generative distribution. We first test the effectiveness of DLN-1 in multiple reasoning and natural language understanding tasks. Then, we show that DLN-2 can reach higher performance than a single layer, showing promise that we might reach comparable performance to GPT-4, even when each LLM in the network is smaller and less powerful.",
        "comments": "NeurIPS 2023",
        "official_code_urls": [
            "https://github.com/microsoft/deep-language-networks"
        ],
        "pwc_page_url": "https://paperswithcode.com/paper/deep-language-networks-joint-prompt-training",
        "bibtex": "@misc{sordoni2023joint,\n      title={Joint Prompt Optimization of Stacked LLMs using Variational Inference}, \n      author={Alessandro Sordoni and Xingdi Yuan and Marc-Alexandre C\u00f4t\u00e9 and Matheus Pereira and Adam Trischler and Ziang Xiao and Arian Hosseini and Friederike Niedtner and Nicolas Le Roux},\n      year={2023},\n      eprint={2306.12509},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
    }
}