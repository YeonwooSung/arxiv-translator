# LLMEval: A Preliminary Study on How to Evaluate Large Language Models

Yue Zhang\({}^{1}\)

These authors contributed equally.

Ming Zhang\({}^{1}\)

These authors contributed equally.

Haipeng Yuan\({}^{1}\)

Shichun Liu\({}^{1}\)

Yongyao Shi\({}^{3}\)

Tao Gui\({}^{2}\)

Qi Zhang\({}^{1}\)

Corresponding author.

Xuanjing Huang\({}^{1}\)

\({}^{1}\) School of Computer Science, Fudan University, Shanghai, China

\({}^{2}\) Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China

\({}^{3}\) Shanghai Advanced Institute of Finance, Shanghai Jiaotong University, Shanghai, China

yuezhang.fdu@gmail.com, mingzhang23@m.fudan.edu.cn, {fdyhp49,liusc2020}@gmail.com, yyshi.23@saif.sjtu.edu.cn {tgui,qz,xjhuang}@fudan.edu.cn

###### Abstract

Recently, the evaluation of Large Language Models has emerged as a popular area of research. The three crucial questions for LLM evaluation are "what, where, and how to evaluate". However, the existing research mainly focuses on the first two questions, which are basically what tasks to give the LLM during testing and what kind of knowledge it should deal with. As for the third question, which is about what standards to use, the types of evaluators, how to score, and how to rank, there hasn't been much discussion. In this paper, we analyze evaluation methods by comparing various criteria with both manual and automatic evaluation, utilizing onsite, crowd-sourcing, public annotators and GPT-4, with different scoring methods and ranking systems. We propose a new dataset, LLMEval and conduct evaluations on 20 LLMs. A total of 2,186 individuals participated, leading to the generation of 243,337 manual annotations and 57,511 automatic evaluation results. We perform comparisons and analyses of different settings and conduct 10 conclusions that can provide some insights for evaluating LLM in the future. The dataset and the results are publicly available at [https://github.com/llmeval](https://github.com/llmeval).

\({}^{1}\) School of Computer Science, Fudan University, Shanghai, China

\({}^{2}\) Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China

\({}^{3}\) Shanghai Advanced Institute of Finance, Shanghai Jiaotong University, Shanghai, China

yuezhang.fdu@gmail.com, mingzhang23@m.fudan.edu.cn, {fdyhp49,liusc2020}@gmail.com, yyshi.23@saif.sjtu.edu.cn {tgui,qz,xjhuang}@fudan.edu.cn

## Introduction

In recent years, Large Language Models (LLMs) have emerged as a highly significant and extensively explored area of research. As the capabilities of these LLMs continue to advance, it becomes increasingly crucial to assess their performance and understand their limitations. However, traditional metrics for generative models, for example, BLEU[12], ROUGE[13], WMD[14], MoverScore[15], can only capture one or a few aspects of the model's capabilities.

Recent research has started to explore the measurement of LLM from a more synthesized perspective. Those studies can be divided into two categories, automatic and manual evaluation, based on whether scores can be automatically calculated. There have been numerous efforts to carry out automatic evaluation. HELM[11] achieves synthesized evaluation by combining a large number of existing datasets. MMLU[1] employs multiple-choice questions for automated evaluation. C-Eval[16] is a Chinese benchmark similar to MMLU. AGIEval[15] utilizes both cloze tasks and multi-choice question-answering tasks simultaneously. Approaches like BERTScore[17] assign scores to outputs of LLMs by employing another LLM. As the capabilities of LLMs increasingly strengthen, apart from automated evaluations, manual evaluations are also an option. ChatBot Arena[17] allows public evaluator vote between two LLMs to rate them. AlpacaFarm[18] leverages API LLMs to mimic manual evaluations as a low-cost replacement.

In a recent survey [15], three questions are raised about LLM evaluation, "what, where and how to evaluate". "What to evaluate" is about determining the tasks for the LLMs to execute during evaluation. "Where to evaluate" discusses the knowledge domains in which to evaluate the LLMs. These two questions have been quite extensively discussed. However, there's less research on "how to evaluate," which refers to the specific methods for evaluation. This includes scoring criteria, grading approaches, ranking systems, and the type of annotators to use if manual evaluation is employed.

In this paper, we focus on "how to evaluate". As shown in Table 1, our study examines both manual evaluation and GPT-4 based automatic evaluation. Compared to LLM-as-a-Judge [17], our study employs a greater number of annotator types in manual evaluation. Besides that, we also compare various scoring criteria, grading methods, and ranking systems. In total, we gathered 243,337 manual annotations and 57,511 automatic evaluation results. We will release all the annotated data to Github when the anonymity period ends.

In general, when considering how to conduct an evaluation of an LLM, we come across three crucial questions that need to be addressed.

**Q1: Which criteria should we take into account when evaluating LLMs?** We can judge an LLM from various angles, like how accurate and fluent its answers are. But are all these criteria really needed? Could there be some aspects where all current LLMs have already done well enough, sofurther evaluation might not be necessary?

We conduct a comparison to the five criteria, accuracy, informativeness, fluency, logical coherence and harmlessness. The results show that across various criteria, existing LLMs all have demonstrated notable performance in terms of harmlessness. The differentiating factors lie in the metrics of informativeness and accuracy.

**Q2: Which annotation methods should be employed to annotate the output of LLMs?** We should consider how to score LLMs, whether to give each LLM's answer a separate score or have a competition between two LLMs answering the same question to determine the better one. Besides that, we should decide whether to evaluate them manually or automatically. If manual evaluation is applied, we also need to choose the type of annotators, onsite, crowd-sourcing, or public.

In this paper, we use a combination of onsite, crowd-sourcing, and public annotators for manual annotation and GPT-4 for automatic evaluation. Our experiments demonstrate that onsite evaluation exhibits superior accuracy and consistency in manual evaluations. We also find a higher alignment level between onsite annotators and GPT-4.

**Q3: Which ranking systems should be utilized to rank LLMs?** In evaluation methods that entail pairwise comparison, a ranking system is required to convert win/loss/draw outcomes in to scores.

In our study, we compare two commonly used ranking systems in competitive sports: the Elo rating system (used in chess games) and the Points scoring system(used in football matches). We discovered that the Elo rating system exhibits poor stability in LLM evaluation tasks. It demonstrates significant variance in results when different match sequences are considered and is highly sensitive to noise data which is difficult to avoid in manual annotation.

In general, our main contributions are in three folders: (1)We looked into the issue of "how to evaluate LLMs," comparing various criteria, different types of annotators, rating methods, and ranking approaches. (2)We introduced a fresh dataset called LLMEval and evaluated 20 models through both manual and automatic evaluations. (3)From our experimental results, we drew 10 conclusions that can offer some insights for future LLM evaluation.

## Design

In this section, we introduce our approach to criteria selection, annotation methods design and ranking system choice.

### Criteria

Generally speaking, LLM systems should follow the 3H principle(Askell et al., 2021), Helpfulness, Honesty, and Harmlessness. To more accurately evaluate these principles, we have further refined them into five rating criteria, accuracy, fluency, informativeness, logical coherence, and harmlessness. By considering these rating criteria, we are able to assess and evaluate the performance of LLM systems from the perspective of human preference.

**Accuracy** This evaluation aims to determine the accuracy of the answers, verifying the correctness and reliability of the provided information. A high-quality response is expected to be factually reliable.

**Fluency** This evaluation involves examining whether the answers adhere to natural language conventions. A high-quality response should be easily understandable, free from convoluted or ambiguous sentences that could hinder interpretation.

**Informativeness** This evaluation examines whether the answers provide sufficient and meaningful information. A high-quality response should be able to provide the questioner with useful and relevant information.

**Logical Coherence** This evaluation assesses whether the answers are logically rigorous and correct, determining the rationality of the presented viewpoints and arguments. A high-quality response should adhere to logical principles, demonstrating clear reasoning and coherence.

**Harmlessness** The evaluation examines whether the answers refrain from containing unethical information, ensuring compliance with ethical standards. A high-quality response should adhere to ethical principles, avoiding the dissemination of harmful or unethical information.

### Annotation Method

When considering the annotation method, we mainly discussed three issues: the scoring method, whether to use automatic or manual scoring, and the type of annotators to employ for manual scoring. We employ two different scoring methods, namely star scoring and pairwise comparison, with three different types of annotators, onsite, crowd-sourcing, and public. In addition to manual annotation, we perform automated evaluation using GPT-4, prompting the same scoring requirement and criteria as those of human annotators. Specifically, we utilize the following five settings.

\begin{table}
\begin{tabular}{l c c c}
**Studies** & & & & \\ \hline HELM(Liang et al., 2022) & ✓ & & \\ MMLU(Hendrycks et al., 2021) & ✓ & & \\ C-Eval(Huang et al., 2023) & ✓ & & & \\ AGIEval(Zhong et al., 2023) & ✓ & & & \\ BERTScore(Zhang et al., 2020) & ✓ & & & \\ AlpacaFarm(Dubois et al., 2023) & ✓ & & ✓ & \\ Chatbot Arena(Zheng et al., 2023) & ✓ & & ✓ \\
**Ours\({}^{\ddagger}\)** & ✓ & ✓ & ✓ & ✓ \\ \hline \multicolumn{4}{l}{\({}^{\dagger}\) Manual evaluation with different types of annotators} \\ \multicolumn{4}{l}{\({}^{\ddagger}\) Despite the _type of annotator_, our study also addresses the problems of _what criteria to use, how to score and how to rank_.} \\ \end{tabular}
\end{table}
Table 1: Evaluation Methods employed in LLM Evaluations

**Onsite Star Scoring** For the onsite annotators, they are instructed to evaluate the answers for each question based on five criteria with one to three stars.

**Crowd-sourcing Pairwise Comparison** For crowdsourcing annotators, we pair the responses from LLMs for the same question in a pairwise manner. These pairs are randomly presented side by side to the annotators. The annotators are asked to give an overall judgment of two responses and determine which response is better or if they are equally good. The option setting is similar to LLM-as-a-Judge[16].

**Public Pairwise Comparison** In the public pairwise comparison evaluation, we employ a method similar to crowd-sourcing, with the difference being that annotators are replaced by the general public. We've launched an evaluation website for the public annotators to conduct evaluations.

**GPT-4 Star Scoring** To compare manual evaluation with GPT-4 automated evaluation, we utilize the criteria used in onsite star scoring and the response of an LLM as inputs and conduct evaluations using the GPT-4 API. Please refer to the appendix for the input templates used.

**GPT-4 Pairwise Comparison** Similarly, we conduct evaluations using the GPT-4 API for the pairwise comparison annotation. Please refer to the appendix for the input templates used.

In all evaluations, a double-blind testing method is employed. The LLM name is concealed. Tasks are randomly assigned to different users.

### Ranking System

As mentioned above, we employ two scoring methods, star scoring and pairwise comparison. For star scoring annotation, we can utilize the average scores to rank the systems. However, when it comes to pairwise comparison annotation, determining the sorting method is also a research question. Therefore, we compared the Elo rating system(used in chess games) and the Points scoring system(used in football matches).

**Points Scoring System** This straightforward system awards points to participants based on their performance per match or event, disregarding the skill level of opponents. It focuses on absolute performance in each individual event, which is often used in football matches.

The points for Player A (\(P_{A}\)) before each game are represented as a summation of scores from all previous games. After each game, the points are updated using the formula:

\[P^{\prime}_{A}=P_{A}+S_{A} \tag{1}\]

Here, \(P^{\prime}_{A}\) denotes the updated points for Player A, and \(P_{A}\) stands for Player A's points before the game.

The scoring for Player A (\(S_{A}\)) from each game is represented as:

\[S_{A}=\begin{cases}1&\text{if Player A wins}\\ 0.5&\text{if the game is a draw}\\ 0&\text{if Player A loses}\end{cases} \tag{2}\]

In this formula, \(S_{A}\) represents the score gained by Player A from the game (1 for a win, 0.5 for a draw, 0 for a loss).

This system provides a clear, absolute reward for each individual performance, regardless of the relative skill levels of the competitors.

**Elo Rating System** The Elo rating system, initially devised for chess, is a method for quantifying the relative skill levels in player vs. player games. This system takes into account the skill level of opponents and dynamically adjusts the ratings based on the outcomes of each game.

The operation of the Elo rating system revolves around two key calculations. The first one predicts the expected score or winning probability for a player, computed using the formula:

\[E_{A}=\frac{1}{1+10^{(R_{B}-R_{A})/400}} \tag{3}\]

In this equation, \(E_{A}\) represents the expected score for player A, \(R_{A}\) denotes the current Elo rating for player A, and \(R_{B}\) symbolizes the current Elo rating for player B.

After the game concludes, player A's Elo rating gets updated using the following formula:

\[R^{\prime}_{A}=R_{A}+K\cdot(S_{A}-E_{A}) \tag{4}\]

Here, \(R^{\prime}_{A}\) signifies the updated Elo rating for player A, \(R_{A}\) denotes player A's prior Elo rating, \(K\) is a constant factor typically ranging from 10 to 40, which signifies the weight of the game, and \(S_{A}\) represents the actual game result for player A (1 for a win, 0.5 for a draw, and 0 for a loss). In our experiment, the \(K\) factor is set to 32, implying a moderate weight for each game.

## Experiments

In this section, we introduce our dataset and metrics used to evaluate annotation methods.

### Dataset

We constructed two datasets, LLMEval-1 and LLMEval-2, to conduct the evaluation of LLMs.

**LLMEval-1** To evaluate the aforementioned five criteria, we designed 17 different types of questions, including classification, code, conversation, factual questions, math solving, open questions, outline generation, paragraph generation, poetry, reading comprehension, reasoning, retrieval, rewrite, role-playing, story generation, summary, translation.

**LLMEval-2** To further investigate the effectiveness of LLMs in specialized domains, we developed the LLMEval-2 dataset. We selected a total of 12 academic subjects, including biological science, chemistry, Chinese language and literature, computer science, economics, foreign languages, law, mathematics, medicine, optics, physics, and social science. We created a set of questions for each subject comprised an equal number of both objective and subjective questions.

### Metrics

To objectively assess the annotation methods mentioned in the above section, we've established accuracy and consistency as measurable indicators. Their definitions are as follows.

**Accuracy** In order to assess the accuracy of different annotation methods, it is essential to establish the generation method for the ground truth. In this study, we calculate the average score of multiple annotators' results as the ground truth, \(gt\_score\). Additionally, we define an annotation as correct if the difference between the score given by an annotator and the ground truth is less than the standard deviation, \(\sigma\); otherwise, it is considered an incorrect annotation(Equation 5).

\[is\_correct=\begin{cases}1&abs(score-gt\_score)<\sigma\\ 0&otherwise\end{cases} \tag{5}\]

**Consistency** In all the evaluations, we include approximately 2% of repeated tasks to assess whether the annotator maintains consistent judgment criteria. For these repeated tasks, we conduct a statistical analysis of the annotations provided by each annotator. We calculate the proportion of consistent results by dividing the number of identical annotations by the total number of repeated tasks. This served as a measure of annotator consistency. For instance, if annotator A's annotations for task 1 were (1, 1, 1, 0) in four different attempts, the consistency rate would be calculated as 3/4, which is 75%.

To compare the quality of different annotators, we mixed the manually annotated results with the annotations generated by GPT-4 to compute the ground truth. We excluded user annotations with fewer than 5 results since we could not assess the quality of their annotations.

## Results

In this section, we compare different criteria, various annotation methods, and the ranking systems on the evaluation and give answers to the three questions we raise in the introduction section.

### Comparison of Criteria

To identify the most differentiating criteria, we utilize the results of manual star scoring evaluation. By comparing the scores of different models on various criteria, we can draw the following conclusions.

**1) The differentiating criteria are informativeness and accuracy.** Among all five criteria, all the LLMs in our test have performed well in terms of harmlessness. The most distinguishing criteria are accuracy and informativeness. Figure 1 demonstrates the scores of 5 models across five criteria. The top-ranked and bottom-ranked differ by 0.853 in terms of informativeness and by 0.776 in terms of accuracy.

**2) The task that best differentiates the capabilities of models is conversation.** Figure 2 shows the top-ranked LLM surpasses other models mainly in conversation, math solving and reasoning tasks. The score of GPT4.0 on the conversation task is 1.125 higher than ChatYuan-Large.

### Comparison of Annotation Methods

For the annotation methods, we want to figure out the best scoring method and type of annotator by comparing their accuracy and consistency. We also want to see if automatic evaluation can replace manual evaluation, or at least partially, by comparing their alignment. Our findings are as follows:

**3) Onsite annotators exhibit the best quality in terms of accuracy and consistency.** As shown in Figure 3, the average accuracy of onsite star scoring evaluations is 0.892, with a minimum accuracy of 0.825, higher than crowdsourcing and public pairwise comparison evaluation. The star scoring evaluation accuracy of GPT-4 is close to the human average, with a value of 0.908. The accuracy of GPT-4 in pairwise comparison evaluation is 0.688, indicating a greater discrepancy between human and GPT-4 evaluations in pairwise comparison, aligning with our previous findings. The consistency metric indicates a similar result.

**4) The public annotators show the lowest level of consistency and accuracy.** As depicted in Figure 3, public evaluations exhibit a considerable variance in both accuracy and consistency. The minimum accuracy is 0, while the lowest level of consistency is 0.3. It is important to note that these results are derived after excluding annotations from public annotators with fewer than 5 evaluations.

**5) The alignment between automated and manual evaluation is better under the setting of star scoring evaluation.** there exists a certain degree of discrepancy between manual evaluation and automated evaluation. To further elucidate the differences between them, we calculated the correlation coefficients among different ranks. As shown in Table 2, when using star scoring, the Spearman's correlation coefficient (\(\rho\)) betwe

Figure 1: Scoring of Different Criteria in LLMEval-1. Among all five criteria, all the LLMs in our test have performed well in terms of harmlessness. The most distinguishing criteria are accuracy and informativeness.

manual evaluation is 0.949, even higher than the correlation between manual star scoring and pairwise comparison. Meanwhile, The pairwise comparison between manual and GPT-4 evaluation exhibits the largest discrepancy in ranks. The Spearman's correlation coefficient (\(\rho\)) is 0.902. Compared to [23]'s study, our experimental results demonstrate that when using the star scoring evaluation method, the evaluation results of GPT-4 align more closely with manual evaluation.

**6) GPT-4 as an evaluator has a stronger bias on longer and more verbose responses than human evaluators.** As shown in Table 3, when there is a difference in length of more than 300 characters between two responses, GPT-4 has a 78.8% likelihood of selecting the longer text as the better one. In contrast, human annotators have a probability of 51.4% of choosing the longer text.

**7) Manual evaluation and GPT-4 automatic evaluation scores are less consistent on subjective questions.** In LLMEval-2, we have employed a broader range of domain-specific questions to evaluate LLMs. We also conduct manual and automatic evaluations for 20 different models across these domains. To assess the alignment between manual evaluation and GPT-4 auto evaluation in different question types, we calculated the proportion of questions with significant score differences. For objective questions, the proportion of accuracy score differences exceeding 2 points is 12.98%, while for subjective questions, this proportion increases to 37.05%. This phenomenon indicates that GPT-4 auto evaluation shows a higher level of consistency in judging objective questions with formatted answers. The proportion of questions with significant score differences for other criteria can be found in Table 4 and 5.

**8) Annotators tend to give higher scores when answer hints are not provided.** As mentioned earlier, for those evaluation questions with determined answers, we provided hints for annotators to refer to. We conducted additional manual annotation experiments to compare the impact of the

\begin{table}
\begin{tabular}{l c c} \hline
**Differences in Scores - Manual/GPT-4** & **\%** \\ \hline \(\Delta\)Accuracy \(\geq 2\) & 37.05\% \\ \(\Delta\)Accuracy \(\geq 4\) & 6.99\% \\ \(\Delta\)Fluency \(\geq 2\) & 3.49\% \\ \(\Delta\)Logicality \(\geq 2\) & 7.87\% \\ \(\Delta\)Informativeness \(\geq 2\) & 9.97\% \\ \hline \end{tabular}
\end{table}
Table 4: The proportion of the difference between manual evaluation and GPT-4 automatic evaluation of subjective questions in LLMEval-2

\begin{table}
\begin{tabular}{l l c c} \hline
**Annotator** & **Choice** & \(\boldsymbol{\Delta}\)length \(\boldsymbol{\geq 100}\) & \(\boldsymbol{\Delta}\)length \(\boldsymbol{\geq 300}\) \\ \hline \multirow{3}{*}{Human} & win & 32534(46.4\%) & 14679(51.4\%) \\  & draw & 30395(43.4\%) & 11360(39.8\%) \\  & loss & 7128(10.2\%) & 2523(8.8\%) \\ \hline \multirow{3}{*}{GPT-4} & win & 12183(73.3\%) & **5606(78.8\%)** \\  & draw & 1440(8.7\%) & 538(7.6\%) \\ \cline{1-1}  & loss & 2989(18.0\%) & 970(13.6\%) \\ \hline \end{tabular}
\end{table}
Table 3: Length Bias Comparison between Manual and GPT-4 Evaluation in LLMEval-1

Figure 3: Onsite annotators exhibit the best quality in terms of accuracy and consistency, higher than crowd-sourcing and public pairwise comparison evaluation

\begin{table}
\begin{tabular}{l c c} \hline
**Settings** & \(\boldsymbol{\rho}\) & \(\boldsymbol{\tau}\) \\ \hline Manual Star Scoring v.s. Pairwise & 0.938 & 0.839 \\ GPT-4 Star Scoring v.s. Pairwise & 0.965 & 0.878 \\ Star Scoring Manual v.s. GPT-4 & 0.949 & 0.839 \\ Pairwise Manual v.s. GPT-4 & 0.902 & 0.787 \\ \hline \end{tabular} 
\begin{tabular}{l c} A larger value of \(\boldsymbol{\rho}\) or \(\boldsymbol{\tau}\) indicates a higher level of alignment between two ranks. \\ \end{tabular}
\end{table}
Table 2: Spearman’s Correlation Coefficient(\(\rho\)) and Kendall Tau Correlation Coefficient(\(\tau\)) of Ranks under Different Settings in LLMEval-1

Figure 2: Scoring of Different Tasks in LLMEval-1. The top-ranked LLM surpasses other models mainly in conversation, math solving and reasoning tasks.

presence of hints on the scores. And the result is as follows. As shown in Figure 4, annotators gave scores that were on average 9.79% higher. This indicates that hints greatly assist annotators in identifying factual errors in LLMs.

### Comparison of Ranking Systems

In our study, we explored two ranking systems often used in pairwise comparison evaluations. Throughout the course of our study, we detected notable volatility in the rankings derived from the Elo rating system. Specifically, the rankings of LLMs exhibited dramatic shifts between consecutive time points. Different models presented only marginal differences, which led us to question the stability of the Elo rating system, especially when applied to large-scale annotations. Furthermore, the sequence of the evaluation process itself could potentially sway the final outcomes.

To validate our hypothesis, we calculate the variance of Elo rating scores. Given a user's annotated accuracy \(p\), we can estimate the variance of Elo rating scores, \(\mathrm{Var}[R_{A}^{\infty}]\) using the Equation 6 for an approximation. Due to limited space, please refer to the appendix for the complete derivation.

\[\begin{split}\mathrm{Var}[R_{A}^{\infty}]&=32^{2} \mathrm{Var}[S_{A}^{0}]\sum_{i=0}^{\infty}0.9264^{2i}\\ &=7211.27p(1-p)\end{split} \tag{6}\]

To illustrate this observation, we also conduct experiments with actual manual pairwise comparison results. And the result is as follows:

**9) The ranks generated by the Elo rating system continue to exhibit significant fluctuations even after 100,000 rounds of comparison.** We extracted the variations in ranks and scores resulting from pairwise comparisons conducted between rounds 100,000 and 100,100, and plotted them in Figure 5. Even though GPT-4 has won many times in the previous 100,000 rounds of comparisons, only a few recent losses are sufficient to impact the final ranking.

**10) The Elo rating system is sensitive to the order of matches, as different orderings can lead to different ranks.** To demonstrate this, we randomly selected 10,000 pairwise comparison results. Then we performed 10 random shufflings of this dataset and plotted the outcomes in Figure 6. Even with the same annotation results, simply by changing the order of the annotations, GPT-4's ranking exhibited fluctuations within the range of 1 to 3.

## Details

In this section, we provide more details that are not covered in the experiment section. We present the steps in the order they were conducted, including question collection, LLM response generation, and annotation process.

On LLMEval-1, we recruited 20 college students to contribute 15 to 25 questions each to form a question set. To facilitate the annotation process and mitigate the difficulty faced by annotators, answer hints have been provided for factual questions, coding and math-solving tasks. We collected 453 questions in 17 different tasks in total. Please refer to Figure 7 for the distribution of question types. Then, we collected 12 available open-source and

Figure 4: Annotators tend to give higher scores when answer hints are not provided

Figure 5: The fluctuation of Elo rating result after 100,000 rounds of pairwise comparison is still immense

\begin{table}
\begin{tabular}{l c} \hline \hline
**Differences in Scores - Manual/GPT-4** & **\%** \\ \hline \(\Delta\)Correctness \(\geq 3\) & 12.98\% \\ \(\Delta\)Explanation \(\geq 1\) & 24.98\% \\ \hline \hline \end{tabular}
\end{table}
Table 5: The proportion of the difference between manual evaluation and GPT-4 automatic evaluation of objective questions in LLMEval-2

Figure 6: In the Elo rating system, the same annotations can lead to changes in rank and score due to different orders.

commercial LLMs, and obtained responses from them. For each question, we initiated a new conversation to avoid potential interference from previous dialogues. We only considered the first response provided by the LLMs to ensure fairness. Our tests were conducted between May 1st and May 8th, 2023. Therefore, any updates made to these LLMs after May 8th will not be reflected in the results of this study. Eventually, we obtained a total of 5436 responses, comprising 29,898 pairs. All questions and answers are in Chinese. For each response, we sought star-scoring results from 3 onsite annotators. For each pair, we enlisted at least 3 crowd-sourcing or public annotators for pairwise comparison. We also shared our website for public annotation. Similarly, for these responses and pairs, we conducted an automated evaluation with GPT4 using scoring and pairwise comparison templates mentioned above. A total of 33 million tokens were consumed in this process.

On LLMEval-2, We evaluated 20 major open-source and commercial models. We conducted the LLMEval-2 from June 24th to July 10th, 2023, to delve deeper into the capabilities of LLMs in specialized domains. We recruited 12 college students from 12 distinct disciplines to formulate a question set. These questions were collected from the specific fields they each have been studying. For each discipline, we created around 25-30 objective and 10-15 subjective questions approximately, accumulating 480 questions in total. The evaluation criteria are similar to LLMEval-1, with a few modifications We set correctness and explanation correctness criteria for objective questions, and accuracy, fluency, informativeness, and logicality for subjective questions. The maximum score for objective questions is 5, and for subjective questions, it is 14 points. Correctness and accuracy are assigned a higher proportion of the total score. We exclude the criterion of harmlessness, as questions within academic disciplines seldom yield harmful outcomes. We utilized both onsite star scoring and GPT-4 star scoring for manual evaluation of 20 open-source and commercial models. A comparison of these two evaluation methods was also conducted.

## Related Works

Large Language Models(LLMs) have indeed achieved impressive results in many downstream tasks. Meanwhile, there are various approaches available for evaluating generative models. In earlier studies, the evaluation of generative models primarily relied on n-gram based, such as BLEU(Papineni et al., 2002), ROUGE(Lin, 2004) or embedding-based methods, such as WMD(Kusner et al., 2015), MoverScore(Zhao et al., 2019).

However these evaluation methods often only consider the model's performance on a limited set of tasks and fail to assess its overall capability, such as comparing the model's performance to human cognitive abilities. As LLMs continue to advance, they are approaching human-level cognitive abilities. Recent studies have made attempts to evaluate LLMs from a more comprehensive perspective. These methods can be broadly classified into automatic and manual evaluations.

**Automatic Evaluations** In NLP, there exist numerous benchmarks that have been developed. Some studies, such as HELM(Liang et al., 2022) have undertaken combinations of these benchmarks to evaluate LLMs. In other works, such as MMLU(Hendrycks et al., 2021), C-Eval(Huang et al., 2023) and AGIEval(Zhong et al., 2023), leverages multiple choices questions or cloze tasks to evaluate LLMs. The advantage of this is that for multiple-choice questions and cloze tasks, the answers are definite, and the scoring can be done automatically. While these methods excel in terms of knowledge coverage, we argue that they can not completely evaluate the fluency, coherence, and harmlessness of an LLM response simultaneously. To tackle the above issue, there have also been studies that employ the LLM itself as an evaluator, such as BERTScore(Zhang et al., 2020), GPTScore(Fu et al., 2023), GptEvaluator(Wang et al., 2023), FairEvaluators(Wang et al., 2023), and GEval(Liu et al., 2023). However, the evaluation results derived from LLM outputs often exhibit discrepancies compared to manual evaluations and are susceptible to factors such as response position and length.

**Manual Evaluations** Using manually annotated data as an evaluation criterion is expensive but essential. Many studies have incorporated a portion of manually annotated data as an evaluation methodology. AlpacaFarm(Dubois et al., 2023) proposed API LLMs to replace manual evaluations. Chatbot Arena(Zheng et al., 2023) tried to compare the differences between evaluation results from GPT-4 and humans. In our research, we have also conducted a similar comparison. Furthermore, we have examined the impact of different scoring methods, diverse annotator types, and various ranking systems on the evaluation results.

## Discussion

In our study, we discover that the most distinguishing criteria for evaluating LLMs are informativeness and accuracy. Moving forward, we will continue to prioritize these aspects in future evaluations.

Additionally, our research reveals that onsite star scoring was the optimal manual evaluation method in terms of accuracy, consistency and alignment between human and LLM evaluator. We will prefer this method in future work. Meanwhile, automated evaluation can cover a large number

Figure 7: Distribution of Tasks in LLMEval-1

of tasks in a short time and exhibits reasonable alignment with humans. It could be a complementary approach.

Another point worth mentioning is that the difference between automated evaluation and manual evaluation is most noticeable in subjective questions. Clearly, since there's no standard answer, evaluating LLM's performance in subjective questions is a challenging task.

## Acknowledgments

The authors wish to thank the anonymous reviewers for their helpful comments. This work was partially funded by National Natural Science Foundation of China (No.62206057,61976056,62076069), Shanghai Rising-Star Program (23QA1400200), Natural Science Foundation of Shanghai (23ZR1403500), and Program of Shanghai Academic Research Leader under grant 22XD1401100.

## References

* A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, J. Kernion, K. J. Ndousse, C. Olsson, D. Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah, and J. Kaplan (2021)A general language assistant as a laboratory for alignment. arXiv:2112.00861. Cited by: SS1.
* Y. Chang, X. Wang, J. Wang, Y. Wu, K. Chen, H. Yang, X. Wang, Y. Ye, Y. Zhang, and P. Yu (2023)A survey on evaluation of large language models. Cited by: SS1.
* Y. Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin, P. Liang, and T. B. Hashimoto (2023)AlpacaFarm: a simulation framework for methods that learn from human feedback. arXiv:2305.14387. Cited by: SS1.
* J. Fu, S. Ng, Z. Jiang, and P. Liu (2023)GPTScore: evaluate as you desire. arXiv:2302.04166. Cited by: SS1.
* D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt (2021)Measuring massive multitask language understanding. arXiv:2009.03300. Cited by: SS1.
* Y. Huang, Y. Bai, Z. Zhu, J. Zhang, T. Zhang, J. Su, C. Liu, Y. Zhang, J. Lei, Y. Fu, M. Sun, and J. He (2023)C-eval: a multi-level multi-discipline chinese evaluation suite for foundation models. arXiv:2305.08322. Cited by: SS1.
* M. J. Kusner, Y. Sun, N. I. Kolkin, and K. Q. Weinberger (2015)From word embeddings to document distances. In International Conference on Machine Learning, Cited by: SS1.
* P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, B. Newman, B. Yuan, B. Yan, C. Zhang, C. Cosgrove, C. D. Manning, C. De, C. Re, D. Acosta-Navas, D. Hudson, E. Zelikman, E. Durmus, F. Ladhak, F. Rong, H. Ren, H. Yao, J. Wang, K. Santhanam, L. Orr, L. Zheng, M. Yuksekgonl, M. Suzgun, N. Kim, N. Guha, N. Chatterji, O. Khattab, P. Henderson, Q. Huang, R. Chi, S. M. Xie, S. M. Santurkar, S. Ganguli, T. Hashimoto, T. Icard, T. Zhang, V. Chaudhary, W. Wang, X. Li, Y. Mai, and Y. Zhang (2022)Holistic evaluation of language models. arXiv:2211.09110. Cited by: SS1.
* C. Lin (2004)ROUGE: a package for automatic evaluation of summaries. In Annual Meeting of the Association for Computational Linguistics, Cited by: SS1.
* Y. Liu, D. Iter, Y. Xu, S. Wang, R. Xu, and C. Zhu (2023)G-eval: NLG evaluation using GPT-4 with better human alignment. arXiv:2303.16634. Cited by: SS1.
* K. Papineni, S. Roukos, T. Ward, and W. Zhu (2002)Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311-318. Cited by: SS1.
* J. Wang, Y. Liang, F. Meng, Z. Sun, H. Shi, Z. Li, J. Xu, J. Qu, and J. Zhou (2023)Is chatgPT a good nlg evaluator? a preliminary study. arXiv:2303.04048. Cited by: SS1.
* P. Wang, L. Li, L. Chen, D. Zhu, B. Lin, Y. Cao, Q. Liu, T. Liu, and Z. Sui (2023)Large language models are not fair evaluators. arXiv:2305.17926. Cited by: SS1.
* T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi (2020)BERTScore: evaluating text generation with bert. arXiv:1904.09675. Cited by: SS1.
* W. Zhao, M. Peyrard, F. Liu, Y. Gao, C. M. Meyer, and S. Eger (2019)MoverScore: text generation evaluating with contextualized embeddings and earth mover distance. arXiv:1909.02622. Cited by: SS1.
* L. Zheng, W. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica (2023)Judging lll-as-a-judge with mt-bench and chatbot arena. arXiv:2306.05685. Cited by: SS1.
* W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan (2023)AGIEval: a human-centric benchmark for evaluating foundation models. arXiv:2304.06364. Cited by: SS1.

## Appendix

### Dataset

In LLMEval-1, we designed 17 different types of questions from the perspective of cognitive psychology. These question types include: factual questions, open questions, translation, retrieval, code, role-playing, classification, outline generation, math solving, summary, reading comprehension, poetry, reasoning, paragraph generation, conversation, rewriting and story generation. Please refer to Figure 7 for the distribution of question types.

In LLMEval-2, we selected 12 academic subjects, including biological science, chemistry, Chinese language and literature, computer science, economics, foreign languages, law, mathematics, medicine, optics, physics, social science. We created a set of questions for each subject comprised of both objective and subjective questions. The distribution of subjects is illustrated in Figure 8.

### Mathematical Proof of Elo Rating Instability

To validate our hypothesis, we can consider two models A and B, both starting with an Elo rating of 1500. Let'sconsider Model A as our primary model and denote its Elo rating at time \(t\) as \(R_{A}^{t}\). At the beginning (\(t=0\)), we have \(R_{A}^{0}=R_{B}^{0}=1500\).

We assume that the answers from Model A always outperform Model B, and a judge evaluates the answers with an accuracy rate of \(p\). We denote \(S_{A}^{t}\) as the random variable indicating whether the judge correctly evaluated Model A at time \(t\), with \(S_{A}^{t}=1\) if the judgment is correct. In this case, \(S_{A}^{t}\) follows a Bernoulli distribution with parameter \(p\) on each trial.

The Elo rating for Model A at time \(t\) is then calculated as:

\[R_{A}^{t}=R_{A}^{t-1}+32(S_{A}^{t-1}-\frac{1}{1+10^{(3000-2R_{A}^{t-1})/400}}) \tag{7}\]

To roughly estimate the properties of \(R_{A}^{t}\) over time, we make a first-order linear approximation using Taylor's expansion:

\[R_{A}^{t}=R_{A}^{t-1}+32(S_{A}^{t-1}+\frac{15\ln 10}{2}-\frac{\ln 10}{200}R_{A}^{ t-1}) \tag{8}\]

By approximating \(\ln 10\) as 2.3, we can use the recursive equation 8 to get:

\[R_{A}^{t}=0.632R_{A}^{t-1}+32S_{A}^{t-1}+552 \tag{9}\]

As \(t\rightarrow+\infty\), from equation 9 we get:

\[R_{A}^{\infty}=32\sum_{i=0}^{t-1}0.632^{t-1-i}S_{A}^{i}+1500 \tag{10}\]

Assuming that \(S_{A}^{t}\) is independently and identically distributed over time, taking expectations on both sides, we get \(\mathrm{E}[R_{A}^{\infty}]=87p+1500\).

Furthermore, calculating variance on both sides:

\[\mathrm{Var}[R_{A}^{\infty}]=32^{2}\sum_{i=0}^{t-1}0.632^{2(t-1-i)}\mathrm{ Var}[S_{A}^{i}] \tag{11}\]

Since \(S_{A}^{t}\) is independently and identically distributed, \(\mathrm{Var}[R_{A}^{\infty}]=1706.67p(1-p)\).

However, while using Taylor's expansion for a first-order linear estimation, the approximation must be around 0 and higher-order terms are disregarded, leading to potentially imprecise results. Despite this, it can offer a conceptual framework for linear fitting.

We can approximate \(\frac{1}{1+10^{(3000-2R_{A}^{t-1})/400}}\) by linear fitting over the range of \(R_{A}^{t-1}\) from 1300 to 1700, giving \(0.0023R_{A}^{t-1}-2.9555\). This leads to the recursive formula:

\[R_{A}^{t}=0.9264R_{A}^{t-1}+32S_{A}^{t-1}+94.6 \tag{12}\]

Following similar steps, we get the expectation and variance:

\[\mathrm{E}[R_{A}^{\infty}] =32\mathrm{E}[S_{A}^{0}]\sum_{i=0}^{\infty}0.9264^{i}+\frac{94.6} {1-0.9264} \tag{13}\] \[=434.78p+1285.32\]

\[\mathrm{Var}[R_{A}^{\infty}] =32^{2}\mathrm{Var}[S_{A}^{0}]\sum_{i=0}^{\infty}0.9264^{2i} \tag{14}\] \[=7211.27p(1-p)\]

Drawing from the reasoning process outlined above, we conducted a thorough analysis of the Elo rating system. In a hypothetical scenario with 70% manual evaluation accuracy and an initial score of 1500, the estimated variance of the Elo rating increased to 1514. Across a dataset comprising 200,000 evaluation points, the Elo rating exhibited substantial fluctuations due to even a small proportion of noisy samples, underscoring its unsuitability for ranking large models. Persistent instability of the Elo rating system became evident even after 100,000 rounds, as depicted in Figure 5.

We also pinpointed substantial sequence dependence and instability in the Elo rating system. These deficiencies were especially apparent during large-scale evaluations or when handling numerous annotations. The high variance, as indicated in equation 14, further emphasized this instability, which became considerably acute with lower evaluator accuracy.

Equation 10 also unveiled the sequence dependence of the Elo rating system. It suggested an increasing influence of game outcomes based on their temporal closeness to the time index \(t\). Thus, the Elo system is overly sensitive to the sequence of wins and losses, leading to varying Elo ratings for game series with identical win-loss counts but differing sequences.

Recognizing the Elo rating system's shortcomings, namely, sequence dependence and instability, we advocate exploring alternative rating systems that could provide improved stability, immunity to sequence dependence, and resilience to evaluator accuracy. Herein, the Points scoring system stands out as a potential candidate, providing a significant departure from the Elo rating system.

The Points scoring system awards points purely based on individual performance in each event, independent of competitors' skill levels. A participant's total points represent a simple summation of scores from all prior games, updated post each game with the latest score. This performance-centric approach inherently nullifies sequence dependence

Figure 8: Distribution of Subjects in LLMEval-2

and significantly curtails potential ranking instability, even during large-scale evaluations or with extensive annotations.

In summary, the Points scoring system provides a stable, absolute, and sequence-independent evaluation structure. Its inherent robustness against evaluation sequences and indifference to the order of wins and losses propose it as a viable alternative for scenarios requiring extensive annotations.

### Implementation Details

For onsite annotators, they were instructed to evaluate the answers for each question based on the following five criteria with one to three stars. To ensure completion rate and accuracy, we provided a small cash incentive and informed users in advance that a portion of the reward would be deducted if their consistency with other users was low. Please refer to Figure 13, 14, 15 and 16 for GUI design.

* 1 star means completely wrong, 2 stars means partially correct, 3 stars means completely correct
* 1 star means lack of necessary information or off-topic response respect to following aspects with 1 to 3 stars.
* Accuracy: 1 star means Completely wrong 2 stars means Partially correct 3 stars means Completely correct

Informativeness: 1 star means Lack of necessary information or off-topic response 2 stars means Insufficient information to provide a complete answer 3 stars means Information is correct and sufficient.

Fluency: 1 star means Many grammar errors 2 stars means Generally fluent but with some grammar errors 3 stars means Language is fluent and in accordance with human conventions.

Logicality: 1 star means Confused and full of logical flaws 2 stars means Some logical issues present 3 stars means Logically sound.

Harmlessness: 1 star means Violates ethics or is objectionable 2 stars means Mostly compliant but with a few flaws 3 stars means Fully compliant with public morality.

user: [Question]

Imm: [LLM Output]

The correct answer to user's question is: [Hint]

Stars:

### Implementation Details

For onsite annotators, they were instructed to evaluate the answers for each question based on the following five criteria with one to three stars. To ensure completion rate and accuracy, we provided a small cash incentive and informed users in advance that a portion of the reward would be deducted if their consistency with other users was low. Please refer to Figure 13, 14, 15 and 16 for GUI design.

* 1 star means completely wrong, 2 stars means partially correct, 3 stars means completely correct
* 1 star means lack of necessary information or off-topic response, 2 stars means insufficient information to provide a complete answer, 3 stars means information is correct and sufficient.
* 1 star means confused and full of logical flaws, 2 stars means some logical issues present, 3 stars means logically sound.
* 1 star means many grammar errors, 2 stars means generally fluent but with some grammar errors, 3 stars means language is fluent and in accordance with human conventions.
* 1 star means violate ethics or is objectionable, 2 stars means mostly compliant but with a few flaws, 3 stars means fully compliant with public morality.

For crowd-sourcing annotators, we paired the responses from LLMs for the same question in a pairwise manner. These pairs were then randomly presented side by side to the annotators. The annotators were asked to give an overall judgment of two responses and choose from the following four options. The option setting is the same with [22].

* A: Left is better
* B: Right is better
* C: Tie
* D: Both are bad

In LLMEval-1, We use the following prompt templates for automatic evaluation of GPT-4. The content inside the square brackets will be replaced by specific text. Please refer

Figure 10: Prompts for Pairwise Comparison in LLMEval-1

Figure 9: Prompts for Star Scoring in LLMEval-1

[MISSING_PAGE_EMPTY:11]

Figure 14: GUI for Pairwise Comparison in LLMEval-1

Figure 13: GUI for Star Scoring in LLMEval-1

Figure 16: GUI for Subjective Questions in LLMEval-2

Figure 15: GUI for Objective Questions in LLMEval-2

[MISSING_PAGE_FAIL:14]

\begin{table}
\begin{tabular}{l c c} \hline
**LLM Name** & **Score** & **Rank** \\ \hline GPT4.0 & 0.701(0.894) & 1(1) \\ GPT3.5 & 0.643(0.818) & 2(2) \\ Baichuan-7B-Align & 0.603(0.621) & 3(4) \\ ChatGLM-6B & 0.579(0.547) & 4(5) \\ Xunfei-xinghuo & 0.550(0.623) & 5(3) \\ Chinese-LLAMA-7B & 0.506(0.457) & 6(7) \\ Ali-Tongyiqianwen & 0.491(0.507) & 7(6) \\ ChatYuan-Large & 0.426(0.245) & 8(12) \\ NewBing & 0.415(0.425) & 9(8) \\ Lilly-ChatFlow-13B & 0.398(0.339) & 10(9) \\ MOSS-16B & 0.377(0.272) & 11(10) \\ MOSS-w-Plugin-16B & 0.352(0.254) & 12(11) \\ \hline \end{tabular}

* The values in parentheses represent the evaluation scores or ranks provided by GPT-4.

\end{table}
Table 7: Pairwise Comparison Result in LLMEval-1 - Manual/GPT-4

\begin{table}
\begin{tabular}{l c c} \hline
**LLM** & **Correctness** & **Explanation** \\ \hline GPT4.0 & 2.378 (2.395) & 1.670 (1.595) \\ GPT3.5 & 2.160 (2.138) & 1.542 (1.503) \\ Xunfei-Xinghuo & 2.114 (2.243) & 1.557 (1.632) \\ Baichuan-13B-Chat & 2.003 (2.013) & 1.428 (1.441) \\ MiniMax-abab5 & 1.922 (1.928) & 1.443 (1.493) \\ NewBing & 2.197 (2.211) & 1.583 (1.615) \\ Claude & 1.923 (2.066) & 1.463 (1.576) \\ moss-mars & 1.961 (1.967) & 1.465 (1.470) \\ Kunlun-Tiangong & 1.933 (1.961) & 1.354 (1.500) \\ Ziya-LLaMA-13B-v1 & 1.681 (1.592) & 1.306 (1.201) \\ Ali-Tongyiqianwen & 1.638 (1.618) & 1.275 (1.280) \\
360 & 1.720 (1.678) & 1.322 (1.352) \\ CIIC-Zhigong & 1.680 (2.072) & 1.297 (1.516) \\ ChatGLM2-6B & 1.690 (1.671) & 1.345 (1.306) \\ Vicuna-33B & 1.567 (1.684) & 1.277 (1.270) \\ InternLM-7B & 1.655 (1.658) & 1.355 (1.174) \\ ChatGLM-130B & 1.602 (1.638) & 1.239 (1.280) \\ TigerBot-180B & 1.604 (1.592) & 1.294 (1.220) \\ AquilaChat-7b & 1.548 (1.553) & 1.239 (1.207) \\ BELLE-7B-2M & 1.484 (1.461) & 1.224 (1.164) \\ \hline \end{tabular}

\end{table}
Table 8: Ranking and Final Scores of LLMEval-2

\begin{table}
\begin{tabular}{l c c} \hline
**LLM** & **Ranking** & **Scores** \\ \hline GPT4.0 & 1(1) & 86.72 (89.54) \\ GPT3.5 & 2(2) & 80.71 (84.69) \\ Xunfei-Xinghuo & 3(5) & 78.05 (82.26) \\ Baichuan-13B-Chat & 4(6) & 77.51 (81.82) \\ MiniMax-abab5 & 5(7) & 77.47 (80.64) \\ NewBing & 6(4) & 77.28 (82.63) \\ Claude & 7(3) & 75.57 (83.49) \\ moss-mars & 8(9) & 74.41 (79.21) \\ Kunlun-Tiangong & 9(8) & 74.36 (79.31) \\ Ziya-LLaMA-13B-v1 & 10(13) & 69.48 (70.92) \\ Ali-Tongyiqianwen & 11(12) & 68.01 (71.02) \\
360 & 12(10) & 67.97 (72.86) \\ CIIC-Zhigong & 13(14) & 67.27 (70.53) \\ ChatGLM2-6B & 14(17) & 67.07 (69.06) \\ Vicuna-33B & 15(16) & 66.53 (69.16) \\ InternLM-7B & 16(18) & 66.52 (69.00) \\ ChatGLM-130B & 17(15) & 66.05 (69.48) \\ TigerBot-180B & 18(11) & 65.90 (71.77) \\ AquilaChat-7b & 19(19) & 64.82 (68.19) \\ BELLE-7B-2M & 20(20) & 62.98 (65.27) \\ \hline \end{tabular}

\end{table}
Table 8: Ranking and Final Scores of LLMEval-2

\begin{table}
\begin{tabular}{l c c} \hline
**LLM** & **Correctness** & **Explanation** \\ \hline GPT4.0 & 2.378 (2.395) & 1.670 (1.595) \\ GPT3.5 & 2.160 (2.138) & 1.542 (1.503) \\ Xunfei-Xinghuo & 2.114 (2.243) & 1.557 (1.632) \\ Baichuan-13B-Chat & 2.003 (2.013) & 1.428 (1.441) \\ MiniMax-abab5 & 1.922 (1.928) & 1.443 (1.493) \\ NewBing & 2.197 (2.211) & 1.583 (1.615) \\ Claude & 1.923 (2.066) & 1.463 (1.576) \\ moss-mars & 1.961 (1.967) & 1.465 (1.470) \\ Kunlun-Tiangong & 1.933 (1.961) & 1.354 (1.500) \\ Ziya-LLaMA-13B-v1 & 1.681 (1.592) & 1.306 (1.201) \\ Ali-Tongyiqianwen & 1.638 (1.618) & 1.275 (1.280) \\
360 & 1.720 (1.678) & 1.322 (1.352) \\ CIIC-Zhigong & 1.680 (2.072) & 1.297 (1.516) \\ ChatGLM2-6B & 1.690 (1.671) & 1.345 (1.306) \\ Vicuna-33B & 1.567 (1.684) & 1.277 (1.270) \\ InternLM-7B & 1.655 (1.658) & 1.355 (1.174) \\ ChatGLM-130B & 1.602 (1.638) & 1.239 (1.280) \\ TigerBot-180B & 1.604 (1.592) & 1.294 (1.220) \\ AquilaChat-7b & 1.548 (1.553) & 1.239 (1.207) \\ BELLE-7B-2M & 1.484 (1.461) & 1.224 (1.164) \\ \hline \end{tabular}

\end{table}
Table 9: Objective Question Scores of LLMEval-2

\begin{table}
\begin{tabular}{l l l l l} \hline
**LLM** & **Fluency** & **Accuracy** & **Logicality** & **Informativeness** \\ \hline GPT4.0 & 2.895 (2.989) & 4.260 (4.545) & 2.779 (2.903) & 2.691 (2.886) \\ GPT3.5 & 2.861 (3.000) & 3.822 (4.295) & 2.694 (2.818) & 2.489 (2.750) \\ Xunfei-Xinghuo & 2.815 (2.977) & 3.750 (4.193) & 2.560 (2.739) & 2.196 (2.716) \\ Baichuan-13B-Chat & 2.847 (2.949) & 3.727 (4.102) & 2.631 (2.778) & 2.472 (2.756) \\ MiniMax-abab5 & 2.878 (2.989) & 3.800 (3.977) & 2.656 (2.722) & 2.478 (2.699) \\ NewBing & 2.796 (2.989) & 3.608 (3.875) & 2.558 (2.773) & 2.061 (2.511) \\ Claude & 2.680 (2.977) & 3.597 (4.125) & 2.613 (2.801) & 2.414 (2.710) \\ moss-mars & 2.737 (3.000) & 3.480 (3.807) & 2.508 (2.648) & 2.229 (2.534) \\ Kunlun-Tiangong & 2.774 (2.983) & 3.520 (3.807) & 2.576 (2.682) & 2.339 (2.523) \\ Ziya-LLaMA-13B-v1 & 2.804 (3.000) & 3.207 (3.364) & 2.473 (2.585) & 2.120 (2.278) \\ Ali-Tongyiqianwen & 2.776 (3.000) & 3.098 (3.239) & 2.443 (2.511) & 2.126 (2.335) \\
360 & 2.700 (2.989) & 3.022 (3.352) & 2.394 (2.608) & 2.056 (2.313) \\ CIIC-Zhigong & 2.764 (2.983) & 3.067 (4.080) & 2.427 (2.744) & 1.916 (2.631) \\ ChatGLM2-6B & 2.758 (2.920) & 2.934 (3.011) & 2.401 (2.386) & 1.956 (2.210) \\ Vicuna-33B & 2.599 (2.943) & 3.033 (3.080) & 2.440 (2.398) & 2.143 (2.199) \\ InternLM-7B & 2.636 (2.847) & 3.091 (3.330) & 2.295 (2.392) & 1.938 (2.233) \\ ChatGLM-130B & 2.670 (2.926) & 3.022 (3.114) & 2.374 (2.443) & 2.084 (2.278) \\ TigerBot-180B & 2.573 (2.926) & 3.079 (3.557) & 2.489 (2.602) & 1.882 (2.352) \\ AquilaChat-7b & 2.710 (2.932) & 2.945 (3.136) & 2.383 (2.443) & 1.918 (2.244) \\ BELLE-7B-2M & 2.685 (2.824) & 2.695 (3.000) & 2.347 (2.335) & 1.880 (2.131) \\ \hline \end{tabular}
\end{table}
Table 10: Subjective Question Scores of LLMEval-2