{
    "2312.07398": {
        "paper_id": "2312.07398",
        "abs_url": "https://arxiv.org/abs/2312.07398",
        "pdf_url": "https://arxiv.org/pdf/2312.07398.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2312.07398_LLMEval_A_Preliminary_Study_on_How_to_Evaluate_Large_Language_Models.pdf",
        "title": "LLMEval: A Preliminary Study on How to Evaluate Large Language Models",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Yue Zhang",
            "Ming Zhang",
            "Haipeng Yuan",
            "Shichun Liu",
            "Yongyao Shi",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "abstract": ".",
        "comments": "",
        "official_code_urls": [],
        "pwc_page_url": "https://paperswithcode.com/paper/llmeval-a-preliminary-study-on-how-to",
        "bibtex": "@misc{zhang2023llmeval,\n      title={LLMEval: A Preliminary Study on How to Evaluate Large Language Models}, \n      author={Yue Zhang and Ming Zhang and Haipeng Yuan and Shichun Liu and Yongyao Shi and Tao Gui and Qi Zhang and Xuanjing Huang},\n      year={2023},\n      eprint={2312.07398},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}"
    }
}