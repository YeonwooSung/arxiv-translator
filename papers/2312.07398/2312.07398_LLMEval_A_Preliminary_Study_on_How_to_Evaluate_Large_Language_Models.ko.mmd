# LLMEval: 대용량 언어 모델 평가 방법에 대한 예비 연구

Yue Zhang\({}^{1}\)

이 작가들은 동등하게 기여했다.

Ming Zhang\({}^{1}\)

이 작가들은 동등하게 기여했다.

Haipeng Yuan\({}^{1}\)

Shichun Liu\({}^{1}\)

Yongyao Shi\({}^{3}\)

Tao Gui\({}^{2}\)

Qi Zhang\({}^{1}\)

Corresponding author.

Xuanjing Huang\({}^{1}\)

\({}^{1}\)중국 상하이 푸단대학교 컴퓨터과학부

\({}^{2}\) 중국 상하이 푸단대학교 현대언어언어연구소

\({}^{3}\) Shanghai Advanced Institute of Shanghai Jiaotong University, Shanghai, China

yuezhang.fdu@gmail.com, mingzhang23@m.fudan.edu.cn, {fdyhp49,liusc2020}@gmail.com, yyshi.23@saif.sjtu.edu.cn {tgui,qz,xjhuang}@fudan.edu.cn

###### Abstract

최근 대용량 언어 모델에 대한 평가가 대중적인 연구 분야로 떠오르고 있다. LLM 평가를 위한 세 가지 중요한 질문은 "무엇을, 어디서, 어떻게 평가할 것인가"이다. 그러나 기존 연구는 기본적으로 테스트 중 LLM에 어떤 과제를 주어야 하는지, 어떤 지식을 다루어야 하는지라는 처음 두 가지 질문에 주로 초점을 맞추고 있다. 어떤 기준을 사용할지, 평가자의 유형, 점수 매기기, 순위 매기기 등에 관한 세 번째 문항은 그동안 논의가 많지 않았다. 본 논문에서는 현장평가, 크라우드소싱, 퍼블릭 어노테이션, GPT-4를 활용하여 다양한 평가기준을 수동평가와 자동평가로 비교하여 평가방법 및 평가체계를 분석하였다. 본 논문에서는 새로운 데이터셋인 LLMEval을 제안하고, 20개의 LLMs에 대한 평가를 수행한다. 총 2,186명의 개인이 참여하여 243,337개의 수동 주석과 57,511개의 자동 평가 결과를 생성했다. 다양한 설정에 대한 비교 및 분석을 수행하고 향후 LLM을 평가하기 위한 몇 가지 통찰력을 제공할 수 있는 10가지 결론을 수행한다. 데이터 세트 및 결과는 [https://github.com/llmeval](https://github.com/llmeval)에서 공개적으로 사용할 수 있습니다.

\({}^{1}\)중국 상하이 푸단대학교 컴퓨터과학부

\({}^{2}\) 중국 상하이 푸단대학교 현대언어언어연구소

\({}^{3}\) Shanghai Advanced Institute of Shanghai Jiaotong University, Shanghai, China

yuezhang.fdu@gmail.com, mingzhang23@m.fudan.edu.cn, {fdyhp49,liusc2020}@gmail.com, yyshi.23@saif.sjtu.edu.cn {tgui,qz,xjhuang}@fudan.edu.cn

## Introduction

최근 LMM(Large Language Models)은 매우 중요하고 광범위하게 탐구되는 연구 분야로 부상하였다. 이러한 LLM의 기능이 계속 발전함에 따라 성능을 평가하고 한계를 이해하는 것이 점점 더 중요해지고 있다. 그러나, 생성 모델에 대한 전통적인 메트릭, 예를 들어, BLEU[12], ROUGE[13], WMD[14], MoverScore[15]는 모델의 능력의 하나 또는 몇 가지 측면만을 캡처할 수 있다.

최근 연구는 LLM의 측정을 보다 종합적 관점에서 탐구하기 시작했다. 이러한 연구는 점수를 자동으로 계산할 수 있는지 여부에 따라 자동 평가와 수동 평가의 두 가지 범주로 나눌 수 있다. 자동 평가를 수행하기 위한 많은 노력이 있어 왔다. HELM[11]은 많은 수의 기존 데이터 세트를 결합하여 합성된 평가를 달성한다. MMLU[1]는 자동 평가를 위해 객관식 문항을 사용한다. C-Eval[16]은 MMLU와 유사한 중국 벤치마크이다. AGIEval[15]은 클로즈 태스크와 객관식 질문-답변 태스크를 동시에 활용한다. BERTScore[17]와 같은 접근법은 다른 LLM을 사용하여 LLM의 출력에 점수를 할당한다. LLM의 역량이 점차 강화됨에 따라 자동화된 평가와 별개로 수동 평가도 옵션이다. 챗봇 아레나[17]는 두 LLM 사이의 공개 평가자 투표를 통해 등급을 매길 수 있다. 알파카팜[18]은 API LLM을 활용하여 수동 평가를 저비용 대체품으로 모방한다.

최근 설문[15]에서는 LLM 평가에 대해 "무엇을, 어디서, 어떻게 평가할 것인가"라는 3가지 질문이 제기된다. "평가할 것"은 평가 동안 LLM들이 실행할 태스크들을 결정하는 것에 관한 것이다. "평가할 곳"은 LLM을 평가할 지식 영역을 논의한다. 이 두 가지 질문은 상당히 광범위하게 논의되었다. 그러나 구체적인 평가 방법을 의미하는 "평가 방법"에 대한 연구는 부족한 실정이다. 여기에는 채점 기준, 채점 접근법, 순위 시스템 및 수동 평가가 사용되는 경우 사용할 주석기의 유형이 포함된다.

본 논문에서는 "평가 방법"에 초점을 맞춘다. 표 1과 같이, 우리의 연구는 수동 평가와 GPT-4 기반 자동 평가를 모두 조사한다. LLM-as-a-판사 [17]에 비해 본 연구는 수동 평가에서 더 많은 수의 주석기 유형을 사용한다. 그 외에도 다양한 채점 기준, 채점 방법 및 순위 시스템을 비교합니다. 총 243,337개의 수동 주석과 57,511개의 자동 평가 결과를 수집했다. 익명 기간이 종료되면 주석이 달린 모든 데이터를 Github로 공개합니다.

일반적으로 LLM의 평가를 수행하는 방법을 고려할 때 해결해야 할 세 가지 중요한 질문을 접하게 된다.

**Q1: LLM을 평가할 때 어떤 기준을 고려해야 하나요?** LLM의 답변이 얼마나 정확하고 유창한지와 같이 다양한 각도에서 LLM을 판단할 수 있습니다. 하지만 이 모든 기준이 정말 필요한가요? 현재 모든 LLM이 이미 충분히 잘 수행한 측면이 있을 수 있으며 추가 평가가 필요하지 않을 수 있다.

정확성, 정보성, 유창성, 논리적 일관성 및 무해성의 5가지 기준과 비교를 수행한다. 그 결과 다양한 기준에서 기존 LLM은 모두 무해성 측면에서 주목할 만한 성능을 보여주었다. 차별화 요인은 정보성과 정확성의 지표에 있다.

**Q2: LLM의 출력에 주석을 달려면 어떤 주석 방법을 사용해야 합니까?* * 각 LLM의 답변에 별도의 점수를 부여할지 또는 더 나은 질문을 결정하기 위해 동일한 질문에 답하는 두 LLM 간에 경쟁 여부를 고려하여 LLM을 점수를 매기는 방법을 고려해야 합니다. 그 외에도 수동으로 평가할지 자동으로 평가할지 결정해야 합니다. 수동 평가가 적용되는 경우, 어노테이션, 현장, 크라우드 소싱 또는 공개 유형을 선택해야 합니다.

본 논문에서는 수동 어노테이션은 현장 어노테이션, 크라우드 소싱, 공개 어노테이션과 자동 평가를 위한 GPT-4의 조합을 사용한다. 우리의 실험은 현장 평가가 수동 평가에서 우수한 정확도와 일관성을 나타냄을 보여준다. 또한 현장 주석기와 GPT-4 사이에 더 높은 정렬 수준을 발견했다.

**Q3: LLM의 순위를 매기기 위해 어떤 순위 시스템을 사용해야 하나요?* * 쌍별 비교를 수반하는 평가 방법에서는 승패/드래그 결과를 점수로 변환하기 위해 순위 시스템이 필요합니다.

본 연구에서는 경쟁 스포츠에서 일반적으로 사용되는 랭킹 시스템인 엘로 등급 시스템(체스 게임에서 사용됨)과 포인트 점수 시스템(축구 경기에서 사용됨)을 비교한다. 우리는 엘로 등급 시스템이 LLM 평가 작업에서 좋지 않은 안정성을 보인다는 것을 발견했다. 이는 서로 다른 일치 시퀀스를 고려할 때 결과의 상당한 편차를 보여주며 수동 주석에서 피하기 어려운 노이즈 데이터에 매우 민감하다.

일반적으로 우리의 주요 기여는 세 가지 폴더에 있다. (1) 다양한 기준, 다양한 유형의 주석기, 등급 방법 및 순위 접근 방식을 비교하여 "LLM을 평가하는 방법"의 문제를 조사했다. (2) LLMEval이라는 새로운 데이터 세트를 도입하고 수동 및 자동 평가를 통해 20개의 모델을 평가했다. (3) 실험 결과로부터 향후 LLM 평가에 대한 몇 가지 통찰력을 제공할 수 있는 10가지 결론을 도출했다.

## Design

이 섹션에서는 기준 선택, 주석 방법 설계 및 순위 시스템 선택에 대한 접근법을 소개한다.

### Criteria

일반적으로 LLM 시스템은 3H 원칙(Askell 등, 2021), Helpfulness, Honesty, Harmlessness를 따라야 한다. 이러한 원칙을 보다 정확하게 평가하기 위해 정확도, 유창성, 정보성, 논리적 일관성 및 무해성의 5가지 평가 기준으로 추가 정제했다. 이러한 평가 기준을 고려함으로써 인간의 선호 관점에서 LLM 시스템의 성능을 평가하고 평가할 수 있다.

**정확도** 이 평가는 답변의 정확도를 결정 하 여 제공 된 정보의 정확성과 신뢰성을 확인 하는 것을 목표로 합니다. 고품질의 대응은 사실적으로 신뢰할 수 있을 것으로 예상됩니다.

**유창성** 이 평가에는 답변이 자연어 규칙을 준수하는지 여부를 조사하는 작업이 포함됩니다. 양질의 응답은 해석을 방해할 수 있는 난해하거나 모호한 문장에서 벗어나 쉽게 이해할 수 있어야 한다.

**정보성** 이 평가는 답변이 충분 하 고 의미 있는 정보를 제공 하는지 여부를 조사 합니다. 질 높은 응답은 질문자에게 유용하고 관련된 정보를 제공할 수 있어야 한다.

**논리 일관성** 이 평가는 답변이 논리적으로 엄격하고 올바른지 여부를 평가 하 여 제시된 관점과 주장의 합리성을 결정 합니다. 질 높은 대응은 논리적 원칙을 준수하여 명확한 추론과 일관성을 보여야 한다.

**무해함** 평가는 답변이 비윤리적 정보를 포함 하는 것을 자제 하 여 윤리적 기준을 준수 하는지 여부를 조사 합니다. 질 높은 대응은 유해하거나 비윤리적 정보의 전파를 피하면서 윤리적 원칙을 준수해야 한다.

### Annotation Method

주석 방법을 고려할 때, 우리는 주로 채점 방법, 자동 채점 또는 수동 채점 사용 여부, 수동 채점에 사용할 주석기의 유형 등 세 가지 문제에 대해 논의했다. 우리는 스타 채점과 쌍별 비교라는 두 가지 다른 채점 방법을 사용하며, 현장, 크라우드 소싱 및 공개의 세 가지 다른 유형의 주석기와 함께 사용한다. 수동 주석 외에도 GPT-4를 사용하여 자동화된 평가를 수행하여 인간 주석자와 동일한 점수 요구 사항 및 기준을 프롬프트한다. 구체적으로 다음과 같은 5가지 설정을 사용합니다.

\begin{table}
\begin{tabular}{l c c c}
**Studies** & & & & \\ \hline HELM(Liang et al., 2022) & ✓ & & \\ MMLU(Hendrycks et al., 2021) & ✓ & & \\ C-Eval(Huang et al., 2023) & ✓ & & & \\ AGIEval(Zhong et al., 2023) & ✓ & & & \\ BERTScore(Zhang et al., 2020) & ✓ & & & \\ AlpacaFarm(Dubois et al., 2023) & ✓ & & ✓ & \\ Chatbot Arena(Zheng et al., 2023) & ✓ & & ✓ \\
**Ours\({}^{\ddagger}\)** & ✓ & ✓ & ✓ & ✓ \\ \hline \multicolumn{4}{l}{\({}^{\dagger}\) Manual evaluation with different types of annotators} \\ \multicolumn{4}{l}{\({}^{\ddagger}\) Despite the _type of annotator_, our study also addresses the problems of _what criteria to use, how to score and how to rank_.} \\ \end{tabular}
\end{table}
표 1: LLM 평가에 채용된 평가 방법

**사이트 별 점수** 사이트 주석의 경우 1~3개의 별이 있는 5가지 기준에 따라 각 질문에 대한 답변을 평가하도록 지시됩니다.

**크라우드 소싱 쌍별 비교** 크라우드 소싱 주석의 경우 동일한 질문에 대 한 LLM의 응답을 쌍별 방식으로 쌍으로 구성 합니다. 이러한 쌍은 주석자에게 무작위로 나란히 제시된다. 주석자는 두 응답에 대한 전반적인 판단을 내리고 어떤 응답이 더 나은지 또는 동등하게 좋은지 결정하도록 요청받는다. 옵션 설정은 LLM-as-a-Judge[16]와 유사합니다.

**공용 쌍별 비교** 공용 쌍별 비교 평가에서 크라우드 소싱과 유사한 방법을 사용 합니다. 차이점은 주석기가 일반 대중으로 대체 된다는 것입니다. 우리는 평가를 수행하기 위해 공개 주석자를 위한 평가 웹사이트를 시작했습니다.

**GPT-4 별 점수** 수동 평가를 GPT-4 자동 평가와 비교 하기 위해 현장 별 점수에 사용 된 기준과 LLM의 응답을 입력으로 사용 하 고 GPT-4 API를 사용 하 여 평가를 수행 합니다. 사용된 입력 템플릿은 부록을 참조하십시오.

**GPT-4 쌍별 비교** 마찬가지로 쌍별 비교 주석에 대해 GPT-4 API를 사용하여 평가를 수행합니다. 사용된 입력 템플릿은 부록을 참조하십시오.

모든 평가에는 이중 맹검 검사 방법이 사용된다. LLM 이름이 숨겨져 있습니다. 작업은 다른 사용자에게 무작위로 할당됩니다.

### Ranking System

위에서 언급한 바와 같이 스타 채점과 쌍별 비교의 두 가지 채점 방법을 사용한다. 별점 주석의 경우 평균 점수를 활용하여 시스템의 순위를 매길 수 있다. 그러나 쌍대 비교 주석의 경우 정렬 방법을 결정하는 것도 연구 문제이다. 따라서 본 연구에서는 Elo 등급제(체스 경기에 사용)와 Points 점수제(축구 경기에 사용)를 비교하였다.

**점수 매기기 시스템** 이 간단한 시스템은 상대의 기술 수준을 무시 하 고 경기 또는 이벤트 당 성능을 기준으로 참가자에게 점수를 부여 합니다. 축구 경기에서 자주 사용되는 개별 종목별 절대 성적에 초점을 맞춘다.

각 게임 전 플레이어 A(\(P_{A}\))의 점수는 모든 이전 게임의 점수 합으로 표시된다. 각각의 게임 후에, 포인트는 공식을 사용하여 업데이트된다:

\[P^{\prime}_{A}=P_{A}+S_{A} \tag{1}\]

여기서, \(P^{\prime}_{A}\)은 플레이어 A의 업데이트된 포인트를 의미하고, \(P_{A}\)은 게임 전 플레이어 A의 포인트를 의미한다.

각 게임에서 플레이어 A(\(S_{A}\))에 대한 스코어링은 다음과 같이 표시된다.

\[S_{A}=\begin{cases}1&\text{if Player A wins}\\ 0.5&\text{if game is a draw}\\ 0&\text{if Player A lose}\end{cases} \tag{2}\]

이 공식에서 \(S_{A}\)은 A 선수가 게임에서 얻은 점수(1승, 0.5 무승, 0패)를 나타낸다.

이 시스템은 경쟁업체의 상대적 기술 수준에 관계없이 개별 성과에 대해 명확하고 절대적인 보상을 제공합니다.

**Elo 등급 시스템** 처음에 체스를 위해 고안 된 Elo 등급 시스템은 플레이어 대 상대 기술 수준을 정량화하는 방법입니다. 플레이어 게임. 이 시스템은 상대의 기술 수준을 고려하여 각 게임의 결과에 따라 동적으로 등급을 조정한다.

엘로 등급제의 작동은 두 가지 핵심 계산을 중심으로 이루어진다. 첫 번째 것은 공식을 사용하여 계산된, 플레이어에 대한 예상 점수 또는 승리 확률을 예측한다:

\[E_{A}=\frac{1}{1+10^{(R_{B}-R_{A})/400}} \tag{3}\]

이 식에서, \(E_{A}\)은 플레이어 A의 예상 점수를 나타내고, \(R_{A}\)은 플레이어 A의 현재 엘로 등급을 나타내고, \(R_{B}\)은 플레이어 B의 현재 엘로 등급을 나타낸다.

게임이 종료된 후, 플레이어 A의 엘로 등급은 다음 공식을 사용하여 업데이트된다:

\[R^{\prime}_{A}=R_{A}+K\cdot(S_{A}-E_{A}) \tag{4}\]

여기서, \(R^{\prime}_{A}\)은 플레이어 A에 대한 업데이트된 Elo 등급을 의미하고, \(R_{A}\)은 플레이어 A의 이전 Elo 등급을 의미하며, \(K\)은 일반적으로 게임의 가중치를 의미하는 10에서 40까지의 상수 인자이며, \(S_{A}\)은 플레이어 A에 대한 실제 게임 결과(승리는 1, 무승리는 0.5, 패배는 0)를 나타낸다. 실험에서 \(K\) 인자는 32로 설정되었으며, 이는 각 게임에 대해 적당한 가중치를 의미한다.

## Experiments

이 섹션에서는 주석 방법을 평가하는 데 사용되는 데이터 세트 및 메트릭을 소개한다.

### Dataset

LLM 평가를 수행하기 위해 LLMEval-1과 LLMEval-2의 두 데이터 세트를 구성했다.

**LLMEval-1** 앞에서 언급한 5가지 기준을 평가하기 위해 분류, 코드, 대화, 사실 질문, 수학 해결, 공개 질문, 개요 생성, 단락 생성, 시, 읽기 이해, 추론, 검색, 다시 쓰기, 역할 놀이, 스토리 생성, 요약, 번역을 포함한 17가지 유형의 질문을 설계했습니다.

**LLMEval-2** 특수 도메인에서 LLM의 효과를 추가로 조사하기 위해 LLMEval-2 데이터 세트를 개발했습니다. 생물과학, 화학, 중국어 및 문학, 컴퓨터과학, 경제학, 외국어, 법학, 수학, 의학, 광학, 물리학, 사회과학 등 총 12개의 학문 과목을 선정하였다. 우리는 객관적인 질문과 주관적인 질문의 동일한 수로 구성된 각 주제에 대한 질문 세트를 만들었다.

### Metrics

위 절에서 언급한 주석 방법을 객관적으로 평가하기 위해 측정 가능한 지표로 정확성과 일관성을 확립했다. 이들의 정의는 다음과 같다.

**정확도** 다양한 주석 방법의 정확도를 평가하려면 Ground truth에 대한 생성 방법을 설정해야 합니다. 본 연구에서는 여러 주석자의 결과의 평균 점수를 Ground truth인 \(gt\_score\)으로 계산한다. 또한 주석이 주어진 점수와 Ground truth의 차이가 표준편차인 \(\sigma\)보다 작으면 정확한 주석으로 정의하고 그렇지 않으면 잘못된 주석으로 간주한다(식 5).

\[is\_correct=\begin{cases}1&abs(score-gt\_score)<\sigma\\ 0&otherwise\end{cases} \tag{5}\]

**일관성** 모든 평가에서 주석이 일관된 판단 기준을 유지 하는지 여부를 평가 하기 위해 반복 된 작업의 약 2%를 포함 합니다. 이러한 반복 작업에 대해 각 주석자가 제공한 주석의 통계 분석을 수행한다. 동일한 주석의 수를 총 반복 작업 수로 나누어 일관된 결과의 비율을 계산한다. 이것은 주석자 일관성의 척도 역할을 했다. 예를 들어, 태스크 1에 대한 주석자 A의 주석이 4가지 다른 시도에서 (1, 1, 1, 0)이면 일관성 비율은 75%인 3/4로 계산된다.

서로 다른 주석기의 품질을 비교하기 위해 수동으로 주석된 결과와 GPT-4에서 생성된 주석을 혼합하여 지상 진실을 계산했다. 주석의 품질을 평가할 수 없기 때문에 5개 미만의 결과로 사용자 주석을 제외했다.

## Results

이 섹션에서는 평가에 대한 다양한 기준, 다양한 주석 방법 및 순위 시스템을 비교하고 소개 섹션에서 제기한 세 가지 질문에 대한 답변을 제공한다.

### 기준 비교

가장 차별적인 기준을 식별하기 위해 수동 스타 점수 평가 결과를 활용한다. 다양한 기준에서 서로 다른 모델의 점수를 비교함으로써 다음과 같은 결론을 도출할 수 있다.

**1) 차별화 기준은 정보성과 정확성입니다.* * 5가지 기준 중 본 테스트의 모든 LLM은 무해성 측면에서 잘 수행되었습니다. 가장 구별되는 기준은 정확성과 정보성이다. 그림 1은 5가지 기준에 걸쳐 5가지 모델의 점수를 보여준다. 상위권과 하위권은 정보성 측면에서 0.853, 정확성 측면에서 0.776 차이가 난다.

**2) 모델의 기능을 가장 잘 차별화하는 작업은 대화입니다.* * 그림 2는 상위 LLM이 대화, 수학 해결 및 추론 작업에서 주로 다른 모델을 능가하는 것을 보여줍니다. 대화 과제에 대한 GPT4.0 점수는 챗원-라지보다 1.125점 높다.

### 주석 방법 비교

어노테이션 방법의 경우, 어노테이션의 정확도와 일관성을 비교하여 최적의 채점 방법과 유형을 파악하고자 한다. 우리는 또한 자동 평가가 수동 평가를 대체할 수 있는지 또는 적어도 부분적으로 정렬을 비교하여 확인하고 싶다. 연구 결과는 다음과 같다.

**3) 현장 주석기는 정확도와 일관성 측면에서 최상의 품질을 나타냅니다.* * 그림 3에서 볼 수 있듯이 현장 별 점수 평가의 평균 정확도는 0.892이며 최소 정확도는 0.825로 크라우드 소싱 및 공개 쌍별 비교 평가보다 높습니다. GPT-4의 스타 점수 평가 정확도는 0.908의 값으로 인간 평균에 가깝다. 쌍대 비교 평가에서 GPT-4의 정확도는 0.688로, 쌍대 비교에서 인간과 GPT-4 평가 사이의 불일치가 더 크다는 것을 나타낸다. 일관성 메트릭은 유사한 결과를 나타냅니다.

**4) 공용 주석기는 일관성과 정확성의 가장 낮은 수준을 보여 줍니다.* * 그림 3에 표시된 것처럼 공용 평가는 정확성과 일관성 모두에서 상당한 분산을 나타냅니다. 최소 정확도는 0인 반면, 가장 낮은 수준의 일관성은 0.3이다. 이러한 결과는 5개 미만의 평가를 가진 공개 주석기에서 주석을 제외한 후 도출된다는 점에 유의하는 것이 중요하다.

**5) 자동 평가와 수동 평가 간의 정렬은 별점 평가 설정에서 더 좋습니다.* * 수동 평가와 자동 평가 간에 어느 정도 불일치가 있습니다. 이들 간의 차이를 추가로 설명하기 위해 서로 다른 순위 간의 상관 계수를 계산했다. 표 2에 도시된 바와 같이, 스타 스코어링을 사용하는 경우, Spearman의 상관계수(\(\rho\)) 쌍

그림 1: LLMEval-1의 다른 기준 점수. 5가지 기준 중 우리 테스트의 모든 LLM은 무해성 측면에서 잘 수행되었다. 가장 구별되는 기준은 정확성과 정보성이다.

수동 평가는 0.949로 수동 별 점수와 쌍별 비교 간의 상관 관계보다 훨씬 높다. 한편, 수동 평가와 GPT-4 평가 간의 쌍대 비교에서 순위가 가장 큰 불일치를 나타낸다. Spearman의 상관계수(\(\rho\))는 0.902이다. [23]의 연구와 비교하여, 우리의 실험 결과는 별점수화 평가 방법을 사용할 때 GPT-4의 평가 결과가 수동 평가와 더 밀접하게 정렬됨을 보여준다.

**6) 평가자로서의 GPT-4는 인간 평가자보다 더 길고 장황한 응답에 대해 더 강한 편향을 갖는다.** 표 3에 나타낸 바와 같이, 두 응답 사이에 300자 이상의 길이의 차이가 있을 때, GPT-4는 더 긴 텍스트를 더 나은 텍스트로 선택할 가능성이 78.8%이다. 대조적으로, 인간 주석자는 더 긴 텍스트를 선택할 확률이 51.4%이다.

**7) 수동 평가 및 GPT-4 자동 평가 점수는 주관적 질문에 대해 일관성이 낮습니다.* * LLMEval-2에서는 LLM을 평가하기 위해 광범위한 도메인별 질문을 사용했습니다. 또한 이러한 도메인에 걸쳐 20가지 다른 모델에 대해 수동 및 자동 평가를 수행합니다. 다양한 질문 유형에서 수동 평가와 GPT-4 자동 평가 간의 정렬을 평가하기 위해 유의미한 점수 차이가 있는 질문의 비율을 계산했다. 객관적 문항의 경우 2점을 초과하는 정확도 점수 차이의 비율이 12.98%인 반면, 주관적 문항의 경우 이 비율이 37.05%로 증가한다. 이러한 현상은 GPT-4 자동 평가가 형식화된 답변으로 객관적인 질문을 판단하는 데 더 높은 수준의 일관성을 보인다는 것을 나타낸다. 다른 기준에 대해 유의미한 점수 차이가 있는 문항의 비율은 <표 4>와 <표 5>에서 확인할 수 있다.

**8) 주석기는 답변 힌트가 제공되지 않을 때 더 높은 점수를 부여하는 경향이 있습니다.* * 앞서 언급한 바와 같이 답변이 결정된 평가 질문에 대해 주석기가 참조할 힌트를 제공했습니다. 우리는 추가 수동 주석 실험을 수행하여 주석의 영향을 비교했다.

\begin{table}
\begin{tabular}{l c c} \hline
**Differences in Scores - Manual/GPT-4** & **\%** \\ \hline \(\Delta\)Accuracy \(\geq 2\) & 37.05\% \\ \(\Delta\)Accuracy \(\geq 4\) & 6.99\% \\ \(\Delta\)Fluency \(\geq 2\) & 3.49\% \\ \(\Delta\)Logicality \(\geq 2\) & 7.87\% \\ \(\Delta\)Informativeness \(\geq 2\) & 9.97\% \\ \hline \end{tabular}
\end{table}
표 4: LLMEval-2에서 주관식 문항의 수동평가와 GPT-4 자동평가의 차이 비율

\begin{table}
\begin{tabular}{l l c c} \hline
**Annotator** & **Choice** & \(\boldsymbol{\Delta}\)length \(\boldsymbol{\geq 100}\) & \(\boldsymbol{\Delta}\)length \(\boldsymbol{\geq 300}\) \\ \hline \multirow{3}{*}{Human} & win & 32534(46.4\%) & 14679(51.4\%) \\  & draw & 30395(43.4\%) & 11360(39.8\%) \\  & loss & 7128(10.2\%) & 2523(8.8\%) \\ \hline \multirow{3}{*}{GPT-4} & win & 12183(73.3\%) & **5606(78.8\%)** \\  & draw & 1440(8.7\%) & 538(7.6\%) \\ \cline{1-1}  & loss & 2989(18.0\%) & 970(13.6\%) \\ \hline \end{tabular}
\end{table}
표 3: LLMEval-1에서 수동 및 GPT-4 평가 간의 길이 편향 비교

도 3: 현장 어노테이션자가 크라우드 소싱 및 공개 쌍대 비교 평가보다 높은 정확도와 일관성 측면에서 최상의 품질을 나타냄

\begin{table}
\begin{tabular}{l c c} \hline
**Settings** & \(\boldsymbol{\rho}\) & \(\boldsymbol{\tau}\) \\ \hline Manual Star Scoring v.s. Pairwise & 0.938 & 0.839 \\ GPT-4 Star Scoring v.s. Pairwise & 0.965 & 0.878 \\ Star Scoring Manual v.s. GPT-4 & 0.949 & 0.839 \\ Pairwise Manual v.s. GPT-4 & 0.902 & 0.787 \\ \hline \end{tabular}
\begin{tabular}{l c} A larger value of \(\boldsymbol{\rho}\) or \(\boldsymbol{\tau}\) indicates a higher level of alignment between two ranks. \\ \end{tabular}
\end{table}
표 2: LLMEval-1에서 서로 다른 설정하의 Ranks의 Spearman's Correlation Coefficient(\(\rho\))와 Kendall Tau Correlation Coefficient(\(\tau\))

도 2: LLMEval-1에서 상이한 태스크의 스코어링. 상위 랭크된 LLM은 대화, 수학 풀이 및 추론 태스크에서 주로 다른 모델을 능가한다.

점수에 대한 힌트의 존재. 그리고 그 결과는 다음과 같다. 그림 4에서 볼 수 있듯이 주석자는 평균 9.79% 더 높은 점수를 주었다. 이는 힌트가 주석이 LLM의 사실적 오류를 식별하는 데 크게 도움이 된다는 것을 나타낸다.

### 랭킹 시스템 비교

본 연구에서는 쌍대 비교 평가에서 자주 사용되는 두 가지 순위 시스템을 탐색했다. 연구 과정에서 우리는 엘로 등급 시스템에서 파생된 순위에서 주목할 만한 변동성을 발견했다. 특히, LLM의 순위는 연속 시점 간에 극적인 변화를 보였다. 다른 모델은 한계 차이만 제시했으며, 이는 특히 대규모 주석에 적용될 때 Elo 등급 시스템의 안정성에 의문을 제기하게 했다. 또한 평가 과정의 순서 자체가 잠재적으로 최종 결과를 흔들 수 있다.

가설을 검증하기 위해 Elo 등급 점수의 분산을 계산한다. 사용자의 주석이 달린 정확도 \(p\)가 주어지면 근사화를 위해 식 6을 사용하여 Elo 평점 점수 \(\mathrm{Var}[R_{A}^{\infty}]\)의 분산을 추정할 수 있다. 공간이 한정되어 있어 전체 도출은 부록을 참조하시기 바랍니다.

\[\begin{split}\mathrm{Var}[R_{A}^{\infty}]&=32^{2} \mathrm{Var}[S_{A}^{0}]\sum_{i=0}^{\infty}0.9264^{2i}\\ &=7211.27p(1-p)\end{split} \tag{6}\]

이 관찰을 설명하기 위해 실제 수동 쌍별 비교 결과를 사용한 실험도 수행한다. 그리고 그 결과는 다음과 같다:

**9) Elo 등급 시스템에 의해 생성된 순위는 100,000 라운드의 비교 후에도 계속 상당한 변동을 나타냅니다.* * 100,000 라운드와 100,100 라운드 간에 수행된 쌍별 비교로 인한 순위 및 점수의 변동을 추출하여 그림 5에 표시합니다. GPT-4가 이전 100,000 라운드의 비교에서 여러 번 이겼음에도 불구하고 최근 몇 번의 손실만으로도 최종 순위에 영향을 미치기에 충분합니다.

**10) Elo 등급 시스템은 다른 순서가 다른 순위를 초래할 수 있으므로 일치 순서에 민감합니다.* * 이를 입증하기 위해 10,000개의 쌍별 비교 결과를 무작위로 선택했습니다. 그런 다음 이 데이터 세트의 10개의 무작위 셔플링을 수행하고 결과를 그림 6에 표시했다. 동일한 주석 결과에서도 단순히 주석의 순서를 변경함으로써 GPT-4의 순위는 1에서 3 범위 내에서 변동을 나타냈다.

## Details

이 섹션에서는 실험 섹션에서 다루지 않은 더 자세한 내용을 제공한다. 질문 수집, LLM 응답 생성 및 주석 프로세스를 포함하여 수행된 순서로 단계를 제시한다.

LLMEval-1에서는 20명의 대학생을 모집하여 각각 15~25개의 질문을 제공하여 질문 세트를 구성했다. 주석 과정을 용이하게 하고 주석자가 직면하는 어려움을 완화하기 위해 사실 질문, 코딩 및 수학 해결 과제에 대한 답변 힌트가 제공되었다. 총 17개의 다른 과제에서 453개의 질문을 수집했다. 질문 유형 분포는 그림 7을 참고하기 바란다. 그런 다음 12개의 오픈 소스를 수집했습니다.

도 4: 답변 힌트가 제공되지 않을 때 주석기가 더 높은 점수를 주는 경향이 있음

도 5: 쌍대비교를 100,000회 반복한 후의 Elo 등급 결과의 변동은 여전히 엄청나다

\begin{table}
\begin{tabular}{l c} \hline \hline
**Differences in Scores - Manual/GPT-4** & **\%** \\ \hline \(\Delta\)Correctness \(\geq 3\) & 12.98\% \\ \(\Delta\)Explanation \(\geq 1\) & 24.98\% \\ \hline \hline \end{tabular}
\end{table}
표 5: LLMEval-2에서 객관식 문항의 수동평가와 GPT-4 자동평가의 차이 비율

도 6: Elo 등급 시스템에서, 동일한 주석은 상이한 순서로 인해 순위 및 점수의 변화를 초래할 수 있다.

상업용 LLM, 그리고 그들로부터 응답을 얻었다. 각 질문에 대해 이전 대화에서 잠재적인 간섭을 피하기 위해 새로운 대화를 시작했다. 공정성을 보장하기 위해 LLM에서 제공하는 첫 번째 응답만 고려했다. 우리의 테스트는 2023년 5월 1일에서 5월 8일 사이에 수행되었다. 따라서 5월 8일 이후에 이러한 LLM에 대한 업데이트는 본 연구 결과에 반영되지 않을 것이다. 결국 29,898쌍으로 구성된 총 5436개의 응답을 얻었다. 모든 질문과 답변은 중국어로 되어 있습니다. 각 응답에 대해 3명의 현장 주석자로부터 별점 결과를 구했다. 각 쌍에 대해 쌍별 비교를 위해 최소 3개의 크라우드 소싱 또는 공용 주석기를 등록했다. 우리는 또한 공개 주석을 위해 웹사이트를 공유했습니다. 유사하게, 이러한 응답 및 쌍에 대해 위에서 언급한 점수 및 쌍별 비교 템플릿을 사용하여 GPT4로 자동화된 평가를 수행했다. 이 과정에서 총 3,300만 개의 토큰이 소비되었다.

LLMEval-2에서 20개의 주요 오픈 소스 및 상업용 모델을 평가했다. 우리는 전문 도메인에서 LLM의 능력을 더 깊이 조사하기 위해 2023년 6월 24일부터 7월 10일까지 LLMEval-2를 수행했다. 우리는 질문 세트를 공식화하기 위해 12개의 별개의 분야에서 12명의 대학생들을 모집했다. 이러한 질문은 각자 연구해 온 특정 분야에서 수집되었다. 각 학문에 대해 대략 25-30개의 객관적 질문과 10-15개의 주관적 질문을 생성하여 총 480개의 질문을 누적하였다. 평가 기준은 LLMEval-1과 유사하며 몇 가지 수정으로 객관적 질문에 대한 정확성 및 설명 정확성 기준을 설정하고 주관적 질문에 대한 정확성, 유창성, 정보성 및 논리적성을 설정한다. 객관적 문항의 최대 점수는 5점이며, 주관적 문항의 경우 14점이다. 정확도와 정확도는 전체 점수에서 더 높은 비율로 할당된다. 학문 분야 내의 질문이 유해한 결과를 낳는 경우가 거의 없기 때문에 무해성의 기준을 배제한다. 20개의 오픈 소스 및 상업용 모델의 수동 평가를 위해 현장 스타 채점과 GPT-4 스타 채점을 모두 활용했다. 이 두 평가 방법의 비교도 수행되었다.

## Related Works

대규모 언어 모델(LLM)은 실제로 많은 다운스트림 작업에서 인상적인 결과를 달성했다. 한편 생성 모델을 평가할 수 있는 다양한 접근법이 있다. 초기 연구에서 생성 모델의 평가는 주로 BLEU(Papineni et al., 2002), ROUGE(Lin, 2004)와 같은 n-gram 기반 또는 WMD(Kusner et al., 2015), MoverScore(Zhao et al., 2019)와 같은 임베딩 기반 방법에 의존했다.

그러나 이러한 평가 방법은 제한된 작업 집합에 대한 모델의 성능만을 고려하고 모델의 성능을 인간의 인지 능력과 비교하는 등 전반적인 능력을 평가하지 못하는 경우가 많다. LLM이 계속 발전하면서 인간 수준의 인지 능력에 접근하고 있다. 최근 연구에서는 LLM을 보다 포괄적인 관점에서 평가하려는 시도를 했다. 이러한 방법은 크게 자동평가와 수동평가로 분류할 수 있다.

**자동 평가** NLP에는 개발된 수많은 벤치마크가 있습니다. HELM(Liang et al., 2022)과 같은 일부 연구에서는 LLM을 평가하기 위해 이러한 벤치마크의 조합을 수행했다. MMLU(Hendrycks et al., 2021), C-Eval(Huang et al., 2023) 및 AGIEval(Zhong et al., 2023)과 같은 다른 작업에서는 LLM을 평가하기 위해 객관식 질문 또는 클로즈 작업을 활용한다. 선다형 문제와 클로즈 과제의 경우 정답이 확실하고 채점이 자동으로 이뤄질 수 있다는 점이 장점이다. 이러한 방법은 지식 범위 측면에서 우수하지만 LLM 반응의 유창성, 일관성 및 무해성을 동시에 완전히 평가할 수 없다고 주장한다. 위의 문제를 해결하기 위해 LLM 자체를 평가자로 사용하는 연구들도 있는데, BERTScore(Zhang et al., 2020), GPTScore(Fu et al., 2023), GptEvaluator(Wang et al., 2023), FairEvaluators(Wang et al., 2023), GEval(Liu et al., 2023) 등이 있다. 그러나 LLM 출력에서 파생된 평가 결과는 수동 평가에 비해 불일치를 나타내는 경우가 많으며 응답 위치 및 길이와 같은 요인에 취약하다.

**수동 평가** 수동으로 주석이 달린 데이터를 평가 기준으로 사용하는 것은 비용이 많이 들지만 필수적입니다. 많은 연구에서 평가 방법론으로 수동으로 주석이 달린 데이터의 일부를 통합했다. AlpacaFarm(Dubois et al., 2023)은 수동 평가를 대체하기 위해 API LLM을 제안했다. Chatbot Arena(Zheng et al., 2023)는 GPT-4의 평가 결과와 인간의 평가 결과의 차이를 비교하고자 하였다. 우리의 연구에서 우리는 또한 유사한 비교를 수행했다. 또한 다양한 채점 방법, 다양한 주석기 유형 및 다양한 순위 시스템이 평가 결과에 미치는 영향을 조사했다.

## Discussion

본 연구에서는 LLM을 평가하기 위한 가장 구별되는 기준이 정보성과 정확성이라는 것을 발견했다. 앞으로, 우리는 향후 평가에서 이러한 측면을 계속해서 우선시할 것입니다.

또한, 현장 스타 채점은 인간과 LLM 평가자 간의 정확도, 일관성 및 정렬 측면에서 최적의 수동 평가 방법임을 보여준다. 우리는 향후 작업에서 이 방법을 선호할 것입니다. 한편, 자동화된 평가는 많은 수를 커버할 수 있다

도 7: LLMEval-1에서의 태스크의 분포

짧은 시간에 작업을 수행하고 인간과의 합리적인 일치를 보여줍니다. 그것은 보완적인 접근일 수 있다.

자동평가와 수동평가의 차이가 주관식 문항에서 가장 두드러진다는 점도 언급할 만하다. 분명히 표준적인 답변이 없기 때문에 주관적인 질문에서 LLM의 성과를 평가하는 것은 어려운 작업이다.

## Acknowledgments

저자들은 익명의 평론가들에게 도움이 되는 논평에 감사하고 싶다. 이 작업은 부분적으로 중국 국립 자연 과학 재단(No.62206057,61976056,62076069), 상하이 라이징 스타 프로그램(23QA1400200), 상하이 자연 과학 재단(23ZR1403500), 보조금 22XD1401100에 따른 상하이 학술 연구 리더의 프로그램이 자금을 지원했다.

## References

* A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. 헨리건, A 존스, N. 조셉 만 다스사르마 엘하이지 Hatfield-Dodds, D. Hernandez, J. Kernion, K. J. Ndousse, C. Olsson, D. Amodei, T. 브라운 클라크 McCandlish, C. Olah, and J. Kaplan (2021)A general language assistant as a laboratory for alignment. arXiv:2112.00861. Cited by: SS1.
*Y. 장석 왕진영 우경 진호양 왕영 예영 Zhang, and P. Yu(2023)A survey on evaluation of large language models. 에 의해 인용된다: SS1.
*Y. 두부아 엑스 이록 타오리 Zhang, I. Gulrajani, J. Ba, C. Guestrin, P. Liang, and T. B. Hashimoto (2023)AlpacaFarm: a simulation framework for methods of learn from human feedback. arXiv:2305.14387. Cited by: SS1.
* J. Fu, S. 응진 Jiang, and P. Liu(2023)GPTScore: Evaluation as you desire. arXiv:2302.04166. Cited by: SS1.
* D. Hendrycks, C. Burns, S. 바사르트 Mazeika, D. Song, and J. Steinhardt (2021) Measure massive multitask language understanding. arXiv:2009.03300. Cited by: SS1.
*Y. 황영 배진 주장태 장재수 장재래 부민 Sun, and J. He (2023)C-eval: a multi-level multi-discipline Chinese evaluation suite for foundation models. arXiv:2305.08322. 인용: SS1.
* M. J. Kusner, Y. Sun, N. I. Kolkin, and K. Q. Weinberger (2015)From word embeddings to document distance. 기계 학습에 관한 국제 회의에서, 인용: SS1.
* P. Liang, R. 봄마사니 이두철 야수나가 장다라얀 Wu, A. Kumar, B. Newman, B. Yuan, B. Yan, C. Zhang, C. Cosgrove, C. D. Manning, C. De, C. Re, D. Acosta-Navas, D. Hudson, E. Zelikman, E. Durmus, F. Ladhak, F. Rong, H. Ren, H. Yao, J. Wang, K. 산타남 오레 정민 육세곤 수즈건 김남 구하남 오차테지 Khattab, P. Henderson, Q. 황록 지성민 강굴리 하시모토 아이카드티 장범 차우드리 왕석 이영 Mai, Y. Zhang(2022)Holistic evaluation of language models. arXiv:2211.09110. Cited by: SS1.
* C. Lin(2004)ROUGE: 요약의 자동 평가를 위한 패키지. 계산 언어학 협회의 연례 회의에서 SS1에 의해 인용되었습니다.
*Y. 류동익 서성 왕록 Xu, 및 C. Zhu(2023)G-eval: 보다 우수한 인간 정렬을 갖는 GPT-4를 사용한 NLG 평가. arXiv:2303.16634. Cited by: SS1.
*K. 파피네니 루코스 Ward, W. Zhu(2002)Bleu: 기계 번역의 자동 평가를 위한 방법. Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311-318. Cited by: SS1.
* J. Wang, Y. 양화멍 선현시 Li, J. Xu, J. Qu, J. Zhou (2023) chatgPT가 좋은 nlg 평가자인가? 예비 연구. arXiv:2303.04048. Cited by: SS1.
* P. Wang, L. 이락 진동주 조택 류태 Liu, Z. Sui(2023) 대형 언어 모델은 공정한 평가자가 아니다. arXiv:2305.17926. Cited by: SS1.
* T. 장범 Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi (2020)BERTScore: Evaluation text generation with bert. arXiv:1904.09675. Cited by: SS1.
* W. 조민 페이라드 가오, C. M. 마이어, S. Eger (2019)MoverScore: 문맥화된 임베딩 및 지구 이동자 거리로 평가하는 텍스트 생성. arXiv:1909.02622. Cited by: SS1.
* L. 정욱 장영 성승 장진 우영 장진 린진 Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica (2023) Judging lll-as-a-judge with mt-bench and chatbot arenna. arXiv:2306.05685. Cited by: SS1.
* W. 종록 최영 곽영 량석 류영 왕애식 Chen, N. Duan (2023)AGIEval: 기초 모델을 평가하기 위한 인간 중심 벤치마크. arXiv:2304.06364. Cited by: SS1.

## Appendix

### Dataset

LLMEval-1에서는 인지 심리학의 관점에서 17가지 유형의 질문을 설계했다. 이러한 질문 유형에는 사실 질문, 공개 질문, 번역, 검색, 코드, 역할-재생, 분류, 개요 생성, 수학 풀이, 요약, 독해, 시, 추론, 단락 생성, 대화, 재작성 및 스토리 생성이 포함된다. 질문 유형 분포는 그림 7을 참고하기 바란다.

LLMEval-2에서는 생물학, 화학, 중국어 및 문학, 컴퓨터과학, 경제학, 외국어, 법, 수학, 의학, 광학, 물리학, 사회과학 등 12개 학문 과목을 선정하였다. 객관적 질문과 주관적 질문으로 구성된 각 주제에 대한 질문 세트를 만들었다. 피험자의 분포는 그림 8에 나와 있다.

### Elo Rating Instability의 수학적 증명

우리의 가설을 검증하기 위해, 우리는 Elo 등급 1500으로 시작하는 두 모델 A와 B를 고려할 수 있다. 모델 A를 우리의 기본 모델로 간주하고 시간 \(t\)에서의 Elo 등급을 \(R_{A}^{t}\)으로 나타내자. 시작(\(t=0))에서 우리는 \(R_{A}^{0}=R_{B}^{0}=1500\)을 갖는다.

우리는 모델 A의 답이 항상 모델 B를 능가한다고 가정하고, 판사는 \(p\)의 정확도로 답을 평가한다. 우리는 판사가 시간 \(t\)에서 모델 A를 올바르게 평가했는지 여부를 나타내는 확률 변수로 \(S_{A}^{t}\)을 나타내며, 판사가 올바른 경우 \(S_{A}^{t}=1\)을 나타낸다. 이 경우, \(S_{A}^{t}\)은 각 시도에서 매개변수 \(p\)를 갖는 베르누이 분포를 따른다.

그런 다음 시간 \(t\)에서 모델 A에 대한 Elo 등급은 다음과 같이 계산된다:

\[R_{A}^{t}=R_{A}^{t-1}+32(S_{A}^{t-1}-\frac{1}{1+10^{(3000-2R_{A}^{t-1})/400}}) \tag{7}\]

시간 경과에 따른 \(R_{A}^{t}\)의 성질을 대략적으로 추정하기 위해 Taylor의 확장을 이용하여 1차 선형근사를 만든다.

\[R_{A}^{t}=R_{A}^{t-1}+32(S_{A}^{t-1}+\frac{15\ln 10}{2}-\frac{\ln 10}{200}R_{A}^{t-1}) \tag{8}\]

\(\ln 10\)을 2.3으로 근사하면 우리는 재귀 방정식 8을 사용하여 다음을 얻을 수 있다.

\[R_{A}^{t}=0.632R_{A}^{t-1}+32S_{A}^{t-1}+552 \tag{9}\]

\(t\rightarrow+\infty\)로서, 방정식 9로부터 다음을 얻는다:

\[R_{A}^{\infty}=32\sum_{i=0}^{t-1}0.632^{t-1-i}S_{A}^{i}+1500 \tag{10}\]

\(S_{A}^{t}\)이 시간에 따라 독립적이고 동일하게 분포한다고 가정하여 양쪽에 대한 기대치를 취하면 \(\mathrm{E}[R_{A}^{\infty}]=87p+1500\)을 얻을 수 있다.

나아가, 양측에 대한 분산을 계산하는 단계:

\[\mathrm{Var}[R_{A}^{\infty}]=32^{2}\sum_{i=0}^{t-1}0.632^{2(t-1-i)}\mathrm{ Var}[S_{A}^{i}] \tag{11}\]

\(S_{A}^{t}\)는 독립적이고 동일하게 분포하므로 \(\mathrm{Var}[R_{A}^{\infty}]=1706.67p(1-p)\).

그러나 1차 선형 추정을 위해 테일러의 확장을 사용하는 동안 근사치는 약 0이어야 하고 고차 항은 무시되어 잠재적으로 부정확한 결과를 초래한다. 그럼에도 불구하고 선형 피팅을 위한 개념적 프레임워크를 제공할 수 있다.

우리는 \(\frac{1}{1+10^{(3000-2R_{A}^{t-1})/400}}\)을 1300에서 1700까지의 \(R_{A}^{t-1}\) 범위에 걸쳐 선형 피팅으로 근사하여 \(0.0023R_{A}^{t-1}-2.9555\)을 얻을 수 있다. 이는 재귀 공식으로 이어진다:

\[R_{A}^{t}=0.9264R_{A}^{t-1}+32S_{A}^{t-1}+94.6 \tag{12}\]

유사한 단계에 따라 예상 및 분산을 가져옵니다.

\[\mathrm{E}[R_{A}^{\infty}] =32\mathrm{E}[S_{A}^{0}]\sum_{i=0}^{\infty}0.9264^{i}+\frac{94.6} {1-0.9264} \tag{13}\] \[=434.78p+1285.32\]

\[\mathrm{Var}[R_{A}^{\infty}] =32^{2}\mathrm{Var}[S_{A}^{0}]\sum_{i=0}^{\infty}0.9264^{2i} \tag{14}\] \[=7211.27p(1-p)\]

위에서 설명한 추론 과정을 바탕으로 Elo 등급 체계에 대한 철저한 분석을 수행했다. 수작업 평가 정확도가 70%이고 초기 점수가 1500인 가상 시나리오에서 Elo 등급의 추정된 분산은 1514로 증가했다. 200,000개의 평가 포인트로 구성된 데이터 세트에서 Elo 등급은 노이즈 샘플의 작은 비율도 상당한 변동을 나타내어 큰 모델의 순위를 매기는 데 적합하지 않음을 강조한다. 엘로 등급 시스템의 지속적인 불안정성은 그림 5와 같이 10만 회차 후에도 분명해졌다.

우리는 또한 Elo 등급 시스템에서 상당한 서열 의존성과 불안정성을 정확히 지적했다. 이러한 결함은 대규모 평가 또는 수많은 주석을 처리할 때 특히 분명했다. 방정식 14에 표시된 높은 분산은 이러한 불안정성을 더욱 강조했으며, 이는 더 낮은 평가자 정확도로 상당히 심각해졌다.

식 10은 Elo 등급제의 서열 의존성을 밝히기도 하였다. 시간 지수 \(t\)에 대한 시간적 친밀도에 따라 게임 결과의 영향력이 증가하는 것을 시사하였다. 따라서, 엘로 시스템은 승패의 시퀀스에 지나치게 민감하여, 동일한 승패 카운트를 갖지만 시퀀스가 다른 게임 시리즈에 대해 다양한 엘로 등급을 초래한다.

엘로 등급 시스템의 단점, 즉 서열 의존성과 불안정성을 인식하여 개선된 안정성, 서열 의존성에 대한 면역성 및 평가자 정확성에 대한 회복력을 제공할 수 있는 대체 등급 시스템을 탐색하는 것을 옹호한다. 여기에서 포인트 점수 체계는 잠재적인 후보로 두드러져 엘로 점수 체계에서 상당한 이탈을 제공한다.

포인트 스코어링 시스템은 순전히 경쟁사의 기술 수준과 무관하게 각 종목에서 개인의 성과에 따라 포인트를 수여한다. 참가자의 총점은 모든 이전 게임의 점수를 단순 합산하여 각 게임을 최신 점수로 업데이트한 게시물을 나타낸다. 이 성능 중심 접근 방식은 본질적으로 서열 의존성을 무효화한다.

도 8: LLMEval-2의 피험자 분포

그리고 대규모 평가 중이나 광범위한 주석이 있는 경우에도 잠재적인 순위 불안정성을 크게 줄인다.

요약하면, 포인트 점수 시스템은 안정적이고 절대적이며 서열 독립적인 평가 구조를 제공한다. 평가 시퀀스에 대한 고유한 견고성과 승패 순서에 대한 무관심은 광범위한 주석이 필요한 시나리오에 대한 실행 가능한 대안으로 제안한다.

### Implementation Details

현장 주석자의 경우 1~3개의 별표로 다음 5가지 기준에 따라 각 질문에 대한 답변을 평가하도록 지시하였다. 완성률과 정확성을 보장하기 위해 작은 현금 인센티브를 제공하고 다른 사용자와의 일관성이 낮으면 보상의 일부가 공제될 것이라고 미리 사용자에게 알렸다. GUI 디자인은 도 13, 도 14, 도 15 및 도 16을 참조하기 바란다.

* 1 별은 완전히 틀렸다는 의미, 2 별은 부분적으로 옳다는 의미, 3 별은 완전히 옳다는 의미
* 1 별은 1에서 3 별을 사용 하 여 다음 측면에 대 한 필수 정보 부족 또는 주제 외 응답을 의미 합니다.
* 정확도: 1 별은 완전히 틀렸다는 의미 2 별은 부분적으로 정확하다는 의미 3 별은 완전히 정확하다는 의미

정보성: 1 별은 필요한 정보 부족 또는 주제 외 응답 2 별은 완전한 답변을 제공하기 위한 불충분한 정보 3 별은 정보가 정확하고 충분하다는 것을 의미한다.

유창성: 1 별은 많은 문법 오류 2 별은 일반적으로 유창하지만 일부 문법 오류 3 별은 언어가 유창하고 인간의 관습에 따른다는 것을 의미한다.

논리성: 1개의 별은 혼란스럽고 논리적 결함으로 가득 차 있다는 것을 의미한다 2개의 별은 몇 개의 논리적 문제가 존재하는 3개의 별은 논리적으로 건전하다는 것을 의미한다.

무해함: 1개의 별은 윤리 위반을 의미하거나 불쾌함을 의미한다. 2개의 별은 대부분 순응하지만 몇 가지 결함이 있는 3개의 별은 공공 도덕을 완전히 준수한다는 것을 의미한다.

user: [Question]

Imm : [LLM Output]

사용자의 질문에 대한 정답은 다음과 같다 : [힌트]

Stars:

### Implementation Details

현장 주석자의 경우 1~3개의 별표로 다음 5가지 기준에 따라 각 질문에 대한 답변을 평가하도록 지시하였다. 완성률과 정확성을 보장하기 위해 작은 현금 인센티브를 제공하고 다른 사용자와의 일관성이 낮으면 보상의 일부가 공제될 것이라고 미리 사용자에게 알렸다. GUI 디자인은 도 13, 도 14, 도 15 및 도 16을 참조하기 바란다.

* 1 별은 완전히 틀렸다는 의미, 2 별은 부분적으로 옳다는 의미, 3 별은 완전히 옳다는 의미
* 1 별은 필요한 정보의 부족 또는 주제 외 응답을 의미하고, 2 별은 완전한 답변을 제공하기에는 불충분한 정보를 의미하고, 3 별은 정보가 정확하고 충분하다는 것을 의미한다.
* 1 별은 혼란스럽고 논리적 결함으로 가득 찬 것을 의미하고, 2 별은 몇 가지 논리적 문제가 있는 것을 의미하고, 3 별은 논리적으로 건전한 것을 의미합니다.
*1 별은 많은 문법 오류를 의미하고, 2 별은 일반적으로 유창하지만 일부 문법 오류가 있는 것을 의미하고, 3 별은 언어가 유창하고 인간의 관습에 따른 것을 의미한다.
* 1개의 별은 윤리를 위반하거나 불쾌하다는 의미, 2개의 별은 대부분 준수하지만 몇 가지 결함이 있는 경우, 3개의 별은 공공 도덕성을 완전히 준수한다는 의미입니다.

크라우드 소싱 주석의 경우 동일한 질문에 대한 LLM의 응답을 쌍별 방식으로 쌍을 이룬다. 그런 다음 이러한 쌍은 주석자에게 무작위로 나란히 제시되었다. 주석자는 두 가지 응답에 대한 전반적인 판단을 내리고 다음 네 가지 옵션 중에서 선택하도록 요청받았다. 옵션 설정은 [22]와 동일하다.

*A : 좌측이 더 좋다
*B: Right가 더 좋다
*C : 타이
*D: 양자가 나쁘다

LLMEval-1에서는 GPT-4의 자동 평가를 위해 다음과 같은 프롬프트 템플릿을 사용한다. 대괄호 안의 내용은 특정 텍스트로 대체된다. 부디 참고 부탁드립니다.

도 10: LLMEval-1에서 쌍별 비교를 위한 프롬프트

도 9: LLMEval-1에서 스타 스코어링을 위한 프롬프트들

[MISSING_PAGE_EMPTY:11]

도 14: LLMEval-1의 쌍별 비교를 위한 GUI

도 13: LLMEval-1에서 스타 스코어링을 위한 GUI

도 16: LLMEval-2에서 주관식 질문을 위한 GUI

도 15: LLMEval-2의 객관적 질문을 위한 GUI

[MISSING_PAGE_FAIL:14]

\begin{table}
\begin{tabular}{l c c} \hline
**LLM Name** & **Score** & **Rank** \\ \hline GPT4.0 & 0.701(0.894) & 1(1) \\ GPT3.5 & 0.643(0.818) & 2(2) \\ Baichuan-7B-Align & 0.603(0.621) & 3(4) \\ ChatGLM-6B & 0.579(0.547) & 4(5) \\ Xunfei-xinghuo & 0.550(0.623) & 5(3) \\ Chinese-LLAMA-7B & 0.506(0.457) & 6(7) \\ Ali-Tongyiqianwen & 0.491(0.507) & 7(6) \\ ChatYuan-Large & 0.426(0.245) & 8(12) \\ NewBing & 0.415(0.425) & 9(8) \\ Lilly-ChatFlow-13B & 0.398(0.339) & 10(9) \\ MOSS-16B & 0.377(0.272) & 11(10) \\ MOSS-w-Plugin-16B & 0.352(0.254) & 12(11) \\ \hline \end{tabular}

* 괄호 안의 값은 GPT-4에서 제공하는 평가 점수 또는 순위를 나타낸다.

\end{table}
표 7: LLMEval-1 - Manual/GPT-4의 쌍대 비교 결과

\begin{table}
\begin{tabular}{l c c} \hline
**LLM** & **Correctness** & **Explanation** \\ \hline GPT4.0 & 2.378 (2.395) & 1.670 (1.595) \\ GPT3.5 & 2.160 (2.138) & 1.542 (1.503) \\ Xunfei-Xinghuo & 2.114 (2.243) & 1.557 (1.632) \\ Baichuan-13B-Chat & 2.003 (2.013) & 1.428 (1.441) \\ MiniMax-abab5 & 1.922 (1.928) & 1.443 (1.493) \\ NewBing & 2.197 (2.211) & 1.583 (1.615) \\ Claude & 1.923 (2.066) & 1.463 (1.576) \\ moss-mars & 1.961 (1.967) & 1.465 (1.470) \\ Kunlun-Tiangong & 1.933 (1.961) & 1.354 (1.500) \\ Ziya-LLaMA-13B-v1 & 1.681 (1.592) & 1.306 (1.201) \\ Ali-Tongyiqianwen & 1.638 (1.618) & 1.275 (1.280) \\
360 & 1.720 (1.678) & 1.322 (1.352) \\ CIIC-Zhigong & 1.680 (2.072) & 1.297 (1.516) \\ ChatGLM2-6B & 1.690 (1.671) & 1.345 (1.306) \\ Vicuna-33B & 1.567 (1.684) & 1.277 (1.270) \\ InternLM-7B & 1.655 (1.658) & 1.355 (1.174) \\ ChatGLM-130B & 1.602 (1.638) & 1.239 (1.280) \\ TigerBot-180B & 1.604 (1.592) & 1.294 (1.220) \\ AquilaChat-7b & 1.548 (1.553) & 1.239 (1.207) \\ BELLE-7B-2M & 1.484 (1.461) & 1.224 (1.164) \\ \hline \end{tabular}

\end{table}
표 8: LLMEval-2의 순위 및 최종 점수

\begin{table}
\begin{tabular}{l c c} \hline
**LLM** & **Ranking** & **Scores** \\ \hline GPT4.0 & 1(1) & 86.72 (89.54) \\ GPT3.5 & 2(2) & 80.71 (84.69) \\ Xunfei-Xinghuo & 3(5) & 78.05 (82.26) \\ Baichuan-13B-Chat & 4(6) & 77.51 (81.82) \\ MiniMax-abab5 & 5(7) & 77.47 (80.64) \\ NewBing & 6(4) & 77.28 (82.63) \\ Claude & 7(3) & 75.57 (83.49) \\ moss-mars & 8(9) & 74.41 (79.21) \\ Kunlun-Tiangong & 9(8) & 74.36 (79.31) \\ Ziya-LLaMA-13B-v1 & 10(13) & 69.48 (70.92) \\ Ali-Tongyiqianwen & 11(12) & 68.01 (71.02) \\
360 & 12(10) & 67.97 (72.86) \\ CIIC-Zhigong & 13(14) & 67.27 (70.53) \\ ChatGLM2-6B & 14(17) & 67.07 (69.06) \\ Vicuna-33B & 15(16) & 66.53 (69.16) \\ InternLM-7B & 16(18) & 66.52 (69.00) \\ ChatGLM-130B & 17(15) & 66.05 (69.48) \\ TigerBot-180B & 18(11) & 65.90 (71.77) \\ AquilaChat-7b & 19(19) & 64.82 (68.19) \\ BELLE-7B-2M & 20(20) & 62.98 (65.27) \\ \hline \end{tabular}

\end{table}
표 8: LLMEval-2의 순위 및 최종 점수

\begin{table}
\begin{tabular}{l c c} \hline
**LLM** & **Correctness** & **Explanation** \\ \hline GPT4.0 & 2.378 (2.395) & 1.670 (1.595) \\ GPT3.5 & 2.160 (2.138) & 1.542 (1.503) \\ Xunfei-Xinghuo & 2.114 (2.243) & 1.557 (1.632) \\ Baichuan-13B-Chat & 2.003 (2.013) & 1.428 (1.441) \\ MiniMax-abab5 & 1.922 (1.928) & 1.443 (1.493) \\ NewBing & 2.197 (2.211) & 1.583 (1.615) \\ Claude & 1.923 (2.066) & 1.463 (1.576) \\ moss-mars & 1.961 (1.967) & 1.465 (1.470) \\ Kunlun-Tiangong & 1.933 (1.961) & 1.354 (1.500) \\ Ziya-LLaMA-13B-v1 & 1.681 (1.592) & 1.306 (1.201) \\ Ali-Tongyiqianwen & 1.638 (1.618) & 1.275 (1.280) \\
360 & 1.720 (1.678) & 1.322 (1.352) \\ CIIC-Zhigong & 1.680 (2.072) & 1.297 (1.516) \\ ChatGLM2-6B & 1.690 (1.671) & 1.345 (1.306) \\ Vicuna-33B & 1.567 (1.684) & 1.277 (1.270) \\ InternLM-7B & 1.655 (1.658) & 1.355 (1.174) \\ ChatGLM-130B & 1.602 (1.638) & 1.239 (1.280) \\ TigerBot-180B & 1.604 (1.592) & 1.294 (1.220) \\ AquilaChat-7b & 1.548 (1.553) & 1.239 (1.207) \\ BELLE-7B-2M & 1.484 (1.461) & 1.224 (1.164) \\ \hline \end{tabular}

\end{table}
표 9: LLMEval-2의 객관적 질문 점수

\begin{table}
\begin{tabular}{l l l l l} \hline
**LLM** & **Fluency** & **Accuracy** & **Logicality** & **Informativeness** \\ \hline GPT4.0 & 2.895 (2.989) & 4.260 (4.545) & 2.779 (2.903) & 2.691 (2.886) \\ GPT3.5 & 2.861 (3.000) & 3.822 (4.295) & 2.694 (2.818) & 2.489 (2.750) \\ Xunfei-Xinghuo & 2.815 (2.977) & 3.750 (4.193) & 2.560 (2.739) & 2.196 (2.716) \\ Baichuan-13B-Chat & 2.847 (2.949) & 3.727 (4.102) & 2.631 (2.778) & 2.472 (2.756) \\ MiniMax-abab5 & 2.878 (2.989) & 3.800 (3.977) & 2.656 (2.722) & 2.478 (2.699) \\ NewBing & 2.796 (2.989) & 3.608 (3.875) & 2.558 (2.773) & 2.061 (2.511) \\ Claude & 2.680 (2.977) & 3.597 (4.125) & 2.613 (2.801) & 2.414 (2.710) \\ moss-mars & 2.737 (3.000) & 3.480 (3.807) & 2.508 (2.648) & 2.229 (2.534) \\ Kunlun-Tiangong & 2.774 (2.983) & 3.520 (3.807) & 2.576 (2.682) & 2.339 (2.523) \\ Ziya-LLaMA-13B-v1 & 2.804 (3.000) & 3.207 (3.364) & 2.473 (2.585) & 2.120 (2.278) \\ Ali-Tongyiqianwen & 2.776 (3.000) & 3.098 (3.239) & 2.443 (2.511) & 2.126 (2.335) \\
360 & 2.700 (2.989) & 3.022 (3.352) & 2.394 (2.608) & 2.056 (2.313) \\ CIIC-Zhigong & 2.764 (2.983) & 3.067 (4.080) & 2.427 (2.744) & 1.916 (2.631) \\ ChatGLM2-6B & 2.758 (2.920) & 2.934 (3.011) & 2.401 (2.386) & 1.956 (2.210) \\ Vicuna-33B & 2.599 (2.943) & 3.033 (3.080) & 2.440 (2.398) & 2.143 (2.199) \\ InternLM-7B & 2.636 (2.847) & 3.091 (3.330) & 2.295 (2.392) & 1.938 (2.233) \\ ChatGLM-130B & 2.670 (2.926) & 3.022 (3.114) & 2.374 (2.443) & 2.084 (2.278) \\ TigerBot-180B & 2.573 (2.926) & 3.079 (3.557) & 2.489 (2.602) & 1.882 (2.352) \\ AquilaChat-7b & 2.710 (2.932) & 2.945 (3.136) & 2.383 (2.443) & 1.918 (2.244) \\ BELLE-7B-2M & 2.685 (2.824) & 2.695 (3.000) & 2.347 (2.335) & 1.880 (2.131) \\ \hline \end{tabular}
\end{table}
표 10: LLMEval-2의 주관적 질문 점수
