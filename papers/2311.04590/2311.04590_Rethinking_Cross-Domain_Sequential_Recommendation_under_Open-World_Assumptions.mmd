[MISSING_PAGE_FAIL:1]

the selection bias in modeling. In the open-world environment, the input/output space expands, and data distribution shifts due to unseen factors. Therefore, to enhance the model's performance in CDSR, it is imperative to address the challenges that arise under open-world assumptions.

First of all, _for multiple domains with few overlapping users, how to construct the model with the non-overlapping users and improve the recommendation performance_? Most previous CDSR methods cannot be directly extended to handle partially overlapping CDSR settings, especially when there are only a few shared users across domains that are often encountered in the real world. To address this challenge, recent researches (Beng et al., 2017; Chen et al., 2018; Li et al., 2019; Li et al., 2019) have explored the use of graphic deep learning to enhance both overlapping and non-overlapping user embeddings and propagate interest information among non-overlapping users. More recently, Cao et al. (Cao et al., 2019) have designed a graph neural network with a contrastive cross-domain infomax objective to improve the learning of non-overlapping user embeddings. Nevertheless, in partially overlapping CDSR scenarios, such methods have great limitations and will drop their performance under open-world assumptions. These CDSR approaches still rely heavily on overlapping users, with more than 70% of users being common across domains, to establish bridge connections among multiple domains and perform knowledge aggregation and transition processes.

The second challenge arises from an observation in open-world scenarios: Recommender systems frequently engage primarily with active users. In the training phase, some unexposed users who are not exposed to the platform, and thus unbeknownst to the models, may surface in the testing stage. This issue causes a performance drop after adapting from an offline to an online environment (Li et al., 2019; Li et al., 2019; Li et al., 2019). So, the second challenge is _how to alleviate the selection bias of the model in the environment with the data distribution shift?_ CaseQ (Li et al., 2019) learns context-specific representations of sequences to capture temporal patterns in various environments. Similarly, DCRec (Li et al., 2019) proposes a novel debiasing contrastive learning paradigm to address the popularity bias issue in single-domain recommendation systems. However, these methods overlook the selection bias among the domains that exists in open-world scenarios, leading to biased performance estimation. To address this selection bias, (Li et al., 2019) proposes an Inverse-Propensity-Score (IPS) estimator, yet often suffering high variance.

In this paper, we first rethink cross-domain sequential recommendation under open-world assumptions and identify the primary challenges. To address these challenges, we propose an adaptive multi-interest debiasing framework for CDSR, which includes a multi-interest information module and a doubly robust estimator.

**Our contributions are as follows.**

**1)** To our best knowledge, this paper is the first effort addressing the open-world challenges in CSDR. We conduct empirical analysis and show that extending existing CDSR models to the open-world environment yields two primary challenges that used to be overlooked: i) How to construct a model in scenarios where the majority of users are non-overlapping, without relying on overlapping users? and ii) How to eliminate selection bias of the model with the data distribution shift?

**2)** We design an **A**daptive **M**ulti-**I**nterest **D**ebiasing framework for cross-domain sequential recommendation (**AMID**), which could be integrated with most off-the-shelf SDSR methods (Li et al., 2019; Li et al., 2019; Li et al., 2019). It is composed of a multi-interest information module (**MIM**) and a doubly robust estimator (**DRE**). MIM transfers cross-domain information for both overlapping and non-overlapping users, while DRE eliminates selection bias and popularity bias to obtain unbiased performance estimation. Moreover, we provide a theoretical analysis that demonstrates the superiority of DRE in terms of bias and tail bound, compared to the IPS estimator used in (Li et al., 2019).

**3)** In order to foster further research in the community, particularly under open-world assumptions, we gathered a real-world financial CDSR dataset from Alipay, called **"MYbank-CDR"**. As far as we are aware, **"MYbank-CDR"** is the first publicly available cross-domain financial dataset. The collected dataset "MYbank-CDR" and the source code will be made publicly available upon acceptance.

**4)** We demonstrate that our proposed **AMID**, when integrated with multiple single-domain sequential recommendation models, achieves state-of-the-art results compared to CDR, CDSR and debiasing methods. Additionally, we conduct online experiments to validate the performance of our proposed framework in a real-world CDSR financial platform with millions of daily traffic logs.

## 2. Motivation: towards open-world CDSR

Current CDSR methods conduct their experiments under closed-world assumptions which assume that there exist fully or mostly overlapping users across domains. However, in real-world applications, the number of overlapping users across domains is typically a minority. To validate their performance in the open-world environment, we perform motivational experiments on the Amazon dataset with three single-domain sequential recommendation methods (BERT4Rec (Liu et al., 2019), GRU4Rec (Li et al., 2019) and SASRec (Li et al., 2019)) and three cross-domain sequential recommendation methods (Pi-Net (Li et al., 2019), DASL (Li et al., 2019) and C\({}^{2}\)DSR (Cao et al., 2019)). Following previous works (Li et al., 2019; Li et al., 2019), we vary the overlapping ratio 1 to simulate different CDSR scenarios. We present a plot of the performance of the models from two domains in Fig 2. In the Movie domain, SASRec (SDSR) achieves the best performance. Similarly, in the Music domain with a 100% overlapping ratio, SASRec (SDSR) outperforms DASL (CDSR). This is attributed to the fact that existing CDSR methods rely on overlapping users to construct their models or transfer information across domains, which leads to a decrease in performance in a partially overlapping scenario. Hence, this finding serves as a motivating factor for us to design a high-performing CDSR model that can be applied in the open-world environment (_**1st challenge**_). Furthermore, a counterintuitive phenomenon has been observed, where SDSR methods exhibit a lesser reliance on overlapping users compared to CDSR methods. In an ideal scenario, SDSR methods should exhibit inferior performance compared to CDSR methods when the overlapping user ratio is high. However, experimental results have revealed that the performance gap actually diminishes as the ratio decreases. Surprisingly, when the ratio is small, the performance of a few SDSR methods outperforms CDSR methods, and in some cases, even when the ratio is high.

To analyze the underlying cause for this phenomenon, we try to visualize the distribution shift between the train set and the test set. Figure 6 displays the t-SNE visualization of user embeddings 2 for the train set (blue dots) and the test set (green dots). Training with a 25% proportion simulates the situation where the training data in a real-world scenario may suffer from unseen factors. We observed that the distribution difference at 25% proportion is larger than that at 100% proportion. These visualization results confirm that the distribution shift from training to testing indeed exists in the CDSR scenarios. The distribution shift often occurs in a real-world platform under open-world assumptions. In real-world recommendation systems, users to be exposed are sometimes selected by the recommendation algorithm based on factors such as estimated conversion rates and business rules. During training, only the data of these exposed users are used because their interaction labels are considered meaningful, while the unexposed users are overlooked. However, during the inference stage, estimated conversion scores are required for all users, including the unexposed ones, in order to determine the selection of users to be exposed in the recommendation system. The rating data of these unexposed users are missing not at random (Beng et al., 2017), leading to selection bias in the CDSR scenario. Therefore, this raises another question: "How can we mitigate data selection bias across multiple domains in the open-world environment?" (_2nd challenge_)

Footnote 2: The trained DASL. (Luo et al., 2018) model is used to generate user embeddings.

## 3. Preliminaries

### Problem Definition

In this paper, we consider a partially overlapping CDSR scenario composed of multiple domains \(\mathcal{Z}=\{Z_{1},...,Z_{|\mathcal{Z}|}\}\). Let \(\mathcal{U}=\{u_{1},...,u_{|\mathcal{U}|}\}\), \(\mathcal{V}=\{v_{1}^{Z_{1}},...,v_{|\mathcal{V}|}^{Z_{|\mathcal{Z}|}}\}\) be the user set, item set, rating set. A user who only has historical behaviors in one domain is referred to as a non-overlapping user, while a user with historical behaviors in multiple domains is referred to as an overlapping user. As a certain user, denote \(\mathcal{S}=\{S^{Z_{1}},...,S^{Z_{|\mathcal{Z}|}}\}\) the corresponding sequential behaviours of the users. For example, \(S^{Z_{i}}=\{o_{1},...,o_{T}\}\) represents the single-domain sequence, where \(T\) is a variable length. Given the data \(\mathcal{D}=\mathcal{U}\times\mathcal{V}\), CDSR aims to develop a personalized ranking function that utilizes the past item sequences from multiple domains of a user and predicts the next item (i.e. \(o_{T+1}\)) in each domain that the user is most likely to choose.

Different from conventional cross-domain recommendation (CDR), CDSR methods pay more attention to modeling sequential behavior dependencies. Mathematically, the objective of CDR methods are formulated as follows:

\[\text{argmax}\ \ p^{X}\left(r_{u,v}^{X}=v|U^{X},U^{Y},V^{X}\right),\text{if} \ \ v\in\mathcal{V}^{X}. \tag{1}\]

where \(r_{u,v}\) denotes the prediction from user \(u\) to item \(v\) in domain \(X\). However, the objective of CDSR approaches is to predict the next item for a given user \(u\) based on their interaction sequences:

\[\text{argmax}\ \ p^{X}\left(r_{|S^{X}|+1}^{X}=v|S^{X},S^{Y},U^{X},U^{Y},V^{X} \right),\text{if}\ \ \ v\in\mathcal{V}^{X}. \tag{2}\]

Figure 3. Selection bias can cause a distribution shift in the cross-domain sequential scenario with few overlapping users (control ratio is 25%).

Figure 2. Solid lines denote the SDSR methods, while dashed lines denote the CDSR methods. Due to the lack of abundant overlapping users, SASRec (SDSR) outperforms all the CDSR methods in the Movie domain.

### Causal Graph

To tackle the issue of incomplete and insufficient observed information, we construct a causal view and propose an adaptive multi-interest debiasing framework. \(\mathcal{S}_{U}^{Z},\mathcal{R}_{U}^{Z}\), \(SC^{Z}\), and \(GC\) denote the random variables of the historical event sequence, the observed ratings, the domain-specific confounder, and the general confounder. \(O\), which represents the observed variable of the instance (\(O=1\), observed; \(O=0\), unobserved), is decided by \(O^{Z_{1}}\) and \(O^{Z_{2}}\) commonly. If a user is not observed in either of the two domains, \(O\) is 0; otherwise, it is 1. The SDSR methods focus on learning the specified confounder \(SC^{Z}\) for a given user, while the previous CDSR approaches construct models based on overlapping users to obtain the general confounder \(GC\). The links between \((SC^{Z}\), \(GC)\rightarrow\mathcal{R}_{U}^{Z}\) represent the causal effect of the domain-specific and general confounders on their interaction label. In observational studies (Han et al., 2017; Wang et al., 2018; Wang et al., 2019), the collected rating data is often unevenly presented, and the variables \(\mathcal{S}_{U}^{Z}\) and \(\mathcal{R}_{U}^{Z}\) can affect the observation \(O\) of a given instance. This mechanism gives rise to selection bias, resulting in an inconsistent distribution of the observed rating data as compared to the ideal test distribution.

From another perspective, the causal graph depicts two sources of association between the causes \(S\) and the outcome \(R\): (1) the desirable causal effect \(S\rightarrow(SC,GC)\to R\); (2) the collision path \(S\to O\gets R\) that connects \(S\) and \(R\) through their common (conditioned on) effects \(O=1\). Analyses conditioned on \(O=1\) may generate spurious associations between \(S\) and \(R\). It is important to note that the domains \(Z_{1}\) and \(Z_{2}\) commonly influence the observed variable. Models learned from observed data may be affected by the issue of selection bias. In the open-world environment, this is a critical issue that must be addressed to prevent a drop in online performance.

### Single-domain Sequential Recommendation Methods

In this research, our primary objective is to develop a universal cross-domain structure that can improve the performance of single-domain sequential recommendation models by seamlessly integrating them into comprehensive CDSR models. To achieve this, we first explore the fundamental mechanisms of the single-domain sequential recommendation network.

**Embedding layer.** We map the fixed-length sequence \(S=\{v_{1},...,v_{T}\}\) to a \(d\)-dimensional embedding space, which is achieved through truncation or padding. Besides, a learnable parameter position embedding matrix is utilized to enhance the chronologically ordered information of the sequence. Specially, we get the sequence embedding \(\mathbf{S}_{u}=\{\mathbf{h}_{v_{1}}^{{}^{\prime}},...,\mathbf{h}_{v_{T}}^{{}^{ \prime}}\}\in\mathbb{R}^{T\times d}\).

**Sequential information encoder.** In the previous single-domain sequential recommendation methods, they designed various sequential information encoders to extract short-/long-term item relationships among the sequence. For example, GRU4Rec (Han et al., 2017) designs multiple GRU layers for the sequential recommendation. SASRec (Han et al., 2017) proposes stacking self-attention blocks with residual connections, while BERT4Rec (Wang et al., 2018) develops the deep bidirectional self-attention to model user behavior sequences. For convenience, we obtain the enhanced sequential embedding via a function \(\mathcal{F}\) which represents their designed sequential information encoder. This process is formulated as \(\mathbf{h}_{v_{1}},..,\mathbf{h}_{v_{T}}=\mathcal{F}(\mathbf{h}_{v_{1}}^{{}^{ \prime}},...,\mathbf{h}_{v_{T}}^{{}^{\prime}})\).

## 4. Methodology

### Multi-interest Information Module

In this section, we will introduce our multi-interest information module, which can convert a normal SDSR model into a CDSR model. The module consists of two steps: interest group construction and information propagation. For simplicity, we only include two domains as examples and show the operation on them, but our methods can be applied to multiple domains. Our design is motivated by the goal of creating interest groups by identifying users with similar preferences and sharing information as widely as possible.

**Group construction.** Initially, we compute the group flag by evaluating the similarity between their sequences point-by-point. For instance, consider two sequences \(\mathcal{S}_{u_{i}}^{Z_{1}}\in\mathbb{R}^{T\times d}\) and \(\mathcal{S}_{u_{j}}^{Z_{2}}\in\mathbb{R}^{T\times d}\) from users \(u_{i}\) and \(u_{j}\) in different domains. We then determine the group flag among users as follows:

\[\mathbf{a}^{\prime}{}_{ij}=\max\{(\mathcal{S}_{u_{i}}^{Z_{1}}\mathbf{W}_{1})( \mathcal{S}_{u_{j}}^{Z_{2}}\mathbf{W}_{2})^{\top}\} \tag{3}\]

where \(\mathbf{W}_{1}\), \(\mathbf{W}_{2}\in\mathbb{R}^{d\times d}\) are the transformation matrix. The max function is utilized to find the nearest similarity between user \(u_{i}\) and \(u_{j}\) from \(T\times T\) similarity relations. Last, we get the group flag via the threshold determination where \(\mathbf{a}_{ij}=1\) means the users \(u_{i}\) and \(u_{j}\) are in the same group.

\[\mathbf{a}_{ij}=\left\{\begin{array}{ll}0\,,&\mathbf{a}^{\prime}{}_{ij}<k\\ 1\,,&\mathbf{a}^{\prime}{}_{ij}\geq k\end{array}\right. \tag{4}\]

**Information propagation.** After constructing the interest groups, the cross-domain information will propagate among the same group. The cross-domain message \(\mathbf{m}_{u_{i}^{Z_{1}}\leftarrow u_{j}^{Z_{2}}}\in\mathbb{R}^{T\times d}\) can be obtained:

\[\mathbf{m}_{u_{i}^{Z_{1}}\leftarrow u_{j}^{Z_{2}}}=\mathbf{a}_{ij}\cdot( \mathcal{S}_{u_{j}}^{Z_{2}}\mathbf{W}_{ip}) \tag{5}\]

where \(\mathbf{W}_{ip}\in\mathbb{R}^{d\times d}\) is the trainable parameter to transfer the cross-domain knowledge. When the user has the behaviors in multiple domains, the message from the same user in a different domain (e.g. \(\mathbf{m}_{u_{i}^{Z_{1}}\leftarrow u_{i}^{Z_{2}}}\)) will also be propagated. For the target user (e.g. \(u_{i}^{Z_{1}}\)), we concatenate all the information from other domains along the last dimension to obtain the aggregated message \(\mathbf{m}^{\prime}{}_{u_{i}^{Z_{1}}}\in\mathbb{R}^{T\times d\times N}\), where \(N\) is the number of the sampled users. We then use transformation matrix \(\mathbf{W}_{C}\in\mathbb{R}^{N\times 1}\) and \(\mathbf{W}_{F}\in\mathbb{R}^{d\times d}\) to fuse the information and get the enhanced sequence representation \(\mathcal{S}_{u_{i}}^{Z_{2}}\in\mathbb{R}^{ZT\times d}\) concatenated with the initial sequence information, where the Squeeze function reduces the dimension of the matrix. **For users in other domains, information propagation occurs in a similar manner.**

\[\mathcal{S}_{u_{i}}^{*Z}=\mathrm{Concat}(\mathcal{S}_{u_{i}}^{Z},\mathrm{Squeeze }(\mathbf{m}^{\prime}{}_{u_{i}^{Z}}\mathbf{W}_{C}))\mathbf{W}_{F} \tag{6}\]

### Prediction Layer

We construct a prediction layer to estimate the user's \(u_{i}\) preference towards the target item \(v_{k}\) as:

\[\hat{\nu}_{u_{i},v_{k}}^{Z}=\sigma(\mathrm{MLPs}(\mathrm{Mean}(\mathcal{S}_{u_{ i}}^{*Z})||\mathbf{v}_{k}^{Z})) \tag{7}\]where MLPs consist of stacked MLP layers that take as input the concatenation of enhanced sequence embedding and item embedding. The sigmoid function is denoted by \(\sigma\), and the Mean function averages the embedding along the temporal dimension.

### Doubly Robust Estimator for CDSR

In this part, we propose a novel doubly robust estimator, which generalizes the traditional DR estimator (Wang et al., 2017) to the cross-domain sequential scenarios. Suppose \(\hat{\mathbf{R}}^{Z}\in\mathbb{R}^{\mathcal{U}^{Z}\times\mathcal{V}^{Z}}\) be a prediction matrix and \(\mathbf{R}^{Z}\in\mathbb{R}^{\mathcal{U}^{Z}\times\mathcal{V}^{Z}}\) be a true rating matrix, the prediction inaccuracy \(\mathcal{P}\) and the doubly robust estimator \(\mathcal{E}^{*}_{\text{DR}}\) are defined as:

\[\mathcal{P}=\frac{1}{|\mathcal{Z}|}\sum_{Z\in\mathcal{Z}}\frac{1}{|\mathcal{D }^{Z}|}\sum_{u,u\in\mathcal{D}^{Z}}e^{Z}_{u,v}. \tag{8}\]

\[\mathcal{E}^{*}_{\text{DR}}=\frac{1}{|\mathcal{Z}|}\sum_{Z\in\mathcal{Z}} \frac{1}{|\mathcal{D}^{Z}|}\sum_{u,u\in\mathcal{D}^{Z}}\left(e^{Z}_{u,v}+ \frac{o^{Z}_{u,v}\hat{o}^{Z}_{u,v}}{\hat{p}^{Z}_{u,v}}\right). \tag{9}\]

where \(e^{Z}_{u,v}=|\hat{r}^{Z}_{u,v}-r^{Z}_{u,v}|\) or \(e^{Z}_{u,v}=(\hat{r}^{Z}_{u,v}-r^{Z}_{u,v})^{2}\) via optional measure metrics for MAE or MSE. The imputation error \(\hat{e}^{Z}_{u,v}=g_{\phi Z}(\text{Mean}(\hat{S}^{Z}_{u})||\psi^{Z}))\) is computed by imputation model which aims to estimate the prediction error \(e_{u,v}\) on the observed data. We also learn the propensity \(\hat{p}_{u,v}=g_{\psi Z}(\text{Mean}(S^{Z}_{u})||\psi^{Z})\). The imputation model \(g_{\phi Z}\) and the propensity model \(g_{\psi Z}\) are implemented in a multi-task manner. The bias of the estimator is derived as follows.

**Lemma 4.1 (Bias of DR Estimator)**. Given imputation errors \(\hat{\mathbf{E}}^{Z}\) and learned propensities \(\hat{\mathbf{P}}^{Z}\) for all user-item pairs, the bias of the DR estimator in the CDSR task is

\[\text{Bias}(\mathcal{E}^{*}_{DR})=\frac{1}{|\mathcal{Z}|}\sum_{Z\in\mathcal{Z} }\left[\frac{1}{|\mathcal{D}^{Z}|}\left|\sum_{u,v\in\mathcal{D}^{Z}}\Delta^{Z }_{u,v}\hat{o}^{Z}_{u,v}\right|\right] \tag{10}\]

where the imputation error \(\delta^{Z}_{u,v}\) and the learned propensities \(\Delta^{Z}_{u,v}\) is defined as:

\[\Delta^{Z}_{u,v}=\frac{\hat{p}^{Z}_{u,v}-\hat{p}^{Z}_{u,v}}{\hat{p}^{Z}_{u,v}},\ \ \delta^{Z}_{u,v}=e^{Z}_{u,v}-\hat{e}^{Z}_{u,v} \tag{11}\]

**Corollary 4.1 (Double Robustness)**. The DR estimator for CDSR is unbiased when either imputed errors \(\hat{\mathbf{E}}^{Z}\) or learned propensities \(\hat{\mathbf{P}}^{Z}\) are accurate for all user-item pairs.

**Lemma 4.2 (Tail Bound of DR Estimator)**. Given imputation errors \(\hat{\mathbf{E}}^{Z}\) and learned propensities \(\hat{\mathbf{P}}^{Z}\), for any prediction matrix \(\hat{\mathbf{R}}^{Z}\), with probability \(1\)-\(\eta\), the deviation of the DR estimator from its expectation has the following tail bound in CDSR task.

\[\left|\mathcal{E}^{*}_{DR}-\mathbb{E}_{0}[\mathcal{E}^{*}_{DR}]\right|\leq \sqrt{\frac{\log(\frac{2}{\eta})}{2|\mathcal{I}|(\sum\limits_{Z\in\mathcal{Z} }|\mathcal{D}^{Z}|)^{2}}\sum_{Z\in\mathcal{Z}}\left[\frac{1}{|\mathcal{D}^{Z}| }\sum_{u,v\in\mathcal{D}^{Z}}\left(\frac{\delta^{Z}_{u,v}}{\hat{p}^{Z}_{u,v} }\right)^{2}\right]}. \tag{12}\]

**Corollary 4.2 (Tail Bound Comparison)**. Suppose imputed errors \(\hat{\mathbf{E}}^{Z}\) are such that \(0\leq\hat{e}^{Z}_{u,v}\leq 2\epsilon^{Z}_{u,v}\) for each \(u,v\in\mathcal{D}^{Z}\), then for any learned propensities \(\hat{\mathbf{P}}\), the tail bound of the proposed estimator will be lower than that of the IPS estimator which is utilized in IPSCDR (Tail and Kumar, 2017). The proof of the lemmas and the corollaries are demonstrated in Appendix A.

### Joint learning

We apply an alternating training for joint learning. In the first step, we train the imputation and prediction models on the observed data by minimizing the proposed hybrid loss in the first step.

\[\mathcal{L}_{e}(\theta,\phi,\psi)= \frac{1}{|\mathcal{Z}|}\sum_{Z\in\mathcal{Z}}\left(\frac{1}{| \mathcal{O}^{Z}|}\sum_{u,v\in\mathcal{O}^{Z}}e_{u,v}+\lambda_{1}\sum_{u,u\in \mathcal{O}^{Z}}\frac{(\hat{e}_{u,v}-e_{u,v})^{2}}{\hat{p}_{u,v}}\right) \tag{14}\] \[+\lambda_{2}||\theta||^{2}_{F}+\lambda_{3}||\phi||^{2}_{F}+\lambda_ {4}||\psi||^{2}_{F} \tag{13}\]

The Frobenius norm \(||.||^{2}_{F}\) is used to measure the norm of matrices. The hyperparameters \(\lambda_{1,2,3,4}\) are used to control the trade-off between regularization and the multiple loss. After training the model on the observed data, we continue to train our prediction model \(\theta\) on \(\mathcal{D}^{Z}\) to alleviate the bias.

\[\mathcal{L}_{r}(\theta,\phi,\psi)= \frac{1}{|\mathcal{Z}|}\sum_{x\in\mathcal{Z}}\left[\frac{1}{| \mathcal{D}^{2}|}\sum_{u,v\in\mathcal{D}^{x}}\left(\hat{e}_{u,v}+\frac{o_{u,v}( e_{u,v}-\hat{e}_{u,v})}{\hat{p}_{u,v}}\right)\right] \tag{16}\] \[+\lambda_{5}||\theta||_{F}^{2} \tag{15}\]

The label \(y_{u,v}\) is unavailable for the data point in the set \(\mathcal{D}^{Z}-\mathcal{O}^{Z}\). \(\lambda_{5}\) balances the regularization term. The learning process of our framework is shown in Appendix B.

## 5. Experiments

### Experimental Setup

To conduct our experiments, we followed the methodology of previous research (Chen et al., 2017; Li et al., 2018) and used the Amazon datasets 3, which consist of 24 item domains. We selected two pairs of domains, namely "Cloth-Sport" and "Phone-Elec", and formulated two different tasks. Besides, we collect a Table 3 summarizes the statistics for each task. To replicate selection bias in online platforms, we adjusted the non-overlapping ratio \(\mathcal{K}_{u}\) for each dataset, selecting from \(\{25\%,75\%\}\). Varying the ratio results in different numbers of non-overlapping users across domains, with a higher \(\mathcal{K}_{u}\) indicating a less biased environment. Following previous CDSR literature (Chen et al., 2017; Li et al., 2018; Li et al., 2018), we employed the leave-one-out technique to evaluate the performance of our developed model. In order to ensure an unbiased evaluation, we adopt the methodology employed in previous works (Li et al., 2018; Li et al., 2018), wherein we randomly select 999 negative items (i.e., items that the user has not interacted with) and combine them with 1 positive item (i.e., a ground-truth interaction) to form our recommendation candidates for the ranking test. We evaluated our model using the normalized discounted cumulative gain (NDCG@10) and hit rate (HR@10) metrics, which are common in CDSR literature. For all comparative models, we ran each experiment five times and reported the results by mean and variance. Further details about our experimental setup are in Appendix C.1.

Footnote 3: [http://jmcualley.ucsd.edu/data/amamamam/index](http://jmcualley.ucsd.edu/data/amamamam/index) 2014.html

### Performance Comparisons

**Compared Methods.** We compare our method with three classes of baselines: (1) Single-domain sequential recommendation methods, i.e., BERT4Rec (Wang et al., 2017), GRU4Rec (He et al., 2017) and SASRec (He et al., 2017). (2) Conventional Cross-domain recommendation methods, i.e., STAR (Wang et al., 2017), MAMDR (Li et al., 2018), SSCDR (He et al., 2017). (3) Cross-domain sequential recommendation methods, i.e., Pi-Net (Li et al., 2018), DASL (Li et al., 2018) and C\({}^{2}\)DSR (Chen et al., 2017). (4) Debiased recommendation methods, i.e., DCRec (Li et al., 2018), CaseQ (Li et al., 2018) and IPSCDR (Li et al., 2018). A detailed introduction to these baselines can be found in Appendix C.2. As shown in Table 4, our AMID is the most versatile and universal approach which considers propagating cross-domain knowledge for both overlapping users and non-overlapping users and can combine with most oft-the-shelf SDSR backbone models. Regarding the debiasing frameworks (Li et al., 2018; Li et al., 2018), our AMID approach can simultaneously alleviate multiple types of bias, especially selection bias, from multiple domains. Additionally, our proposed doubly robust estimator for CDSR has a low variance, which is different from the IPS estimator from IPSCDR (Li et al., 2018) with high variance (Li et al., 2018).

**Quantitative Results.** Tables 1-2 present the quantitative comparison results on two CDSR tasks with different selection bias magnitudes. A larger \(\mathcal{K}_{u}\) indicates a less biased scenario. The best results of each column are highlighted in boldface, while the second-best results are underlined. As expected, the performance of all models increases with increasing \(\mathcal{K}_{u}\), since more biased scenarios may make it harder for models to converge. Our analysis yields the following insightful findings: (1) In most cases, the debiasing baselines, which eliminate the biases produced in domains, perform better than the CDSR baselines in more biased scenarios (i.e. \(\mathcal{K}_{u}=25\%\)). (2) In a more biased scenario with smaller \(\mathcal{K}_{u}\), our framework achieves more significant performance compared to the second-best models, indicating that our AMID effectively alleviates the bias. (3) Benefitting from the low variance in our estimator, our AMID is more unbiased and performs better than IPSCDR.

**Model Efficiency.** All comparative models are trained and tested on the same machine, which has a single NVIDIA GeForce A100 with 80GB memory and an Intel Core i7-8700K CPU with 646 RAM. Notably, the number of parameters for typical C\({}^{2}\)DSR, SASRec + CaseQ, SASRec + IPSCDR, and SASRec + AMID were of the same order of magnitude, denoted as 0.276M, 0.210M, 0.192M, and 0.193M. The training/testing efficiencies of C\({}^{2}\)DSR, SASRec + CaseQ, SASRec + IPSCDR, and SASRec + AMID in processing one batch of samples are 0.130s/0.049s, 0.083s/0.027s, 0.143s/0.045s, and 0.111s/0.032s, respectively. Therefore, our AMID achieves superior performance enhancements in open-world CDSR scenarios while maintaining promising time efficiency.

**Ablation Study.** To better evaluate the effectiveness of each key component in our approach, we conducted an ablation study by comparing it with a variant that only utilized MIM. Notably, we did not include a variant that only employed DRE, as our approach would degrade into an SDSR method in the absence of MIM. However, our variant equipped with only the multi-interest information module (MIM) still achieves state-of-the-art results in most cases. This is because our proposed module can effectively propagate potential interest information among both overlapping and non-overlapping users.

### Online A/B Test

We conduct large-scale online A/B tests on open-world financial CDSR scenarios with partially overlapping users. In the online serving platform, a large number of users participate in one or multiple financial domains, such as purchasing funds, mortgage loans, or discounting bills. Specifically, we selected three popular domains - "Loan," "Fund," and "Account" - from the serving platform, with partially overlapping users, as the targets of our online testing. We calculate the average statistics of online traffic logs for one day and present them in Table 5. For the control group, we adopt the current online solution for recommending themes to users, which is a cross-domain sequential recommendation method that utilizes noisy auxiliary behaviors directly. For the experiment group, we equip our method with a mature SDSR approach that has achieved remarkable success in the business. We evaluate the results based on three metrics: the number of users who have been exposed to 

[MISSING_PAGE_FAIL:7]

exposure by 9.65%, the click rate by 5.69%, and the CVR by 1.32% in the three domains.

### Hyperparameter Analysis

**The threshold \(k\) for the group.** We conduct ablation experiments by varying the threshold \(k\in\{0.5,0.6,0.7,0.8,0.9\}\) to investigate the impact of the threshold \(k\) on constructing interest groups in the multi-interest information module. Our results show that a larger threshold \(k\) (\(0.5\to 0.7\)) leads to better performance, as more related interest information can be transferred. However, when the threshold \(k\) is increased beyond 0.7, the model's performance drops due to noise interference and redundant information. Therefore, to achieve superior performance, we set the threshold \(k\) to 0.7.

**The number of the sampled users.** To explore the impact of the number of sampled users on the multi-interest information module, we conduct ablation experiments varying the number of sampled users from 128 to 1024. Our findings suggest that an increase in the number of sampled users initially improves the recommendation performance, but it eventually declines when the matching neighbors reach 1024. This observation indicates that having too few sampled users would provide insufficient transferred information, while too many sampled users could introduce interference noise and compromise the model's performance. In practice, we select the number of sampled users to be 512 as it results in the best performance for our model. More analysis and results can be found in Appendix C.3.

## 6. Related Work

**Conventional cross-domain recommendation** has emerged as a promising solution for mitigating data sparsity and cold-start issues encountered in single-domain recommendation systems. Early CDR studies (Han et al., 2016; Wang et al., 2017) have primarily focused on developing approaches that transfer cross-domain knowledge by relying on overlapping users. However, real-world CDR scenarios often do not satisfy strict overlapping requirements and exhibit only a small fraction of common users across domains. To tackle this challenge, recent methods (Wang et al., 2017; Wang et al., 2017)) have proposed a network structure comprising shared and domain-specific networks to effectively capture the unique characteristics and commonalities across all domains simultaneously. While these CDR approaches incorporate valuable information from relevant domains to enhance performance in the target domain, they still encounter difficulties in addressing the contextual sequential dependencies within users' interaction history, which are essential for comprehensive modeling in CDSR tasks.

**Cross-domain sequential recommendation** is designed to improve recommendations for SR tasks that involve items from multiple domains. Pi-Net (Pasquin et al., 2017) and PSJNet (Yang et al., 2017) devise the gating mechanisms to transfer the information among the overlapping users. Similarly, DASL (Pasquin et al., 2017) designs a dual-attention mechanism to bidirectionally transfer user preferences within the overlapping users. The interaction bipartite graph (Pasquin et al., 2017; Wang et al., 2017) is constructed to propagate the information among users. C\({}^{2}\)DSR (Chen et al., 2016) introduces a contrastive objective combined with GNNs to enhance the representation of user preferences. However, these works construct their cross-domain unit relying on the overlapping users under closed-world assumptions, leading to worse performance in the open-world case.

**Debias for recommender systems** are proposed to alleviate the widespread bias in the user behavior observed data, including the selection bias (Chen et al., 2016; Wang et al., 2017), position bias (Han et al., 2016; Wang et al., 2017), exposure bias (Chen et al., 2016; Wang et al., 2017) and popularity bias (Chen et al., 2016; Wang et al., 2017). To address selection bias in RS, two standard approaches have been proposed: the error-imputation-based (EIB) approach and the inverse-propensity-scoring (IPS) approach. The EIB approach (Yang et al., 2017) estimates the prediction error for missing ratings, while the IPS approach (Yang et al., 2017; Wang et al., 2017) reduces selection bias by optimizing the risk function with the inverse propensity score. Recently, (Wang et al., 2017) proposes an IPS estimator for cross-domain scenarios with multiple restrictions. However, EIB methods may have a large bias due to imputation inaccuracy (Chen et al., 2016), and propensity-based methods may suffer from high variance (Wang et al., 2017), leading to non-optimal results. (Wang et al., 2017) propose a self-normalized inverse propensity scoring estimator to reduce the variance of the IPS estimator. To design a less biased estimator, the doubly robust model (Wang et al., 2017) integrates the imputation model with the propensity score for the single-domain recommendation.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & \multicolumn{2}{c}{**\# exposure**} & \multicolumn{1}{c}{**\# click**} & \multicolumn{1}{c}{**CVR**} \\ \hline Loan Domain & +9.80\% & +5.27\% & +1.42\% \\ Fund Domain & +7.32\% & +4.94\% & +0.98\% \\ Account Domain & +11.73\% & +6.85\% & +1.57\% \\ \hline \hline \end{tabular}
\end{table}
Table 6. Online A/B testing results from 9.15 to 9.28, 2023

Figure 6. Impact of the threshold \(k\) and the number of the sampled users.

## 7. Conclusions and Discussions

In this paper, we conduct a thorough study of existing CDSR methods under open-world assumptions. To overcome the challenges, we devise an adaptive multi-interest debiasing framework that includes a multi-interest information module (MIM) and a doubly robust estimator (DRE) for CDSR. MIM utilizes the user behaviors to build the interest groups and propagate the information among both overlapping and non-overlapping users, while DRE introduces a cross-domain debiasing estimator to reduce the estimation bias in an open-world environment. Besides, we collect a financial industry dataset from Alipay, which includes over one billion users. Extensive offline and online experiments show the remarkable efficacy of our approach, as it outperforms existing methods including CDSR methods and debiasing methods in various evaluation metrics.

**Limitations.** Our MIM constructs interest groups between pairs of domains, which share cross-domain knowledge as widely as possible. In the real-world platform, commercial activities are usually composed of multiple domains. However, constructing all the groups for \(|\mathcal{Z}|\) domains has a time complexity of \(O(|\mathcal{Z}|^{2})\), which can become quite time-consuming as the number of domains increases. Therefore, it is important to develop more efficient methods for constructing groups among multiple domains in future work.

## References

* H. Abdollapouri, R. Burke, and B. Mobasher (2017)Controlling popularity bias in learning-to-rank recommendation. In Proceedings of the eleventh ACM conference on recommender systems, pp. 42-46. Cited by: SS1.
* J. Cao, X. Cong, J. Sheng, T. Liu, and B. Wang (2022)Cons-native cross-domain sequential recommendation. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pp. 138-147. Cited by: SS1.
* J. Cao, J. Sheng, X. Cong, T. Liu, and B. Wang (2022)Cross-domain recommendation to cold-start users via variational information bottleneck. In 2022 IEEE 38th International Conference on Data Engineering (ICDE), pp. 2209-2223. Cited by: SS1.
* J. Chen, H. Dong, X. Wang, F. Feng, M. Wang, and X. He (2023)Bias and debias in recommender system: a survey and future directions. ACM Transactions on Information Systems41 (3), pp. 1-39. Cited by: SS1.
* J. Chen, C. Wang, M. Ester, Q. Shi, Y. Feng, and C. Chen (2018)Social recommendation with missing not at random data. In 2018 IEEE International Conference on Data Mining (ICDM), pp. 29-38. Cited by: SS1.
* Q. Cui, T. Wei, Y. Zhang, and Q. Zhang (2020)HeroGRAPH: a heterogeneous graph framework for multi-target cross-domain recommendation. In MSUHQ'RS, Cited by: SS1.
* M. Dudik, J. Langford, and L. Li (2011)Doubly robust policy evaluation and learning. arXiv preprint arXiv:1103.4601. Cited by: SS1.
* A. Gilotte, C. Chanaires, T. Nedelec, A. Abraham, and S. Dolle (2018)Offline-a\(b\) testing for recommender systems. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pp. 198-206. Cited by: SS1.
* L. Guo, L. Tang, T. Chen, L. Zhu, Q. Viet Hung Nguyen, and H. Yin (2021)DA-gcn: a domain-aware attentive graph convolution network for shared-accent cross-domain sequential recommendation. arXiv preprint arXiv:2105.03300. Cited by: SS1.
* J. M. Hernandez-Lobato, N. Houlsby, and Z. Ghahramani (2014)Probabilistic matrix factorization with non-random missing data. In International conference on machine learning, pp. 1512-1520. Cited by: SS1.
* J. Mignard, L. Houlsby, and Z. Ghahramani (2014)Probabilistic matrix factorization with non-random missing data. In International conference on machine learning, pp. 138-152. Cited by: SS1.
* B. Hidiasi, A. Karatzoglou, L. Baltrunas, and D. Tikk (2015)Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06899. Cited by: SS1.
* K. Hofmann, A. Schutl, S. Whiteson, and M. De Rijke (2013)Reusing historical interaction data for faster online learning to rank for 1. In Proceedings of the sixth ACM international conference on Web search and data mining, pp. 183-192. Cited by: SS1.
* G. Hu, Y. Zhang, and Q. Yang (2018)Conet: collaborative cross networks for cross-domain recommendation. In Proceedings of the 27th ACM international conference on information and knowledge management, pp. 667-676. Cited by: SS1.
* T. Joachims, A. Swaminathan, and T. Schnabel (2017)Unbiased learning-to-rank with biased feedback. In Proceedings of the tenth ACM international conference on web search and data mining, pp. 781-789. Cited by: SS1.
* S. et al. (2019)Semi-supervised learning for cross-domain recommendation to cold-start users. In CIKM, Cited by: SS1.
* W. Krichene and S. Rendle (2020)On sampled metrics for item recommendation. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 1748-1757. Cited by: SS1.
* P. Li, Z. Jiang, M. Qiu, Y. Hu, and A. Tuzhilin (2021)Dual attentive sequential learning for cross-domain click-through rate prediction. In Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining, pp. 3172-3180. Cited by: SS1.
* P. Li and A. Tuzhilin (2020)DDLDT: deep dual transfer cross domain recommendation. In Proceedings of the 13th International Conference on Web Search and Data Mining, pp. 331-339. Cited by: SS1.
* S. Li, L. Yao, S. Mu, W. Zhao, Y. Li, T. Guo, B. Ding, and J. Wen (2021)DBLoising learning based cross-domain recommendation. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 3190-3199. Cited by: SS1.
* D. Liu, P. Cheng, Z. Dong, X. He, W. Pan, and Z. Ming (2020)A general knowledge distillation framework for counterfactual recommendation via uniform data. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 831-840. Cited by: SS1.
* M. Liu, J. Li, G. Li, and P. Pan (2020)Cross domain recommendation via bi-directional transfer graph collaborative filtering networks. In Proceedings of the 29th ACM international conference on information & knowledge management, pp. 885-894. Cited by: SS1.
* W. Liu, X. Zheng, J. Su, M. Hu, Y. Tan, and C. Chen (2022)Exploiting variational domain-invariant user embedding for partially overplayed cross domain recommendation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 312-321. Cited by: SS1.
* L. Luo, Y. Li, B. Gao, S. Tang, S. Wang, J. Li, T. Liu, Z. Li, and S. Pin (2023)NAMD: a model agnostic learning framework for multi-domain recommendation. In 2023 IEEE 39th International Conference on Data Engineering (ICDE), pp. 2102-2103. Cited by: SS1.
* M. Ma, P. Ren, X. Chen, Z. Ren, L. Zhao, F. Liu, J. Ma, and M. de Rijke (2022)Mixed information flow for cross-domain sequential recommendations. ACM Transactions on Knowledge Discovery from Data (TKDD)16 (4), pp. 32-32. Cited by: SS1.
* M. Ma, P. Ren, Y. Lin, Z. Chen, J. Ma, and M. de Rijke (2019)\(\pi\)-net: a parallel information-sharing network for shared-account cross-domain sequential recommendations. In Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval, pp. 685-694. Cited by: SS1.
* B. Marlin, R. S. Zemel, S. Roweis, and M. Slaney (2012)Collaborative filtering and the missing at random assumption. UAI. Cited by: SS1.
* W. Ouyang, X. Zhang, L. Zhao, J. Luo, Y. Zhang, H. Zou, Z. Liu, and Y. Du (2020)Minet: mixed interest network for cross-domain click-through rate prediction. In Proceedings of the 29th ACM international conference on information & knowledge management, pp. 2669-2676. Cited by: SS1.
* L. Pexa and P. Volras (2020)Off-line vs. On-line evaluation of recommender systems in small e-commerce. In Proceedings of the 31st ACM Conference on Hypertext and Social Media, pp. 391-300. Cited by: SS1.
* M. E. Roberts, B. M. Stewart, and R. A. Nielsen (2020)Adjusting for confounding with test matching. American Journal of Political Science64 (4), pp. 887-903. Cited by: SS1.
* Y. Saito (2020)A symmetric tri-training for debiasing missing-not-at-random explicit feedback. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 309-318. Cited by: SS1.
* Y. Saito (2020)Unbiased pairwise learning from biased implicit feedback. In Proceedings of the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval, Cited by: SS1.
* T. Schnabel, A. Swaminathan, A. Singh, N. Chandak, and T. Joachims (2016)Recommendations as treatments: debiasing learning and evaluation. In international conference on machine learning, pp. 1670-1679. Cited by: SS1.
* X. et al. (2021)One model to serve all: star topology adaptive recommender for multi-domain tf prediction. In CIKM, Cited by: SS1.
* H. Steck (2010)Training and testing of recommender systems on data missing not at random. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 713-722. Cited by: SS1.
* H. Steck (2013)Evaluation of recommendations: rating-prediction and ranking. In Proceedings of the 7th ACM conference on Recommender systems, Cited by: SS1.

* Sun et al. (2019) Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In _Proceedings of the 28th ACM international conference on information and knowledge management_. 1441-1450.
* Sun et al. (2021) Wenchao Sun, Muyang Ma, Pengfei Ren, Yujie Lin, Zhumin Chen, Zhaochun Ren, Jun Ma, and Maarten De Rijke. 2021. Parallel Split-Join Networks for Shared Account Cross-domain Sequential Recommendations. _IEEE Transactions on Knowledge and Data Engineering_ (2021).
* Swaminathan and Joachims (2015) Adith Swaminathan and Thostern Joachims. 2015. The self-normalized estimator for counterfactual learning. _advances in neural information processing systems_ 28 (2015).
* Vershynin (2018) Roman Vershynin. 2018. _High-dimensional probability: An introduction with applications in data science_. Vol. 47. Cambridge university press.
* Wang et al. (2019) Xiaotio Wang, Rui Zhang, Yu Sun, and Jianzhong Qi. 2019. Doubly robust joint learning for recommendation on data missing not at random. In _International Conference on Machine Learning_. PMLR, 6638-6647.
* Wang et al. (2021) Xiaotio Wang, Rui Zhang, Yu Sun, and Jianzhong Qi. 2021. Combining selection biases in recommender systems with a few unbiased ratings. In _Proceedings of the 14th ACM International Conference on Web Search and Data Mining_. 427-435.
* Wu et al. (2021) Qitun Wu, Henri Zhang, Xiaofeng Cao, Junichi Wang, and Hongyuan Zha. 2021. Towards open-world recommendation: An inductive model-based collaborative filtering approach. In _International Conference on Machine Learning_. PMLR, 11329-11339.
* Xu et al. (2023) Wuiyang Xu, Shaohuai Li, Mingming Ha, Xiaobo Guo, Qiongun Ma, Xiaolei Liu, Linxun Chen, and Zhenfeng Zhu. 2023. Neural Node Matching for Multi-Target Cross Domain Recommendation. _arXiv preprint arXiv:2302.05919_ (2023).
* Yang et al. (2022) Chenxiao Yang, Qitun Wu, Qingsong Wen, Zhiqiang Zhu, Liang Sun, and Junchi Yan. 2022. Towards out-of-distribution sequential event prediction: A causal treatment. _arXiv preprint arXiv:2210.1305_ (2022).
* Yang et al. (2023) Yuhao Yang, Chao Huang, Lianghao Xia, Chunzhen Huang, Da Luo, and Kangyi Lin. 2023. Debiased Contrastive Learning for Sequential Recommendation. In _The Web Conference 2023: International World Wide Web Conference_.
* Yue et al. (2020) Kun Yue, Jiahui Wang, Xinhai Li, and Kuang Hu. 2020. Representation-based completion of knowledge graph with open-world data. In _2020 5th International Conference on Computer and Communication Systems (ICCCS)_. IEEE, 1-8.
* Zhang et al. (2018) Qian Zhang, Dianshuang Wu, Je Lu, and Guangwang Zhang. 2018. Cross-domain recommendation with probabilistic knowledge transfer. In _International Conference on Neural Information Processing_. Springer, 208-219.
* Zhao et al. (2017) Lii Zhao, Sinno Jialin Pan, and Qiang Yang. 2017. A unified framework of active transfer learning for cross-system recommendation. _Artificial Intelligence_ 245 (2017), 38-55.
* Zhao et al. (2020) Wayne Xin Zhao, Junhua Chen, Pengfei Wang, Qi Gu, and Ji-Rong Wen. 2020. Revisiting alternative experimental settings for evaluating top-n item recommendation algorithms. In _Proceedings of the 28th ACM International Conference on Information & Knowledge Management_. 2329-2332.
* Zhu et al. (2020) Feng Zhu, Yan Wang, Chao Chao Chen, Guanfeng Liu, Mehmet Orgun, and Jia Wu. 2020. A deep framework for cross-domain and cross-system recommendations. _arXiv preprint arXiv:2009.06245_ (2020).

## Appendix A Appendix A: Proofs of Lemmas and Theorems

**Lemma 4.1** (Bias of DR Estimator).: Given imputation errors \(\hat{\mathbf{E}}^{Z}\) and learned propensities \(\hat{\mathbf{p}}^{Z}\) for all user-item pairs, the bias of the DR estimator in the CDSR task is

\[\text{Bias}(\mathcal{E}^{*}_{DR})=\frac{1}{|\mathcal{Z}|}\sum_{Z\in\mathcal{Z }}\left[\frac{1}{|\mathcal{D}^{Z}|}\left|\sum_{\mu,\nu\in\mathcal{D}^{Z}} \Delta^{Z}_{u,\nu}\delta^{Z}_{u,\nu}\right|\right] \tag{17}\]

Proof.: According to the definition of the bias, we can derive the bias of the DR estimator for CDSR as follows.

\[\text{Bias}(\mathcal{E}^{*}_{DR}) \tag{19}\] \[=|\mathcal{P}-\mathbb{E}_{\mathbf{Q}}[\mathcal{E}^{*}_{DR}]|,\] (20) \[=|\mathcal{P}-\mathbb{E}_{\mathbf{Q}}[\mathcal{E}^{*}_{DR}]|,\] (21) \[=|\frac{1}{|\mathcal{Z}|}\sum_{Z\in\mathcal{Z}}\left[\frac{1}{| \mathcal{D}^{Z}|}\sum_{u,\nu\in\mathcal{D}^{Z}}\left(\epsilon^{Z}_{u,\nu}- \hat{\epsilon}^{Z}_{u,\nu}-\frac{p^{Z}_{u,\nu}\delta^{Z}_{u,\nu}}{\hat{p}^{Z} _{u,\nu}}\right)\right]\right]\!,\] (22) \[=\frac{1}{|\mathcal{Z}|}\sum_{Z\in\mathcal{Z}}\left[\frac{1}{| \mathcal{D}^{Z}|}\sum_{u,\nu\in\mathcal{D}^{Z}}\left(\delta^{Z}_{u,\nu}- \frac{p^{Z}_{u,\nu}\delta^{Z}_{u,\nu}}{\hat{p}^{Z}_{u,\nu}}\right)\right]\!,\] (23) \[=\frac{1}{|\mathcal{Z}|}\sum_{Z\in\mathcal{Z}}\left[\frac{1}{| \mathcal{D}^{Z}|}\left|\sum_{u,\nu\in\mathcal{D}^{Z}}\Delta^{Z}_{u,\nu}\delta^ {Z}_{u,\nu}\right|\right], \tag{18}\]

which completes the proof. Following the previous works (Zhao et al., 2019; Zhang et al., 2020), the imputation error \(\delta^{Z}_{u,\nu}\) and the learned propensities \(\Lambda^{Z}_{u,\nu}\) is defined as:

\[\delta^{Z}_{u,\nu}=\epsilon^{Z}_{u,\nu}-\hat{\epsilon}^{Z}_{u,\nu}\quad \Lambda^{Z}_{u,\nu}=\frac{\hat{p}^{Z}_{u,\nu}-p^{Z}_{u,\nu}}{\hat{p}^{Z}_{u, \nu}} \tag{24}\]

**Corollary 4.1** (Double Robustness).: The DR estimator for CDSR is unbiased when either imputed errors \(\hat{\mathbf{E}}^{Z}\) or learned propensities \(\hat{\mathbf{P}}^{Z}\) are accurate for all user-item pairs.

Proof.: In one respect, when the imputation error is accurate, we have \(\delta^{Z}_{u,\nu}=0\) for \(u,\nu\in\mathcal{D}^{Z}\). In this case, the bias of the DR estimator for CDSR is computed by

(25) \[\text{Bias}(\mathcal{E}^{*}_{DR})\] (26) \[=\frac{1}{|\mathcal{Z}|}\sum_{Z\in\mathcal{Z}}\left[\frac{1}{| \mathcal{D}^{Z}|}\left|\sum_{\mu,\nu\in\mathcal{D}^{Z}}\Delta^{Z}_{u,\nu} \delta^{Z}_{u,\nu}\right|\right],\] (27) \[=\frac{1}{|\mathcal{Z}|}\sum_{Z\in\mathcal{Z}}\left[\frac{1}{| \mathcal{D}^{Z}|}\left|\sum_{\mu,\nu\in\mathcal{D}^{Z}}\Delta^{Z}_{u,\nu}\cdot 0 \right|\right],\] (28) \[=0.\] (29)

In the other respect, when the learned propensities are accurate, we have \(\Delta^{Z}_{u,\nu}=0\) for \(u,\nu\in\mathcal{D}^{Z}\). In such case, we can compute the bias of the DR estimator for CDSR by

(30) \[\text{Bias}(\mathcal{E}^{*}_{DR})\] (31) \[=\frac{1}{|\mathcal{Z}|}\sum_{Z\in\mathcal{Z}}\left[\frac{1}{| \mathcal{D}^{Z}|}\left|\sum_{\mu,\nu\in\mathcal{D}^{Z}}\Delta^{Z}_{u,\nu} \delta^{Z}_{u,\nu}\right|\right],\] (32) \[=0.\] (33)

When either imputed errors or learned propensities are accurate, the bias of the proposed estimator is accurate. The proof is completed.

**Lemma 4.2** (Tail Bound of DR Estimator).: Given imputation errors \(\hat{\mathbf{E}}^{Z}\) and learned propensities \(\hat{\mathbf{P}}^{Z}\), for any prediction matrix \(\hat{\mathbf{R}}^{Z}\), with probability 1-\(\eta\), the deviation of the DR estimator from its expectation has the following tail bound in CDSR task.

(34) \[\sqrt{\frac{\log(\frac{\eta}{\eta})}{2|\mathcal{Z}|(\sum\limits_{Z \in\mathcal{Z}}|\mathcal{D}^{Z}|)^{2}}\sum\limits_{Z\in\mathcal{Z}}\left[ \frac{1}{|\mathcal{D}^{Z}|}\sum\limits_{u,u\in\mathcal{D}^{Z}}\left(\frac{ \delta_{u,v}^{Z}}{\hat{p}_{u,v}^{Z}}\right)^{2}\right]}\] (35)

Proof.: To avoid cluttering the notation, we introduce a random variable \(X_{u,v}^{Z}\) denoted by

\[\chi_{u,v}^{Z}=\hat{\epsilon}_{u,v}^{Z}+\frac{\alpha_{u,v}^{Z}\delta_{u,v}^{Z} }{\hat{p}_{u,v}^{Z}} \tag{36}\]

Considering the observation indicator \(\alpha_{u,v}^{Z}\) follows a Bernoulli distribution with probability \(p_{u,v}^{Z}\), we can obtain the distribution pattern of the random variable \(X_{u,v}^{Z}\) as follows.

(37) \[P(\chi_{u,v}^{Z})=\left\{\begin{array}{cc}p_{u,v}^{Z}\\ 1-p_{u,v}^{Z}\end{array},&\chi_{u,v}^{Z}=\hat{\epsilon}_{u,v}^{Z}+\kappa_{u,v} ^{Z}\\ \chi_{u,v}^{Z}=\hat{\epsilon}_{u,v}^{Z}\end{array}\right.\] (38)

where \(\kappa_{u,v}^{Z}\) is given by

\[\kappa_{u,v}^{Z}=\frac{\epsilon_{u,v}^{Z}-\hat{\epsilon}_{u,v}^{Z}}{\hat{p}_{ u,v}^{Z}}=\frac{\delta_{u,v}^{Z}}{\hat{p}_{u,v}^{Z}} \tag{39}\]

Determining the interval \([\hat{\epsilon}_{u,v}^{Z},\hat{\epsilon}_{u,v}^{Z}+\kappa_{u,v}^{Z}]\) for the random variable \(\kappa_{u,v}^{Z}\) of size \(\kappa_{u,v}^{Z}\) with probability 1 is a simple process. This is facilitated by the assumption that the observation indicators \(\alpha_{u,v}^{Z}\) are independent random variables, which ensures that the random variables \(\kappa_{u,v}^{Z}\) are also independent. The general form of Hoeffding's inequality for bounded random variables (Hoffding, 1994) can be expressed as 39. Let \(X_{1},...,X_{N}\) be independent random variables. For each \(i\), we assume that \(X_{i}\in[a_{i},b_{i}]\). For any \(\epsilon>0\), we have the inequality

\[P\left(\left|\frac{1}{|\mathcal{Z}|}\sum\limits_{Z\in\mathcal{Z}}\frac{1}{| \mathcal{D}^{Z}|}\left[\sum\limits_{u,u\in\mathcal{D}^{Z}}\kappa_{u,v}^{Z} \right]-\frac{1}{|\mathcal{Z}|}\sum\limits_{Z\in\mathcal{Z}}\frac{1}{| \mathcal{D}^{Z}|}\left[\sum\limits_{u,u\in\mathcal{D}^{Z}}\mathbb{B}_{0}( \chi_{u,v}^{Z})\right]\right|\geq\epsilon\right) \tag{40}\]

\[\leq 2\exp\left(\frac{-2\epsilon^{2}\left(\sum\limits_{Z\in \mathcal{Z}}|\mathcal{D}^{Z}|\right)^{2}}{\frac{1}{|\mathcal{Z}|}\sum\limits_ {Z\in\mathcal{Z}}\frac{1}{|\mathcal{D}^{Z}|}\sum\limits_{u,v\in\mathcal{D}^{Z} }(\kappa_{u,v}^{Z})^{2}}\right) \tag{41}\]

To solve for \(\epsilon\), one can set the right side of the inequality to be \(\eta\) and proceed with the following steps.

\[\eta=2\exp\left(\frac{-2\epsilon^{2}\left(\sum\limits_{Z\in \mathcal{Z}}|\mathcal{D}^{Z}|\right)^{2}}{\frac{1}{|\mathcal{Z}|}\sum\limits_{Z \in\mathcal{Z}}\frac{1}{|\mathcal{D}^{Z}|}\sum\limits_{u,v\in\mathcal{D}^{Z} }(\kappa_{u,v}^{Z})^{2}}\right) \tag{43}\] \[\iff\log(\frac{\eta}{2})=\frac{-2\epsilon^{2}\left(\sum\limits_{Z \in\mathcal{Z}}|\mathcal{D}^{Z}|\right)^{2}}{\frac{1}{|\mathcal{Z}|}\sum\limits_ {Z\in\mathcal{Z}}\frac{1}{|\mathcal{D}^{Z}|}\sum\limits_{u,v\in\mathcal{D}^{Z} }(\kappa_{u,v}^{Z})^{2}}\] (44) \[\iff\epsilon=\sqrt{\frac{\log(\frac{\eta}{\eta})}{2|\mathcal{Z}| (\sum\limits_{Z\in\mathcal{Z}}|\mathcal{D}^{Z}|)^{2}}\sum\limits_{Z\in \mathcal{Z}}\left[\frac{1}{|\mathcal{D}^{Z}|}\sum\limits_{u,v\in\mathcal{D}^{Z} }(\kappa_{u,v}^{Z})^{2}\right]}\] (45) \[\iff\epsilon=\sqrt{\frac{\log(\frac{\eta}{\eta})}{2|\mathcal{Z}| (\sum\limits_{Z\in\mathcal{Z}}|\mathcal{D}^{Z}|)^{2}}\sum\limits_{Z\in \mathcal{Z}}\left[\frac{1}{|\mathcal{D}^{Z}|}\sum\limits_{u,v\in\mathcal{D}^{Z} }(\frac{\delta_{u,v}^{Z}}{\hat{p}_{u,v}^{Z}})^{2}\right]} \tag{42}\]

The proof is completed. 

**Corollary 4.2** (Tail Bound Comparison).: Suppose imputed errors \(\hat{\mathbf{E}}^{Z}\) are such that \(0\leq\hat{\epsilon}_{u,v}^{Z}\leq 2\epsilon_{u,v}^{Z}\) for each \(u,v\in\mathcal{D}^{Z}\), then for any learned propensities \(\hat{\mathbf{P}}\), the tail bound of the proposed estimator will be lower than that of the IPS estimator which is utilized in IPSCDR (Tail Bound, 2009).

Proof.: We can derive the following inequalities

\[0\leq\hat{\epsilon}_{u,v}^{Z}\leq 2\epsilon_{u,v}^{Z}\text{ for }Z\in \mathcal{Z}\text{ for }u,v\in\mathcal{D}^{Z} \tag{47}\] \[\implies\hat{\epsilon}_{u,v}^{Z}-\epsilon_{u,v}^{Z}\leq\epsilon_{u,v }^{Z}\] (48) \[\implies(\delta_{u,v}^{Z})^{2}\leq(\epsilon_{u,v}^{Z})^{2}\] (49) \[\implies\sqrt{\frac{\log(\frac{\eta}{\eta})}{2|\mathcal{Z}|(\sum \limits_{Z\in\mathcal{Z}}|\mathcal{D}^{Z}|)^{2}}\sum\limits_{Z\in\mathcal{Z}} \left[\frac{1}{|\mathcal{D}^{Z}|}\sum\limits_{u,v\in\mathcal{D}^{Z}}(\frac{ \delta_{u,v}^{Z}}{\hat{p}_{u,v}^{Z}})^{2}\right]}\] (50) \[\leq\sqrt{\frac{\log(\frac{\eta}{\eta})}{2|\mathcal{Z}|(\sum \limits_{Z\in\mathcal{Z}}|\mathcal{D}^{Z}|)^{2}}\sum\limits_{Z\in\mathcal{Z}}\left[ \frac{1}{|\mathcal{D}^{Z}|}\sum\limits_{u,v\in\mathcal{D}^{Z}}(\frac{\epsilon_{u,v }^{Z}}{\hat{p}_{u,v}^{Z}})^{2}\right]} \tag{46}\]

In the last inequality, the first row denotes the tail bound of the DR estimator for CDSR and the second row denotes the tail bound of the IPS estimator for CDSR (Tail Bound, 2009; D'Auria, 2010). This completes the proof.

## Appendix B. Appendix B. Methodology

Following the previous work (Hoffding, 2009), we also adopt a joint learning mechanism for training our model. The optimization procedures are shown in Alg. 1.

```
0: The true ratings \(\mathbf{R}\), the learned propensities \(\hat{\mathbf{P}}\) from observed data \(\mathcal{O}\).
1:forstep \(q\in\{1,...,Q\}\)do
2: Sample a batch of user-item pairs in multiple domains \(\mathcal{Z}\) from \(\mathcal{O}\).
3: Compute the loss function \(\mathcal{L}_{e}\).
4: Update the model parameter \(\theta^{q+1}\leftarrow\theta^{q}-\eta\nabla_{\theta}\mathcal{L}_{e}(\theta, \phi,\psi)\)
5: Update the propensities model parameter \(\phi^{q+1}\leftarrow\phi^{q}-\eta\nabla_{\phi}\mathcal{L}_{e}(\theta,\phi,\psi)\)
6: Update the imputation model parameter \(\psi^{q+1}\leftarrow\psi^{q}-\eta\nabla_{\phi}\mathcal{L}_{e}(\theta,\phi,\psi)\)
7:endfor
8:forstep \(q\in\{1,...,Q^{\prime}\}\)do
9: Sample a batch of user-item pairs in multiple domains \(\mathcal{Z}\) from \(\mathcal{D}\).
10: Compute the loss function \(\mathcal{L}_{r}\).
11: Update the model parameter \(\theta^{q+1}\leftarrow\theta^{q}-\eta^{\prime}\nabla_{\theta}\mathcal{L}_{r}( \theta,\phi,\psi)\)
12:endfor
```

**Algorithm 1** The optimization scheme of AMID.

## Appendix C. Experiments

### Experiment Setup

**Evaluation Metrics** In each domain, we divided the users into three sets: 80% for training, 10% for validation, and 10% for testing. To preprocess the data, items with fewer than 10 interactions and users with fewer than 5 interactions in their respective domains were filtered out, following the approach of previous studies (Beng et al., 2017; Liu et al., 2018). This ensured that the embeddings learned by the users/items were representative of their source domain. A non-overlapping ratio \(\mathcal{K}_{u}\) was introduced to control the number of non-overlapping users and simulate different debiased scenarios. For example, in the Amazon "Cloth-Sport" dataset with \(\mathcal{K}_{u}=25\%\), the number of overlapped users in the training set was calculated as (27,519 + 107,984 - 16,377 * 2) * 0.25 * 0.8 = 20,549. The same sampling strategy was applied to the validation set, while the test set was not downsampled. This sampling strategy can simulate the occurrence of selection bias in the open-world environment, where the training set mainly consists of users who are more likely to be selected or exposed, while the test set covers a wider range of users without careful selection. All evaluation metrics used in this study indicate better performance with higher values. Regarding the unseen users in \(\mathcal{D}\), we use the non-overlapping users who were not selected for the observed data as a substitute for unseen users. We remove the actual ratings for the unseen users while retaining their sequences.

**Parameter Settings** To ensure a fair comparison between different approaches, we set the same hyper-parameters for all of them. Specifically, we fixed the embedding dimension to 128, batch size to 512, learning rate to 0.001, and negative sampling number to 1 for training and 199 for validation and testing. We used the Adam optimizer to update all parameters. For the comparison baselines, we adopted the hyper-parameter values reported in the official literature. In the case of SDSR models combined with our model, we did not modify the hyper-parameters of the SDSR models. Additionally, the threshold value to control the group flag \(k\) is set to 0.7 and the size of the sampled users is set to the batch size. We set \(\lambda_{1}=0.01\) and \(\lambda_{23,4,5}=1e^{-4}\). The learning rate for the first step with loss \(\mathcal{L}_{e}\) is \(1e^{-3}\) and the rate for the second step with loss \(\mathcal{L}_{r}\) is \(1e^{-5}\).

### Compared methods

**Single-domain sequential recommendation methods:**

**BERT4Rec**(Wang et al., 2017) designs a bidirectional self-attention network to model user behavior sequences. To prevent information leakage and optimize the training of the bidirectional model, a Cloze objective is used to predict the randomly masked items in the sequence by considering both their left and right context. The implementation of BERT4Rec in PyTorch can be found at the URL. 4

Footnote 4: [https://github.com/jaywonchung/BERT4Rec-VAE-Pytorch](https://github.com/jaywonchung/BERT4Rec-VAE-Pytorch)

**GRU4Rec**(Wang et al., 2017) tackles the issue of modeling sparse sequential data while also adapting RNN models to the recommender system. To achieve this, the authors propose a new ranking loss function that is specifically designed for training these models. The implementation of GRU4Rec in PyTorch can be found at the URL 5.

Footnote 5: [https://github.com/hungpathah/GRU4REC-pytorch](https://github.com/hungpathah/GRU4REC-pytorch)

**SARec**(Wang et al., 2017) is a self-attention based sequential model that addresses the challenge of balancing model parsimony and complexity in recommendation systems. By using an attention mechanism, SASRec identifies relevant items in a user's action history and predicts the next item based on relatively few actions, while also capturing long-term semantics like an RNN. This enables SASRec to perform well in both extremely sparse and denser datasets. The implementation of SASRec in PyTorch can be found at the URL 6.

Footnote 6: [https://github.com/primer/SARec/pytorch](https://github.com/primer/SARec/pytorch)

**Conventional Cross-domain recommendation methods:**

**STAR**(Wang et al., 2017) aims to train a single model to serve multiple domains by leveraging data from all domains simultaneously. The model captures the unique characteristics of each domain while also modeling the commonalities between different domains. It achieves this by using a network structure consisting of two factorized networks for each domain: one shared network that is common to all domains and one domain-specific network tailored to each domain. The weights of these two networks are combined to generate a unified network. The implementation of Pi-Net in Tensorflow can be found at the URL 7.

Footnote 7: [https://github.com/RManLoa/MAMDR/tree/master](https://github.com/RManLoa/MAMDR/tree/master)

**MAMDR**(Wang et al., 2017) presents a novel model agnostic learning framework called MAMDR for multi-domain recommendation (MDR). It addresses the challenges of varying data distribution and conflicts between domains in MDR. MAMDR incorporates a Domain Negotiation strategy to alleviate conflicts and a Domain Regularization approach to improve parameter generalizability. It can be applied to any model structure for multi-domain recommendation. The work also includes a scalable MDR platform used in Taobao for serving thousands of domains without specialists. In the comparison, we utilize their official implementation in Tensorflow, which can be found at the URL. 8

Footnote 8: [https://github.com/#ManLoa/MAMDR/tree/master](https://github.com/#ManLoa/MAMDR/tree/master)

**SSCDR**(Wang et al., 2017) addresses the challenge of inferring preferences for cold-start users based on their preferences observed in other domains. SSCDR proposes a semi-supervised mapping approach that effectively learns the cross-domain relationship even with limited labeled data. It learns latent vectors for users and items in each domain and encodes their interactions as distances. The framework then trains a cross-domain mapping function using both labeled data from overlapping users and unlabeled data from all items. SSCDR also incorporates an effective inference technique that predicts latent vectors for cold-start users by aggregating their neighborhood information.

**Cross-domain sequential recommendation methods:**

**Pi-Net**[(27)] simultaneously generates recommendations for two domains and shares user behaviors at each timestamp to address the challenges of identifying different user behaviors under the same account and discriminating behaviors from one domain that could improve recommendations in another. By leveraging parallel information sharing, Pi-Net improves recommendation accuracy and efficiency for cross-domain scenarios in the Shared-account Cross-domain Sequential Recommendation task. The implementation of Pi-Net in Tensorflow can be found at the URL 9.

Footnote 9: [https://github.com/manurysang/PINet](https://github.com/manurysang/PINet)

**DASL**[(19)] addresses the limitation of previous cross-domain sequential recommendation models by considering bidirectional latent relations of user preferences across source-target domain pairs, providing enhanced cross-domain CTR predictions for both domains simultaneously. The proposed approach features a dual learning mechanism and includes the dual Embedding and dual Attention components to extract user preferences in both domains and provide cross-domain recommendations through a dual-attention learning mechanism. The implementation of DASL in Tensorflow can be found at the URL 10.

Footnote 10: [https://github.com/dpv6x/DASL](https://github.com/dpv6x/DASL)

**C\({}^{2}\)DSR**[(2)] enhances recommendation accuracy by addressing the bottleneck of the transferring module and jointly learning single- and cross-domain user preferences through leveraging intra- and inter-sequence item relationships. This approach overcomes the limitations of previous methods and captures precise user preferences. The implementation of C\({}^{2}\)DSR in PyTorch can be found at the URL 11.

Footnote 11: [https://github.com/ltx/DASL](https://github.com/ltx/DASL)

**Debiasing methods for recommendation:**

**DCRee**[(47)] is a new recommendation paradigm that claims to unify sequential pattern encoding with global collaborative relation modeling. It attempts to address the issues of label shortage and the inability of current contrastive learning methods to tackle popularity bias and disentangle user conformity and real interest. The implementation of DCRee in PyTorch can be found at the URL 12.

Footnote 12: [https://github.com/ltx/DASL](https://github.com/ltx/DASL)

**CaseQ**[(46)] is proposed to alleviate the effects of popularity bias and temporal distribution shift in a single-domain sequential recommendation from training to testing. To achieve this, CaseQ employs a hierarchical branching structure combined with a learning objective based on backdoor adjustment, which enables the learning of context-specific representations. The implementation of CaseQ in PyTorch can be found at the URL 13.

Footnote 13: [https://github.com/cltx/DCRee](https://github.com/cltx/DCRee)

**IPSCDR**[(21)] has developed a novel Inverse-Propensity-Score (IPS) estimator that is tailored for cross-domain scenarios. The approach also incorporates three types of restrictions for propensity score learning. By utilizing these methods, IPSCDR effectively alleviates domain biases, including selection bias and popularity bias, when transferring user information between domains. As there is no official code release available, we have reconstructed the code for IPSCDR by adapting a related IPS-based framework. The implementation of the IPS-based framework in PyTorch can be found at the URL 14.

Footnote 14: [https://github.com/kmk/ipsc/caseq](https://github.com/kmk/ipsc/caseq)

### Hyperparameter Analysis

**The trade-off parameter \(\lambda_{1}\).** To evaluate the impact of the trade-off parameter \(\lambda_{1}\) in the loss function, we conduct a series of experiments with different values of \(\lambda_{1}=0.001,0.01,0.1\) to search for the optimal value for our AMID model. The experiments are run five times and the results are reported by mean and variance. From the results in Table 7-8, it can be observed that the SDSR models with \(\lambda=0.01\) achieved the best performance on both the Cloth-Sport and Phone-Elec scenarios with different \(\mathcal{K}_{u}\). Moreover, in the Cloth-Sport scenario, the models with \(\lambda=0.001\) perform better than those with \(\lambda=0.1\), while in the Phone-Elec scenario, the models with \(\lambda=0.1\) perform better than those with \(\lambda=0.001\).

**The threshold \(k\) for the group.** We conduct ablation experiments by varying the threshold \(k\in\{0.5,0.6,0.7,0.8,0.9\}\) to investigate the impact of the threshold \(k\) on constructing interest groups in the multi-interest information module. We measure the average evaluation scores using NDCG@10 and HR@10 on the Cloth-Sport and Phone-Elec scenarios. Our results show that a larger threshold \(k\) (\(0.5\to 0.7\)) leads to better performance, as more related interest information can be transferred. However, when the threshold \(k\) is increased beyond 0.7, the model's performance drops due to noise interference and redundant information. Therefore, to achieve superior performance, we set the threshold \(k\) to 0.7.

**The number of the sampled users.** To explore the impact of the number of sampled users on the multi-interest information module, we conduct ablation experiments varying the number of sampled users from 128 to 1024. We measure the average evaluation scores (NDCG@10 and HR@10) for both Cloth-Sport and Phone-Elec scenarios, and the results are shown in Figure 8. Our findings suggest that an increase in the number of sampled users initially improves the recommendation performance, but it eventually declines when the matching neighbors reach 1024. This observation indicates that having too few sampled users would provide insufficient transferred information, while too many sampled users could introduce interference noise and compromise the model's performance. In practice, we select the number of sampled users to be 512 as it results in the best performance for our model.

## Appendix D Potential Societal Impacts

We propose an adaptive multi-interest debiasing framework to enhance the performance of most off-the-shelf SDSR methods. By utilizing our approach, e-commerce companies can recommend more relevant products to users and increase their revenue. Furthermore, our model AMID devises a debiasing framework that attends to minority users and explores their potential interests. As a result, our work promotes fairness in the recommender system and may help mitigate social inequality that may arise from algorithmic biases.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \multirow{2}{*}{**Methods**} & \multicolumn{4}{c}{**Cloth-domain recommendation**} & \multicolumn{4}{c}{**Sport-domain recommendation**} \\ \cline{2-9}  & \multicolumn{2}{c}{\(\mathcal{K}_{u}\)=25\%} & \multicolumn{2}{c}{\(\mathcal{K}_{u}\)=75\%} & \multicolumn{2}{c}{\(\mathcal{K}_{u}\)=25\%} & \multicolumn{2}{c}{\(\mathcal{K}_{u}\)=75\%} \\ \cline{2-9}  & NDCG@10 & HR@10 & NDCG@10 & HR@10 & NDCG@10 & HR@10 & NDCG@10 & HR@10 \\ \hline \hline \(\lambda_{4}\) = **0.001** : & & & & & & & & \\ BERT4Rec [38] + AMID & 2.89\(\pm\)0.10 & 5.63\(\pm\)0.12 & 3.74\(\pm\)0.21 & 6.98\(\pm\)0.25 & 4.65\(\pm\)0.27 & 9.17\(\pm\)0.30 & 5.63\(\pm\)0.21 & 10.55\(\pm\)0.43 \\ GRU4Rec [12] + AMID & 3.01\(\pm\)0.15 & 5.82\(\pm\)0.19 & 3.79\(\pm\)0.13 & 7.20\(\pm\)0.32 & 4.77\(\pm\)0.11 & 9.32\(\pm\)0.31 & 5.81\(\pm\)0.22 & 10.91\(\pm\)0.25 \\ SA8Rec [17] + AMID & 3.08\(\pm\)0.29 & 6.05\(\pm\)0.35 & 4.11\(\pm\)0.09 & 7.50\(\pm\)0.27 & 4.88\(\pm\)0.19 & 9.36\(\pm\)0.25 & 5.87\(\pm\)0.22 & 10.94\(\pm\)0.20 \\ \hline \hline \(\lambda_{4}\) = **0.01** : & & & & & & & \\ BERT4Rec [38] + AMID & 2.99\(\pm\)0.07 & 5.70\(\pm\)0.13 & 3.79\(\pm\)0.15 & 7.09\(\pm\)0.32 & 4.73\(\pm\)0.29 & 9.30\(\pm\)0.41 & 5.70\(\pm\)0.12 & 10.63\(\pm\)0.36 \\ GRU4Rec [12] + AMID & 3.10\(\pm\)0.17 & 5.95\(\pm\)0.16 & 3.94\(\pm\)0.15 & 7.30\(\pm\)0.30 & 4.89\(\pm\)0.10 & 9.42\(\pm\)0.28 & 5.90\(\pm\)0.17 & 11.01\(\pm\)0.17 \\ SA8Rec [17] + AMID & **3.20\(\pm\)0.22** & **6.14\(\pm\)0.33** & **4.19\(\pm\)0.10** & **7.62\(\pm\)0.20** & **4.97\(\pm\)0.15** & **9.48\(\pm\)0.22** & **5.96\(\pm\)0.14** & **11.04\(\pm\)0.26** \\ \hline \hline \(\lambda_{4}\) = **0.01** : & & & & & & & \\ BERT4Rec [38] + AMID & 2.75\(\pm\)0.12 & 5.51\(\pm\)0.15 & 3.56\(\pm\)0.10 & 6.88\(\pm\)0.39 & 4.50\(\pm\)0.27 & 9.13\(\pm\)0.51 & 5.50\(\pm\)0.17 & 10.44\(\pm\)0.34 \\ GRU4Rec [12] + AMID & 2.91\(\pm\)0.19 & 5.76\(\pm\)0.13 & 3.79\(\pm\)0.14 & 7.11\(\pm\)0.29 & 4.68\(\pm\)0.09 & 9.24\(\pm\)0.34 & 5.73\(\pm\)0.15 & 10.79\(\pm\)0.31 \\ SA8Rec [17] + AMID & 3.02\(\pm\)0.21 & 5.97\(\pm\)0.28 & 3.99\(\pm\)0.15 & 7.40\(\pm\)0.17 & 4.75\(\pm\)0.11 & 9.25\(\pm\)0.30 & 5.77\(\pm\)0.19 & 10.83\(\pm\)0.24 \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Hyperparameter anaysis (%) of \(\lambda_{4}\) on the bi-directional Cloth-Sport CDR scenario with different \(\mathcal{K}_{u}\).**

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline \multirow{2}{*}{**Methods**} & \multicolumn{4}{c}{**Phone-domain recommendation**} & \multicolumn{4}{c}{**Elec-domain recommendation**} \\ \cline{2-9}  & \multicolumn{2}{c}{\(\mathcal{K}_{u}\)=25\%} & \multicolumn{2}{c}{\(\mathcal{K}_{u}\)=75\%} & \multicolumn{2}{c}{\(\mathcal{K}_{u}\)=25\%} & \multicolumn{2}{c}{\(\mathcal{K}_{u}\)=75\%} \\ \cline{2-9}  & NDCG@10 & HR@10 & NDCG@10 & HR@10 & NDCG@10 & HR@10 & NDCG@10 & HR@10 \\ \hline \hline \(\lambda_{4}\) = **0.001** : & & & & & & & \\ BERT4Rec [38] + AMID & 7.67\(\pm\)0.25 & 14.18\(\pm\)0.41 & 7.88\(\pm\)0.39 & 14.54\(\pm\)0.21 & 10.87\(\pm\)0.14 & 18.29\(\pm\)0.30 & 11.83\(\pm\)0.15 & 19.77\(\pm\)0.37 \\ GRU4Rec [12] + AMID & 8.05\(\pm\)0.13 & 15.20\(\pm\)0.45 & 8.08\(\pm\)0.10 & 15.02\(\pm\)0.23 & 11.44\(\pm\)0.10 & 19.21\(\pm\)0.14 & 12.25\(\pm\)0.15 & 20.58\(\pm\)0.25 \\ SA8Rec [17] + AMID & 7.94\(\pm\)0.11 & 14.69\(\pm\)0.09 & 8.05\(\pm\)0.32 & 14.68\(\pm\)0.23 & 11.40\(\pm\)0.29 & 19.03\(\pm\)0.24 & 12.21\(\pm\)0.15 & 20.48\(\pm\)0.19 \\ \hline \hline \(\lambda_{4}\) = **0.01** : & & & & & & & \\ BERT4Rec [38] + AMID & 7.95\(\pm\)0.23 & 14.49\(\pm\)0.36 & 8.15\(\pm\)0.33 & 14.85\(\pm\)0.28 & 11.26\(\pm\)0.07 & 18.58\(\pm\)0.27 & 11.16\(\pm\)0.09 & 20.08\(\pm\)0.31 \\ GRU4Rec [12] + AMID & **8.33\(\pm\)0.09** & **15.49\(\pm\)0.50** & **8.34\(\pm\)0.21** & **15.29\(\pm\)0.29** & **17.14\(\pm\)0.07** & **19.50\(\pm\)0.08** & **12.53\(\pm\)0.11** & **20.85\(\pm\)0.20** \\ SA8Rec [17] + AMID & 8.20\(\pm\)0.10 & 14.99\(\pm\)0.16 & 8.32\(\pm\)0.20 & 14.96\(\pm\)0.15 & 11.71\(\pm\)0.23 & 19.28\(\pm\)0.23 & 12.52\(\pm\)0.13Figure 8. The results of SASRec\(\star\)AMID with different number of the sampled users on the Cloth-Sport & Phone-Elec scenarios. 25% and 75% denotes different \(\mathcal{K}_{u}\).

Figure 7. The results of SASRec\(\star\)AMID with different threshold \(k\) on the Cloth-Sport & Phone-Elec scenarios. 25% and 75% denotes different \(\mathcal{K}_{u}\).