<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_EMPTY:1]\n' +
      '\n' +
      'systems fall into _offline tuning_ systems since the tuning process runs in a copied instance using a workload trace within a certain period. On the one hand, they only consider part of the workload, assuming the workload pattern never shifts too far. The tuning process usually takes hours or days to find near-to-optimal configurations. Consequently, the offline methods can not adapt to the dynamic workload from real-world applications. On the other hand, these systems rely on a copied instance and require the infrastructure to replay the workloads [69]. For the consideration of data privacy, the copied instance should be launched in the users\' environment (e.g., Virtual Private Cloud). Every time end-users want to tune the configurations, they should purchase a copied instance and prepare the workload and data based on a snapshot. This cloning process is time-consuming makes the tuning task expensive and delayed. The Total Cost of Ownership (TCO) is increased for end-users, which is an important factor in the cloud.\n' +
      '\n' +
      'To fill the above gaps, it is desirable to design a configuration tuning system coordinating with the online database to adapt to the workload changes directly and conveniently. Tuning configuration knobs for an online database does not need to copy the instance, so the TCO issue is directly solved. However, the online tuning system should address the following challenges. First, high availability is essential to online databases, and it is not allowed to sample bad configurations during tuning, causing the performance downgrade. However, it is inevitable for existing approaches to sample bad configurations. Figure 1 (c) shows the process of existing automatic configuration tuning systems tuning the TPCC workload in a MySQL instance. OtterTune utilizes Bayesian Optimization to suggest promising configurations by balancing exploration and exploitation, and CDBTune adopts reinforcement learning to learn the tuning policy via trials-and-error. They could find near-to-optimal configurations. However, 50%-70% of their recommended configurations are worse than the default. The tuning even causes two system hangings by configuring the total memory (e.g., buffer pool, insert buffer, sort buffer, etc.) larger than the machine\'s physical capacity. Such harmful recommendations would expose enormous risks to online databases. Second, the dynamic environment in the cloud should be considered, and we can not directly leverage the offline tuning approaches. Figure 1 (d) shows the performance of applying the best configurations obtained from the tuning processes in Figure 1 (c) to a dynamic workload (changing the transaction weight gradually from the original TPC-C workload). We observe that the applied configurations have better performance at the beginning. However, the configurations recommended by the offline tuning systems mismatch the dynamic workload afterward. They become worse than the default after 75 minutes. From these perspectives, the online database tuning should fulfill the following desiderata:\n' +
      '\n' +
      '**Dynamicity**: The tuner is capable of responding to the dynamic environment (e.g., workload and its underlying data) adaptively.\n' +
      '\n' +
      '**Safety**: The tuner should recommend configurations that do not downgrade the database performance during the tuning process.\n' +
      '\n' +
      'In this paper, we propose OnlineTune, an online tuning system that tunes the databases safely and adaptively in the constantly changing cloud environment. For **dynamicity**, we formulate the online tuning problem into a contextual bandit problem. OnlineTune features the context and optimizes the database performance over a context-configuration joint space. We propose a clustering and model section strategy to scale up OnlineTune with accumulated observations. OnlineTune clusters the observations, fit multiple models, and selects the appreciate one for a given context. For **safety**, OnlineTune safely explores the configuration space. It evaluates the safety of configurations by leveraging both the black-box knowledge (i.e., posterior estimate from the model) and white-box knowledge (i.e., heuristics-based rules from the domain experience). Since satisfying the safety constraint in the continuous and high-dimension space is non-trivial [34], we transform the high-dimension optimization problem into a sequence of subspace problems that can be solved efficiently. Each model maintains a configuration subspace centered around the best configuration found so far. OnlineTune starts from configurations similar to those known to be safe and expands the subspace to facilitate further explorations. Specifically, we make the following contributions:\n' +
      '\n' +
      '* To address the challenges in real DBMS scenarios with the dynamic workload, we define the online tuning problem and solve it as a contextual bandit problem with safety constraints. To the best of our knowledge, OnlineTune is the first online configuration tuning system for DBMS with safety consideration.\n' +
      '* We propose a context featurization model that extracts features of workloads and underlying data. Using this, OnlineTune adopts the contextual Bayesian optimization technique to optimize the database adaptively with constantly changing environments.\n' +
      '* To enhance the scalability of OnlineTune with extensive data in the cloud, we propose a clustering and model selection strategy that significantly decreases the computation complexity.\n' +
      '\n' +
      'Figure 1. Motivating Examples: Figures (a) and (b) show the dynamic environment in the cloud. Figures (c) and (d) show the performance of existing auto-tuners. In (c), the tuners tune a static workload with numerous unsafe trials. In (d), we apply the best configurations found in (c) to a dynamic workload and observe decreasing improvement.\n' +
      '\n' +
      '* To solve the safety issue, we combine the black-box and the white-box knowledge to evaluate the safety of configurations and propose a safe exploration strategy via subspace adaptation, largely reducing the risks of applying harmful configurations.\n' +
      '* We implement the proposed method and evaluate on dynamic workloads from benchmarks and the real-world application. Compared with the state-of-the-art techniques, OnlineTune achieves 14.4%-165.3% improvement on cumulative improvement while decreasing 91.0%-99.5% unsafe recommendations.\n' +
      '\n' +
      'The remainder of the paper is organized as follows. We review the related works in Section 2 and formally define the online tuning problem in Section 3. A system overview of OnlineTune is presented in Section 4, followed by a description of our techniques for contextual performance modeling in Section 5, safe configuration recommendation in Section 6. Section 7 presents our experimental evaluation. Finally, we conclude in Section 8.\n' +
      '\n' +
      '## 2. Related Work\n' +
      '\n' +
      '**Configuration Tuning.** There has been an active area of research on tuning configurations of DBMS, which can be summarized as:\n' +
      '\n' +
      '* **Rule-based.** Rule-based methods recommend configurations using heuristic rules. Database vendors develop tuning tools to provide DBA with knobs recommendations through identifying database\'s bottlenecks due to misconfigurations (Krishnan et al., 2017) or asking the DBA questions about their application (Wainwright et al., 2018). Wei et al. propose to generate fuzzy rules for database tuning (Wang et al., 2019). BestConfig (Wang et al., 2019) searches configurations based on several heuristics. Rule-based methods strongly depend on the assumptions of their heuristics and fail to utilize knowledge gained from previous tuning efforts.\n' +
      '* **Learning-based.** iTuned (Wang et al., 2019), Ottertune (Brandt et al., 2019) and ResTune (Wang et al., 2019) use Bayesian Optimization (BO) based method, modeling the tuning as a black-box optimization problem. Reinforcement Learning (RL) is adopted in (Wang et al., 2019; Wang et al., 2019) to tune DBMS by learning a neural network between the internal metrics and the configurations. In the field of data analytic systems, ReIM (Wang et al., 2019) studies the problem of tuning the memory allocation and develops an empirically-driven algorithm. And Tuneful (Wang et al., 2019; Wang et al., 2019) combines incremental Sensitivity Analysis and BO to prone configuration space. Although RL methods can adapt to workloads by fine-tuning its neural network, additional time and evaluation samples are needed. All the above methods train a machine learning model to learn the offline tuning policy and cannot promptly respond to the dynamic environment. Besides, they do not consider the safety constraints when interacting with the database, which restricts them from being deployed in production.\n' +
      '\n' +
      '**Query Featurizing.** Query featurizing aims at translating plain SQL into their vectorized representations. We summarize it into SQL text parsing and logical plan parsing based on the input. SQL text parsing directly processes the query texts. TF-IDF is used to represent a query as a collection of the weighted frequencies of its individual word tokens (Wang et al., 2019; Wang et al., 2019). However, its unbounded vocabulary makes generalization across workloads difficult. Therefore ResTune restricts its calculation to reserved SQL keywords (Wang et al., 2019). To support larger vocabulary, (Wang et al., 2019; Wang et al., 2019) resort to representation learning which is frequently used in NLP. Representation learning produces dense vectors capturing nuanced relationships of unstructured data. They use deep models, e.g., LSTM or CNN, to learn the distributional embeddings. Logical plan parsing parses a query plan to aggregate key features, including the cost or categories of operators, scanned tables, and predicates. It has been adopted by many state-of-the-art works in the field of query-performance prediction or cardinality estimation (Chen et al., 2018; Chen et al., 2018; Chen et al., 2018; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019). QTune encodes query type, involved tables, query operators and the corresponding costs to predict the internal metrics of a database (Wang et al., 2019).\n' +
      '\n' +
      '**Bayesian Optimization.** Our algorithm falls under the general umbrella of Bayesian optimization (BO). It learns and optimizes a black-box function over configuration space (Wang et al., 2019). BO works iteratively: (1) updating the surrogate model that describes the relationship between configurations and their performances and (2) choosing the next configuration to evaluate by computing the acquisition function value. The acquisition function measures the utility of candidate points for the next evaluation by trading off the exploration of uncertain areas and exploiting promising regions.\n' +
      '\n' +
      'BO has been extensively used in many scenarios, including hyper-parameter tuning (Chen et al., 2018; Chen et al., 2018; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019), experimental design (Krishnan et al., 2017) and controller tuning (Chen et al., 2018; Chen et al., 2018; Wang et al., 2019; Wang et al., 2019). Contextual BO considers the environmental conditions by augmenting the GP kernel with extra context variables and uses \\(CGP-UCB\\) to select promising action (Wang et al., 2019). DBA Bandit (DBA Bandit, 2019) chooses a set of indices from finite and discrete configuration space based on the context of indexed columns and derived statistics from database optimizer. It achieves an \\(\\tilde{O}(\\sqrt{n})\\) regret bound after playing \\(n\\) rounds as a safety guarantee, implying that the per-step average cumulative regret approaches zero after sufficiently many steps. However, guaranteeing the safety of every step is still challenging but vital for mission-critical applications. Recently, Constrained BO is proposed to optimize a black-box function with unknown constraint (Wang et al., 2019; Wang et al., 2019; Wang et al., 2019). But the constraint is not considered safety-critical, and the algorithm is allowed to evaluate unsafe parameters. The main instance of safe optimization is SAFEOPT algorithm (Wang et al., 2019). However, its formulation relies on a discretization of configuration space, which hinders high-dimensional applications. Meanwhile, scaling up BO with high dimensions is another active area. Recent works propose local modeling (Wang et al., 2019; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019) and space partitioning (Wang et al., 2019; Wang et al., 2019), which achieve strong empirical results in high dimensional problems. Motivated by the advances, OnlineTune refines a configuration _subspace_ that can be discretized efficiently.\n' +
      '\n' +
      '## 3. Problem Statement\n' +
      '\n' +
      'Consider a database system with a continuous configuration space \\(\\Theta=\\Theta_{1}\\times\\Theta_{2}\\times...\\times\\Theta_{m}\\) and a context space \\(C\\). The context \\(c\\in C\\) is uncontrollable environmental conditions, e.g., dynamic workloads. We denote the database performance metrics as \\(f\\), which can be any chosen metric to be optimized, such as _throughput_, _99%th percentile latency_, etc. Given a configuration \\(\\theta\\) and context \\(c\\), the corresponding performance \\(f(\\theta,c)\\) can be observed only after evaluation.\n' +
      '\n' +
      'As we discussed, the online tuner should satisfy two requirements: **dynamicicity** and **safety**. The **dynamicicity** requirement means the tuner should consider changing environmental conditions (context) when recommending the configurations. We formalize it as a contextual bandit problem (Wang et al., 2019), where at each iteration \\(t\\), the tuner receives context \\(c_{t}\\) and outputs a configuration \\(\\theta_{t}\\) to maximize the payoff (i.e., database performance \\(f\\)). The **safety** requirement indicates that we additionally need to ensure that, for each tuning iteration \\(t\\), \\(f_{t}\\geq\\tau\\) holds, where \\(f_{t}=f(\\theta_{t},c_{t})\\) and \\(\\tau\\in\\mathbb{R}\\) is a specific safety threshold. We define the configurations that satisfy the above condition to be _safe_. The safety threshold indicates the degree of risk end-users can tolerate when adopting the tuning approach. As the database is a mission-critical system, the service level agreement must be guaranteed by the cloud providers under the vendor default configuration (Zhu et al., 2017; Zhang et al., 2018). Intuitively, the safety threshold is set to the database performance under the default configuration (denoted as default performance). In case the default performance fluctuates with workload changes, we assume that the default performance for any given workload can be acquired. Note in practice, the default performance can be easily obtained or predicted when a historical knowledge base is available (such as in (Zhu et al., 2017; Zhang et al., 2018; Zhang et al., 2018)). For example, the user could train a regression model that inputs the context and outputs the default performance. Even without any previous knowledge, we can take some time to observe the default performance. At the very beginning, without any assumptions about \\(f\\), searching the safe configurations is a nonsensical task. Hence, we also assume that, before the optimization, we are given an initial safety set that contains at least one safe configuration. Assuming that the objective is a maximization problem, the online tuning problem to solve for \\(c_{t}\\in\\mathcal{C}\\) is :\n' +
      '\n' +
      '\\[\\begin{split}\\operatorname*{arg\\,max}_{\\theta_{t}\\in\\Theta}f( \\theta_{t},c_{t}),\\\\ \\text{subject to }f(\\theta_{t},c_{t})\\geq\\tau.\\end{split} \\tag{1}\\]\n' +
      '\n' +
      '## 4. Overview of Onlinetune\n' +
      '\n' +
      '**Workflow.** To conduct online tuning, OnlineTune first queries the default configuration and its performance to build an initial safety set. The safety threshold is set to the default performance, as discussed in Section 3. Then, OnlineTune functions iteratively. It adapts to the dynamic environment by featurizing the context, recommending a promising configuration \\(\\theta_{t}\\) at the beginning of an iteration, and evaluating its performance during the iteration to update the model. The interval size (i.e., time for one iteration) controls the granularity of OnlineTune\'s adaptation to the dynamicity. Given a tuning time, OnlineTune can collect more observations and make more fine-grained suggestions with a small interval size. We use a three-minute interval by default and conduct sensitivity analysis in our experiments. Within one iteration, the workflow of OnlineTune forms an iterative cycle, as presented by 1-2 in Figure 2. The workflow consists of two stages: contextual performance modeling and safe configuration recommendation.\n' +
      '\n' +
      'Contextual performance modeling aims at obtaining a surrogate model that predicts the performance of given configurations in dynamic environments. The surrogate is a contextual Gaussian Process (GP) model fitted on historical observations (or the initial safety set, if no historical observations exist). 1 OnlineTune first captures the dynamic factors (e.g., workload and its underlying data) through context featurization, obtaining a context (see Section 5.1 for details). 2 It then selects a contextual GP model fitted on the observations from the cluster with similar contexts. The clusters are periodically re-clustered in an offline manner (Section 5.3). 3 The construction of contextual GP is introduced in Section 5.2.\n' +
      '\n' +
      'Safe configuration recommendation aims at selecting a safe and promising configuration from the configuration space. To avoid the aggressive exploration of BO, especially the over-exploration of boundaries, OnlineTune reduces the optimization over the whole configuration space into a sequence of _subspace_ optimization. All operations in this stage are restricted in the _subspace_ where the safe optimization problem can be discretized and solved efficiently. In essence, the _subspace_ is centered around the best configuration estimated so far, gradually moving towards the optimal. This stage inputs the selected surrogate model and its corresponding subspace. 3 For a newly fitted model, OnlineTune initializes a subspace centered around the best-estimated configuration. Otherwise, OnlineTune adapts the subspace according to the tuning history, e.g., expands the subspace when making consecutive successes (Section 6.1). 4 The adapted subspace is discretized to build a candidate set. OnlineTune assesses the safety of the candidates based on the model\'s lower bound estimate, forming a safety set. It also results the white box to dismiss unsafe configurations. In case the heuristic white box excludes the optimal configurations from the safety set, OnlineTune relaxes inappropriate rules (Section 6.2.2). 5 OnlineTune then selects a configuration from the safety set by either maximizing the acquisition function or exploring the safe boundaries of subspace. 6 Finally, OnlineTune applies the configuration to the online database and evaluates its performance.\n' +
      '\n' +
      '**Architecture.** The main parts of OnlineTune are deployed in a backend tuning cluster (OnlineTune sever), while the context featurization module is deployed in the database instance for data\n' +
      '\n' +
      'Figure 2. OnlineTune Workflow.\n' +
      '\n' +
      'privacy concerns. OnlineTune server maintains a data repository that stores the historical observations from the previous tuning iterations, which can be initially empty. OnlineTune server interacts with the database instance via a controller that monitors the tuning tasks\' states and transfers the data.\n' +
      '\n' +
      '## 5. Contextual Performance Modeling\n' +
      '\n' +
      'OnlineTune responds to dynamic environments when tuning online DBMSs. It augments the GP kernel with context variables to learn the relationship between \\(\\langle\\)context, configuration\\(\\rangle\\), and database performance. We first discuss how to featurize context and construct a contextual GP model. Then, we present a clustering and model selection strategy to enhance the model\'s scalability.\n' +
      '\n' +
      '### Context Featurization\n' +
      '\n' +
      'Context featurization aims to capture uncontrollable dynamic factors, which affects the relationship between configurations and database performances. When tuning the database, the workload and underlying data are constantly and continuously changing due to the upstream applications and the DML statements (e.g., insert, delete and update). OnlineTune featurizes the two factors to adapt to the dynamicity.\n' +
      '\n' +
      '#### 5.1.1. Workload Featurization\n' +
      '\n' +
      'We now illustrate how to featurize the changing workload. There are two dynamic aspects of workloads (Kang et al., 2017): (1) _query arrival rate_: the number of arriving queries per second can fluctuate. (2) _query composition_: the types of queries may change, and the ratio of queries composition may vary.\n' +
      '\n' +
      '_Query arrival rate_ can be encoded by one dimension. For _query composition_, we need to translate the plain queries into vectorized representations. We adopt representation learning techniques (Beng et al., 2017) to extract informative encoding of queries and generalize across workloads. We choose LSTM, which have been used successfully for SQL query analysis (Srivastava et al., 2017; Wang et al., 2018; Wang et al., 2018). We use a standard LSTM encoder-decoder network (Wang et al., 2018) to ease the burden of collecting labeling data. The final hidden state on the encoder network provides a dense encoding for the query. Lastly, we average the query encoding, obtaining the _queries composition_ feature of a workload.\n' +
      '\n' +
      '#### 5.1.2. Underlying Data Featurization\n' +
      '\n' +
      'Learning the distribution of database data is a non-trivial task (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). Based on the observation that only the data changes affecting the workload queries are related to the tuning policy, we use the following features from the DBMS optimizer: (1) estimate of rows to be examined by queries, (2) the percentage of rows filtered by table conditions in queries, (3) whether an index is used. The first two features are queries\' cardinality estimation, capturing the effective changes in data size and distribution. The last feature indicates the index building/dropping operation. We average the three features of queries, obtaining the underlying data feature of a workload.\n' +
      '\n' +
      'Finally, we concatenate the workload feature and underlying data feature to obtain the final context features. Although query plans can provide extra information about the query execution, e.g., operators and costs, we do not encode them as contexts. This is because the extra information is affected by OnlineTune\'s previous configuration, which is unsuitable as a context for future tuning. In addition, modern DBMSs have hundreds of plan operators (Wang et al., 2018), encoding the plan operators could cause the "dimensionality issue" when augmenting the sparse and high-dimensional variables to GP kernel. Other dynamic factors could affect database performance, such as hardware configuration, data partition change, user-invoke configurations, etc. Their changes usually occur intermittently. OnlineTune can re-initialize a tuning task when the changes occur or encode these factors. For example, OnlineTune can encode hardware configuration (e.g., memory size, #CPU) to support hardware updates. Database anomalies (e.g., out of disk space and cybercriminal attack) are out of OnlineTune\'s scope.\n' +
      '\n' +
      '### Performance Modeling with Contexts\n' +
      '\n' +
      'A performance model estimates the performance metrics given context and potential configurations. We adopt the contextual GP as the performance model, which extends the GP to support dynamic environments. GP is a popular surrogate model for objective modeling in Bayesian Optimization (Wang et al., 2018) due to its expressiveness and well-calibrated uncertainty estimates. GP provides confidence bounds on its predictions to model the objective function with limited samples. In such a scenario, other data-intensive techniques, e.g., deep learning (Wang et al., 2018; Wang et al., 2018) may struggle with low data efficiency and interpretability (Wang et al., 2018). In this section, we focus on learning the performance model and leave the phase of safe tuning in Section 6.\n' +
      '\n' +
      'We aim to learn a performance model for different but correlated contexts. In the learning phase, we construct the probability distribution \\(p\\left(f|\\theta,c,H_{t}\\right)\\) of the target function \\(f=f\\left(\\theta,c\\right)\\) given an unspecified input \\(\\theta\\), the observed context feature \\(c\\) and the observations \\(H_{t}=\\{c_{i},\\theta_{i},y_{t}\\}_{i=1}^{t}\\). The posterior distribution over \\(f\\) is a contextual GP with mean \\(\\mu_{t}\\left(\\theta,c\\right)\\) and variance \\(\\sigma_{t}^{2}\\left(\\theta,c\\right)\\):\n' +
      '\n' +
      '\\[\\begin{split}\\mu_{t}(\\theta,c)&=k^{T}\\left(K+ \\sigma^{2}I\\right)^{-1}y_{1:t},\\\\ \\sigma_{t}^{2}(\\theta)&=k\\big{(}\\left(\\theta,c \\right),\\left(\\theta,c\\right)\\big{)}-k^{T}\\Big{(}K+\\sigma^{2}I\\Big{)}^{-1}k, \\end{split} \\tag{2}\\]\n' +
      '\n' +
      'Figure 3. Generalization over Contexts: OnlineTune has the three observations (blue points) under the context \\(c=0\\) (left) and fits a posterior distribution with mean (blue line) and confidence bound (light blue area). By exploiting correlations between different contexts, OnlineTune can transfer knowledge of the objective function to a different context \\(c=0.1\\) (middle), but little knowledge to the distant context (right). The red label shows the estimated safety set with zero as the safety threshold.\n' +
      '\n' +
      'where \\(k=[k((\\theta_{1},c_{1}),(\\theta,c)),...,k((\\theta_{t},c_{t}),(\\theta,c))]^{T}\\) and K is a covariance matrix whose \\((i,j)\\)th entry is \\(K_{i,j}=k((\\theta_{i},c_{i}),(\\theta_{j},c_{j}))\\). The kernel \\(k((\\theta,c),(\\theta^{{}^{\\prime}},c^{{}^{\\prime}}))\\) should model the distances between points of configuration and context. Concretely, we construct an additive kernel \\(k_{\\Theta}(\\theta,\\theta^{{}^{\\prime}})+k_{C}(c,c^{{}^{\\prime}})\\). We use a linear kernel \\(k_{C}(c,c^{{}^{\\prime}})\\) to model the dependence on contexts and a Martin kernel \\(k_{\\Theta}(\\theta,\\theta^{{}^{\\prime}})\\) to model the nonlinear performance on configurations. Intuitively, such design could model overall trends according to the context, and the configuration-specific deviation from this trend (Mikolov et al., 2017).\n' +
      '\n' +
      'The composite kernel implies that function values are correlated when configurations and contexts are similar (Kang et al., 2017). We expect the same configuration across correlated contexts to have similar performance predictions. The correlations between contexts can significantly speed up the tuning. Figure 3 shows a simple scenario with a one-dimension context feature and a one-dimension configuration. Even though the algorithm has only explored the configuration space at the first context (z = 0, left figure), the correlation between the functions generalizes information to the unobserved context (z = 0.1, central figure). The knowledge transfer improves data efficiency and reduces the number of required evaluations.\n' +
      '\n' +
      '### Clustering and Model Selection\n' +
      '\n' +
      'The contextual GP can model the database performance in dynamic environments. However, it has \\(O(n^{3})\\) complexity with \\(n\\) observations. The cubical computation complexity limits the applicability with increasing observations in the cloud. To tackle this problem, we propose a clustering and model selection strategy based on the similarity of context features. The observations are clustered, and the number of observations in each cluster can be bounded under a constant number \\(P\\). OnlineTune fits multiple contextual GPs based on the clusters and learns a decision boundary for model selection. Therefore the complexity is bounded by \\(O(P^{3})\\), which makes OnlineTune can scale with increasing observations.\n' +
      '\n' +
      'Algorithm 1 presents the procedure. We first perform the DBSCAN clustering algorithm (Kang et al., 2017) based on context features \\(\\{c_{i}\\}_{1}^{t}\\), obtaining a cluster label \\(l_{i}\\) for each \\(c_{i}\\) (Line 2), as shown in Figure 4 (a). For each cluster, OnlineTune fits a contextual GP model using its observations (Line 3). To select a model for unseen contexts, OnlineTune uses SVM to learn a non-linear decision boundary (Line 4, as shown in Figure 4 (b)). We choose SVM for its simplicity, ease of training, and the need for fewer samples to generalize well in practices (Zhu et al., 2017). Besides improving scalability, such clustering excludes the observations with distant contexts from the training set for the GP model, preventing the "negative transfer" (Zhu et al., 2017).\n' +
      '\n' +
      'Augmented observations are classified into a cluster based on their context and the learned boundary. However, the distribution of context features may shift as more observations are collected, and the previous clustering and boundary need to be re-learned periodically. OnlineTune maintains the existing clustering and a simulated new clustering to determine whether to re-learn or not (Line 1). The difference between the two kinds of clusterings indicates context shifts. We use a mutual information-based score (MI) to quantify the difference. MIs close to zero indicate two vastly dissimilar clusterings, while MIs close to one indicates the opposite. When the MI score is smaller than a threshold (we set 0.5 in the experiments), the re-clustering is triggered, and the boundary and models are updated based on the revised clusters (Line 2-4).\n' +
      '\n' +
      '## 6. Safe Configuration Recommendation\n' +
      '\n' +
      'OnlineTune aims to optimize the database performance while ensuring safety throughout the tuning process. Instead of optimizing the performance function over the global configuration space, OnlineTune restricts its optimization in a subspace and gradually expands it towards the optimum. The subspace restriction enables a fine discretization to generalize safety and mitigates the over-exploring nature of BO. This section introduces how OnlineTune adapts the subspace, forms a safety set within the subspace, and selects a safe and promising configuration.\n' +
      '\n' +
      '### Subspace Adaptation\n' +
      '\n' +
      'To expand towards the global optimum, the configuration subspace is adjusted iteratively and alternated between hypercube and line regions, as shown in Algorithm 2.\n' +
      '\n' +
      'The hypercube region \\(\\{\\theta\\left\\|\\left\\|\\theta-\\theta_{best}\\right\\|_{2}\\leq R_{n}\\}\\cap\\Theta\\) is defined with a region center \\(\\theta_{best}\\) and a radius \\(R_{m}\\). The region center \\(\\theta_{best}\\) is set to the best configuration observed so far. The radius \\(R_{n}\\) controls the optimization space. If \\(R_{n}\\) is large enough for the hypercube region to contain the whole configuration space, this will be equivalent to running standard global BO, while a small \\(R_{n}\\) may slow down the exploration. At initialization, \\(R_{n}\\) is set to a base value (e.g., 5% ranges of each dimension). It is typical behavior to shrink \\(R_{n}\\) after consecutive "failures" and expand it after consecutive "successes" (Zhu et al., 2017). We define a "success" as recommending a configuration better than the previous one and a "failure" as a recommendation that does not. The details about adjusting \\(R_{n}\\) is illustrated in Line 4-7.\n' +
      '\n' +
      'The line region is a one-dimensional affine subspace \\(\\{\\theta_{best}+ad:a\\in\\mathbb{R}\\}\\cap\\Theta\\), defined with the offset \\(\\theta_{best}\\) and a direction \\(d\\in\\mathbb{R}^{m}\\). It is proved that the optimization with line regions can converge globally (Zhu et al., 2017). The direction of the line region determines the trace of optimization. We implement two strategies to generate the directions: random direction (increasing the exploration) and important direction (aligned with the important configuration knob, increasing exploitation). Appendix contains the details.\n' +
      '\n' +
      'Figure 4. Clustering and Model Selection: Each point denotes a sample and the learned boundary splits the context space.\n' +
      '\n' +
      'The subspace is first initialized as a hypercube region to restrict the optimization near the initial safety set (Lines 1-2). The hypercube region encourages optimization densely in the interior but may lead to an over-exploitation, even getting trapped in a local optimum (Kumar et al., 2017). OnlineTune switches to line region to control the trade-off between exploitation and safe exploration (Lines 3-13). The alternation is triggered by a switching rule (Lines 8 and 12): when no unevaluated safe configuration exits in \\(\\Theta_{m}\\) or a certain number of consecutive failures to recommend better configurations, OnlineTune switches to another type of subspace. The update of \\(\\theta_{best}\\) moves the subspace towards the optimum.\n' +
      '\n' +
      '### Safety Assessment\n' +
      '\n' +
      'OnlineTune discretizes the adapted subspace to build a candidate set. Then the safety of each candidate is assessed based on the confidence bounds of the contextual GP (black-box knowledge) and the existing domain knowledge (white-box knowledge).\n' +
      '\n' +
      '#### 6.2.1. Black-Box Knowledge\n' +
      '\n' +
      'Given context \\(c\\), OnlineTune utilizes the confidence bounds of the selected contextual GP model \\(m^{\\text{tr}}\\) to access the safety of \\(\\theta\\):\n' +
      '\n' +
      '\\[\\begin{split}& l_{n}(\\theta,c)=\\mu_{n}(\\theta,c)-\\beta\\sigma_{n}( \\theta,c),\\\\ & u_{n}(\\theta,c)=\\mu_{n}(\\theta,c)+\\beta\\sigma_{n}(\\theta,c), \\end{split} \\tag{3}\\]\n' +
      '\n' +
      'where \\(l_{n}\\) and \\(u_{n}\\) are the lower and upper bound predictions, with \\(\\mu_{n}\\) and \\(\\sigma_{n}\\) from Equation 2. The parameter \\(\\beta\\) controls the tightness of the confidence bounds, and we set its value following the study of Srinivas et al. (Srinivas et al., 2019). The true function value \\(f\\) falls into the confidence interval \\([l_{n}(\\theta,c),u_{n}(\\theta,c)]\\) with a high probability. We can determine safe configurations with \\(l_{n}(\\theta,c)>\\tau\\); that is, configurations with worst-case performance still above the safety threshold \\(\\tau\\). We restrict the performance modeling in a local subspace containing previously evaluated configurations, and the local modeling can be trusted more than global modeling. This is also the rationale behind the trust-region methods from stochastic optimization (Srinivas et al., 2019).\n' +
      '\n' +
      '#### 6.2.2. White-Box Knowledge\n' +
      '\n' +
      'Although the relationship between configurations and database performances is complex. Domain knowledge does exist to dismiss bad configurations. For example, the total buffer size can not exceed the physical memory capacity of the deployed machine. Experienced DBA tunes the databases based on domain knowledge, and some database tuning tools also use heuristics to give tuning suggestions (Bradner et al., 2017; Krizhevsky et al., 2017; Krizhevsky et al., 2017; Krizhevsky et al., 2017). Such white-box tuning provides intuitive suggestions and could serve as a warm starting and space pruning component for ML-based tuning. When forming the final safety set, OnlineTune results the white box and dismisses the unsafe or unproming configurations deviated from the white box\'s suggestions. We implement OnlineTune\'s white-box assistant using MysqlTuner (Bradner et al., 2017). MysqlTuner examines the DBMS metrics and uses static heuristics to suggest setting ranges for configurations. The ranges are generated based on rules (e.g., setting key buffer size larger than the total MyISAM indexes size or increasing the join buffer size if #joins without indexes per day is larger than 250). OnlineTune removes the configurations not satisfying the white box\'s suggestions from its safety set. If the white box suggests a specific configuration instead of the ranges, OnlineTune filters the configurations far away from the suggested one. Other white-box tuning tools or hand-crafted rules can coexist with OnlineTune if they provide suggestions for OnlineTune\'s tuning knobs. Instead of directly applying, their suggestions can be used as white-box assistants for OnlineTune.\n' +
      '\n' +
      'However, the white box does not evolve according to the feedback, causing the trap in local optimum. The rules could even be inappropriate, excluding the optimal configurations from \\(S_{n}\\). This happens only when the white-box rule rejects a configuration while the black-box algorithm recommends this configuration (i.e., decision conflict). To prevent this, OnlineTune uses a relaxation strategy to relax inappropriate rules. OnlineTune maintains a conflict counter and a conflict-safe counter for each rule. When the decision conflict happens several times, reaching a threshold, OnlineTune will ignore the rule and recommend the controversial configuration. Note that only one rule can be ignored in the recommendation to control the interdependence between different rules. After evaluation, if the configuration is safe, the conflict-safe counter will increase by one. When a rule\'s conflict-safe counter reaches a threshold, the rule will be relaxed (e.g., the configuration range given by the rule is enlarged). The thresholds for each white-box rule can be set differently according to its credibility. A larger threshold leads to more trust in the white-box rule.\n' +
      '\n' +
      '### Candidate Selection\n' +
      '\n' +
      'After the safety set is generated, OnlineTune selects a configuration from the candidates in the safety set. Like all the black-box optimization, the selection should trade-off between two objectives: (1) exploitation, trying to localize the high-performance regions within the current safety set, (2) exploration, acquiring new knowledge, and trying to expand the current estimate of the safety set. We adopt Upper Confidence Bound (UCB) (Srinivas et al., 2019) constrained to the safety set as a sampling criterion, shown in Equation 4:\n' +
      '\n' +
      '\\[\\theta_{max}=\\operatorname*{arg\\,max}_{\\theta\\in S_{n}}\\mu_{n}(\\theta,c)+\\beta \\sigma_{n}(\\theta,c). \\tag{4}\\]\n' +
      '\n' +
      'UCB selects the configuration at locations where the upper bound of its confidence interval is maximal. Repeatedly evaluating the system performance at configurations given by UCB improves the mean estimate of the underlying function and decreases the uncertainty at candidate configurations for the maximum. The global maximum is provably found eventually (Shi et al., 2019).\n' +
      '\n' +
      'To expand the safe subspace explicitly, OnlineTune also selects the safe configurations at the boundary of the safety set with the highest uncertainty since they are promising candidates for expanding the safety set. To unify the two sampling criteria, we adopt the epsilon-greedy policy (Shi et al., 2019), which selects the maximal UCB configuration with a probability of \\(1-\\epsilon\\) and the boundary point with a possibility of \\(\\epsilon\\).\n' +
      '\n' +
      '## 7. Evaluation\n' +
      '\n' +
      '**Outline**: We first compare OnlineTune with the state-of-the-art database configuration tuning methods on dynamic environments in Section 7.1. The comparison is conducted under three settings: (1) _dynamic queries composition_, where the queries composition of the workload is constantly changing, (2) _transactional-analytical_, simulating a daily transactional-analytical cycle by alternating the execution of OLTP and OLAP workloads, and (3) _real-world workload_. In Section 7.1.4, we analyze the overhead of the compared methods. In Section 7.2, a case study is presented. In Section 7.3, we conduct ablation studies to evaluate the effectiveness of OnlineTune\'s design, including the context space design and the safe exploration strategy. We also validate the robustness of OnlineTune on different initial safety sets and interval sizes. In addition, we evaluate the baselines under _static workload_ setting to analyze the search efficiency in Section 7.4.\n' +
      '\n' +
      '**Baselines**: The baselines are explained below:\n' +
      '\n' +
      '* _DBA Default_ is the configuration provided by experienced DBAs.\n' +
      '* OnlineTune is our safe and contextual tuner. We implement OnlineTune\'s prediction model using GPy library (Bondel et al., 2019).\n' +
      '* _BDG_ is a Bayesian Optimization approach, widely used in database configuration tuning (Bondel et al., 2019; Bondel et al., 2019; Shi et al., 2019). We use similar design with OLTP(Shi et al., 2019): Gaussian process as surrogate model and EI as acquisition function. We also implemented BO via GPy library.\n' +
      '* _DDPG_ is a reinforcement learning agent which is used to tune the database configuration (Shi et al., 2019). The agent inputs internal metrics of the DBMS and outputs proper configurations. The DDPG algorithm is implemented using PyTorch library (Bondel et al., 2019) with its neural network architecture borrowed from CDBTune(Shi et al., 2019).\n' +
      '* _Qtune_ is a query-aware tuner that supports three tuning granularities (Shi et al., 2019). We adopt its workload-level tuning. It embeds workload features to predict internal metrics via a pre-trained model, while CDBTune uses the measured internal metrics.\n' +
      '* _ResTune_ adopts constrained Bayesian Optimization to minimize resource utilization with SLA constraints (Shi et al., 2019). It uses an ensemble framework (i.e., RGPE) to transfer historical knowledge from observations of source workloads. We modify ResTune to maximize database performance with the same safety constraints as OnlineTune. To adopt RGPE in online tuning, we cluster every 25 observations as one source workload.\n' +
      '* _MysqlTuner_ is a MySQL tuning tool that examines DBMS metrics and uses static heuristics to suggest configurations (Bondel et al., 2019). It is also the white-box assistant that OnlineTune results.\n' +
      '\n' +
      '**Workloads**: We use three workloads with different characteristics from well-known benchmarks and a real-world workload. The three workloads are TPC-C, Twitter from OLTP-Bench (Bondel et al., 2019) and JOB (Shi et al., 2019). TPC-C is a traditional OLTP benchmark characterized by write-heavy transactions with complex relations. Twitter is extracted from web-based applications, characterized by heavily skewed many-to-many relationships and non-uniform access. JOB is an analytical workload with 113 multi-join queries, characterized by realistic and complex joins (Shi et al., 2019). We load about 29 GB data for Twitter, 18 GB for TPC-C, 9 GB for JOB and use unlimited arrival rates for OLTP workloads to fully evaluate the benefits from tuning, as (Bondel et al., 2019; Shi et al., 2019; Shi et al., 2019; Shi et al., 2019; Shi et al., 2019) did. The real-world workload comes from a database monitoring service. In our experiments, we use the workload from 10:00 to 16:00 on September 2nd, 2021. And the read-write ratio per minute varies from 3:1 -74:1 in this period.\n' +
      '\n' +
      '**Setup**: We use version 5.7 of RDS MySQL. We tune 40 dynamic configuration knobs without restarting the database since the restart is not acceptable for online databases. The 40 configuration knobs are chosen based on their importance by DBAs. The experiments run on a cloud instance with 8 vCPU and 16GB RAM. The interval size is set to 3 minutes. We use the DBA default configuration as the initial safety set and its performance as the safety threshold, which are also added to the training set of other baselines for fairness.\n' +
      '\n' +
      '**Metrics**: We evaluate the baselines from two perspectives: safety and cumulative performance during tuning. For safety, we count the number of unsafe configuration recommendations (#Unsafe) and the number of system failures (#Failure) within the tuning period. For the cumulative performance, it is measured by the number of transactions (#txn) processed by the database during tuning for OLTP workload and the sum of execution time for OLAP workload.\n' +
      '\n' +
      '### Evaluation on Dynamic Workload\n' +
      '\n' +
      'We evaluate the baselines tuning online database with constructed dynamic workloads and real-workload workload with dynamic query arrival rates. We intend to answer two questions: (1) _Can they recommend the configuration adaptively with the dynamic workload?_ (2) _Can they reliably respect the safety requirement?_\n' +
      '\n' +
      '#### 7.1.1. Evaluation on workloads with dynamic query compositions\n' +
      '\n' +
      'To simulate the dynamicity, we construct workload with dynamic query compositions. For TPC-C and Twitter, we vary transaction weights via OLTP-Bench (Bondel et al., 2019). The weights are sampled from a normal distribution with a sine function of iterations as mean and a 10% standard deviation. For JOB, we execute ten queries per iteration, and five out of them are re-sampled. If the execution time exceeds the interval size, we kill the queries. Since TPC-C is a write-heavy workload, its underlying data is also changing (e.g., its data size change from 18 GB to 48 GB during tuning).\n' +
      '\n' +
      '**OnlineTune finds the workload-specific configuration.** As shown in Figure 5, OnlineTune achieves 54.3% -93.8% improvement on cumulative performance than the MySQL default, and 16.2% -21.9% improvement than the DBA default. The DBA default is expected to have a relatively robust performance across workloads. OnlineTune applies better configuration, illustrating its adaptability in dynamic environments. In addition, OnlineTune achieves 14.4%-165.3% improvement on cumulative performance than existing offline approaches. The reasons are two-fold. First, the offline methods struggle to handle dynamic environments. BO uses observations \\(\\{\\theta_{i},y_{i}\\}_{1}^{t}\\) to fit the GP model, ignoring dynamic environmental factors. When workload drifts, BO fails to learn suitable configurations. ResTune adopts an ensemble GP that assigns different weights to its base models but its base models still ignore the dynamic factors. DDPG and QTune fine-tune the models when environments change, but fine-tuning a neural network needs lots of training samples, which is inefficient for online tuning. Second, the offline methods over-explore configuration spaces. Evaluating unsafe configurations is a part of learning for offline methods. Although ResTune aims at finding the configurations satisfying the constraints, it still needs to evaluate and learn the unsafe area. Compared with the white-box method (MysqTuner), OnlineTune achieves 10.1%-19.7% improvement. Although MysqTuner does not have the over-exploration issue, it relies on heuristic rules and traps in local optimum.\n' +
      '\n' +
      '**OnlineTune reliably respects the safety requirement when tuning the online database.** Recommending safe configurations is non-trivial when tuning online databases since the workload changes may cause the shifting of safe configurations. As shown in Figure 5, none of the system failures occur when applying OnlineTune, while offline tuning methods cause several system failures. The offline methods have 22.2% -97.8% unsafe configuration recommendations within the 400 tuning intervals. Compared to them, OnlineTune reduces 91.0%-99.5% unsafe recommendations. This is contributed to the safe exploration strategy of OnlineTune, which we analyze in detail with an ablation study in Section 7.3.2.\n' +
      '\n' +
      '#### 7.1.2. Evaluation on Transactional-Analytical Cycle\n' +
      '\n' +
      'We simulate a daily transactional-analytical workload cycle by alternating dynamic TPC-C and JOB workloads. We use 99% latency as the optimization objective. The workload characterization of TPC-C (OLTP, write-heavy) and JOB (OLAP, complex joins) are significantly different. Respecting safety constraints is rather tricky in such a scenario. Figure 6 (a) shows the performance of OnlineTune and the DBA default. The workload starts a TPC-C and repeatedly alternates with JOB every 100 iterations. OnlineTune gradually finds configurations better than the DBA default, as shown in the zoom-in plot. When switching to JOB, OnlineTune takes some iterations to explore the configuration space. Meanwhile, several unsafe configurations with latency slightly larger than the safety threshold occur. Then, OnlineTune finds suitable configuration adaptively (e.g., larger sort buffer size for JOB workload).When switching from TPC-C to JOB again, OnlineTune selects the surrogate model fitted with previous observations for JOB and recommends suitable configurations more quickly. OnlineTune achieves performance than the DBA default while respecting the safety requirement, while other approaches fail, as shown in Figure 7 (a).\n' +
      '\n' +
      '#### 7.1.3. Evaluation on Real-World Workload\n' +
      '\n' +
      'We evaluate on a workload from the real-workload application. Figure 6 (b) shows the tuning process of OnlineTune and Figure 7 (b) presents the performances of all the baselines. We observe that OnlineTune achieves 39.4% improvement on cumulative improvement compared to the DBA default and 25.5% \\(\\sim\\)64.2% improvement compared to offline methods. Although OnlineTune applies several unsafe configurations at the beginning, their performances are within 10% deviation of the default performance as shown in Figure 6 (b).\n' +
      '\n' +
      '#### 7.1.4. Algorithm Overhead\n' +
      '\n' +
      'Figure 8 shows the computation time when tuning JOB workload. For BO, DDPG and ResTune, the computation time consists of (1) statistics collection, (2) model fitting, and (3) model probe. For OnlineTune and QTune, the time also includes featurization. OnlineTune\'s computation time is slightly larger than BO at the beginning. However, BO\'s overhead increases exponentially over tuning iteration, as GP suffers from cubic complexity on sample number. Instead, OnlineTune\'s computation time is within 3.79 seconds due to its clustering strategy.\n' +
      '\n' +
      'In addition to the computation time, we also analyze the impact of OnlineTune\'s tuning overhead. OnlineTune\'s featurization module is deployed in the database instance, while the other parts are deployed in the backend tuning severs whose overhead does not influence the database instance. Therefore, we focus on the impact of featurization. This module takes about 57.7 ms per iteration on average. To measure its impact, we keep the configuration unchanged and compare the resource usages and database performance.We observe that the average CPU usage with featurization is 77.28%, and the one without featurization is 77.27%. The increase is negligible since the featurization time only accounts for a slight proportion of a tuning interval (180 s). Its impact on database performance\n' +
      '\n' +
      'Figure 5. Cumulative performance (for TPC-C and Twitter, the higher is the better; for JOB, the lower is the better) and safety statistics when tuning dynamic workloads.\n' +
      '\n' +
      'can also be overlooked due to marginal resource consumption. For evaluation, we keep configurations unchanged to observe the impact. In practice, the database performance will be improved when running OnlineTune, as evaluated above.\n' +
      '\n' +
      '### Case Study\n' +
      '\n' +
      'To further investigate OnlineTune\'s tuning performance, we conduct a case study tuning five knobs. We construct a workload trace using YCSB with different read/write transaction compositions, as shown in Figure 9. The joint context-configuration space is smaller than the other experiments in the paper. Therefore, we can use extensive evaluations to explore the space and obtain the best configurations (denoted as the Best) and the unsafe areas for each workload composition. As shown in Figure 10, we observe that there are interactions between knobs. And, regular patterns exist among workload compositions, e.g., when buffer pool size (k1) is large and heap table size (k2) is small, the throughput is relatively low. However, the overall effects of knobs are quite different among the workloads, e.g., distinct optimal positions. Thus, it is necessary to consider the workload dynamicity when tuning online.\n' +
      '\n' +
      'Figure 11 presents the tuning result, which aligns with the evaluation in Section 7.1. As shown in the iterative plot, the distances between OnlineTune and the Best gradually decrease as OnlineTune safely finds configurations near the optimum. The iterative performances of the offline baselines have large fluctuations due to their trials and errors. In Figure 12, we focus on the configuration values of the top-2 important knobs. The unsafe region is approximated by excluding the knob\'s values from all the safe configurations. Note that if a configuration is assigned with a knob value in the unsafe region, the configuration is unsafe, but not vice versa. This is due to knobs interaction may cause unsafe. We observe that the optimal configurations (the Best) are not portable across workloads. OnlineTune applies configurations adaptive to the dynamic workload, which safely proceeds towards the optimum. The other baselines explore the space aggressively in the first 50 iterations and always have a chance to explore the unsafe region.\n' +
      '\n' +
      'Figure 13 visualizes the working process of OnlineTune\'s modules. From the left figure, we make two observations. First, OnlineTune can select the corresponding model for the observed context. Second, the subspace is initially centered around the default configuration and moves towards the optimum. The configuration subspaces maintained by each model gradually move far from the default, safely exploring the configuration space. From the right figure, we observe that the size of safety set is updated with the augmented observations. If OnlineTune evaluates unsafe configuration, its safety estimation will be immediately tightened, and OnlineTune will recommend conservative configurations near the evaluated-best ones, avoiding successive regression.\n' +
      '\n' +
      '### Analysis of OnlineTune\n' +
      '\n' +
      'We carefully design OnlineTune with contextual modeling and safe exploration strategy to explore configurations safely in dynamic environments. We evaluate the corresponding designs via ablation study and validate the robustness of OnlineTune.\n' +
      '\n' +
      'Figure 8. Computation time of different approaches.\n' +
      '\n' +
      'Figure 6. Iterative Performance on OLTP-OLAP circle and real-world workload.\n' +
      '\n' +
      'Figure 7. Cumulative performance and safety statistics on OLTP-OLAP circle and real-world workload.\n' +
      '\n' +
      '#### 7.3.1. Ablation Study on Contextual Modeling\n' +
      '\n' +
      'We remove certain components of contextual modeling (featurization of workload and data changes, clustering, and model selection) in OnlineTune to understand their contribution to the overall system. We compare (1) OnlineTune, (2) OnlineTune-w/o-workload, removing workload feature from context, (3) OnlineTune-w/o-data, removing optimizer statistics (underlying data feature) from context, (4) OnlineTune-w/o-clustering, removing clustering and model selection\n' +
      '\n' +
      'Figure 11. Results on YCSB workload: The left figure shows the cumulative result and the right shows the iterative performance. For clarity of presentation, we only show the iterative performances of the top 3 baselines (OnlineTune, ResTune and BO).\n' +
      '\n' +
      'Figure 12. Configurations applied by OnlineTune, ResTune and BO on YCSB workload: We present the top 2 important knobs.\n' +
      '\n' +
      'Figure 13. Visualization of OnlineTune. In the left figure, different colors denote different models selected over iterations. The lines denote the distance between the recommended configuration and the default, and the dash denotes the distance between subspace centers and the default. The right figure presents the size of safety set estimated by OnlineTune in its restricted subspace. For reference, we also plot the performance improvement over iterations.\n' +
      '\n' +
      'Figure 9. Pattern of YCSB workload. Figure 10. Throughput as a function of configurations: k1 denotes sort_buffer_pool_size and k2 denotes max_heap_table_size.\n' +
      '\n' +
      'design. The experiments are conducted in dynamic TPC-C and JOB with the same setting as in Section 7.1.1. Figure 14 shows the improvement in cumulative performance against the DBA default and safety statistic of those baselines. In the following evaluation, instead of cumulative performance, we use cumulative improvement that shows the benefit of online tuning more clearly. For the read-only workload JOB where no data changes occur, OnlineTune-w/o-data exceeds OnlineTune slightly because the dimension of context feature decreases with no information loss, which means that modeling over contexts becomes easier. However, when data change occurs as in TPC-C workload, OnlineTune outperforms the other baselines since its context feature gives a comprehensive abstraction of the dynamic environment. Since the clustering and model selection strategy can prevent "negative transfer", OnlineTune outperforms OnlineTune-w/o-clustering in all the cases.\n' +
      '\n' +
      '#### 7.3.2. Ablation Study on Safe Exploration\n' +
      '\n' +
      'To safely optimize the online database, OnlineTune utilizes contextual GP (black-box knowledge) and domain knowledge (white-box knowledge) and gradually expands the optimization subspace. To analyze their functionality, we compare: (1) OnlineTune, (2) OnlineTune-w/o-white, removing white-box safety assessment (3) OnlineTune-w/o-black, removing black-box safety assessment, (4) OnlineTune-w/o-subspace, optimizing in the whole configuration space, (5) OnlineTune-w/o-safe, removing all the safety strategy, i.e., vanilla contextual BO. As shown in Figure 15, OnlineTune beats other baselines in both performance and safety degree. We make the following observations: (1) The black-box knowledge largely reduces the unsafe recommendations. The limited domain rules only cover a small subset of unsafe cases and fail to capture the complex and high-dimension relationship between configuration, environment, and database performance. Therefore, OnlineTune-w/o-black\'s unsafe recommendations are much more than OnlineTune-w/o-white\'s. (2) The white-box knowledge assists in filtering the unsafe configurations. We find that the unsafe recommendation in OnlineTune-w/o-white is mainly caused by the knobs without intrinsic ordering, e.g., thread_concurrency. Thread_concurrency defines the maximum number of threads permitted inside InnoDB. But a value of 0 (the default value) is interpreted as infinite concurrency (no limit). The GP model depends on the natural ordering property and smoothness of space. When OnlineTune-w/o-white expands the safe space, it is likely to try a value near zero, like one, casing performance downgrade due to the lack of computing resources. OnlineTune has domain rules to filter the value of thread concurrency less than half of the number of virtual CPUs, preventing the unsafe case. (3) Optimizing over promising subspace instead of the whole configuration space can enhance safety and localize good configurations. OnlineTune-w/o-subspace recommends more unsafe configurations than OnlineTune. As discussed, it is hard to generalize the safety of configurations in the whole space, and the prediction of GP is more trustworthy in a small trust region. Besides, the global optimization approaches (i.e., OnlineTune-w/o-subspace, OnlineTune-w/o-safe) they over-explore boundaries of configuration spaces. (4) Without any safety designs, OnlineTune-w/o-safe has the worst performance.\n' +
      '\n' +
      '#### 7.3.3. Varying Interval Sizes\n' +
      '\n' +
      'We run OnlineTune under different interval sizes: 5 seconds, 1 minute, 3 minutes, 6 minutes, and 12\n' +
      '\n' +
      'Figure 16. Tuning Twitter with different interval sizes.\n' +
      '\n' +
      'Figure 14. Ablation study on context space design.\n' +
      '\n' +
      'Figure 17. Starting from MySQL Default.\n' +
      '\n' +
      'Figure 15. Ablation study on safe exploration.\n' +
      '\n' +
      'minutes, as shown in Figure 16. Within a certain range, a smaller interval leads to quicker adaptation. The reason is two-fold. First, OnlineTune could recommend fine-grained configurations suitable for dynamic workloads. Second, OnlineTune could collect more observations in a given time, leading to better tuning policies. However, the interval size also determines the time for evaluating the database performance. It cannot be too small to avoid the instability caused by performance fluctuation (Beng et al., 2019). As shown, tuning with a 5-second interval performs worse than the 1-minute one with more unsafe recommendations. And we have observed significant performance variance for 5-second intervals on a fixed configuration.\n' +
      '\n' +
      '#### 7.3.4. Varying Initial Safety Set and Safety Threshold\n' +
      '\n' +
      'In the above evaluation, we use the DBA default as the initial safety set and its performance as a safety threshold. However, there exists a question: _Can OnlineTune recommend suitable configurations with an inferior starting point?_ Therefore, we use the MySQL default configuration as the initial safety set and its performance as the safety threshold. As shown in Figure 17, the MySQL default\'s performance is worse than the DBA default\'s. One main difference is that the MySQL default sets the buffer pool size 128 MB while the DBA default sets the buffer pool size 13 GB. When starting from the MySQL default, OnlineTune applies safe configurations better than the MySQL default. And it achieves a comparable performance to tuning with the DBA default as the starting point after about 150 iterations.\n' +
      '\n' +
      '### Evaluation on Static Workload\n' +
      '\n' +
      'The existing approaches work well to search for optimal configurations on static workloads. This evaluation aims to assess the search efficiency of OnlineTune with the safety constraints. Figure 18 presents the performance with statistics shown in Table 1.\n' +
      '\n' +
      '**OnlineTune reduces the unsafe recommendations significantly with search efficiency comparable to the state-of-the-art offline tuning methods.** Offline methods are designed to search for the optimum without safety considerations. In terms of efficiency, OnlineTune is comparable to BO, ResTune, and better than DDPG, QTune in all the cases. OnlineTune is very unlikely to apply unsafe configurations, while the offline methods violate safety constraints considerably. Although OnlineTune\'s safety consideration may make the exploration slower, adaptively restricting the search space could localize good solutions, which improves the convergence, especially in JOB\'s case.\n' +
      '\n' +
      '## 8. Conclusion\n' +
      '\n' +
      'We introduce OnlineTune, an online tuning system that is aware of the dynamic environments and optimizes the database safely. OnlineTune featurizes the dynamic environmental factors as context feature and leverages Contextual Bayesian Optimization to optimize the context-configuration joint space. We propose a safe exploration strategy, greatly enhancing the safety of online tuning. As future extensions, we plan to investigate combining OnlineTune with offline tuning. The offline process could explore more configurations on replicas of the target DBMS, replaying the historical workloads to collect observations for online tuning. Therefore, OnlineTune could exploit the promising configuration space in the online phase, thus responding to the dynamic environment more quickly. In addition, OnlineTune could pause online configuring after applying suitable configurations. This could be achieved by a stopping and triggering mechanism. We could keep OnlineTune\'s workflow at each iteration (including context featurization and acquisition value calculation for candidate points) but not change the database configurations until more promising candidates appear. For example, we can measure whether more promising candidates exist by calculating the Expected Improvement (EI) value against the applied configuration. The configuring is triggered when candidates with EI values larger than a threshold exist, indicating that the context changes lead to the need for re-configuring.\n' +
      '\n' +
      '#### Acknowledgments\n' +
      '\n' +
      'This work is supported by National Natural Science Foundation of China (NSFC)(No. 61832001), Alibaba Group through Alibaba Innovative Research Program and National Key Research, and the Beijing Academy of Artificial Intelligence. Bin Cui is the corresponding author.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c|c c|c c|c c|c c} \\hline \\hline  & \\multicolumn{2}{c}{**OnlineTune**} & \\multicolumn{2}{c}{**BO**} & \\multicolumn{2}{c}{**DDPG**} & \\multicolumn{2}{c}{**ResTune**} & \\multicolumn{2}{c}{**QTune**} & \\multicolumn{2}{c}{**MySQLTune**} \\\\\n' +
      '**Workload** & Max Improv. & Search Step & Max Improv. & Search Step & Max Improv. & Search Step & Max Improv. & Search Step & Max Improv. & Search Step \\\\ \\hline TPC-C & 17.0\\% & 176 & **19.99\\%** & **74** & 16.64\\% & 76 & 12.0\\% & \\multirow{2}{*}{\\(\\backslash\\)} & 12.02\\% & \\multirow{2}{*}{\\(\\backslash\\)} & 13.44\\% & \\\\ Twitter & **48.18\\%** & 129 & 43.43\\% & 158 & 35.79\\% & \\(\\backslash\\) & 46.95\\% & **10** & 8.06\\% & \\multirow{2}{*}{\\(\\backslash\\)} & 13.07\\% & \\\\ JOB & 11.67\\% & **141** & 7.77\\% & \\(\\backslash\\) & 7.60\\% & \\(\\backslash\\) & **11.84\\%** & 155 & 11.24\\% & 168 & 7.23\\% & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1. Statistics on static workloads: For each workload, bold face indicates the best value. Search Step denotes the iteration needed to find a configuration within 10% of the estimated optimum, \\(\\backslash\\) denotes such configuration is not found.\n' +
      '\n' +
      'Figure 18. Iterative performance on static workloads: minimal throughput (0) and maximal latency (200) denote system failure.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* (1)\n' +
      '* (2015) 2015. GPy Library. [https://sheffieldml.github.io/GPy](https://sheffieldml.github.io/GPy).\n' +
      '* (2017) 2017. PyTorch Library. [https://pytorch.org](https://pytorch.org).\n' +
      '* (2019) 2019. MYSQL Tuning Primer Script. [https://github.com/major/MySQL_Tunter-perl](https://github.com/major/MySQL_Tunter-perl).\n' +
      '* (2011) Mert Akdere, Ugur Cetintemel, Matteo Rionlado, Eli Upfal, and Stanley B. Zdonik. 2011. The Case for Predictive Database Systems: Opportunities and Challenges. In _CIDR_. www.cidrhorg, 167-174.\n' +
      '* (2012) Mater Akdere, Ugur Cetintemel, Matteo Rionlado, Eli Upfal, and Stanley B. Zdonik. 2012. Learning-based Query Performance Modeling and Prediction. In _ICDE_. IEEE Computer Society, 390-401.\n' +
      '* (2017) Dana Yan Aken, Andrew Pavlo, Geoffrey J. Gordon, and Bohan Zhang. 2017. Automatic Database Management System Tuning Through Large-scale Machine Learning. In _SIGMOD Conference_. ACM, 1009-1024.\n' +
      '* (2018) Dana Yan Aken, Dongsheng Yang, Sebastien Brillard, Ari Fiorino, Bohan Zhang, Christian Billan, and Andrew Pavlo. 2021. An Inquiry into Machine Learning-based Automatic Configuration Tuning Services on Real-World Database Management Systems. Proc. VLDB Endow. 14, 7 (2021), 1241-1253.\n' +
      '* (2013) Yoshua Bengio, Aaron C. Courville, and Pascal Vincent. 2013. Representation Learning: A Review and New Perspectives. _EEE Trans. Pattern Anal. Mach. Intell._ 35, 8 (2013), 1798-1828.\n' +
      '* (2011) James Bergstra, Remi Bardenet, Yoshua Bengio, and Balasz Kegl. 2011. Algorithms for Hyper-Parameter Optimization. In _NIPS_. 2546-2554.\n' +
      '* (2016) Felix Berikenkamp, Andreas Krause, and Angela P. Schoellig. 2016. Bayesian Optimization with Safety Constraints: Safe and Automatic Parameter Tuning in Robotics. _CoRR_ abs/1602.04450 (2016).\n' +
      '* (2014) Roberto Calandras, Andre Seyfarth, Jan Peters, and Marc Peter Deisenroth. 2014. An experimental comparison of Bayesian optimization for bipedal locomotion. In _ICRA_. IEEE, 1951-1958.\n' +
      '* (2005) Karl Dias, Mark Ramacher, Uri Shaft, Venkateshwaran Venkataramani, and Graham Wood. 2005. Automatic Performance Diagnosis and Tuning in Oracle. In _CIDR_. workcidrhorg, 84-94.\n' +
      '* (2013) Jiejelle Eddine Difallah, Andrew Pavlo, Carlo Curino, and Philippe Cuche-Mauroux. 2013. CLTP-Bench: An Extensible Testbed for Benchmarking Relational Databases. _Proc. VLDB Endow. 4_, 7 (2013), 277-288.\n' +
      '* (2012) Liang Du, Ruobin Gao, Ponmuthuri Nagataman Suganthan, and David Z. W. Wang. 2022. Bayesian optimization based dynamic ensemble for time series forecasting. In _Ist._ 551 (2022), 155-175.\n' +
      '* (2009) Songyuan Duan, Yannakshin Thummala, and Shirvnath Babu. 2009. Tuning Database Configuration Parameters with iTuned. _Proc. VLDB Endow. 2_, 1 (2009), 1246-1257.\n' +
      '* (2019) David Eriksson, Michael Pearace, Jacob R. Gardner, Ryan Turner, and Matthias Poloeczek. 2019. Scalable Global Optimization via Local Bayesian Optimization. In _NeurIPS_. 5497-5508.\n' +
      '* (2019) Martin Ester, Hans-Peter Kriegel, Jorg Sander, and Xiaowei Xu. 1996. A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. In _KDD_. AAAI Press, 226-231.\n' +
      '* (2020) Araf Fekry, Lucian Carata, Thomas F. J.-M. Pasquier, Andrew Rice, and Andy Hopper. 2020. To tune or Not to Tune?. In _S_ectural of Optimal Configators for Data Analytics. In _KDD_. ACM, 2494-2504.\n' +
      '* (2020) Araf Fekry, Lucian Carata, Thomas F. J.-M. Pasquier, Andrew Rice, and Andy Hopper. 2020. Tuneful: An Online Significance-Aware Configuration Tune for Big Data Analytics. _CoRR_ abs/2001.08002 (2020).\n' +
      '* (2020) Marcelo Fiducaro, Sebastien Curi, Benedik Schumacher, Markus Guerer, and Andreas Krause. 2019. Safe Contextual Bayesian Optimization for Sustainable Room Temperature PID Control Tuning. In _IJCAI_. ivsci.org, 5850-5856.\n' +
      '* (2015) Lorenz Fischer, Shen Gao, and Abraham Bernstein. 2015. Machines Tuning Machines: Configuring Distributed Stream Processors with Bayesian Optimization. In _CLUSTER_. IEEE Computer Society, 22-31.\n' +
      '* (2015) Adam Foster, Martin Jankowik, Eli Engham, Paul Horsfall, Yee Whye Teh, Tom Rainforth, and Noah D. Goodman. 2019. Variational Bayesian Optimal Experimental Design. In _NeurIPS_. 14036-14047.\n' +
      '* (2009) Archana Ganapathi, Harun A. Kuno, Umeshwar Dayal, Janet L. Wiener, Armando Fox, Michael J. Jordan, and David A. Patterson. 2009. Predicting Multiple Metrics for Queries: Better Decisions Enabled by Machine Learning. In _ICDE_. IEEE Computer Society, 592-603.\n' +
      '* (2014) Jacob R. Gardner, Matt J. Kunner, Zhixiang Eddie Xu, Kilian Q. Weinberger, and John P. Cunningham. 2014. Bayesian Optimization with Inequality Constraints. In _ICML (JMLR Workshop and Conference Proceedings, Vol. 32)_. JMLR, org, 937-945.\n' +
      '* (2014) Michael A. Gelhart, Japper Smock, and Ryan P. Adams. 2014. Bayesian Optimization with Unknown Constraints. In _IAI_. AUAI Press, 250-259.\n' +
      '* (2020) Benjamin Hilprecht, Andreas Schmati, Moritz Kulessa, Alejandro Molina, Kristina Kerting, and Carsten Binnig. 2020. DeepDB: Learn from Data, not from Queries!. _Proc. VLDB Endow._ 13, 7 (2020), 992-1005.\n' +
      '* (2014) Frank Hutter, Jeffger H. Hoos, and Kevin Leyton-Brown. 2014. An Efficient Approach for Assessing Hyperparameter Importance. In _ICML (JMLR Workshop and Conference Proceedings, Vol. 32)_. JMLR org, 754-762.\n' +
      '* (2019) Frank Hutter, Lars Kotthoff, and Joquin Vanschoren (Eds.). 2019. _Automatic Machine Learning: Methods, Systems, Challenges_. Springer.\n' +
      '* (2018) Shraink Jain and Bill Howe. 2018. Query2Vec: NLP Meets Databases for Generalized Workload Analytics. _CoRR_ abs/1801.05613 (2018).\n' +
      '* (2019) Shraink Jain, Jiaqi Yan, Thierry Crunas, and Bill Howe. 2019. Database-Agnostic Workload Management. In _CIDR_. www.cidrhorg.\n' +
      '* (2013) Marjan Backait, Nasser Ghasen-Aghagan-Abdang, and Chang Wook Ahn. 2013. Holographic memory-based Bayesian optimization algorithm (HM-BOA) in dynamic environments. _Sci. China Inf. Sci._ 56, 9 (2013), 1-17.\n' +
      '* (2020) Konstantinos Kannelis, Ramatthan Alagapour, and Shivaram Venkataraman. 2020. Too Many Knobs to Tune? Towards Faster Database Tuning by Pre-selecting Important Knobs. In _InfStorage_. USENIX Association.\n' +
      '* (2020) Andreas Kipt, Thomas Kipf, Bernhard Radke, Viktor Leis, Peter A. Boncz, and Alfos Kemper. 2020. Leonard Cardinales: Estimating Correlated Jons with Deep Learning. In _CIDR_. www.cidrhorg.\n' +
      '* (2021) Johannes Kirschener, Mojin Mutny, Neocle Hiller, Rasmus Ischebeck, and Andreas Krause. 2019. Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional Subspaces. In _ICML (Proceedings of Machine Learning Research, Vol. 97)_. PMLR, 3429-343.\n' +
      '* (2011) Andreas Krause and Cheng Soon Ong. 2011. Contextual Gaussian Process Bandit Optimization. In _NIPS_. 2447-2455.\n' +
      '* (2020) Adri Krishnan, Mahasukeu Das, Mangesh Bendre, Hao Yang, and Hari Sundaram. 2020. Transfer Learning via Contextual Invariants for One-to-Many Cross-Domain Recommendation. In _SIGIR_. ACM, 1081-1090.\n' +
      '* (2020) Mayuresh Kunjiar and Shirvnath Babu. 2020. Black or White? How to Develop an AutoTune? How to Develop an AutoTune for Memory-based Analytics. In _SIGMOD Conference_. ACM, 1667-1683.\n' +
      '* (2021) Hari Lan, Zhifeng Bao, and Yu Yuwei Peng. 2021. A Survey on Advancing the DBMS Query Optimizer: Cardinality Estimation, Cost Model, and Plan Enumeration. _Data Sci. Eng._ 6, 1 (2021), 86-101.\n' +
      '* (2015) Viktor Leis, Andrey Guhchiken, Matzas Mirchev, Peter A. Boncz, Alfons Kemper, and Thomas Neumann. 2015. How Good Are Query Optimizers, Really? _Proc. VLDB Endow._ 9, 3 (2015), 204-215.\n' +
      '* (2017) Benjamin Leichnan, Brian Karrer, Guilherme Ottoni, and Eytan Balsky. 2017. Constrained Bayesian Optimization with Noisy Experiments. _CoRR_ abs/1706.07094 (2017).\n' +
      '* (2019) Guoliang Li, Xuanhe Zhou, Shifu Li, and Bo Gao. 2019. QTune: A Query-Aware Database Tuning System with Deep Reinforcement Learning. _Proc. VLDB Endow._ 12, 12 (2019), 2118-2130.\n' +
      '* (2021) Edo Liberty, Zohar S. Karmin, Bing Xiang, Laurence Rousesel, Baris Coskun, Ramas Nallapati, Julio Delgado, Amir Sadoughi, Yury Astashhokok, Patil Das, Can Balogu, Sawanta Chakravarty, Madhya Jha, Philip Gautier, David Arpin, Tim Januschowski, Valentin Plunker, Yuyang Wang, Jan Gasthaus, Lorenzo Stella, Syama Sundar Rangouram, David Salinas, Sebastian Schelter, and Alex Smola. 2020. Elastic Machine Learning Algorithms in Amazon SageMaker. In _SIGMOD Conference_. ACM, 731-737.\n' +
      '* (2018) Lin Ma, Daana Van Aken, Ahmed Hefny, Gustavo Mezerhane, Andrew Pavlo, and Geoffrey J. Gordon. 2018. Query-based Workload Forecasting for Self-Driving Database Management Systems. In _SIGMOD Conference_. ACM, 631-645.\n' +
      '* (2019) Lin Ma, William Zhang, Jie Jiao, Wuwen Wang, Matthew Burtovich, Wan Shen Lim, Prashanth Menon, and Andrew Pavlo. 2021. MB2: Decomposed Behavior Modeling for Self-Driving Database Management Systems. In _SIGMOD Conference_. ACM, 1248-1261.\n' +
      '* (2019) Victor Hirota Makiyama, M. Jordan Raddijek, and Rafael D. C. Santos. 2019. Text Mining Applied to SQL Queries: A Case Study for the SDSS/Server. In _SMBig (CEURUR Workshop, Vol. 1478)_. CEUR-UR-WS,org, 66-72.\n' +
      '* (2017) Alonso Marco, Felix Berikenkamp, Philipp Heming, Angela P. Schoellig, Andreas Krause, Stefan Schaal, and Sebastian Trimpe. 2017. Virtual vs real: Trading of simulations and physical experiments in reinforcement learning with Bayesian optimization. In _ICRA_. IEEE, 1557-1563.\n' +
      '* (2019) Ryan C. Marcus, Parimara Nregi, Hongyi Mao, Chi Zhang, Mohammad Alizadeh, Tim Kraska, Olga Pap* Snoek et al. (2012) Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. 2012. Practical Bayesian Optimization of Machine Learning Algorithms. In _NIPS_. 2960-2968.\n' +
      '* Srinivas et al. (2010) Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. 2010. Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design. In _ICML_. Omnipress, 1015-1022.\n' +
      '* Sui et al. (2015) Yanan Sui, Alkis Gotovos, Joel W. Burdick, and Andreas Krause. 2015. Safe Exploration for Optimization with Gaussian Processes. In _ICML_ (_JMLR Workshop and Conference Proceedings_, Vol. 37).MLR_.org, 997-1005.\n' +
      '* Sullivan et al. (2004) David G. Sullivan, Margo I. Seltzer, and Avi Pfeffer. 2004. Using probabilistic reasoning to automate software tuning. In _SIGMETRICS_. ACM, 404-405.\n' +
      '* Sun and Li (2019) Ji Sun and Guoliang Li. 2019. An End-to-End Learning-based Cost Estimator. _Proc. VLDB Endow._ 13, 3 (2019), 307-319.\n' +
      '* Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Networks. In _NIPS_. 3104-3112.\n' +
      '* Taff et al. (2018) Rebecca Taff, Nosayba El-Sayed, Marco Serafini, Yu Lu, Ashraf Aboulnaga, Michael Stonebrenker, Ricardo Mayerhofer, and Francisco Jose Andrade. 2018. P-Store- An Elastic Database System with Predictive Provisioning. In _SIGMOD Conference_. ACM, 2015-219.\n' +
      '* Tan et al. (2021) Jian Tan, Niv Nayman, Mengchang Wang, and Rong Jin. 2021. CoBBO: Coordinate Backoff Bayesian Optimization with Two-Stage Kernels. _CoRR_ abs/2101.05147 (2021).\n' +
      '* Tan et al. (2019) Jian Tan, Tiseying Zhang, Feifei Li, Lie Chen, Qixing Zheng, Ping Zhang, Honglin Qiu, Yue Shi, Wei Cao, and Rui Zhang. 2019. The: Individual Buffer Tuning for Large-scale Cloud Databases. _Proc. VLDB Endow._ 12, 10 (2019), 1221-1234.\n' +
      '* Wang et al. (2020) Linnan Wang, Rodrigo Fonseca, and Yuandong Tian. 2020. Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search. In _NeurIPS_.\n' +
      '* Wang et al. (2018) Zi Wang, Clement Gehring, Pushner Kohli, and Stefanie Jegelka. 2018. Batched Large-scale Bayesian Optimization in High-dimensional Spaces. In _AISTATS (Proceedings of Machine Learning Research, Vol. 84)_. PMLR, 745-754.\n' +
      '* Wang et al. (2014) Zijyu Wang, Babak Shakhidi, Lin, Jin, and Nando de Freitas. 2014. Bayesian Multi-Scale Optimatic Optimization. In _AISTATS (_QMLR Workshop and Conference Proceedings_, Vol. 33). JMLR.org, 1005-1014.\n' +
      '* Wei et al. (2014) Zhibe Wei, Zuohua Ding, and Jipeling Hu. 2014. Self-tuning performance of database systems based on fuzzy rules. In _FSDI_. IEEE, 194-198.\n' +
      '* Weikum et al. (2002) Gerhard Weikum, Axel Monkerey, Christof Hasse, and Peter Zabback. 2002. Self-tuning Database Technology and Information Services: from Wishful Thinking to Viable Engineering. In _VLDB_. Morgan Kaufmann, 20-31.\n' +
      '* Wistuba et al. (2015) Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. 2015. Sequential Model-Free Hyperparameter Tuning. In _ICDM_. IEEE Computer Society, 1033-1038.\n' +
      '* Yan et al. (2018) Jiaqi Yan, Qiuye Jin, Shrainik Jain, Stratis D. Viglas, and Allison W. Lee. 2018. Snowtrial: Testing with Production Queries on a Cloud Database. In _DITest@SIGMOD_. ACM, 41-46.\n' +
      '* Yang et al. (2020) Zongheng Yang, Amog Kammetty, Sifei Luan, Eric Liang, Yan Duan, Peter Chen, and Ion Stoica. 2020. Next-Accurad: One Cardinality Estimator for All Tables. _Proc. VLDB Endow._ 14, 1 (2020), 61-73.\n' +
      '* Yang et al. (2019) Zongheng Yang, Eric Liang, Aong Kammetty, Chenggang Wu, Yan Duan, Peter Chen, Pieter Abbeel, Joseph M. Hellerstein, Sanjay Krishnan, and Ion Stoica. 2019. Deep Unsupervised Cardinality Estimation. _Proc. VLDB Endow._ 13, (2019), 279-292.\n' +
      '* Yuan ([n.d.]) Yu-Xiang Yuan. [n.d.]. A Review of Trust Region Algorithms for Optimization. _J. Zhang et al. (2019) J. Zhang, Yu Liu, Ke Zhou, Guoliang Li, Zhili Xiao, Bin Cheng, Jiashu Xing, Yanjong Wang, Tianheng Cheng, Li Liu, Minwei Ran, and Zekang Li. 2019. An End-to-End Automatic Cloud Database Tuning System Using Deep Reinforcement Learning. In _SIGMOD Conference_. ACM, 415-432.\n' +
      '* Zhang et al. (2021) Xinyi Zhang, Hong Wu, Zhuo Chang, Shuowei Jin, Jian Tan, Feifei Li, Tieying Zhang, and Bin Cui. 2021. ResTune: Resource Oriented Tuning Boosted by Meta-Learning for Cloud Databases. In _SIGMOD Conference_. ACM, 2021-2114.\n' +
      '* Zhu et al. (2017) Yuqing Zhu, Jianxun Liu, Mengying Guo, Yungang Bao, Wenlong Ma, Zhuoyue Liu, Kunpeng Song, and Yingchun Yang. 2017. BestConfig: tapping the performance potential of systems via automatic configuration tuning. In _SoCC_. ACM, 338-350.\n' +
      '* Zolakraf et al. (2020) Zainab Zolakraf, Mostafa Milani, and Rachel Pottinger. 2020. Facilitating SQL Query Composition and Analysis. In _SIGMOD Conference_. ACM, 209-224.\n' +
      '\n' +
      '## Appendix A Outline\n' +
      '\n' +
      'This supplemental material is organized as follows:\n' +
      '\n' +
      '**A.1** More details about OnlineTune Architecture.\n' +
      '\n' +
      '**A.2** High-Level Algorithm.\n' +
      '\n' +
      '**A.3** Details about subspace adaptation.\n' +
      '\n' +
      '## A1. More Details about OnlineTune Architecture\n' +
      '\n' +
      '**System Architecture.** Figure A1 presents the architecture of OnlineTune. The left part shows the online database system in the cloud environment that runs ever-changing workloads. The right part represents the OnlineTune server deployed in the backend tuning cluster. OnlineTune maintains a data repository that stores the historical observations \\(\\{<e_{i}\\,\\theta_{i},y_{i}>\\}_{1}^{t}\\) from the previous tuning iterations, which could be initially empty. The controller monitors the states of tuning tasks and transfers the data between the database side and OnlineTune server side. The main parts of OnlineTune run on the server side, whose resource consumption doesn\'t affect the online database. The context featurization module is deployed in the database instance for data privacy concerns.\n' +
      '\n' +
      '**Discussion.** OnlineTune is a reactive approach: it featurizes the online workloads at the beginning of an iteration and applies a configuration for the later time of the iteration. Since changes in real-world workloads are gradual, the little lagging can be acceptable. A predictive module for the context might help. However, it still assumes gradual and predictable changes based on historical data, and an inaccurate prediction module could greatly affect the tuner\'s performance.\n' +
      '\n' +
      '## Appendix B. Top-level Algorithm\n' +
      '\n' +
      'Algorithm 3 presents the top-level algorithm. OnlineTune first queries the default configuration and its performance to form the initial safety set and initializes the data repository (Line1-3). The safety threshold is set as the default performance. At the beginning of an iteration, OnlineTune collects the incoming workload queries and corresponding optimizer\'s estimations to calculate context vector \\(c_{i}\\) (Line 5, Section 5.1). OnlineTune loads a prediction model \\(m_{i-1}^{n}\\) selected by a SVM model that inputs the context vector and outputs a cluster label \\(n\\) (Line 6, Section 5.3). Then, a configuration subspace is initialized or adapted (Line 7, Section 6.1). OnlineTune discretizes the configuration subspace and assesses the safety of unevaluated configurations based on the black-and-white prior knowledge to form a safe candidate set (safety set) (Line 8, Section 6.2). Next, OnlineTune recommends a configuration \\(\\theta_{i}\\) within the safety set to trade-off exploitation (i.e., making decisions based on existing knowledge) and exploration (i.e., acquiring new knowledge or expanding the safety set) (Line 9, Section 6.3). The configuration \\(\\theta_{i}\\) is applied to the online database, and the database performance \\(y_{i}\\) during the tuning interval is collected. Finally, OnlineTune updates the prediction model and data repository with \\(\\{\\theta_{i},c_{i},y_{i}\\}\\) (Lines 12 to 11). OnlineTune determines whether to re-cluster or not ((Line 13, see Algorithm 1 for details). If needing re-cluster, OnlineTune clusters the observations based on \\(\\{c_{i}\\}_{i}^{t}\\), fits prediction models for each cluster, and re-train the SVM model, as illustrated in Section 5.3.\n' +
      '\n' +
      '```\n' +
      'Output: Configuration recommendation adaptively with changing environment\n' +
      '1Fauturize the environment factor, get context \\(c_{0}\\).\n' +
      '2Query default configuration \\(\\theta_{0}\\) and get its performance \\(y_{0}\\).\n' +
      '3Initialize a data repository \\(H_{0}\\) with \\(<c_{0},\\theta_{0},y_{0}>\\).\n' +
      '4for\\(i\\gets 1\\) to \\(K\\)do\n' +
      '5Fauturize and get context \\(c_{i}\\).\n' +
      '6Select a model \\(m_{i-1}^{n}\\), where \\(n=SVM(c_{i})\\).\n' +
      '7\\(\\Theta_{i}^{n}=\\text{Subspace\\_Adaptation}(\\Theta_{i-1}^{n},H_{i-1})\\).\n' +
      '8Generate safe candidates \\(S_{i}^{n}\\in\\Theta_{i}^{n}\\).\n' +
      '9Select a configuration \\(\\theta_{i}\\) within \\(S_{i}^{n}\\).\n' +
      '10Apply \\(\\theta_{i}\\) and evaluate its performance \\(y_{i}\\).\n' +
      '11\\(H_{i}=\\text{Append}(H_{i-1},<\\theta_{i},c_{i},y_{i}>)\\).\n' +
      '12Fit prediction model \\(m_{i}^{n}\\) on \\(H_{i}\\).\n' +
      '13Offline_Clustering(\\(H_{i}\\)).\n' +
      '```\n' +
      '\n' +
      '**Algorithm 3**Top-Level Algorithm of OnlineTune\n' +
      '\n' +
      '## Appendix C. A3. Details about Subspace Adaptation\n' +
      '\n' +
      '### More Rationale about Subspace Restriction\n' +
      '\n' +
      'Given a configuration space \\(\\Theta\\) and a contextual GP model, we can discretize \\(\\Theta\\) to obtain a set of candidates and assess the safety of each candidate based on the estimation of the GP model. However, the direct operation over the whole space suffers from the curse of dimensionality. First, the candidates need to be close to each other in order to use the GP to generalize safety. When tuning more than a couple of knobs, it is computationally affordable to conduct a fine discretization over the whole configuration space (Srivastava et al., 2017). In addition, the database performance function is often complex, making the estimation of the GP model problematic for high dimensional configuration (more than ten parameters) (Srivastava et al., 2017). To dealwith those challenges, we draw inspiration from a class of trust-region methods in stochastic optimization [72]. These methods use a linear and quadratic prediction model inside a trust region, which is often a sphere or a polytope centered at the best observation. Intuitively, while linear and quadratic models are likely to be inadequate to model globally, they can be accurate in a sufficiently small trust region [16]. However, the linear models struggle to handle noisy observations and require small trust regions to provide accurate modeling behavior. Naturally, we use contextual GP as the prediction model to describe the function \\(f(\\theta,c)\\) and restrict OnlineTune\'s optimization space in the trust region (i.e., configuration subspace). The optimization can be solved efficiently within the trust region, and the GP model\'s estimation can be trusted in the trust region [16].\n' +
      '\n' +
      '### Direction Oracles for Line Region\n' +
      '\n' +
      'OnlineTune implements two strategies to generate the directions, including random direction and important direction.\n' +
      '\n' +
      '**Random direction.** A strategy is to pick the direction uniformly (random direction), increasing the exploration.\n' +
      '\n' +
      '**Important direction.** OnlineTune also chooses the directions aligned with the important configuration knob (important direction), inspired by the important knobs pre-selecting procedure before tuning used by existing configuration tuning approaches [6, 18, 32]. It is empirically shown that restricting the optimization space in several important knobs can largely reduce the tuning iterations and achieve similar improvement [32]. The importance of knobs is quantified by Fanova [27], a line-time approach for assessing feature importance. However, detecting important knobs needs thousands of evaluation samples for a given workload. And fixing the configuration space wrongly (e.g., filtering important knobs) will severely hinder the optimization. OnlineTune\'s adaptation of subspace solves this problem by adjusting the subspace over iterations.\n' +
      '\n' +
      'A random direction is chosen if the performance improvement in the previous hypercube region is lower than a threshold (exploration). Otherwise, an important direction is chosen by sampling from the top-5 important knobs (exploitation). The importance of knobs is updated with the increasing observations.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>