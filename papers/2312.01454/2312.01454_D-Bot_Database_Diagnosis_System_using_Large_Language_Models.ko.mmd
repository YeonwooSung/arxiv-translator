[MISSING_PAGE_FAIL:1]

데이터베이스 모듈은 잠재적으로 상당한 재정적 손실을 초래한다. 따라서 _일반적인 이상을 자동으로 해결할 수 있는 경우 인간 DBA의 부담을 덜어주고 리소스를 절약할 수 있습니다._

이러한 동기에 힘입어, 많은 데이터베이스 제품들은 반자동 진단 툴[(20; 22; 29; 30; 32)]을 구비하고 있다. 그러나 몇 가지 제한 사항이 있습니다. 첫째, 시나리오 이해 능력이 떨어지고 진단 지식을 활용할 수 없는 경험적 규칙[(58; 12)] 또는 소규모 ML 모델(예: 분류기[(34)]에 의해 구축된다. 둘째, 시나리오 변화에 유연하게 일반화할 수 없다. 경험적 방법들의 경우, 최신 버전의 문서들에 의해 규칙들을 수동으로 업데이트하고 검증하는 것은 지루하다. 그리고 학습된 방법들(예를 들어, XGBoost[(9)], KNN[(17))은 입력 메트릭들 및 라벨들을 재설계하고, 새로운 시나리오에 대한 모델들을 재트레이닝할 것을 요구한다(도 1(d)). 셋째, 이러한 방법은 초기 분석 결과를 기반으로 시스템 뷰를 재귀적으로 탐색하여 근본 원인을 추론하는 등 인간 DBA로서의 추론 능력이 없다.

이를 위해, 우리는 세 가지 주요 장점을 가진 지능형 진단 시스템 구축을 목표로 한다. (1) 정밀 진단._ 첫째, 본 시스템은 실제 진단에 필요한 시나리오 정보(예: 화염 그래프를 이용한 질의 분석)를 수집하거나 최적화 조언(예: 인덱스 선택)을 도출하는 도구를 활용할 수 있다. 그러나, 그것은 전통적인 방법으로 거의 지지되지 않는다. 둘째, 기본적인 논리적 추론(즉, 진단 계획 수립)을 수행할 수 있다. _ (2) 비용 및 시간 절약._ 시스템은 인간 DBA들을 통화중 의무로부터 어느 정도 완화시킬 수 있다(예를 들어, 규칙들이 지원할 수 없는 전형적인 이상들을 해결함). _ (3) 일반화 가능성이 높다._ 시스템은 주어진 문서(예를 들어, 새로운 메트릭, 뷰, 로그) 및 과거 경험에 기초하여 보이지 않는 이상을 분석하는 유연성을 나타낸다.

LOM(Large Language Models)의 최근 발전은 자연어 이해 및 프로그래밍에서 우월함을 입증한 이 목표를 달성할 수 있는 잠재력을 제공한다[(60; 40; 63; 42)] 그러나 데이터베이스 진단에는 광범위한 도메인별 기술이 필요하며, GPT-4 모델조차도 진단 지식(50% 미만의 정확도)을 직접 마스터할 수 없다. 이렇게 하면 세 가지 문제가 발생 합니다. _ (C1) 진단 문제에 대한 LIM의 이해를 높이는 방법?_ 광범위한 말뭉치에 대해 미리 훈련되었음에도 불구하고, LLM은 여전히 적절한 프롬프트2(예를 들어, 데이터베이스 지식을 알지 못함) 없이 효과적으로 진단하는 데 어려움을 겪는다. 상기 과제들은 (_i_) 긴 문서들로부터 유용한 지식들을 추출하는 것(예를 들어, 챕터들에 걸친 상관들); (_ii_) 주어진 컨텍스트에 의한 적합한 지식과의 매칭(예를 들어, 높은 노드 로드의 경보를 검출하는 것); (_iii_) 잠재적으로 유용한 도구들(예를 들어, 데이터베이스 카탈로그들)을 검색하는 것 _ (C2) 단일 원인 이상에 대한 LIM의 진단 성능을 어떻게 향상시킬 수 있는가?_ 지식 및 도구 프롬프트로 LLM은 주어진 이상에 대해 신중하게 추론할 필요가 있다. 첫째, 많은 LLM 태스크[(13)]와는 달리, 데이터베이스 진단은 일반적으로 여러 번 분석해야 하는 대화형 절차인 반면, LLM은 조기 정지 문제가 있다[(14)]. 둘째, LLM은 "환각" 문제가 있으며[(44)], 심층적이고 합리적인 분석을 도출하기 위해 LLM을 안내하는 전략을 설계하는 것이 중요하다.

각주 2: 프롬프트는 LLM 입력에 추가 정보를 추가하는 것이다. LLM은 미세 조정으로 새로운 지식을 암기할 수 있지만 이전 지식을 잊어버리거나 부정확하거나 혼합된 응답을 생성할 수 있으며, 이는 데이터베이스 진단에서 허용되지 않는다.

_(C3) 다중 원인 이상에 대한 LIM의 진단 능력을 향상시키는 방법?_ 우리의 관찰에서, 시간 예산 내에서 단일 LLM은 복잡한 이상(예: 여러 근본 원인이 있고 중요한 메트릭이 더 미세한 세분성)에 대해 정확하게 분석하기 어렵다. 따라서 다중 LLM이 복잡한 데이터베이스 문제(예: 교차 검토)를 공동으로 해결하고 진단 정확성과 효율성을 모두 향상시킬 수 있는 효율적인 진단 메커니즘을 설계하는 것이 중요하다.

본 논문에서는 이러한 문제점을 해결하기 위해 대용량 언어 모델을 이용한 데이터베이스 진단 시스템인 D-Bot을 제안한다. 먼저 문서에서 유용한 지식 청크를 추출하고(요약 트리 기반 지식 추출) 상세한 사용 지침으로 도구의 계층 구조를 구성하며, 이를 기반으로 LLM 진단을 위한 프롬프트 템플릿을 초기화한다(그림 3 참조). 둘째, 프롬프트 템플릿에 따라 LLM이 합리적인 진단을 위한 모니터링 및 최적화 결과를 획득하기 위해 활용할 수 있는 대부분의 관련 지식(키 메트릭 검색) 및 도구(파인 튜닝된 SentenceBert)와 매칭하여 새로운 프롬프트를 생성한다. 셋째, LLM이 과거의 진단 시도를 반영하고 가장 유망한 것을 선택하도록 안내하여 진단 성능을 크게 향상시키는 트리 기반 검색 전략을 소개한다. 마지막으로, 복잡한 이상(예를 들어, 여러 근본 원인이 있는)에 대해, 우리는 주어진 이상을 해결하기 위해 여러 LLM 전문가가 비동기식(예를 들어, 분석 결과를 공유하고, 교차 검토를 수행하는)으로 진단할 수 있는 협력 진단 메커니즘을 제안한다.

**기여도.** 다음과 같은 기여를 합니다.

(1) 정밀 진단을 달성하기 위해 LLM 기반 데이터베이스 진단 프레임워크를 설계한다(섹션 3 참조).

(2) LLM을 비교하여 문서에서 추출한 관련 지식과 (_i_) 매칭하고 (_ii_) 도구를 미세 조정된 임베딩 모델로 검색하여 진단을 수행하는 진단 프롬프트 생성 방법을 제안한다(섹션 4 및 5 참조).

(3) LLM이 다단계 분석을 수행하도록 안내하는 트리 탐색 기반 알고리즘을 이용하여 진단 성능을 향상시키는 근본 원인 분석 방법을 제안한다(섹션 6 참조).

(4) 도메인 지식에 의해 여러 개의 LLM이 동시에 문제를 분석하는 협업 진단 메커니즘을 제안한다(섹션 7 참조).

(5) 우리의 실험 결과는 D-Bot이 허용 가능한 시간 내에 전형적인 근본 원인을 정확하게 식별할 수 있음을 입증한다(섹션 8 참조).

## 2. Preliminaries

### Database Anomalies

_Anomalies_는 진단 절차가 필요한 불규칙적이거나 예상치 못한 문제를 지칭한다[(43). 그림 2는 데이터베이스의 일반적인 4가지 이상을 보여준다.

_(1) 느린 쿼리 실행입니다._ 데이터베이스는 예상보다 더 긴 응답 시간을 경험한다. 예를 들어, 느린 쿼리는 CPU 사용량(시스템 부하) 및 쿼리 지속 시간의 상당한 증가를 야기하지만, 활성 프로세스의 수는 여전히 낮다.

_(2) 전체 리소스 사용량._ 일부 시스템 리소스가 소진되어 새 요청을 수락하거나 오류를 유발하지 않습니다(예: 메모리 부족에 대한 오류 삽입). 예를 들어, 높은 동시성 워크로드는 큰 CPU 및 메모리 사용을 야기할 수 있을 뿐만 아니라, 활성 프로세스의 수를 상당히 증가시킨다.

_(3) 데이터베이스 걸기._ 데이터베이스는 일반적으로 장기 실행 쿼리, 교착 상태 또는 리소스 경합으로 인해 발생하는 응답하지 않습니다. 예를 들어 _전체 리소스 사용량_ 이상과 같이 CPU 소비가 많기 때문에 제출된 트랜잭션에서 비정상적으로 대기하지만 빈번한 프로세스 중단 및 전환도 포함됩니다.

_(4) 데이터베이스 충돌._ 데이터베이스가 예기치 않게 종료되어 데이터에 액세스할 수 없게 됩니다. 일반적인 근본 원인은 전체 디스크 공간으로 인해 새 데이터를 쓰거나 필요한 작업을 수행할 수 없게 되어 궁극적으로 데이터베이스 오류가 발생 하 고 획득한 리소스를 해제 합니다.

### Database Diagnosis

_데이터베이스 진단_은 데이터베이스 시스템 내에서 발생하는 이상(통상적으로 일련의 경보 형태)을 분석하고 해결하는 과정을 의미한다. 데이터베이스 진단의 주요 목적은 기본 _루트 원인_ 을 정확히 찾는 것입니다. 여기서는 독립 실행형 데이터베이스3에서 몇 가지 예제 근본 원인을 보여 줍니다.

각주 3: 애플리케이션 또는 네트워크 쪽의 이상은 이 작업의 범위를 벗어납니다.

_(1) 동시성 워크로드_: 여러 데이터베이스 작업이 시스템 리소스에 대해 경쟁하여 성능 저하로 이어지는 심각한 워크로드 경쟁을 특징으로 하는 문제입니다.

_(2) 쿼리 연산자 문제_: 큰 테이블 삽입, 많은 양의 데이터 가져오기 및 복잡한 술어 실행과 같은 문제로 인해 데이터베이스 시스템의 처리 기능이 변형될 수 있습니다.

_(3) 계획 및 실행_: 이 카테고리의 근본 원인은 비정상적인 계획 시간과 장기간의 데이터베이스 대기 시간을 포함하며, 이는 쿼리 계획 및 실행 프로세스의 비효율성을 나타낸다.

_(4) 데이터-특정 이슈_: 데이터 손상 및 데드 튜플(더 이상 필요하지 않지만 물리적 저장소에 남아 있는 행)과 같은 문제는 성능 문제로 이어질 수 있다.

_(5) 데이터베이스 스키마 및 설정_: 데이터베이스 스키마(예를 들어, 인덱스) 및 구성 설정과 관련된 이러한 문제. 예제에는 쿼리 최적화 및 메모리 관리에 영향을 줄 수 있는 누락된 인덱스 및 작은 공유 버퍼 크기가 포함됩니다.

_(6) 유해 배경 작업_: 저장 공간 재생을 위한 "진공"과 같은 일부 데이터베이스 유지 관리 작업은 너무 자주 호출될 때 문제가 될 수 있다(이러한 작업은 사용자 쿼리와 시스템 리소스를 경쟁할 것이다).

루트 원인이 확인되면 _최적화 작업_ 집합을 제안하여 이러한 문제를 해결하고 정상적인 데이터베이스 작업을 복원할 수 있습니다. 여기에서 몇 가지 최적화 도구를 소개합니다.

_(1) Query Rewrite Tools_. 대부분의 데이터베이스는 논리적 변환에 약하기 때문에(Shen 등, 2016)(예를 들어, 복잡한 술어 단순화), 느린 쿼리를 최적화하는 데 도움이 되는 외부 재작성 도구(예를 들어, Calcite(Caleit, 2017)에서 약 120 규칙)가 있다.

_(2) 노브 튜닝 툴_. 노브 값이 잘못되면 데이터베이스 오류(예: 최대 연결 수 초과) 또는 성능 저하(예: 할당된 작업 메모리가 너무 작음)가 발생할 수 있습니다. 따라서, 튜닝 제안들을 제공하기 위해 규칙들을 활용하는 도구들이 있다(Caleit, 2017; Krizhevsky et al., 2017). 예를 들어 메모리 사용량이 60% 미만이면 MySQL의 inodb_buffer_pool_size 값을 5% 증가시킵니다.

_(3) 인덱스 튜닝 툴_. 유사하게, 잠재적으로 유용한 인덱스를 생성하는 인덱스 튜닝 규칙(Caleit, 2017; Krizhevsky et al., 2017; Krizhevsky et al., 2017; Krizhevsky et al., 2017)이 있는데, 이는 동일한 술어에 열을 갖는 복합 인덱스를 생성하는 것과 같다.

예 1().: _도 2에 도시된 바와 같이, 높은 메모리 사용량을 나타내는 이상 경보가 주어지면, 먼저 이상 시간 동안 시스템 부하(예를 들어, 메모리 사용에 대한 node_memory_total)를 조사한다. 데이터는 비정상적인 메모리 사용률(90% 이상)을 확인합니다. 이를 이해하기 위해, 관련 메모리 메트릭들(예를 들어, node_memory_inactive_anon_bytes)을 더 획득한다. 이러한 메트릭들의 분석은 과도한 메모리 사용량이 데이터를 테이블에 삽입하는 집중적인 워크로드에 의해 야기될 수 있음을 시사한다. 이를 해결하기 위해 최적화 전략이 메모리 소비를 줄이는 데 도움이 될 수 있는지(예: 테이블 데이터를 파티션으로 나누는 것)를 조사한다.

대형 언어 모델

다음으로, LLM 아키텍처, LLM 프롬프팅 및 LLM 미세 조정을 포함한 LLM(Large Language Models)의 기본 개념을 소개하고, 데이터베이스 진단의 기능을 활용하는 데 중추적인 LLM 미세 조정을 소개한다.

_Transformer-Based LLMs_. 기존의 LLM은 주목 메커니즘과 피드포워드 신경망으로 구별되는 트랜스포머 구조를 주로 채택한다. 어텐션 메커니즘은 입력에서 요소의 무게를 동적으로 측정하여 모델이 텍스트의 다른 부분에 집중할 수 있도록 한다. 즉, 어텐션 점수는 \(Attention(Q,K,V)=\text{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V\), 여기서 \(Q\)(쿼리), \(K\)(키), \(V\)(값)은 텍스트의 다른 측면을 나타내고 \(d_{k}\)은 키의 차원을 나타낸다. 또한, LLM은 피드-포워드

도 2. 데이터베이스 진단의 예.

각 계층의 신경망은 주의 계층의 출력에 위치별 선형 변환을 적용한다. 주목 및 피드-포워드 네트워크의 이러한 조합은 후속 텍스트 요소의 정확한 예측을 용이하게 한다.

_LLM Prompting._ LLM에 응답 생성을 안내하는 특정 지침을 제공하기 위해 입력 \(X\)에 프롬프트 \(P\)을 미리 추가하거나 추가하여 \(X^{\prime}=P\oplus X\) 또는 \(X^{\prime}=X\oplus P\)으로 표시되는 새로운 입력을 생성할 수 있다. 그런 다음 LLM은 이 수정된 입력에 기초하여 출력 텍스트를 생성한다. 참고(\(i\)) 프롬프트는 모델 파라미터에 대한 추가 업데이트를 필요로 하지 않으며 (\(ii\)) 프롬프트는 수동으로 조작되거나 데이터로부터 자동으로 학습될 수 있다(Zhu 등, 2019).

_LLM Fine-tuning_은 작고 태스크-특정 데이터세트(예를 들어, 수천 개의 샘플) 상에서 모델 파라미터를 조정하는 것을 포함한다. 초기에는 \(\theta\)로 표시된 모델 매개변수가 사전 훈련 단계에서 상속된다. Fine-tuning은 특정 태스크(예: 분류 또는 회귀)에 맞춘 손실 함수 \(\mathcal{L}\)를 태스크별 데이터셋 \(\mathcal{D}\)에 비해 최소화하는 것을 목표로 한다. 이는 \(\theta_{\text{new}}=\theta_{\text{old}}-\alpha\cdot\nabla\mathcal{L}(\theta_ {\text{old}};\mathcal{D})\)로 표현되며, 여기서 작은 학습률 \(\alpha\)은 점진적인 파라미터 갱신을 보장하기 위해 자주 사용된다(Zhu et al., 2019).

우리는 GPT-4와 같은 근접 공급 LLM을 안내하는 LLM 프롬프트에 의존하여 진단하고(섹션 5 참조), LLM 미세 조정을 활용하여 국소 LLM을 준비한다(섹션 8.5 참조).

## 3. D-Bot 개요

**아키텍처.** 그림 4와 같이 먼저 D-Bot의 전체 아키텍처를 보여 줍니다. (1) _이상 모니터_ 는 경고 규칙을 사용 하 여 데이터베이스의 상태를 지속적으로 모니터링 합니다. (2) 데이터베이스가 비정상 상태로 전환되면 _Anomaly Monitor_는 트리거된 경고를 _Anomaly Profiler_ 로 보냅니다. (3) _Anomaly Profiler_ 는 경고 및 기본 데이터베이스 정보 (예: 경고의 시간 기간 동안 쿼리 통계)를 기반으로 이상 설명을 생성 합니다. (4) _데이터베이스 진단_ 에는 하나 또는 여러 LLM 전문가가 함께 이상 현상에 대 한 분석 보고서를 생성 하도록 노출 합니다.

다음으로, 데이터베이스 진단 컴포넌트4(도 3 참조)에서의 도전과 기술을 제시한다.

각주 4: 본 논문은 비전의 확장이다(Zhu 등, 2019)

_(1) 오프라인 준비._ 오프라인 준비_는 D-봇에 데이터베이스 진단에 필요한 지식과 도구를 제공합니다.

_(i) 문서 학습:_ 진단 문서는 사전 훈련 코퍼스에서 흔히 발견되지 않고 문서 섹션은 상호 참조와 같은 관계를 갖기 때문에, 문서 구조를 나타내기 위해 요약 트리를 구축하고 트리로부터 지식 청크를 추출하는 지식 추출을 수행한다(예를 들어, 부모-자식 노드 및 유사한 요약을 갖는 노드를 횡단함).

_(ii) 도구 준비:_ 이 단계는 API 설명(API가 무엇을 하는지, 어떻게 사용되는지, 어떤 종류의 데이터를 반환하는지 설명)을 제공하고 API가 진단 중에 LLM에 의해 사용될 수 있도록 시스템에 등록하는 것과 같은 진단 도구를 구성한다.

추출된 모든 지식 청크를 군집화하고(그림 6), 각 군집에 대해 LLM 전문가의 문자를 나타내는 프롬프트 템플릿(예: 전문가 역할, 작업 설명, 기본 진단 단계, 사용 가능한 도구)을 생성한다. 예를 들어 LLM 전문가 한 명이 CPU 문제(지식) 분석을 담당하며 _cpu_usage_api_ 및 _index_tuning_api_와 같은 도구를 활용할 수 있습니다. 이러한 방식으로, 각각의 LLM 전문가는 데이터베이스 문제의 특정 영역을 처리한다.

_(2) 진단 프롬프트 생성._ 필요한 모든 지식과 도구를 쉽게 사용할 수 있는 경우 _진단 프롬프트 생성_은 진단 프로세스를 안내하는 상황 인식 프롬프트를 만드는 역할을 합니다. 프롬프트 템플릿에 따라 실제 진단의 각 새 프롬프트에는 세 가지 필수 유형의 정보가 필요합니다.

도 4. D-Bor Architecture.

도 3. D-Bor에서의 데이터베이스 진단.

(_i_) _Anomaly Description_. 발생 시간, 이상 상태 요약, 심각도 수준(예: 경고 또는 중요), 추가 기능(예: "해결됨"과 같은 이상 상태)을 포함하여 트리거된 경고입니다.

(_ii_) _Tools_. 서로 다른 데이터베이스 사용에 대한 다양한 도구(예: 모니터링, 인덱싱, 쿼리 재작성, 쿼리 힌트)로 인해, 관련 도구(예: _fetch_abnormal_metrics_API)를 사용하여 현재 컨텍스트(예: LLM이 어떤 메트릭이 비정상인지 알지 못하는 경우)와 일치하도록 미리 훈련된 Sentence-BERT 모델(Wang et al., 2019)을 미세 조정한다.

(_iii_) 획득된 비정상 메트릭으로 진단하는 방법을 LLM에 지시하는 지식 Chunks. 키워드 검색 방법(예: BM25)을 사용하여 가장 관련성이 높은 지식 청크를 식별한다.

(_iii_) 트리 검색 기반 진단(예: 이전 단계에 의한 결정)에 중요한 과거 도구 호출 결과와 같은 귀중한 정보를 포함하는 이력 메시지_입니다.

_(3) Tree-Search Based Diagnosis_. _ 트리 검색 기반 진단_은 생성된 프롬프트(도구를 호출하거나 지식을 따라 분석)를 사용하여 잠재적인 근본 원인 및 솔루션을 식별하는 것을 목표로 합니다. 데이터베이스 진단의 경우 LLM은 진단 실패를 유발할 수 있는 환각 및 불안정한 LLM 응답(예: 잘못된 API 요청, 지나치게 일반적인 분석)과 같은 문제에 직면합니다. 이 문제를 해결 하기 위해 LLM이 여러 가능한 추론 체인을 탐색 하 고 탐색할 가장 유익한 체인을 선택 하는 _생각 트리_ 전략을 사용 합니다 (LLM의 표와 선택 빈도를 모두 기준으로). 또한, LLM은 _반사_ 메커니즘을 통해 현재 단계가 유용한 정보를 감지하지 못하면 _이전 단계_로 역추적할 수 있으며, 이는 합리적인 진단 결과에 도달할 가능성을 크게 증가시킨다.

_(4) 협업 진단 메커니즘_. _Tree-Search Based Diagnosis_의 비용은 탑재된 지식과 도구의 수에 따라 크게 증가하므로, _Collaborative Diagnosis Mechanism_은 다수의 LLM 전문가를 활용하여 진단 성능을 향상시키는 것을 목표로 한다. 변칙이 주어지면 트리거된 경고 및 이상 복잡성을 기반으로 관련 전문가를 선택하는 것으로 시작합니다. 다음으로, 선택된 전문가는 _보다 포커싱된 도구 세트 및 지식 청크_ 로 (_Tree-Search Based Diagnosis_)를 별도로 분석한다. 여기서 비동기 전략을 채택한다: 선택된 전문가가 진단 중에 중간 진단 결과를 공유하여 실시간 정보 교환을 용이하게 하고 중복 분석(예: 동일한 쿼리 문제)을 줄인다. 진단 후, 이러한 전문가들은 교차 검토(예를 들어, 지나치게 일반적인 근본 원인을 식별)를 수행하고, 이를 기반으로 분석 결과를 반복적으로 정제하고 궁극적으로 특정 템플릿(예를 들어, 배경, 근본 원인, 솔루션 및 세부 단계를 포함) 하에서 포괄적인 보고서를 생성한다.

## 4. 오프라인 준비

본 절에서는 LLM 진단을 위해 필요한 지식과 도구를 준비하는 방법을 설명한다.

### Document Learning

먼저 LLM 프롬프트에서 사용하기에 적합한 지식 형식을 결정합니다. 다음으로, 주어진 문서로부터 이러한 지식 청크를 얻기 위한 추출 방법을 소개한다. 마지막으로, 얻어진 지식 청크와 클러스터링 결과를 보여준다.

#### 4.1.1. Knowledge Format

도 2의 진단 증거와 유사하게, 일부 문서들이 주어지면, 원하는 지식 청크는 네 부분으로 구성된다 : (_i_) "_Name_"은 LLM이 전체 기능을 이해하는 것을 돕는다; (_ii_) "_Content_"는 근본 원인이 데이터베이스 성능에 어떻게 영향을 미칠 수 있는지를 설명한다(예를 들어, 과도한 수의 죽은 튜플들로 인한 성능 저하); (_iii_) "_Metrics_"는 프롬프트 생성에서 지식 매칭을 위해 사용되는 관련된 메트릭 이름의 목록이다(섹션 5.1); (_iv_) "_Steps_"는 관련 메트릭으로 분석하는 세부 절차를 제공한다. 이를 통해 LLM은 모방하여 단계별 분석을 수행할 수 있다.

#### 4.1.2. 지식 추출

다음으로, 문서로부터 이러한 지식을 추출하는 방법을 설명한다. 데이터베이스 진단에서, 관련 문서들은 두 개의 문자, 즉 (_i_) 대부분의 문서들은 다양한 측면을 포함하는 긴 컨텍스트(예: 유지 관리 가이드에서 리소스 및 구성 문제 모두 논의됨)를 가지며, (_ii_) 일부 단락들은 서로 상관된다. 예를 들어, "many_dead_tuples"(3.2장과 같은)에 나타나는 "blouat-table"의 개념은 다른 섹션(1.1.3장과 같은)에서 설명된다.

긴 문서들을 입력으로서 지원하는 몇몇 긴-컨텍스트 LLM들(Beng et al., 2019)이 이미 존재하지만, 이들은 응답된 지식의 품질을 보장할 수 없다(예를 들어, 누락되거나 중요한 세부사항들을 구성한다(Song et al., 2019; Wang et al., 2019). 따라서 본 논문에서는 결정론적 지식 추출 알고리즘을 제안한다.

_Step1: Chapter Splitting_. 문서를 고정된 길이의 세그먼트로 직접 분할하는 대신에, 챕터 구조 및 그 내용(예를 들어, 애플리케이션들은 "테넌트 예들"과 같은 키워드들에 의해 분할되는)에 기초하여 분할한다. 블록이 LLM이 처리할 수 있는 최대 블록 크기(예를 들어, 4k 토큰)를 초과하는 경우, 우리는 그것을 더 작은 블록들로 재귀적으로 분할한다.

_Step2: 요약 트리 구성_. 다음으로 챕터 관계를 기반으로 루트 노드가 문서 제목이고 다른 노드가 분할된 문서 블록을 나타내는 트리 구조를 초기화한다. 각 노드 \(i\)에 대해 자식 노드는 \(i\)의 하위 섹션을 나타내고 노드 \(i\)에는 (1) 장의 내용 \(i\) 및 (2) 장의 요약 \(i\)이 포함되며, 요약 프롬프트가 있는 LLM에 콘텐츠를 공급하여 생성됩니다. 즉, \(p_{Summarize}\) = _제공된 청크를 간략하게 요약 \(\cdots\) 요약은 다른 사용자가 데이터베이스 유지 관리와 관련된 기술적 세부 정보를 찾는 인덱스 역할을 합니다 \(\cdots\) 청크가 다른 주제를 포함하더라도 예제에 주의를 기울입니다.

도 5. 문서 학습.

생성된 요약은 노드 \(i\)의 텍스트 인덱스로 작용하여 유사한 콘텐츠 또는 교차 참조와 같은 관계를 갖는 블록의 매칭을 가능하게 한다.

_Step3: 지식 추출._ 요약 트리를 생성한 후, LLM은 각각의 문서 블록 \(i\)(양 노드 \(i\) 및 그 자식 노드로부터의 콘텐츠와 함께)을 파싱하고, 추출 프롬프트에 의해 안내되는 유사한 콘텐츠를 갖는 다른 블록의 요약, 즉 \(p_{extract}\) = "_청크 요약이 주어지면, 청크로부터 진단 경험을 추출한다. 불확실한 경우, 자식 노드 또는 유사한 요약이 있는 청크의 진단 경험을 탐색한다._"

이러한 방식으로, 요약들로부터의 키 포인트들과 상관되는 지식이 검출된다. 검출된 각 지식 \(C_{i}\)에 대해 우리는 \(C_{i}\)을 하이브리드 방식으로 유지할지를 결정한다. 구체적으로, LLM이 \(C_{i}\)이 중복될 가능성이 낮은 경우(기존 지식에 비해), 이를 통합할 것이다. 그렇지 않으면, 우리는 기존의 지식들과 상당한 중복이 있음에도 불구하고, 새로운 통찰력을 발견하면, \(C_{i}\)을 유지할 수 있는 \(C_{i}\)에 대한 수동 조사를 수행할 것이다. 이러한 방식으로 _대부분의 진단 지식을 포함 하 고 중복 정보의 가능성을 줄일 수 있습니다._

#### 4.1.3. 추출된 지식의 클러스터링 결과

일반 진단 가이드, 사례 및 세부 보고서 5를 포함하여 81페이지의 문서에서 추출한 188개의 지식 청크를 보여준다.

각주 5: github.com/TsinghuaDatabaseGroup/DB-GPT/tree/main/doc2knowledge/docs

이 다양한 지식 청크 집합으로부터 인사이트를 도출하기 위해 사전 학습된 임베딩 모델(예: Ada-002 (Beng et al., 2015))을 사용하여 청크를 수치 벡터로 변환하고(\(ii\)) DBSCAN 알고리즘(Krishnan et al., 2017)을 적용하여 지식 청크를 텍스트 임베딩의 유사도에 따라 그룹화하고(\(iii\)) 주성분 분석(PCA)을 사용하여 텍스트 임베딩의 차원을 3차원으로 축소한다(\(iii\). 이러한 방식으로 우리는 그림 6에서 지식 추출 결과를 시각화할 수 있으며, 이는 _지식 분포가 근본 원인 유형과 크게 일치함_을 보여준다. 여기서는 몇 가지 지식 항목을 강조 표시합니다. _ (1) 워크로드_에는 (\(i\)) 워크로드 경합 문제(예: 과도한 연결), (\(ii\)) 데이터베이스 대기 이벤트의 영향을 분석 하는 것, (\(iii\)) 비정상 영구 이벤트를 처리 하는 것, (\(iv\)) 장기/단기 성능 변동을 처리 하는 것이 포함 됩니다. _ (2) 쿼리 오퍼레이터_는 (\(i\)) 쿼리 계획에 영향을 미치는 부정확한 오퍼레이터 비용을 분석하는 것, (\(ii\)) 느린 오퍼레이터를 식별하는 것(예: 빈약한 조인, 집계, 인덱스 필터링), (\(iii\)) 비정상적인 SQL 구조의 영향을 분석하는 것을 포함한다. _ (3) 인덱스 이슈_는 (\(i\)) 사용되지 않거나 중복된 인덱스를 식별하는 것, (\(ii\)) 테이블에 너무 많은 인덱스가 특히 삽입 및 업데이트 동작에 미치는 성능 영향을 다루는 것, (\(iii\)) 누락된 인덱스를 검출하는 것, 및 (\(iv\)) 쿼리 플랜에서 인덱스가 사용된 이유(예를 들어, 인덱스 무효화, 암시적 유형 변환)를 분석하는 것을 포함한다.

지식 청크가 다수의 토픽과 관련될 수 있다는 것이 명백하다(예를 들어, 느린 쿼리는 CPU 및 오퍼레이터 분석 모두에 관여할 수 있다). 따라서, 이러한 지식 청크(예를 들어, 다른 주제의 전문가)의 효과적인 활용 및 커뮤니케이션은 다음 진단에 필수적이다.

### Tool Preparation

지식과는 별개로, 인간 DBA는 모니터링 및 최적화 도구(예를 들어, 데이터베이스 뷰, 시스템 명령, 인덱스 튜닝 도구)와 자주 상호 작용할 필요가 있다. 효과적인 LLM 진단을 용이하게 하기 위해서는 LLM이 사용 가능한 도구 내에서 복잡한 API 기능을 이해하도록 하는 것이 필수적이다.

먼저, "범주-도구-API"를 분류하고 정리하기 위해 구조화된 계층 구조를 구축하며, 여기서 "API"는 도구의 특정 기능을 나타낸다. 예를 들어 인덱스 선택 도구는 "최적화"로 분류되며, "구성 도구"를 도구 유형으로 사용하고 "휴리스틱_index_selection"을 예제 API로 사용합니다 (그림 3). 이 계층 구조는 다양한 데이터베이스 도구를 구성하고 이해하는 데 도움이 됩니다.

둘째, 각 도구 기능에 대해 자세한 _활용 사양_ (함수 주석 형식으로)을 제공 합니다. 여기에는 함수의 설명, 매개 변수 및 관련 사용 사례(섹션 5.2)가 포함됩니다. 예를 들어 "휴리스틱_index_selection"에 대 한 함수 설명은 _"쿼리 패턴 및 워크로드에 따라 비용 절감 인덱스를 자동으로 선택 합니다. 인수에는 쿼리 빈도, 데이터 볼륨, 인덱스 저장소 제약 조건,..._가 포함 됩니다.

마지막으로 주어진 도구 모듈에서 API를 통해 반복하여 도구 함수를 동적으로 등록하고 각 API의 함수 이름을 _활용 사양_과 함께 가져옵니다.

## 5. 진단 프롬프트 생성

다음으로 추출된 지식 및 도구와 매칭하여 자동으로 진단 프롬프트를 생성하는 방법을 설명한다.

### Knowledge Retrieval

일반 진단 프로세스(프롬프트 템플릿에 포함)를 제공하는 지식 외에도 대부분의 지식 청크는 특정 컨텍스트에서만 유용합니다. 비정상 CPU 메트릭의 분석(그림 7)과 같다. 따라서, 주어진 컨텍스트(예를 들어, 5개의 비정상적인 CPU 메트릭을 갖는)에 대해, 가장 관련된 지식 청크의 순위를 매기기 위해 근사 알고리즘 BM25(Zhu 등, 2017)를 채택한다. 구체적으로, BM25 알고리즘은 다음과 같이 계산되는, 그들의 "메트릭" 속성에 기초하여 지식 청크들의 세트를 랭킹한다:

\[\text{Score}(D,Q)=\sum_{i=1}^{n}\text{IDF}(q_{i})\cdot\frac{f(q_{i},D)\cdot(k_ {1}+1)}{f(q_{i},D)+k_{1}\cdot(1-b+b\cdot\frac{|D|}{\text{avgDL}})} \tag{1}\]

도 6. 추출된 지식의 클러스터링 결과.

여기서, \(D\)은 지식 블록이고, \(Q\)은 이상 메트릭의 집합(KS-Test(Krishnaman et al., 2017)과 같은 이상 검출 알고리즘에 의해), \(f(q_{i},D)\)은 \(D\)에서 메트릭의 빈도 \(q_{i}\)이고, avgDL은 평균 지식 블록 길이이고, \(k_{1}\) 및 \(b\)은 자유 하이퍼-파라미터이다. IDF(\(q_{i}\))는 메트릭 \(q_{i}\)의 역 문서 빈도이며 다음과 같이 계산됩니다.

\[\text{IDF}(q_{i})=\ln\left(\frac{N-n(q_{i})+0.5}{n(q_{i})+0.5}+1\right) \tag{2}\]

여기서, \(N\)은 추출된 지식 청크의 총 개수이고, \(n(q_{i})\)은 메트릭 \(q_{i}\)을 포함하는 문서의 개수이다.

이 접근법의 장점은 관련된 메트릭의 이름이나 의미가 정확히 동일하지 않은 경우에도 지식 청크를 일치시킬 수 있고 다양한 모니터링 도구 또는 시스템 전반에 걸쳐 추출 지식을 쉽게 적용할 수 있다는 것이다.

### Tool Matching

지식 청크를 비정상 메트릭과 일치시키는 것과 달리 데이터베이스 도구는 복잡한 API를 포함하며 API 이름은 컨텍스트와 직접 관련되지 않을 수 있습니다 (예: 느린 쿼리에 대한 _sort_remove_와 같은 API). 이와 같이, _BM25_ 알고리즘은 상대적으로 높은 에러율을 가질 수 있다. 따라서, 진단 컨텍스트에 적합한 도구를 정확하게 매칭하는 보다 강력한 사전 트레이닝된 Sentence-BERT 모델(Zhu 등, 2018)을 미세 조정할 것을 제안한다. 이 절차는 두 가지 주요 단계, 즉 모델 미세 조정 및 도구 매칭을 포함한다.

\(\bullet\)_Sentence-BERT Fine-tuning:_ Let \(S=\{s_{1},s_{2},\ldots,s_{n}\}\)은 진단 맥락의 집합을 의미하고, \(T=\{t_{1},t_{2},\ldots,t_{m}\}\)은 데이터베이스 도구의 집합을 의미한다. 본 연구는 사전 학습된 Sentence-BERT 모델을 미세 조정하여 이상과 데이터베이스 도구 사이의 관계적 맥락을 이해하는 것을 목표로 한다. 미세 조정 과정은 \(D=\{(s_{i},t_{j},y_{ij})\}_{i=1,j=1}^{n,m}\), 여기서 \(y_{ij}\)은 진단 컨텍스트 \(s_{i}\)에 대한 도구 \(t_{j}\)의 관련성을 나타내는 레이블이다. 목적 함수는 교차 엔트로피 손실에 의해 계산된다:

\[\mathcal{L}=-\sum_{i=1}^{n}\sum_{j=1}^{m}y_{ij}\log(p_{ij})+(1-y_{ij})\log(1-p_{ij}), \tag{3}\]

여기서, \(p_{ij}\)은 도구 \(t_{j}\)이 sigmoid 함수를 통해 \(s_{i}\)과 \(t_{j}\)의 연결 임베딩을 통과함으로써 얻어지는 이상 \(s_{i}\)과 관련이 있을 것으로 예측되는 확률이다.

\(\bullet\)_적합한 도구 일치:_ 미세 조정 후 모델을 사용하여 새 진단 컨텍스트에 적합한 데이터베이스 도구를 일치시킵니다. \(s\) 진단 컨텍스트 \(s\)와 데이터베이스 도구 \(t_{j}\) 사이의 일치 점수는 임베딩 사이의 코사인 유사성으로 계산된다.

\[\text{sim}(s,t_{j})=\frac{\text{emb}(s)\cdot\text{emb}(t_{j})}{||\text{emb}( s)||_{2}||\text{emb}(t_{j})||_{2}}, \tag{4}\]

여기서 emb()는 미세 조정된 Sentence-BERT 모델의 임베딩 함수를 나타낸다. 진단 컨텍스트에 대 한 권장 데이터베이스 도구 집합 \(\hat{T}\) \(s\)은 일치 점수가 가장 높은 상위 \(k\) 도구를 선택 하 여 얻습니다.

\[\hat{T}=\text{arg top}_{k}\{\text{sim}(s,t_{j})\}_{j=1}^{m}. \tag{5}\]

마지막으로, 선택된 top-\(k\) 도구는 이름, 함수 설명 및 인수 목록을 포함한 프롬프트에 통합되어 LLM이 호출 요청을 생성하고 도구 실행 결과를 얻어 근본 원인 진단을 강화할 수 있다.

## 6. Llm Diagnosis을 위한 Tree Search

도 7에 도시된 바와 같이, LLM은 (_i_) 잘못된 툴 호출 요청을 생성하거나 요청 실패(예를 들어, 일시적으로 서비스를 이용할 수 없음)를 수신하는 것, (_ii_) 제안된 근본 원인에 대해 주의 깊게 반영하지 않고 진단을 조기에 중단하는 것과 같은 실수를 쉽게 한다. 이러한 문제를 해결하기 위해, 본 논문에서는 LLM이 현재 동작이 실패하거나 가치 있는 근본 원인을 찾을 수 없는 경우 이전 동작으로 돌아가도록 안내할 수 있는 트리 검색 기반 알고리즘을 제안한다(알고리즘 1).

_Step1: Tree Initialization._ 우리는 각 노드가 동작을 나타내는 트리 구조를 초기화하고(예: 도구 호출 또는 일치된 지식에 기반한 분석), 에지는 한 동작에서 다른 동작으로의 흐름을 나타낸다.

_Step2: Simulate Execution._ 이 단계는 루트 노드로부터 리프 노드(즉, 완전한 진단의 종료)로 시작하는 시뮬레이션의 실행을 시작한다. 특정 표준(예를 들어, 검출된 비정상 메트릭)에 기초하여 노드를 선택하는 것을 포함한다. 본 논문에서는 UCT(Upper Confidence Bound applied to Trees) 함수(Zhu et al., 2018)를 이용하여 탐색 경로를 결정한다. 즉, \(UCT(n)=\frac{W(n)}{N(n)}+C\sqrt{\frac{2\ln N(p)}{N(n)}}\), 여기서 \(W(n)\)은 \(n\)번째 이동 후의 승리 수, \(N(n)\)은 \(n\)번째 이동 후의 시뮬레이션 수, \(N(n)\)은 부모 노드에 대한 총 시뮬레이션 수, \(N(p)\)은 탐색 하이퍼-파라미터이다. 횡단하는 동안 UCT 값이 가장 높은 노드가 선택됩니다.

UCT 함수 내에서, \(W(n)\)은 여러 평가 LLMs(예를 들어, 세 개)의 투표에 의해 계산된다. 각 리프 노드에 대해 LLM에는 시나리오 컨텍스트 및 노드로 이어지는 과거 작업이 표시된다. 이 정보에 기초하여, 그들은 유망한 리프 노드(예를 들어, 관련된 루트 원인의 수 및 정확도에 기초하여)에 찬성표를 던진다. 가장 많은 득표수(\(W(n)\))를 받은 노드가 가장 유리한 경로로 결정된다.

도 7. LLM에 의한 다단계 진단 예시.

_Step3: 기존 Node Reflection._ 루트 노드에서 선택한 노드까지의 경로에 있는 각 노드에 대해 LLM에 의존하여 자식 노드의 프롬프트에 추가되는 작업을 수행(예: _"나중에 추적에 상속할 몇 가지 반영"_으로 프롬프트)하는 것의 이점을 다시 생각합니다. 다음 단계의 분석 품질을 향상시킬 수 있습니다. 또한, LLM이 유용한 정보가 없다고 판단되면 진단 효율을 높이기 위해 노드를 _"pruned"_로 표시한다.

_Step4: 단말 조건._ LLM이 임계 시간(예를 들어, 20 턴) 동안 더 이상 루트 원인(리프 노드)을 찾을 수 없는 경우, 알고리즘은 최상의 노드의 루트 원인 및 솔루션을 출력함으로써 종료된다.

## 7. 복합 이상에 대 한 협력 진단

도구 기울기 및 트리 탐색 알고리즘을 사용하면 단일 LLM의 진단 정확도를 크게 향상시킬 수 있다. 그럼에도 불구하고 단일 LLM은 여러 근본 원인을 가진 복잡한 이상 현상을 해결하는 데 어려움을 겪는다(예: 제한된 원인을 반복하고 추가 원인을 식별하는 데 어려움을 겪는다). 이를 해결하기 위해, 각각 툴 및 트리 탐색 알고리즘을 갖춘 다수의 LLM이 복잡한 경우를 다루기 위해 집합적으로 작동하는 협력 메커니즘을 제안한다(Beng et al., 2017).

_Step1: 전문가 준비._ 지식 클러스터링 결과에 따라 7명의 LLM 전문가를 초기화한다(섹션 4). 각 전문가는 프롬프트에 서로 다른 지식과 필요한 도구를 갖추고 있다.

_Step2: 전문가 배정._ 다음으로 자원 낭비를 피하고 진단 효율을 높이기 위해 적절한 전문가를 할당하여 진단한다. 즉, 어노말리가 주어지면, 먼저 어노말리에 대한 설명(예를 들어, 시간 주기, 경보 유형, 심각도 레벨)을 생성한다. 다음으로 이상 설명을 기반으로 _전문가 할당자_ 는 LLM (예: GPT-4)을 사용 하 여 대부분의 관련 전문가 집합을 선택 합니다. 예를 들어 _Load_High_ 경고에 대한 _CPU Expert_ 및 _Out_of_Memory_ 경고에 대한 _메모리 Expert_입니다. 새 경고 규칙 또는 전문가 역할을 플러그 인하는 데 더 유연한 규칙보다는 LLM을 채택합니다.

_Step3: 비동기 진단._ 선택된 전문가들은 동시에 진단한다(섹션 6). 공통 LLM을 활용함에도 불구하고 각 전문가는 역할별 설정과 도메인 지식을 고유하게 갖추고 있다. 본 논문에서는 퍼블리싱-구독 모델을 기반으로 구축된 비동기 통신 메커니즘(Zhu et al., 2017)을 사용하여 진단 프로세스를 개선한다. 즉, 전문가들은 그들의 발견들 또는 업데이트들을 "게시"하고, 그 다음, 이러한 특정 타입들의 업데이트들(예를 들어, 모든 리셋 선택된 전문가들)에 "구독"된 다른 전문가들에게 자동으로 "배달"된다.

이 메커니즘은 LLM 전문가들 간의 효율적이고 비차단적인 정보 교환(예를 들어, 메트릭 분석, 툴 출력, 결과)을 가능하게 한다. 예를 들어 CPU 전문가가 느린 쿼리의 비정상적인 CPU 부하 패턴에 대한 발견을 게시하여 다른 전문가에게 이벤트 기반 알림을 트리거할 수 있습니다. 이러한 이벤트-구동 접근법은 메모리 전문가가 이러한 느린 쿼리들에 의해 잠재적으로 야기되는 메모리 스왑 활동들을 신속하게 검출할 수 있게 한다.

_Step4: Cross Review._ 전문가들이 상이한 도메인 지식(예를 들어, CPU Expert에 대한 _os_resource_contention_ 및 메모리 Expert에 대한 _swap_usage_analysis_)을 소유하지만, 이들 전문가의 일부 공통 분석 증거(예를 들어, 오퍼레이터 유형, 구성 설정)는 서로 영감을 줄 수 있다. 따라서 모든 전문가의 진단 결과를 얻은 후 교차 검토를 수행하며 세 가지 주요 하위 단계가 있다.

\(\bullet\)_Diagnosis Summary._ 전문가의 경우 심층적인 분석을 제공하기 위해 수십 번의 반복이 필요하여 광범위한 분석 기록이 발생한다. 따라서 이러한 기록에서 핵심 정보를 효과적으로 요약하는 것이 중요하다. 이를 위해 특정 도구 및 해당 결과에 대한 입력 또는 관련 지식을 포함하는 레코드 라인(\(r_{t}\))을 점진적으로 요약한다. 특히 각 단계 \(t\)에 대해 실행 요약(\(s_{t-1}\))을 유지하여 이전 작업과 결과를 캡슐화한다. 새로운 레코드 \(r_{t}\)을 생성할 때, LLM은 \(r_{t}\)의 주요 개념을 \(s_{t-1}\)에 통합하도록 할당되어 새로운 요약 \(s_{t}\)으로 이어진다. 더 명확한 설명을 위해 다음 예제를 고려해 주십시오.

``` [Current summary \(s_{t-1}\)] - I know the start and end time of the anomaly. [새 레코드 \(r_{t}\)] - 생각: 이제 이상 작업의 시작 및 종료 시간이 있으므로 이상 작업의 원인을 진단해야 합니다. is_abnormal_metric 작업 입력: {"start_time": 1684600070, "end_time": 1684600074, "metric_name": "cpu_usage"} 관찰: "메트릭이 비정상입니다" [새 요약 \(s_{t}\)] - 이상 작업의 시작 및 종료 시간을 알 수 있습니다. - is_abnormal_metric을 검색했으며 이제 CPU 사용량이 비정상임을 알 수 있습니다. \ (\bullet\)Review Advice._ 다음으로 각 전문가가 진단 결과와 다른 전문가의 요약된 절차를 바탕으로 개선 조언을 한다. 검토 프롬프트는 \(p_{review}=\)_"위의 진단 결과를 검토 하 고 잘못된 분석 또는 불명확한 결과를 수정 하는 데 필요한 조언을 제공 하세요."_ \(\bullet\)Diagnosis Refinement._ 교차 검토 후, 각 전문가는 초기 진단을 재평가하고 더 많은 추론(예를 들어, 더 관련된 메트릭 또는 설정을 분석하기 위한 호출 도구)을 수행한다. 이러한 방식으로 진단 결과에 추가 증거, 가설 수정 또는 간과된 측면을 통합할 수 있다.

_Step5: Report Generation._ 정제된 진단 결과를 기반으로 _전문가 할당자_ 는 (_i_) 제목 (이상 항목의 요약); (_ii_) 이상 날짜; (_iii_) 자세한 이상 설명 (경고); (_iv_) 근본 원인 (진단 결과 내부); (_iv_) 솔루션 (진단 결과 내부); (_v_) 요약 된 진단 프로세스를 포함 하 여 지정 된 이상에 대 한 자세한 진단 보고서를 생성 합니다.

## 8. 실험 결과

세밀하게 준비된 마이크로 벤치마크를 사용하여 D-Bot에서 제안된 기법을 평가하기 위한 광범위한 실험을 수행한다.

### Environment Setup

**데이터베이스.** 우리는 (_i_) 빈발 쿼리를 추적 하기 위한 pg_stat_statements 플러그 인 및 (_ii_) 가상 인덱스를 만들기 위한 hypopg 플러그 인을 사용 하 여 PostgreSQL 12.5에서 D-Bot을 구현 합니다 (Bordes and Goyal, 2017).

**LLMs.** GPT-4-0613 및 gpt-3.5-turbo-16k (Goyal, 2017)를 포함 하는 프롬프트 기반 LLMs를 지원 합니다. 여기서 _온도_ 매개 변수는 재생에 유리 하 게 0으로 설정 됩니다. 미세 조정된 LLM은 _Llama 2_, _CodeLlama_ 및 _Baichuan 2_를 포함한다.

**평가된 방법.** 평가된 방법에는 (1) _HumanDBA_가 포함됩니다. 2년 근무 경력을 가진 인간 DBA가 근본 원인을 분석합니다. (2) _D-Bot_(_GPT-4_)는 GPT-4-0613(각 추론에 대해 8,192 토큰의 한도 내)에 의해 구동되는 D-Bot의 버전으로, 도메인 지식이 다른 8개의 전문가 역할 역할을 한다(섹션 4.1.3). (3) _D-Bot_ (_GPT-3.5_)는 GPT-3.5 모델에 의해 구동되는 D-Bot의 버전이다. 토큰 제한을 초과 하는 경우 _gpt-3.5-turbo-16k_ (최대 16,385 토큰)를 사용 합니다. (4) DNN은 얕은 신경망(두 개의 레이어 및 ReLU 활성화를 갖는)을 이용하여 입력 비정상 메트릭 벡터를 하나 또는 다수의 루트 원인으로 분류한다(Goyal, 2017). (5) _DecisionTree_는 입력 메트릭 값들에 대한 루트 원인들을 결정 트리 알고리즘hot 라벨을 채용한다(Zhu 등, 2018). (6) (_i_) 적합한 작업 설명 및 시연 예들을 입력하고 (_ii_) 루트 원인들을 출력하는 D-Bot에서의 기법들을 활용하지 않는 _GPT-4_ 모델. (7) _GPT-3.5_. 유사하게, 우리는 D-Bot에서 기술이 없는 GPT-3.5 모델의 성능을 테스트한다.

**절제 방법.** 절제 분석을 위한 D-Bot의 변형을 제공합니다. (1) _NoKnowledge_는 추출된 지식을 활용하지 않는 _D-Bot_(_GPT-4_). (2) _NoTreeSearch_는 체인-생각 추론(예를 들어, LangChain(Bordes and Goyal, 2017))을 채택한 _D-Bot_(_GPT-4_)이다. (3) _SingleLLM_은 진단을 위해 단일 LLM을 활용하는 _D-Bot_(_GPT-4_)이다.

### Micro Diagnosis Benchmark

(Kang et al., 2018; Goyal, 2017)과 같은 작업을 기반으로 (_i_) 다양한 이상 시나리오(예: 다른 애플리케이션, 워크로드 및 이상 유형), (_ii_) 실행 스크립트, (_iii_) 명확한 시나리오 설명(예: _전자 상거래 플랫폼의 데이터베이스, 91명의 사용자가 동시에 검색 \(\cdots\)_을 수행함), 진단 성능을 반영할 수 있는 (_iv_ 평가 메트릭을 제공하는 마이크로 벤치마크를 설계한다.

**이상 사례.** 표 1에 나와 있는 것처럼 시뮬레이션 된 다양한 응용 프로그램 집합을 포함 합니다. (_i_) _IoT (Internet of Things)_ 응용 프로그램은 센서에서 들어오는 많은 데이터를 처리 하 여 발생 하는 "높은 커밋" 이상을 주로 발생 합니다. (_ii_) _전자 상거래_ 응용 프로그램은 제품 데이터베이스에 대 한 동시 업데이트 및 판매 중 대량 데이터 검색으로 인해 발생 하는 여러 이상 (예: "높은 업데이트" 및 "대규모 데이터 가져오기")을 표시 합니다. (_iii_) _재무_ 응용 프로그램은 "빈약한 조인"과 같은 이상을 포함 하 여 복잡한 트랜잭션 작업을 제안 합니다. (_iv_) _비즈니스 인텔리전스_ 응용 프로그램은 주로 "중복 인덱스" 및 " 누락 인덱스" 이상을 포함 하 여 데이터 액세스 경로 최적화의 중요성을 강조 합니다. (_v_) _파일 공유_ 응용 프로그램 (예: Dropbox, Google Drive)은 멀티미디어 콘텐츠의 데이터 검색으로 인해 발생 하는 "대규모 데이터 가져오기" 이상을 자주 발생 합니다. (_vi_) _소셜 미디어_ 애플리케이션(예: 원래 Twitter용 MySQL)은 데이터를 빠르게 읽고 쓸 때 "높은 커밋" 변칙에 주로 직면합니다. 또한 _IoT_와 달리 "상관 관계 하위 쿼리" 변칙을 발생 시키는 복잡한 쿼리를 포함 합니다.

**평가 메트릭.** 실제 진단 평가를 위해 두 가지 메트릭을 채택합니다. 먼저, (Kang et al., 2018; Goyal, 2017)과 같은 작업들과 유사하게, 우리는 _Result Accuracy_(Acc)를 사용하여 추천된 근본 원인들, 즉, 정밀도를 정량화한다.

\[\text{Acc}=\begin{cases}\frac{A_{\text{c}}-\sigma\cdot A_{\text{w}}}{A_{\text{ a}},&\text{if }A_{a}>0\wedge A_{\text{c}}\geq\sigma\cdot A_{\text{w}}\\ 0,&\text{otherwise}\end{cases}\]

여기서, \(A_{\text{c}}\)은 정확한 원인 수를 나타내고, \(A_{\text{a}}\)은 총 원인 수를 나타내고, \(A_{\text{w}}\)은 잘못 탐지된 원인 수를 나타내며, \(\sigma\)는 0.1을 기본값으로 하는 하이퍼파라미터로, _중복원인은 누락원인에 비해 덜 유해하며 이상 원인에 대해서는 최대 4개의 근본원인으로 제한한다.

둘째, _Human Evaluated Accuracy_(HEval)는 Acc와 동일한 식을 공유한다. 그러나 HEval의 \(A^{\prime}_{\text{c}}\)은 (_i_)가 정확하게 검출되는 원인 수를 나타내며, (_ii_) 분석 과정도 일리가 있다(인간 평가). HEval은 온라인 사용에 대한 신뢰할 수 있는 진단을 제공하기 위한 필수입니다.

### Performance Comparison

D-Bot을 6개의 응용 프로그램에서 수동 진단(HumanDBA), 기존 기계 학습 방법(_DNN_, _DecisionTree_), 원본 LLMs(GPT-4, GPT-3.5) 등 세 가지 유형의 기준선과 비교한다. 각 응용 프로그램에 대해 마이크로 벤치마크에서 10개의 테스트 이상을 샘플링한다. 나머지 이상은 _DNN_, _DecisionTree_에 대한 학습 샘플로 사용됩니다. 성능 결과는 도 8 내지 도 9에 예시되어 있다.

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|} \hline
**Application** & \begin{tabular}{c} **Sync** \\ **Commits** \\ \end{tabular} & \begin{tabular}{c} **Many** \\ **Inserts** \\ \end{tabular} & \begin{tabular}{c} **High** \\ **Updates** \\ \end{tabular} & \begin{tabular}{c} **Many** \\ **Deletes** \\ \end{tabular} & \begin{tabular}{c} **Index** \\ **Missing** \\ \end{tabular} & \begin{tabular}{c} **Redundant** \\ **Massing** \\ \end{tabular} & \begin{tabular}{c} **Large** \\ **Indexes** \\ \end{tabular} & \begin{tabular}{c} **Large** \\ **Data Insert** \\ \end{tabular} & \begin{tabular}{c} **Zero** \\ **Data Feitch** \\ \end{tabular} &
\begin{tabular}{c} **Poor** \\ **Join** \\ \end{tabular} & **Cases** \\ \hline Internet of Things & ✓ & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) & 83 \\ \hline E-Commerce & \(\times\) & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & 211 \\ \hline Financial & ✓ & ✓ & \(\times\) & \(\times\) & \(\times\) & ✓ & ✓ & ✓ & ✓ & ✓ & \(\times\) & 31 \\ \hline Business Intel. & \(\times\) & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & \(\times\) & 20 \\ \hline File Sharing & \(\times\) & ✓ & ✓ & \(\times\) & \(\times\) & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & 47 \\ \hline Social Media & ✓ & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) & \(\times\) & ✓ & 147 \\ \hline \end{tabular}
\end{table}
표 1. 마이크로 벤치마크 통계. 응용 프로그램에는 10가지 일반적인 근본 원인(섹션 2.2에 소개됨)이 포함됩니다.

**진단 성능.** D-Bot은 소셜 미디어 애플리케이션에 대해 80%(_D-Bot(GPT-3.5))의 정확도로 _HumanDBA_를 능가하는 등 _HumanDBA_와 같은 경쟁적인 성능을 달성합니다. D-봇은 또한 나머지 기준선에 비해 상당한 성능 이득을 보여준다(예를 들어, _DNN_ 및 _DecisionTree_에 대해 8% 내지 54% 범위의 정확도 개선). 이유는 세 가지입니다.

첫째, D-봇은 도구를 현명하게 활용하고 정보에 입각한 진단을 제공할 수 있습니다. 예를 들어 _pg_stat_statements_ 보기를 쿼리하여 _"동일한 테이블을 통한 UPDATE 및 INSERT 작업의 과도한 사용으로 인한 높은 메모리 사용"_과 같은 특정 문제를 식별합니다. 반대로, 기준선은 근본 원인을 감지하기 위해 고군분투하며, 종종 실행 가능한 개선에 필요한 특수성이 부족한 _"리소스 경쟁 문제 해결__과 같은 일반적인 조언에 결함이 있어 실제 응용 프로그램에서 덜 효과적이다.

둘째, _D-봇(GPT-4)_은 상황 이해(LLM) 및 트리-검색 추론 능력을 소유한다. 예를 들어, 반사 메커니즘으로, _D-봇(GPT-4)_은 포괄적인 분석을 수행하고 가장 유익한 액션 체인(예를 들어, 계획의 총 비용을 계산하고 최적화 액션을 결정하는 것)을 따를 수 있다. 대조적으로, 기준선은 기본 비정상 메트릭 값으로만 입력하고 일반 분석을 수행하며 종종 근본적인 원인을 간과한다. 예를 들어 _INSERT_LARGE_DATA_의 경우 _GPT-4_는 _node_procs_running_ 메트릭을 사용 하 여 실행 프로세스의 증가된 수를 식별 하 여 조기 진단을 종료 합니다. 또한 _DNN_ 및 _DecisionTree_는 텍스트 데이터를 활용할 수 없으므로 _결합 불량_과 같은 복잡한 이상을 해결할 수 없습니다.

셋째, _D-Bot(GPT-4)_는 문서 지식을 활용하여 상관 서브쿼리 구조와 같은 잠재적인 성능 병목 현상의 분석을 학습한다. 우리는 _GPT-4_ 및 _GPT-3.5_가 지원되지 않는 가설을 만드는 경향이 있어 부정확한 진단을 초래한다는 것을 발견했다. 예를 들어 로그된 쿼리에서 _SORT_ 작업을 감지하면 _GPT-3.5_ 병목 현상을 _"대용량 데이터 볼륨의 자주 읽고 정렬"_, 누락된 쿼리 구조 문제로 간주합니다. _HumanDBA_에 비해 _D-Bot(GPT-4)_는 근본 원인을 찾는 데 도움이 되는 중요한 세부 사항을 캡처하는 데 더 신중하다. 예를 들어, _소셜 미디어_에서 _D-봇(GPT-4)_은 다양한 소스(여러 시스템 메트릭 및 쿼리 오퍼레이터가 리소스를 소비하는 방식 등)로부터 데이터를 수집함으로써 _HumanDBA_보다 더 우수하다. 이렇게 하면 몇 가지 느린 쿼리에 초점을 맞출 때 _HumanDBA_가 무시할 수 있는 동시 삽입으로 인한 높은 I/O 문제와 같은 문제를 해결할 수 있습니다.

**진단 오버헤드.**_(1) 진단 시간입니다. HumanDBA_는 전형적인 이상에도 진단 보고서를 작성하는 데 1~2시간이 필요하다. 이 시간은 근본 원인이 비교적 간단한 경우에도 인덱싱 및 쿼리 다시 쓰기와 같은 솔루션을 고안하는 데 주로 사용됩니다. 대신 D-Bot은 비교적 복잡한 이상(예: 두 개의 근본 원인이 있는 복합 이상 \(k\)의 경우 _5.38분_)을 진단하는 데 10~몇 분이 걸립니다. 왜냐하면 D-Bot은 미리 구비된 툴과 효율적으로 상호 작용할 수 있고(컨텍스트 임베딩), 효율성을 높일 수 있기 때문이다(여러 LLM의 협업). 기존의 분류기는 제한된 메트릭을 미리 정의된 원인에 매핑하기 때문에 진단 시간이 가장 낮다. (2) _진단 비용._ 전통적인 분류기

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline
**진단** & \multicolumn{2}{c|}{**단일 원인 이상**} & \multicolumn{2}{c|}{**다중 원인 이상**} \\ \cline{2-5}
**Method** & **Acc** & **HEval** & **Acc** & **HEval** \\ \hline _HumanDBA_ & 0.955 & 0.720 & 0.487 & 0.806 \\ \hline _D-Bot (GPT-4)_ & 0.754 & 0.500 & 0.655 & 0.669 \\ \hline _D-Bot (GPT-3.5)_ & 0.542 & 0.370 & 0.533 & 0.493 \\ \hline _DNN_ & 0.352 & N/A & 0.036 & N/A \\ \hline _DecisionTree_ & 0.331 & N/A & 0.086 & N/A \\ \hline _GPT-4_ & 0.351 & 0.39 & 0.105 & 0.151 \\ \hline _GPT-3.5_ & 0.266 & 0.2 & 0.144 & 0.130 \\ \hline \end{tabular}
\end{table}
표 2. 상이한 이상에 대한 성능.

도 8. 성능 비교(결과 정확도). 참고 _Acc_는 일반적인 수치 메트릭으로 진단 용량(예: 진단 프로세스가 합리적인지 여부)을 완전히 반영할 수 없다.

도 9. 성능 비교(Human Evaluation). 우리는 블랙박스 문제가 있거나 사람이 이해하기 쉬운 근본 원인 분석을 제공하지 못하기 때문에 _DNN_ 및 _DecisionTree_를 포함하지 않는다.

및 D-Bot이 _HumanDBA_보다 더 경제적이다. DNN_ 및 _DecisionTree_는 최소한의 시스템 리소스가 필요합니다. 그리고 D-Bot은 40k LLM 토큰으로 이상 \(k\)을 진단하는 데 1.8달러 정도의 경제적 비용으로 많은 인력을 절감할 수 있다.

1. _D-봇은 고급 상황 이해 및 지식 및 도구 활용으로 인해 기준선(8%~54%)에 비해 놀라운 개선을 달성하고 심지어 인간의 전문 지식과 긴밀히 경쟁합니다._

다른 Anomalies에 대한 성능입니다._\ (D\)-_Bot (GPT-4)_는 단일 원인 이상(0.754)에서 더 낮은 정확도를 가지지만, 0.655의 정확도로 다중 원인 이상(0.655)에서 현저한 일관성을 보인다. 이러한 일관성은 _HEval_ 점수(각각 0.500 및 0.669)에도 반영되어 _D-Bot (GPT-4)_가 다양한 유형의 이상에서 안정적인 성능을 유지함을 시사한다. D-Bot(GPT-3.5)_ 및 _DNN_, _DecisionTree_, _GPT-4_ 및 _GPT-3.5_와 같은 다른 방법은 특히 다중 원인 이상에서 _Acc_ 및 _HEval_ 모두에서 성능이 낮은 일반적인 경향을 보여 D-Bot과 같은 고급 진단 방법이 필요한 이러한 시나리오의 복잡성을 강조한다. 한편, _HumanDBA_의 경우 단일(0.720) 및 다중 원인 이상(0.806) 모두에서 _HEval_ 점수가 상대적으로 높아 인간 경험에 대한 이해의 필요성을 보여준다.

2. _D-봇은 다양하고 복잡한 이상 유형에서 보다 균형 있고 신뢰할 수 있는 성능을 제공합니다._

LLM Factors. _D-Bot(GPT-4)_와 _D-Bot(GPT-3.5)_ 사이의 성능 갭은 상당하며, _D-Bot(GPT-4)_는 _D-Bot(GPT-3.5)_보다 애플리케이션의 정확도와 안정성 면에서 최대 30%까지 우수하다. _ D-봇(GPT-4)_은 정밀한 도구 호출 명령 및 종합적인 진단 요약을 생성하는 데 탁월합니다. 예를 들어 _D-Bot(GPT-3.5)_이 종종 부족 하는 작업인 큰 테이블 페치를 포함 하는 복잡한 쿼리를 능숙 하 게 식별 합니다. 대조적으로, _D-봇(GPT-3.5)_은 더 일반화되고 때때로 부정확한 동작 명령을 생성하는 경향이 있어, 덜 효과적인 결과를 초래한다.

3. _A powerful LLM은 D-Bot의 진단 성능에 도움이 될 수 있으며, 이는 신속한 추적의 효과와 근본 원인 분석의 깊이를 반영합니다._

### Ablation Study

그림 10과 같이 D-Bot의 세 가지 주요 구성 요소, 즉 문서 지식 매칭(_NoKnowledge_), 트리 검색 기반 추론(_NoTreeSearch_), 다중 에이전트 진단(_SingleLLM_)의 유효성을 검증한다.

#### 8.4.1. 문서 지식 매칭

프롬프트에서 관련 지식이 없으면 LLM 전문가는 도구를 호출하고 근본 원인을 분석하기 위해 주로 전문가 설정(즉, 역할, 작업, 단계)에 의존한다. _NoKnowledge_를 D-Bot과 비교할 때 19.2%에서 64.1% 범위의 진단 정확도가 감소하는 것을 관찰한다. 두 가지 관찰이 있습니다. 먼저 _NoKnowledge_ 는 컨텍스트만 사용 하 여 관련 근본 원인을 명확하게 구분할 수 없기 때문에 _D-Bot (GPT-4)_ 에 대해 2.05 배 더 많은 중복 근본 원인을 생성 합니다. 예를 들어 _"많은 삽입"_ 및 _"대규모 데이터 삽입"_과 같은 근본 원인은 둘 다 삽입 작업을 포함 하지만 올바르게 식별 하려면 삽입 작업 수 및 테이블 크기와 같은 세부 정보에 대 한 특정 지식이 필요 합니다. 둘째, 기준선과 마찬가지로 _NoKnowledge_ 는 종종 매우 일반적인 진단 (예: "CPU 프로세스의 비정상 패턴")을 제공 하 고 많은 이상을 정확하게 식별 하지 못합니다. 또한, _GPT-4_와 같은 LLM은 개방형 말뭉치에서 사전 학습되지만 데이터베이스 진단과 같은 전문 작업을 위해 외부 지식 매칭(미세 조정은 지식 업데이트에 제한)이 필요하다는 것을 발견했다.

#### 8.4.2. Tree Search Based Diagnosis

_NoTreeSearch_는 _D-Bot(GPT-4)_보다 덜 효과적으로 진단하여 35.85% 이상의 성능 감소를 보인다. 트리 검색은 잘못된 지식 매칭 또는 도구 API 호출(자식 노드 확장을 위한 동작)을 수정하는 데 중요한 역할을 하며, 이는 특히 다양한 추론 선택을 포함하는 단일 원인 이상에 대한 진단 정확도를 크게 향상시킨다. 예를 들어 특정 쿼리 관련 문제를 식별 하거나 데이터베이스 노브를 최적화 하는 것과 같은 시나리오에서 트리 검색을 사용 하면 _D-봇 (GPT-4)_이 여러 잠재적 솔루션을 탐색 하 고 가장 효과적인 솔루션을 정확하게 찾을 수 있습니다.

#### 8.4.3. Multi-Agent Diagnosis

본 논문에서는 다중 에이전트 모드(_D-Bot(GPT-4)_)가 단일 에이전트 모드(single)에 비해 효율적인지 검증한다. 예를 들어 IoT 애플리케이션에서 _D-Bot(GPT-4)_은 근본 원인을 식별하는 데 77.27%의 성공률을 달성했으며, 이는 _SingleLLM_의 39.09% 성공률보다 크게 증가했습니다. 또한, 평균 진단 시간에 대한 테스트에서 D-Bot(다중 에이전트 모드)가 _SingleLLM_(단일 에이전트 모드)에 비해 더 효율적임을 보여주었다. 이유는 이중적이다. 먼저 D-Bot은 평균 2명 이상의 전문가(최대 3명)를 고용하며, 서로 다른 메트릭과 도메인 지식을 활용하여 근본 원인을 탐색하고 _SingleLLM_보다 더 많은 근본 원인을 도출합니다. 그리고 이러한 근본 원인은 추가로 조사되고, 선택된다.

도 10. 절제 연구(인간 평가)

교차 검토 중에 개선됩니다. 따라서 D-Bot은 _SingleLLM_ 보다 높은 진단 정확도를 달성 합니다. 둘째, D-Bot은 전문가 선정 및 교차 검토에 시간이 걸리지만, 비동기 메커니즘은 단일 전문가에서 트리 탐색 알고리즘의 반복 회수를 줄여 일반적으로 대부분의 진단 시간이 소요된다. 따라서 D-Bot은 진단 시간에서 _SingleLLM_ 보다 효율적입니다.

4. _D-Bot에서 제안된 기술은 중복된 근본 원인을 줄이고 정확한 이상 식별을 강화하여 진단 정확도를 높이는 데 중요합니다._

### Model Fine-tuning

**준비.** 먼저 5개의 하위 작업(예: 도구 호출)과 총 2819개의 샘플로 구성된 _D-Bot(GPT-4)_의 진단 프로세스를 기록합니다(그림 11(a) 참조). 멀티 태스크 미세 조정 데이터 세트로 함께 혼합합니다. 특히 모델 입력에는 프롬프트 및 이력 메시지가 포함 되며, 해당 _D-Bot (GPT-4)_ 응답을 시뮬레이션 하기 위해 LLM을 미세 조정 합니다 (클렌징 후). LLM은 503GB RAM 및 1 NVIDIA A100 GPU를 갖는 머신 상에서 트레이닝된 PyTorch 및 BMTrain(Tran 등, 2019)을 사용하여 구현된다.

**훈련 절차.** 3개의 로컬화된 SOTA LLM(예: _Llama 2-13B_, _CodeLlama-13B_, _Baichuan2-13B_)을 미세 조정합니다. 그림 11(b)와 같이 모든 LLM은 10에포크 내에서 수렴한다. 그런 다음 최상의 에포크 체크포인트(즉, _Llama 2_의 경우 4번째 에포크, _CodeLlama_의 경우 1번째 에포크, _Baichuan 2_의 경우 10번째 에포크)를 수동으로 선택한다. 명백한 손실 감소는 모델 성능 증가를 의미 하지 않습니다. 우리는 손실이 적은 많은 획기적인 체크포인트를 종종 미세 조정 데이터에 과도하게 맞춤(예를 들어, 텍스트 생성 능력을 상실하고 짧은 혼란스러운 응답을 생성하는 경향이 있음)한다. 또한, _Llama 2_는 최상의 에포크에서도 합리적인 진단 결과(_Acc_는 대부분의 경우에 0과 같다)를 생성할 수 없다.

**성능 비교.** 그림 11(c)-(d)에 표시된 것처럼 미세 조정 후 시연된 LLM은 27개의 테스트 사례에서 _GPT-4_와 유사한 성능을 달성합니다. 몇 가지 관찰이 있습니다. 먼저 _CodeLlama_는 메트릭 및 쿼리에 더 민감한 코드 생성에 특화되어 있기 때문에 _CodeLlama_는 재무, IoT 및 BI 응용 프로그램에서 가장 잘 수행 됩니다. 예를 들어 여러 JON을 포함하는 _느린 쿼리_ 를 근본 원인으로 정확하게 식별할 수 있습니다. 둘째, _Baichuan2_는 적절한 전문가(예: 메모리 전문가)를 할당하고 근본 원인을 자세히 분석할 수 있는 파일 애플리케이션에 가장 적합합니다(예: 하드웨어 구성에서 프로비전되지 않은 디스크 I/O 가리키기). 그러나 금융 애플리케이션에서 _Baichuan2_의 _HEval_ 성능은 크게 저하된다. 예를 들어, 모델은 많은 근본 원인을 나열할 수 있지만 근거가 충분한 분석을 제공하지 않는다. 셋째, _GPT-4_는 전자 상거래 및 미디어 애플리케이션에 대해 가장 잘 수행하고, _모든 애플리케이션에 걸쳐 균형 잡힌 성능_을 보여준다. 또한 지역화된 LLM은 익숙하지 않은 이상 현상에 대해 덜 일반화 가능성을 보여준다. 예를 들어 _삭제 작업_을 근본 원인으로 하는 샘플 수는 다른 샘플보다 훨씬 작으므로 이러한 경우 미세 조정 된 LLM이 실패 하는 경우가 많습니다.

5. 로컬화된 SOTA LLM을 사용하는 _D-봇은 D-봇(GPT-4)과 유사한 진단 성능을 달성할 수 있지만, 이들의 일반화 가능성은 미세 조정 샘플의 영향을 크게 받는다._

## 9. 관련 작업

**데이터베이스 진단.** 기존 작업은 주로 근본 원인을 분석하기 위해 경험적 규칙 및 분류 방법에 의존합니다. ADDM 도구(Han et al., 2017)는 데이터베이스 자원 모듈의 그래프를 유지하며, 이를 기반으로 질의 수행 시간을 추정하고 병목 현상을 추론한다. DBSherlock(Wang et al., 2018)은 결정-트리-유사 방법을 이용하여 술어(\(Attr>k\)의 형태로)를 구성한다. ISQUAD(Srivastava et al., 2017)는 질의들을 그들의 메트릭 벡터들로 클러스터링함으로써 루트 원인들을 생성한다. 그러나, 이러한 방법들은 큰 인간 개입(예를 들어, 규칙, 특징, 라벨 설계)을 필요로 한다. 또한 실제 진단을 위한 몇 가지 중요한 기능(예: 새 컨텍스트 정보 수용, 쿼리 로그 분석)이 부족합니다. 유지 관리 지식을 통합하는 일부 LLM 기반 방법(Srivastava 등, 2017)이 있지만, 이들은 일반적인 챗봇 도구(예를 들어, Q&A 연습)에 초점을 맞추고 또한 시나리오별 진단을 수행하지 못한다.

**LLM 에이전트.** 최근 작업에 따르면 LLM은 메모리 메커니즘 및 도구와 결합할 때 실제 컨텍스트에서 인간과 유사한 상호 작용 및 의사 결정을 모방할 수 있습니다 (Srivastava 등, 2017). 첫째, 웹 브라우저(Wang et al., 2018; Wang et al., 2018) 및 위키피디아 검색(Wang et al., 2018; Wang et al., 2018)으로부터 코드 인터프리터(Han et al., 2017; Wang et al., 2018; Wang et al., 2018) 및 다면 도구 세트(Wang et al., 2018; Wang et al., 2018)에 이르기까지 다양한 도구를 갖는 LLM 에이전트의 증강은 LLM의 적응성을 상당히 향상시켰다. 개별 에이전트 기술 외에도, 집단 지성을 위해 다수의 LLM 에이전트를 조정하는 것에 대한 관심이 증가하고 있다(Han et al., 2017; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). 특히, AgentVerse(Han et al., 2017)는 다수의 LLM 에이전트들 간의 팀워크가 많은 작업들에서 더 나은 단일 에이전트들을 수행할 수 있음을 보여준다. D-봇은 다중 에이전트 패러다임에서 _LLM-동력 진단 시스템을 제시한다._

## 10. Conclusion

본 논문에서는 대용량 언어 모델을 활용한 데이터베이스 진단 시스템을 제안한다. 문서로부터 오프라인 지식 추출을 수행하고 기존 도구로부터 함수 API를 준비했다. 온라인 진단을 위한 LLM 프롬프트에 적합한 지식과 API를 매칭하고 도구를 정확하고 효과적으로 활용하고 지식으로 분석을 수행하기 위해 트리 검색 기반 알고리즘을 제안했다. 우리는 협업으로 효율성을 향상시킨 협업 진단 메커니즘을 설계했다.

도 11. 모델 파인튜닝의 성능.

를 포함하는 것을 특징으로 하는 액정표시장치. 실험 결과는 D-Bot이 기준선 및 인간 DBA에 비해 현저한 개선을 달성했음을 보여준다.

## Acknowledgments

(1) 이상 시뮬레이션 지원에 대한 둥펑리, (2) 도구 플러그인 보조에 대한 웨이저우, (3) 샤오휘 니에, 단페이, 빈위안 후이, 첸첸, 유셴에게 이 연구에 대한 귀중한 조언에 감사한다.

## References

* [1][https://github.com/openmb/agentverse](https://github.com/openmb/agentverse), 2023년 12월에 마지막으로 액세스되었습니다.
* [2][https://openai.com/](https://openai.com/), Last Accessed In December, 2023.
* [3][https://github.com/hypog/hypog](https://github.com/hypog/hypog), 2023년 9월에 마지막으로 액세스했습니다.
* [4][https://petune.leopard.in./](https://petune.leopard.in./), Last Accessed In September, 2023.
* [5] Edmon Begoli, Jesus Camacho-Rodriguez, Julian Hyde, Michael Jior, and Daniel Lemire. Apache calcite: A foundational framework for optimized query processing over heterogeneous data sources. In _Proceedings of the 2018 International Conference on Management of Data_, pages 221-230, 2018.
* [6] Yance W Berger and Yan-Yan Zhou. Kolmogorov-smirnov test: Overview. Wiley &starfest: Statistics refer online. 2014.
* [7] Harrison Chase. LangChain. [https://github.com/hwchaeae17/langchain](https://github.com/hwchaeae17/langchain), 2022.
* [8] Surjit Chaudhuri and Vivek R. Narayan. An efficient cost-driven index selection tool for microsecond SQL server. In _VLDB_, pages 146-155, 1997.
* [9] Tamel Chen and Carlos Guestrin. Xebboost: A scalable tree boosting system. In _SIGKDD_, pages 785-794, 2016.
* [10] Weize Chen, Yuchange Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yanxi Lu, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and lie Zhou. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. _CoRR_, abs/2308.10848, 2023.
* [11] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. _CoRR_, abs/2211.1258, 2022.
* [12] Karl Dias, Mark Ramacher, Uri Shaft, Venkateshwaran Venkataramani, and Graham Wood. Automatic performance diagnosis and tuning in oracle. In _Second International Conference on Innovative Data Systems Research, CIDR 2005. Asilamar, CA, USA, January 4-7, 2005. Online Proceedings_, pages 84-94. www.cd.uk/h.org, 2005.
* [13] Ning Ding, Shengling Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Hai-Tao Zheng, and Maosong Sun. Openprompt: An open-source framework for prompt-learning. _arXiv preprint arXiv:1211.01989_, 2021.
* [14] Jesse Dodge, Gabriel Illnerao, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. _arXiv preprint arXiv:2002.06305_, 2020.
* [15] Yihu Du, Shuang Li, Antonio Torralillo, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. _CoRR_, abs/2305.14352, 2023.
* [16] Luyu Gao, Anam Madan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: program-aided language models. In Andreas Krause, Emura Brunkull, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlettej, _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, volume 202 of _Proceedings of Machine Learning Research_, pages 1076-1079, 1979. PML, 2023.
* [17] Google Guo, Hui Wang, David Bell, Yuxin Bai, and Kieran Greer. Knn model-based approach in classification. In _CoRR_, pages 986-996. Springer, 2003.
* [18] Hai-Xiang, Li Xiao-Yan Liu Chang, and et al. Systematic definition and classification of data anomalies in dbms (engils version). _arXiv preprint arXiv:2110.14230_, 2021.
* [19] Siriu Hong, Xiuxi Zheng, Jonathan Chen, and et al. Metagpt: Meta programming for multi-agent collaborative framework. _CoRR_, abs/2308.00352, 2023.
* [20] Shiyue Huang, Ziwei Wang, Xinyi Zhang, Yaofeng Tu, Zhongliang Li, and Bin Cui. Dpra: A benchmark for transactional database performance anomalies. _Proc. ACM Manag._, 2017(1):27:1-226, 2023.
* [21] Lianyan Jin and Gouliang Li. Ai-based database performance diagnosis. _Journal of Software_, 32(3):845-858, 2021.
* July 5, 2019_, pages 918-935. ACM, 2019.
* [23] Kamran Khan, Saif Ur Rehman, Kamran Aziz, Simon Fong, and Sababady Sarasvady. Dbscan: Past, present and future. In _ICADIWT 2014_, pages 232-238. IEEE, 2014.
* [24] Jay Kreps, Neha Narkhede, Jun Rao, et al. Kafka: A distributed messaging system for log processing. In _Proceedings of the NetDB_, volume 11, pages 1-7. Athens, Greece, 2011.
* [25] Hai Lan, Zhifeng Bao, and Yuwei Peng. An index advisor using deep reinforcement learning. In _CIKM_, pages 2105-2108. ACM, 2020.
* [26] Guohao Li, Hasan Abel Al Kaler Hannamoudi, Hani Irani, Dmitri Kihzbullin, and Bernard Ghanem. CAMEL: communicative analysis efforts for "mind" exploration of large scale language model society. _CoRR_, abs/2303.17760, 2023.
* [27] Jiqi Li, Mengheng Wang, Zliong Zheng, and Muhan Zhang. Google: Can long-context language models understand long contexts? _arXiv preprint arXiv:2311.04039_, 2023.
* [28] Zeyan Li, Nengwen Zhao, Mingjie Li, et al. Actionable and interpretable fault localization for recurring failures in online service systems. In _Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering_, pages 906-1008, 2022.
* [29] Ping Liu, Shenglin Zhang, Yongqian Sun, Yuan Meng, Jiahai Yang, and Dan Pei. Fluxinfer: Automatic diagnosis of performance anomaly for online database system. In _39th IEEE International Performance Computing and Communications Conference, TCCC 2020, Austin, TX, USA, November 6-8, 2020_, pages 1-8. IEEE, 2020.
* [30] Xiaozu Liu, Zheng Yin, Chao Zhao, Congcong Ge, Lu Chen, Yunjun Gao, Dimeing Li, Ziting Wang, Gaozhong Liang, Jian Tan, and Feifei Li. Pinsol: Pimpoint root cause sqla to resolve performance issues in cloud databases. In _38th IEEE International Conference on Data Engineering, ICDE 2022, Kuala Lumpur, Malaysia, May 9-12, 2022_, pages 2549-2561. IEEE, 2022.
* [31] Yuhe Liu, Changhua Pei, Longlong Xu, Bohan Chen, Mingse Sun, Zhirui Zhang, Yongqian Sun, Shenglin Zhang, Kwan Wang, Haiming Zhang, et al. Openeval: A comprehensive task-oriented alogs benchmark for large language models. _arXiv preprint arXiv:2310.07637_, 2023.
* [32] Xiangliu Lu, Zhe Xie, Zeyan Li, Mingjie Li, Xiaohui Nie, Nengwen Zhao, Qingyang Yu, Shenglin Zhang, Kaixin Sui, Lin Zhu, and Dan Pei. Generic and robust performance diagnosis via causal inference for OLTP database systems. In _22nd IEEE International Symposium on Cluster, Cloud and Internet Computing, CCGrid 2022, Taormina, Italy, May 16-19, 2022_, pages 655-664. IEEE, 2022.
* [33] Killan Lucas. Open interpreter. [https://github.com/charlesepwd/project-title](https://github.com/charlesepwd/project-title), 2023.
* [34] Minghua Ma, Zheng Yin, Shenglin Zhang, and et al. Diagnosing root causes of intermittent slow queries in large-scale cloud databases. _Proc. VLDB Endow._, 13(8):1176-1189, 2020.
* [35] Reichiro Nakano, Jacob Hilton, Suchir Balaji, and et al. Webgpt: Browser-assisted question-answering with human feedback. _CoRR_, abs/2112.09332, 2021.
* [36] Branmurett Ottens, Christos Dimitrakakis, and Boi Faltings. Duct: An upper confidence bound approach to distributed constraint optimization problems. _ACM Transactions on Intelligent Systems and Technology (TIST)_, 8(5):1-27, 2017.
* [37] Joon Sung Park, Joseph C. O'Brien, Carrie J. Cai, Meredith Ringel Morris, Frey Liang, and Michael S. Bernstein. Generative agents: Interactive simulance and human behavior. _CoRR_, abs/2304.03442, 2023.
* [38] Chen Qian, Xin Cong, Cheng Yang, Weizen Chen, Yusheng Su, and et al. Communicative agents for software development. _arXiv preprint arXiv:2307.07924_, 2023.
* [39] Yujia Qin, Ziban Cai, Dian Jin, and et al. Webgpt: Interactive web search for chinese long-form question answering. In Anna Rogers, Jordan L. Boyd-Graber, and Naosoli Okazaki, editors, _ACL_, pages 8968-8988. Association for Computational Linguistics, 2023.
* [40] Yujia Qin, Shenggling Hu, Yankai Lin, and et al. Tool learning with foundation models. _arXiv preprint arXiv:2304.08354_, 2023.
* [41] Yujia Qin, Shenggling Hu, Yanxixi Lin, and et al. Tool learning with foundation models. _CoRR_, abs/2304.08354, 2023.
* [42] Yujia Qin, Shihao Liang, Yining Ye, and et al. Toollim: Facilitating large language models to master 16000 real-world apis. _CoRR_, abs/2307.16789, 2023.
* [43] Raj Ramachandran, R. Nohdin, and P P Shogdi. Anomaly detection in role administered relational databases-a novel method. In _ICACI_, pages 1017-1021. IEEE, 2018.
* [44] Vipula Rawte, Amit Sheth, and Amitava Das. A survey of hallucination in large foundation models. _arXiv preprint arXiv:2309.05922_, 2023.
* [45] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Kentara Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, _EANLIP-IJCNLP_, pages 3980-3990. Association for Computational Linguistics, 2019.
* [46] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. _arXiv preprint arXiv:1908.10084_, 2019.
* [47] Stephen Robertson, Hugo Zangza, et al. The probabilistic relevance framework: Bm25 and beyond. _Foundations and Trends in Information Retrieval_, 3(4):338-389, 2009.
* [48] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raleanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Sciaolom. Toolformer: Language models can teach themselves to use tools. _CoRR_, abs/2302.04674, 2023.
* [49] Yan-Yan Song and LIU Ying. Decision tree methods: applications for classification and prediction. _Shanghai archives of psychiatry_, 27(2):130, 2015.
* [50] HarICDE_, pages 101-110, 2000.
* Wang et al. [2022] Zhaoguo Wang, Zhou Zhou, Yicun Yang, Haoran Ding, Gansen Hu, Ding Ding, Chuhu Tang, Haibo Chen, and Jinyang Li. Wetime: Automatic discovery and verification of query rewrite rules. In _Proceedings of the 2022 International Conference on Management of Data_, pages 94-107, 2022.
*황[1987] 류영황. 관계형 데이터베이스에서 인덱스 선택 _ Foundations of Data Organization_, pages 487-500, 1987.
* Wu et al. [2023] Qingyu Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beihin Li Li, Ziaoyun Zhang, and Chi Wang. Autogen: Enabling next-genl LML applications via multi-agent conversation framework. _CoRR_ abs/2308.08155, 2023.
* Xiong et al. [2023] Wenhan Xiong, Jinyang Liu, Igor Molybgo, et al. Effective long-context scaling of foundation models. _arXiv preprint arXiv:2309.16039_, 2023.
* Yao et al. [2023] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. Rect: Synergizing reasoning and acting in language models. In _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023_ OpenReview.net, 2023.
* July 01, 2016_, pages 1599-1614. ACM, 2016.
* Zeng et al. [2023] Guoyang Zeng, Xu Han, Zhengyan Zhang, Zhiyuan Liu, Yankai Lin, and Maosong Sun. Openbank: Big model systems for large-scale representation learning. In _Representation Learning for Natural Language Processing_, pages 463-489. Springer Nature Singapore, 2023.
* Zheng et al. [2023] Qinkai Zheng, Xiao Xia, Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Li Shen, Anolu Wang, Yang Li, et al. Codegevec: A pre-trained model for code generation with multilingual evaluations on human-val. _arXiv preprint arXiv:2303.17568_, 2023.
* Zhou et al. [2023] Xuanhe Zhou, Guoliang Li, and Zhiyuan Liu. Llm as dba. _arXiv preprint arXiv:2308.05481_, 2023.
* Zhou et al. [2022] Xuanhe Zhou, Luyang Liu, and et al. Automated: An incremental index management system for dynamic workloads. In _ICDE_, pages 2196-2208. IEEE, 2022.
* Zhou et al. [2022] Yongchao Zhou, Andrei Iona Muresanu, Ziwen Han, Keiran Paster, Silvin Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. _arXiv preprint arXiv:2211.01910_, 2022.
