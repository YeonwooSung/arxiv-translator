<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 지수적으로 빠른 언어 모델링\n' +
      '\n' +
      '피터 벨칵과 로저 왓텐호퍼\n' +
      '\n' +
      'ETH Zurich\n' +
      '\n' +
      '{belcak,wattenhofer}@ethz.ch\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '언어 모델은 개인의 추론을 위해 뉴런의 지수적인 부분만 사용하면 된다.\n' +
      '\n' +
      '증명으로서, 유사한 BERT 모델과 동등하게 수행하면서 추론 동안 뉴런의 0.3%를 사용하는 BERT 변형인 울트라패스트BERT를 제시한다. UltraFastBERT는 각 계층 추론을 위해 4095개의 뉴런 중 12개만 선택적으로 결합한다. 이는 피드포워드 네트워크를 고속 피드포워드 네트워크(FFF)로 대체함으로써 달성된다.\n' +
      '\n' +
      '조건부 신경망 실행의 전체 가속 가능성을 해제하기 위한 진정한 효율적인 구현은 현재 존재하지 않지만, 최적화된 베이스라인 피드포워드 구현보다 78배 속도 향상을 달성하는 고급 CPU 코드와 동등한 배치 피드포워드 추론보다 40배 속도 향상을 제공하는 PyTorch 구현을 제공한다.\n' +
      '\n' +
      '훈련 코드, 벤치마킹 설정 및 모델 가중치를 게시합니다.\n' +
      '\n' +
      '각주 1: [https://github.com/pbelcak/UltraFastBERT](https://github.com/pbelcak/UltraFastBERT)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '피드포워드 계층들은 대형 언어 모델들의 파라미터들의 대부분을 보유한다(Brown et al., 2020; Anil et al., 2023). 그러나 모든 뉴런이 모든 입력에 대한 추론 시간에 피드포워드 계층 출력의 계산에 관여할 필요는 없다.\n' +
      '\n' +
      '일반적으로 접근 가능한 증명을 위해 피드포워드 계층을 빠른 피드포워드 네트워크로 대체하는 BERT 아키텍처(Devlin et al., 2018)의 변형인 UltraFastBERT를 제시한다. 다운스트림 성능 측면에서 UltraFastBERT는 크기가 비슷하고 유사한 훈련 절차를 거치는 다른 BERT 유사 모델과 동등한 성능을 보인다. 그러나 UltraFastBERT의 중간 계층은 설계에 따라 지수적으로 더 빠르다: 각각 \\(n\\) 뉴런이 있는 피드포워드(FF)와 패스트 피드포워드(FFF) 네트워크가 주어졌을 때, FFF를 통과하는 순방향 통과의 시간 복잡도는 FF의 경우 \\(\\mathcal{O}\\left(n\\right)\\) 대신 \\(\\mathcal{O}\\left(\\log_{2}n\\right)\\)이다. 이는 FFF가 뉴런을 균형 잡힌 이진 트리로 구성하고 트리의 한 가지 가지만 입력에 조건부로 실행한다는 사실의 결과이다.\n' +
      '\n' +
      'FFF에 대한 추론을 수행하는 것은 조건부 행렬 곱셈(CMM)을 수행하는 데 해당하며, 여기서 입력 도트의 행과 신경 가중치 열이 한 번에 하나씩 있고 진행할 가중치 열이 이전 도트 곱 연산의 출력에 따라 선택된다. 이러한 방식으로, 모든 뉴런은 일부 입력에 의해서만 사용되고, 어떠한 입력도 네트워크에 의해 처리되기 위해 단지 소수의 뉴런 이상을 필요로 하지 않는다. 이것은 전통적인 피드포워드 네트워크의 중심에 있고 모든 열을 가진 모든 행의 내적을 계산하는 밀집 행렬 곱셈(DMM)과 대조적이다.\n' +
      '\n' +
      '조건부 행렬 곱셈의 기본적이고 효율적인 구현이 존재하지 않으며, 인기 있는 딥러닝 프레임워크는 고수준의 시뮬레이션 외에 이를 구현하는 데 사용할 수 있는 인터페이스를 제공하지 않는다. 따라서 우리는 BLAS 라이브러리의 포인터 배치 행렬 곱셈 루틴을 기반으로 한 CPU 구현 세트를 제공한다. 이후 섹션에서는 다양한 수준의 최적화에서 CPU와 GPU 구현 간의 비교를 제공하고 이미 상당한 가속도의 명백한 증거가 있지만 더 많은 가능성이 있음을 주목한다.\n' +
      '\n' +
      '주의의 역할.많은 문헌은 이미 주의 메커니즘의 실행을 가속화하는 주제를 다룬다. 통상적인 사전-트레이닝 컨텍스트 크기가 128인 BERT-베이스-사이즈 모델의 경우(Devlin 등, 2018), 다른 모든 토큰에 대한 주의의 토큰당 추론 비용은 128-뉴런 피드포워드 네트워크 추론의 비용보다 약간 더 많은 것에 불과하다는 점에 주목한다. 따라서 주의 계층은 그대로 두고 피드포워드 네트워크를 호스팅하는 중간 계층에만 집중한다.\n' +
      '\n' +
      '비교 지점.BERT 기반 피드포워드 네트워크는 3072개의 뉴런으로 구성된다. 이것은 두 개의 전력에 가깝지 않기 때문에 UltraFastBERT의 설계에서 최대 깊이 11의 균형 이진 트리의 노드 수인 4095로 반올림한다. 이 참조 프레임에서 UltraFastBERT는 추론을 위해 3072개의 BERTbase 뉴런 중 1/256(0.04%)만을 사용한다. 그럼에도 불구하고 UltraFastBERT는 4095개의 뉴런으로 구성되어 있으므로 추론에 1/341(0.03%)의 뉴런을 사용한다.\n' +
      '\n' +
      '섹션 3.3의 다운스트림 작업에 대한 모델 성능을 보고할 때 완전성을 위해 3072-뉴런과 4095-뉴런 기준선을 모두 제공한다.\n' +
      '\n' +
      '왜 341배 속도 향상?밀도 행렬 곱셈은 계산 역사상 가장 최적화된 수학적 연산이다. 메모리, 칩, 명령어 세트 및 이를 가능한 한 빨리 실행하는 소프트웨어 루틴을 설계하는 데 엄청난 노력이 투입되었다. 이러한 많은 발전은 복잡성 때문이든 경쟁 우위 때문이든 기밀을 유지하고 강력하지만 제한적인 프로그래밍 인터페이스를 통해서만 최종 사용자에게 노출되었다.\n' +
      '\n' +
      '따라서 새로운 하드웨어가 필요 없음에도 불구하고 CMM을 구현하기 위해 높은 수준의 선형 대수 루틴을 결합하는 것에 의존해야 하므로 속도가 감소한다. 우리는 3절에서 이것에 대해 자세히 설명한다.\n' +
      '\n' +
      '재현성. 최고의 모델의 가중치를 공유합니다. CMM의 효율적인 PyTorch 또는 TensorFlow 구현을 제공하지 않지만, UltraFastBERT의 추론에 12개의 뉴런만 사용된다는 사실은 선택된 뉴런을 제외한 모든 뉴런의 출력을 마스킹하는 것만으로 검증될 수 있으며, 이에 대한 코드를 제공한다.\n' +
      '\n' +
      'Takeaways.\n' +
      '* 4095개의 뉴런이 있지만 추론을 위해 12개(0.03%)만 선택적으로 사용하는 BERT 유사 모델인 UltraFastBERT를 소개합니다.\n' +
      '* 표준 다운스트림 작업에 대해 UltraFastBERT를 세분화하고 BERT 피어와 동등한 성능을 발휘하는지 확인합니다.\n' +
      '* 빠른 피드포워드 네트워크 추론의 기초가 되는 조건부 행렬 곱셈의 순진한 구현을 제공합니다. 이를 통해 기존의 최적화된 조밀한 행렬 곱셈에 비해 78배의 속도 향상을 얻을 수 있음을 알 수 있었다.\n' +
      '* UltraFastBERT 및 간단한 FFF 구현에 의한 이미 상당한 속도 향상을 통해 언어 모델링에서 조건부 신경 실행의 상당한 잠재력을 보여줍니다.\n' +
      '\n' +
      '## 2 Model\n' +
      '\n' +
      '### Architecture\n' +
      '\n' +
      '우리의 건축적 출발점은 크라우메드BERT 건축(Geiping & Goldstein, 2023)인데, 이 건축은 중간층의 특성을 제외한 모든 글자에 적용된다. 그곳에서, crammedBERT 트랜스포머 인코더의 중간 계층들에 포함된 피드포워드 네트워크들은 패스트 피드포워드 네트워크들(Belcak & Wattenhofer, 2023)로 대체된다.\n' +
      '\n' +
      '우리는 원래의 빠른 피드포워드 네트워크들을 다음과 같이 단순하게 변경한다:\n' +
      '\n' +
      '1. _잎 노드와 비잎 노드 간의 모든 차이를 제거합니다._ 특히, 모든 노드에 걸쳐 동일한(GeLU) 활성화 함수를 사용하고, 모든 노드에 출력 가중치를 부여하고, 모든 출력 바이어스를 제거한다.\n' +
      '2. _잎크기를 1로 고정._\n' +
      '3. _여러 FFF 트리를 병렬로 허용_ 여러 FFF 트리가 중간 레이어 출력을 공동으로 계산할 수 있습니다. 이는 개별 트리의 출력을 합산하고 그 합을 중간층 출력으로 제시함으로써 달성된다.\n' +
      '\n' +
      '우리는 모델명, 즉 UltraFastBERT-\\(K\\)x\\(D\\)에 접미사를 추가하여 깊이 \\(D+1\\)의 트리가 있는 모델을 나타낸다. 추론 코드와의 일관성을 위해 에지가 없는 트리는 깊이 \\(0\\)을 가지므로 최대 깊이 \\(D\\)의 트리는 깊이 \\(D+1\\)을 갖는 것으로 간주한다. 폭 3072의 전통적인 피드포워드 층을 갖는 BERT-베이스 크기의 모델은 울트라패스트BERT의 특별한 경우, 즉 울트라패스트BERT-3072x0이다.\n' +
      '\n' +
      '가장 빠른 모델만 공유하지만 울트라패스트BERT-3072x0에서 시작하여 울트라패스트BERT-1536x1, 울트라패스트BERT-512x2 등으로 진행하는 점점 더 깊고 좁은 모델의 전체 범위를 훈련한다.\n' +
      '\n' +
      '### Training\n' +
      '\n' +
      '우리는 crammedBERT(Geiping & Goldstein, 2023)의 최종 훈련 절차, 즉 사전 훈련에서 중도탈락을 불가능하게 하는 것과 1 사이클 삼각 학습률 스케줄을 사용하는 것을 따른다. 기본적으로 모든 모델을 단일 A6000 GPU에서 1일 동안 훈련합니다. 최종 UltraFastBERT-1x11 길이 모델을 제외하고 약간 더 나은 다운스트림 성능을 위해 동일한 체제를 사용하여 2배 더 오래 훈련합니다.\n' +
      '\n' +
      '### Downstream Performance\n' +
      '\n' +
      '#### 2.3.1 Setup\n' +
      '\n' +
      '우리는 GLUE 벤치마크(Wang et al., 2018)의 RTE, MRPC, SST, STS-B, MNLI, QQP, QNLI, 및 CoLA 태스크에 대한 모든 UltraFastBERT 모델을 세분화하고 일관성을 위해 Geiping & Goldstein(2023)에서와 같이 평가 점수를 보고한다. 간단히 말해서, 이 접근법은 모든 작업에 걸쳐 학습률 \\(4\\times 10^{-5}\\)을 갖는 5 에폭에 대한 피니튜닝에 해당한다.\n' +
      '\n' +
      '우리는 CoLA에 대해 이러한 방식으로 미세 조정된 울트라패스트BERT 모델이 5개의 훈련 에폭만 사용되면 수행된다는 것을 발견했다. 따라서 본 논문에서는 CoLA 피니튜닝 에폭의 수를 15개로 확장하였으며, 이는 베이스라인 크라메드BERT 모델의 성능 향상을 거의 내지 전혀 가져오지 못하지만 UltraFastBERT의 CoLA 성능에 큰 영향을 미친다.\n' +
      '\n' +
      '#### 2.3.2 Results\n' +
      '\n' +
      '우리의 피니튜닝 결과는 표 1에 나열되어 있다.\n' +
      '\n' +
      '우리는 단일 A6000 GPU 상에서 1일 동안 트레이닝된 UltraFastBERT 변이체들이 모두 원래의 BERT-베이스 모델의 GLUE 다운스트림 예측 성능의 적어도 96.0%를 유지한다는 것을 안다(Devlin 등, 2018). 또한 FFF의 깊이가 증가함에 따라 성능이 감소하는 것을 관찰합니다. 그러나, 깊이 증가로 인한 성능 감소의 대부분은 단일 태스크 - CoLA에 의해서만 발생한다는 점에 유의한다. 이 행동은 이전에 문헌에서 관찰되었으며 BERT 행동을 더 작은 모델로 압축하려는 다른 작업과 일치한다(Sun et al., 2019; Turc et al., 2019; Mukherjee et al., 2021). CoLA를 무시하면 모든 UltraFastBERT 모델에 의해 예측 성능의 최소 98.6%가 보존된다.\n' +
      '\n' +
      '또한, 가장 좋은 모델인 UltraFastBERT-1x11-long CoLA로부터 절약이 원래의 BERT-base 모델과 동등하게 수행하면서 자체 뉴런의 0.3%만 사용한다는 것을 알 수 있으며, 이는 BERT-base 뉴런의 0.4%에 불과하다. 우리는 이 모델의 가중치를 공개합니다.\n' +
      '\n' +
      '## 3 Inference\n' +
      '\n' +
      '위 부분의 목적이 추론당 매우 적은 수의 뉴런만 필요하다는 발견을 보고하는 것이었다면, 엔지니어링 관점을 채택하고 구현 전선에서 이를 어떻게 활용할 수 있는지 개략적으로 설명하는 것이 이 섹션의 목표이다.\n' +
      '\n' +
      '대규모 언어 모델의 일부로서 빠른 피드포워드 네트워크는 엄청난 가속 가능성을 가지고 있다. 기대할 수 있는 스피드업 야구장의 종류를 나타내기 위해, GPT-3(Brown et al., 2020)을 가져가라, 첫 번째 대형 언어 모델은 그것의 산출물의 타당성에 대해 널리 칭찬받았다. GPT-3의 각 변압기 층의 피드포워드 네트워크는 49152개의 뉴런으로 구성된다. 훈련가능하다면, 이 네트워크는 최대 깊이 15의 빠른 피드포워드 네트워크로 대체될 수 있으며, 이는,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c|c c c c c c c|c|c} \\hline \\hline Model & \\(N_{\\text{T}}\\) & \\(N_{\\text{I}}/N_{\\text{T}}\\) & RTE & MRPC & STSB & SST-2 & MNLI & QNLI & QQP & Avg & CoLA & Avg \\\\ \\hline \\multicolumn{10}{l}{**Baselines**} \\\\ \\hline crammedBERT-3072 & 4095 & 100.0\\% & 58.8 & 87.6 & 85.2 & 91.9 & 82.8 & 90.4 & 89.0 & 83.6 & 45.0 & 79.3 \\\\ crammedBERT-4095 & 3072 & 100.0\\% & 57.6 & 89.1 & 85.9 & 91.9 & 81.3 & 90.9 & 87.6 & 83.2 & 47.9 & 79.3 \\\\ \\hline \\multicolumn{10}{l}{**UltraFastBERTs**} \\\\ \\hline UltraFastBERT-3072x0 & 3072 & 100.0\\% & 56.7 & 88.9 & 86.3 & 92.3 & **82.9** & **92.3** & 88.0 & **83.8** & **48.4** & **79.9** \\\\ UltraFastBERT-1536x1 & 4608 & 66.6\\% & 55.2 & **89.4** & 85.0 & 91.9 & 82.2 & 90.1 & 89.0 & 83.1 & 47.5 & 79.2 \\\\ UltraFastBERT-512x2 & 3584 & 42.9\\% & 59.2 & 87.7 & 86.0 & 89.9 & 81.9 & 90.3 & 89.3 & 83.3 & 46.2 & 79.2 \\\\ UltraFastBERT-256x3 & 3840 & 26.7\\% & 54.2 & 87.4 & 85.9 & 91.6 & 81.6 & 90.0 & 89.1 & 82.7 & 48.0 & 78.8 \\\\ UltraFastBERT-128x4 & 3968 & 16.1\\% & 58.4 & 87.5 & **87.2** & **92.3** & 81.2 & 89.9 & **90.0** & 83.5 & 45.9 & 79.3 \\\\ UltraFastBERT-64x5 & 4032 & 9.5\\% & 55.7 & 89.0 & **87.2** & 91.4 & 81.6 & 90.2 & 89.4 & 83.3 & 46.1 & 79.1 \\\\ UltraFastBERT-32x6 & 4064 & 5.5\\% & 57.6 & 88.2 & 86.1 & 91.2 & 81.0 & 89.2 & 88.3 & 82.8 & 40.6 & 78.1 \\\\ UltraFastBERT-16x7 & 4080 & 3.1\\% & 55.5 & 89.0 & 86.7 & 88.9 & 80.1 & 89.4 & 86.9 & 82.1 & 41.5 & 77.6 \\\\ UltraFastBERT-8x8 & 4088 & 1.8\\% & 56.2 & 88.4 & 85.4 & 88.7 & 80.6 & 89.3 & 86.4 & 81.9 & 32.7 & 76.5 \\\\ UltraFastBERT-4x9 & 4092 & 1.0\\% & 53.8 & 85.9 & 85.7 & 89.6 & 81.9 & 89.3 & 88.0 & 82.0 & 31.8 & 76.4 \\\\ UltraFastBERT-2x10 & 4094 & 0.5\\% & **59.9** & 88.8 & 85.3 & 87.4 & 79.9 & 89.2 & 86.1 & 82.0 & 35.4 & 76.9 \\\\ UltraFastBERT-1x11 & **4095** & **0.3\\%** & 57.8 & 88.1 & 86.1 & 89.7 & 80.2 & 89.3 & 87.1 & 82.3 & 37.1 & 77.3 \\\\ \\hline \\multicolumn{10}{l}{**Final Model**} \\\\ \\hline UltraFastBERT-1x11-long & 4095 & 0.3\\% & 60.7 & 87.5 & 86.4 & 89.9 & 81.3 & 89.7 & 87.6 & 83.0 & 35.1 & 77.7 \\\\ \\hline \\multicolumn{10}{l}{**External Baselines**} \\\\ \\hline OpenAI GPT & 3072 & 100\\% & 56.0 & 82.3 & 80.0 & 91.3 & 81.4 & 87.4 & 70.3 & 78.8 & 45.4 & 75.1 \\\\ DistilBERT & 3072 & 100\\% & 59.9 & 87.5 & 86.9 & 91.3 & 82.2 & 89.2 & 71.3 & 81.2 & 52.1 & 77.6 \\\\ BERT-base & 3072 & 100\\% & 66.4 & 88.9 & 85.8 & 93.5 & 83.4 & 90.5 & 71.2 & 83.0 & 51.3 & 79.6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: GLUE-dev 테스트 세트에 대한 다양한 언어 모델의 결과. \\ (N_{\\text{T}}\\)은 훈련에 사용할 수 있는 뉴런의 수를 의미하며, \\(N_{\\text{I}}/N_{\\text{T}}\\)은 단일 추론에 사용되는 뉴런의 비율을 의미한다. "Avg"는 열의 왼쪽에 있는 모든 작업 결과의 평균 점수를 나타낸다. **강조** 는 지정된 열에 대해 가장 잘 채워진 1일 UltraFastBERT 성능을 나타냅니다. OpenAI GPT, DistilBERT, BERT-base는 Radford et al.(2018); Sanh et al.(2019); Devlin et al.(2018)에 보고된 모델을 의미한다.\n' +
      '\n' +
      '65536개의 뉴런을 포함하지만 추론에는 16개만 사용한다. 이것은 GPT-3의 뉴런의 약 **0.03%**에 해당한다.\n' +
      '\n' +
      '이 약속의 중심에는 조건부 행렬 곱셈의 연산이 있으며, 아래 슈도코드가 주어지고, 우리의 미래 노력은 효율적인 구현에 중점을 둔다.\n' +
      '\n' +
      '### Algorithm\n' +
      '\n' +
      'Belcak and Wattenhofer(2023)는 FFF 추론을 위한 재귀적 의사코드를 제공한다. CMM에 대한 의사 코드와 FFF에 대한 연속 추론을 섹션 2.1에 따라 수정한다. 알고리즘 1에서 \\(B\\)은 배치 크기, \\(H\\)은 레이어 입력 폭(변환기 은닉 차원), \\(2^{D}-1\\)은 뉴런 수, \\(M_{\\star,k},M_{l,\\star}\\)은 \\(k\\)번째 열 및 \\(l\\)번째 행을 각각 나타낸다. CMM에서 \\(>\\)-비교의 결과는 정수 \\(\\in\\{0,1\\}\\)으로 가정한다.\n' +
      '\n' +
      '### Compatibility\n' +
      '\n' +
      'CMM의 사용에 의해 도입된 조건성이 FFF가 조밀한 행렬 곱셈 및 딥 러닝을 위해 이미 제자리에 있는 프로세스 및 하드웨어와 더 광범위하게 호환되지 않도록 하는지 여부를 물어볼 수 있다. 간단히 말해서, 대답은 "아니요, 그렇지 않습니다. 캐싱 복잡성을 높이기 위해 절약하십시오."입니다.\n' +
      '\n' +
      '피드포워드 추론의 일부로서 단일 스레드 _CPU_ DMM은 곱셈 및 누적(MAC) 명령어의 순차적 실행에 의존한다. 이와 같이, CPU들, 특히 에지 CPU들은, 단순히 계층 출력을 계산하기 위해 엘리먼트별 MAC 명령들의 더 적은 실행들이 필요하기 때문에, UltraFastBERT에서 볼 수 있는 바와 같이 DMM을 CMM으로 대체함으로써 가장 쉽게 이익을 얻는다. 일반적으로 CPU 코드의 분기와 관련된 조건성의 명백한 사용에도 불구하고 CMM에서 볼 수 있는 "신경 분기"는 관련 포인터에 메모리 오프셋을 추가하는 것으로만 나타난다. 따라서 명령어 분기 예측은 CMM 조건성을 용이하게 하기 위해 결코 사용되지 않는다. 가중치에 대한 액세스 속도를 높이기 위해 가중치 캐싱을 최대한 활용하려면 CPU에 가중치 매트릭스의 관련 열만 로드하고 한 번에 하나씩만 로드하도록 힌트해야 할 수 있습니다. CMM은 행-열 내적을 계속 수행하기 때문에 벡터 단일 명령-다중 데이터(SIMD) 병렬 처리는 장치별 추론 구현 속도를 높이기 위한 실행 가능한 옵션으로 남아 있다.\n' +
      '\n' +
      '암묵적으로 다중 스레드된 _GPU_ DMM 계산은 매트릭스들의 상이한 패치들 상에서, 각각의 스레드에서 동일한 MAC 명령어들을 실행함으로써 현대 GPU들 뒤에 있는 단일 명령어-다중-스레드(SIMT) 접근법을 광범위하게 사용한다. 상기와 같이, 가중치 행렬들의 상이한 열들로 진행함으로써 표현되는 조건성은 사용되는 메모리에 대한 오프셋에만 영향을 미치고, MAC 명령들이 어느 경우, 또는 몇 번 실행되는지는 영향을 미치지 않기 때문에 이것은 CMM으로 쉽게 전달된다는 점에 유의한다. 그럼에도 불구하고, 효율적인 DMM 구현들은 분산 캐시의 사용을 최대화하는 방식으로 매트릭스 곱셈 워크로드(곱해질 매트릭스 패치들의 쌍들)를 분배하여, 캐시에 액세스하는 것보다 상당히 느린, 글로벌 디바이스 메모리에 대한 액세스가 제한된다. DMM 기준선에 대한 전체 잠재력을 달성하기 위해 CMM의 효율적인 구현\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|r r r r|r r r} \\hline \\hline  & & \\multicolumn{3}{c|}{CPU Implementation} & \\multicolumn{3}{c}{GPU Implementation} \\\\ \\hline Model & Limit & Level 1 & Level 2 & Level 3 & Native fused & Pytorch BMM & Naive CUDA \\\\ \\hline BERT-base-4095 & 1.00x & 1.00x & 1.00x & 1.00x & 1.00x & 1.00x & 1.00x \\\\ BERT-base-3072 & 1.33x & 1.55x & 1.74x & 1.39x & 1.33x & 1.61x & 1.82x \\\\ UltraFastBERT-1x11 & **341.25x** & **130.7** & **255.1** & - & - & **39.45x** & **117.83x** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 추론 가속도 평가의 결과. **강조** 는 최상의 "공정 비교" 성능을 강조 표시합니다.\n' +
      '\n' +
      '트리 횡단에 최적인 방식으로 캐싱을 명시적으로 관리하고 밀집 행렬 곱셈을 패치하지 않아야 한다. 이는 관련 하위 트리의 가중치를 항상 미리 로드하거나 DMM 패치 전략을 사용하지만 필요하지 않은 경우 패치 마진의 결과에서 중간 결과를 폐기함으로써 수행할 수 있다. 어느 쪽이든, 구현의 타겟 장치에 대한 친밀한(그리고 종종 비밀스러운) 지식 없이 이러한 최적화를 만드는 것은 여전히 도전 과제이다.\n' +
      '\n' +
      '### Inference Performance\n' +
      '\n' +
      '우리는 몇 가지 사용 가능한 FF/FFF 추론 구현의 속도를 비교한다.\n' +
      '\n' +
      '구현.CPU 추론을 위해 Intel oneAPI의 일부로 사용할 수 있는 Math Kernel 라이브러리를 사용합니다.\n' +
      '\n' +
      '* **레벨 1** 구현은 BLAS 레벨 1 루틴 및 BLAS 유사 레벨 1 확장, 즉 벡터-벡터 내적 및 스칼라-벡터 곱만을 사용하여 구성된 구현입니다.\n' +
      '* **레벨 2** 구현은 배치 된 BLAS Level 2 루틴과 BLAS 유사 Level 1 확장, 즉 배치 된 행렬-벡터 곱셈 및 배치 된 스칼라-벡터 곱을 사용 합니다.\n' +
      '* **레벨 3** 구현은 (배치되지 않은) BLAS 레벨 3 행렬-행렬 곱셈을 사용 합니다. 이것은 FF에 대한 가장 빠른 CPU 구현이지만 CMM의 벡터 수준 희소성이 라이브러리에서 지원되지 않기 때문에 FFF에 대해 현재 그러한 구현을 제공할 수 없다.\n' +
      '\n' +
      'GPU 구현을 위해 PyTorch 커널 또는 사용자 지정 CUDA 커널을 사용합니다.\n' +
      '\n' +
      '* **네이티브 융합** 구현은 네이티브 융합 피드포워드 계층 커널을 사용합니다. 이것은 FF 계층에 대한 가장 빠른 GPU 구현이지만 CMM의 특성으로 인해 FFF에 대해 현재 그러한 커널이 존재하지 않는다는 점에 유의한다.\n' +
      '* **BMM** 구현은 FF 및 FFF 모두에 대해 배치 된 행렬 곱셈 및 활성화 커널을 사용 합니다. FFF의 경우 조건성을 시뮬레이션하기 위해 트리 하강의 각 단계에서 벡터 복사를 광범위하게 사용한다.\n' +
      '* **Naive CUDA** 구현은 PyTorch 확장으로 실행되는 벡터/행렬 요소 수준에서 융합된 DMM/CMM 및 활성화를 수행하는 FF 및 FFF 모두에 대한 사용자 지정 CUDA 커널 코드입니다.\n' +
      '\n' +
      '방법론. CPU 추론을 위해 Intel MKL v2023.2.0에서 Intel(R) Core(TM) i7-6700HQ CPU에 대해 모든 루틴의 64비트 변형을 사용하여 엔트리당 250개의 순방향 패스를 수행한다. 우리는 표준 편차의 값이 항상 평균의 2% 아래에 있다는 점에 주목하여 단일 추론에 걸린 평균 시간을 보고한다. GPU 추론을 위해 CUDA v11.7 및 PyTorch 2.0.1에서 NVIDIA RTX A6000 GPU에 대해 엔트리당 1000회의 순방향 통과를 수행한다. GPU 시간을 측정하고 평균 소요 시간을 보고하며, 표준 편차는 모든 경우에 평균의 2% 미만이다. BERT 사전 훈련 컨텍스트 토큰 배치 크기와 동일한 배치 크기 \\(B=128\\times128\\)과 숨겨진 차원 \\(H=768\\)을 사용한다.\n' +
      '\n' +
      '결과.표 2는 BERT-베이스 및 UltraFastBERT-1x11에 나타나는 피드포워드 및 패스트 피드포워드 레이어의 성능 비교를 나열한다. 표의 각 열은 상대 추론 FFF-오버-FF 구현 속도 향상 _동일한 선형 대수 루틴 프리미티브를 사용할 때_를 나열한다.\n' +
      '\n' +
      '표 2가 누락된 두 항목은 현재 사용할 수 없는 BLAS 레벨 3 및 FFF의 네이티브 융합 구현에 대한 것이다.\n' +
      '\n' +
      '추가 비교.표 2에 보고된 모든 속도 향상은 "공정 비교"를 제공하며, 이는 각 경우에 FF 및 FFF 구현 모두 정확히 동일한 원시 선형 대수 연산을 사용했음을 의미한다. FF에 대한 구현이 FFF에서 사용할 수 없는 프리미티브를 사용하더라도 FFF의 최상의 구현이 현재 FF의 최상의 구현과 어떻게 공평한지 아는 데 관심이 있을 수도 있다. CPU에서 FFF의 Level 1 및 Level 2 구현은 각각 FF의 가장 빠른(Level 3) 구현보다 **48x 및 78x** 추론을 더 빠르게 수행합니다. GPU에서 FFF의 PyTorch BMM 구현은 FF의 가장 빠른(네이티브 융합) 구현보다 **3.15x** 속도 향상을 제공합니다.\n' +
      '\n' +
      '### Future outlook\n' +
      '\n' +
      'FFF 추론의 효율적인 구현을 시작하기 위한 광범위한 스트로크는 이미 PyTorch 라이브러리의 일부로 그려졌다. 하이브리드 벡터 수준 희소 텐서는 단수 및 배치 행렬 곱셈에 대해 완전히 지원된다면 알고리즘 1에서와 같이 CMM 및 FFF 추론을 구현하기에 충분할 것이다.\n' +
      '\n' +
      '장치별 Intel MKL/NVIDIA cuBLAS 코드의 일부로 CMM을 추가 기본 구현하면 341배 속도 향상을 약속할 수 있는 실제 가능성이 있다.\n' +
      '\n' +
      '## 4 Conclusion\n' +
      '\n' +
      '본 논문에서는 중간 계층에서 피드포워드 네트워크 대신 빠른 피드포워드를 사용하는 (crammed)BERT 구조의 수정된 버전인 UltraFastBERT를 제시한다. UltraFastBERT는 대규모 언어 모델이 개별 추론을 수행하기 위해 매개변수의 지수 분수에 참여하기만 하면 된다는 증거 역할을 한다. 최대 가속도를 갖는 가장 깊은 모델인 UltraFastBERT-1x11은 추론 동안 뉴런의 0.3%만을 사용하고 이미 해당 피드포워드 계층의 추론 시간 동안 78배 CPU 속도 향상을 달성한다. BERT 기반 모델의 규모에서 341x의 이론적 속도 향상 약속으로, 우리는 우리의 작업이 디바이스 프로그래밍 인터페이스의 일부로서 조건부 신경 실행을 위한 프리미티브를 구현하기 위한 노력을 고무할 수 있기를 바란다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Anil et al. (2023) Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. Palm 2 기술 보고서. _ arXiv preprint arXiv:2305.10403_, 2023.\n' +
      '* Belcak & Wattenhofer (2023) Belcak, P. and Wattenhofer, R. 빠른 피드포워드 네트워크입니다. _ arXiv preprint arXiv:2308.14711_, 2023.\n' +
      '* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. _ Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* Devlin 등(2018) Devlin, J., Chang, M. - W., Lee, K., and Toutanova, K. Bert: 언어 이해를 위한 깊은 양방향 트랜스포머의 사전 훈련. _ arXiv preprint arXiv:1810.04805_, 2018.\n' +
      '* Geiping & Goldstein (2023) Geiping, J. and Goldstein, T. 크래밍: 하루 만에 단일 gpu에서 언어 모델을 학습합니다. _International Conference on Machine Learning_, pp. 11117-11143. PMLR, 2023.\n' +
      '* Mukherjee et al. (2021) Mukherjee, S., Awadallah, A. H., and Gao, J. Xtremedistill-transformers: Task transfer for task-agnostic distillation. _ arXiv preprint arXiv:2106.04563_, 2021.\n' +
      '* Radford 등(2018) Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. Improving language understanding by generative pre-training. 2018.\n' +
      '* Sanh et al.(2019) Sanh, V., Debut, L., Chaumond, J., and Wolf, T. Distilbert, 증류된 버전의 bert: 더 작고, 빠르고, 저렴하고, 가볍습니다. _ arXiv preprint arXiv:1910.01108_, 2019.\n' +
      '* Sun et al. (2019) Sun, S., Cheng, Y., Gan, Z., and Liu, J. Patient knowledge distillation for bert model compression. _ arXiv preprint arXiv:1908.09355_, 2019.\n' +
      '* Turc 등(2019) Turc, I., Chang, M. - W., Lee, K., and Toutanova, K. 잘 읽은 학생들은 더 잘 배웁니다: 컴팩트 모델을 미리 훈련하는 것의 중요성에 대해서요. _ arXiv preprint arXiv:1908.08962_, 2019.\n' +
      '* Wang et al. (2018) Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Glue: A multi-task benchmark and analysis platform for natural language understanding. _ arXiv preprint arXiv:1804.07461_, 2018.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>