<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Switch Transformers: Simple and Efficient Sparity를 사용하여 Trillion 파라미터 모델로 스케일링\n' +
      '\n' +
      'William Fedus\n' +
      '\n' +
      '1. Switch Transformer용 JAX 코드 및 모든 모델 검사점은 [https://github.com/google-research/t5x](https://github.com/google-research/t5x)에서 사용할 수 있습니다.\n' +
      '\n' +
      '1. Jianfedus@google.com\n' +
      '\n' +
      'Barret Zoph\n' +
      '\n' +
      'barretzoph@google.com\n' +
      '\n' +
      'Noam Shazeer\n' +
      '\n' +
      'noam@google.com\n' +
      '\n' +
      '구글, 마운틴뷰, CA 94043, USA\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '딥 러닝에서, 모델들은 전형적으로 모든 입력들에 대해 동일한 파라미터들을 재사용한다. MoE (전문가 혼합) 모델은 이를 거부 하 고 대신 들어오는 각 예제에 대 한 _다른_ 매개 변수를 선택 합니다. 결과는 희소하게 활성화된 모델이며, 엄청난 수의 매개변수가 있지만, 일정한 계산 비용이 든다. 그러나 MoE의 몇 가지 주목할만한 성공에도 불구하고 광범위한 채택은 복잡성, 통신 비용 및 훈련 불안정으로 인해 방해를 받았다. 스위치 트랜스포머의 소개로 이 문제를 해결합니다. 우리는 MoE 라우팅 알고리즘을 단순화하고 통신 및 계산 비용이 절감된 직관적인 개선된 모델을 설계한다. 제안된 학습 기법은 불안정성을 완화하며, 큰 희소 모델들이 더 낮은 정밀도(bfloat16) 포맷으로 처음으로 학습될 수 있음을 보여준다. T5-Base와 T5-Large(Raffel et al., 2019)를 기반으로 한 모델을 설계하여 동일한 계산 자원으로 사전 훈련 속도를 최대 7배까지 향상시켰다. 이러한 개선 사항은 모든 101개 언어에 걸쳐 mT5 기반 버전에 대 한 이득을 측정 하는 다국어 설정으로 확장 됩니다. 마지막으로, "거시적 클린 크롤드 코퍼스"에 대해 최대 조 개의 파라미터 모델을 사전 훈련하여 언어 모델의 현재 규모를 개선하고 T5-XXL 모델보다 4배 빠른 속도를 달성한다.\n' +
      '\n' +
      '각주 1: 라이선스: CC-BY 4.0 [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)를 참조하세요. 귀인 요구 사항은 [http://jmlr.org/papers/v23/21-0998.html](http://jmlr.org/papers/v23/21-0998.html)에 제공 됩니다.\n' +
      '\n' +
      ' 전문가 혼합, 자연어 처리, 희소성, 대규모 기계 학습, 분산 컴퓨팅\n' +
      '###### Contents\n' +
      '\n' +
      '* 1 소개\n' +
      '* 2 스위치 변압기\n' +
      '	* 2.1 희소 라우팅 단순화\n' +
      '	* 2.2 효율적인 희소 라우팅\n' +
      '	* 2.3 Putting It All Together: Switch Transformer\n' +
      '	* 2.4 향상된 훈련 및 미세 조정 기법\n' +
      '* 3 스케일링 특성\n' +
      '	* 3.1 Step-Basis의 Scaling 결과\n' +
      '	* 3.2 Time-Basis에 대한 Scaling 결과\n' +
      '	* 3.3 Scaling Versus 더 큰 밀도 모델\n' +
      '* 4 다운스트림 결과\n' +
      '	* 4.1 Fine-Tuning\n' +
      '	* 4.2 증류\n' +
      '	* 4.3 다국어 학습\n' +
      '* 5 데이터, 모델 및 전문가-병렬성을 갖는 모델 설계\n' +
      '	* 5.1 데이터 병렬성\n' +
      '	* 5.2 모델 병렬성\n' +
      '	* 5.3 모델 및 데이터 병렬성\n' +
      '	* 5.4 전문가 및 데이터 병렬성\n' +
      '	* 5.5 전문가, 모델 및 데이터 병렬성\n' +
      '	* 5.6조 파라미터 모델 대비\n' +
      '* 6 관련 작업\n' +
      '* 7 토론\n' +
      '* 8 미래 작업\n' +
      '* 9 결론\n' +
      '* 주의 집중을 위한 스위치\n' +
      '* B _No-Token-Left-Behind_ 로 토큰 드롭 방지\n' +
      '* 전문가에 걸친 C 격려 탐색\n' +
      '* 낮은 계산 영역에서의 D 스위치 변압기\n' +
      '* E 업스트림과 다운스트림 모델 성능의 관계\n' +
      '* 스위치 변압기의 F 의사 코드\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대규모 훈련은 유연하고 강력한 신경 언어 모델을 향한 효과적인 경로였다(Radford et al., 2018; Kaplan et al., 2020; Brown et al., 2020). 넉넉한 계산 예산, 데이터 세트 크기 및 매개 변수 수에 힘입어 간단한 아키텍처가 더 복잡한 알고리즘을 통과합니다(서튼, 2019). Radford 등(2018); Raffel 등(2019); Brown 등(2020)은 조밀하게 활성화된 Transformer의 모델 크기를 확장한다(Vaswani 등, 2017). 효과적이기는 하지만, 또한 극도로 계산 집약적이다(Strubell et al., 2019). 모델 규모의 성공에서 영감을 얻었지만 더 큰 계산 효율성을 추구 하기 위해 대신 _sparsely-activated_ 전문가 모델인 스위치 변압기를 제안 합니다. 우리의 경우 희소성은 들어오는 각 예제에 대해 신경망 가중치의 _서브셋_ 을 활성화 하는 것에서 비롯 됩니다.\n' +
      '\n' +
      '희소 트레이닝은 연구 및 엔지니어링의 활성 영역(Gray et al., 2017; Gale et al., 2020)이지만, 오늘날 현재, 머신 러닝 라이브러리들 및 하드웨어 가속기들은 여전히 밀집 매트릭스 곱셈들을 수용한다. 효율적인 희소 알고리즘을 갖기 위해, Mixture-of-Expert(MoE) 패러다임(Jacobs et al., 1991; Jordan and Jacobs, 1994; Shazeer et al., 2017)으로 시작하고, 이를 단순화하여 트레이닝 안정성 및 계산 이점을 산출한다. MoE 모델은 기계 번역에서 주목할 만한 성공을 거두었지만(Shazeer et al., 2017, 2018; Lepikhin et al., 2020), 복잡성, 통신 비용 및 훈련 불안정으로 인해 광범위한 채택이 방해받고 있다.\n' +
      '\n' +
      '우리는 이러한 문제를 해결하고 번역을 넘어 이러한 종류의 알고리즘이 자연어에서 광범위하게 가치가 있음을 발견한다. 우리는 다양한 자연어 작업 집합과 NLP의 세 가지 체제(사전 훈련, 미세 조정 및 다중 작업 훈련)에 걸쳐 우수한 스케일링을 측정한다. 본 연구는 규모에 초점을 맞추고 있지만, 스위치 트랜스포머 구조가 슈퍼컴퓨터 분야에서 뛰어나다는 것을 보여준다.\n' +
      '\n' +
      '도 1: 스위치 변압기의 스케일링 및 샘플 효율. 좌측 플롯: 점점 희소해지는(더 많은 전문가) 스위치 변압기에 대한 스케일링 특성. Right Plot: Switch Transformers를 T5(Raffel et al., 2019) 모델과 비교하여 동일한 컴퓨팅 예산을 사용하는 Negative log perplexity.\n' +
      '\n' +
      '계산 코어가 몇 개 없어도 유용합니다. 또한, 우리의 큰 희소 모델은 희소 모델 품질 이득의 30%를 보존하면서 작은 조밀한 버전으로 증류될 수 있다(힌튼 등, 2015). 우리의 기여는 다음과 같습니다.\n' +
      '\n' +
      '* Experts의 혼합을 단순화하고 개선하는 Switch Transformer 아키텍처입니다.\n' +
      '* 토큰당 동일한 FLOPS를 사용하면서 7x+ 사전 학습 속도 향상을 측정하는 강력한 튜닝 T5 모델(Raffel et al., 2019)에 대한 스케일링 특성 및 벤치마크입니다. 또한 두 명의 전문가를 사용하여 제한된 계산 자원에도 개선이 있음을 보여준다.\n' +
      '* 희박 사전 훈련되고 전문화된 미세 조정 모델을 작은 조밀한 모델로 성공적으로 증류합니다. 모델 크기를 최대 99%까지 감소시키면서, 큰 희소 교사의 품질 이득의 30%를 보존한다.\n' +
      '* 향상된 사전 훈련 및 미세 조정 기술: **(1)** 더 낮은 bfloat16 정밀도로 훈련을 가능하게 하는 선택적 정밀 훈련 **(2)** 더 많은 수의 전문가로 확장할 수 있는 초기화 스킴 및 **(3)** 희소 모델 미세 조정 및 다중 작업 훈련을 개선 하는 전문가 정규화 증가입니다.\n' +
      '* 모든 101개 언어에 걸쳐 보편적인 개선을 발견하며 91%의 언어가 mT5 베이스라인에 걸쳐 4x+ 스피드업으로부터 혜택을 받는 다국어 데이터에 대한 사전 훈련 혜택의 측정(Xue 등, 2020).\n' +
      '* 데이터, 모델 및 전문가-병렬 처리를 효율적으로 결합하여 최대 1조 개의 매개 변수가 있는 모델을 생성함으로써 달성되는 신경망 언어 모델의 규모 증가. 이 모델은 강하게 조정된 T5-XXL 기준선의 사전 훈련 속도를 4배 향상시킵니다.\n' +
      '\n' +
      '## 2 Switch Transformer\n' +
      '\n' +
      '스위치 트랜스포머에 대한 안내 설계 원리는 간단하고 계산적으로 효율적인 방식으로 트랜스포머 모델(Vaswani 등, 2017)의 파라미터 카운트를 최대화하는 것이다. 스케일의 이점은 Kaplan et al.(2020)에서 완전하게 연구되었으며, 모델 크기, 데이터 세트 크기 및 계산 예산을 사용하여 멱법칙 스케일링을 밝혀냈다. 중요한 것은 이 작업은 계산 최적 접근법으로 상대적으로 적은 양의 데이터에 대해 대규모 모델을 훈련하는 것을 옹호한다는 것이다.\n' +
      '\n' +
      '이러한 결과를 통해 네 번째 축을 조사합니다. 예제당 부동 소수점 연산(FLOP)을 일정하게 유지하면서 _매개 변수 수_를 증가시킵니다. 우리의 가설은 수행된 총 계산과 무관한 매개변수 계수가 축척에 대한 별도로 중요한 축이라는 것이다. 이를 위해 GPU, TPU와 같은 밀집 행렬 곱셈을 위해 설계된 하드웨어를 효율적으로 사용하는 희소 활성화 모델을 설계한다. 여기서 우리의 작업은 TPU 아키텍처에 초점을 맞추지만, 이러한 클래스 모델은 GPU 클러스터에서 유사하게 훈련될 수 있다. 분산 학습 설정에서 희박하게 활성화된 계층은 다른 디바이스에서 _고유_ 가중치를 분할합니다. 따라서 모델의 가중치는 장치의 수에 따라 증가하며, 모든 장치는 관리 가능한 메모리와 계산 공간을 유지한다.\n' +
      '\n' +
      '### Sparse 라우팅 단순화\n' +
      '\n' +
      '**전문가 라우팅의 혼합물.** Shazeer 등 (2017)은 입력으로 토큰 표현 \\(x\\)을 취한 다음, \\(N\\) 전문가의 집합 \\(\\{E_{i}(x)\\}_{i=1}^{N}\\)에서 선택한 가장 잘 결정된 상위 \\(k\\) 전문가에게 라우팅하는 자연어 혼합 전문가 (MoE) 계층을 제안했습니다. 라우터 변수 \\(W_{r}\\)는 해당 계층에서 사용 가능한 \\(N\\) 전문가에 대해 소프트맥스 분포를 통해 정규화된 logits \\(h(x)=W_{r}\\cdot x\\)를 생성한다. 전문가 \\(i\\)에 대한 게이트-값은,\n' +
      '\n' +
      '\\[p_{i}(x)=\\frac{e^{h(x)_{i}}}{\\sum_{j}^{N}e^{h(x)_{j}}}. \\tag{1}\\]\n' +
      '\n' +
      'top-\\(k\\) 게이트 값은 토큰 \\(x\\)을 라우팅하기 위해 선택됩니다. \\(\\mathcal{T}\\)이 선택된 상위-\\(k\\) 인덱스들의 집합이라면, 레이어의 출력 계산은 게이트 값에 의해 토큰에 대한 각 전문가의 계산의 선형 가중 조합이고,\n' +
      '\n' +
      '\\[y=\\sum_{i\\in\\mathcal{T}}p_{i}(x)E_{i}(x). \\tag{2}\\]\n' +
      '\n' +
      '**Switch Routing: Mixed-of-Experts의 재고** Shazeer et al. (2017)는 \\(k>1\\) 전문가로 라우팅하는 것이 라우팅 함수에 대한 자명하지 않은 기울기를 가지기 위해 필요하다고 추측했습니다. 저자들은 적어도 두 명의 전문가를 비교할 수 있는 능력 없이는 경로를 배우는 것이 작동하지 않을 것이라고 직관했다. Ramachandran and Le(2018)는 더 나아가\n' +
      '\n' +
      '도 2: 스위치 트랜스포머 인코더 블록의 예시. 우리는 트랜스포머에 존재하는 조밀한 피드 포워드 네트워크(FFN) 층을 희소 스위치 FFN 층(가벼운 청색)으로 대체한다. 레이어는 시퀀스의 토큰 상에서 독립적으로 동작한다. 라우터가 각 토큰을 독립적으로 라우팅하는 4명의 FFN 전문가에 걸쳐 라우팅되는 두 개의 토큰(\\(x_{1}\\) = "더" 및 \\(x_{2}\\) = "매개 변수" 아래)을 다이어그램합니다. 스위치 FFN 레이어는 선택된 FFN의 출력에 라우터 게이트 값(점선)을 곱한 값을 반환한다.\n' +
      '\n' +
      '최상위\\(k\\) 결정을 연구하여 모델의 하위 계층에서 높은 \\(k\\)-값이 라우팅 계층이 많은 모델에서 중요하다는 것을 발견했다. 이러한 생각과 달리 우리는 대신 단순화된 전략을 사용하여 _단일_ 전문가로만 라우팅한다. 이 단순화는 모델 품질을 보존하고 라우팅 계산을 줄이며 더 나은 성능을 보여준다. 이 \\(k=1\\) 라우팅 전략을 나중에 스위치 계층이라고 합니다. MoE 및 스위치 라우팅 모두에 대해, 수학식 2의 게이트 값 \\(p_{i}(x)\\)은 라우터의 미분가능성을 허용한다는 점에 유의한다.\n' +
      '\n' +
      '스위치 계층의 이점은 3배입니다. **(1)** 토큰을 단일 전문가에게만 라우팅하므로 라우터 계산이 줄어듭니다. **(2)** 각 토큰이 단일 전문가로 라우팅되기 때문에 각 전문가의 배치 크기(전문가 용량)는 최소한 절반 이상 줄일 수 있습니다.\n' +
      '\n' +
      '각주 3: 기술 설명은 섹션 2.2를 참조하십시오.\n' +
      '\n' +
      '**(3)** 라우팅 구현이 단순화되고 통신 비용이 절감됩니다. 도 3은 상이한 전문가 용량 인자를 갖는 라우팅의 예를 도시한다.\n' +
      '\n' +
      '### 효율적인 Sparse 라우팅\n' +
      '\n' +
      '우리는 효율적인 분산 데이터 및 모델 병렬 아키텍처를 용이하게 하는 Tensorflow(Abadi et al., 2016)와 유사한 의미론 및 API를 갖는 라이브러리인 Mesh-Tensorflow(MTF)(Shazeer et al., 2018)를 사용한다. 코어들의 물리적 세트를 프로세서들의 논리적 메시로 추상화함으로써 그렇게 한다. 이어서, 텐서들 및 계산들은 명명된 차원들마다 샤딩되어, 차원들에 걸친 모델들의 용이한 분할을 용이하게 할 수 있다. 정적으로 선언된 크기가 필요한 TPU를 염두에 두고 모델을 설계합니다. 아래에서는 분산 스위치 변압기 구현에 대해 설명한다.\n' +
      '\n' +
      '도 3: 토큰 라우팅 다이내믹스의 예시. 각 전문가는 _용량 팩터_ 에 의해 변조 된 고정 일괄 처리 크기의 토큰을 처리 합니다. 각각의 토큰은 라우터 확률이 가장 높은 전문가에게 라우팅되지만, 각각의 전문가는 (total_tokens/num_experts) \\(\\times\\) capacity_factor의 고정된 배치 크기를 갖는다. 토큰이 고르지 않게 발송되면 특정 전문가가 오버플로(빨간색 점선으로 표시)되어 이러한 토큰이 이 계층에서 처리되지 않습니다. 더 큰 용량 요소는 이러한 오버플로 문제를 완화하지만 계산 및 통신 비용(패딩된 흰색/빈 슬롯으로 표시됨)도 증가합니다.\n' +
      '\n' +
      '**분산 스위치 구현.** 모든 텐서 모양은 컴파일 시간에 정적으로 결정되지만 학습 및 추론 시 라우팅 결정으로 인해 계산이 _동적_입니다. 이러한 이유로 한 가지 중요한 기술적 고려 사항은 _전문가 용량_ 을 설정하는 방법입니다. 전문가 용량(각 전문가가 계산 하는 토큰 수)은 배치의 토큰 수를 전문가 수에 걸쳐 균등하게 나눈 다음 _용량 인자_만큼 더 확장 하 여 설정 됩니다.\n' +
      '\n' +
      '\\[\\text{expert capacity}= \\left(\\frac{\\text{tokens per batch}}{\\text{number of experts}}\\right) \\times\\text{capacity factor}. \\tag{3}\\]\n' +
      '\n' +
      '1.0보다 큰 용량 팩터는 토큰이 전문가 간에 완벽하게 균형을 이루지 않을 때 사용할 추가 버퍼를 만듭니다. 너무 많은 토큰이 전문가로 라우팅되는 경우(나중에 드롭된 토큰이라고 함) 계산은 건너뛰고 토큰 표현은 잔차 연결을 통해 다음 계층으로 직접 전달됩니다. 그러나 높은 값은 계산 및 메모리를 낭비하기 때문에 전문가 용량을 늘리는 데 단점이 없는 것은 아닙니다. 이 트레이드오프는 그림 3에서 설명된다. 경험적으로 우리는 드롭된 토큰의 더 낮은 비율을 보장하는 것이 희소한 전문가 모델의 스케일링에 중요하다는 것을 발견한다. 실험을 통해 우리는 드롭된 토큰의 수에 대한 전문가 수에 대한 의존성을 알아차리지 못했다(일반적으로 \\(<1\\%\\). 충분한 계수를 갖는 보조 부하 밸런싱 손실(다음 섹션)을 사용하는 것은 양호한 부하 밸런싱을 보장하였다. 우리는 이러한 설계 결정이 모델 품질과 속도에 미치는 영향을 표 1에서 연구한다.\n' +
      '\n' +
      '**Differentiable Load Balancing Loss.** 전문가 간에 균형 잡힌 부하를 권장 하기 위해 보조 손실을 추가 합니다 (Shazeer et al., 2017, 2018; Lepikhin et al., 2020). Shazeer 등(2018); Lepikhin 등(2020)에서와 같이, Switch Transformers는 부하-균형 및 중요도-가중 손실이 분리된 Shazeer 등(2017)에서 원래의 설계를 단순화한다. 각 스위치 계층에 대해, 이 보조 손실은 트레이닝 동안 총 모델 손실에 추가된다. \\(N\\) 전문가가 \\(i=1\\)에서 \\(N\\)로 인덱싱되고 \\(T\\) 토큰이 있는 일괄 처리 \\(\\mathcal{B}\\)이 주어지면 보조 손실은 벡터 \\(f\\)와 \\(P\\) 사이의 조정된 점 곱으로 계산됩니다.\n' +
      '\n' +
      '\\[\\text{loss}=\\alpha\\cdot N\\cdot\\sum_{i=1}^{N}f_{i}\\cdot P_{i} \\tag{4}\\]\n' +
      '\n' +
      '여기서, \\(f_{i}\\)은 전문가 \\(i\\)에게 파견된 토큰의 분수이고,\n' +
      '\n' +
      '\\[f_{i}=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}\\mathbbm{1}\\left\\{\\text{argmax}\\:p(x)=i\\right\\} \\tag{5}\\]\n' +
      '\n' +
      '그리고 \\(P_{i}\\)은 전문가에게 할당된 라우터 확률의 비율 \\(i\\), 2\n' +
      '\n' +
      '각주 2: 혼란의 잠재적 원인: \\(p_{i}(x)\\)는 토큰이 전문가로 라우팅될 확률 \\(x\\) \\(i\\)입니다. \\ (P_{i}\\)는 Batch의 _모든 토큰_에 대한 전문가 \\(i\\)에 대한 확률 분율 \\(\\mathcal{B}\\)입니다.\n' +
      '\n' +
      '\\[P_{i}=\\frac{1}{T}\\sum_{x\\in\\mathcal{B}}p_{i}(x). \\tag{6}\\]\n' +
      '\n' +
      '우리는 \\(N\\) 전문가들에 걸쳐 토큰들의 배치에 대한 균일한 라우팅을 추구하기 때문에, 두 벡터들 모두 \\(1/N\\)의 값을 갖기를 원한다. 수학식 4의 보조 손실은 균일한 분포 하에서 최소화되기 때문에 균일한 라우팅을 장려한다. 목적은 또한 \\(P\\)-벡터는 미분 가능하지만 \\(f\\)-벡터는 미분할 수 없다. 최종 손실에 전문가 수 \\(N\\)을 곱하여 균일한 라우팅에서 \\(\\sum_{i=1}^{N}(f_{i}\\cdot P_{1})=\\sum_{i=1}^{N}(\\frac{1}{N}\\cdot\\frac{1}{N})= \\frac{1}{N}\\)로 전문가의 수가 달라짐에 따라 손실을 일정하게 유지한다. 마지막으로, 하이퍼-파라미터 \\(\\alpha\\)는 이러한 보조손실에 대한 곱셈 계수이며, 이 작업 전반에 걸쳐 1차 교차 엔트로피 목표를 압도하지 않을 만큼 충분히 크지만 로드 밸런싱을 보장하기에 충분히 작은 \\(\\alpha=10^{-2}\\)을 사용한다. 우리는 \\(10^{-1}\\)에서 \\(10^{-5}\\)의 초매개변수 범위를 10의 거듭제곱으로 스위핑하여 \\(10^{-2}\\)의 균형 부하를 훈련손실을 방해하지 않고 빠르게 찾았다.\n' +
      '\n' +
      '### Putting It All Together: Switch Transformer\n' +
      '\n' +
      '스위치 트랜스포머의 우리의 첫 번째 테스트는 (Raffel 등, 2019)에 소개된 "Colossal Clean Crawled Corpus"(C4)에 대한 사전 훈련으로 시작한다. 우리의 사전 트레이닝 목적을 위해, 우리는 모델이 누락된 토큰들을 예측하도록 트레이닝되는 마스킹된 언어 모델링 태스크(Taylor, 1953; Fedus et al., 2018; Devlin et al., 2018)를 사용한다. 우리의 사전 훈련 설정에서, Raffel 등(2019)에서 최적이라고 결정된 바와 같이, 우리는 15%의 토큰을 드롭아웃한 다음 마스킹된 시퀀스를 단일 센티넬 토큰으로 대체한다. 이 모델을 비교하기 위해 음의 로그 복잡도를 기록한다. 4 논문의 모든 테이블에서 \\(\\uparrow\\)은 해당 메트릭의 값이 더 좋음을 나타내고 \\(\\downarrow\\)은 그 반대임을 나타낸다. 이 작업에서 연구된 모든 모델의 비교는 표 9에 나와 있다.\n' +
      '\n' +
      '각주 4: 이 메트릭에 로그 베이스-\\(e\\)를 사용 하 여 단위가 nats입니다.\n' +
      '\n' +
      'Switch Transformer와 MoE Transformer의 정면 비교가 표 1에 제시된다. 우리의 Switch Transformer 모델은 \'T5-Base\'(Raffel et al., 2019)와 FLOP-매칭된다(토큰당 동일한 계산량이 적용된다). Top-2 라우팅을 사용하는 MoE Transformer는 각 토큰에 별도의 FFN을 적용하는 두 명의 전문가가 있어 FLOPS가 더 크다. 모든 모델은 동일한 하드웨어에서 동일한 수의 단계에 대해 훈련되었다. 용량 인자 2.0에서 1.25로 가는 MoE 모델은 상기 실험 설정에서 실제로 느려진다(840에서 790). 이는 예상치 못한 것이다.5\n' +
      '\n' +
      '각주 5: 속도 측정은 알고리즘 및 구현 세부사항의 기능이라는 점에 유의한다. 스위치 트랜스포머는 MoE(알고리즘)에 비해 필요한 계산을 감소시키지만, 최종 속도 차이는 낮은 수준의 최적화(구현)에 의해 영향을 받는다.\n' +
      '\n' +
      '표 1의 세 가지 주요 결과를 강조합니다. **(1)** 스위치 변압기는 속도 품질 기준으로 신중하게 조정된 조밀한 모델과 MoE 변압기 모두를 능가합니다. 고정 계산량과 벽시계 시간 동안 스위치 변압기가 최상의 결과를 얻습니다. **(2)** 스위치 변압기는 MoE 대응 장치보다 계산 면적이 작습니다. MoE Transformer의 훈련 속도와 일치하도록 크기를 증가시키면 모든 MoE 및 Dense 모델보다 스텝 단위로 성능이 향상됨을 알 수 있다. **(3)** 스위치 변압기는 낮은 용량 요인 (1.0, 1.25)에서 더 잘 수행 됩니다. 더 작은 전문가 능력은 모델 메모리가 매우 부족하고 용량 인자가 가능한 한 작게 만들어지기를 원할 큰 모델 체제에서의 시나리오를 나타낸다.\n' +
      '\n' +
      '### 향상된 훈련 및 Fine-Tuning 기술\n' +
      '\n' +
      '희소 전문가 모델은 바닐라 변압기에 대한 훈련 어려움을 도입할 수 있다. 이들 계층들 각각에서의 하드-스위칭(라우팅) 결정들 때문에 불안정성이 초래될 수 있다. 또한 bfloat16(Wang and Kanwar, 2019)과 같은 낮은 정밀도 포맷은 라우터의 소프트맥스 계산 문제를 악화시킬 수 있다. 우리는 여기서 훈련의 어려움과 안정적이고 확장 가능한 훈련을 달성하기 위해 이를 극복하기 위해 사용하는 방법을 설명한다.\n' +
      '\n' +
      '**큰 희소 모델을 사용 하는 선택적 정밀도.** 모델 불안정성은 효율적인 bfloat16 정밀도를 사용 하 여 훈련 하는 능력을 방해 하 고 결과적으로 Lepikhin et al. (2020)은 MoE Transformer 전체에서 float32 정밀도를 사용 하 여 훈련 합니다. 그러나 우리는 대신 모델의 국부화된 부분 내에서 float32 정밀도에 선택적으로 캐스팅함으로써 float32 텐서의 값비싼 통신 비용을 발생시키지 않고 안정성을 달성할 수 있음을 보여준다. 이 기술은 모델의 특정 부분 및 구배 업데이트가 더 높은 정밀도 Micikevicius 등(2017)에서 수행되는 현대의 혼합 정밀 훈련 전략과 일맥상통한다. 표 2는 우리의 접근법이 float32의 훈련 안정성을 부여하면서 bfloat16 훈련과 거의 동일한 속도를 허용한다는 것을 보여준다.\n' +
      '\n' +
      '이를 위해 라우터 입력을 float32 정밀도로 캐스팅한다. 라우터 함수는 토큰을 입력으로 받아 디스패치를 생성하고 전문가 계산의 선택 및 재조합에 사용되는 텐서를 결합한다(자세한 내용은 부록의 코드 블록 15 참조). 중요 하 게 float32 정밀도는 해당 장치에 로컬 된 계산에서 라우터 함수의 본문 내에서만 사용 됩니다. 결과적인 디스패치 및 결합 텐서는 기능 종료시 bfloat16 정밀도로 재전송되기 때문에, 비싼 float32 텐서는 없다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline Model & Capacity & Quality after & Time to Quality & Speed (\\(\\uparrow\\)) \\\\  & Factor & 100k steps (\\(\\uparrow\\)) & Threshold (\\(\\downarrow\\)) & (examples/sec) \\\\  & & (Neg. Log Perp.) & (hours) & \\\\ \\hline T5-Base & — & -1.731 & Not achieved\\({}^{\\dagger}\\) & 1600 \\\\ T5-Large & — & -1.550 & 131.1 & 470 \\\\ \\hline MoE-Base & 2.0 & -1.547 & 68.7 & 840 \\\\ Switch-Base & 2.0 & -1.554 & 72.8 & 860 \\\\ \\hline MoE-Base & 1.25 & -1.559 & 80.7 & 790 \\\\ Switch-Base & 1.25 & -1.553 & 65.0 & 910 \\\\ \\hline MoE-Base & 1.0 & -1.572 & 80.1 & 860 \\\\ Switch-Base & 1.0 & -1.561 & **62.8** & 1000 \\\\ Switch-Base+ & 1.0 & **-1.534** & 67.6 & 780 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 벤치마킹 스위치 대 MoE. MoE 변압기 및 T5 조밀한 베이스라인에 비해 스위치 변압기의 단계별 및 시간별 편익을 측정하는 헤드 투 헤드 비교. 우리는 Neg의 임의의 선택된 품질 임계값에 도달하는 시간과 음의 로그 복잡성으로 품질을 측정한다. Log Perp.= -1.50. 모든 MoE 및 스위치 변압기 모델은 128명의 전문가를 사용하며, 모든 다른 피드포워드 계층에 전문가가 있다. Switch-Base+의 경우 모델 은닉 크기를 768에서 896으로, 헤드 수를 14에서 16으로 증가시켜 MoE 모델의 속도와 일치할 때까지 모델 크기를 증가시킨다. 모든 모델은 동일한 계산량(32코어)과 동일한 하드웨어(TPUv3)로 학습된다. 또한 모든 모델은 -1.50의 수준 임계값을 달성하기 위해 100k 단계 이상의 사전 훈련이 필요했다. \\(\\dagger\\) T5-Base는 모델이 훈련된 100k 단계에서 이러한 부정적인 로그 복잡성을 달성하지 못했다.\n' +
      '\n' +
      '는 전체-대-전체 통신 동작들을 통해 브로드캐스트되지만, 우리는 여전히 float32의 증가된 안정성으로부터 이익을 얻는다.\n' +
      '\n' +
      '**안정성을 위한 Smalller 매개 변수 초기화**. 딥러닝에서 성공적인 학습을 위해서는 적절한 초기화가 매우 중요하며, 특히 스위치 변압기의 경우 이러한 초기화가 사실임을 알 수 있다. 평균 \\(\\mu=0\\)과 표준편차 \\(\\sigma=\\sqrt{s/n}\\)을 갖는 절단된 정규분포로부터 원소를 추출하여 가중치 행렬을 초기화한다. 여기서 \\(s\\)은 스케일 하이퍼-파라미터이고 \\(n\\)은 가중치 텐서(예: fan-in)의 입력 단위 수이다. 6\n' +
      '\n' +
      '각주 6: 평균에서 두 표준 편차보다 큰 값이 다시 샘플링됩니다.\n' +
      '\n' +
      '불안정성에 대한 추가적인 해결책으로 기본 변압기 초기화 스케일 \\(s=1.0\\)을 10배 줄이는 것이 좋다. 이 두 가지 방법은 모두 품질을 향상시키고 불안정 훈련 가능성을 줄인다. 표 3은 훈련 초기에 모델 품질의 개선과 분산의 감소를 측정한다.\n' +
      '\n' +
      '우리는 Neg에 의해 측정된 평균 모델 품질을 발견했다. 로그 퍼프는 극적으로 개선되었으며 실행 전반에 걸쳐 분산이 훨씬 감소했다. 또한, 이러한 동일한 초기화 방식은 여러 자릿수에 걸쳐 있는 모델에 대해 광범위하게 효과적이다. 우리는 동일한 접근법을 사용하여 223M 매개변수 기준선만큼 작은 모델을 1 tr을 초과하는 거대한 모델에 안정적으로 훈련한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Model & Quality & Speed \\\\ (precision) & (Neg. Log Perp.) (\\(\\uparrow\\)) & (Examples/sec) (\\(\\uparrow\\)) \\\\ \\hline Switch-Base (float32) & -1.718 & 1160 \\\\ Switch-Base (bfloat16) & -3.780 [_diverged_] & **1390** \\\\ Switch-Base (Selective precision) & **-1.716** & 1390 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 선택 정밀도. 우리는 bfloat16 정밀도 훈련과 거의 동일한 속도를 달성하면서 모델을 안정화하기 위해 다른 곳에서 bfloat16 정밀도를 유지하면서 로컬 라우팅 작업을 float32로 캐스팅한다. 속도 성능 훈련 초기에 고정된 스텝 카운트를 수행한 후 32개의 전문가 모델의 품질을 측정한다. float32의 스위치-베이스와 선택적 정밀도 모두에 대해 유사한 학습 역학이 눈에 띈다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Model (Initialization scale) & Average Quality & Std. Dev. of Quality \\\\  & (Neg. Log Perp.) & (Neg. Log Perp.) \\\\ \\hline Switch-Base (0.1x-init) & **-2.72** & **0.01** \\\\ Switch-Base (1.0x-init) & -3.60 & 0.68 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 감소된 초기화 스케일은 안정성을 향상시킨다. 초기화 규모를 줄이면 모델 품질이 향상되고 스위치 변압기가 안정적으로 훈련될 수 있습니다. 여기서는 3.5k 단계(각각 3개의 무작위 종자) 후에 32명의 전문가 모델의 음의 로그 복잡성으로 측정된 모델 품질의 평균 및 표준 편차를 기록한다.\n' +
      '\n' +
      '**큰 희소 모델을 정규화 합니다.* * 본 논문은 요약 또는 질문 응답과 같은 더 작은 다운스트림 작업에 대 한 미세 조정에 이어 큰 코퍼스에 대 한 사전 훈련의 일반적인 NLP 접근법을 고려 합니다. 많은 미세 조정 작업에는 예가 거의 없기 때문에 자연적으로 발생하는 한 가지 문제는 과적합이다. 표준 트랜스포머의 미세 조정 동안, Raffel 등(2019)은 과적합을 방지하기 위해 각 층에서 드롭아웃(Srivastava 등, 2014)을 사용한다. 우리의 스위치 트랜스포머는 FLOP에 매칭된 밀집 기준선보다 훨씬 더 많은 매개변수를 가지고 있으며, 이는 이러한 더 작은 다운스트림 작업에 더 심각한 과적합으로 이어질 수 있다.\n' +
      '\n' +
      '따라서 미세 조정 중에이 문제를 완화 하는 간단한 방법을 제안 합니다. 즉, 전문가 내부의 드롭아웃을 증가 하 고 _전문가 드롭아웃_ 이라고 합니다. 미세 조정 동안 우리는 각 전문가 계층에서 중간 피드포워드 계산에서만 드롭아웃 비율을 상당한 양만큼 증가시킨다. 표 4는 우리의 전문가 중도 탈락 프로토콜에 대한 결과를 가지고 있다. 우리는 모든 계층에서 드롭아웃을 단순히 증가시키는 것이 더 나쁜 성능으로 이어진다는 것을 관찰한다. 그러나 비전문가 계층에서 더 작은 드롭아웃 비율(0.1)과 전문가 계층에서 훨씬 더 큰 드롭아웃 비율(0.4)을 설정하면 4개의 더 작은 다운스트림 작업에 대한 성능 개선으로 이어진다.\n' +
      '\n' +
      '## 3 Scaling Properties\n' +
      '\n' +
      '본 논문에서는 사전 훈련 시 스위치 트랜스포머 아키텍처의 _scaling properties_에 대한 연구를 제시한다. Kaplan et al.(2020)에 따르면, 우리는 모델이 계산 예산이나 데이터 양에 의해 병목 현상이 발생하지 않는 체제를 고려한다. 데이터 병목 현상을 피하기 위해, 우리는 180B 이상의 타겟 토큰을 갖는 큰 C4 코퍼스를 사용하고(Raffel 등, 2019), 수익이 감소될 때까지 훈련한다.\n' +
      '\n' +
      '전문가 수는 모델을 확장하는 데 가장 효율적인 차원이다. 모델이 선택할 전문가의 수에 관계없이 토큰당 하나의 전문가만 선택하기 때문에 전문가를 늘리면 계산 비용이 대략적으로 고정된다. 라우터는 더 많은 전문가에 대한 확률 분포를 계산해야 하지만, 이것은 비용 \\(O(d_{model}\\times\\text{num experts})\\)의 경량 계산이며, \\(d_{model}\\)은 전문가의 임베딩 차원이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline Model (dropout) & GLUE & CNNDM & SQuAD & SuperGLUE \\\\ \\hline T5-Base (d=0.1) & 82.9 & **19.6** & 83.5 & 72.4 \\\\ Switch-Base (d=0.1) & 84.7 & 19.1 & **83.7** & **73.0** \\\\ Switch-Base (d=0.2) & 84.4 & 19.2 & **83.9** & **73.2** \\\\ Switch-Base (d=0.3) & 83.9 & 19.6 & 83.4 & 70.7 \\\\ Switch-Base (d=0.1, ed=0.4) & **85.2** & **19.6** & **83.7** & **73.0** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 미세 조정 정규화 결과. 스위치 트랜스포머 모델을 C4 데이터 세트의 34B 토큰에 미리 트레이닝하는 동안 드롭아웃 속도의 스윕(더 높은 수가 더 좋다)이다. 우리는 최상의 수행을 위해 전문가 피드포워드 계층에서 훨씬 더 큰 드롭아웃 비율과 함께 모든 비전문가 계층에서 더 낮은 표준 드롭아웃 비율을 사용하는 것을 관찰한다.\n' +
      '\n' +
      'tokens는 층 사이를 통과하였다. 본 절에서는 고정된 계산 예산을 갖는 단계 기반과 시간 기반에 대한 스케일링 특성을 고려한다.\n' +
      '\n' +
      '### 단계 기반에서 크기 조정 결과\n' +
      '\n' +
      '그림 4는 고정된 수의 단계에 대해 모든 모델을 훈련할 때 전문가 수에 따른 일관된 스케일링 이점을 보여준다. 토큰당 FLOPS를 고정 하 고 더 많은 매개 변수 (전문가)를 사용 하면 교육 속도가 빨라지는 명확한 추세를 관찰 합니다. 왼쪽 그림은 희소 모델 매개 변수와 테스트 손실 간의 일관성 있는 크기 조정 속성(토큰당 고정 FLOPS 포함)을 보여 줍니다. 이것은 희소 모델 파라미터들의 이러한 추가 축을 따라 스케일링의 이점을 드러낸다. 오른쪽 그림은 조밀한 모델 변형과 4개의 FLOP 일치 희소 변형의 샘플 효율성을 측정한다. 전문가 수를 늘리면 더 많은 표본 효율 모델이 생성된다는 것을 발견했다. 스위치-베이스 64 전문가 모델은 스텝 시간 측면에서 7.5배 속도 향상인 스텝 450k에서 스텝 60k에서 T5-베이스 모델과 동일한 성능을 달성한다. 또한, Kaplan 등(2020)의 발견과 일치하여, 더 큰 모델이 또한 고정된 수의 관찰된 토큰에 대해 더 빠르게 _샘플 효율_-학습된다는 것을 발견한다.\n' +
      '\n' +
      '도 4: 스위치 변압기의 스케일링 특성. 왼쪽 그림: 전문가 수를 스케일링하여 매개변수가 증가함에 따라 복잡성으로 측정한 품질 개선을 측정한다. 좌상단 지점은 223M 매개변수가 있는 T5-Base 모델에 해당한다. 왼쪽 위에서 오른쪽 아래로 이동하면 14.7B 매개변수가 있는 256명의 전문가 모델의 오른쪽 아래 지점까지 전문가의 수를 2, 4, 8등에서 두 배로 늘린다. 동일한 계산 예산을 사용하는 모든 모델에도 불구하고, 우리는 전문가의 수를 조정하는 일관된 개선을 관찰한다. 오른쪽 그림: 전문가 수를 훑는 단계당 네거티브 로그 복잡도입니다. 조밀한 기준선은 보라색 선으로 표시되며 스위치-베이스 모델의 개선된 샘플 효율에 주목한다.\n' +
      '\n' +
      '### Time-Basis에 대한 Scaling 결과\n' +
      '\n' +
      '그림 4는 스텝 단위로 전문가 수를 늘릴수록 성능이 지속적으로 향상됨을 보여준다. 토큰당 FLOPS의 양은 기준값과 거의 동일하지만, 스위치 트랜스포머는 라우팅 메커니즘의 추가 계산뿐만 아니라 장치 전반에 걸쳐 추가 통신 비용을 발생시킨다. 따라서 스텝 베이스에서 관찰된 증가된 샘플 효율이 벽 시계로 측정한 더 나은 모델 품질로 반드시 해석되는 것은 아니다. 이것은 질문을 제기한다:\n' +
      '\n' +
      '_고정 된 훈련 기간과 계산 예산에 대해 조밀한 모델 또는 희소한 모델을 훈련 해야 하나요?_\n' +
      '\n' +
      '그림 5와 6은 이 질문을 다룬다. 그림 5는 시간의 함수로 사전 훈련 모델 품질을 측정한다. 고정 트레이닝 기간과 계산 예산 동안, 스위치 트랜스포머는 상당한 속도 향상을 산출한다. 이 설정에서, 우리의 스위치-베이스 64 전문가 모델은 T5-베이스가 유사한 당혹감을 얻는 데 걸리는 시간을 _일곱 번째_로 훈련한다.\n' +
      '\n' +
      '### Scaling Versus 더 큰 밀도 모델\n' +
      '\n' +
      '위의 분석은 계산적으로 일치하는 조밀 모델이 스위치 대응물에 의해 앞서는 것을 보여준다. 그림 6은 다른 시나리오를 고려합니다. 대신 더 큰 밀집 모델에 자원을 할당했다면 어떨까요? 우리는 다음 강력한 기준선인 _T5-Large_에 대해 스위치 베이스를 측정하면서 지금 그렇게 한다. 그러나 T5-Large가 토큰당 3.5배 더 많은 FLOP를 적용함에도 불구하고,\n' +
      '\n' +
      '도 5 : 스위치 트랜스포머의 속도장점. 모든 모델은 예제당 동일한 FLOP를 사용하여 32개의 TPUv3 코어에 대해 훈련되었다. 고정된 계산량과 훈련 시간 동안 스위치 변압기는 조밀한 변압기 기준선을 훨씬 능가한다. 당사의 64개 전문가 스위치-베이스 모델은 T5-베이스의 _일곱 번째_ 시간에 동일한 품질을 달성하고 계속 개선됩니다.\n' +
      '\n' +
      '스위치-베이스는 여전히 더 많은 샘플 효율적이며 2.5배 속도 향상을 산출한다. 또한, T5-Large에 FLOP 매칭되는 새로운 더 큰 희소 버전인 Switch-Large를 설계하기만 하면 더 많은 이득을 얻을 수 있다. 우리는 이것을 하고 다음 섹션에서 우수한 스케일링 및 미세 조정을 보여준다.\n' +
      '\n' +
      '## 4 Downstream Results\n' +
      '\n' +
      '섹션 3은 사전 훈련 동안 우수한 스케일링 속성을 보여주었지만 이제 이러한 이득이 다운스트림 작업에 대한 향상된 언어 학습 능력으로 변환되는지 검증한다. 우리는 다양한 NLP 작업 세트를 미세 조정하는 것으로 시작한다. 다음으로, 우리는 작고 쉽게 배포되는 밀도가 높은 베이스라인으로 증류하여 희소 모델의 메모리 발자국을 90% 이상 줄이는 것을 연구한다. 마지막으로, 본 절에서는 다중 작업, 다국어 설정의 개선 사항을 측정하는 섹션을 마무리하고, 스위치 트랜스포머가 강력한 다중 작업 학습자임을 보여줌으로써 모든 101개 언어에 걸쳐 다국어 T5 기반 모델보다 개선된다.\n' +
      '\n' +
      '### Fine-Tuning\n' +
      '\n' +
      '**미세 조정에 사용되는 기준선 및 스위치 모델.** 우리의 기준선은 고도로 조정된 223M 매개 변수 T5-Base 모델과 739M 매개 변수 T5-Large 모델입니다 (Raffel et al., 2019). 두 버전 모두에 대해 FLOP 매칭 스위치 트랜스포머를 설계하며, 더 많은 매개변수가 표 9.7에 요약되어 있다. 우리의 기준선은 Raffel 등(2019)의 기준선과 약간 다르다. 왜냐하면 우리는 개선된 C4 코퍼스를 사전 훈련하여 예내 텍스트 복제를 제거하고 따라서 Lee 등으로서 사전 훈련 작업으로서의 효능을 증가시키기 때문이다.\n' +
      '\n' +
      '도 6: 스위치 레이어를 갖는 또는 표준 조밀 모델 스케일링을 갖는 스케일링 변압기 모델. 왼쪽 그림: 스위치 베이스는 토큰당 3.5배 더 많은 FLOPS를 적용하는 T5-Base 및 T5-Large 변형보다 샘플 효율성이 더 높다. 오른쪽 그림: 이전과 마찬가지로 벽시계 기준으로 스위치 베이스가 여전히 더 빠르고 T5-라지보다 2.5배 빠른 속도를 낸다는 것을 발견했습니다.\n' +
      '\n' +
      '(2021). 제안된 프로토콜에서는 총 576B 토큰에 해당하는 550k 단계에 대해 일괄 처리당 \\(2^{20}\\)(1,048,576) 토큰으로 사전 훈련한다. 그런 다음, 0.4의 드롭아웃 비율을 사용하는 스위치 계층을 제외한 모든 계층에 대해 0.1의 드롭아웃 비율을 사용하여 다양한 태스크 세트에 걸쳐 미세 조정한다(표 4 참조). 16k 단계에 대해 1M의 배치 크기를 사용하여 미세 조정하고 각 작업에 대해 200단계마다 모델 품질을 평가하고 검증 세트에서 계산된 최대 성능을 보고한다.\n' +
      '\n' +
      '**작업 및 데이터 세트를 미세 조정 합니다.* * 질문에 응답, 요약 및 세계에 대 한 지식을 포함 하 여 언어 기능을 프로 비전 하는 작업을 선택 합니다. 언어 벤치마크들 GLUE(Wang et al., 2018) 및 SuperGLUE(Wang et al., 2019)는 각각에 존재하는 토큰들의 양에 비례하여 혼합된 모든 태스크들을 갖는 복합 혼합물들로서 취급된다. 이러한 벤치마크는 감성 분석(SST-2), 단어 의미 중의성 해소(WIC), 문장 유사성(MRPC, STS-B, QQP), 자연어 추론(MNLI, QNLI, RTE, CB), 질의 응답(MultiRC, RECORD, BoolQ), 핵심 참조 해결(WNLI, WSC) 및 문장 완성(COPA) 및 문장 수용성(CoLA)으로 구성된다. CNNDM(Hermann et al., 2015) 및 BBC XSum(Narayan et al., 2018) 데이터 세트는 기사를 요약하는 능력을 측정하는 데 사용된다. 질문 응답은 SQuAD 데이터 세트(Rajpurkar et al., 2016) 및 ARC Reasoning Challenge(Clark et al., 2018)로 프로빙된다. 그리고 Roberts et al., (2020)과 같이 자연 질의(Natural Questions, Kwiatkowski et al., 2019), 웹 질의(Web Questions, Berant et al., 2013) 및 트리비아 질의(Trivia QA, Joshi et al., 2017)의 세 가지 폐쇄형 질의 응답 데이터 세트에 대해 미세 조정을 통해 모델의 지식을 평가한다. 폐쇄북은 보충 참조나 맥락 자료가 없는 질문을 말한다. 모델의 상식 추론을 측정하기 위해 Winogrande Schema Challenge(Sakaguchi et al., 2020)에서 평가한다. 그리고 마지막으로, Adversarial NLI Benchmark(Nie et al., 2019) 상에서 본 모델의 자연어 추론 능력을 테스트한다.\n' +
      '\n' +
      '**미세 조정 메트릭.** 다음 평가 메트릭이 논문 전체에서 사용 됩니다. GLUE 및 SuperGLUE에 대 한 모든 하위 작업에 대 한 평균 점수를 보고 합니다. 루즈-2 메트릭은 CNNDM과 XSum을 모두 사용한다. SQuAD 및 폐쇄된 책 과제(Web, Natural, 및 Trivia Questions)에서 본 측정의 추가 세부사항 및 결함에 대해 타겟과 정확히 일치하는 답변의 비율을 보고한다(Roberts 등(2020) 참조). 마지막으로 ARC Easy, ARC Challenge, ANLI 및 Winogrande에서 생성된 응답의 정확도를 보고한다.\n' +
      '\n' +
      '**미세 조정 결과.** 많은 자연어 작업에서 상당한 다운스트림 개선을 관찰합니다. 슈퍼GLUE에서 주목할 만한 개선 사항은 FLOP 일치 스위치 변종이 T5-Base 및 T5-Large 기준선보다 각각 4.4% 및 2% 포인트 향상되었음을 발견했으며 위노그란데, 폐쇄 책 트리비아 QA 및 XSum.8의 큰 개선뿐만 아니라 미세 조정 연구에서 이득을 관찰하지 못하는 유일한 작업은 AI2 추론 챌린지(ARC) 데이터 세트이며 T5-Base가 챌린지 데이터 세트에서는 Switch-Base보다 우수하고 T5-Large가 쉬운 데이터 세트에서는 Switch-Large보다 우수하다. 전체적으로 볼 때, 우리는 추론과 지식이 많은 과제 모두에 걸쳐 상당한 개선을 관찰한다. 이것은 우리의 아키텍처를 잘 미리 훈련하는 것이 아니라 미세 조정을 통해 품질 개선을 다운스트림 작업으로 변환할 수 있다는 것을 입증합니다.\n' +
      '\n' +
      '### Distillation\n' +
      '\n' +
      '수십억 또는 수조 개의 매개변수를 가진 거대한 신경망을 배포하는 것은 불편하다. 이를 완화하기 위해, 우리는 큰 희소 모델을 작은 조밀 모델로 증류하는 것을 연구한다(Hinton et al., 2015). 향후 작업은 큰 모델을 더 작은 _sparse_ 모델로 증류하는 것을 추가로 연구할 수 있다.\n' +
      '\n' +
      '**증류 기술.** 표 6에서 다양한 증류 기술을 연구합니다. 이러한 기술은 BERT 모델에 대한 증류 방법을 연구하는 Sanh 등(2019)에 의해 구축된다. 비전문가 가중치를 사용하여 조밀한 모델을 초기화하는 것이 적당한 개선을 가져온다는 것을 발견했다. 이는 모든 모델이 FLOP 매칭되기 때문에 가능하므로 비전문가 레이어는 동일한 치수를 가질 것이다. 전문가 계층은 일반적으로 트랜스포머의 모든 또는 모든 다른 FFN 계층에서만 추가되기 때문에, 이는 많은 가중치가 훈련된 파라미터로 초기화될 수 있게 한다. 또한, 교사 확률의 경우 0.25, 지상 진실 레이블의 경우 0.75의 혼합물을 사용하여 증류 개선을 관찰했다. 두 기법을 결합함으로써, 더 큰 희소 모델로부터 얻어지는 품질 이득의 약 30%를 보존하고, 단지 약 1/20({}^{th}\\)의 파라미터만을 보존한다. 품질 향상은\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline Model & GLUE & SQuAD & SuperGLUE & Winogrande (XL) \\\\ \\hline T5-Base & 84.3 & 85.5 & 75.1 & 66.6 \\\\ Switch-Base & **86.7** & **87.2** & **79.5** & **73.3** \\\\ T5-Large & 87.8 & 88.1 & 82.7 & 79.1 \\\\ Switch-Large & **88.5** & **88.6** & **84.7** & **83.0** \\\\ \\hline \\hline Model & XSum & ANLI (R3) & ARC Easy & ARC Chal. \\\\ \\hline T5-Base & 18.7 & 51.8 & 56.7 & **35.5** \\\\ Switch-Base & **20.3** & **54.0** & **61.3** & 32.8 \\\\ T5-Large & 20.9 & 56.6 & **68.8** & **35.5** \\\\ Switch-Large & **22.3** & **58.6** & 66.0 & **35.5** \\\\ \\hline \\hline Model & CB Web QA & CB Natural QA & CB Trivia QA & \\\\ \\hline T5-Base & 26.6 & 25.8 & 24.5 & \\\\ Switch-Base & **27.4** & **26.8** & **30.7** & \\\\ T5-Large & 27.7 & 27.6 & 29.5 & \\\\ Switch-Large & **31.3** & **29.5** & **36.9** & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 미세 조정 결과. 다양한 자연어 테스트 세트에 걸쳐 T5 기준선과 스위치 모델의 미세 조정 결과(검증 세트; 높은 수가 더 좋다). FLOP 매칭 스위치 모델을 T5-Base 및 T5-Large 베이스라인과 비교한다. 고려된 대부분의 작업들에 대해, 우리는 스위치-변수가 상당히 개선되었음을 발견한다. 우리는 모델 크기와 추론 및 지식 과중한 언어 작업 모두에 걸쳐 이득을 관찰한다.\n' +
      '\n' +
      '스위치 베이스(교사)와 T5 베이스(학생)의 품질 차이입니다. 따라서 100%의 품질 향상은 학생이 교사의 성과와 같다는 것을 의미한다.\n' +
      '\n' +
      '**가능한 압축률.** 표 6에 설명된 최상의 증류 기술을 사용하여 다양한 희소 모델을 조밀한 모델로 증류합니다. 우리는 1.1B에서 14.7B 사이의 다양한 매개변수에 해당하는 증가하는 수의 전문가를 휩쓸며 스위치 베이스 버전을 증류한다. 증류를 통해 1.1B 파라미터 모델의 품질 이득의 37%를 유지하면서 82%를 압축할 수 있다. 우리가 모델을 99% 압축하는 극단에서는 여전히 교사의 모델 품질 개선의 28%를 유지할 수 있다.\n' +
      '\n' +
      '**미세 조정 모델을 증류합니다.* * 미세 조정 희소 모델을 조밀한 모델로 증류하는 연구로 결론을 내립니다. 표 8은 SuperGLUE 태스크 상에서 미세 조정된 7.4B 파라미터 Switch-Base 모델을 223M T5-Base로 증류한 결과를 나타낸다. 사전 훈련 결과와 유사하게 FLOP 일치 밀도 변형으로 증류할 때 희소 모델의 이득의 30%를 보존할 수 있음을 발견했다. 여기에서 고려되지 않은 하나의 잠재적인 미래 방법은 작업을 미세 조정하고 더 나은 모델 압축을 달성하기 위해 추출하는 데 사용되는 특정 전문가를 조사할 수 있다.\n' +
      '\n' +
      '### Multilingual Learning\n' +
      '\n' +
      '최종 다운스트림 실험 세트에서 101개의 다른 언어를 혼합하여 사전 훈련하는 동안 모델 품질과 속도 절충을 측정한다. 우리는 T5로의 다국어 확장인 mT5(Xue 등, 2020)의 최근 작업을 구축하고 벤치마크한다. mT5에 소개된 101개 언어에 걸쳐 있는 공통 크롤 데이터 세트(mC4)의 다국어 변형에 대해 사전 훈련하지만, 특정 언어 내의 스크립트 변형으로 인해, 혼합물은 107개의 태스크를 포함한다.\n' +
      '\n' +
      '그림 7에서 우리는 FLOP 일치 스위치 모델인 mSwitch-Base에서 T5 염기 변형인 mT5-Base의 모든 언어에 대한 음의 로그 복잡도의 품질 개선을 플롯한다. 이후\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r} \\hline \\hline Technique & Parameters & Quality (\\(\\uparrow\\)) \\\\ \\hline T5-Base & 223M & -1.636 \\\\ Switch-Base & 3,800M & -1.444 \\\\ \\hline Distillation & 223M & (3\\%) -1.631 \\\\ + Init. non-expert weights from teacher & 223M & (20\\%) -1.598 \\\\ + 0.75 mix of hard and soft loss & 223M & (29\\%) -1.580 \\\\ \\hline Initialization Baseline (no distillation) & & \\\\ Init. non-expert weights from teacher & 223M & -1.639 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 언어 모델링을 위한 증류 스위치 변압기. Switch-Base에서 비전문가 가중치를 사용하여 T5-Base를 초기화하고 교사와 지상 진실 레이블의 혼합으로 인한 손실을 사용하면 최상의 성능을 얻을 수 있다. 우리는 100배 더 많은 매개변수를 가진 큰 희소 모델의 성능 개선의 30%를 작은 밀집 모델로 다시 증류할 수 있다. 최종 기준선의 경우 전문가 가중치로 초기화되었지만 증류 없이 정상적으로 훈련된 T5-베이스의 개선이 없음을 발견했다.\n' +
      '\n' +
      '두 버전을 1M 단계에 대해 사전 훈련하면 _모든_ 101개 언어가 고려된 경우 Switch Transformer가 기준선에 비해 최종 음의 로그 복잡성을 증가시킨다는 것을 알 수 있습니다. 도 8에서, 우리는 다른 뷰를 제시하고 이제 mT5-Base.9에 걸쳐 스위치 트랜스포머를 사용하는 단계별 _속도-향상_ 히스토그램을 제시한다. 우리는 5x의 mT5-Base에 걸쳐 평균 속도-향상 및 91%의 언어가 적어도 4x의 속도 향상을 달성한다는 것을 발견한다. 이것은 스위치 트랜스포머가 효과적인 다중 작업 및 다중 언어 학습자라는 증거를 제시한다.\n' +
      '\n' +
      '각주 9: 단계 단위의 속도 향상은 기준선에 대한 단계 수를 동일한 품질에 도달하기 위해 모델에 필요한 단계 수로 나눈 비율로 계산된다.\n' +
      '\n' +
      '## 5 데이터, 모델 및 Expert-Parallelism을 사용하여 모델 설계\n' +
      '\n' +
      '전문가의 수를 임의적으로 늘리는 것은 수익률 감소의 대상이 된다(그림 4). 여기서는 _보완적_ 크기 조정 전략을 설명 합니다. 트랜스포머를 스케일링하는 일반적인 방법은 \\(d_{model}\\) 또는 \\(d_{ff}\\)과 같이 직렬로 차원을 증가시키는 것이다. 이것은 두 파라미터 모두를 증가시킨다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c c c c} \\hline \\hline  & Dense & \\multicolumn{5}{c}{Sparse} \\\\ \\hline Parameters & 223M & 1.1B & 2.0B & 3.8B & 7.4B & 14.7B \\\\ \\hline Pre-trained Neg. Log Perp. (\\(\\uparrow\\)) & -1.636 & -1.505 & -1.474 & -1.444 & -1.432 & -1.427 \\\\ Distilled Neg. Log Perp. (\\(\\uparrow\\)) & — & -1.587 & -1.585 & -1.579 & -1.582 & -1.578 \\\\ Percent of Teacher Performance & — & 37\\% & 32\\% & 30 \\% & 27 \\% & 28 \\% \\\\ Compression Percent & — & 82 \\% & 90 \\% & 95 \\% & 97 \\% & 99 \\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 증류 압축률. 우리는 큰 희소 모델을 조밀한 기준선으로 증류할 때 품질을 측정한다. 우리의 기준선 T5-Base는 -1.636 Neg를 가지고 있다. 로그 퍼프 품질. 오른쪽 열에서 우리는 점점 더 큰 희소 모델을 동일한 아키텍처로 증류한다. 체중 초기화와 딱딱한 손실과 부드러운 손실의 조합을 통해 우리는 질적인 이득의 30%를 유지하면서 희박한 선생님을 95% 이상 줄일 수 있다. 그러나 훨씬 더 우수하고 더 큰 사전 훈련 교사를 위해서는 이러한 압축률을 달성하기 위해 더 큰 학생 모델이 필요할 것으로 예상한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c|c} \\hline \\hline Model & Parameters & FLOPS & SuperGLUE (\\(\\uparrow\\)) \\\\ \\hline T5-Base & 223M & 124B & 74.6 \\\\ Switch-Base & 7410M & 124B & 81.3 \\\\ Distilled T5-Base & 223M & 124B & (30\\%) 76.6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 미세 조정된 수퍼GLUE 모델을 증류하는 단계. SuperGLUE 태스크에서 미세 조정된 스위치-베이스 모델을 T5-베이스 모델로 증류한다. 우리는 더 작은 데이터 집합에서 우리의 큰 희소 모델이 증류에 효과적인 교사가 될 수 있음을 관찰한다. 우리는 97% 압축 모델에서 교사의 수행의 30%를 다시 달성한다는 것을 알게 된다.\n' +
      '\n' +
      '그리고 계산이 수행되고 궁극적으로 가속기당 메모리에 의해 제한된다. 가속기의 메모리 크기를 초과하면 단일 프로그램 다중 데이터(SPMD) 모델-병렬성을 사용할 수 있다. 이 섹션에서는 데이터, 모델 및 전문가-병렬성을 결합하는 트레이드오프를 연구한다.\n' +
      '\n' +
      '**FFN (피드 포워드 네트워크) 계층 검토** Mesh TensorFlow (Shazeer et al., 2018)에서 데이터, 모델 및 전문가 병렬 처리가 작동 하는 방법의 예로 FFN 계층을 사용 하 고 여기에서 간략하게 검토 합니다. 각 차원의 배치에서 \\(B\\) 토큰을 가정합니다.\n' +
      '\n' +
      '도 8: 101개 언어에 대한 다국어 사전 훈련. 각 언어에 대한 히스토그램, FLOP를 통한 스위치 변압기의 스텝 속도 향상은 동일한 품질에 도달하기 위해 T5 조밀한 기준선과 일치했다. 모든 101개 언어에 걸쳐 mT5-Base 5배속 평균 스텝 속도를 달성하고 91%의 언어에 대해 mT5-Base의 최종 복잡도에 도달하기 위해 4배속 이상의 속도를 기록한다.\n' +
      '\n' +
      '도 7: 101개 언어에 대한 다국어 사전 훈련. 101개 언어에 대한 다중 작업 교육 시 조밀한 기준선보다 스위치 T5 기준선 모델의 개선입니다. 스위치 트랜스포머를 관찰하여 다중 작업 훈련 설정을 상당히 잘 수행하고 모든 101개 언어에서 향상된 결과를 얻을 수 있습니다.\n' +
      '\n' +
      '\\(d_{model}\\) FFN의 입력(\\(x\\))과 출력(\\(y\\))은 모두 크기[\\(B\\), \\(d_{model}\\)]이고 중간(\\(h\\))은 크기[\\(B\\), \\(d_{ff}\\)]이며, 여기서 \\(d_{ff}\\)은 일반적으로 \\(d_{model}\\)보다 몇 배 크다. FFN에서 중간체는 \\(h=xW_{in}\\)이고, 층의 출력은 \\(y=ReLU(h)W_{out}\\)이다. 따라서 \\(W_{in}\\)과 \\(W_{out}\\)은 각 토큰에 독립적으로 적용되며 [\\(d_{model}\\), \\(d_{ff}\\)]와 [\\(d_{ff}\\), \\(d_{model}\\)]의 크기를 갖는다.\n' +
      '\n' +
      '분할의 두 가지 측면을 설명 합니다. _가중치_ 및 _데이터 배치_가 코어를 분할 하는 방법 (그림 9)에 표시 됩니다. 사용 가능한 모든 코어를 \\(N\\)으로 표시 합니다. 그러면 Mesh Tensorflow가 프로세서의 논리적 다차원 메시로 다시 매핑할 수 있습니다. 여기서 우리는 데이터-병렬 샤딩(\\(n\\))과 모델-병렬 샤딩(\\(m\\))의 방법의 수를 나타내는 한 차원의 2차원 논리 메쉬를 생성한다. 전체 코어는 데이터 및 모델 병렬성(예: \\(N=n\\times m\\)을 모두 분할하는 방법과 동일해야 합니다. 코어 간에 계층을 샤드하려면 \\(B\\) 토큰의 배치를 포함하는 텐서가 \\(n\\) 데이터 병렬 코어 간에 샤드되므로 각 코어에는 \\(B/n\\) 토큰이 포함됩니다. 그런 다음 \\(d_{ff}\\)이 있는 텐서와 변수는 \\(m\\) 모델 병렬 코어에 걸쳐 샤드됩니다. 전문가 계층이 있는 변형의 경우 각각 최대 \\(C \\) 토큰을 처리할 수 있는 \\(E \\) 전문가를 고려 합니다.\n' +
      '\n' +
      '### Data Parallelism\n' +
      '\n' +
      '분산 훈련의 표준인 데이터 병렬 모델을 훈련할 때 모든 코어는 데이터 병렬 차원 또는 \\(n=N,m=1\\)에 할당된다. 이것은 전체 순방향 및 역방향 패스가 종료되고 그래디언트들이 그 후 모든 코어들에 걸쳐 집성될 필요가 있을 때까지 통신이 필요하지 않다는 이점을 갖는다. 이는 그림 9의 맨 왼쪽 열에 해당한다.\n' +
      '\n' +
      '### Model Parallelism\n' +
      '\n' +
      '이제 모든 코어가 모델-병렬 차원에만 할당되어 \\(n=1,m=N\\)이 되는 시나리오를 고려한다. 이제 모든 코어는 전체 \\(B\\) 토큰을 유지해야 하며 각 코어에는 고유한 가중치 조각이 포함됩니다. 전후진 패스마다, 이제 통신 비용이 발생한다. 각 코어는 [\\(B\\), \\(d_{model}\\)] 텐서를 보내서 두 번째 행렬 곱셈 \\(ReLU(h)W_{out}\\)을 계산하는데 \\(d_{ff}\\) 차원이 분할되어 합이 되어야 하기 때문이다. 일반적으로 코어에 걸쳐 분할 된 차원을 합산 해야 할 때마다 순방향 및 역방향 통과 모두에 대 한 전체 감소 작업이 추가 됩니다. 이는 전-축소가 전체 전진 및 후진 통과의 끝에서만 발생하는 순수한 데이터 병렬성과 대조된다.\n' +
      '\n' +
      '### 모델 및 데이터 병렬 처리\n' +
      '\n' +
      '대규모 모델에 대해 모델과 데이터 병렬성을 모두 혼합하는 것이 일반적이며, 이는 가장 큰 T5 모델(Raffel et al., 2019; Xue et al., 2020) 및 GPT-3(Brown et al., 2020)에서 수행되었다. 총 \\(N=n\\times m\\) 코어를 사용 하 여 이제 각 코어는 가중치와 중간 활성화의 \\(B/n\\) 토큰 및 \\(d_{ff}/m\\)을 담당 합니다. 전진통과와 후진통과에서 각 코어는 전체 축소 연산에서 크기 \\([B/n,d_{model}]\\)의 텐서를 통신한다.\n' +
      '\n' +
      '도 9: 데이터 및 가중치 분할 전략. 각 4\\(\\times\\)4 점선 그리드는 16개의 코어를 나타내며 음영 처리된 사각형은 해당 코어에 포함된 데이터입니다(모델 가중치 또는 토큰 배치). 우리는 각 전략에 대해 모델 가중치와 데이터 텐서가 어떻게 분할되는지를 모두 설명한다. **첫 번째 행:** _모델 가중치_ 가 코어에 걸쳐 분할 되는 방법을 보여 줍니다. 이 행에서 서로 다른 크기의 모양은 FFN(Feed Forward Network) 계층에서 더 큰 가중치 행렬(예: 더 큰 \\(d_{ff}\\) 크기)을 나타낸다. 음영 처리된 사각형의 각 색상은 고유한 가중치 행렬을 식별합니다. 매개 변수 _코어당_ 수는 고정되지만 더 큰 가중치 행렬은 각 토큰에 더 많은 계산을 적용합니다. **두 번째 행:** _데이터 일괄 처리_가 코어 간에 분할되는 방법을 보여 줍니다. 각 코어에는 모든 전략에 걸쳐 고정된 메모리 사용량을 유지하는 동일한 수의 토큰이 있습니다. 분할 전략은 각 코어가 동일한 토큰을 가지거나 코어에 걸쳐 서로 다른 토큰을 가질 수 있도록 하는 서로 다른 속성을 가지며, 이는 서로 다른 색상이 상징하는 것이다.\n' +
      '\n' +
      '### 전문가 및 데이터 병렬 처리\n' +
      '\n' +
      '다음으로 전문가와 데이터 병렬성을 위한 분할 전략을 설명한다. 스위치 트랜스포머는 모든 코어를 데이터 분할 차원 \\(n\\)에 할당하며, 이는 또한 모델의 전문가 수에 해당한다. 코어당 각 토큰에 대해 라우터는 전문가에 대한 할당을 로컬로 계산합니다. 출력은 크기 [\\(n\\), \\(B/n\\), \\(E\\), \\(C\\)]의 이진 행렬로, 첫 번째 차원에 걸쳐 분할되고 전문가 할당을 결정한다. 이 이진 행렬은 [\\(n\\), \\(B/n\\), \\(d_{model}\\)]의 입력 텐서를 이용하여 행렬 곱셈을 통해 모음을 수행한다.\n' +
      '\n' +
      '\\[\\text{einsum}([n,B/n,d_{model}],[n,B/n,E,C],\\text{dimension}=[B/n]) \\tag{7}\\]\n' +
      '\n' +
      '최종 텐서 모양[\\(n\\), \\(E\\), \\(C\\), \\(d_{model}\\)]을 1차원에 걸쳐 음영처리한다. 각 코어는 자체 전문가를 가지고 있기 때문에 크기[\\(E\\), \\(C\\), \\(d_{model}\\)]의 전 대 전 통신을 수행하여 \\(n\\) 차원 대신 \\(E\\) 차원을 분할한다. 순방향 패스에는 서로 다른 코어에 위치한 각 전문가로부터 토큰을 아날로그 방식으로 수신하기 위해 bfloat16 텐서 크기 \\(E\\times C\\times d_{model}\\)의 추가 통신 비용이 발생한다. 전문가 분할 코드의 자세한 분석은 부록 F를 참조하십시오.\n' +
      '\n' +
      '### 전문가, 모델 및 데이터 병렬 처리\n' +
      '\n' +
      '최상의 모델 설계에서는 토큰당 FLOPS와 매개변수 카운트의 균형을 맞추려고 합니다. 전문가 수를 확장할 때 매개 변수의 수를 늘리지만 토큰당 FLOP를 변경하지 않습니다. FLOP를 증가시키기 위해서는 \\(d_{ff}\\) 차원도 증가시켜야 하는데, 이는 매개변수도 증가시키지만 속도는 느리다. 이는 \\(d_{ff}\\)을 증가시키면 코어당 메모리가 고갈되어 \\(m\\)을 증가시켜야 한다는 절충안을 제시한다. 그러나 고정된 코어의 수가 \\(N\\), \\(N=n\\times m\\)이기 때문에, 코어의 상수당 토큰을 유지하기 위해서는 더 작은 배치 크기를 사용해야 하는 \\(n\\)을 줄여야 한다.\n' +
      '\n' +
      '모델 병렬성과 전문가 병렬성을 결합할 때, 우리는 모델 병렬성에서 내부 전체 감소 통신과 함께 토큰을 올바른 전문가로 라우팅하는 데 따른 전체 대 전체 통신 비용을 갖게 된다. FLOPS의 균형, 코어당 통신 비용 및 메모리는 최상의 매핑이 경험적으로 결정되는 세 가지 방법을 모두 결합할 때 상당히 복잡해진다. 전문가 수가 다운스트림 성능에도 어떤 영향을 미치는지 섹션 5.6의 추가 분석을 참조하십시오.\n' +
      '\n' +
      '### 3조 매개 변수 모델에 대 한\n' +
      '\n' +
      '전문가, 모델 및 데이터 병렬성을 결합하여 각각 3,950억 개 및 1조 6,000억 개의 파라미터를 갖는 두 개의 대형 스위치 변압기 모델을 설계한다. 이 모델들이 언어 모델로서 업스트림 사전 훈련과 다운스트림 미세 조정 성능 모두에서 어떻게 수행되는지 연구한다. 두 모델의 파라미터, 시퀀스별 FLOP 및 하이퍼파라미터는 표 9에 다음과 같이 나열되어 있다. 변환기의 표준 하이퍼파라미터는 \\(d_{model}\\), \\(d_{ff}\\), \\(d_{kv}\\), 헤드의 수 및 레이어의 수를 포함하여 설명되며, 덜 일반적인 특징인 \\(FFN_{GEGLU}\\)은 확장 행렬이 비선형 결합된 두 세트의 가중치로 대체되는 FFN 레이어의 변형을 나타낸다(Shazeer, 2020).\n' +
      '\n' +
      '스위치-C 모델은 섹션 5.4에서 앞서 설명한 바와 같이 전문가-병렬성만을 사용하여 설계되고 모델-병렬성은 없다. 그 결과, 폭, 깊이, 헤드 수 등을 제어하는 하이퍼-파라미터는 모두 T5-XXL 모델보다 훨씬 작다. 대조적으로, 스위치-XXL은 T5-XXL 모델에 FLOP-매칭되며, 이는 하이퍼-파라미터들의 더 큰 차원들을 허용하지만, 모델-병렬성에 의해 유도된 추가적인 통신 비용을 희생시킨다(더 자세한 내용은 섹션 5.5 참조).\n' +
      '\n' +
      '**샘플 효율성 대 T5-XXL.** 표 9의 마지막 두 열에서 각각 250k 및 500k 단계 후에 C4 코퍼스에 음의 로그 복잡성을 기록합니다. 250k 단계 후에 T5-XXL 버전의 음의 로그 복잡성을 0.061.10 이상 개선하기 위해 스위치 변압기 변형을 모두 찾습니다. 0.061의 갭의 중요성을 맥락화하기 위해 T5-XXL 모델은 0.052를 증가 시키기 위해 _추가_ 250k 단계를 훈련 해야 했습니다. 스위치-XXL 모델은 T5-XXL을 0.087 x 500k 단계를 초과 수행 하 여 추가 훈련에 따라 갭이 계속 증가 합니다.\n' +
      '\n' +
      '각주 10: 이 보고된 품질 차이는 하한이며, 실제로 더 클 수 있다. T5-XXL은 예제 내에서 복제되어 쉽게 복사되는 코드 조각을 포함하는 더 쉬운 C4 데이터 세트에 대해 사전 훈련되었다.\n' +
      '\n' +
      '**트레이닝 불안정성.** 그러나 서론에서 설명한 대로 큰 희소 모델이 불안정할 수 있으며 규모를 늘리면 몇 가지 산발적인 문제가 발생합니다. 1.6T 파라미터와 2048명의 전문가가 포함된 더 큰 스위치-C 모델은 훈련 불안정성을 전혀 나타내지 않는다는 것을 발견했다. 대신 시퀀스당 거의 10배 더 큰 FLOP를 사용하는 스위치 XXL 버전은 때때로 불안정하다. 결과적으로, 이것은 단계 기반에서 우리의 더 나은 모델이지만, T5의 최종 보고된 결과와 인라인으로 전체 1M 단계에 대해 사전 훈련하지 않는다(Raffel 등, 2019).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c c c c} \\hline \\hline Model & Parameters & FLOPs/seq & \\(d_{\\text{mubd}}\\) & \\(FFN_{\\text{CEGLU}}\\) & \\(d_{ff}\\) & \\(d_{\\text{ks}}\\) & Num. Heads \\\\ \\hline T5-Base & 0.2B & 124B & 768 & ✓ & 2048 & 64 & 12 \\\\ T5-Large & 0.7B & 425B & 1024 & ✓ & 2816 & 64 & 16 \\\\ T5-XXL & 11B & 6.3T & 4096 & ✓ & 10240 & 64 & 64 \\\\ \\hline Switch-Base & 7B & 124B & 768 & ✓ & 2048 & 64 & 12 \\\\ Switch-Large & 26B & 425B & 1024 & ✓ & 2816 & 64 & 16 \\\\ Switch-XXL & 395B & 6.3T & 4096 & ✓ & 10240 & 64 & 64 \\\\ Switch-C & 1571B & 890B & 2080 & & 6144 & 64 & 32 \\\\ \\hline \\hline Model & Expert Freq. & Num. Layers & Num Experts & Neg. Log Perp. @250k & Neg. Log Perp. @ 500k & \\\\ \\hline T5-Base & – & 12 & – & -1.599 & -1.556 & \\\\ T5-Large & – & 24 & – & -1.402 & -1.350 & \\\\ T5-XXL & – & 24 & – & -1.147 & -1.095 & \\\\ \\hline Switch-Base & 1/2 & 12 & 128 & -1.370 & -1.306 & \\\\ Switch-Large & 1/2 & 24 & 128 & -1.248 & -1.177 & \\\\ Switch-XXL & 1/2 & 24 & 64 & -**1.086** & **-1.008** & \\\\ Switch-C & 1 & 15 & 2048 & -1.096 & -1.043 & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 스위치 모델 설계 및 사전 트레이닝 성능. T5 모델의 하이퍼 파라미터와 사전 훈련 성능을 스위치 변압기 변형과 비교한다. 마지막 두 열은 각각 250k 및 500k 단계 후에 C4 데이터 세트에 사전 훈련 모델 품질을 기록한다. 스위치-C 변압기 변형은 T5-XXL 모델보다 고정된 복잡도(동일한 계산 예산)에서 4배 더 빠르며 훈련이 진행됨에 따라 간격이 증가한다는 것을 관찰한다.\n' +
      '\n' +
      '**미세 조정 성능을 추론 합니다.* * 모델 품질에 대 한 예비 평가로 503B 토큰에 대해 부분적으로 미리 훈련 된 스위치-XXL 모델을 사용 하거나 T5-XXL 모델에서 사용 하는 텍스트의 약 절반을 사용 합니다. 이 체크포인트를 활용하여 개별적으로 미세조정된 작업이 아닌 모든 작업을 공동으로 학습하는 효율성을 위한 멀티 태스크 교육을 진행한다. 검증 세트에 대한 SQuAD 정확도가 89.7 대 최첨단 91.3으로 증가한다는 것을 발견한다. 다음으로, 평균 SuperGLUE 테스트 점수는 87.5 대 T5 버전으로 기록되고 최첨단 90.0과 비교하여 89.3의 점수를 획득한다(Wang et al., 2019). ANLI(Nie et al., 2019) 상에서, 스위치 XXL은 49.4의 이전 베스트 대비 65.7 정확도를 얻기 위해 이전 최신 기술에 비해 개선된다(Yang et al., 2020). 우리는 스위치-XXL이 최첨단 Neg를 가지고 있는 동안 주목한다. 로그 퍼프 업스트림 사전 훈련 작업에서 그 이득은 아직 SOTA 다운스트림 성능으로 완전히 변환되지 않았다. 우리는 부록 E에서 이 문제를 더 많이 연구한다.\n' +
      '\n' +
      '**지식 기반 미세 조정 성능.** 마지막으로 Salient Span Masking을 사용 하 여 추가 사전 교육 없이 자연 질문, WebQuestions 및 TriviaQA의 세 가지 닫힌 문서 지식 기반 작업으로 모델의 지식을 조기에 검사 합니다 (Guu et al., 2020). 세 가지 경우 모두에서 이전 최신 T5-XXL 모델(SSM 없음)에 비해 개선 사항을 관찰한다. 자연 질문의 정확한 일치도는 32.8의 이전 베스트에 비해 34.4로 증가하고 웹 질문은 37.2에 걸쳐 41.0으로 증가하고 트리비아QA는 47.5 대 42.9로 증가한다.\n' +
      '\n' +
      '요약하면, 다른 모델의 데이터의 절반 미만에 대한 훈련에도 불구하고, 우리는 이미 비교할 수 있고 때로는 최첨단 모델 품질을 발견한다. 현재, 스위치 트랜스포머는 상당한 업스트림 이득을 추론 태스크보다 지식 기반 태스크로 더 잘 변환한다(부록 E 참조). 대규모 전문가 모델에서 더 강력한 미세 조정 성능을 추출하는 것은 활발한 연구 문제이며 사전 훈련 복잡성은 향후 개선이 가능해야 함을 나타낸다.\n' +
      '\n' +
      '## 6 관련 작업\n' +
      '\n' +
      '신경망에서 척도의 중요성은 널리 인식되고 있으며 몇 가지 접근법이 제안되었다. 최근의 작업은 모델 병렬성(예를 들어, 다수의 코어에 걸쳐 가중치 및 텐서를 분할하는 것)을 사용하여 모델을 수십억 개의 파라미터로 스케일링하였다(Shazeer et al., 2018; Rajbhandari et al., 2019; Raffel et al., 2019; Brown et al., 2020; Shoeybi et al., 2019). 대안적으로, Harlap 등(2018); Huang 등(2019)은 파이프라인 기반 모델 병렬성을 사용하는 것을 제안하며, 여기서 상이한 계층은 디바이스에 걸쳐 분할되고 마이크로-배치는 상이한 계층에 _파이프라인드_된다. 마지막으로, 프로덕트 키 네트워크(Lample et al., 2019)는 주어진 계층에 들어오는 토큰 표현을 기반으로 학습 가능한 임베딩을 룩업하여 신경망의 용량을 확장하기 위해 제안되었다.\n' +
      '\n' +
      '우리 작업은 입력에 따라 계산 결정이 동적으로 수행되는 _조건부_ 계산을 수행하는 메서드 클래스의 특정 모델을 연구합니다. Cho와 Bengio (2014)는 모델 은닉 상태에서 발생하는 특정 비트 패턴에 따라 가중치를 적응적으로 선택하는 것을 제안했다. Eigen et al.(2013)은 고밀도 행렬 곱셈과 ReLU 활성화로 적층된 전문가 계층을 구축했으며 지터드 MNIST와 단조 음성에 대해 유망한 결과를 보여주었다. 컴퓨터 비전 Puigcerver 등(2020)에서는 업스트림 프리 트레이닝 시 시맨틱 클래스를 기반으로 토큰을 수동으로 라우팅한 후 다운스트림 태스크에 따라 사용할 관련 전문가를 선정한다.\n' +
      '\n' +
      'Mixture of Experts(MoE)는 현대 딥러닝 아키텍처의 맥락에서 Shazeer 등(2017)에서 효과적인 것으로 입증되었다. 그 작업은 LSTM(Hochreiter and Schmidhuber, 1997) 레이어 사이에 적층된 MoE 레이어를 추가했으며 토큰은 전문가 조합으로 별도로 라우팅되었다. 이로 인해 언어 모델링 및 기계 번역 벤치마크에서 최첨단 결과가 나왔다. Mesh Tensorflow 라이브러리(Shazeer et al., 2018)에 의해 MoE 층이 Transformer 아키텍처에 재도입되었지만, FFN 층의 대체물로 MoE 층이 도입되었지만, 수반되는 NLP 결과는 없었다. 보다 최근에, 기계 학습 인프라의 발전을 통해, XLA 컴파일러를 확장한 GShard(Lepikhin et al., 2020)는 MoE Transformer를 사용하여 100개 언어에 걸친 기계 번역을 획기적으로 개선하였다. 마지막으로, Fan 등(2021)은 모델 파라미터를 언어의 비중첩 그룹으로 분할하기 위해 상이한 결정론적 MoE 전략을 선택한다.\n' +
      '\n' +
      'Transformer_attention patterns_에서 시퀀스 길이 차원(\\(L\\))에 따른 희소성은 \\(O(L^{2})\\(Child et al., 2019; Correia et al., 2019; Sukhbaatar et al., 2019; Kitaev et al., 2020; Zaheer et al., 2020; Beltagy et al., 2020)에서 주의 복잡도를 줄이는 성공적인 기술이었다. 이것은 이전에 가능했던 것보다 더 긴 시퀀스를 학습할 수 있게 했다. 이 버전의 스위치 트랜스포머는 주의력 희소성을 사용하지 않지만 이러한 기술은 무료이며 향후 작업으로 긴 컨텍스트가 필요한 작업에 대한 학습을 잠재적으로 개선하기 위해 결합될 수 있다.\n' +
      '\n' +
      '## 7 Discussion\n' +
      '\n' +
      '스위치 트랜스포머에 대한 질문과 일반적으로 희소성이 주의 패턴이 아닌 가중치를 나타내는 희소 전문가 모델에 대해 포즈를 취하고 논의한다.\n' +
      '\n' +
      '**순수한 매개 변수 수로 인해 Switch Transformer가 더 좋지 않나요?* * 예, 디자인적으로요! 사용된 총 FLOP와 무관한 매개변수는 신경 언어 모델을 확장하는 데 유용한 축이다. 대형 모델들이 더 나은 성능을 발휘하는 것으로 철저하게 보여 왔다(Kaplan et al., 2020). 그러나 이 경우, 우리의 모델은 동일한 계산 자원을 사용하면서 더 효율적이고 더 빠르다.\n' +
      '\n' +
      '**슈퍼컴퓨터에 액세스할 수 없습니다. 여전히 유용합니까?* * 이 작업은 매우 큰 모델에 중점을 두었지만, 일반적으로 사용 가능한 GPU 또는 TPU(부록 D의 세부 정보)의 메모리 제약 조건 내에서 쉽게 적합하면서 2명의 전문가가 있는 모델이 성능을 향상시킨다는 것을 발견했습니다. 따라서 우리는 우리의 기술이 소규모 환경에서 유용하다고 믿는다.\n' +
      '\n' +
      '**희소 모델이 속도-정확도 Pareto 곡선에서 조밀한 모델보다 성능이 우수합니까?* * 예. 매우 다양한 모델 크기에 걸쳐, 희소 모델은 단계당 및 벽 시계 시간에서 조밀한 모델보다 성능이 우수하다. 제어된 실험은 고정된 계산량과 시간에 대해 희소 모델이 밀집 모델보다 성능이 우수함을 보여준다.\n' +
      '\n' +
      '**조 매개 변수 모델을 배포할 수 없습니다. 이러한 모델을 축소할 수 있습니까?** 모델 품질을 완전히 보존할 수는 없지만, 전문가 모델의 품질 이득의 \\(\\approx\\)30%를 달성하면서 희소 모델을 조밀한 모델로 증류하여 10~100배의 압축률을 달성할 수 있습니다.\n' +
      '\n' +
      '**모델-병렬 밀집 모델 대신 Switch Transformer를 사용 하는 이유?* * 시간 기준으로 Switch Transformer는 샤드 매개 변수가 있는 밀집 모델보다 훨씬 더 효율적일 수 있습니다 (그림 6). 또한 이 결정은 상호 배타적이지 않다는 점을 지적하고 스위치 트랜스포머에서 모델 병렬성을 사용하여 토큰당 FLOP를 증가시키지만 기존 모델 병렬성의 둔화를 발생시킨다.\n' +
      '\n' +
      '**희소 모델이 이미 널리 사용 되지 않습니까?* * 희소 모델을 시도 하려는 동기는 밀도 모델을 확장 하는 대규모 성공 (후커 (2020)에서 주장 된 대로 딥 러닝 하드웨어와의 공동 적응에 의해 부분적으로 주도 됨)으로 인해 방해 되었습니다. 또한, 희소 모델은 (1) 모델 복잡성, (2) 훈련 어려움, (3) 통신 비용을 포함한 여러 가지 문제가 있다. 스위치 트랜스포머는 이러한 문제를 완화하기 위해 발전합니다.\n' +
      '\n' +
      '## 8 Future Work\n' +
      '\n' +
      '본 논문은 간략화된 아키텍처, 개선된 훈련 절차 및 희소 모델이 어떻게 스케일링되는지에 대한 연구를 제시한다. 그러나 여기에서 간략하게 설명하는 많은 열린 미래 방향이 남아 있다.\n' +
      '\n' +
      '1. 상당한 도전은 가장 큰 모델에 대한 훈련 안정성을 더욱 향상시키는 것이다. 스위치-베이스, 스위치-라지 및 스위치-C 모델(불안정이 관찰되지 않음)에는 안정성 기술이 효과적이었지만 스위치-XXL에는 충분하지 않았다. 안정성을 개선하기 위한 정규화기 및 적응된 형태의 구배 클리핑을 사용하는 것을 포함하여 일반적으로 큰 모델에 유용할 수 있다고 생각하는 이러한 모델을 안정화하기 위한 초기 단계를 수행했지만 이는 여전히 해결되지 않은 상태로 남아 있다.\n' +
      '2. 일반적으로 향상된 사전 훈련 품질이 더 나은 다운스트림 결과(부록 E)로 이어지지만 간혹 현저한 이상 현상이 발생한다. 예를 들어, C4 데이터 세트를 모델링하는 유사한 복잡성에도 불구하고 1.6T 매개변수 스위치-C는 SQuAD에서 87.7의 정확한 일치 점수에 불과하며, 이는 더 작은 스위치-XXL 모델의 경우 89.6과 불리하게 비교된다. 한 가지 주목할 만한 차이점은 Switch-XXL 모델이 Switch-C 모델보다 토큰당 FLOPS를 \\(\\approx\\)10배 적용한다는 것이다. 비록 \\(\\approx\\)4배 적은 고유 파라미터(395B vs 1.6T). 이것은 미세 조정 품질, _토큰당 FLOPS_와 _매개 변수 수_ 사이의 잘 이해되지 않은 의존성을 시사한다.\n' +
      '3. 스케일링 관계에 대한 포괄적인 연구를 수행하여 데이터, 모델 및 전문가-병렬성을 블렌딩하는 아키텍처의 설계를 안내한다. 이상적으로는 하드웨어 구성(계산, 메모리, 통신)의 사양을 고려할 때 최적의 모델을 보다 신속하게 설계할 수 있습니다. 그리고, 그 반대의 경우도 마찬가지이며, 이는 향후 하드웨어의 설계에도 도움이 될 수 있다.\n' +
      '4. 본 연구는 적응형 계산 알고리즘 계열에 속한다. 우리의 접근법은 항상 동일하고 동질적인 전문가를 사용했지만 미래 설계(보다 유연한 인프라에 의해 촉진됨)는 이질적인 전문가를 지원할 수 있다. 이것은 더 많은 계산이 필요할 때 더 큰 전문가에게 라우팅함으로써 보다 유연한 적응을 가능하게 할 것이다.\n' +
      '5. 트랜스포머의 FFN 층 외부의 전문가 층을 조사한다. 우리는 이것이 유사하게 모델 품질을 향상시킬 수 있다는 예비 증거를 찾는다. 부록 A에서는 Q, K, V를 생성하는 가중치 행렬을 계층이 대체하는 Self-Attention 계층 내부에 이를 추가하여 품질 개선을 보고한다. 그러나 bfloat16 형식의 훈련 불안정으로 인해 대신 이를 향후 작업을 위한 영역으로 남겨둔다.\n' +
      '6. 스위치 트랜스포머를 새로운 양식과 다른 양식에 걸쳐 검사한다. 우리는 지금까지 언어만을 고려했지만 모델 희소성이 멀티모달 네트워크뿐만 아니라 새로운 모달리티에서도 유사하게 이점을 제공할 수 있다고 믿는다.\n' +
      '\n' +
      '이 목록은 쉽게 확장될 수 있지만, 이것이 우리가 생각하고 있는 도전의 유형과 미래의 유망한 방향에 대한 풍미를 제공하기를 바랍니다.\n' +
      '\n' +
      '## 9 Conclusion\n' +
      '\n' +
      '스위치 트랜스포머는 확장 가능하고 효과적인 자연어 학습자입니다. 전문가 혼합을 단순화하여 이해하기 쉽고 안정적으로 훈련할 수 있으며 동등 크기의 고밀도 모델보다 훨씬 더 효율적인 샘플을 생성할 수 있습니다. 우리는 이러한 모델이 사전 훈련, 미세 조정 및 다중 작업 훈련을 포함한 다양한 자연어 작업 집합과 다양한 훈련 체제에서 탁월하다는 것을 발견했다. 이러한 발전은 수천억에서 조밀한 T5 기준선에 비해 상당한 속도 향상을 달성하는 수천억에서 조밀한 매개변수를 가진 모델을 훈련하는 것을 가능하게 한다. 우리는 우리의 작업이 희박한 모델을 효과적인 아키텍처로 동기부여하고 이것이 연구자와 실무자들이 자연어 작업과 그 너머에서 이러한 유연한 모델을 고려하도록 장려하기를 바란다.\n' +
      '\n' +
      '저자들은 경험적 연구를 위한 알고리즘 개선 및 제안에 대한 몇 달간의 주요 통찰력을 제공한 마가렛 리에게 감사하고 싶다. 초안에 대한 의견을 조언하고 명확히 하는 휴고 라로셸, 상세한 의견과 신중한 수정은 이르완 벨로, 신경 언어 모델과 T5 코드 기반에 대한 시의적절한 조언은 콜린 래펠과 애덤 로버츠, 적응 계산 연구에 대한 조언과 격려는 요슈아 벵지오, 새로운 대규모 모델과 논문 수정을 안정화하기 위한 흥미로운 새로운 방향은 자샤 솔릭슈타인, 논문에 대한 유용한 토론은 구글 브레인 팀이다. 블레이크 헥트먼은 우리 모델들의 훈련 성과를 프로파일링하고 개선하는 데 귀중한 도움을 주었다.\n' +
      '\n' +
      '## Attention용 스위치 부록\n' +
      '\n' +
      'Shazeer 등(2018); Lepikhin 등(2020)은 Transformer의 고밀도 피드파워드 네트워크(FFN) 계산들에 MoE 층들을 추가하여 MoE Transformers(Shazeer 등, 2017)를 설계하였다. 마찬가지로, 우리의 작업도 트랜스포머의 FFN 층을 대체했지만 여기에서 대체 설계를 간략하게 탐구한다. 트랜스포머 _Self-Attention_ 계층에 스위치 계층을 추가합니다. 이를 위해 쿼리, 키 및 값을 생성하는 훈련 가능한 가중치 행렬을 그림 10에서 볼 수 있는 스위치 계층으로 대체한다.\n' +
      '\n' +
      '표 10은 여러 변형에 대한 훈련 시간뿐만 아니라 고정된 수의 단계 이후의 품질을 기록한다. 개선점을 찾았지만 bfloat16 정밀도를 사용할 때 이러한 층이 더 불안정하다는 것을 발견하여 최종 변형에 포함하지 않았다.\n' +
      '\n' +
      '그러나 이러한 층이 안정적으로 훈련될 때 예비 긍정적인 결과가 미래의 유망한 방향을 제시한다고 믿는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c} \\hline \\hline Model & Precision & Quality & Quality & Speed \\\\  & & @100k Steps (\\(\\uparrow\\)) & @16H (\\(\\uparrow\\)) & (ex/sec) (\\(\\uparrow\\)) \\\\ \\hline Experts FF & float32 & -1.548 & -1.614 & 1480 \\\\ Expert Attention & float32 & -1.524 & **-1.606** & 1330 \\\\ Expert Attention & bfloat16 & [diverges] & [diverges] & – \\\\ Experts FF + Attention & float32 & **-1.513** & -1.607 & 1240 \\\\ Expert FF + Attention & bfloat16 & [diverges] & [diverges] & – \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: 주목층 전환 결과. 모든 모델에는 32명의 전문가가 있으며 배치당 524k 토큰으로 훈련합니다. 전문가 FF는 전문가들이 논문 전반에 걸쳐 우리의 표준 설정인 트랜스포머에서 FFN을 교체하는 것이다. 전문가 FF + Attention은 FFN과 Self-Attention 계층을 모두 대체하는 데 전문가가 사용되는 경우이다. bfloat16 정밀도로 훈련할 때 전문가가 주목하는 모델은 다르다.\n' +
      '\n' +
      '도 10: 주목하여 층들을 스위칭한다. 스위치 레이어를 셀프 어텐션 변압기 블록에 통합하는 방법을 도식화합니다. 각 토큰 (여기서는 \\(x_{1}\\) = "더" 및 \\(x_{2}\\) = "매개 변수")에 대해 한 세트의 가중치가 쿼리를 생성 하 고 다른 세트의 고유 가중치가 공유 키와 값을 생성 합니다. 우리는 이 작업의 경우와 마찬가지로 각 전문가를 선형 작업과 FFN으로 실험했다. 이를 사용하여 품질 개선을 발견한 반면, 낮은 정밀도 번호 형식으로 사용할 때 더 불안정하므로 향후 작업을 위해 남겨둔다.\n' +
      '\n' +
      '## 부록 B _No-Token-Left-Behind_ 로 토큰 드롭 방지\n' +
      '\n' +
      'TPU 가속기의 소프트웨어 제약으로 인해 텐서의 모양은 정적으로 크기 조정되어야 합니다. 결과적으로 각 전문가는 토큰 표현을 처리할 수 있는 유한하고 고정된 용량을 가지고 있다. 그러나 이것은 전문가에 대한 불균등한 분포를 초래할 수 있는 런타임에 토큰을 동적으로 라우팅하는 모델에 대한 문제를 제시한다. 전문가에게 전송된 토큰의 수가 전문가 용량보다 적은 경우, 계산은 단순히 패딩될 수 있는데, 이는 하드웨어의 비효율적인 사용이지만 수학적으로 정확하다. 그러나, 전문가에게 전송된 토큰의 수가 그 용량보다 많을 때(전문가 오버플로우), 이를 처리하기 위한 프로토콜이 필요하다. Lepikhin et al. (2020)은 Mixture-of-Expert 모델을 적용하고, 또한 우리가 따르는 잔여 연결을 처리하지 않고 그 표현을 다음 레이어에 전달함으로써 전문가 오버플로우를 해결한다.\n' +
      '\n' +
      '우리는 토큰에 적용되는 계산이 없는 것이 매우 낭비적일 수 있다고 의심했는데, 특히 한 전문가에 오버플로우가 있는 경우 다른 전문가가 여분의 용량을 갖게 될 것이기 때문이다. 이 직관을 사용 하 여 _No-Token-Left-Behind_ 를 만듭니다. _No-Token-Left-Behind_ 는 처음에 오버플로 되는 전문가로 라우팅 되는 모든 토큰을 반복적으로 라우팅 합니다. 그림 11은 이 방법에 대한 그래픽 설명을 보여주며, 이를 통해 훈련 및 추론 중에 삭제되는 토큰이 거의 없음을 보장할 수 있다. 우리는 이것이 성과를 개선하고 훈련을 더욱 안정시킬 수 있다고 가정했지만 경험적 이점을 찾지 못했다. 우리는 네트워크가 서로 다른 토큰과 전문가 사이의 연관성을 학습하면 이 연관성이 변경되면(예: 토큰을 두 번째로 높은 전문가에게 전송) 성능이 저하될 수 있다고 의심한다.\n' +
      '\n' +
      '## Appendix C Encouraging Exploration Acversvers Experts\n' +
      '\n' +
      '각 전문가 계층에서 라우터는 토큰을 보낼 전문가를 결정합니다. 이것은 토큰의 표현에 대한 정보를 조건으로 하는 사용 가능한 전문가에 대한 별개의 결정이다. 들어오는 토큰 표현에 기초하여, 라우터는 최상의 전문가를 결정하지만, 대체 전문가를 선택하는 것이 얼마나 잘 이루어졌을지에 대한 반사실적 정보를 수신하지 못한다. 강화학습에서와 같이 고전적인 탐구-탐색 딜레마가 발생한다(Sutton and Barto, 2018). 이러한 문제는 Rosenbaum 등(2017)에 의해 유사하게 언급되고 다르게 다루어졌으며, 이는 다중 작업 학습에서 성공을 보여주었다. 이 특정 설정은 문맥적 도적(로빈스, 1952)의 설정과 가장 밀접하게 일치한다. 결정적으로 최고 전문가를 선택하는 것은 항상 착취적인 전략에 해당하며, 우리는 더 나은 전문가 할당을 찾기 위해 탐구의 균형을 맞추는 것을 고려한다.\n' +
      '\n' +
      '탐사를 도입하기 위해, 우리는 1) 결정론적 또는 아르그맥스 2) 소프트맥스 분포로부터 샘플링 3) 입력 표현에 대한 드롭아웃 4) 입력 표현에 대한 곱셈 지터 잡음과 같은 몇 가지 접근법을 고려한다. 결과적으로 모델 품질에 미치는 영향은 표 11에 보고되어 있다. 이 작업을 통해 우리는 입력 지터를 사용하여 잡음을 주입하는 것이 경험적으로 가장 잘 수행된다는 것을 발견했다.\n' +
      '\n' +
      '## 부록 D 스위치 변환기 하위 계산 영역에서\n' +
      '\n' +
      '스위치 트랜스포머는 또한 수천 개의 코어와 수조 개의 파라미터를 가진 레짐뿐만 아니라 작은 규모에서도 효과적인 아키텍처이다. 많은 이전 실험은 10B+ 매개변수 모델의 규모에 있었지만 그림 12에서 2명의 전문가가 FLOP와 일치하는 대응물에 대해 강력한 이득을 생성한다는 것을 보여준다. 슈퍼 컴퓨터를 쉽게 사용할 수 없더라도, (일반적으로 코어당 한 명의 전문가를 추천하기 때문에) 2, 4 또는 8명의 전문가로 스위치 변압기를 훈련시키면 T5 밀집 기준선에 비해 견고한 개선이 이루어진다.\n' +
      '\n' +
      '도 11: _No-Token-Left-Behind Routing_의 다이어그램. 1단계는 토큰이 라우터에서 가장 높은 확률로 전문가에게 라우팅되는 스위치 라우팅과 동일합니다. 2단계에서 우리는 넘친 모든 토큰을 살펴보고 두 번째로 높은 확률을 가진 전문가에게 전달한다. 토큰은 두 번째로 높은 전문가가 너무 많은 토큰을 가지고 있는 경우 여전히 오버플로우될 수 있지만, 이는 대부분의 토큰을 라우팅할 수 있게 한다. 이 프로세스는 사실상 어떠한 토큰도 전혀 드롭되지 않는다는 것을 보장하기 위해 반복될 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c} \\hline Model & Quality (Neg. Log Perp.) (\\(\\uparrow\\)) \\\\ \\hline Argmax & -1.471 \\\\ Sample softmax & -1.570 \\\\ Input dropout & -1.480 \\\\ Input jitter & **-1.468** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 11: 라우터 탐색 전략. 음수 로그 복잡성으로 측정된 스위치 변압기의 품질은 전문가를 선택하기 위한 다른 무작위성 전략(낮은 것이 더 좋다) 하에서 측정된다. 변종 간에 재료 속도 성능 차이가 없습니다.\n' +
      '\n' +
      '도 12: 전문가가 거의 없는 스위치 트랜스포머. 스위치 트랜스포머는 전문가가 거의 없어도 기준선보다 개선됩니다. 여기서는 2, 4, 8명의 전문가를 사용하여 T5-Base 모델보다 개선된 매우 작은 규모에서 스케일링 특성을 보여준다.\n' +
      '\n' +
      '## 부록 E 업스트림과 다운스트림 모델 성능 관계\n' +
      '\n' +
      '사전 훈련 목표에 대한 모델의 품질이 다운스트림 작업 결과로 변환된다는 보장은 없다. 그림 13은 평균 슈퍼GLUE 성능과 트리비아QA 점수의 두 가지 다운스트림 작업 측정값을 사용하여 C4 사전 훈련 작업에서 조밀 모델과 스위치 모델 모두에 대한 업스트림 모델 품질의 상관 관계를 보여준다. 우리는 이 두 가지 과제를 하나의 탐침으로 모델의 추론과 다른 사실적 지식을 선택한다.\n' +
      '\n' +
      '기준선과 스위치 모델 모두에 대해 개선된 사전 훈련이 더 나은 다운스트림 결과로 이어진다는 것을 나타내는 일관된 상관 관계를 찾는다. 또한, 고정된 업스트림 복잡도에 대해 스위치 모델과 조밀한 모델 모두 중소 모델 크기 영역에서 유사하게 수행됨을 발견했다. 그러나 가장 큰 모델 체제(T5-11B/T5-XXL)에서 섹션 5.6에서 언급한 바와 같이 우리의 가장 큰 스위치 모델은 항상 슈퍼GLUE 작업에서 업스트림 복잡성을 다운스트림 미세 조정으로 잘 변환하지 않는다. 이는 희소 모델의 잠재력을 완전히 실현하기 위한 향후 조사와 연구를 보증한다. 전문가 모델을 사용한 미세 조정 역학을 이해하는 것은 매우 복잡하며 정규화, 부하 균형 및 미세 조정 하이퍼 매개변수에 의존한다.\n' +
      '\n' +
      '그림 13: 상류 사전 훈련된 품질에서 하류 모델 품질로. SuperGLUE와 TriviaQA(SSM 없이 기록된 SOTA), 추론 및 지식 가중 벤치마크(검증 세트) 모두에서 업스트림 성능과 다운스트림 품질을 상관시킨다. 우리는 기준선과 마찬가지로 스위치 모델이 업스트림 사전 훈련 작업의 개선과 함께 확장된다는 것을 발견했다. SuperGLUE의 경우 음의 로그 복잡성과 평균 SuperGLUE 점수 사이의 느슨한 선형 관계를 찾는다. 그러나 고밀도 모델은 특히 대규모 체제에서 고정된 복잡성에 대해 종종 더 나은 성능을 보인다. 반대로 지식이 많은 태스크인 트리비아QA에서 스위치 트랜스포머가 개선된 스케일링 관계를 따를 수 있음을 발견한다. 이러한 관찰을 확인하기 위해서는 추가 통계(수집 비용이 많이 들고 향후 작업에 맡겨질 것)가 필요할 것이다.\n' +
      '\n' +
      '## Appendix F Pseudo Code for Switch Transformer\n' +
      '\n' +
      'Pseudocode for Switch Transformers in Mesh Tensorflow (Shazeer et al., 2018). 아래 코드에는 모델 병렬성이 사용되지 않습니다(자세한 내용은 5.4 참조).\n' +
      '\n' +
      '그림 14: Mesh 텐서플로우에서 스위치 변압기의 부하 균형 손실을 위한 의사 코드.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:34]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:35]\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Abadi 등(2016) Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: a system for large-scale machine learning. 12차 \\(\\{\\)USENIX\\(\\}\\) symposium on operating systems design and implementation (\\(\\{\\)OSDI\\(\\}\\) 16)_, pages 265-283, 2016.\n' +
      '* Beltagy 등(2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: Long-document transformer. _ arXiv preprint arXiv:2004.05150_, 2020.\n' +
      '* Berant 등(2013) Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 문답 쌍의 자유 베이스에 대한 의미 구문 분석입니다. _Proceedings of the 2013 Conference on empirical methods in natural language processing_, pages 1533-1544, 2013.\n' +
      '* Brown et al.(2020) Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _ arXiv preprint arXiv:2005.14165_, 2020.\n' +
      '* Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 희소 트랜스포머로 긴 시퀀스를 생성하는 단계; _ arXiv preprint arXiv:1904.10509_, 2019.\n' +
      '* Cho and Bengio (2014) Kyungghyun Cho and Yoshua Bengio. 딥러닝에서 조건부 연산에 대한 용량-대-연산 비율을 기하급수적으로 증가시키는 단계. _ arXiv preprint arXiv:1406.7362_, 2014.\n' +
      '* Clark 등(2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 질문에 대한 답을 풀었다고 생각해? try arc, the ai2 reasoning challenge. _ arXiv preprint arXiv:1803.05457_, 2018.\n' +
      '* Correia 등(2019) Goncalo M Correia, Vlad Niculae, Andre FT Martins. 적응적으로 희소 변압기. _ arXiv preprint arXiv:1909.00015_, 2019.\n' +
      '* Devlin 등(2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: 언어 이해를 위한 깊은 양방향 트랜스포머의 사전 훈련. _ arXiv preprint arXiv:1810.04805_, 2018.\n' +
      '* Eigen 등(2013) David Eigen, Marc\'Aurelio Ranzato, and Ilya Sutskever. 전문가들의 심층 혼합에서 팩터링된 표현들을 학습하는 단계 _ arXiv preprint arXiv:1312.4314_, 2013.\n' +
      '* Fan et al. (2021) Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. Beyond English-centric multilingual machine translation. _ Journal of Machine Learning Research_, 22(107):1-48, 2021.\n' +
      '* Fedus 등(2018) William Fedus, Ian Goodfellow, and Andrew M Dai. Maskgan:_를 채움으로써 더 나은 텍스트 생성. _ arXiv preprint arXiv:1801.07736_, 2018.\n' +
      '* Gale 등(2020) Trevor Gale, Matei Zaharia, Cliff Young, and Erich Elsen. 딥 러닝을 위해 gpu 커널을 희소 합니다. _ arXiv preprint arXiv:2006.10901_, 2020.\n' +
      '* Gray 등(2017) Scott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weight. _[https://openai.com/blog/block-sparse-gpu-kernels/_] (https://openai.com/blog/block-sparse-gpu-kernels/_), 2017.\n' +
      '\n' +
      '* Guu 등(2020) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 영역: 검색-증강 언어 모델 사전-훈련. _ arXiv preprint arXiv:2002.08909_, 2020.\n' +
      '* Harlap 등(2018) Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg Ganger, and Phil Gibbons. 파이프라인: 빠르고 효율적인 파이프라인 병렬 dnn 학습 _ arXiv preprint arXiv:1806.03377_, 2018.\n' +
      '* Hermann 등(2015) Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 기계가 읽고 이해할 수 있도록 가르칩니다. C. Cortes, N. 로렌스 Sugiyama, R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 28, pages 1693-1701. Curran Associates, Inc., 2015. URL [https://proceedings.neurips.cc/paper/2015/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf](https://proceedings.neurips.cc/paper/2015/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf).\n' +
      '* Hinton 등(2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 상기 지식을 신경망에서 증류하는 단계 _ arXiv preprint arXiv:1503.02531_, 2015.\n' +
      '* Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jurgen Schmidhuber. 장단기 기억. _ Neural computation_, 9(8):1735-1780, 1997.\n' +
      '* Hooker (2020) Sara Hooker. 하드웨어 추첨. _ arXiv preprint arXiv:2009.06489_, 2020.\n' +
      '* Huang et al. (2019) Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, Hyouk Joongong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: 파이프라인 병렬성을 이용한 거대 신경망의 효율적인 훈련. _신경 정보 처리 시스템의 발전_에서, 페이지 103-112, 2019.\n' +
      '* Jacobs 등(1991) Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 지역 전문가의 적응형 혼합입니다. _ Neural computation_, 3(1):79-87, 1991).\n' +
      '* Jordan and Jacobs (1994) Michael I Jordan and Robert A Jacobs. 전문가와 em 알고리즘의 계층적 혼합물입니다. _ Neural computation_, 6(2):181-214, 1994.\n' +
      '* Joshi 등(2017) Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: 읽기 이해를 위한 대규모 원거리 감독 챌린지 데이터 세트입니다. _ arXiv preprint arXiv:1705.03551_, 2017.\n' +
      '* Kaplan 등(2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 신경 언어 모델에 대한 법칙을 스케일링합니다. _ arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '* Kitaev 등(2020) Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 개질기: 효율적인 변압기. _ arXiv preprint arXiv:2001.04451_, 2020.\n' +
      '* Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: question answer research의 벤치마크. _ Transactions of the Association for Computational Linguistics_, 7:453-466, 2019.\n' +
      '\n' +
      '* Lample 등(2019) Guillaume Lample, Alexandre Sablayrolles, Marc\'Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. 제품 키가 있는 대용량 메모리 레이어입니다. _Advances in Neural Information Processing Systems_, pages 8548-8559, 2019.\n' +
      '* Lee 등(2021) Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 학습 데이터를 중복하면 언어 모델이 더 좋아집니다. _ arXiv preprint arXiv:2107.06499_, 2021.\n' +
      '* Lepikhin 등(2020) Dmitry Lepikhin, Hyouk중 Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: 조건부 계산 및 자동 샤딩이 있는 크기 조정 거대 모델입니다. _ arXiv preprint arXiv:2006.16668_, 2020.\n' +
      '* Micikevicius et al.(2017) Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. _ arXiv preprint arXiv:1710.03740_, 2017.\n' +
      '* Narayan 등(2018) Shashi Narayan, Shay B Cohen, and Mirella Lapata. 자세히 알려주지 말고 요약만 해주세요! 극단적인 요약을 위한 주제 인식 컨볼루션 신경망. _ arXiv preprint arXiv:1808.08745_, 2018.\n' +
      '*Nie 등(2019) Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: 자연어 이해를 위한 새로운 벤치마크. _ arXiv preprint arXiv:1910.14599_, 2019.\n' +
      '* Puigcerver 등(2020) Joan Puigcerver, Carlos Riquelme, Basil Mustafa, Cedric Renggli, Andre Susano Pinto, Sylvain Gelly, Daniel Keysers, and Neil Houlsby. 전문가 모델을 사용하여 확장성 있는 전이 학습입니다. _ arXiv preprint arXiv:2009.13239_, 2020.\n' +
      '* Radford 등(2018) Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018년 생성 사전 교육을 통해 언어 이해력을 향상시킵니다.\n' +
      '* Raffel 등(2019) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계를 탐구합니다. _ arXiv preprint arXiv:1910.10683_, 2019.\n' +
      '* Rajbhandari 등(2019) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 0: 1 조 개의 매개 변수 모델을 학습 하기 위한 메모리 최적화 _ arXiv preprint arXiv:1910.02054_, 2019.\n' +
      '* Rajpurkar 등(2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 쿼드: 텍스트의 기계 이해를 위한 10만 개 이상의 질문입니다. _ arXiv preprint arXiv:1606.05250_, 2016.\n' +
      '* Ramachandran and Le(2018) Prajit Ramachandran and Quoc V Le. 예제별 라우팅 모델에서 다양성과 깊이입니다. _International Conference on Learning Representations_, 2018.\n' +
      '* Robbins (1952) Herbert Robbins. 실험의 순차적 설계의 일부 측면입니다. _ Bulletin of the American Mathematical Society_, 58(5):527-535, 1952.\n' +
      '\n' +
      '* Roberts 등(2020) Adam Roberts, Colin Raffel, and Noam Shazeer. 언어 모델의 매개 변수에 얼마나 많은 지식을 담을 수 있습니까? _ arXiv preprint arXiv:2002.08910_, 2020.\n' +
      '* Rosenbaum 등(2017) Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. 라우팅 네트워크: 다중 작업 학습을 위해 비선형 함수를 적응적으로 선택 합니다. _ arXiv preprint arXiv:1711.01239_, 2017.\n' +
      '* Sakaguchi 등(2020) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 위노그란데: 규모의 적대적인 위노그라드 스키마 도전입니다. _Proceedings of the AAAI Conference on Artificial Intelligence_, Volume 34, pages 8732-8740, 2020.\n' +
      '* Sanh 등(2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 디스트릴버트, 버트의 증류 버전: 2019년 더 작고, 빠르고, 저렴하고, 가볍다.\n' +
      '* Shazeer(2020) Noam Shazeer. Glu 변종은 2020년 변압기를 개선합니다.\n' +
      '* Shazeer 등(2017) Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 대단히 큰 신경망: 희박하게 게이트된 혼합물-오-전문가 계층. _ arXiv preprint arXiv:1701.06538_, 2017.\n' +
      '* Shazeer 등(2018) Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJungong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorflow: Deep learning for supercomputers. _신경 정보 처리 시스템의 발전_에서, 페이지 10414-10423, 2018.\n' +
      '* Shoeybi 등(2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 메가트론-lm: gpu 모델 병렬 처리를 사용 하 여 수십억 매개 변수 언어 모델을 학습 합니다. _ arXiv preprint arXiv:1909.08053_, 2019.\n' +
      '* Srivastava 등(2014) Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: 신경망이 과적합되는 것을 방지하는 간단한 방법. _ Journal of Machine Learning Research_, 15(1):1929-1958, 2014. URL [http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf).\n' +
      '* Strubell 등(2019) Emma Strubell, Ananya Ganesh, and Andrew McCallum. nlp에서 딥러닝을 위한 에너지 및 정책 고려 사항 _ arXiv preprint arXiv:1906.02243_, 2019.\n' +
      '* Sukhbaatar 등(2019) Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. 트랜스포머의 적응형 주의 범위 _ arXiv preprint arXiv:1905.07799_, 2019.\n' +
      '* Sutton(2019) Rich Sutton. The Bitter Lesson. _[http://www.incompleteideas.net/IncIdeas/BitterLesson.html_] (http://www.incompleteideas.net/IncIdeas/BitterLesson.html_), 2019.\n' +
      '* Sutton and Barto (2018) Richard S Sutton and Andrew G Barto. _ 강화 학습: 도입부_. 스탠포드 대학교, 2018년\n' +
      '* Taylor (1953) Wilson L Taylor. "cloze procedure": 가독성을 측정하는 새로운 도구입니다. _ Journalism quarterly_, 30(4):415-433, 1953.\n' +
      '\n' +
      '* Vaswani 등(2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 관심만 있으면 됩니다. _신경 정보 처리 시스템의 발전_에서, 2017년 5998-6008 페이지가 있다.\n' +
      '* Wang 등(2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 글루: 자연어 이해를 위한 다중 작업 벤치마크 및 분석 플랫폼. _ arXiv preprint arXiv:1804.07461_, 2018.\n' +
      '* Wang 등(2019) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 슈퍼글루: 범용 언어 이해 시스템을 위한 보다 엄격한 벤치마크입니다. _Neural Information Processing Systems의 발전_에서, 페이지 3266-3280, 2019.\n' +
      '* Wang and Kanwar (2019) Shibo Wang and Pankaj Kanwar. Bfloat16: 클라우드 tpus에서 고성능의 비결입니다. _ Google Cloud Blog_, 2019.\n' +
      '* Xue 등(2020) Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: 대용량 다국어 사전 훈련된 텍스트-텍스트 변환기. _ arXiv preprint arXiv:2010.11934_, 2020.\n' +
      '* Yang et al. (2020) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. 레 Xlnet: Generalized autoregressive pretraining for language understanding, 2020.\n' +
      '* Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. _ arXiv preprint arXiv: 2007.14062_, 2020.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>