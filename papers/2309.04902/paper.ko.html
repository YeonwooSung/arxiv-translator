<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Small Object Detection에서 Transformers: Benchmark and Survey of the State-of-Art\n' +
      '\n' +
      'Aref Miri Rekavandi, Shima Rashidi, Farid Boussaid, Stephen Hoefs, Emre Akbas, and Mohammed Bennamoun,\n' +
      '\n' +
      'Aref Miri Rekavandi and Mohammed Bennamoun is with the Computer Science and Software Engineering, the University of West Australia (Emails: isgmitriekavandi@nwa.edu.au, mohammed.bennamoun@nwa.edu.au). Shima Rashidi는 독립적인 연구자(Email: shima. Rashidi?@gmail.com)이다. Farid Boussaid는 West Australia University of Electrical, Electronics and Computer Engineering(Email: farid.boussaid@nwa.edu.au)과 함께 있다. Stephen Hoefs는 호주 국방과학기술그룹(Email: Stephen.hoefs@lederence.gov)의 규율 리더이다. 에머 아크바스는 터키 중동공업대학 컴퓨터공학과와 함께 있다. (Email: emre@ceng.meth.edu.tr).\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '트랜스포머는 컴퓨터 비전, 특히 물체 인식 및 감지 분야에서 빠르게 인기를 얻고 있다. 최첨단 객체 검출 방법의 결과를 조사한 결과, 트랜스포머는 거의 모든 비디오 또는 이미지 데이터 세트에서 잘 확립된 CNN 기반 검출기를 일관되게 능가하는 것으로 나타났다. 변압기 기반 접근법이 소형 객체 탐지(SOD) 기술의 최전선에 있는 반면, 본 논문은 이러한 광범위한 네트워크가 제공하는 성능 이점을 탐색하고 SOD 우위에 대한 잠재적인 이유를 식별하는 것을 목표로 한다. 작은 객체는 가시성이 낮기 때문에 탐지 프레임워크에서 가장 어려운 객체 유형 중 하나로 식별되었다. SOD에서 변압기의 성능을 향상시킬 수 있는 잠재적 전략을 조사하는 것을 목표로 한다. 본 연구는 2020~2023년에 걸쳐 개발된 변압기에 대한 60개 이상의 연구 연구를 분류한다. 이러한 연구는 일반 이미지, 항공 이미지, 의료 이미지, 활성 밀리미터 이미지, 수중 이미지 및 비디오에서 소형 물체 탐지를 포함한 다양한 탐지 응용 분야를 포함한다. 또한 기존 연구에서 간과되었던 SOD에 적합한 12개의 대규모 데이터 세트 목록을 컴파일하여 제시하고 평균 평균 정밀도(mAP), 초당 프레임(FPS), 매개변수 수 등과 같은 인기 있는 메트릭을 사용하여 검토된 연구의 성능을 비교한다. 연구원들은 우리의 웹 페이지에서 새로운 연구를 추적할 수 있습니다.\n' +
      '\n' +
      '[https://github.com/arekavandi/Transformer-SOD](https://github.com/arekavandi/Transformer-SOD).\n' +
      '\n' +
      ' 오브젝트 인식, 소형 오브젝트 검출, 비전 트랜스포머, 오브젝트 로컬라이제이션, 딥 러닝, 어텐션, MS COCO 데이터셋.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'SOD(Small Object Detection)는 SOTA(State-Of-The-Art) 객체 검출 방법들에 대한 중요한 도전으로 인식되어 왔다[1]. 용어 "작은 물체"는 입력 이미지의 작은 부분을 차지하는 물체를 지칭한다. 예를 들어, 널리 사용되는 MS COCO 데이터세트 [2]에서는 일반적인 \\(480\\times 640\\) 이미지에서 바운딩 박스가 \\(32\\times 32\\) 픽셀 이하인 객체를 정의한다(도 1). 다른 데이터 세트에는 이미지의 \\(10\\%\\)를 차지하는 개체와 같은 고유한 정의가 있습니다. 작은 객체들은 종종 부정확하게 국부화된 바운딩 박스들로, 그리고 때때로 부정확한 라벨들로 놓치거나 검출된다. SOD에서 국소화가 부족한 주요 이유는 입력 이미지 또는 비디오 프레임에 제공된 제한된 정보에서 비롯되며, 심층 네트워크에서 여러 층을 통과할 때 경험하는 후속 공간 열화에 의해 복합된다. 작은 물체는 보행자 감지[3], 의료 영상 분석[4], 얼굴 인식[5], 교통 표지판 감지[6], 신호등 감지[7], 선박 감지[8], 합성 개구 레이더(Synthetic Aperture Radar, SAR) 기반 물체 감지[9] 등 다양한 응용 영역에서 자주 등장하기 때문에, 현대 딥러닝 SOD 기법의 성능을 살펴볼 가치가 있다. 본 논문에서는 변압기 기반 검출기와 합성곱 신경망 기반 검출기를 소형 물체 검출 성능 측면에서 비교한다. 명확한 마진으로 CNN을 능가하는 경우 변압기의 강력한 성능의 원인을 밝혀내려고 한다. 한 가지 즉각적인 설명은 변압기가 입력 이미지의 쌍별 위치 간의 상호 작용을 모델링한다는 것이다. 이것은 효과적으로 컨텍스트를 인코딩하는 방법입니다. 그리고, 컨텍스트는 인간과 계산 모델들 모두에서 작은 물체들을 검출하고 인식하기 위한 주요 정보 소스라는 것이 잘 확립되어 있다[10]. 그러나 이것이 유일한 요인이 아닐 수 있습니다.\n' +
      '\n' +
      '도. 1: MS COCO 데이터세트로부터의 작은 크기의 객체의 예[2]. 객체는 색상 세그먼트로 강조 표시됩니다.\n' +
      '\n' +
      '트랜스포머의 성공을 설명하십시오. 특히, 객체 표현, 고해상도 또는 다중 스케일 특징 맵에 대한 빠른 주의, 완전 변압기 기반 탐지, 아키텍처 및 블록 수정, 보조 기술, 개선된 특징 표현 및 시공간 정보를 포함한 여러 차원에 따라 이러한 성공을 분석하는 것을 목표로 한다. 또한, SOD에 대한 변압기의 성능을 잠재적으로 향상시킬 수 있는 접근법을 지적한다.\n' +
      '\n' +
      '이전 연구에서, 우리는 2022년 [11]년까지 광학 이미지 및 비디오에서 작은 물체 검출의 성능을 향상시키기 위해 딥 러닝에 사용된 수많은 전략을 조사했다. 변압기와 같은 새로운 딥 러닝 구조의 적응을 넘어 데이터 증강, 초해상도, 다중 스케일 특징 학습, 컨텍스트 학습, 주의 기반 학습, 지역 제안, 손실 함수 정규화, 보조 작업 레버리지 및 시공간 특징 집적이 널리 퍼져 있음을 보여주었다. 또한, 트랜스포머가 대부분의 데이터 세트에 걸쳐 작은 객체를 로컬화하는 주요 방법 중 하나임을 관찰했다. 그러나 [11]이 CNN 기반 네트워크에 초점을 맞춘 160개 이상의 논문을 주로 평가했다는 점을 감안할 때 변압기 중심 방법에 대한 심층적인 탐구는 수행되지 않았다. 현장의 성장과 탐사 속도를 인식하여 소형 물체 탐지를 목표로 하는 변류기 모델을 조사할 적기 창이 있다.\n' +
      '\n' +
      '본 논문에서는 소형 물체 탐지에 적용할 때 변압기의 인상적인 성능에 기여하는 요인과 일반적인 물체 탐지에 사용되는 전략과의 구별을 종합적으로 이해하는 것을 목표로 한다. 먼저, 기존의 CNN 기반 방법론과 비교하여 SOD를 위한 저명한 변압기 기반 객체 검출기를 강조한다.\n' +
      '\n' +
      '2017년 이후, 이 분야는 수많은 리뷰 기사가 출판되는 것을 보았다. 이러한 검토에 대한 광범위한 논의와 목록은 이전 조사[11]에서 제시된다. 또 다른 최근 조사 기사 [12]는 대부분 CNN 기반 기술에도 초점을 맞추고 있다. 이 현재 조사의 내러티브는 이전 조사와 구별된다. 이 논문의 초점은 특히 이전에 탐색되지 않은 측면인 트랜스포머를 이미지 및 비디오 SOD의 지배적인 네트워크 아키텍처로 위치시키는 것으로 좁혀진다. 이는 CNN 기반 방법을 의식적으로 배제하는 이 혁신적인 아키텍처에 맞춘 독특한 분류법을 수반한다. 이 주제에 대한 신규성과 복잡성을 감안할 때, 본 리뷰는 주로 2022년 이후에 제기된 작업을 우선시한다. 또한 광범위한 응용 분야에서 작은 객체의 위치 결정 및 검출에 사용되는 새로운 데이터 세트에 대해 조명한다.\n' +
      '\n' +
      '이 조사에서 조사된 연구는 주로 작은 객체 위치 및 분류에 맞게 조정된 방법을 제시하거나 간접적으로 SOD 문제를 해결했다. 우리의 분석을 이끈 것은 이 논문의 작은 객체에 대해 지정된 탐지 결과였다. 그러나 SOD 결과에 주목했지만 개발 접근법에서 하위 성능을 보여주거나 SOD 특정 매개변수를 간과한 초기 연구는 이 검토에 포함되는 것으로 간주되지 않았다. 이 조사에서는 판독기가 이미 일반 객체 탐지 기술, 아키텍처 및 관련 성능 측정에 익숙하다고 가정한다. 독자가 이러한 영역에 대한 기초적 통찰력을 필요로 하는 경우, 우리는 독자를 이전 작업[11]에 참조한다.\n' +
      '\n' +
      '본 논문의 구조는 다음과 같다. 섹션 2는 인코더와 디코더를 포함한 CNN 기반 객체 검출기, 변압기 및 그 구성 요소에 대한 개요를 제공한다. 이 섹션은 또한 변압기 기반 물체 검출기의 두 개의 초기 반복, 즉 DETR 및 ViT-FRCNN을 터치한다. 섹션 3에서는 변압기 기반 SOD 기술에 대한 분류를 제시하고 각 범주를 포괄적으로 조사한다. 섹션 4는 SOD에 사용되는 다양한 데이터 세트를 보여주고 다양한 응용 프로그램에 걸쳐 평가한다. 섹션 5에서는 이러한 결과를 CNN 네트워크에서 파생된 이전 결과와 분석하고 대조한다. 그 논문은 섹션 6의 결론으로 마무리한다.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '객체 검출 및 특히 SOD는 오랫동안 CNN 기반 딥러닝 모델에 의존해 왔다. YOLO(You Only Look Once) 변형예[13, 14, 15, 16, 17, 18, 19], SSD(Single Shot Multi-box Detector)[20], RetinaNet[21], SPP-Net[22], Fast R-CNN[23], Faster R-CNN[24], Region-Based Fully Convolutional Networks(R-FCN)[25], Mask R-CNN[26], Feature Pyramid Networks(FPN)[27], cascade R-CNN[28], Libra R-CNN[29]과 같은 여러 단일 스테이지 및 두 스테이지 검출기가 시간이 지남에 따라 등장했다. SOD에 대한 검출 성능을 향상시키기 위해 이러한 기술과 함께 다양한 전략이 사용되었으며 다중 규모 특징 학습이 가장 일반적으로 사용되는 접근법이다.\n' +
      '\n' +
      '변압기 모델은 기계 번역을 위한 새로운 기술로 [30]에서 처음 도입되었다. 이 모델은 주의 메커니즘에만 기반한 새로운 네트워크 아키텍처를 도입하여 재발 및 컨볼루션의 필요성을 제거함으로써 전통적인 순환 네트워크와 CNN을 넘어 발전하는 것을 목표로 했다. 트랜스포머 모델은 인코더와 디코더의 두 가지 주요 모듈로 구성된다. 도 2는 시각적 표현을 제공한다\n' +
      '\n' +
      '도. 2: 시퀀스 대 시퀀스 변환에 사용되는 인코더(좌측 모듈) 및 디코더(우측 모듈)를 포함하는 트랜스포머 아키텍처([30]으로부터의 도면).\n' +
      '\n' +
      '를 포함하는 것을 특징으로 하는 반도체 메모리 장치. 컴퓨터 비전을 위해 트랜스포머에서 일반적으로 사용되는 용어에 대한 설명은 주제에 익숙하지 않은 독자를 위해 표 I에 나와 있다. SOD의 컨텍스트 내에서, 인코더 모듈은 이미지 패치 또는 비디오 클립을 참조할 수 있는 입력 토큰을 수집하며, 적절한 표현을 추출하기 위해 미리 훈련된 CNN을 활용하는 것과 같은 다양한 특징 임베딩 접근법을 채택한다. 위치 인코딩 블록은 위치 정보를 각 토큰의 특징 표현들에 임베딩한다. 위치 인코딩은 다양한 응용에서 상당한 성능 향상을 보여주었다. 부호화된 표현들은 3개의 주요 행렬인 \\(\\textbf{W}_{q}\\in\\mathbf{R}^{d_{q}\\times d}\\), \\(\\textbf{W}_{k}\\in\\mathbf{R}^{d_{k}\\times d}\\), \\(\\textbf{W}_{v}\\in\\mathbf{R}^{d_{v}\\times d}\\)으로 파라미터화된 다중 헤드 어텐션 블록에 전달되어 각각 **q**, **k**, **v**로 표시되는 쿼리, 키 및 값 벡터를 얻는다. 즉,\n' +
      '\n' +
      '\\[\\textbf{q}_{i}=\\textbf{W}_{q}\\textbf{x}_{i},\\quad\\textbf{k}_{i}=\\textbf{W}_{k} \\textbf{x}_{i},\\quad\\textbf{v}_{i}=\\textbf{W}_{v}\\textbf{x}_{i},\\quad i=1, \\cdots,T, \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(T\\)는 총 토큰 수이며 각 토큰은 **x** 로 표시됩니다. 상기 멀티 헤드 어텐션 블록의 출력은,\n' +
      '\n' +
      '\\[\\text{MH Attention}(\\textbf{Q},\\textbf{K},\\textbf{V})=\\text{ Concat}(\\text{head}_{1},\\cdots,\\text{head}_{h})\\textbf{W}^{O}. \\tag{2}\\]\n' +
      '\n' +
      '여기서 \\(\\textbf{W}^{O}\\in\\mathbf{R}^{hd_{v}\\times d}\\), \\(d_{k}=d_{q}\\), 및\n' +
      '\n' +
      '\\[\\text{head}_{h}=\\text{Attention}(\\textbf{Q}_{h},\\textbf{K}_{h},\\textbf{V}_{h})=\\text{Softmax}(\\frac{\\textbf{K}_{h}^{\\top}\\textbf{Q}_{h}}{\\sqrt{d_{k}}}) \\textbf{V}_{h}^{\\top}. \\tag{3}\\]\n' +
      '\n' +
      '마지막으로, 이전 단계에서 얻은 결과를 스킵 연결 및 정규화 블록과 결합한다. 그런 다음 이러한 벡터는 완전히 연결된 계층을 개별적으로 통과하여 활성화 함수를 적용하여 네트워크에 비선형성을 도입한다. 이 블록의 파라미터들은 모든 벡터들에 걸쳐 공유된다. 이 과정은 심층 네트워크의 레이어 수에 해당하는 총 \\(N\\)번 반복된다. 디코더 모듈에서는 인코더에서 생성된 벡터들을 사용하여 유사한 프로세스가 적용되는 한편, 이전에 생성된 예측들/출력들을 또한 추가 입력으로서 소비한다. 궁극적으로, 가능한 출력 클래스들에 대한 출력 확률들이 계산된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{142.3pt} p{284.5pt}} \\hline Full Term & Description \\\\ \\hline Encoder & Encoder in transformers consists of multiple layers of self-attention modules and feed-forward neural networks to extract local and global semantic information from the input data. \\\\ Decoder & Decoder module is responsible to generate the output (either sequence or independent) based on the concept of self and cross attention applied to the object queries and encoder’s output. \\\\ Token & Token refers to the most basic unit of data input into the transformers. It can be image pixels, patches, or video clips. \\\\ Multi-Head Attention & Multi-Head Attention is a mechanism in transformers that enhances the learning capacity and representational power of self-attention. It divides the input into multiple subspaces and performs attention computations independently on each subspace, known as attention heads. \\\\ Spatial Attention & Spatial attention in transformers refers to a type of attention mechanism that attends to the spatial positions of tokens within a sequence. It allows the model to focus on the relative positions of tokens and capture spatial relationships. \\\\ Channel Attention & Channel attention in transformers refers to an attention mechanism that operates across different channels or feature dimensions of the input. It allows the model to dynamically adjust the importance of different channels, enhancing the representation and modeling of channel-specific information in tasks \\\\ Object Query & It refers to a learned vector representation that is used to query and attend to specific objects or entities within a scene. \\\\ Positional Embedding & It refers to a learned representation that encodes the positional information of tokens in an input sequence, enabling the model to capture sequential dependencies. \\\\ \\hline \\end{tabular}\n' +
      '\\end{table} TABLE I: A list of terminologies used in this paper with their meanings.\n' +
      '\n' +
      '도. 3: Top: DETR([31]로부터의 도면). Bottom: ViT-FRCNN (그림 [32]).\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:4]\n' +
      '\n' +
      '토큰 크기를 \\(16\\times 16\\)으로 설정하고 모든 중간 변환기 상태를 최종 변환 계층과 연결했을 때 가장 좋은 결과를 얻었다. 추가로, 두 검출기 모두 특징 추출을 위한 백본으로서 DETR에서, 검출 헤드를 위한 ViT-FRCNN에서, 상이한 스테이지에서 CNN에 의존한다. 작은 물체 검출 결과를 개선하기 위해서는 공간 해상도를 유지하기 위해 가능한 한 작은 이미지 패치를 유지하는 것이 중요하며, 이는 결과적으로 계산 비용을 증가시킨다. 이러한 한계와 과제를 해결하기 위해 추가 연구가 수행되었으며 다음 섹션에서 자세히 설명한다.\n' +
      '\n' +
      '## 3 Transformer For Small Object Detection\n' +
      '\n' +
      '본 절에서는 SOD를 위한 변압기 기반 네트워크에 대해 논의한다. 소형 물체 검출기의 분류는 그림 4와 같다. 우리는 새로운 변압기를 기반으로 한 기존 검출기가 물체 표현, 고해상도 또는 다중 스케일 특징 맵에 대한 빠른 주의, 완전 변압기 기반 검출, 아키텍처 및 블록 수정, 보조 기술, 개선된 특징 표현 및 시공간 정보 중 하나 또는 몇 가지 관점을 통해 분석될 수 있음을 보여준다. 다음의 하위 섹션에서는 이러한 범주의 각각에 대해 별도로 자세히 논의한다.\n' +
      '\n' +
      '### _Object Representation_\n' +
      '\n' +
      '객체 검출 기법에는 다양한 객체 표현 기법들이 채택되어 왔다. 관심 객체는 직사각형 박스[23], 중심점[36] 및 포인트 세트[37], 확률 객체[38], 및 키포인트[39]와 같은 포인트로 표현될 수 있다. 각 객체 표현 기법은 주석 형식 및 작은 객체 표현의 필요성과 관련하여 고유한 장단점을 가지고 있다. 기존의 표현들의 장점을 모두 유지하면서 최적의 표현 기법을 찾는 추구는 RelationNet++[35]로 시작되었다. 이 접근법은 다양한 이질적인 시각적 표현을 브리징하고 브릿지 시각적 표현(BVR: Bridging Visual Representations)이라는 모듈을 통해 그들의 장점을 결합한다. BVR은 주요 표현에 의해 채택된 전체 추론 프로세스를 방해하지 않고 효율적으로 작동하며, 키 샘플링 및 공유 위치 임베딩의 새로운 기술을 활용한다. 더 중요하게는, BVR은 하나의 표현 형태를 "마스터 표현"(또는 쿼리)으로 지정하는 어텐션 모듈에 의존하는 반면, 다른 표현들은 "보조" 표현들(또는 키들)로 지정된다. BVR 블록은 그림 5에 나와 있으며, 여기서 중심점과 코너점(키)을 앵커 기반(쿼리) 객체 탐지 방법론에 원활하게 통합하여 앵커 박스의 특징 표현을 강화한다. 다른 객체 표현도 그림 5에 나와 있다. CenterNet++ [40]은 새로운 상향식 접근법으로 제안되었다. 센터넷++는 모든 객체의 파라미터를 한 번에 추정하는 대신 객체의 개별 구성 요소, 즉 상단-좌측, 하단-좌측 및 중심 키포인트를 전략적으로 식별한다. 그런 다음, 동일한 객체와 연관된 포인트들을 클러스터링하기 위해 후처리 방법론이 채택된다. 이 기법은 전체 객체를 전체적으로 추정하는 하향식 접근법에 비해 SOD에서 우수한 재현율을 입증했다.\n' +
      '\n' +
      '### _고해상도 또는 Multi-Scale Feature Maps_ 에 대 한 빠른 주의 사항\n' +
      '\n' +
      '기존 연구에서는 SOD에서 높은 성능을 유지하기 위해 특징 맵의 높은 해상도를 유지하는 것이 필요한 단계라는 것을 보여주었다. 트랜스포머는 토큰의 수(예를 들어, 픽셀 수)에 대한 복잡도의 이차적 증가로 인해 본질적으로 CNN에 비해 현저하게 더 높은 복잡도를 나타낸다. 이러한 복잡성은 모든 토큰에 걸친 쌍별 상관 계산의 요구로부터 나타난다. 결과적으로, 훈련 및 추론 시간 모두 예상을 초과하여, 검출기가 고해상도 이미지 및 비디오에서 작은 객체 검출에 적용 불가능하게 된다. 변형 가능한 DETR에 대한 작업에서 Zhu _et al._[41]은 DETR에서 처음 관찰된 이 문제를 해결했다. 그들은 참조 주변의 작은 키 샘플링 포인트 집합에만 주의를 기울이는 것을 제안하여 복잡성을 크게 줄였다. 이 전략을 채택함으로써 그들은 효과적으로 보존되었다.\n' +
      '\n' +
      '도. 5: BVR은 앵커-기반 검출을 위한 특징들을 향상시키기 위해 상이한 표현들, 즉 코너 및 중심점들을 사용한다(좌측 도면). 레드 대시들이 그라운드 트루스([35]로부터의 그림)를 보여주는 다른 이미지(고양이)에 대해 객체 표현들이 도시된다.\n' +
      '\n' +
      '도. 6: 변형 가능한 주의 모듈에 대한 블록 다이어그램입니다. \\ (\\textbf{z}_{q}\\)는 쿼리의 콘텐츠 피쳐, **x**는 피쳐 맵, \\(\\textbf{p}_{q}\\)은 2D 그리드의 참조 지점입니다. 요컨대, 변형 어텐션 모듈은 기준점 주위의 키 샘플링 포인트들의 작은 세트(각각의 머리에서 상이함)에만 참석한다. 이는 복잡성을 상당히 감소시키고 수렴성을 더욱 향상시킨다([41]로부터의 도면).\n' +
      '\n' +
      '다중 스케일 변형 주의 모듈 사용을 통한 공간 해상도입니다. 놀랍게도, 이 방법은 특징 피라미드 네트워크의 필요성을 제거하여 작은 물체의 검출 및 인식을 크게 향상시켰다. 변형 어텐션에서 멀티 헤드 어텐션 모듈의 \\(i-\\)번째 출력은 다음과 같이 주어진다:\n' +
      '\n' +
      '\\[\\text{MH Attention}^{i}=\\sum_{h}\\mathbf{W}_{h}^{O}\\big{[}\\sum_{k=1}^{K}A_{hik} \\mathbf{W}_{v}\\mathbf{x}_{k}(\\mathbf{p}_{i}+\\Delta\\mathbf{p}_{hik})\\big{]}, \\tag{6}\\]\n' +
      '\n' +
      '여기서 \\(i=1,\\cdots,T\\)와 \\(\\mathbf{p}_{i}\\)은 질의의 참조점이고 \\(\\Delta\\mathbf{p}_{hik}\\)은 k개의 샘플링(K\\(<<\\)T=HW)을 갖는 \\(h-\\)번째 헤드의 샘플링 오프셋(2D)이다. 도 6은 그의 멀티 헤드 어텐션 모듈 내의 계산 프로세스를 예시한다. 변형 가능한 DETR은 인코더와 디코더 모듈 모두에서 이득을 얻는데, 인코더 내의 복잡도 순서는 \\(\\mathcal{O}(HWC^{2})\\)이고, \\(H\\)과 \\(W\\)은 입력 특징 맵의 높이와 너비, \\(C\\)은 채널 수이다. DETR 부호화기의 경우 복잡도의 순서는 \\(\\mathcal{O}(H^{2}W^{2}C)\\)이며, 크기는 \\(H\\)과 \\(W\\)이 증가함에 따라 2차 증가를 나타낸다. 변형 가능한 주의력은 다양한 다른 검출기, 예를 들어 T-TRD[43]에서 두드러진 역할을 했다. 이어서, [44]에서 동적 DETR이 제안되었는데, 동적 인코더와 특징 피라미드를 저해상도 표현으로부터 고해상도 표현까지 이용하는 동적 디코더를 특징으로 하여, 효율적인 거친-대-미세 물체 검출 및 더 빠른 수렴을 야기한다. 동적 인코더는 스케일, 공간 중요도 및 표현에 기초하여 동적으로 주의 메커니즘을 조정하는 완전 자기 주의의 순차적으로 분해된 근사치로 볼 수 있다. 변형 가능한 DETR과 동적 DETR은 모두 특징 추출을 위해 변형 가능한 컨벌루션을 사용한다. 서로 다른 접근법에서, \\(\\text{O}^{2}\\text{DETR}\\)[45]는 자기 주의 모듈이 제공하는 전역 추론이 실제로 동일한 이미지 영역에 객체가 밀집되어 있는 항공 이미지에 필수적이지 않다는 것을 보여주었다. 따라서, 다중 스케일 특징 맵의 통합과 결합된 로컬 컨볼루션으로 주의 모듈을 대체하는 것은 지향 객체 검출의 맥락에서 검출 성능을 향상시키는 것으로 입증되었다. [46]의 저자는 Row-Column Decoupled Attention(RCDA) 개념을 제안하여 주요 특징의 2D 주의를 1D 행 방향 주의와 열 방향 주의의 두 가지 간단한 형태로 분해했다. CF-DETR[47]의 경우, FPN에 대한 대안적인 접근법이 제안되었고, 이에 의해 C5 피처들이 레벨 5(E5)에서 인코더 피처들로 대체되어, 개선된 오브젝트 제시를 야기하였다. 이러한 혁신을 TEF(Transformer Enhanced FPN) 모듈이라고 명명하였다. 또 다른 연구에서 Xu _et al._[48]은 Swin 변압기와 스킵 연결 연산의 통합을 통해 가중 양방향 특징 피라미드 네트워크(BiFPN)를 개발했다. 이 접근법은 작은 물체와 관련된 정보를 효과적으로 보존했다.\n' +
      '\n' +
      '### _Fully Transformer-Based Detectors_\n' +
      '\n' +
      '변압기의 출현과 컴퓨터 비전의 많은 복잡한 작업에서 뛰어난 성능은 연구자들이 점차적으로 CNN 기반 또는 혼합 시스템에서 완전히 변압기 기반 비전 시스템으로 전환하도록 동기를 부여했다. 이러한 작업 라인은 [33]에서 제안된 ViT로 알려진 이미지 인식 작업에 트랜스포머 전용 아키텍처를 적용하는 것으로 시작되었다. [42]에서 ViDT는 SOD에 적합한 최초의 효율적인 검출기를 개발하기 위해 YOLOS 모델[49]을 확장했다. ViDT에서 특징 추출을 위해 DETR에 사용되는 ResNet은 RAM(Reconfigured Attention Module)과 함께 Swin Transformer[50], ViTDet[51], DeiT[52]와 같은 다양한 ViT 변형으로 대체된다. 램은 \\([\\text{PATCH}]\\times[\\text{PATCH}]\\), \\([\\text{DET}]\\times[\\text{PATCH}]\\), \\([\\text{DET}]\\times[\\text{DET}]\\)의 주의사항을 처리할 수 있다. 이러한 크로스 및 셀프-어텐션 모듈들은 YOLOS와 유사하게 ViDT가 입력에 [DET] 및 [PATCH] 토큰들을 부가하기 때문에 필요하다. ViDT는 바디 스텝의 각 단계에서 생성되는 멀티 스케일 특징을 이용하기 위해 변압기 디코더만을 넥으로 사용한다. 그림 7은 ViDT의 일반적인 구조를 보여주고 DETR 및 YOLOS와의 차이점을 강조한다.\n' +
      '\n' +
      '디코더 모듈이 변압기 기반 물체 검출의 비효율성의 주요 원인임을 인식하고, DFFT(Decoder-Free Fully Transformer)[53]는 높은 정확도를 유지하기 위해 스케일-집계 인코더(Scale-Aggregated Encoder; SAE) 및 태스크-정렬 인코더(Task-Aligned Encoder; TAE)의 두 인코더를 활용한다. SAE는 다중 스케일 특징(4개의 스케일)을 단일 특징 맵으로 집계하는 반면, TAE는 객체 유형 및 위치 분류 및 회귀를 위해 단일 특징 맵을 정렬한다. DOT(Detection-Oriented Transformer) 백본을 이용하여 강한 의미론을 갖는 다중 스케일 특징 추출을 수행한다.\n' +
      '\n' +
      'Sparse RoI 기반 변형 가능한 DETR(SRDD)[54]에서, 저자들은 궁극적으로 인코더 내의 중복 토큰을 제거하기 위해 스코어링 시스템을 갖는 경량 트랜스포머를 제안했다. 이는 종단간 학습 방식에서 RoI 기반 검출을 사용하여 달성된다.\n' +
      '\n' +
      '### _Architecture and Block Modifications_\n' +
      '\n' +
      '첫 번째 엔드 투 엔드 객체 검출 방법인 DETR은 훈련 중에 확장된 수렴 시간에 어려움을 겪으며 작은 객체에 대해 제대로 수행하지 못한다. SOD 성능을 향상시키기 위해 여러 연구 작업에서 이러한 문제를 해결했다. 한 가지 주목할 만한 기여는 Sun et al. [55]에서 왔는데, 그는 FCOS[56](완전 컨볼루션 단일 단계 검출기)와 Faster RCNN에서 영감을 얻었으며, TSP-FCOS와 TSP-RCNN이라는 특징 피라미드를 가진 두 개의 인코더 전용 DETR 변형을 제안했다. 이것은 디코더로부터 크로스-어텐션 모듈들을 제거함으로써 달성되었다. 그들이\n' +
      '\n' +
      '도. 도 7: ViDT(c)는 SOTA 결과를 달성하기 위해 다중 스케일 특징 학습 파이프라인에서 DETR(ViT 백본 또는 다른 완전 변압기 기반 백본과 함께)(a)과 YOLOS 아키텍처(b)를 혼합한다([42]로부터의 그림).\n' +
      '\n' +
      '분석 결과, 디코더의 교차 주의와 헝가리 손실의 불안정성이 DETR에서 수렴이 늦은 주요 원인임을 보여주었다. 이러한 통찰력은 디코더를 폐기하고 이러한 새로운 변형, 즉 TSP-FCOS 및 TSP-RCNN에 새로운 이분 매칭 기술을 도입하게 했다.\n' +
      '\n' +
      'CNN과 트랜스포머를 이용한 결합 접근법에서 Peng et al. [57, 58]은 "Conformer"라는 하이브리드 네트워크 구조를 제안하였다. 이 구조는 CNN에 의해 제공되는 로컬 특징 표현을 다양한 해상도로 트랜스포머에 의해 제공되는 글로벌 특징 표현과 융합한다(도 8 참조). 이는 특징 결합 장치(FCU)를 통해 달성되었으며 실험 결과는 ResNet50, ResNet101, DeiT 및 기타 모델과 비교하여 그 효과를 보여준다. CNN과 트랜스포머를 결합한 유사한 하이브리드 기법이 [59]에서 제안되었다. 국지적 지각과 원거리 상관의 중요성을 인식한 Xu 등[60]은 Swin Transformer에서 Swin Transformer 블록에 Local Perception Block(LPB)을 추가하였다. Local Perception Swin Transformer(LPSW)라고 불리는 이 새로운 백본은 항공 영상에서 작은 크기의 물체를 탐지하는 것을 크게 향상시켰다. DIAG-TR[61]은 로컬 특징들을 글로벌 표현들에 적응적이고 계층적으로 임베딩하기 위해 인코더에 글로벌-로컬 특징 인터웨빙(GLFI: Global-Local Feature Interweaving) 모듈을 도입하였다. 이 기법은 작은 객체의 축척 불일치에 대한 균형을 조정합니다. 또한, 학습 가능한 앵커 박스 좌표가 트랜스포머 디코더의 콘텐츠 쿼리에 추가되어 유도성 바이어스를 제공했다. 최근 연구에서 Chen 등[62]은 컨볼루션을 변압기 블록에 임베딩하여 지역 정보의 범위를 확장한 하이브리드 네트워크 트랜스포머(Hybrid Network Transformer: Hyneter)를 제안하였다. 이러한 개선은 MS COCO 데이터 세트에 대한 탐지 결과를 향상시켰다. 유사한 하이브리드 접근법이 [63]에서 채택되었다. 또 다른 연구[64]에서, 저자들은 CNN과 트랜스포머를 결합하여 작은 물체의 지역적 세부 사항과 특징을 향상시키는 동시에 전역적 수용 필드를 제공하는 NeXtFormer라는 새로운 백본을 제안했다.\n' +
      '\n' +
      '다양한 방법 중 O\\({}^{2}\\)DETR [45]는 변압기의 주의 메커니즘을 깊이 분리 가능한 컨볼루션으로 대체했다. 이러한 변화는 다중 스케일 특징과 관련된 메모리 사용 및 계산 비용을 감소시킬 뿐만 아니라 항공 사진에서 탐지 정확도를 잠재적으로 향상시킬 수 있다.\n' +
      '\n' +
      '기존 연구에서 사용된 객체 질의에 대한 질문으로 Wang et al. [46]은 객체 질의에 앵커 포인트를 사용한 Anchor DETR을 제안하였다. 이러한 앵커 포인트는 대상 쿼리 위치의 해석 가능성을 향상시킵니다. 각 앵커 포인트에 대한 다중 패턴의 사용은 하나의 영역에서 다중 객체의 검출을 향상시킨다. 대조적으로, 조건부 DETR[65]은 공간 주의 예측으로 이어지는 디코더 콘텐츠로부터 유도된 조건부 공간 질의를 강조한다. 후속 버전인 조건부 DETR v2 [66]은 객체 쿼리를 박스 쿼리의 형태로 재구성하여 아키텍처를 향상시켰다. 이 수정은 기준점을 임베딩하고 기준점에 대해 박스들을 변환하는 것을 포함한다. 후속 연구에서 DAB-DETR[67]은 동적으로 조정 가능한 앵커 박스를 사용하여 쿼리 설계의 개념을 더욱 개선했다. 이러한 앵커 박스는 참조 쿼리 포인트 및 앵커 치수 모두의 역할을 한다(도 9 참조).\n' +
      '\n' +
      '다른 작업 [47]에서 저자들은 DETR에서 작은 물체의 평균 평균 정밀도(mAP)가 최첨단(SOTA) 기술과 경쟁하지 않지만 작은 교차 초과 결합(IoU) 임계값에 대한 성능은 경쟁자보다 놀랍게도 더 우수하다는 것을 관찰했다. 이는 DETR이 강력한 인식 능력을 제공하지만 더 나은 위치 지정 정확도를 달성하기 위해서는 미세 조정이 필요함을 나타낸다. 그 해결책으로, Coarse-to-Fine Detection Transformer (CF-DETR)는 디코더 계층에서 적응 스케일 퓨전 (ASF) 및 로컬 크로스 어텐션 (LCA) 모듈을 통해 이러한 미세화를 수행하기 위해 제안되었다. [68]에서 저자들은 변압기 기반 탐지기의 차선책은 범주화와 회귀 모두에 특이 교차 주의 모듈을 사용하는 것, 콘텐츠 쿼리에 대한 부적절한 초기화, 자기 주의 모듈에서 사전 지식을 활용하는 것의 부재와 같은 요인에 기인할 수 있다고 주장한다. 이러한 문제를 해결하기 위해, 그들은 검출 분할 변압기(DESTR)를 제안했다. 이 모델은 교차 주의를 분류용과 회귀용 두 가지로 나눈다. 또한 DESTR은 디코더에서 적절한 콘텐츠 질의 초기화를 보장하고 셀프-어텐션 모듈을 향상시키기 위해 미니 디텍터를 사용한다. 또 다른 연구[48]는 진보된 전경을 활용하는 FEA-Swin을 소개했다\n' +
      '\n' +
      '도. 8: 피처 커플링 유닛(FCU)에서 CNN에 의해 제공되는 로컬 피처 및 변압기에 의해 제공되는 글로벌 피처를 모두 활용하는 컨포머 아키텍처([58]로부터의 그림).\n' +
      '\n' +
      '스윈 트랜스포머 프레임워크에서 주의력을 강화하여 컨텍스트 정보를 원래 백본에 통합합니다. 이는 Swin Transformer가 인접 객체 간의 연결 누락으로 인해 밀집 객체 탐지를 적절하게 처리하지 못한다는 사실에 의해 동기가 부여되었다. 따라서, 전경 향상은 추가적인 상관 분석을 위해 객체들을 강조한다. TOLO[69]는 간단한 넥 모듈을 통해 변압기 아키텍처에 유도 바이어스(CNN을 사용)를 가져오는 것을 목표로 하는 최근의 작업 중 하나이다. 이 모듈은 고해상도 및 고의미 특성을 통합하기 위해 서로 다른 계층의 기능을 결합합니다. 여러 개의 광 변압기 헤드가 서로 다른 스케일에서 물체를 감지하도록 설계되었습니다. 다른 접근법에서, Liang 등[70]에 의해 제안된 각각의 아키텍처, CBNet에서의 모듈들을 수정하는 대신에, 복합 연결들을 통해 연결되는 다수의 동일한 백본들을 그룹화한다.\n' +
      '\n' +
      'MATR(Multi-Source Aggregation Transformer) [71]에서, 트랜스포머의 크로스-어텐션 모듈은 상이한 뷰들로부터 동일한 객체의 다른 지원 이미지들을 레버리지하기 위해 사용된다. 유사한 접근법이 [72]에서 채택되며, 여기서 MVViT(Multi-View Vision Transformer) 프레임워크는 단일 뷰에서 객체가 보이지 않을 때 검출 성능을 향상시키기 위해 타겟 뷰를 포함한 다수의 뷰로부터의 정보를 결합한다.\n' +
      '\n' +
      '다른 작품들은 욜로 가족 건축을 고수하는 것을 선호한다. 예를 들어, SPH-Yolov5[73]는 개선된 소형 객체 로컬화를 위해 특징들을 융합하기 위해 Yolov5 네트워크의 얕은 층들에 새로운 분기를 추가한다. 또한 Yolov5 파이프라인에 Swin Transformer 예측 헤드를 처음으로 통합한다.\n' +
      '\n' +
      '[74]에서 저자는 헝가리 손실의 직접적인 일대일 경계 상자 일치 접근법이 항상 유리한 것은 아닐 수 있다고 주장한다. 그들은 일대다 할당 전략을 사용하고 NMS(Non-Maximum Suppression) 모듈을 사용하면 더 나은 탐지 결과를 얻을 수 있음을 보여준다. 이러한 관점을 반영하여, 그룹 DETR[75]은 일대일 라벨 할당으로 K개의 객체 쿼리 그룹을 구현하여, 성능을 향상시키기 위해 각각의 지상-진리 객체에 대해 K개의 긍정적인 객체 쿼리를 유도한다.\n' +
      '\n' +
      'DKTNet(Dual-Key Transformer Network)은 [76]에서 제안되며, 여기서 두 개의 키가 사용됩니다. 하나는 **Q** 스트림과 함께, 다른 하나는 **V** 스트림과 함께입니다. 이렇게 하면 **Q** 와 **V** 간의 일관성이 향상되어 학습이 향상됩니다. 추가적으로, 채널 어텐션은 공간 어텐션 대신에 계산되고, 프로세스를 가속화하기 위해 1D 컨볼루션을 사용한다.\n' +
      '\n' +
      '### _Auxiliary Techniques_\n' +
      '\n' +
      '실험 결과는 보조 기술이나 태스크가 주 태스크와 결합될 때 성능을 향상시킬 수 있음을 보여주었다. 트랜스포머의 맥락에서, **(i)** 보조 디코딩/인코딩 손실: 이것은 바운딩 박스 회귀 및 객체 분류를 위해 설계된 피드-포워드 네트워크가 별개의 디코딩 계층에 연결되는 접근법을 지칭한다. 따라서 다양한 규모의 개별 손실이 결합되어 더 나은 탐지 결과로 이어지는 모델을 훈련한다. 이 기술 또는 그 변형은 ViDT[42], MDef-DETR[77], CBNet[70], SRDD[54]에서 사용되었다. **(ii)** 반복 박스 정제: 이 방법에서, 각각의 디코딩 계층 내의 바운딩 박스들은 이전 계층들로부터의 예측들에 기초하여 정제된다. 이러한 피드백 메커니즘은 점진적으로 검출 정확도를 향상시킨다. 이 기술은 ViDT[42]에서 사용되어 왔다. **(iii)** Top-Down Supervision: 이 접근법은 MDef-DETR[77]에서 정렬된 이미지-텍스트 쌍들, 또는 TGOD[78]에서 텍스트-유도된 객체 검출기와 같은 작은 또는 클래스-불가지론적인 객체들을 검출하는 복잡한 작업에 도움을 주기 위해 인간 이해가능한 의미론을 활용한다. **(iv)** 사전 훈련: 대규모 데이터 세트에 대한 훈련 후 탐지 작업에 대한 특정 미세 조정을 포함합니다. 이 기술은 CBNet V2-TTA [79], FP-DETR [80], T-TRD [43], SPH-Yolov5 [73], MATR [71], 그룹 DETR v2 [81]에서 광범위하게 사용되었다. **(v)** 데이터 보강: 이 기술은 다양한 애플리케이션을 적용하여 탐지 데이터 세트를 풍부하게 합니다.\n' +
      '\n' +
      '도. 9: DAB-DETR은 조건부 DETR을 개선하고 동적 앵커 박스를 활용하여 더 나은 참조 쿼리 포인트 및 앵커 크기를 순차적으로 제공한다([67]로부터의 그림).\n' +
      '\n' +
      '회전, 뒤집기, 확대/축소, 자르기, 병진, 소음 추가 등과 같은 증강 기술입니다. 데이터 증강은 딥 러닝 데이터세트 내에서 다양한 불균형 문제(예를 들어, 객체 크기의 불균형)를 해결하기 위해 일반적으로 사용되는 접근법이다[82]. 데이터 증강은 열차와 시험 세트 사이의 갭을 최소화하기 위한 간접적인 접근으로 볼 수 있다[83]. T-TRD[43], SPH-Volov5[73], MATR[71], NLFFTNet[84], DeoT[85], HTDet[86], Sw-YoloX[63]를 포함한 여러 방법이 탐지 작업에서 증강을 사용했다. **(vi)** 일대다 레이블 할당: DETR에서 일대일 일치하면 인코더 내에서 식별 기능이 저하될 수 있습니다. 따라서 CO-DETR[87]과 같은 일부 연구에서는 Faster-RCNN, RetinaNet 및 FCOS와 같은 다른 방법의 일대다 할당이 보조 헤드로 사용되었다. **(vii)** 잡음 제거 훈련: 이 기술은 종종 이분 매칭으로 인해 불안정한 수렴에 직면하는 DETR에서 디코더의 수렴 속도를 향상시키는 것을 목표로 한다. 노이즈 제거 훈련에서, 디코더는 노이즈가 있는 접지-진실 라벨들 및 박스들을 디코더로 공급된다. 그 다음, 모델은 (보조 손실에 의해 유도되는) 원래의 지상 진리를 재구성하도록 트레이닝된다. DINO[88] 및 DN-DETR[89]과 같은 구현은 디코더의 안정성을 향상시키는 데 있어서 이 기술의 유효성을 입증하였다.\n' +
      '\n' +
      '### _Improved Feature Representation_\n' +
      '\n' +
      '현재 객체 검출기는 일반 크기 또는 대형 객체에 대한 광범위한 응용 분야에서 우수하지만 특정 사용 사례는 개선된 SOD를 위한 특수 기능 표현을 필요로 한다. 예를 들어, 항공 이미지에서 지향된 객체를 검출하는 경우, 임의의 객체 회전은 배경 잡음 또는 장면의 클러터(영역 제안) 증가로 인해 특징 표현을 크게 변경할 수 있다. 이를 해결하기 위해 Dai _et al._[90] | 임의 개체 회전에 견고하도록 설계된 방법인 AO2-DETR을 제안했습니다. 이는 **(i)** 지향 제안의 생성, **(ii)** 회전 불변 기능을 추출하는 지향 제안의 정제 모듈 및 **(iii)** 회전 인식 집합 일치 손실의 세 가지 주요 구성 요소를 통해 달성됩니다. 이러한 모듈은 객체의 회전 효과를 무효화하는 데 도움이 됩니다. 이와 관련된 접근 방식인 DETR++[91]에서는 C3, C4, C5의 특징맵에 상향식으로 적용되는 다중 BiFPN(Bi-Directional Feature Pyramid Layer)을 사용한다. 그리고 모든 스케일에서 특징을 대표하는 하나의 스케일만을 선택하여 DETR 프레임워크에 입력한다. 관심 객체가 일반적으로 인간 작업자와 관련된 플랜트 안전 모니터링과 같은 일부 특정 애플리케이션의 경우, 이러한 컨텍스트 정보를 활용하는 것은 특징 표현을 크게 향상시킬 수 있다. PointDet++[92]는 인간의 포즈 추정 기술을 통합하고, SOD 성능을 향상시키기 위해 로컬 및 글로벌 특징을 통합함으로써 이를 활용한다. 특징 품질에 영향을 미치는 또 다른 중요한 요소는 백본 네트워크와 의미 및 고해상도 특징을 모두 추출하는 능력이다. [93]에 소개된 고스트넷은 트랜스포머에 고품질의 다중 스케일 특징을 전달하는 능률적이고 효율적인 네트워크를 제공한다. 이 네트워크의 고스트 모듈은 출력 특징 맵을 부분적으로 생성하고 나머지는 간단한 선형 연산을 사용하여 복구한다. 이것은 백본 네트워크의 복잡성을 완화하기 위한 핵심 단계이다. 의료 영상 분석의 맥락에서, MS Transformer[94]는 노이즈에 덜 민감한 더 풍부한 특징들을 재구성하는 데 도움이 되는 입력 영상에 대해 랜덤 마스크를 수행하기 위해 자기 지도 학습 접근법을 사용하였다. 계층적 트랜스포머와 함께, 이 접근법은 다양한 백본들을 갖는 DETR 프레임워크들보다 더 우수하다. Small Object Favoring DETR(SOF-DETR)[95]은 특히 DETR-Transformer로의 입력 전에 정규화된 유도성 바이어스 모듈에서 계층 3 및 계층 4로부터의 컨볼루션 특징들을 병합함으로써 작은 객체들의 검출을 선호한다. NLFFTNet[84]은 비국소 특징-융합 변압기 컨볼루션 네트워크를 도입함으로써, 상이한 특징 계층들 사이의 장거리 의미 관계를 포착함으로써 현재의 융합 기술들에서 국부적 상호작용만을 고려하는 한계를 해결한다. DeoT[85]는 새로운 특징 피라미드 융합 모듈과 인코더 전용 트랜스포머를 병합한다. 이 융합은 채널 정제 모듈(CRM)과 공간 정제 모듈(SRM)에서 채널과 공간 주의의 사용으로 향상되어 더 풍부한 특징을 추출할 수 있다. HTDet [86]의 저자는 더 나은 객체 검출을 위해 저수준 및 고수준 특징을 누적 융합하기 위해 세밀한 FPN을 제안했다. 한편, MDCT[96]에서 저자는 소형 객체의 온톨로지 및 인접 공간 특징을 모두 이용하여 소형 객체 관련 특징 추출의 성능을 향상시키기 위해 MDC(Multi-kernel Dilated Convolution) 모듈을 제안하였다. 제안된 모듈은 계산 비용을 줄이기 위해 깊이별 분리 가능한 컨벌루션을 활용한다. 마지막으로 [97]에서는 경량 백본과 쌍을 이루는 특징 융합 모듈을 설계하여 수용 영역을 넓힘으로써 작은 물체의 시각적 특징을 강화한다. RTD-Net [97]의 하이브리드 어텐션 모듈은 작은 객체를 둘러싼 상황 정보를 통합하여 부분적으로 가려진 객체를 탐지할 수 있도록 시스템에 권한을 부여한다.\n' +
      '\n' +
      '### _Spatio-Temporal Information_\n' +
      '\n' +
      '이 섹션에서는 작은 물체를 식별하는 것을 목표로 하는 비디오 기반 물체 감지기에 독점적으로 초점을 맞춘다. 이러한 많은 연구가 이미지넷 VID 데이터 세트 1[98]에서 테스트되었지만 이 데이터 세트는 원래 작은 객체 검출을 위한 것이 아니다. 그럼에도 불구하고, 일부 작업은 이미지넷 VID 데이터 세트의 작은 개체에 대한 결과를 보고했다. 비디오에서 작은 물체를 추적하고 검출하는 주제 또한 트랜스포머 아키텍처를 사용하여 탐구되었다. 영상 기반 SOD 기술은 비디오에 적용될 수 있지만 일반적으로 어수선한 프레임이나 가려진 프레임에서 작은 물체를 식별하는 데 특히 유리할 수 있는 귀중한 시간적 정보를 활용하지 않는다. 일반 객체 검출/추적에 트랜스포머의 적용은 TrackFormer[99] 및 TransT[100]에서 시작되었다. 이 모델들은 프레임 대 프레임(이전 프레임을 기준으로 설정) 설정 예측 및 템플릿 대 프레임(템플릿 프레임을 기준으로 설정) 검출을 사용했다. [101]의 Liu _et al._은 비디오 기반 소형 물체 검출 및 추적을 위해 특별히 트랜스포머를 사용한 최초의 사람 중 하나였다. 그들의 핵심 개념은 작은 오브젝트들의 존재에 의해 유도된 임의의 작은 변화들을 캡처하기 위해 템플릿 프레임들을 업데이트하고 템플릿 프레임과 검색 프레임 사이에 전역적 주의-유도 관계를 제공하는 것이다.\n' +
      '\n' +
      '각주 1: [https://paperswithcode.com/sota/video-object-detection-on-imagenet-vid](https://paperswithcode.com/sota/video-object-detection-on-imagenet-vid)\n' +
      '\n' +
      '트랜스포머 기반 물체 검출은 [102] 및 [103]에서 제시된 바와 같이 종단간 물체 검출기인 TransVOD의 도입으로 형식적인 인식을 얻었다. 이 모델은 일련의 비디오 프레임에 공간 변환기와 시간 변환기를 모두 적용하여 이러한 프레임에 걸쳐 객체를 식별하고 연결한다. TransVOD는 실시간 검출을 위한 기능을 포함하여 각각 고유한 기능을 가진 여러 변형을 낳았다. PTSEFormer[104]는 프레임들 사이의 시간 정보 및 객체들의 공간 전이들 모두에 초점을 맞추어 진보적인 전략을 채택한다. 이를 위해 다중 스케일 특징 추출을 사용한다. 다른 모델과 달리 PTSE 전자는 전체 데이터 세트가 아닌 인접한 프레임에서 객체 쿼리를 직접 회귀하여 보다 지역화된 접근법을 제공한다. Sparse VOD[105]는 시간 정보를 통합하여 지역 제안을 제안하는 종단간 훈련 가능한 비디오 객체 검출기를 제안하였다. 대조적으로, DAFA[106]는 로컬 시간적 특징들과 대조적으로 비디오 내의 글로벌 특징들의 중요성을 강조한다. DEFA는 FIFO(First In First Out) 메모리 구조의 비효율성을 보였고, 주의 모듈을 위해 프레임 레벨 메모리 대신 객체 레벨 메모리를 사용하는 다이버시티 인식 메모리를 제안하였다. VSTAM[107]은 요소별로 특징 품질을 개선한 다음, 이러한 향상된 특징들이 객체 후보 영역 검출에 사용되기 전에 희소 집성을 수행한다. 이 모델은 또한 장기 상황 정보를 활용하기 위해 외부 메모리를 통합한다.\n' +
      '\n' +
      'FAQ 작업[108]에서, 디코더 모듈에서의 질의 특징 집성을 이용하는 새로운 비디오 객체 검출기가 제안된다. 이는 인코더에서 특징 집합에 초점을 맞추는 방법이나 다양한 프레임에 대한 후처리를 수행하는 방법과는 차이가 있다. 이 기법은 SOTA 기법보다 탐지 성능을 향상시킨다는 것을 보여준다.\n' +
      '\n' +
      '## 4 결과 및 벤치마크\n' +
      '\n' +
      '본 절에서는 소형 객체 검출의 이전 연구를 정량적, 정성적으로 평가하여 특정 응용에 가장 효과적인 기법을 식별한다. 이 비교에 앞서, 우리는 다양한 응용을 위한 비디오와 이미지를 포함하여 작은 물체 탐지 전용의 다양한 새로운 데이터 세트를 소개한다.\n' +
      '\n' +
      '### _Datasets_\n' +
      '\n' +
      '이 하위 섹션에서는 널리 사용되는 MS COCO 데이터 세트 외에도 12개의 새로운 SOD 데이터 세트를 컴파일하고 제시한다. 이러한 새로운 데이터 세트는 주로 일반 및 해상 환경을 제외한 특정 응용 프로그램에 맞게 조정된다(이전 조사 [11]에서 다루었다). 구글 스콜라에 따르면 그림 10은 이러한 데이터 세트의 시간 순서를 2023년 6월 15일 기준으로 인용 수와 함께 표시한다.\n' +
      '\n' +
      '**Uav123**[109]: 이 데이터 세트는 UAV로 획득한 123개의 비디오를 포함하며 110K 프레임 이상의 가장 큰 객체 추적 데이터 세트 중 하나입니다.\n' +
      '\n' +
      '**MRS-1800**[60]: l: 이 데이터 세트는 DIOR [115], NWPU VHR-10 [116], HRRSD [117]의 세 가지 다른 원격 감지 데이터 세트의 이미지 조합으로 구성됩니다. MRD-1800은 탐지 및 인스턴스 분할의 이중 목적을 위해 생성되었으며, 1800개의 수동으로 주석이 달린 이미지에는 비행기, 선박 및 저장 탱크의 3가지 유형이 포함된다.\n' +
      '\n' +
      '**SKU-110K [110]**: 이 데이터 세트는 전 세계의 다양한 슈퍼마커에서 캡처된 이미지를 특징으로 하는 상품 탐지를 위한 엄격한 테스트 베드 역할을 합니다. 데이터 세트에는 눈금 범위, 카메라 각도, 조명 조건 등이 포함됩니다.\n' +
      '\n' +
      '**BigDetection**[79]: 기존 데이터 세트를 통합 하 고 간과 된 개체에 레이블을 지정 하는 동안 중복 된 상자를 세심하게 제거 하 여 만든 대규모 데이터 세트입니다. 모든 크기에 걸쳐 균형 잡힌 수의 객체를 가지고 있어 현장 객체 탐지를 진행하기 위한 중추적인 자원이 됩니다. 이 데이터 세트를 사전 훈련 및 후속적으로 MS COCO에 대한 미세 조정을 위해 사용하면 성능 결과가 크게 향상된다.\n' +
      '\n' +
      '**Tang et al.**[92]: 화학 공장 내 현장 활동의 비디오 영상으로부터 시작된 이 데이터 세트는 핫 작업, 공중 작업, 제한된 공간 작업 등과 같은 다양한 유형의 작업을 다룹니다. 여기에는 사람, 헬멧, 소화기, 장갑, 작업복 및 기타 관련 물건과 같은 카테고리 라벨이 포함됩니다.\n' +
      '\n' +
      '**Xu et al.**[48]: 이 공개적으로 사용 가능한 데이터 세트는 UAV(Unmanned Aerial Vehicle)-캡처된 이미지에 초점을 맞추고 보행자와 차량 모두를 감지하는 것을 목표로 하는 2K 이미지를 포함합니다. DJI 드론을 이용하여 영상을 수집하였으며, 다양한 조도와 조밀하게 주차된 차량 등 다양한 조건을 특징으로 하였다.\n' +
      '\n' +
      '**DeepLesion**[111]: 4,427명의 환자로부터 CT 스캔을 포함 하는 이 데이터 세트는 가장 큰 종류 중 하나입니다. 폐결절, 뼈 이상, 신장 병변, 림프절 비대 등 다양한 병변 유형을 포함한다. 이러한 이미지들에서 관심있는 객체들은 전형적으로 작고 노이즈를 수반하여, 그들의 식별을 어렵게 한다.\n' +
      '\n' +
      '**Udacity Self Driving Car**[112]: 교육용 전용으로 설계되었으며, 이 데이터 세트는 마운틴 뷰에서 운전 시나리오 및\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c||c|c|c|c|c|c|c|} \\hline\n' +
      '**Dataset** & **Application** & **Video** & **Image** & **Shading Capst (Type)** & **Resolution (static)** & **Object Classes** & **Instances** & **Image/Video** & **Public?** \\\\ \\hline \\hline\n' +
      '**UAV123**[107] & UAV Tracking & \\(\\tau\\) & Aerial Propose/KOD & - & - & 123 \\(\\times\\)110K frames & Train Class Here \\\\ \\hline\n' +
      '**MRS-1800**[60] & Remote Sensing & \\(\\tau\\) & Satellite DeepKOD & NP & 5 & 16,318 & 1200 & - \\\\ \\hline\n' +
      '**SKU-110K**[11] & Community Detection & \\(\\tau\\) & Normal & NP & 116,712 & 147.40 per image & 11,762 & Yes. Click Here \\\\ \\hline\n' +
      '**BigDetection**[79] & Genetic & \\(\\tau\\) & Normal & NP & 600 & 300 & 341K Tat & Yes. Click Here \\\\ \\hline\n' +
      '**Tang et al.**[92] & Clinical Plant Monitoring & \\(\\tau\\) & Normal & - & 99 & - & 2500 & - \\\\ \\hline\n' +
      '**Xu et al.**[48] & UAV Detection & \\(\\tau^{\\prime}\\) & Aerial OD & 1920\\(\\times\\)1000 & 2 & 12.55 & 25.5 & Yes. Click Here \\\\ \\hline\n' +
      '**DeepLesion**[111] & Latent Detection & \\(\\tau\\) & (C1) & - & 8 & 32.7k & 32.1k & Yes. Click Here \\\\ \\hline\n' +
      '**KGADN filtering**[57] & 520K frames & \\(\\tau^{\\prime}\\) & - & Normal & 1920\\(\\times\\)1200 & 3 & 68k & 9,423 & Yes. Click Here \\\\ \\hline\n' +
      '**AMAMW Dataset**[11] & Security Inspection & \\(\\tau\\) & Normal (AMW) & 1600\\(\\times\\)200 & - & - & -5,98k & - \\\\ \\hline\n' +
      '**UERC 2018 Dataset**[4] & Uncharacteristic Detection & \\(\\tau\\) & Normal & - & 4 & - & 800 Toutage & - \\\\ \\hline\n' +
      '**UAV dataset**[97] & UAV-based detection & \\(\\tau\\) & Aerial Perspective (KOD) & - & 7 & 530,634 & 9,650 & - \\\\ \\hline\n' +
      '**Dense-KOD**[114] & Photo Detection & \\(\\tau\\) & Normal & NP & 2 & - & 77 Training & 134K Tat & Yes. Click Here \\\\ \\hline \\end{tabular}\n' +
      '\\end{table} TABLE II: Commonly used datasets for SOD. NF: Not fixed.\n' +
      '\n' +
      '도. 10: 인용 횟수에 따른 SOD 데이터셋의 연대기(Google Scholar 기준).\n' +
      '\n' +
      '주변 도시는 2Hz 이미지 획득 속도로 캡처됩니다. 이 데이터 세트 내의 범주 레이블에는 자동차, 트럭 및 보행자가 포함됩니다. **AMMW Dataset**[113]: 보안 응용 프로그램을 위해 생성 된 이 활성 밀리미터파 이미지 데이터 세트는 30 개 이상의 다른 유형의 개체를 포함 합니다. 여기에는 두 종류의 라이터(플라스틱과 금속으로 만들어짐), 모의 화기, 칼, 칼날, 총알 껍질, 전화기, 수프, 열쇠, 자석, 액체 병, 흡수 물질, 성냥 등이 포함된다.\n' +
      '\n' +
      '**URPC 2018 데이터 세트**: 이 수중 이미지 데이터 세트에는 lobothurian, echinus, 가리비 및 불가사리의 4가지 유형의 개체가 포함됩니다. [121].\n' +
      '\n' +
      '**UAV 데이터 세트**[97]: 이 이미지 데이터 세트는 다양한 날씨 및 조명 조건과 다양한 복잡한 배경에서 UAV를 통해 캡처된 9K 이상의 이미지를 포함합니다. 이 데이터 세트의 객체는 세단, 사람, 모터, 자전거, 트럭, 버스 및 세발자전거입니다.\n' +
      '\n' +
      '**Drone-vs-bird**[114]: 이 비디오 데이터 세트는 민감한 지역을 비행하는 드론의 보안 문제를 해결하는 것을 목표로 합니다. 다양한 조명, 조명, 날씨 및 배경 조건에서 새와 드론을 구별하기 위해 라벨이 붙은 비디오 시퀀스를 제공한다.\n' +
      '\n' +
      '애플리케이션, 유형, 해상도, 클래스/인스턴스/이미지/프레임 수 및 웹페이지에 대한 링크를 포함하는 이러한 데이터 세트의 요약이 표 2에 제공된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline Model & Backbone & GFLOPS/FPS & \\#params\\(\\downarrow\\) & mAP\\({}^{(90.5,0.95)}\\uparrow\\) & Epochs\\(\\downarrow\\) & URL \\\\ \\hline Faster RCNN-DC5 (NeurIPS2015)[24] & ResNet50 & 320/16 & 166M & 21.4 & 37 & Link \\\\ Faster RCNN-FPN (NeurIPS2015)[24] & ResNet50 & 180/26 & 42M & 24.2 & 37 & Link \\\\ Faster RCNN-FPN (NeurIPS2015)[24] & ResNet101 & 246/20 & 60M & 25.2 & – & Link \\\\ RepPoints 5-DCN-MS (NeurIPS2020)[119] & ResNeXt101 & –/– & – & 34.5* & 24 & Link \\\\ FOCS (ICCV2019)[56] & ResNet50 & 177/171 & – & 26.2 & 36 & Link \\\\ CBNet V2-DCN(ATSS120) (TIP2022)[70] & Res2Net101 & –/– & 107M & 35.7* & 20 & Link \\\\ CBNet V2-DCN(Cascade RCNN) (TIP2022)[70] & Res2Net101 & –/– & 146M & 37.4* & 32 & Link \\\\ \\hline DETER (ECCV2020)[31] & ResNet50 & 86/**28** & 41M & 20.5 & 500 & Link \\\\ DETER-DCS (ECCV2020)[31] & ResNet50 & 187/12 & 41M & 22.5 & 500 & Link \\\\ DETER (ECCV2020)[31] & ResNet101 & **52/20** & 60M & 21.9 & – & Link \\\\ DETER-DCS (ECCV2020)[31] & ResNet101 & 25/310 & 60M & 23.7 & – & Link \\\\ ViT-FRCNN (arXiv2020)[32] & –/– & – & 17.8 & – & – \\\\ RelationNet+Net (NeurIPS2020)[35] & ResNeXt101 & –/– & – & 32.8* & – & Link \\\\ RelationNet+MS (NeurIPS2020)[35] & ResNeXt101 & –/– & – & 35.8* & – & Link \\\\ Deformable DETER (ICLR2021)[41] & ResNet50 & 173/19 & 40M & 26.4 & 50 & Link \\\\ Deformable DETER-TB (ICLR2021)[41] & ResNet50 & 173/19 & 40M & 26.8 & 50 & Link \\\\ Deformable DETER-TS (ICLR2021)[41] & ResNet50 & 173/19 & 40M & 28.8 & 50 & Link \\\\ Deformable DETER-TS-IBR-DCN (ICLR2021)[41] & ResNeXt101 & –/– & – & 34.4* & – & Link \\\\ Dynamic DETER (ICCV2021)[44] & ResNet50 & –/– & – & 28.6* & – & – \\\\ Dynamic DETER-DCN (ICCV2021)[44] & ResNeXt101 & –/– & – & 30.3* & – & – \\\\ TSP-FCOS (ICCV2021)[55] & ResNet101 & 255/12 & – & 27.7 & 36 & Link \\\\ TSP-RCNN (ICCV2021)[55] & ResNet101 & 254/9 & – & 29.9 & 96 & Link \\\\ Mask R-CNN (ICCV2021)[57] & Conformer-S/16 & 457.7/– & 56.9M & 28.7 & **12** & Link \\\\ Conditional DETER-DCS (ICCV2021)[65] & ResNet101 & 262/– & 63M & 27.2 & 108 & Link \\\\ SOF-DETER (2022JVCIR) & ResNet50 & –/– & – & 21.7 & – & Link \\\\ DETER (arXiv202191) & ResNet50 & –/– & – & 22.1 & – & – \\\\ TOLO-MS (NC2022)[69] & – & – & -/57 & – & 24.1 & – & – \\\\ Anchor DETER-DCS (AAAI2022) [46] & ResNet101 & –/– & – & 25.8 & 50 & Link \\\\ DETER-DCS (CVPR2022)[68] & ResNet101 & 299/– & 88M & 28.2 & 50 & – \\\\ Conditional DETER V2-DCS (arXiv2022)[66] & ResNet101 & 228/– & 65M & 26.3 & 50 & – \\\\ Conditional DETER V2 (arXiv2022)[66] & Hourglass48 & 521/– & 90M & 32.1 & 50 & – \\\\ FP-DETER-IN (ICLR2022) [80] & – & –/– & **36M** & 26.5 & 50 & Link \\\\ DAB-DETER-DCS (arXiv2022)[67] & ResNet101 & 296/– & 63M & 28.1 & 50 & Link \\\\ Ghostformer-MS (Sensors2022)[93] & GhostNet & –/– & – & 29.2 & 100 & – \\\\ CF-DETER-DCN-TTA (AAAI2022)[47] & ResNeXt101 & –/– & – & 35.1* & – & – \\\\ CBNet V2-TTA (CVPR2022)[79] & Swin Transformer-base & –/– & – & 41.7 & – & Link \\\\ CBNet V2-TTA-BD (CVPR2022)[79] & Swin Transformer-base & –/– & – & 42.2 & – & Link \\\\ DETA (arXiv2022)[74] & ResNet50 & –/13 & 48M & 34.3 & 24 & Link \\\\ DINO (arXiv2022)[88] & ResNet50 & 860/10 & 47M & 32.3 & **12** & Link \\\\ CO-DIM Defending DETER-MS-IN (arXiv2022)[87] & Swin Transformer-large & –/– & – & 43.7 & 36 & Link \\\\ HYNETER (ICASSP2023)[62] & Hyneter-Max & –/– & 247M & 29.8* & – & – \\\\ Doeft (TIP2017)[283] & ResNet101 & 217/14 & 58M & 31.4 & 34 & – \\\\ ConformerDet-MS (TPAMI2023) [58] & Conformer-B & –/– & 147M & 35.3 & 36 & Link \\\\ \\hline YOLOS (NeurIPS2021)[49] & DeiT-base & –/3.9 & 100M & 19.5 & 150 & Link \\\\ DETER(VIT) (arXiv2022)[42] & Swin Transformer-base & –/–9.7 & 100M & 18.3 & 50 & Link \\\\ Deformable DETER(VIT) (arXiv2021)[42] & Swin Transformer-base & –/4.8 & 100M & 34.5 & 50 & Link \\\\ ViDT (arXiv2022)[42] & Swin Transformer-base & –/–9 & 100M & 30.6 & 50 & Link \\\\ DPFT (ECCV2022)[53] & DOT-medium & 67/– & – & 25.5 & 36 & Link \\\\ CenterNet++MS (arXiv2022)[40] & Swin Transformer-large & –/– & – & 38.7* & – & Link \\\\ DETA-OB (arXiv2022)[74] & Swin Transformer-large & –/–4.2 & – & 46.1* & 24 & Link \\\\ Group DETER v2-MS-IN-OB (arXiv2022) [81] & ViT-Huge & –/– & 629M & **48.4*** & – & – \\\\ \\hline Best Results & NA & DETR & FP-DETR & Group DETR v2 & DINO & NA \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE 3: Detection performance (%) for small-scale objects on MS COCO image dataset [2]. The top section shows results for CNN-based techniques, the middle section shows results for mixed architectures, and the bottom section presents from transformer-only networks. DCS: Dilated C5 stage, MS: Multi-scale network, IBR: Iterative bounding box refinement, TS: Two-stage detection, DCN: Deformable convnets, TTA: Test time augmentation, BD: Pre-trained on BigDetection dataset, IN: Pre-trained on ImageNet, OB: Pre-trained on Object-365 [118]. While * shows the results for COCO test-dev, the other values are reported for COCO val set.\n' +
      '\n' +
      '### _Benchmark in Vision Applications_\n' +
      '\n' +
      '이 부분에서는 작은 물체의 탐지 성능이 필수적인 다양한 비전 기반 응용 프로그램을 소개한다. 각 응용 프로그램에 대해 가장 인기 있는 데이터 세트 중 하나를 선택하고 실험 설정에 대한 세부 정보와 함께 성능 메트릭을 보고한다.\n' +
      '\n' +
      '#### 4.2.1 Generic Applications\n' +
      '\n' +
      '일반적인 응용에 대해, 우리는 도전적인 MS COCO 벤치마크에서 모든 소형 물체 검출기의 성능을 평가한다[2]. 이 데이터 세트의 선택은 객체 검출 분야에서 광범위한 수용성과 성능 결과의 접근성에 기초한다. MS COCO 데이터 세트는 80개 카테고리에 걸쳐 약 160K 이미지로 구성된다. 저자들은 COCO 2017 훈련 및 검증 세트를 사용하여 알고리즘을 훈련하는 것이 좋지만 이러한 하위 집합에 국한되지 않는다.\n' +
      '\n' +
      '표 III에서 MS COCO(논문에서 컴파일)에 대한 결과를 보고한 검토 중인 모든 기술의 성능을 조사하고 평가한다. 이 테이블은 백본 아키텍처, GFLOPS/FPS(계산 오버헤드 및 실행 속도 표시), 매개변수 수(모델의 척도 표시), mAP(평균 평균 정밀도: 객체 탐지 성능의 척도), 에포크(추론 시간 및 수렴 특성 표시)에 대한 정보를 제공한다. 추가적으로, 추가 정보를 위해 각 방법의 웹페이지에 대한 링크가 제공된다. 이 방법은 CNN 기반 방법, 혼합 방법, 변압기 전용 방법의 세 그룹으로 분류된다. 각 메트릭에 대한 최상위 수행 방법은 표의 마지막 행에 나와 있다. 이 비교는 실현 가능했을 뿐이라는 점에 유의해야 한다.\n' +
      '\n' +
      '도. 도 11: Convolutional 네트워크와 비교된 트랜스포머 기반 SOTA 소형 물체 검출기에 대한 COCO 데이터세트 [2]에 대한 검출 결과의 예.\n' +
      '\n' +
      '각 특정 메트릭에 대해 보고 된 값이 있는 메서드에 대 한 것입니다. 동점이 있는 경우 평균 평균 정밀도가 가장 높은 방법이 가장 좋은 것으로 간주하였다. 기본 mAP 값은 "COCO 2017 val" 세트에 대한 값인 반면 "COCO 테스트-dev" 세트에 대한 값은 별표로 표시된다. 보고된 mAP는 영역\\(<32^{2}\\)이 있는 객체에만 해당됩니다.\n' +
      '\n' +
      '표 III를 검토할 때, 대부분의 기술은 본질적으로 하이브리드 전략을 채택하는 CNN 및 변압기 아키텍처의 혼합을 사용하는 것으로부터 이익을 얻는다는 것이 분명하다. 특히, 변압기 기반 구조에만 의존하는 그룹 DETR v2는 48.4\\(\\%\\)의 mAP를 달성한다. 그러나 이러한 성능을 달성하기 위해서는 두 개의 대규모 데이터 세트에 대한 사전 훈련 및 다중 규모 학습과 같은 추가 기술의 채택이 필요하다. 수렴도 측면에서 DINO는 12 에포크 후에 안정적인 결과를 얻음으로써 우수한 성능을 보이는 동시에 32.3\\(\\%\\)의 우수한 mAP를 확보한다. 반대로 원래의 DETR 모델은 추론 시간이 가장 빠르고 GFLOPS가 가장 낮다. FP-DETR은 36M 파라미터만으로 가장 가벼운 네트워크를 갖는 것이 특징이다.\n' +
      '\n' +
      '이러한 연구 결과를 바탕으로, 사전 훈련과 다중 척도 학습이 소형 물체 탐지에 가장 효과적인 전략으로 부상한다는 결론을 내릴 수 있다. 이는 다운스트림 작업의 불균형과 작은 객체의 정보 기능이 부족하기 때문일 수 있다.\n' +
      '\n' +
      '그림 12의 보다 상세한 대응물과 함께 두 페이지에 걸쳐 있는 그림 11은 다양한 변압기 및 CNN 기반 방법의 탐지 결과를 보여준다. 이들은 COCO 데이터세트에서 선택된 이미지를 사용하여 서로 비교되고 GitHub 페이지에서 사용할 수 있는 공개 모델을 사용하여 구현된다. 분석 결과, Faster RCNN과 SSD는 작은 물체를 정확하게 탐지하는데 부족함을 알 수 있었다. 구체적으로, SSD는 대부분의 오브젝트들을 놓치거나 잘못된 라벨들과 제대로 위치되지 않은 바운딩 박스들을 갖는 수많은 바운딩 박스들을 생성한다. Faster RCNN이 더 나은 성능을 발휘하지만, 여전히 낮은 신뢰도의 바운딩 박스들을 생성하고 때때로 부정확한 라벨들을 할당한다.\n' +
      '\n' +
      '대조적으로, DETR은 객체들의 수를 과대 추정하는 경향을 가지며, 개별 객체들에 대한 다수의 바운딩 박스들로 이어진다. 일반적으로 DETR은 위양성을 생성하기 쉽다는 점에 주목한다. 마지막으로 평가된 방법 중 CBNet V2가 우수한 성능을 자랑한다. 관찰된 바와 같이, 그것은 때때로 일부 객체들을 잘못 식별할 수 있음에도 불구하고, 그것이 검출하는 객체들에 대해 높은 신뢰도 스코어들을 생성한다.\n' +
      '\n' +
      '#### 4.2.2 항공 이미지에서 작은 개체 탐지\n' +
      '\n' +
      '작은 물체를 감지하는 또 다른 흥미로운 용도는 원격 감지 영역에 있다. 이 분야는 특히 매력적인데, 많은 기관과 연구 기관이 항공 이미지를 통해 지구 표면을 일상적으로 모니터링하여 통계를 위한 국가 및 국제 데이터를 수집하는 것을 목표로 하기 때문이다. 이러한 영상은 다양한 모달리티를 이용하여 획득할 수 있지만, 본 연구에서는 비 SAR 영상만을 대상으로 한다. 이는 SAR 영상이 광범위하게 연구되어 왔으며 독자적인 연구를 할 가치가 있기 때문이다. 그럼에도 불구하고 이 조사에서 논의된 학습 기술은 SAR 영상에도 적용할 수 있다.\n' +
      '\n' +
      '항공 이미지에서 물체는 카메라로부터 상당한 거리로 인해 종종 작게 나타난다. 또한 조감도 뷰는 객체가 이미지 내 어디에나 위치할 수 있기 때문에 객체 탐지 작업에 복잡성을 더합니다. 이러한 응용을 위해 설계된 변압기 기반 검출기의 성능을 평가하기 위해 객체 검출 분야에서 널리 사용되는 벤치마크가 된 DOTA 이미지 데이터 세트[122]를 선택했다. 그림 13은 작은 개체를 특징으로 하는 DOTA 데이터 세트의 일부 샘플 이미지를 표시한다. 데이터 세트에는 미리 정의된 Training 집합, Validation 집합 및 Testing 집합이 포함됩니다. 일반 애플리케이션들과 비교하여, 이 특정 애플리케이션은 상대적으로 덜 주목을 받았다:\n' +
      '\n' +
      '도. 도 11: Convolutional 네트워크와 비교된 트랜스포머 기반 SOTA 소형 물체 검출기에 대한 COCO 데이터세트 [2]에 대한 검출 결과의 예.\n' +
      '\n' +
      '변압기 전문가들 그러나 표 IV에 표시된 바와 같이(결과는 논문으로부터 컴파일됨), ReDet는 이미지넷 데이터 세트에 대한 다중 스케일 학습 전략과 사전 훈련을 통해 자신을 구별하여 가장 높은 정밀도 값(80.89\\(\\%\\))을 달성하고 12개의 훈련 에포크만 필요로 한다. 이는 COCO 데이터 세트 분석에서 얻은 통찰력을 반영하며, 다운스트림 작업의 불균형을 해결하고 작은 객체의 유익한 기능을 포함하여 최적의 성능을 얻을 수 있음을 시사한다.\n' +
      '\n' +
      '#### 4.2.3 의료 이미지에서 작은 개체 탐지\n' +
      '\n' +
      '의료 영상 분야에서 전문의는 종종 기형의 조기 발견 및 식별을 담당한다. 거의 보이지 않거나 작은 비정상적인 세포조차도 놓치면 암과 생명을 위협하는 상태를 포함한 환자에게 심각한 영향을 미칠 수 있다. 이러한 작은 크기의 물체는 이상 징후로 발견될 수 있습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c} \\hline \\hline Model & Backbone & FPS \\(\\uparrow\\) & \\#params\\(\\downarrow\\) & mAP \\(\\uparrow\\) & Epochs\\(\\downarrow\\) & URL \\\\ \\hline Rotated Faster RCNN-MS (NeurIPS2015)[24] & ResNet101 & – & 64M & 67.71 & 50 & Link \\\\ SSD (ECCV2016) [20] & – & – & – & 56.1 & – & Link \\\\ RetinaNet-MS (ICCV2017)[21] & ResNet101 & – & **59M** & 66.53 & 50 & Link \\\\ ROI-Transformer-MS-IN (CVPR2019) [123, 124] & ResNet50 & – & – & 80.06 & **12** & Link \\\\ Yolov5 (2020)[17] & – & **95** & – & 64.5 & – & Link \\\\ ReDet-MS-FPN (CVPR2021)[125] & ResNet50 & – & – & 80.1 & – & Link \\\\ \\hline Q2DETR-MS (arXiv2012)[45] & ResNet101 & – & 63M & 70.02 & 50 & – \\\\ Q2DETR-MS-FT (arXiv2021)[45] & ResNet101 & – & – & 76.23 & 62 & – \\\\ Q2DETR-MS-FPN-FT (arXiv2021)[45] & ResNet50 & – & – & 79.66 & – & – \\\\ SPH-Yolov5 (RS2022)[73] & Swin Transformer-base & 51 & – & 71.6 & 150 & – \\\\ A02-DETR-MS (TCSV2022)[90] & ResNet50 & – & – & 79.22 & – & Link \\\\ MDCT (RS2023)[96] & – & – & – & 75.7 & – & – \\\\ ReDet-MS-IN (arXiv2023)[124] & ViTDet, ViT-B & – & – & **80.89** & **12** & Link \\\\ \\hline Best Results & NA & Yolov5 & RetinaNet & ReDet-MS-IN & ReDet-MS-IN & NA \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE IV: Detection performance (%) for small-scale objects on DOTA image dataset [122]. The top section shows results for CNN-based techniques, the middle section shows results for mixed architectures. MS: Multi-scale network, FT: Fine-tuned, FPN: Feature pyramid network, IN: Pre-trained on ImageNet.\n' +
      '\n' +
      '도. 12: 줌인될 때 샘플 이미지에 대한 검출 결과. 왼쪽에서 첫 번째 행: 입력 이미지, SSD, Faster RCNN, DETR. 왼쪽에서 두 번째 행: ViDT, DETA-OB, DINO, CBNet v2.\n' +
      '\n' +
      '도. 13: DOTA 이미지 데이터세트의 작은 객체들의 예.\n' +
      '\n' +
      '당뇨병 환자, 초기 종양, 혈관 플라크 등을 유지한다. 이 연구 영역의 중요한 성격과 잠재적인 생명을 위협하는 영향에도 불구하고 이 중요한 응용 프로그램에서 작은 물체를 감지하는 것과 관련된 문제를 다룬 연구는 소수에 불과하다. 이 주제에 관심이 있는 사람들을 위해, DeepLesion CT 이미지 데이터세트[111]는 이 특정 데이터세트[126]에 대한 결과의 가용성으로 인해 벤치마크로 선택되었다. 이 데이터 세트의 샘플 이미지는 그림 14에 나와 있다. 이 데이터 세트는 훈련(\\(70\\%\\)), 검증(\\(15\\%\\)), 테스트(\\(15\\%\\)) 세트의 세 가지 집합으로 나뉜다[94]. 표 V는 2단계 및 1단계 검출기 모두에 대해 3개의 변압기 기반 연구의 정확도와 mAP를 비교한다(결과는 논문에서 컴파일된다). MS 트랜스포머는 제한된 경쟁에도 불구하고 이 데이터 세트를 사용하여 최고의 기술로 부상한다. 그것의 주요 혁신은 자기 지도 학습과 계층적 트랜스포머 모델 내의 마스킹 메커니즘의 통합에 있다. 전반적으로 90.3\\(\\%\\)의 정확도와 89.6\\(\\%\\)의 mAP로 이 데이터 세트는 특히 일부 종양 감지 작업이 인간의 눈에 사실상 보이지 않는다는 점을 고려할 때 다른 의료 영상 작업에 비해 덜 어려운 것으로 판단된다.\n' +
      '\n' +
      '#### 4.2.4 수중 이미지에서 작은 객체 탐지\n' +
      '\n' +
      '수중 활동이 증가함에 따라 생태 감시, 장비 유지 관리 및 난파선 어업 모니터링과 같은 목적으로 흐리고 저조도 환경을 모니터링하려는 수요가 증가했다. 물의 산란 및 빛 흡수와 같은 요인은 SOD 작업을 훨씬 더 어렵게 만든다. 그림 15에는 이러한 어려운 환경의 예제 이미지가 표시된다. 변압기 기반 탐지 방법은 작은 물체를 식별하는 데 능숙할 뿐만 아니라 깊은 물에서 발견되는 열악한 이미지 품질뿐만 아니라 각 채널에 대한 광 감쇠율의 차이로 인한 색상 채널의 변화에 대해 강력해야 한다.\n' +
      '\n' +
      '표 VI는 이 데이터 세트에 대한 기존 연구에서 보고된 성능 메트릭을 보여준다(결과는 그들의 논문에서 컴파일됨). HTDet은 이 특정 응용 프로그램에 대해 식별된 단독 변압기 기반 기술이다. MAP에서 3.4\\(\\%\\)의 큰 마진으로 SOTA CNN 기반 방법에 비해 상당히 우수한 성능을 보였다. 그러나 상대적으로 낮은 mAP 점수는 수중 영상에서의 객체 검출이 여전히 어려운 과제로 남아 있음을 확인시켜준다. URPC 2018의 트레이닝 세트는 2901개의 라벨링된 이미지를 포함하고, 테스트 세트는 800개의 라벨링되지 않은 이미지를 포함한다는 점에 주목할 필요가 있다[86].\n' +
      '\n' +
      '#### 4.2.5 Active Milli-Meter Wave Image에서 작은 객체 탐지\n' +
      '\n' +
      '작은 물체들은, 예를 들어, 공항에서 사람의 의복 내에서, 일반적인 RGB 카메라들로부터 쉽게 은닉되거나 숨겨질 수 있다. 따라서 보안을 위해 능동 이미징 기술이 필수적입니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Model & \\(\\#\\)Param\\(\\downarrow\\) & mAP\\({}^{0.5,0.95}\\uparrow\\) & mAP\\({}^{0.5}\\uparrow\\) \\\\ \\hline Faster RCNN (NeurIPS2015)[24] & 33.6M & 16.4 & – \\\\ Cascade RCNN (CVPR2018)[28] & 68.9M & 16 & – \\\\ Dynamic RCNN (ECCV2020)[127] & 41.5M & 13.3 & – \\\\ Voloxity (17) & 61.5M & 19.4 & – \\\\ RollMis (ICASSP2020)[121] & – & – & **74.92** \\\\ \\hline HTDet (RS2023)[86] & **7.7M** & **22.8** & – \\\\ \\hline Best Results & HTDet & HTDet & RoMis \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VI: Detection performance (%) for URPC2018 dataset [121]. The top section shows results for CNN-based techniques, the middle section shows results for mixed architectures.\n' +
      '\n' +
      '도. 14: DeepLesion 이미지 데이터셋의 작은 이상 예[111].\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Model & Accuracy \\(\\uparrow\\) & mAP\\({}^{0.5}\\uparrow\\) \\\\ \\hline Faster RCNN (NeurIPS2015)[24] & 83.3 & 83.3 \\\\ Voloxy [17] & 85.2 & 88.2 \\\\ \\hline DETR (ECCV2020)[31] & 86.7 & 87.8 \\\\ Swin Transformer & 82.9 & 81.2 \\\\ MS Transformer (CIN2022)[94] & **90.3** & **89.6** \\\\ \\hline Best Results & MS Transformer & MS Transformer \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE V: Detection performance (%) for DeepLesion CT image dataset [111]. The top section shows results for CNN-based techniques, the middle section shows results for mixed architectures.\n' +
      '\n' +
      '도. 도 15: URPC2018 이미지 데이터세트에서 저화질 이미지의 예.\n' +
      '\n' +
      '목적. 이러한 시나리오들에서, 다수의 이미지들은 심지어 작은 객체들을 검출할 가능성을 향상시키기 위해 종종 상이한 각도들로부터 캡처된다. 흥미롭게도 의료 영상 분야와 마찬가지로 변압기는 이 특정 응용 분야에 거의 사용되지 않는다.\n' +
      '\n' +
      '본 연구에서는 표 VII와 같이 AMMW 데이터 세트[113]를 사용하여 기존 기술의 탐지 성능에 초점을 맞추었다(결과는 논문에서 컴파일됨). 이 데이터 세트에 대해 변압기와 CNN을 결합하는 유일한 기술로 MATR이 등장했음을 확인했다. 유일한 트랜스포머 기반 기법임에도 불구하고 동일한 백본(ResNet50)으로 mAP\\({}^{0.5}\\)에서는 5.49\\(\\%\\)\\(\\uparrow\\), mAP\\({}^{\\text{th}[0.5,0.95]}\\)에서는 4.22\\(\\%\\)\\(\\uparrow\\)의 SOD 성능을 크게 향상시킬 수 있었다. 그림 16은 MATR을 다른 SOTA CNN 기반 기술과 시각적으로 비교한다. 상이한 각도들로부터의 이미지들을 결합하는 것은 이러한 이미징 접근법 내에서 심지어 작은 객체들을 식별하는 데 크게 도움이 된다. 훈련 및 테스트를 위해 각각 35426 및 4019개의 이미지가 사용되었다[71].\n' +
      '\n' +
      '#### 4.2.6 비디오에서 작은 개체 탐지\n' +
      '\n' +
      '비디오에서 객체 검출 분야는 비디오의 시간적 정보가 검출 성능을 향상시킬 수 있기 때문에 최근 상당한 관심을 받고 있다. SOTA 기법을 벤치마킹하기 위해 이미지넷 VID 데이터셋은 특히 데이터셋의 작은 객체에 초점을 맞춘 결과와 함께 사용되었다. 이 데이터 세트에는 30개의 클래스가 있는 3862개의 훈련 비디오와 555개의 검증 비디오가 포함된다. 표 VIII는 최근에 개발된 여러 변압기 기반 기술의 mAP를 보고한다(결과는 그로부터 컴파일된다).\n' +
      '\n' +
      '도. 도 16: SOTA 소형 객체 검출기들에 대한 AMMW 이미지 데이터세트[113] 상의 검출 결과들의 예들([71]로부터의 그림).\n' +
      '\n' +
      'paper). 트랜스포머가 비디오 객체 검출에 점점 더 많이 사용되고 있지만 SOD에서의 성능은 덜 탐구되어 있다. ImageNet VID 데이터셋에서 SOD 성능을 보고한 방법 중 FAQ를 사용한 Deformable DETR은 mAP\\({}^{\\text{g}[0.5,0.95]}\\)의 경우 13.2 \\(\\%\\)으로 현저히 낮지만 가장 높은 성능을 달성하는 것으로 두드러진다. 이는 비디오 기반 SOD 영역에서 상당한 연구 격차를 강조한다.\n' +
      '\n' +
      '## 5 Discussion\n' +
      '\n' +
      '이 조사 기사에서는 변압기 기반 접근법이 SOD의 문제를 해결할 수 있는 방법을 조사했다. 본 분류법은 트랜스포머 기반 소형 객체 검출기를 객체 표현, 고속 주의(고해상도 및 다중 스케일 특징 맵에 유용), 아키텍처 및 블록 수정, 시공간 정보, 개선된 특징 표현, 보조 기술 및 완전 트랜스포머 기반 검출기의 7가지 주요 범주로 나눈다.\n' +
      '\n' +
      '이 분류법을 CNN 기반 기술에 대한 분류법과 병치할 때 [11] 이러한 범주 중 일부가 겹치는 반면 다른 범주는 변압기 기반 기술에 고유하다는 것을 관찰한다. 특정 전략은 인코더 및 디코더 내의 자체 및 교차 주의 모듈을 통해 수행되는 주의 및 컨텍스트 학습과 같은 변압기에 암시적으로 내장된다. 한편, 멀티 스케일 학습, 보조 작업, 아키텍처 수정 및 데이터 증강은 두 패러다임 모두에서 일반적으로 사용된다. 그러나, CNN이 시간에 따른 3D-CNN, RNN 또는 특징 집성을 통한 시공간 분석을 처리하는 동안, 트랜스포머는 연속적인 공간 및 시간 트랜스포머를 사용하거나 디코더에서 연속적인 프레임에 대한 객체 질의를 업데이트함으로써 이를 달성한다는 점에 유의하는 것이 중요하다.\n' +
      '\n' +
      '우리는 사전 훈련과 다중 규모 학습이 가장 일반적으로 채택되는 전략으로 두드러져 다양한 데이터 세트에 대한 다양한 데이터 세트 성능에 최첨단의 성능에 기여한다는 것을 관찰했다. 데이터 융합은 SOD에 널리 사용되는 또 다른 접근법이다. 비디오 기반 검출 시스템의 맥락에서, 시간 데이터를 수집하고 프레임 특정 검출 모듈에 통합하기 위한 효과적인 방법에 초점이 맞춰져 있다.\n' +
      '\n' +
      '변압기는 소형 물체의 위치 및 분류에 상당한 발전을 가져왔지만 관련된 절충안을 인정하는 것이 중요하다. 여기에는 많은 수의 매개변수(수십억 개), 며칠 동안의 훈련(수백 에포크), 매우 큰 데이터 세트에 대한 사전 훈련(강력한 계산 자원 없이는 실현 가능하지 않음)이 포함된다. 이러한 모든 측면은 다운스트림 작업에 대해 이러한 기술을 훈련하고 테스트할 수 있는 사용자 풀에 제한을 가한다. 이제 효율적인 학습 패러다임과 아키텍처를 가진 경량 네트워크의 필요성을 인식하는 것이 그 어느 때보다 중요하다. 현재 인간의 뇌와 동등한 매개변수의 수에도 불구하고 작은 물체 감지에서의 성능은 여전히 인간의 능력에 상당히 뒤처져 현재 연구에서 상당한 격차를 강조한다.\n' +
      '\n' +
      '또한 그림 11과 12에 제시된 결과를 기반으로 작은 물체 감지에서 누락된 물체 또는 거짓 음성 및 중복 감지된 상자의 두 가지 주요 문제를 확인했다. 누락된 객체의 문제는 토큰에 포함된 제한된 정보에 기인할 가능성이 높다. 이는 고해상도 이미지를 사용하거나 피쳐 피라미드를 강화하여 해결할 수 있지만 지연 시간이 증가하는 단점이 있으며, 이는 잠재적으로 더 효율적이고 가벼운 네트워크를 사용하여 상쇄될 수 있다. 반복 탐지의 문제는 전통적으로 NMS(Non-Maximum Suppression)와 같은 후처리 기법을 통해 관리되어 왔다. 그러나 트랜스포머의 경우, 이 문제는 보조 손실 함수의 사용을 통해 디코더에서 객체 질의 유사성을 최소화함으로써 접근되어야 한다.\n' +
      '\n' +
      '또한 다양한 비전 기반 작업에 걸쳐 SOD(Small Object Detection) 전용 트랜스포머 기반 방법을 사용한 연구를 조사했다. 일반 탐지, 항공 영상에서의 탐지, 의료 영상에서의 이상 탐지, 보안 목적의 능동 밀리미터파 영상에서의 소형 은닉 객체 탐지, 수중 객체 탐지, 동영상에서의 소형 객체 탐지 등이 그것이다. 일반 및 항공 이미지 응용 외에도 변압기는 해상 탐지에 관한 레카반디 _et al._[11]에서 수행된 관찰을 반영하여 다른 응용 분야에서 저개발되어 있다. 이것은 의료 영상과 같은 생명 중요한 분야에서 잠재적으로 중요한 영향을 미칠 수 있는 변압기를 고려할 때 특히 놀라운 일이다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '이 조사 논문은 순수 변압기 기반 기술과 CNN을 통합하는 하이브리드 기술을 모두 포함하여 소형 물체 탐지 작업을 위한 변압기 개발에 초점을 맞춘 60개 이상의 연구 논문을 검토했다. 이러한 기법들은 객체 표현, 고해상도 또는 다중 스케일 특징 맵을 위한 빠른 주의 메커니즘, 아키텍처 및 블록 수정, 시공간 정보, 개선된 특징 표현, 보조 기술 및 완전 변압기 기반 검출의 7가지 다른 관점에서 조사되었다. 이러한 카테고리들 각각은 각자의 장점 세트를 갖는 여러 최신 기술(SOTA)을 포함한다. 또한 이러한 트랜스포머 기반 접근 방식을 CNN 기반 프레임워크와 비교하여 둘 사이의 유사점과 차이점을 논의했다. 또한, 다양한 비전 응용 프로그램에 대해 향후 연구를 위한 벤치마크 역할을 하는 잘 확립된 데이터 세트를 도입했다. 또한, SOD 응용 프로그램에서 사용된 12개의 데이터 세트에 대해 자세히 논의하여 향후 연구 노력에 대한 편의를 제공한다. 향후 연구에서는 각 응용 프로그램에서 작은 물체를 감지하는 것과 관련된 고유한 문제를 탐색하고 해결할 수 있다. 의료 영상 및 수중 영상 분석 스탠드 같은 분야\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Model & Backbone & mAP\\({}^{\\text{g}[0.5,0.95]}\\) \\(\\uparrow\\) \\\\ \\hline Faster RCNN (NeurIPS2015)[24] & ResNet50 & 70.7 & 26.83 \\\\ Cascade RCNN (CVPR2018)[28] & ResNet50 & 74.7 & 27.8 \\\\ TridentNet (ICCV2019)[128] & ResNet50 & 77.3 & 29.2 \\\\ Dynamic RCNN (ECCV2020)[127] & ResNet50 & 76.3 & 27.6 \\\\ Yolov [17] & ResNet50 & 66.7 & 28.48 \\\\ \\hline MATR (TCSV2022)[71] & ResNet50 & **82.16** & **33.42** \\\\ \\hline Best Results & NA & MATR & MATR \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VII: Detection performance (%) for AMWW image dataset [113]. The top section shows results for CNN-based techniques, the middle section shows results for mixed architectures.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Model & Backbone & mAP\\({}^{\\text{g}[0.5,0.95]}\\) \\(\\uparrow\\) \\\\ \\hline Faster RCNN (NeurIPS2015)[24][+SIEA[12] & ResNet50 & 8.5 \\\\ \\hline Deformable-DETR-PT [4] & ResNet50 & 10.5 \\\\ Deformable-DETR[4]+TransV-PDT[103] & ResNet50 & 11 \\\\ DAB-DETR[6]+FAQPT[108] & ResNet50 & 12 \\\\ Deformable-DETR[4]+FAQPT[108] & ResNet50 & **13.2** \\\\ \\hline Best Results & NA & Deformable-DETR+FAQ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VIII: Detection performance (%) for ImageNet VID dataset [98] for small objects. The top section shows results for CNN-based techniques, the middle section shows results for mixed architectures.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:18]\n' +
      '\n' +
      '* [49] Y. Fang, B. Liao, X. Wang, J. Fang, J. Qi, R. Wu, J. Niu, and W. Liu, "You only look at one sequence: Rethinking transformer in vision through object detection," _Advances in Neural Information Processing Systems_, vol. 34, pp. 26 183-26 197, 2021.\n' +
      '* [50] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, "Swin transformer: Hierarchical vision transformer using shifted windows," in _Proceedings of the IEEE/CVF international conference on computer vision_, 2021, pp. 10 012-10 022.\n' +
      '* [51] Y. Li, H. Mao, R. Girshick, and K. He, "Exploring plain vision transformer backbones for object detection," in _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part IX_. Springer, 2022, pp. 280-296.\n' +
      '* [52] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jegou, "Training data-efficient image transformers & distillation through attention," in _International conference on machine learning_. PMLR, 2021, pp. 10 347-10 357.\n' +
      '* [53] P. Chen, M. Zhang, Y. Shen, K. Sheng, Y. Gao, X. Sun, K. Li, and C. Shen, "Efficient decoder-free object detection with transformers," in _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part X_. Springer, 2022, pp. 70-86.\n' +
      '* [54] Y. Zhu, Q. Xia, and W. Jin, "SrdR: a lightweight end-to-end object detection with transformer," _Connection Science_, vol. 34, no. 1, pp. 2448-2465, 2022.\n' +
      '* [55] Z. Sun, S. Cao, Y. Yang, and K. M. Kitani, "Rethinking transformer-based set prediction for object detection," in _Proceedings of the IEEE/CVF international conference on computer vision_, 2021, pp. 3611-3620.\n' +
      '* [56] Z. Tian, C. Shen, H. Chen, and T. He, "Fcos: Fully convolutional one-stage object detection," in _Proceedings of the IEEE/CVF international conference on computer vision_, 2019, pp. 9627-9636.\n' +
      '* [57] Z. Peng, W. Huang, S. Gu, L. Xie, Y. Wang, J. Jiao, and Q. Ye, "Conformer: Local features coupling global representations for visual recognition," in _Proceedings of the IEEE/CVF international conference on computer vision_, 2021, pp. 367-376.\n' +
      '* [58] Z. Peng, Z. Guo, W. Huang, Y. Wang, L. Xie, J. Jiao, Q. Tian, and Q. Ye, "Conformer: Local features coupling global representations for recognition and detection," _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.\n' +
      '* [59] W. Lu, C. Lan, C. Niu, W. Liu, L. Lyu, Q. Shi, and S. Wang, "A cnnn-transformer hybrid model based on cswin transformer for uav image object detection," _IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing_, 2023.\n' +
      '* [60] X. Xu, Z. Feng, C. Cao, M. Li, J. Wu, Z. Wu, Y. Shang, and S. Ye, "An improved swin transformer-based model for remote sensing object detection and instance segmentation," _Remote Sensing_, vol. 13, no. 23, p. 4779, 2021.\n' +
      '* [61] J. Xue, D. He, M. Liu, and Q. Shi, "Dual network structure with interweaved global-local feature hierarchy for transformer-based object detection in remote sensing image," _IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing_, vol. 15, pp. 6856-6866, 2022.\n' +
      '* [62] D. Chen, D. Miao, and X. Zhao, "Hyperter: Hybrid network transformer for object detection," in _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_. IEEE, 2023, pp. 1-5.\n' +
      '* [63] J. Ding, W. Li, L. Pei, M. Yang, C. Ye, and B. Yuan, "Sw-yolov: An anchor-free detector based transformer for sea surface object detection," _Expert Systems with Applications_, p. 119560, 2023.\n' +
      '* [64] H. Yang, Z. Yang, A. Hu, C. Liu, T. J. Cui, and J. Miao, "Unifying convolution and transformer for efficient concealed object detection in passive millimeter-wave images," _IEEE Transactions on Circuits and Systems for Video Technology_, 2023.\n' +
      '* [65] D. Meng, X. Chen, Z. Fan, G. Zeng, H. Li, Y. Yuan, L. Sun, and J. Wang, "Conditional detr for fast training convergence," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2021, pp. 3651-3660.\n' +
      '* [66] X. Chen, F. Wei, G. Zeng, and J. Wang, "Conditional detr v2: Efficient detection transformer with box queries," _arXiv preprint arXiv:2207.08914_, 2022.\n' +
      '* [67] S. Liu, F. Li, H. Zhang, X. Yang, X. Qi, H. Su, J. Zhu, and L. Zhang, "Db-detr: Dynamic anchor boxes are better queries for detr," _arXiv preprint arXiv:2201.12329_, 2022.\n' +
      '* [68] L. He and S. Todorovic, "Destr: Object detection with split transformer," in _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2022, pp. 9377-9386.\n' +
      '* [69] R. Xia, G. Li, Z. Huang, Y. Pang, and M. Qi, "Transformers only look once with nonlinear combination for real-time object detection," _Neural Computing and Applications_, vol. 34, no. 15, pp. 12 571-12 585, 2022.\n' +
      '* [70] T. Liang, X. Chu, Y. Liu, Y. Wang, Z. Tang, W. Chu, J. Chen, and H. Ling, "Cbnet: A composite backbone network architecture for object detection," _IEEE Transactions on Image Processing_, vol. 31, pp. 6893-6906, 2022.\n' +
      '* [71] P. Sun, T. Liu, X. Chen, S. Zhang, Y. Zhao, and S. Wei, "Multi-source aggregation transformer for concealed object detection in millimeter-wave images," _IEEE Transactions on Circuits and Systems for Video Technology_, vol. 32, no. 9, pp. 6148-6159, 2022.\n' +
      '* [72] B. K. Isaac-Medina, C. G. Willococks, and T. P. Breckon, "Multi-view vision transformers for object detection," in _2022 26th International Conference on Pattern Recognition (ICPR)_. IEEE, 2022, pp. 4678-4684.\n' +
      '* [73] H. Gong, T. Mu, Q. Li, H. Dai, C. Li, Z. He, W. Wang, F. Han, A. Tuniyazi, H. Li _et al._, "Swin-transformer-enabled yolov5 with attention mechanism for small object detection on satellite images," _Remote Sensing_, vol. 14, no. 12, p. 2861, 2022.\n' +
      '* [74] J. Ouyang-Zhang, J. H. Cho, X. Zhou, and P. Krahenbuhl, "Nms strikes back," _arXiv preprint arXiv:2212.06137_, 2022.\n' +
      '* [75] Q. Chen, X. Chen, J. Wang, H. Feng, J. Han, E. Ding, G. Zeng, and J. Wang, "Group detr: Fast detr training with group-wise one-to-many assignment," _arXiv preprint arXiv:2207.13085_, vol. 1, no. 2, 2022.\n' +
      '* [76] S. Xu, J. Gu, Y. Hua, and Y. Liu, "Dktrnet: Dual-key transformer network for small object detection," _Computing_, 2023.\n' +
      '* [77] M. Maaz, H. Rasheed, S. Khan, F. S. Khan, R. M. Anwer, and M.-H. Yang, "Class-agnostic object detection with multi-modal transformer," in _17th European Conference on Computer Vision (ECCV)_. Springer, 2022.\n' +
      '* [78] R. Shen, N. Inoue, and K. Shinoda, "Text-guided object detector for multi-modal video question answering," in _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, 2023, pp. 1032-1042.\n' +
      '* [79] L. Cai, Z. Zhang, Y. Zhu, L. Zhang, M. Li, and X. Xue, "Bigdetection: A large-scale benchmark for improved object detector pre-training," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022, pp. 4777-4787.\n' +
      '* [80] W. Wang, Y. Cao, J. Zhang, and D. Tao, "Fp-detr: Detection transformer advanced by fully pre-training," in _International Conference on Learning Representations_, 2022.\n' +
      '* [81] Q. Chen, J. Wang, C. Han, S. Zhang, Z. Li, X. Chen, J. Chen, X. Wang, S. Han, G. Zhang _et al._, "Group detr v2: Strong object detector with encoder-decoder pretraining," _arXiv preprint arXiv:2211.03594_, 2022.\n' +
      '* [82] K. Oksuz, B. C. Cam, S. Kalkan, and E. Akbas, "Thulance problems in object detection: A review," _IEEE transactions on pattern analysis and machine intelligence_, vol. 43, no. 10, pp. 3388-3415, 2020.\n' +
      '* [83] S. Rashidi, R. Temankoon, A. M. Rekavandi, P. Jessadatavornwong, A. Freis, G. Huff, M. Easton, A. Mouritz, R. Hosemzadeh, and A. Babadiashar, "It-tda: Information theory assisted robust unsupervised domain adaptation," _arXiv preprint arXiv:2210.12947_, 2022.\n' +
      '* [84] K. Zeng, Q. Ma, J. Wu, S. Xiang, T. Shen, and L. Zhang, "Niffnet: A non-local feature fusion transformer network for multi-scale object detection," _Neurocomputing_, vol. 493, pp. 15-27, 2022.\n' +
      '* [85] T. Ding, K. Feng, Y. Wei, Y. Han, and T. Li, "Deot: an end-to-end encoder-only transformer object detector," _Journal of Real-Time Image Processing_, vol. 20, no. 1, p. 1, 2023.\n' +
      '* [86] G. Chen, Z. Mao, K. Wang, and J. Shen, "Hddet: A hybrid transformer-based approach for underwater small object detection," _Remote Sensing_, vol. 15, no. 4, p. 1076, 2023.\n' +
      '* [87] Z. Gong, G. Song, and Y. Liu, "Detrs with collaborative hybrid assignments training," _arXiv preprint arXiv:2211.12860_, 2022.\n' +
      '* [88] H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. M. Ni, and H.-Y. Shum, "Dino: Detr with improved denoising anchor boxes for end-to-end object detection," _arXiv preprint arXiv:2203.03605_, 2022.\n' +
      '* [89] F. Li, H. Zhang, S. Liu, J. Guo, L. M. Ni, and L. Zhang, "Dn-detr: Accelerate der training by introducing query denoising," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022, pp. 13 619-13 627.\n' +
      '* [90] L. Dai, H. Liu, H. Tang, Z. Wu, and P. Song, "Ao2-detr: Arbitrary-oriented object detection transformer," _IEEE Transactions on Circuits and Systems for Video Technology_, 2022.\n' +
      '* [91] C. Zhang, L. Liu, X.\n' +
      '\n' +
      '* [92] Y. Tang, B. Wang, W. He, and F. Qian, "Pointdet++: an object detection framework based on human local features with transformer encoder," _Neural Computing and Applications_, pp. 1-12, 2022.\n' +
      '* [93] S. Li, F. Sultonov, J. Tursunboev, J.-H. Park, S. Yun, and J.-M. Kang, "Ghostformer: A ghostnet-based two-stage transformer for small object detection," _Sensors_, vol. 22, no. 18, p. 6939, 2022.\n' +
      '* [94] Y. Shou, T. Meng, W. Ai, C. Xie, H. Liu, and Y. Wang, "Object detection in medical images based on hierarchical transformer and mask mechanism," _Computational Intelligence and Neuroscience_, vol. 2022, 2022.\n' +
      '* [95] S. Dubey, F. Olimov, M. A. Rafique, and M. Jeon, "Improving small objects detection using transformer," _Journal of Visual Communication and Image Representation_, vol. 89, p. 103620, 2022.\n' +
      '* [96] J. Chen, H. Hong, B. Song, J. Guo, C. Chen, and J. Xu, "Mdct: Multi-kernel dilated convolution and transformer for one-stage object detection of remote sensing images," _Remote Sensing_, vol. 15, no. 2, p. 371, 2023.\n' +
      '* [97] T. Ye, W. Qin, Z. Zhao, X. Gao, X. Deng, and Y. Ouyang, "Real-time object detection network in uav-vision based on cnn and transformer," _IEEE Transactions on Instrumentation and Measurement_, vol. 72, pp. 1-13, 2023.\n' +
      '* [98] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein _et al._, "Imagenet large scale visual recognition challenge," _International journal of computer vision_, vol. 115, pp. 211-225, 2015.\n' +
      '* [99] T. Meinhardt, A. Kirillov, L. Leal-Taixe, and C. Feichtenhofer, "Trackformer: Multi-object tracking with transformers," in _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2022, pp. 8844-8854.\n' +
      '* [100] X. Chen, B. Yan, J. Zhu, D. Wang, X. Yang, and H. Lu, "Transformer tracking," in _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2021, pp. 816-8135.\n' +
      '* [101] C. Liu, S. Xu, and B. Zhang, "Aerial small object tracking with transformers," in _2021 IEEE International Conference on Unmanned Systems (ICUSS)_. IEEE, 2021, pp. 954-959.\n' +
      '* [102] L. He, Q. Zhou, X. Li, L. Niu, G. Cheng, X. Li, W. Liu, Y. Tong, L. Ma, and L. Zhang, "End-to-end video object detection with spatial-temporal transformers," in _Proceedings of the 29th ACM International Conference on Multimedia_, 2021, pp. 1507-1516.\n' +
      '* [103] Q. Zhou, X. Li, L. He, Y. Yang, G. Cheng, Y. Tong, L. Ma, and D. Tao, "Transvod: end-to-end video object detection with spatial-temporal transformers," _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.\n' +
      '* [104] H. Wang, J. Tang, X. Liu, S. Guan, R. Xie, and L. Song, "Piseformer: Progressive temporal-spatial enhanced transformer towards video object detection," in _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VIII_. Springer, 2022, pp. 732-747.\n' +
      '* [105] K. A. Hashmi, D. Stricker, and M. Z. Afral, "Spatio-temporal learnable proposals for end-to-end video object detection," _arXiv preprint arXiv:2210.02368_, 2022.\n' +
      '* [106] S.-D. Roh and K.-S. Chung, "Dafa: Diversity-aware feature aggregation for attention-based video object detection," _IEEE Access_, vol. 10, pp. 93 453-93 463, 2022.\n' +
      '* [107] M. Fujitake and A. Sugimoto, "Video sparse transformer with attention-guided memory for video object detection," _IEEE Access_, vol. 10, pp. 65 886-65 900, 2022.\n' +
      '* [108] Y. Cui, "Faq: Feature aggregated queries for transformer-based video object detectors," _arXiv preprint arXiv:2303.08319_, 2023.\n' +
      '* [109] M. Mueller, N. Smith, and B. Ghanem, "A benchmark and simulator for uav tracking," in _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part I 14_. Springer, 2016, pp. 445-461.\n' +
      '* [110] E. Goldman, R. Iferzig, A. Eisenstchatt, J. Goldberger, and T. Hassner, "Precise detection in densely packed scenes," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019, pp. 5227-5236.\n' +
      '* [111] K. Yan, X. Wang, L. Lu, and R. M. Summers, "Deeplesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning," _Journal of medical imaging_, vol. 5, no. 3, pp. 036 501-036 561, 2018.\n' +
      '* [112] "Udacity self-driving car driving data, 2017 transformer," _[https://github.com/udacity/selfdriving-car/tree/master/annotations_](https://github.com/udacity/selfdriving-car/tree/master/annotations_).\n' +
      '* [113] T. Liu, Y. Zhao, Y. Wei, Y. Zhao, and S. Wei, "Concealed object detection for activate millimeter wave image," _IEEE Transactions on Industrial Electronics_, vol. 66, no. 12, pp. 9909-9917, 2019.\n' +
      '* [114] A. Coluccia, A. Fascista, A. Schumann, L. Sommer, A. Dimou, D. Zarpalas, F. C. Akyon, O. Eryuksel, K. A. Ozfuttu, S. O. Altinuc _et al._, "Drone vs-bird detection challenge at ieee avsx2021," in _2021 17th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)_. IEEE, 2021, pp. 1-8.\n' +
      '* [115] K. Li, G. Wan, G. Cheng, L. Meng, and J. Han, "Object detection in optical remote sensing images: A survey and a new benchmark," _ISPRS journal of photogrammetry and remote sensing_, vol. 159, pp. 296-307, 2020.\n' +
      '* [116] G. Cheng, P. Zhou, and J. Han, "Learning rotation-invariant convolutional neural networks for object detection in vhr optical remote sensing images," _IEEE Transactions on Geoscience and Remote Sensing_, vol. 54, no. 12, pp. 7405-7415, 2016.\n' +
      '* [117] Y. Zhang, Y. Yuan, Y. Feng, and X. Lu, "Hierarchical and robust convolutional neural network for very high-resolution remote sensing object detection," _IEEE Transactions on Geoscience and Remote Sensing_, vol. 57, no. 8, pp. 5535-5548, 2019.\n' +
      '* [118] Y. Gao, H. Shen, D. Zhong, J. Wang, Z. Liu, T. Bai, X. Long, and S. Wen, "A solution for densely annotated large scale object detection task," 2019.\n' +
      '* [119] Y. Chen, Z. Zhang, Y. Cao, L. Wang, S. Lin, and H. Hu, "Reppoints v2: Verification meets regression for object detection," _Advances in Neural Information Processing Systems_, vol. 33, pp. 5621-5631, 2020.\n' +
      '* [120] S. Zhang, C. Chi, Y. Yao, Z. Lei, and S. Z. Li, "Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection," in _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2020, pp. 9759-9768.\n' +
      '* [121] W.-H. Lin, J.-X. Zhong, S. Liu, T. Li, and G. Li, "Roimix: proposal-fusion among multiple images for underwater object detection," in _ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_. IEEE, 2020, pp. 2588-2592.\n' +
      '* [122] G.-S. Xia, X. Bai, J. Ding, Z. Zhu, S. Belongie, J. Luo, M. Datcu, M. Pelillo, and L. Zhang, "Dota: A large-scale dataset for object detection in aerial images," in _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2018, pp. 3974-3983.\n' +
      '* [123] J. Ding, N. Xue, Y. Long, G.-S. Xia, and Q. Lu, "Learning roi transformer for oriented object detection in aerial images," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019, pp. 2849-2858.\n' +
      '* [124] L. Wang and A. Tien, "Aerial image object detection with vision transformer detector (videt)," _arXiv preprint arXiv:2301.12058_, 2023.\n' +
      '* [125] J. Han, J. Ding, N. Xue, and G.-S. Xia, "Redet: A rotation-equivariant detector for aerial object detection," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021, pp. 2786-2795.\n' +
      '* [126] J. Li, G. Zhu, C. Hua, M. Feng, B. Bennamoun, P. Li, X. Lu, J. Song, P. Shen, X. Xu _et al._, "A systematic collection of medical image datasets for deep learning," _ACM Computing Surveys_, 2021.\n' +
      '* [127] H. Zhang, H. Chang, B. Ma, N. Wang, and X. Chen, "Dynamic r-cnn: Towards high quality object detection via dynamic training," in _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XV 16_. Springer, 2020, pp. 260-275.\n' +
      '* [128] Y. Li, Y. Chen, N. Wang, and Z. Zhang, "Scale-aware trident networks for object detection," in _Proceedings of the IEEE/CVF international conference on computer vision_, 2019, pp. 6054-6063.\n' +
      '* [129] H. Wu, Y. Chen, N. Wang, and Z. Zhang, "Sequence level semantics aggregation for video object detection," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2019, pp. 9217-9225.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>