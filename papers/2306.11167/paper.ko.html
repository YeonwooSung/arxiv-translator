<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'Red Herrings에 의해 수정되는 대용량 언어 모델: OnlyConnect Wall Dataset을 이용한 창의적 문제 해결 및 Einstellung 효과 탐색\n' +
      '\n' +
      ' Saeid Alavi Naeini\\({}^{1,3,4}\\) Raeid Saqur\\({}^{2,4}\\) Mozhgan Saeidi\\({}^{2,4,6}\\) John Giorgi\\({}^{2,4,5}\\) Babak Taati\\({}^{1,2,3,4}\\)\n' +
      '\n' +
      '토론토 재활연구소 연연구소\n' +
      '\n' +
      '토론토대학교 컴퓨터과학과\n' +
      '\n' +
      '토론토대학교 의공학연구소\n' +
      '\n' +
      '({}^{4}\\) AI 벡터 연구소\n' +
      '\n' +
      '토론토대학교 세포생명분자연구센터\n' +
      '\n' +
      '스탠포드대학교 의학데이터과학과\n' +
      '\n' +
      '{saeid.alavi, john.giorgi}@mail.utoronto.ca\n' +
      '\n' +
      'raeidsaqur@cs.toronto.edu, mozhgans@stanford.edu, babak.taati@uhn.ca\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '인간 모방 AI에 대한 탐구는 시작부터 AI 연구에서 지속적인 주제였다. 대형 언어 모델(LLM)의 최신 코호트의 기술적 진화와 신흥 역량은 학계를 넘어 문화적 시대정신으로 주제를 다시 활성화했다. 최근 NLP 평가 벤치마크 작업은 인간 모방 행동(예: BIG-bench의 \'인간 유사 행동\' 작업)의 일부 측면을 테스트하지만, 크리에이티브 문제 해결 능력을 검사하는 것은 거의 없다. 인간의 창의적 문제 해결은 단어들 간의 (이질적인) 연결을 창의성의 척도로 주로 사용하는 표준화된 테스트와 함께 인지 신경 과학에서 잘 연구된 주제이다. 오해의 소지가 있는 자극에 노출되면 _붉은 청어_라고 하는 교란자가 _고정 효과_ 및 Einstellung 패러다임을 통해 이러한 작업에서 인간 수행을 방해한다. 인지 신경 과학 연구에서 이러한 고정은 실험 참가자를 후속 단어 단편 또는 단서에 직교적으로 유사한 잘못된 단어에 사전 노출함으로써 실험적으로 유도된다. 영국의 인기 퀴즈쇼 Only Connect의 _Connecting Wall_ 세그먼트는 본질적으로 내장적이고 의도적인 붉은 청어가 있는 Mednick의 RAT(Remote Associates Test) 공식을 모방하여 LLM의 인지 신경 과학에서 고정 효과와 Einstellung 패러다임을 탐색하고 연구하는 이상적인 대리 작업이다. 본 논문에서는 새로운 OCN(Only Connect Wall) 데이터셋을 제시하고, 이종의 연결에 의해 단서 단어를 그룹화하고 각 그룹의 올바른 오픈 지식 도메인 연결을 식별하는 것과 같은 창의적인 문제 해결 작업에 대해 선택된 사전 훈련 언어 모델과 LLM을 평가한 결과를 보고한다. 우리는 언어 모델에서 레드허링 가설을 추가로 분석하기 위해 OCW-랜덤화, OCW-워드넷의 두 가지 추가 데이터 세트를 합성적으로 생성한다. 데이터 세트에 대한 코드 및 링크는 [https://github.com/TaatiTeam/OCW](https://github.com/TaatiTeam/OCW)에서 사용할 수 있습니다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '다양한 도메인 및 다운스트림 작업[78; 10]에 걸쳐 최첨단 대형 언어 모델(LLM)[91]의 놀라운 능력은 인공 일반 지능(AGI)[5; 14] 및 인간 모방 AI [31] 시스템과의 비교에 박차를 가했다. 트랜스포머 기반 [69] 사전 훈련된 상황 인식 언어 모델 (PLMs) [52; 17; 40; 36; 53]의 출현에서 2018년부터 2020년까지 점점 더 큰 (모수 수십 개) 현재 및 최신 코호트 LMs [59; 77; 57; 89; 16; 67; 18] OpenAI의 GPT 시리즈 [13], 특히 ChatGPT [49] 및 GPT-4 [48]에 앞장선 LMs [59; 77; 57; 89; 16; 67; 18]의 놀라운 능력은 그러한 비교를 정당화한다. 이러한 LLM의 평가를 표준화하기 위해 MMLU[27], BIG-bench[66], HELM[38], Global-Bench[65]를 포함한 여러 자연어 처리(NLP) 벤치마크가 제안되었다. 이러한 벤치마크 아래의 작업 인벤토리는 개방형(작업 유형) 및 동적(롤링 추가)입니다. 이러한 태스크들의 서브세트가 인간 이니셔티브 지능을 테스트하는 것을 목표로 하는 반면(예를 들어, BIG-bench의 _인간-유사 행동_ 카테고리 아래에 나열된 19개의 태스크), 인간-유사 지능의 특징인 _창의적 문제 해결_ 능력에 대한 테스트는 없다[44].\n' +
      '\n' +
      '인간의 창의적 문제 해결은 인지 신경 과학 및 인간 행동 과학 문헌에서 잘 연구된 주제이다. 이러한 연구 및 방법은 (단어)연관 유창성을 사용하여 창의성을 객관적으로 모델링하고 테스트한다[44, 9]. 이러한 맥락에서 실증적 연구는 일반적으로 Mednick의 정액 원격 연관 테스트(RAT)의 변형인 단일 또는 연속 단어 연관 테스트를 사용한다[45]. 이러한 테스트는 이질적일 수 있는 연관(예를 들어, 동의어, 의미, 합성)을 사용하여 제시된 단어 그룹 간의 연결 또는 링크를 찾는 것을 수반한다[86, 43]. 예를 들어 _[테니스, 동일, 머리]_ 라는 큐 단어를 고려 합니다. 이 삼중항에서의 정확한 연결은: 의미론적 링크(_tennis match_), 동의어(_same match_), 및 복합화(_match head_)에 의해 연결되는 _Match_이다. 또한, 단어 연결은 또한 비유성의 정도(예를 들어, _Star-Actress_ vs. _Star-Planet_)에서 변할 수 있다. 및 추상성(예를 들어, _Humor-Sense_ vs. _Apple-Tree_). 인간의 경우, 이러한 창의적인 문제 해결 능력은 오답에 노출됨으로써 저해된다[61, 62, 85]. 즉, _고정 효과_라고 지칭되는 발견[34, 82]. 밀접하게 관련된 유사한 개념은 새로운 문제를 해결할 때 이전 경험의 부정적인 영향을 가정하는 _Einstellung effect_[42]이다.\n' +
      '\n' +
      '고착 효과를 살펴본 연구들은 오답으로 의도된 단서 단어(오인 자극)를 제시함으로써 고착을 유도한다[61]. "붉은 청어"로 명명되거나, RAT[45].와 같은 창의적 문제 해결 과제를 시도하기 전에 참여자들을 붉은 청어에 미리 노출시킴으로써 고착을 유도한다. 인간의 인지에서 부정 전이 학습에서 많은 연구는 기억의 간접적 또는 암시적 측정에 대한 사전 학습의 부정적인 영향에 의해 재청어에 사전 노출을 포함하는 RAT 고정 현상을 설명하려고 시도한다[63]. 이러한 부정적인 전이 효과는 후속 테스트 단어 단편과 직교적으로 유사한 단어를 적색 청어로 사용하여 입증되고 연구되었다[63]. 직관적으로, 재청어는 참가자들을 올바른 응답에 필요한 기억 검색(또는 헤비언 용어에 의한 부정확한 신경 경로 다운)으로부터 벗어나게 하고 잘못된 연결에 고정시킨다[60]. 붉은 청어를 더 회수할 수 있게 함으로써 창의적인 문제 해결에서의 고정을 높일 수 있다. 따라서 창의적 문제 해결은 부정적인 전달 효과로 인해 재청어에 의해 검색이 저하되는 간접 기억 척도의 한 유형으로 생각할 수 있다. 상기 _붉은 청어 검색 가설_은 붉은 청어를 더 회수할 수 있게 하는 요인이 창의적인 문제 해결 성능을 감소시켜야 한다고 명시하며,\n' +
      '\n' +
      '도 1: 지반-진실 그룹핑(행) 및 연결(마지막 열)이 있는 _Only Connect_ 벽의 예. Red Herrings_에는 서로 다른 연결된 그룹의 **Gala**, Churchill 및 Chelsea와 같은 직교적으로 동일한 단어가 포함됩니다. **Gala**: _Gala night, Apples_, _Swimming gala_, Churchill: _Advert Animals, English Playwrights_ 및 Chelsea: _Gay Villages, Boots_. 벽 **A**(왼쪽 상단)에서 처칠, 마르크스, 카스트로의 단서는 벽 내의 역사적 인물에 그럴듯한 고정을 유도하는 오해의 소지가 있는 자극을 제공한다.\n' +
      '\n' +
      '(p<0.05). 그러한 두 가지 요인은 반복과 맥락이다. 다음 결과는 적색 청어의 기억 강도가 고정 효과의 크기를 결정한다고 명시한다[8].\n' +
      '\n' +
      '본 연구에서는 인간의 인지신경과학(_fixations, negative transfer learning, redherring memory retrieval hypothesis_) 이론을 LLM과 자연어 처리의 맥락에서 병행하여 연구한다. 부정 전이 학습이 AI 연구에서 관찰되고 연구되었지만[76; 22], 이러한 연구의 맥락은 통계적 분포 측정 및 컴퓨터 비전과 같은 엄격한 기계 학습 하위 영역으로 제한된다. AI 연구에서 이러한 구체적인 개념들의 관계를 체계적으로 살펴본 연구는 없었다. 우리의 주요 기여는 다음과 같습니다.\n' +
      '\n' +
      '1. OCW(Only Connect Wall) 데이터셋과 창의적 문제 해결 과제.영국의 인기 퀴즈쇼 Only Connect[81;3]의 문제와 인간의 수행 결과를 큐레이션하여 _창의적 문제 해결_ 과제를 평가하기 위한 새로운 데이터셋을 소개한다. 구체적으로, 태스크가 16개(16개)의 단서 단어를 4개(4개)의 연결된 그룹으로 그룹화하고 올바른 연결의 이름을 지정하는 것을 수반하는 쇼의 _연결 벽_ 세그먼트가 있다(도 1). 제시된 단어들은 역사, 장소, 유명인, 도구, 문화 참조와 같은 개방형 지식 검색과 이질적인 연관성을 가지고 있다. 이러한 \'벽\'에는 설계에 따라 붉은 청어 또는 오해의 소지가 있는 자극이 포함되어 있으므로 이 데이터 세트는 창의적인 문제 해결을 위한 LLM을 평가할 때 RAT 테스트를 위한 아날로그 프록시이다. 섹션 SS2는 데이터 세트에 대한 자세한 설명을 제공합니다.\n' +
      '\n' +
      '2. 기본 LLMs 평가의 실험, 결과 및 주요 결과. 정적 임베딩에서 PLMs에서 LLMs까지 NLP 모델 세트를 평가하고 OCW 데이터 세트의 작업을 해결할 수 없음을 보여준다. 우리의 연구 결과는 SOTA LLM(예: GPT-4[48])이 전문가 인간 기준보다 훨씬 더 나쁜 성능을 발휘하고 다소 놀랍게도 소수의 샷 인-컨텍스트 학습에서 인-컨텍스트 예제의 수를 늘리는 것이 효과적이지 않다는 것을 보여준다. 섹션 SS3 및 SS4는 세부 정보를 제공합니다.\n' +
      '\n' +
      '## 2 Only Connect Walls Dataset\n' +
      '\n' +
      '여기서는 퀴즈 쇼의 _벽 연결_ 세그먼트(일반적으로 세 번째 라운드)에 중점을 둡니다. 각 벽에는 4개의 연결된 단어가 있는 4개의 그룹으로 분류되어야 하는 16개의 뒤섞인 단어 단서가 들어 있다. 일단 그룹이 형성되면 참가자들은 또한 각 그룹의 항목들 사이의 올바른 연결 또는 관계를 식별해야 한다. 각 벽에는 하나의 올바른 솔루션만 있지만, 퍼즐은 다른 범주에 들어갈 수 있는 여러 개의 청어 단서와 여러 개의 단서에 맞는 재청어 범주를 포함하도록 설계되었다. 그림 1은 몇 가지 일반적인 붉은 청어를 강조하는 쇼에서 해결된 샘플 벽을 보여준다.\n' +
      '\n' +
      '### 데이터 집합 컬렉션 및 구조\n' +
      '\n' +
      'OCW 데이터 세트에는 쇼의 15개 시즌부터 총 618개의 연결 벽 퍼즐과 솔루션이 포함되어 있다. 각 쇼 에피소드에는 두 개의 벽이 있습니다. 시즌당 총 벽 수는 방영된 시즌 에피소드의 수에 따라 달라진다. 팬 웹사이트 1에서 벽을 긁어내고 모든 에피소드를 시청하여 인간 성능 결과(그룹화 및 연결 작업에 대한)를 수동으로 선별했다. 그림 2는 자체 설명 객체 키와 코멘트를 사용하여 JSON 형식으로 데이터 세트의 상위 수준 구조를 보여준다.\n' +
      '\n' +
      '각주 1: 기본 원본은 [https://ocdb.cc](https://ocdb.cc) [6]의 Only Connect 팬 웹사이트였습니다.\n' +
      '\n' +
      '### 작업 및 평가 메트릭\n' +
      '\n' +
      '**작업 1(그룹화)** 및 **작업 2(연결)** 의 두 데이터 세트 작업은 퀴즈 쇼의 인간 참가자 작업과 동일합니다. 우리는 해결된 벽의 수, 올바른 그룹의 수(최대값)의 6가지 메트릭을 통해 작업 1(그룹화)을 평가한다. 벽당 4개), 조정된 상호 정보(AMI)[71], 조정된 랜드 인덱스(ARI)[28], 가금류 맬로우즈 점수(FMS)[21], 와서스타인 거리(WD)[54], \\((0,1)\\) 범위로 정규화된 예측 및 지상 진실 레이블[88; 70]이다.\n' +
      '\n' +
      '우리는 마찬가지로 태스크 2(Connections)를 정확한 스트링 매칭, ROUGE-1 F1[39], 및 BERTScore F1[90]의 세 가지 메트릭으로 평가한다. 정확한 매치는 가장 엄격하며, 예측된 연결이 지상-진실과 동일할 때 1의 점수를 할당하고 그렇지 않을 때 0의 점수를 할당한다. ROUGE-1 F1은 이 기준을 완화하는데, 모델의 예측된 연결에서 지상 진실 토큰의 비율이 높고 비 지상 진실 토큰의 비율이 낮을 때 크다. BERTScore F1은 유사하지만 이 기준을 더욱 완화하여 _semantically_ 유사(그러나 동일하지 않음) 예측 토큰에 대해 0이 아닌 점수를 할당합니다. 이 세 가지 메트릭을 함께 사용하면 하나의 메트릭 단독보다 작업 2에서 모델 성능에 대한 보다 총체적인 관점을 제공합니다. 경험적으로 우리는 \\(\\geq 0.5\\)의 ROUGE-1 또는 BERTScore F1이 예측된 연결이 _correct_로 간주될 가능성이 있음을 나타낸다(표 1). BERTScore는 최종 점수에 영향을 미치는 많은 매개변수를 가지고 있으며, 재현성을 위해 해시코드가 생성 및 보고된다.\n' +
      '\n' +
      '작업 2의 작업 1에 대한 각 평가 메트릭은 벽당, 에피소드당, 시즌당 또는 전체 테스트 세트에 대해 계산할 수 있다. 본 논문의 전체 테스트 세트(SS4)에 대한 결과를 제시한다. 데이터셋을 열차 집합(62개 벽), 검증 집합(62개 벽), 테스트 집합(494개 벽)으로 분할하였다. 데이터 세트의 주요 목표는 LLM의 제로 및 소수의 창의적 문제 해결 능력을 평가하는 것이며, 따라서 테스트 세트의 크기를 훈련 또는 검증 세트보다 훨씬 더 크게 설정하도록 선택한다.\n' +
      '\n' +
      '## 3 실험: 언어 모델 평가\n' +
      '\n' +
      '이 섹션에서는 데이터 세트에 대한 기본 결과를 제공하는 데 사용되는 방법 및 모델에 대해 설명합니다. 과제 1(Grouping)의 경우, 클래식과 사전 학습된 언어 모델(PLM)의 단어 임베딩(SS3.1)과 LLM(SS3.2)을 사용한 소수의 샷 인 컨텍스트 학습(ICL)에 대한 클러스터링 기법을 사용한다. 작업 2(연결)의 경우, LLM(SS3.2)과 함께 소수의 샷 ICL을 사용하는 기준 결과만을 제공한다.\n' +
      '\n' +
      '도 2: OCW 데이터세트의 JSON 구조. 하나의 잘린 예가 도시되어 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c} Predicted Connection & Ground-truth Connection & Exact Match & ROUGE-1 F1 & BERTScore F1 \\\\ \\hline Types of numbers & Types of numbers & 1.00 & 1.00 & 1.00 \\\\ Slang terms for money & Slang for money & 0.00 & 0.86 & 0.79 \\\\ Types of trees & Trees & 0.00 & 0.50 & 0.63 \\\\ Bridges in London & Thames bridges & 0.00 & 0.40 & 0.31 \\\\ Medieval occupations & Chaucer characters & 0.00 & 0.00 & 0.15 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 예측된 및 접지-진실 연결의 예들 및 선택된 메트릭들에 따른 그들의 성능. 동일한 문자열을 제외한 모든 항목에 대해 정확한 일치는 0입니다. 경험적으로 우리는 \\(\\geq 0.5\\)의 ROUGE-1/BERTScore F1이 예측된 연결이 _correct_일 가능성이 있음을 나타낸다.\n' +
      '\n' +
      '### 작업 1: Word Embeddings를 사용한 그룹화\n' +
      '\n' +
      '_grouping task_ evaluation (SS2.2)를 위해 각 벽에 있는 16개의 단서 단어에 대한 단어 임베딩에 클러스터링 알고리즘을 사용하여 각 벽에 대한 4개의 지상-진실 그룹에 대해 후속적으로 평가되는 4개의 예측 그룹으로 그룹화한다. Banilla \\(k\\)-means (with \\(k=4\\)) 클러스터링 알고리즘 [25]는 각 예측 그룹이 4개의 단어를 갖는 것을 보장하지 않으므로 제약 클러스터링과 같은 변형을 사용한다.\n' +
      '\n' +
      'ClusteringSemi-supervised constrained clustering[72;7]은 사용자가 원하는 분할(우리의 경우, 4개의 그룹)에 대한 기존 지식을 가질 때 사용된다. 여기서는 그룹화를 위해 클러스터 크기가 4인 _최소 비용 흐름 네트워크_ 클러스터링 접근 방식 [12]을 채택합니다. 우리의 예비 분석에서는 군집링 결과가 실행 전반에 걸쳐 약간의 변화를 나타냄을 보여주었다. 이 약간의 불일치는 군집 중심의 초기화에 기인할 수 있다. 이 문제를 해결하고 신뢰할 수 있는 결과를 보장하기 위해 각각 고유한 시드 및 16개 단어 단서의 무작위 순서가 있는 16개 실행 전반에 걸쳐 결과의 평균 및 분산(표 3)을 보고한다. [47; 19]: (1) 제약 클러스터링을 적용하기 전에 단어에 대한 쌍별 유사 정보를 포함하는 자기 유사성 행렬을 구성하고 (2) 제약 클러스터링을 적용하기 전에 주성분 분석(PCA)[58]과 t-분산 확률적 이웃 임베딩(t-SNE)[68]을 사용하여 차원 축소를 수행했다. 어떤 접근법도 원시 임베딩의 클러스터에 비해 성능을 향상시키지 않았으며 간결성을 위해 결과는 포함되지 않았다.\n' +
      '\n' +
      '정적 단어 임베딩은 잘 알려진 두 가지 고전적인 단어 임베딩 모델인 GloVe[51] 및 FastText[23]을 사용했으며, 둘 다 Flair 라이브러리를 통해 액세스된다(표 2). 우리는 두 개의 FastText 모델을 사용했는데, 하나는 Common Crawl 코퍼스에서 미리 훈련되고 다른 하나는 위키피디아에서 훈련되었다. 데이터셋에서 접한 전체 단서의 약 10%가 어휘 외(OOV)였다. 하나의 통합된 임베딩을 얻기 위해 여러 단어로 구성된 단서에 대한 평균 풀링으로 OOV 사례의 상당 부분(~80%)을 해결했다. 나머지 OOV 인스턴스의 경우 정적 임베딩과 BytePair 인코딩된[26] 서브 워드를 결합했다.\n' +
      '\n' +
      'PLMs 우리는 범용 PLMs(BERT[17], RoBERTa[40], DistilBERT[56], ELMo[52])뿐만 아니라 Sentence Transformers(MPNet[64], E5[75]; 표 2 참조)를 탐색하였다. 문맥 임베딩 유무에 따른 성능을 평가하였다. 2 문맥에 따라 데이터 세트의 일부 단서가 서로 다른 의미를 가진 서로 다른 벽에 나타날 수 있다. 일 예로서, 상기 단어는\n' +
      '\n' +
      '도 3: 정적 임베딩 및 문맥 임베딩 둘 다를 갖는 최상의 수행 모델(E5BASE)을 사용하는 태스크 1(그룹화)에 대한 해결된 벽(wall_id="8cde"). **왼쪽**: 정적 임베딩을 사용하여 벽을 해결했습니다. **오른쪽**: 컨텍스트 임베딩을 사용하여 해결되지 않은 벽입니다. t-SNE를 이용한 임베딩의 2D 투영이 도시된다. 색상과 모양은 실제 군집에 해당하고 회색 볼록 영역은 예측된 군집에 해당한다. 전설은 각 그룹에 대한 지상 진실 연결을 보여준다.\n' +
      '\n' +
      '"갈라"는 각각 다른 의미와 관련된 세 개의 별개의 벽에서 발견되었다: _사과_, _수영_ 및 _야_. 문맥 임베딩은 단서 간의 맥락적 의미 유사성을 포착하는 것을 목표로 했다. 그들은 벽 속의 16개의 단서를 유사문장으로 결합함으로써 생성되었다. 우리는 위치 순서를 설명하기 위해 각 벽에 대해 16개의 다른 런에 걸쳐 단어 순서를 무작위로 섞는다. 우리는 이러한 인조 문장이 (문맥을 유도하기 위한) 유효한 영어 구문 문장이 아니라는 점에 주목한다. 우리는 전체 단서의 집합적 의미를 포착하기 위해 여러 단어로 구성된 단서에 대한 임베딩을 생성하기 위해 평균 풀링을 사용했다.\n' +
      '\n' +
      '### 작업 2: LLMs를 사용 하 여 Few-shot In-context Learning (ICL)을 사용 하는 연결\n' +
      '\n' +
      'LLM이 있는 소수의 샷 ICL은 NLP [13]에서 수행자이자 광범위하게 적용할 수 있는 패러다임으로 등장했다. 제안된 데이터셋에서 이 방법의 성능을 평가하기 위해 GPT-3.5-turbo와 GPT-4 [48]에 대한 몇 개의 샷 프롬프트를 설계했는데, 이는 현재 사용 가능한 가장 강력한 수행 LLMs 중 하나이다. 3 태스크 1(그룹화, SS2.2)의 경우 프롬프트는 일부 자연어 지침, 훈련 집합에서 해결된 벽의 여러 예, 현재 예제의 16개의 단서를 무작위로 정렬한 것으로 구성된다. 작업 2(연결)의 경우 16개의 단서 대신 프롬프트에는 연결이 없는 해결된 벽이 포함됩니다 (그림 4).\n' +
      '\n' +
      '각주 3: 예비 실험에서 LLaMA[67]와 같은 오픈 소스 LLM이 제대로 작동하지 않고 일반적으로 작업 지침을 따르지 않는다는 것을 발견했다.\n' +
      '\n' +
      '검증 세트에 대한 프롬프트를 개발하고 테스트 세트에 대한 최종 성능을 보고했다. 맥락 내 예들은 열차 세트로부터 랜덤하게 선택되며; 동일한 예들이 모든 테스트 입력들에 걸쳐 사용된다. 우리는 0, 1, 3, 5, 10개의 문맥 내 예제들로 실험한다. 필요한 경우 간단한 후처리를 LLMs 출력에 적용합니다. 예를 들어, 태스크 1과 태스크 2 모두에서 그룹 및 연결에 대해 각각 최대 4개의 예측을 취하고 모델이 4.4 미만으로 출력하는 경우 빈 문자열로 최대 4개까지 패딩하여 결과를 가능한 한 재현 가능하게 만들기 위해 온도=0을 설정하고 03/01/2023 GPT-3.5-터보 스냅샷과 03/14/2023 GPT-4 스냅샷을 사용했다. 최대 출력 길이는 144 토큰으로 설정됩니다. OpenAI API의 다른 모든 하이퍼 매개 변수는 기본값으로 남겨집니다. [2]. 프롬프트는 지침 라이브러리 [1]에 따라 설계되었다.\n' +
      '\n' +
      '각주 4: [https://github.com/TaatiTeam/OCW](https://github.com/TaatiTeam/OCW)의 모든 후처리 단계에 대한 코드 베이스를 참조하세요.\n' +
      '\n' +
      '## 4 결과 및 논의\n' +
      '\n' +
      '### 작업 1: 그룹화 결과\n' +
      '\n' +
      '표 3에서 임베딩 클러스터링 기법들은 태스크 1(Grouping)에 대한 몇몇 정적 임베딩 베이스라인들의 성능을 보고한다. E5BASE는 가장 성능이 좋은 모델이었고 평균적으로 해결되었다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Model & \\multicolumn{2}{c}{\\# Parameters} & Version & Accessed via \\\\ \\hline \\hline  & \\multicolumn{4}{c}{_Word Embeddings_} \\\\ \\hline BPEmb [26] & En & – & en & Flair [4] \\\\ GloVe [51] & 6B & – & glove & Flair \\\\ FastText [23] & Crawl & – & crawl & Flair \\\\  & News & – & news & Flair \\\\ \\hline  & \\multicolumn{4}{c}{_Pre-trained Language Models (PLMs)_} \\\\ \\hline ELMo\\({}_{\\text{LARGE}}\\)[52] & \\multicolumn{2}{c}{94M} & large & Flair [4] \\\\ DistilBERT\\({}_{\\text{BASE}}\\)[56] & uncased & 66M & distilbert-base-uncased & HuggingFace [83] \\\\ BERT\\({}_{\\text{BASE}}\\)[17] & uncased & 110M & bert-base-uncased & HuggingFace \\\\ BERT\\({}_{\\text{LARGE}}\\)[40] & uncased & 340M & bert-large-uncased & HuggingFace \\\\ RoBERT\\({}_{\\text{LARGE}}\\)[40] & \\multicolumn{2}{c}{355M} & \\multicolumn{2}{c}{forebear-large} & HuggingFace \\\\ \\hline  & \\multicolumn{4}{c}{_Sentence Transformers_} \\\\ \\hline all-mpnet\\({}_{\\text{BASE}}\\)[64] & V2 & 110M & sentence-transformers/all-mpnet-base-v2 & HuggingFace \\\\ E5BASE [75] & V2 & 110M & infloat/e6-base-v2 & HuggingFace \\\\ E5\\({}_{\\text{LARGE}}\\) & V2 & 340M & infloat/e5-large-v2 & HuggingFace \\\\ \\hline  & \\multicolumn{4}{c}{_Large Language Models (LLM)_} \\\\ \\hline GPT-3.5-turbo & \\multicolumn{2}{c}{–} & gpt-3.5-turbo-0301 & OpenAI API \\\\ GPT-4 & – & gpt-4-0314 & OpenAI API \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 실험에 사용된 기준선과 모델에 대한 세부 정보.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '3-샷), 잘못된 형식의 출력(모든 예측 그룹의 4.4%)과 환각 단서(6.6%)를 포함하는 일반적인 오류 원인을 발견했다.\n' +
      '\n' +
      '놀랍게도, 더 많은 문맥 내 예들(1번에서 10번 샷)은 성능을 향상시키지 못했다. 이 관찰에 대한 한 가지 가능한 설명은 가능한 연결 유형이 매우 다양하기 때문에 문맥 내 예제의 주요 이점은 단일 예만 필요할 가능성이 있는 작업을 수행하는 방법을 설명하는 대신 예상되는 출력 형식을 설명하는 것이다. 이것은 ICL이 데모를 활용하는 두 가지 별개의 메커니즘으로 생각되는 _작업 학습_ 대 _작업 인식_의 개념과 관련이 있다[50, 32]. 많은 단서들은 틈새 주제 영역들에 대한 개방된 도메인, 불가사의한, 문화적 및 친밀한 지식(예를 들어, "_전문가 누커 플레이어_", "_여성 라디오 1 DJs_")을 요구하며, 이는 사전 암기 없이는 도움이 되지 않을 것이다. 맥락 내 예시에서 정형적으로 유사한 단서 단어의 존재는 그 자체로 재청어 역할을 할 수 있고, 그럴듯하게 부정적 전이 학습을 유도할 수 있다. 흥미로운 미래 방향은 검색 증강 모델의 평가일 것이다[24, 37, 11, 29]. 이는 매우 특정한 주제 영역에 대한 그룹을 해결할 수 있을 것이다.\n' +
      '\n' +
      '### 작업 2: 연결 결과\n' +
      '\n' +
      '<그림 6>에서는 Task 2(Connections)에 대한 결과를 제시한다. 일반적으로, GPT-4는 특히 0-샷 체제에서 GPT-3.5-터보보다 성능이 우수하다. GPT-4에 대한 성능은 개선들이 때때로 작지만(예를 들어), 증가하는 수의 문맥 내 예들과 함께 단조롭게 개선된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline  & \\# In-context Examples & WD \\(\\downarrow\\) & FMS \\(\\uparrow\\) & ARI \\(\\uparrow\\) & AMI \\(\\uparrow\\) & \\# Solved Walls & \\# Correct Groups \\\\ \\hline GPT-3.5-turbo & 0-shot & \\(82.5\\) & \\(34.0\\) & \\(18.4\\) & \\(21.6\\) & \\(0\\) & \\(114\\) \\\\  & 1-shot & \\(82.3\\) & \\(34.4\\) & \\(18.2\\) & \\(21.2\\) & \\(0\\) & \\(123\\) \\\\  & 3-shot & \\(80.9\\) & \\(36.8\\) & \\(21.3\\) & \\(24.7\\) & \\(0\\) & \\(140\\) \\\\  & 5-shot & \\(80.6\\) & \\(37.3\\) & \\(22.0\\) & \\(25.4\\) & \\(2\\) & \\(149\\) \\\\  & 10-shot & \\(81.2\\) & \\(36.1\\) & \\(20.4\\) & \\(24.0\\) & \\(2\\) & \\(137\\) \\\\ \\multirow{2}{*}{GPT-4} & 0-shot & \\(75.8\\) & \\(41.5\\) & \\(27.2\\) & \\(30.7\\) & \\(6\\) & \\(239\\) \\\\  & 1-shot & \\(73.4\\) & \\(43.7\\) & \\(29.7\\) & \\(33.5\\) & \\(4\\) & \\(262\\) \\\\  & 3-shot & \\(73.7\\) & \\(43.9\\) & \\(29.9\\) & \\(33.6\\) & \\(5\\) & \\(272\\) \\\\  & 5-shot & \\(\\mathbf{72.9}\\) & \\(43.4\\) & \\(29.1\\) & \\(32.8\\) & \\(\\mathbf{7}\\) & \\(269\\) \\\\  & 10-shot & \\(73.6\\) & \\(42.8\\) & \\(28.5\\) & \\(32.3\\) & \\(3\\) & \\(249\\) \\\\ \\hline Human Performance & – & – & – & – & \\(285\\,/\\,494\\) & \\(1405\\,/\\,1976\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 대용량 언어 모델을 이용한 태스크 1(그룹화)에 대한 결과. WD: Wasserstein Distance. FMS: 폴크스 맬로우스 점수입니다. ARI: 조정된 랜드 인덱스입니다. NMI: Normalized Mutual Information. **대담한**: 최상의 점수입니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline  & WD \\(\\downarrow\\) & FMS \\(\\uparrow\\) & ARI \\(\\uparrow\\) & AMI \\(\\uparrow\\) & \\# Solved Walls & \\# Correct Groups \\\\ \\hline \\hline \\multicolumn{6}{c}{_Classic Word Embeddings_} \\\\ \\hline GloVe & \\(84.9\\pm.4\\) & \\(31.5\\pm.3\\) & \\(14.4\\pm.3\\) & \\(17.6\\pm.4\\) & \\(0\\pm 0\\) & \\(68\\pm 4\\) \\\\ FastText (Crawl) & \\(84.2\\pm.5\\) & \\(32.1\\pm.3\\) & \\(15.2\\pm.3\\) & \\(18.4\\pm.4\\) & \\(0\\pm 0\\) & \\(80\\pm 4\\) \\\\ FastText (News) & \\(85.5\\pm.5\\) & \\(30.4\\pm.2\\) & \\(13.0\\pm.2\\) & \\(15.8\\pm.3\\) & \\(0\\pm 0\\) & \\(62\\pm 3\\) \\\\ \\hline \\multicolumn{6}{c}{_Pre-trained Language Models (PLMs)_} \\\\ \\hline ELMoLAGE & \\(86.3\\pm.6\\) & \\(29.5\\pm.3\\) & \\(11.8\\pm.4\\) & \\(14.5\\pm.4\\) & \\(0\\pm 0\\) & \\(55\\pm 4\\) \\\\ DistilBERTBASE & \\(86.7\\pm.6\\) & \\(29.1\\pm.2\\) & \\(11.3\\pm.3\\) & \\(14.0\\pm.3\\) & \\(0\\pm 0\\) & \\(49\\pm 4\\) \\\\ BERT\\({}_{\\text{LARGE}}\\) & \\(88.3\\pm.5\\) & \\(26.5\\pm.2\\) & \\(8.2\\pm.3\\) & \\(10.3\\pm.3\\) & \\(0\\pm 0\\) & \\(33\\pm 2\\) \\\\ BERT\\({}_{\\text{BASE}}\\) & \\(89.5\\pm.4\\) & \\(25.1\\pm.2\\) & \\(6.4\\pm.3\\) & \\(8.1\\pm.4\\) & \\(0\\pm 0\\) & \\(22\\pm 2\\) \\\\ RoBERT\\({}_{\\text{LARGE}}\\) & \\(88.4\\pm.4\\) & \\(26.7\\pm.2\\) & \\(8.4\\pm.3\\) & \\(9.4\\pm.4\\) & \\(0\\pm 0\\) & \\(29\\pm 3\\) \\\\ \\hline \\multicolumn{6}{c}{_Sentence Transformers_} \\\\ \\hline all-mpnetBASE & \\(86.3\\pm.4\\) & \\(29.4\\pm.3\\) & \\(11.7\\pm.4\\) & \\(14.3\\pm.5\\) & \\(0\\pm 0\\) & \\(50\\pm 4\\) \\\\ ES\\({}_{\\text{LARGE}}\\) & \\(84.4\\pm.7\\) & \\(32.3\\pm.4\\) & \\(15.4\\pm.5\\) & \\(18.5\\pm.6\\) & \\(0\\pm 0\\) & \\(76\\pm 5\\) \\\\ ES\\({}_{\\text{BASE}}\\) & \\(\\mathbf{83.8}\\pm.6\\) & \\(\\mathbf{33.1}\\pm.3\\) & \\(\\mathbf{16.3}\\pm.4\\) & \\(\\mathbf{19.5}\\pm.4\\) & \\(\\mathbf{1}\\pm\\mathbf{0}\\) & \\(\\mathbf{89}\\pm\\mathbf{6}\\) \\\\ \\hline Human Performance & – & – & – & – & \\(285\\,/\\,494\\) & \\(1405\\,/\\,1976\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 정적 임베딩을 사용하여 태스크 1(그룹화)에서 선택된 모델의 결과. WD: Wasserstein Distance. FMS: 폴크스 맬로우스 점수입니다. ARI: 조정된 랜드 인덱스입니다. NMI: Normalized Mutual Information. 16개의 무작위 종자에 대한 평균 \\(\\pm\\) 표준 편차가 표시된다. **대담한**: 최상의 점수입니다.\n' +
      '\n' +
      '\\(<0.01\\)). 예상대로 두 모델의 정확한 일치 점수는 낮다(\\(<15\\%\\). 이는 모델의 예측과 지상 진실 사이의 미미한 차이조차도 0(예를 들어, "Made _of_ rubber" vs. "Made _from_ rubber")의 점수를 초래할 것이라는 사실에 의해 설명된다. 이러한 이유로, 우리는 또한 ROUGE-1 및 BERTScore F1 점수(SS2.2)를 보고한다. 완벽한 비교는 아니지만 이러한 결과를 인간의 성능과 맥락화할 수 있으며, 이는 테스트 세트에서 \\(\\sim\\)80%로 올바르게 추측된 연결의 비율로 기록했다. 퀴즈 쇼 Only Connect는 추측된 연결에서 약간의 작은 편차를 허용하여 정확한 일치보다 ROUGE 및 BERTScore와의 비교가 더 적합하다. 우리의 결과는 41-45% F1에서 소수의 ICL(GPT-4, 10-샷)로 달성된 최상의 성능이 인간 성능보다 훨씬 낮음을 시사한다. 마지막으로, 모델 오류의 공통 원인은 (1) 모델이 그렇게 하도록 지시되지 않았고 (2) 맥락 내 예가 이렇게 포맷되지 않았음에도 불구하고 "벽난로 도구(Spade, Brush, Poker, Tongs)"와 같은 가장 성능이 좋은 모델에 대해 예측된 모든 연결의 8.2%에서 발생하는)에 단서를 포함하는 것이다.\n' +
      '\n' +
      '더 복잡한 후처리 또는 프롬프트 전략(예: "사상 사슬" [79], "사상 트리" [87])은 이러한 문제를 완화하고 성능을 향상시킬 수 있다. 그러나 이러한 복잡한 프롬프트 전략을 OCW 데이터 세트에 적용하는 것은 문제를 중간 단계로 분해해야 하고 이러한 중간 추론 단계가 취해야 하는 수 또는 특성이 불분명하기 때문에 자명하지 않다. 향후 작업을 위해 OCW 데이터 세트에 해당 응용 프로그램을 그대로 둡니다.\n' +
      '\n' +
      '### Red-Herring의 효과: 추가 데이터 세트, 실험 및 분석\n' +
      '\n' +
      '언어 모델에 대한 _red-herring 가설_을 분석하기 위해 추가 제거 실험을 설계하고 수행했다. 원래 OCW 데이터 세트에는 red-herring이 _설계에 따라_ 산만자로 포함 됩니다. Red-herring의 존재를 줄이기 위해 OCW에서 OCW-Randomized와 OCW-WordNet의 두 가지 추가 데이터 세트를 생성한다. 목표, 구성 및 기타 세부 사항은 부록 SSC.1에 나와 있다.\n' +
      '\n' +
      'OCW-랜덤화에서 테스트 세트의 벽 사이에서 그룹을 무작위로 교체하여 붉은 청어의 존재를 희석하여 각 벽에 내재된 고의적인 방해자 그룹을 무효화한다. Redherring을 모두 제거 하 여 OCW-WordNet에서 그룹화 작업을 더욱 단순화 합니다. 이것은 영어 어휘 데이터베이스 워드넷[46, 20]에서 하위-상징어(또는 하이프어-하이퍼니임) 단어 계층구조 및 동의어를 사용함으로써 달성된다. 따라서 표 5의 결과는 왼쪽에서 오른쪽으로 붉은 청어의 비율이 감소하고 우리의 가설에 의해 LLM에 대한 작업 단순성이 증가하는 데이터 세트에 대한 결과를 제시한다. 결과는 테스트 세트에서 적색 청어가 감소함에 따라 GPT-3.5-터보 및 GPT-4 성능이 크게 증가하는 예상과 일치한다.\n' +
      '\n' +
      '## 5 관련 작업\n' +
      '\n' +
      '인간과 같은 언어 능력에 대해 언어 모델을 평가하기 위해 다양한 데이터 세트 및 태스크가 제안되었다. 이러한 태스크들의 앞선 예들은 _워드 센스 명확화_(WSD)[55],\n' +
      '\n' +
      '도 6: GPT-3.5-터보 및 GPT-4를 사용한 태스크 2(연결)에 대한 결과. 참고로, 인간의 성능은 대략 80%이다(정확하게 응답된 연결의 부분). 가독성을 위해 GPT-3.5-turbo의 경우 \\(\\max(\\text{BERTScore},0)\\)를 보고한다.\n' +
      '\n' +
      'Winograd schema challenge[35] and _word sense induction_ (WSI)[80]. WSD는 특정 컨텍스트 내에서 단어의 정확한 의미 또는 감각을 결정하는 것을 목표로 한다. WSI는 단어의 문맥적 사용 패턴에 따라 단어를 다른 감각 또는 의미 범주로 자동으로 클러스터링하는 데 중점을 둔다. GLUE [74] 및 SuperGLUE [73]과 같은 벤치마크는 언어 모델을 평가하기 위해 이러한 고전적인 NLP 작업을 집계하고 표준화하는 것을 목표로 한다. PLM(예를 들어, BERT 변이체) 및 LLM의 첫 번째 세대는 대부분 2020년대까지 이러한 태스크에서 인간 수준의 성능을 해결하거나 달성했다[41].\n' +
      '\n' +
      '현대 LLM의 인간 모방 능력을 평가하기 위해 BIG-bench[66] 및 HumanEval[15]와 같은 최근 벤치마크에서 보다 도전적인 작업이 제안되었다. **BIG 벤치** 는 보다 포괄적이고 개방적이며 동적 (롤링 기반으로 추가 된 작업) 평가 벤치마크를 제공 하 여 기존 벤치마크의 한계를 해결 하는 것을 목표로 합니다. 특히 _인간과 유사한 행동_ 을 대상으로 하는 작업 집합을 포함 하 여 다양 한 작업을 포함 합니다. **HumanEval** 은 문서 문자열 [15]에서 코드 합성의 기능적 정확성을 측정 하기 위한 평가 집합입니다. 이 벤치마크에는 간단한 소프트웨어 인터뷰 질문에 필적하는 언어 이해력, 알고리즘 및 간단한 수학을 평가하는 164개의 독창적인 프로그래밍 문제가 포함된다. 이러한 최근 벤치마크에는 광범위한 LLM 기능을 평가하는 광범위한 복잡한 작업이 포함되어 있지만, 여기서는 둘 중 어느 것도 LLM에서 창의적인 문제 해결 또는 창의성과 그 장애를 구체적으로 측정하는 것을 목표로 하지 않기 때문에 이러한 작업과 직교한다.\n' +
      '\n' +
      '## 6 제한 사항 및 향후 작업\n' +
      '\n' +
      '모든 기계 학습 데이터 세트, 특히 LLM의 성능을 평가하도록 설계된 데이터 세트와 마찬가지로 OCW 데이터 세트는 몇 가지 제한 사항이 있다. 첫째, 맥락적 접근의 성능은 모델에 단서가 제공되는 순서에 따라 크게 달라질 수 있음을 주목하였다. 이를 완화하기 위해(그리고 가능한 경우), 우리는 16개의 무작위 단서 정렬에 걸쳐 모델을 평가한다. 비용으로 인해 이 순서에 대한 GPT-3.5-터보 및 GPT-4의 민감도를 평가하지 않았으며, 향후 작업은 여러 무작위 정렬에 걸쳐 성능을 보고해야 한다. 둘째, 퀴즈 쇼 _Only Connect_의 특성상, 데이터세트 내의 단서, 그룹 및 연결은 서양-(및 구체적으로 영국-) 중심(예를 들어, "_닥터후 동반자_", "_영어 크리켓 주장_", "_아일랜드 카운티_")인 경향이 있다. 따라서 OCW 데이터 세트에 대한 성능은 서양 영어 이외의 언어 또는 문화로 추론되지 않을 수 있다. 실제로 GPT-3.5/4 [84]와 같은 LLM의 _US_ 중심 편향은 _UK_ 중심 OCW 데이터 세트에 대한 낮은 성능을 부분적으로 설명할 수 있다. 여러 언어로 영감을 받은 벽과 향후 작업에서 다양한 문화 및 하위 문화에서 파생된 단서가 있는 _Only Connect_ 를 추가하기를 바랍니다. 마지막으로, 벽이 ocdb.cc와 같은 팬 사이트에서 텍스트로 공개적으로 사용 가능하다는 점을 감안할 때 GPT와 같은 LLM의 훈련 세트에 포함될 가능성이 항상 있다. 그러나 그룹화 및 연결 작업에 대한 성능이 낮기 때문에 가능성이 낮다고 생각합니다. OCW와 같은 공개적으로 사용 가능한 데이터 세트의 테스트 세트가 LLM의 훈련 세트에 "유출"되는 것을 방지하는 것은 흥미롭고 열린 문제로 남아 있다. 우리는 데이터 세트를 압축된 형식으로 배포하여 이러한 누출에 대한 기본 조치를 취했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l} \\hline \\hline  & \\multicolumn{3}{c}{**OCW**} & \\multicolumn{3}{c}{**OCW-Randomized**} & \\multicolumn{3}{c}{**OCW-WordNet**} \\\\ \\cline{3-8}  & & \\# Solved Walls & \\# Correct Groups & \\# Solved Walls & \\# Correct Groups & \\# Solved Walls & \\# Correct Groups \\\\ \\hline GPT-3.5-turbo & 0-shot & 0 & 114 & 5 & 274 & 337 & 1522 \\\\  & 1-shot & 0 & 123 & 12 & 315 & 320 & 1400 \\\\  & 3-shot & 0 & 140 & 10 & 306 & 415 & 1748 \\\\  & 5-shot & **2** & **149** & 16 & **337** & 415 & 1759 \\\\  & 1-shot & 2 & 137 & **17** & 333 & **428** & **1800** \\\\ GPT-4 & 0-shot & 6 & 239 & 59 & 595 & **471** & **1926** \\\\  & 1-shot & 4 & 262 & 57 & 644 & 304 & 1581 \\\\  & 3-shot & 5 & **272** & 62 & 649 & 279 & 1537 \\\\  & 5-shot & **7** & 269 & **68** & **655** & 298 & 1584 \\\\  & 10-shot & 3 & 249 & 55 & 614 & 378 & 1742 \\\\ \\hline Human Performance & 285 / 494 & 1405 / 1976 & – & – & – & – \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 두 개의 추가 테스트 데이터 세트 OCW-랜덤화 및 OCW-WordNet을 사용하여 작업 1(그룹화)에 대한 LLMs 성능 결과를 통합한 결과 원본 OCW 테스트 세트(가장 왼쪽 열)와 병치된 벽에서 빨간색 헤링의 존재가 _왼쪽_에서 _오른쪽_으로 감소했다. 기본 메트릭만 표시 됩니다 (부록 SC의 세부 정보 및 전체 결과). **대담한**: 최상의 점수입니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Microsoft guidance: A guidance language for controlling large language models. [https://github.com/microsoft/guidance](https://github.com/microsoft/guidance). 2023.\n' +
      '* [2] OpenAI API completions reference. [https://platform.openai.com/docs/api-reference/completions](https://platform.openai.com/docs/api-reference/completions). 2023.\n' +
      '* [3] Only Connect. Television show, 2008-2020. Created by Presentable, RDF Television and Parasol, Presented by Victoria Coren Mitchell.\n' +
      '* [4] A. Akbik, T. Bergmann, D. Blythe, K. Rasul, S. Schweter, and R. Vollgraf. FLAIR: An easy-to-use framework for state-of-the-art NLP. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)_, pages 54-59, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4010.\n' +
      '* [5] S. Altman. Planning for AGI and beyond. _OpenAI Blog, February_, 2023.\n' +
      '* [6] C. Anotado, T. Ruddle, and J. Halbur. The Only Connect Database.\n' +
      '* [7] S. Basu, I. Davidson, and K. Wagstaff. _Constrained clustering: Advances in algorithms, theory, and applications_. CRC Press, 2008.\n' +
      '* [8] Z. Beda and S. M. Smith. Chasing red herrings: Memory of distractors causes fixation in creative problem solving. _Memory & Cognition_, 46:671-684, 2018.\n' +
      '* [9] M. Benedek and A. C. Neubauer. Revisiting Mednick\'s model on creativity-related differences in associative hierarchies. Evidence for a common path to uncommon thought. _The Journal of creative behavior_, 47(4):273-289, 2013.\n' +
      '* [10] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. _ArXiv preprint_, abs/2108.07258, 2021.\n' +
      '* [11] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. van den Driessche, J. Lespiau, B. Damoc, A. Clark, D. de Las Casas, A. Guy, J. Menick, R. Ring, T. Hennigan, S. Huang, L. Maggiore, C. Jones, A. Cassirer, A. Brock, M. Paganini, G. Irving, O. Vinyals, S. Osindero, K. Simonyan, J. W. Rae, E. Elsen, and L. Sifre. Improving language models by retrieving from trillions of tokens. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 2206-2240. PMLR, 2022.\n' +
      '* [12] P. S. Bradley, K. P. Bennett, and A. Demiriz. Constrained k-means clustering. _Microsoft Research, Redmond_, 20(0):0, 2000.\n' +
      '* [13] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.\n' +
      '* [14] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with GPT-4. _ArXiv preprint_, abs/2303.12712, 2023.\n' +
      '* [15] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. _ArXiv preprint_, abs/2107.03374, 2021.\n' +
      '\n' +
      '* [16] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. PaLM: Scaling language modeling with pathways. 2022. _ArXiv preprint_, abs/2204.02311, 2022.\n' +
      '* [17] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171-4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\n' +
      '* [18] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, B. Zoph, L. Fedus, M. P. Bosma, Z. Zhou, T. Wang, Y. E. Wang, K. Webster, M. Pellat, K. Robinson, K. S. Meier-Hellstern, T. Duke, L. Dixon, K. Zhang, Q. V. Le, Y. Wu, Z. Chen, and C. Cui. Glam: Efficient scaling of language models with mixture-of-experts. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 5547-5569. PMLR, 2022.\n' +
      '* [19] K. Ethayarajh. How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 55-65, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1006.\n' +
      '* [20] C. Fellbaum. Wordnet. In _Theory and applications of ontology: computer applications_, pages 231-243. Springer, 2010.\n' +
      '* [21] E. B. Fowlkes and C. L. Mallows. A method for comparing two hierarchical clusterings. _Journal of the American statistical association_, 78(383):553-569, 1983.\n' +
      '* [22] J. Gao, L. Ge, K. Li, H. Q. Ngo, and A. Zhang. On handling negative transfer and imbalanced distributions in multiple source transfer learning. In _Proceedings of the 13th SIAM International Conference on Data Mining, May 2-4, 2013. Austin, Texas, USA_, pages 261-269. SIAM, 2013. doi: 10.1137/1.9781611972832.29.\n' +
      '* [23] E. Grave, P. Bojanowski, P. Gupta, A. Joulin, and T. Mikolov. Learning word vectors for 157 languages. In _Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)_, Miyazaki, Japan, 2018. European Language Resources Association (ELRA).\n' +
      '* [24] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang. Retrieval augmented language model pre-training. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 3929-3938. PMLR, 2020.\n' +
      '* [25] J. A. Hartigan and M. A. Wong. Algorithm as 136: A k-means clustering algorithm. _Journal of the royal statistical society. series c (applied statistics)_, 28(1):100-108, 1979.\n' +
      '* [26] B. Heinzerling and M. Strube. BPEmb: Tokenization-free pre-trained subword embeddings in 275 languages. In _Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)_, Miyazaki, Japan, 2018. European Language Resources Association (ELRA).\n' +
      '* [27] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.\n' +
      '* [28] L. Hubert and P. Arabie. Comparing partitions. _Journal of classification_, 2:193-218, 1985.\n' +
      '* [29] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. A. Yu, A. Joulin, S. Riedel, and E. Grave. Few-shot learning with retrieval augmented language models. _ArXiv preprint_, abs/2208.03299, 2022.\n' +
      '\n' +
      '* [30] A. Jacovi, A. Caciularu, O. Goldman, and Y. Goldberg. Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks. _ArXiv preprint_, abs/2305.10160, 2023.\n' +
      '* [31] M. I. Jordan. Artificial intelligence--the revolution hasn\'t happened yet. _Harvard Data Science Review_, 1(1):1-9, 2019.\n' +
      '* [32] J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, and R. McHardy. Challenges and applications of large language models. _ArXiv preprint_, abs/2307.10169, 2023.\n' +
      '* [33] G. Ke, D. He, and T. Liu. Rethinking positional encoding in language pre-training. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.\n' +
      '* [34] N. Kohn and S. M. Smith. Partly versus completely out of your mind: Effects of incubation and distraction on resolving fixation. _The Journal of Creative Behavior_, 43(2):102-118, 2009.\n' +
      '* [35] H. Levesque, E. Davis, and L. Morgenstern. The winograd schema challenge. In _Thirteenth international conference on the principles of knowledge representation and reasoning_, 2012.\n' +
      '* [36] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 7871-7880, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703.\n' +
      '* [37] P. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W. Yih, T. Rocktaschel, S. Riedel, and D. Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.\n' +
      '* [38] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, et al. Holistic evaluation of language models. _ArXiv preprint_, abs/2211.09110, 2022.\n' +
      '* [39] C.-Y. Lin. ROUGE: A package for automatic evaluation of summaries. In _Text Summarization Branches Out_, pages 74-81, Barcelona, Spain, 2004. Association for Computational Linguistics.\n' +
      '* [40] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. _ArXiv preprint_, abs/1907.11692, 2019.\n' +
      '* [41] N. Lu, S. Liu, R. He, and K. Tang. Large language models can be guided to evade AI-generated text detection. _ArXiv preprint_, abs/2305.10847, 2023.\n' +
      '* [42] A. S. Luchins and E. H. Luchins. Rigidity of behavior: A variational approach to the effect of Einstellung. 1959.\n' +
      '* [43] M. Marko, D. Michalko, and I. Riecansky. Remote associates test: An empirical proof of concept. _Behavior research methods_, 51:2700-2711, 2019.\n' +
      '* [44] S. Mednick. The associative basis of the creative process. _Psychological review_, 69(3):220, 1962.\n' +
      '* [45] S. A. Mednick. The remote associates test. _The Journal of Creative Behavior_, 1968.\n' +
      '* [46] G. A. Miller. WordNet: A lexical database for English. In _Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, February 23-26, 1992_, 1992.\n' +
      '* [47] R. Navigli. Word sense disambiguation: A survey. _ACM computing surveys (CSUR)_, 41(2):1-69, 2009.\n' +
      '* [48] OpenAI. GPT-4 technical report. _ArXiv preprint_, abs/2303.08774, 2023.\n' +
      '\n' +
      '* [49] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.\n' +
      '* [50] J. Pan, T. Gao, H. Chen, and D. Chen. What in-context learning "learns" in-context: Disentangling task recognition and task learning. In _Annual Meeting of the Association for Computational Linguistics_, 2023.\n' +
      '* [51] J. Pennington, R. Socher, and C. Manning. GloVe: Global vectors for word representation. In _Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1532-1543, Doha, Qatar, 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1162.\n' +
      '* [52] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep contextualized word representations. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 2227-2237, New Orleans, Louisiana, 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202.\n' +
      '* [53] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _J. Mach. Learn. Res._, 21:140:1-140:67, 2020.\n' +
      '* [54] A. Ramdas, N. Garcia Trillos, and M. Cuturi. On wasserstein two-sample testing and related families of nonparametric tests. _Entropy_, 19(2):47, 2017.\n' +
      '* [55] O. Sainz, O. L. de Lacalle, E. Agirre, and G. Rigau. What do language models know about word senses? zero-shot wsd with language models and domain inventories. _ArXiv preprint_, abs/2302.03353, 2023.\n' +
      '* [56] V. Sanh, L. Debut, J. Chaumond, and T. Wolf. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. _ArXiv preprint_, abs/1910.01108, 2019.\n' +
      '* [57] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagne, A. S. Luccioni, F. Yvon, M. Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. _ArXiv preprint_, abs/2211.05100, 2022.\n' +
      '* [58] J. Shlens. A tutorial on principal component analysis. _arXiv:1404.1100_, 2014.\n' +
      '* [59] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. _ArXiv preprint_, abs/1909.08053, 2019.\n' +
      '* [60] S. M. Smith. The constraining effects of initial ideas. _Group creativity: Innovation through collaboration_, pages 15-31, 2003.\n' +
      '* [61] S. M. Smith and S. E. Blankenship. Incubation effects. _Bulletin of the Psychonomic Society_, 27(4):311-314, 1989.\n' +
      '* [62] S. M. Smith and S. E. Blankenship. Incubation and the persistence of fixation in problem solving. _The American journal of psychology_, pages 61-87, 1991.\n' +
      '* [63] S. M. Smith and D. R. Tindell. Memory blocks in word fragment completion caused by involuntary retrieval of orthographically related primes. _Journal of Experimental Psychology: Learning, Memory, and Cognition_, 23(2):355, 1997.\n' +
      '* [64] K. Song, X. Tan, T. Qin, J. Lu, and T. Liu. Mpnet: Masked and permuted pre-training for language understanding. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.\n' +
      '* [65] Y. Song, C. Cui, S. Khanuja, P. Liu, F. Faisal, A. Ostapenko, G. I. Winata, A. F. Aji, S. Cahyawijaya, Y. Tsvetkov, et al. GlobalBench: A benchmark for global progress in natural language processing. _ArXiv preprint_, abs/2305.14716, 2023.\n' +
      '\n' +
      '* [66] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _ArXiv preprint_, abs/2206.04615, 2022.\n' +
      '* [67] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al. LLaMA: Open and efficient foundation language models. _ArXiv preprint_, abs/2302.13971, 2023.\n' +
      '* [68] L. Van der Maaten and G. Hinton. Visualizing data using t-SNE. _Journal of machine learning research_, 9(11), 2008.\n' +
      '* [69] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 5998-6008, 2017.\n' +
      '* [70] C. Villani. _Topics in optimal transportation_, volume 58. American Mathematical Soc., 2021.\n' +
      '* ICML, 2009.\n' +
      '* July 1, 2001_, pages 577-584. Morgan Kaufmann, 2001.\n' +
      '* [73] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d\'Alche-Buc, E. B. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 3261-3275, 2019.\n' +
      '* [74] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019.\n' +
      '* [75] L. Wang, N. Yang, X. Huang, B. Jiao, L. Yang, D. Jiang, R. Majumder, and F. Wei. Text embeddings by weakly-supervised contrastive pre-training. _ArXiv preprint_, abs/2212.03533, 2022.\n' +
      '* [76] Z. Wang, Z. Dai, B. Poczos, and J. G. Carbonell. Characterizing and avoiding negative transfer. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019_, pages 11293-11302. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.01155.\n' +
      '* [77] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.\n' +
      '* [78] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, et al. Emergent abilities of large language models. _ArXiv preprint_, abs/2206.07682, 2022.\n' +
      '* [79] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022.\n' +
      '\n' +
      '* [80] L. White, R. Togneri, W. Liu, and M. Bennamoun. Finding word sense embeddings of known meaning. In _Computational Linguistics and Intelligent Text Processing: 19th International Conference, CICLing 2018, Hanoi, Vietnam, March 18-24, 2018, Revised Selected Papers, Part II_, pages 3-16. Springer, 2023.\n' +
      '* [81] Wikipedia contributors. Only connect -- Wikipedia, the free encyclopedia. [https://en.wikipedia.org/w/index.php?title=Only_Connect&oldid=1157929067](https://en.wikipedia.org/w/index.php?title=Only_Connect&oldid=1157929067), 2023. [Online; accessed 7-June-2023].\n' +
      '* [82] J. Wiley. Expertise as mental set: The effects of domain knowledge in creative problem solving. _Memory & cognition_, 26:716-730, 1998.\n' +
      '* [83] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame, Q. Lhoest, and A. Rush. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 38-45, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6.\n' +
      '* [84] R. Wolfe and A. Caliskan. American== white in multimodal language-and-image ai. In _Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society_, pages 800-812, 2022.\n' +
      '* [85] G. Wood and J. Pennington. Encoding and retrieval from long-term storage. _Journal of Experimental Psychology_, 99(2):243, 1973.\n' +
      '* [86] C.-L. Wu, S.-Y. Huang, P.-Z. Chen, and H.-C. Chen. A systematic review of creativity-related studies applying the remote associates test from 2000 to 2019. _Frontiers in psychology_, 11:573432, 2020.\n' +
      '* [87] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. _ArXiv preprint_, abs/2305.10601, 2023.\n' +
      '* [88] B. Yin, M. Zhao, L. Guo, and L. Qiao. Sentence-BERT and k-means based clustering technology for scientific and technical literature. In _2023 15th International Conference on Computer Research and Development (ICCRD)_, pages 15-20. IEEE, 2023.\n' +
      '* [89] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al. OPT: Open pre-trained transformer language models. _ArXiv preprint_, abs/2205.01068, 2022.\n' +
      '* [90] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi. Bertscore: Evaluating text generation with BERT. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.\n' +
      '* [91] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, et al. A survey of large language models. _ArXiv preprint_, abs/2303.18223, 2023.\n' +
      '\n' +
      '## 부록 추가 실험\n' +
      '\n' +
      '과제 1 - 그룹핑 토큰 임베딩을 사용하여 단서 단어를 그룹화하는 것 외에도(주요 논문 SS4에서 논의됨), \'맥락적\' 임베딩에 클러스터링하여 단어 그룹화를 실행했다. 16개의 단어 토큰(무작위 순서)을 하나의 유사문장으로 연결하여 실험적으로 \'맥락\'을 유도한다. 각 토큰에 대한 임베딩은 토큰의 순서에 따라 달랐다. 무작위 순서를 16번 반복하고 표 6에서 얻은 결과의 평균과 분산을 보고한다.\n' +
      '\n' +
      '과제 2 - 연결 GPT-4(SS4에서 논의됨)에 기반한 결과를 촉구하는 것 외에도 Meta AI의 허가를 받아 미리 훈련된 구성 가중치를 사용하여 LLaMa [67](7B, 13B)와 같은 추가 LLM에 대한 실험을 실행했다. 그러나, 특정 태스크에 대한 추가적인 미세 조정 없이, 이러한 LLM들은 의미있는 방식으로 태스크를 해결할 수 없었다. 명확히 하기 위해 LLaM은 그룹 크기가 같지 않은 환각 단어 뭉치를 생성했다. 우리는 간결함을 위해 이러한 이해할 수 없는 결과를 생략한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & WD \\(\\downarrow\\) & FMS \\(\\uparrow\\) & ARI \\(\\uparrow\\) & AMI \\(\\uparrow\\) & \\# Solved Walls & \\# Correct Groups \\\\ \\hline ELMoLAGE & \\(90.0\\pm.3\\) & \\(23.6\\pm.4\\) & \\(4.5\\pm.5\\) & \\(5.6\\pm.7\\) & \\(0\\pm 0\\) & \\(19\\pm 3\\) \\\\ DistilBERTBASE & \\(88.4\\pm.7\\) & \\(26.7\\pm.3\\) & \\(8.3\\pm.4\\) & \\(10.4\\pm.5\\) & \\(0\\pm 0\\) & \\(30\\pm 4\\) \\\\ BERTLARGE & \\(\\mathbf{87.2\\pm.6}\\) & \\(\\mathbf{28.3\\pm.5}\\) & \\(\\mathbf{10.4\\pm.6}\\) & \\(\\mathbf{12.8\\pm.7}\\) & \\(0\\pm 0\\) & \\(\\mathbf{46\\pm 5}\\) \\\\ BERTBASE & \\(87.7\\pm.5\\) & \\(28.0\\pm.2\\) & \\(10.0\\pm.3\\) & \\(12.4\\pm.4\\) & \\(0\\pm 0\\) & \\(39\\pm 2\\) \\\\ RoBERTLARGE & \\(88.4\\pm.5\\) & \\(25.9\\pm.2\\) & \\(7.4\\pm.3\\) & \\(9.3\\pm.4\\) & \\(0\\pm 0\\) & \\(30\\pm 4\\) \\\\ all-mpnetBASE & \\(87.6\\pm.5\\) & \\(28.0\\pm.3\\) & \\(10.0\\pm.4\\) & \\(12.4\\pm.5\\) & \\(0\\pm 0\\) & \\(38\\pm 3\\) \\\\ ESLARGE & \\(87.7\\pm.5\\) & \\(28.1\\pm.3\\) & \\(10.2\\pm.4\\) & \\(12.7\\pm.5\\) & \\(0\\pm 0\\) & \\(37\\pm 4\\) \\\\ ESBASE & \\(\\mathbf{87.2\\pm.3}\\) & \\(28.2\\pm.2\\) & \\(10.2\\pm.3\\) & \\(12.5\\pm.4\\) & \\(0\\pm 0\\) & \\(\\mathbf{46\\pm 5}\\) \\\\ \\hline Human Performance & – & – & – & \\(285\\,/\\,494\\) & \\(1405\\,/\\,1976\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 문맥 임베딩을 사용하여 태스크 1(그룹화)에서 선택된 모델의 결과. WD: Wasserstein Distance. FMS: 폴크스 맬로우스 점수입니다. ARI: 조정된 랜드 인덱스입니다. NMI: Normalized Mutual Information. 16개의 무작위 종자에 대한 평균 \\(\\pm\\) 표준 편차가 표시된다. **대담한**: 최상의 점수입니다.\n' +
      '\n' +
      'Additional Figures\n' +
      '\n' +
      '이 섹션에서는 사용된 다양한 방법에서 임베딩의 추가 t-SNE 투영을 제공한다.\n' +
      '\n' +
      '도 8: FastText(Crawl)를 이용한 Task 1에 대한 해결된 벽(Grouping). **왼쪽**: (wall_id="d5e6") 임베딩 모델이 단서 "_Tara_" 다른 소녀의 이름을 잘못 연관시켰지만, 여기서 "_Tara_"는 "Tara_의 힐"의 줄임말이며 "_국가 대관식 사이트_" 그룹에 속합니다. **오른쪽**: (wall_id="4c22") 연결 "_Apple_"과 연결 된 단서 "_Pie_"를 보여 줍니다. 일반적인 맥락에서 받아들여질 수 있지만, 여기서는 그리스 문자 "\\(\\pi\\)"에 대한 동음음을 나타낸다.\n' +
      '\n' +
      '도 7: GloVe를 이용한 Task 1에 대한 해결된 벽(Grouping). **왼쪽**: (wall_id="7ed3"), 임베딩 모델은 단서 "_Suspension_"를 연결 "_Bridges_"와 잘못 연결했지만, 이 연결은 청어의 예입니다. “_Suspension_”은 이 문맥에서 “_a term used in musical harmony_”이다. **오른쪽**: (wall_id="5e3c")는 단서 "_Lord_"가 임베딩 공간의 "_God, Heavens 및 Grief_"에 가깝고, 이는 "_Good — \\(\\tau\\)_" 연결과 일치합니다. 그러나, 이것은 이러한 맥락에서, "_Lord_"가 "_Thomas Lord_"의 이름을 따서 명명된 크리켓 경기장인 "_Lord의 크리켓 그라운드_"를 지칭하는 바와 같이, 붉은 청어의 또 다른 예이다.\n' +
      '\n' +
      '도 9: 정적 임베딩 및 문맥 임베딩을 모두 갖는 BERT\\({}_{\\text{LARGE}}\\)을 사용하는 태스크 1(그룹화)에 대한 해결된 벽(wall_id="2d8f"). **왼쪽**: 상황별 임베딩으로 3/4 그룹을 해결했습니다. 여기에 "_Rambrandt_"라는 단서가 다른 네덜란드 화가들 근처에 놓여 있다. 이 벽에 있는 이 단서에 대한 올바른 그룹화는 "치약 브랜즈_"입니다. **오른쪽**: 정적 임베딩으로 0/4 그룹을 해결했습니다.\n' +
      '\n' +
      '홍허링의 효과: 추가실험, 분석 및 결과\n' +
      '\n' +
      '### Additional Datasets\n' +
      '\n' +
      '절제 실험에 대해 이 섹션에 설명된 추가 데이터 세트는 모두 깃허브 및 포옹 페이스의 코드 리포지토리를 통해 사용할 수 있게 되었다.\n' +
      '\n' +
      '#### c.1.1 OCW-Randomized Dataset\n' +
      '\n' +
      '이 테스트 데이터 세트는 빨간색 청어가 제거 되거나 빈도가 크게 감소 하는 테스트 세트의 버전을 생성 합니다. 이것은 서로 다른 벽에서 무작위로 선택한 그룹을 사용하여 모든 벽을 재구축함으로써 달성됩니다. (원래 OCW) 테스트 세트에만 프로세스를 적용했으며, 열차와 유효성 검사 세트는 손대지 않았습니다.\n' +
      '\n' +
      '방법 기존 테스트 세트의 각 벽에 대해 첫 번째 그룹을 손대지 않은 상태로 두고, 그룹 중 어느 것도 단어를 공유하지 않도록 서로 다른 벽에서 각각 3개의 새로운 그룹을 샘플링한다. 각 그룹의 연결은 수정되지 않습니다. 그 결과는 모든 벽이 4개의 다른 벽에서 4개의 무작위 그룹으로 구성된 테스트 세트의 새로운 버전이다.\n' +
      '\n' +
      '#### c.1.2 OCW-WordNet Dataset\n' +
      '\n' +
      'WordNet[46, 20]은 영어의 대용량의 어휘 데이터베이스이다. 명사, 동사, 형용사, 부사 등은 각각 별개의 개념을 표현하는 인지 동의어 집합(synsets)으로 그룹화된다. 우리는 워드넷에서 집계된 상위어/위어(또는 상위어/하위어) 계층적 어휘 구조를 사용하여 OCW에서 레드허링의 효과를 추가로 분석하기 위한 쉬운 테스트 세트를 생성한다.\n' +
      '\n' +
      'MethodWe use the existing words in the wall to select the synonyms from the word\'s syns 우리는 5개 이상의 동의어 어휘 이름을 가진 동의어만 고려하고, 4개의 단어를 무작위로 샘플링한다. 원래 테스트 세트 단어와 그 정의(ss.definition())가 그룹의 연결 구가 됩니다. 각 벽에 대해 4개의 그룹을 생성하고 원래 테스트 데이터 세트의 총 벽 수(494개)에 대해 쉬운 벽 생성 프로세스를 반복했다.\n' +
      '\n' +
      '그룹 연결의 경우 상위 상위 단어를 단어에 대한 설명을 제공하는 동기화 정의와 연결한다. 이를 통해 BERTScore를 사용하여 이상적인 의미 유사성 점수가 계산될 수 있다. 몇 가지 경우(테스트 세트의 약 70/494 벽)에 대해 단어 동의어에서 직접 동의어를 사용할 수 없기 때문에 벽당 생성된 그룹의 수는 4개 미만이다. 이러한 에지 케이스에서 우리는 동물, 포유류, 가구 등과 같은 일반적인 하이퍼네임 단어를 사용하여 그룹을 생성하고 추가한다. 4개의 그룹이 있는 벽이 유효한지 확인합니다.\n' +
      '\n' +
      '쉬운 그룹을 생성한 샘플은 아래에 나와 있으며, 여기서 매핑 또는 식별을 돕기 위해 \'쉬운\'으로 원래 OCW 데이터 세트로부터 그룹_id를 접두사한다.\n' +
      '\n' +
      '{... "group_3": {  "group_id": "easy_691a_3",  "gt_words": ["gibe","shaft","jibe","barb"],  "gt_connection": "Shaft: a aggressive remark directed to person like a missile and intended to have a telling effect"... } 또한 추가 기여로 **OCW-WordNet** 으로 원본 데이터 세트, 패키지 및 이 세 가지 추가 쉬운 세트를 모방 하 여 쉽게 학습 및 유효성 검사 세트를 생성 합니다.\n' +
      '\n' +
      '### 제거 실험 결과\n' +
      '\n' +
      '#### c.2.1 PLMs: Performance on Task 1 (Grouping)\n' +
      '\n' +
      '작업 설정과 관련된 문맥 임베딩을 사용하여 이미 표시된 우수한 결과와 어순 관련 결함으로 인해 \'태틱\' 임베딩을 사용하여 결과를 수행하고 제시한다.\n' +
      '\n' +
      '#### c.2.2 LLMs: Performance on Task 1 (Grouping) using GPT3.5/4\n' +
      '\n' +
      '본 연구에서는 LLM 성능에 대한 벽의 레드허링의 영향을 분석하기 위해 절제 데이터 세트 OCW-Randomized (C.1.1)와 OCW-Wordnet (C.1.2)에 대한 작업 1(그룹화)을 반복한 결과를 제시한다.\n' +
      '\n' +
      '이 결과는 벽에서 붉은 헤링의 희석/제거로 우수한 성능의 예상 결과와 일치한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & WD \\(\\downarrow\\) & FMS \\(\\uparrow\\) & ARI \\(\\uparrow\\) & AMI \\(\\uparrow\\) & \\# Solved Walls & \\# Correct Groups \\\\ \\hline \\multicolumn{6}{c}{_Classic Word Embeddings_} \\\\ \\hline GloVe & \\(76.8\\pm.7\\) & \\(39.2\\pm.3\\) & \\(24.0\\pm.4\\) & \\(27.7\\pm.4\\) & \\(7\\pm 1\\) & \\(213\\pm 8\\) \\\\ FastText (Crawl) & \\(76.1\\pm.5\\) & \\(40.5\\pm.3\\) & \\(25.0\\pm.6\\) & \\(28.6\\pm.7\\) & \\(\\mathbf{13}\\pm\\mathbf{1}\\) & \\(236\\pm 7\\) \\\\ FastText (News) & \\(79.3\\pm.5\\) & \\(36.8\\pm.3\\) & \\(21.0\\pm.3\\) & \\(24.5\\pm.4\\) & \\(5\\pm 1\\) & \\(176\\pm 6\\) \\\\ \\hline \\multicolumn{6}{c}{_Pre-trained Language Models (PLMs)_} \\\\ \\hline ELMoLARG & \\(80.9\\pm.4\\) & \\(35.2\\pm.3\\) & \\(18.9\\pm.3\\) & \\(22.2\\pm.4\\) & \\(3\\pm 1\\) & \\(154\\pm 6\\) \\\\ DistilBERTBASE & \\(82.3\\pm.6\\) & \\(34.2\\pm.4\\) & \\(17.7\\pm.5\\) & \\(21.1\\pm.5\\) & \\(1\\pm 1\\) & \\(124\\pm 8\\) \\\\ BERTLARG & \\(86.2\\pm.4\\) & \\(29.2\\pm.3\\) & \\(11.5\\pm.3\\) & \\(14.2\\pm.4\\) & \\(0\\pm 0\\) & \\(66\\pm 4\\) \\\\ BERTBASE & \\(87.5\\pm.4\\) & \\(27.7\\pm.3\\) & \\(9.6\\pm.6\\) & \\(11.8\\pm.5\\) & \\(0\\pm 0\\) & \\(48\\pm 4\\) \\\\ RoBERTLARG & \\(86.7\\pm.5\\) & \\(28.6\\pm.2\\) & \\(10.8\\pm.3\\) & \\(13.4\\pm.3\\) & \\(1\\pm 0\\) & \\(56\\pm 4\\) \\\\ \\hline \\multicolumn{6}{c}{_Sentence Transformers_} \\\\ \\hline all-mptepbase & \\(81.4\\pm.4\\) & \\(35.1\\pm.4\\) & \\(18.9\\pm.5\\) & \\(22.0\\pm.6\\) & \\(8\\pm 1\\) & \\(154\\pm 7\\) \\\\ E5LARG & \\(76.0\\pm.5\\) & \\(40.7\\pm.3\\) & \\(25.9\\pm.4\\) & \\(29.7\\pm.4\\) & \\(8\\pm 1\\) & \\(230\\pm 5\\) \\\\ E5BASE & \\(\\mathbf{75.1\\pm.8}\\) & \\(\\mathbf{41.8\\pm.3}\\) & \\(\\mathbf{27.2\\pm.3}\\) & \\(\\mathbf{31.1\\pm.3}\\) & \\(8\\pm 1\\) & \\(\\mathbf{249\\pm 8}\\) \\\\ \\hline \\multicolumn{6}{c}{_Human Performance_} & – & – & – & – & – \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 정적 임베딩을 사용하여 **OCW-Randomized** 결과입니다. WD: Wasserstein Distance. FMS: 폴크스 맬로우스 점수입니다. ARI: 조정된 랜드 인덱스입니다. NMI: Normalized Mutual Information. 16개의 무작위 종자에 대한 평균 \\(\\pm\\) 표준 편차가 표시된다. **대담한**: 최상의 점수입니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & WD \\(\\downarrow\\) & FMS \\(\\uparrow\\) & ARI \\(\\uparrow\\) & AMI \\(\\uparrow\\) & \\# Solved Walls & \\# Correct Groups \\\\ \\hline \\multicolumn{6}{c}{_Classic Word Embeddings_} \\\\ \\hline GloVe & \\(43.0\\pm 1.0\\) & \\(66.1\\pm.4\\) & \\(57.4\\pm.5\\) & \\(60.9\\pm.5\\) & \\(118\\pm 3\\) & \\(886\\pm 1\\) \\\\ FastText (Crawl) & \\(30.6\\pm 1.0\\) & \\(75.8\\pm.6\\) & \\(69.6\\pm.7\\) & \\(72.4\\pm.7\\) & \\(195\\pm 6\\) & \\(1173\\pm 18\\) \\\\ FastText (News) & \\(44.9\\pm 1.2\\) & \\(64.9\\pm.5\\) & \\(55.9\\pm.6\\) & \\(59.5\\pm.6\\) & \\(105\\pm 3\\) & \\(844\\pm 12\\) \\\\ \\hline \\multicolumn{6}{c}{_Pre-trained Language Models (PLMs)_} \\\\ \\hline ELMoLARG & \\(52.5\\pm 1.1\\) & \\(58.9\\pm.3\\) & \\(48.2\\pm.4\\) & \\(52.5\\pm.4\\) & \\(67\\pm 3\\) & \\(682\\pm 9\\) \\\\ DistilBERTBASE & \\(45.5\\pm 1.0\\) & \\(64.1\\pm.4\\) & \\(55.0\\pm.5\\) & \\(58.7\\pm.5\\) & \\(105\\pm 3\\) & \\(835\\pm 13\\) \\\\ BERTLARG & \\(76.9\\pm 1.0\\) & \\(38.9\\pm.2\\) & \\(23.4\\pm.3\\) & \\(27.5\\pm.3\\) & \\(7\\pm 0\\) & \\(197\\pm 6\\) \\\\ BERTBASE & \\(73.0\\pm 1.3\\) & \\(42.5\\pm.5\\) & \\(27.9\\pm.6\\) & \\(32.5\\pm.6\\) & \\(8\\pm 2\\) & \\(268\\pm 12\\) \\\\ RoBERTLARG & \\(57.4\\pm 1.3\\) & \\(54.8\\pm.3\\) & \\(43.3\\pm.3\\) & \\(47.5\\pm.3\\) & \\(48\\pm 2\\) & \\(573\\pm 8\\) \\\\ \\hline \\multicolumn{6}{c}{_Sentence Transformers_} \\\\ \\hline all-mptepbase & \\(\\mathbf{22.6\\pm.7}\\) & \\(\\mathbf{81.9\\pm.4}\\) & \\(\\mathbf{77.1\\pm.5}\\) & \\(\\mathbf{79.4\\pm.4}\\) & \\(\\mathbf{256\\pm 4}\\) & \\(\\mathbf{1365\\pm 12}\\) \\\\ E5LARG & \\(23.6\\pm.8\\) & \\(80.9\\pm.4\\) & \\(75.9\\pm.5\\) & \\(78.3\\pm.4\\) & \\(250\\pm 4\\) & \\(1347\\pm 12\\) \\\\ E5BASE & \\(26.9\\pm.9\\) & \\(78.0\\pm.4\\) & \\(72.3\\pm.5\\) & \\(75.0\\pm.5\\) & \\(224\\pm 4\\) & \\(1259\\pm 10\\) \\\\ \\hline \\multicolumn{6}{c}{_Human Performance_} & – & – & – & – & – \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 정적 임베딩을 사용하는 **OCW-WordNet** 결과입니다. WD: Wasserstein Distance. FMS: 폴크스 맬로우스 점수입니다. ARI: 조정된 랜드 인덱스입니다. NMI: Normalized Mutual Information. 16개의 무작위 종자에 대한 평균 \\(\\pm\\) 표준 편차가 표시된다. **대담한**: 최상의 점수입니다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:22]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>