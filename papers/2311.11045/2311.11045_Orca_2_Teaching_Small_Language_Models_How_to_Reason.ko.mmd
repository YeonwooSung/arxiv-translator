# Orca 2: 소규모 언어 모델 학습 추론 방법

Arindam Mitra, Luciano Del Corro\({}^{\dagger}\), Shwei Mahajan\({}^{\dagger}\), Andres Codas\({}^{\ddagger}\)

**Clarisse Simoes\({}^{\ddagger}\), Sahaj Agarwal, Xuxi Chen\({}^{\ast}\), Anastasia Razdaibiedina\({}^{\ast}\) Erik Jones\({}^{\ast}\), Kriti Aggarwal\({}^{\ast}\), Hamid Palangi, Guoqing Zheng Corby Rosset, Hamed Khanpour, Ahmed Awadallah**

작업은 Microsoft에서 수행 됩니다. \({}^{\dagger}\), \({}^{\ddagger}\)는 동일한 기여를 나타냅니다.

**Microsoft Research**

###### Abstract

오르카 1은 설명 흔적과 같은 풍부한 신호로부터 학습하여 빅벤치 하드 및 AGIEval과 같은 벤치마크에서 기존 명령어 조정 모델을 능가할 수 있다. Orca 2에서는 향상된 훈련 신호가 더 작은 LMs의 추론 능력을 향상시킬 수 있는 방법을 계속 탐구한다. 소규모 LMs 훈련에 대한 연구는 종종 보다 유능한 모델의 출력을 복제하기 위해 모방 학습에 의존해 왔다. 모방에 대한 과도한 강조가 더 작은 모델의 잠재력을 제한할 수 있다고 주장한다. 우리는 작은 LM이 더 큰 모델에서 사용하는 것과 잠재적으로 다른 다른 작업에 대해 다른 솔루션 전략을 사용하도록 가르치려고 한다. 예를 들어 더 큰 모델은 복잡한 작업에 대한 직접적인 답변을 제공할 수 있지만 더 작은 모델은 동일한 용량을 갖지 않을 수 있습니다. Orca 2에서는 다양한 추론 기법(단계별, 회상 후 생성, 회상-추론-생성, 직접답변 등)을 학습하고 각 과제에 대한 가장 효과적인 해결 전략을 결정할 수 있도록 모델을 학습시키는 것을 목표로 한다. 우리는 15개의 다양한 벤치마크 세트(약 100개의 태스크에 해당하고 36K 이상의 고유한 프롬프트)를 사용하여 Orca 2를 평가한다. Orca 2는 제로 샷 설정에서 고급 추론 능력을 테스트하는 복잡한 작업에 대해 평가된 바와 같이 유사한 크기의 모델을 상당히 능가하고 5-10배 더 큰 모델과 유사하거나 더 나은 성능 수준을 달성한다. 우리는 더 작은 LM의 개발, 평가 및 정렬에 대한 연구를 지원하기 위해 오르카 2 가중치를 일명.ms/orca-lm에서 공개적으로 사용할 수 있도록 한다.

그림 1: Orca 2(7B & 13B)와 LLaMA-2-Chat(13B & 70B) 및 WizardLM(13B & 70B)을 언어 이해, 상식 추론, 다단계 추론, 수학 문제 해결 등을 포함하는 다양한 벤치마크(0샷 설정)에서 비교한 결과. Orca 2 모델은 5-10배 큰 모델을 포함한 다른 모든 모델과 일치하거나 능가한다. 모든 모델은 각각의 크기의 동일한 LLaMA-2 기본 모델을 사용하고 있다는 점에 유의한다.

**## Question:**

존과 마크는 공, 바구니, 상자가 있는 방에 있다. 존은 공을 상자에 넣은 다음 일하러 떠난다. 존이 자리를 비운 동안 마크는 공을 바구니에 넣은 후 학교로 떠난다. 둘 다 낮에 다시 모이고, 각자 방을 나간 뒤 방에서 무슨 일이 있었는지 모른다. 그들은 공이 어디에 있다고 생각하나요?

**### Orca-2-13B**:

상황을 단계별로 분석해 보자:

1. 존은 공을 상자에 넣는다.
2. 존은 일하러 간다.
3. 마크가 방으로 들어와 공을 바구니에 넣는다.
4. 마크는 학교로 떠난다.
5. 존과 마크 둘 다 방으로 돌아갑니다.

이 시점에서 존과 마크는 서로의 행동을 보지 못했다. 따라서 그들은 방에 있는 동안에만 무슨 일이 일어났는지 알고 있습니다.

존은 출근하기 전에 상자 안에 있는 공만 보았다. 마크는 학교로 떠나기 전에 바구니에 있는 공만 보았다. 서로의 행동을 목격하지 않았기 때문에 둘 다 공이 마지막으로 본 곳이라고 믿었을 것이다.

존은 공이 상자 안에 있다고 생각하고 마크는 공이 바구니에 있다고 생각할 것입니다.

**## LLaMA2-13B**:

안스: 존이 방을 떠날 때 공이 거기 있었기 때문에 공은 여전히 상자 안에 있다. 그가 돌아왔을 때, 그는 바구니에 있는 공을 보지 못했기 때문에 그것이 여전히 상자 안에 있다고 가정했다. 10번 질문. 한 남자가 술집에 들어가 술을 달라고 한다. 바텐더는 "미안하지만 우리는 여기서 술을 팔지 않는다"며 "아, 여기가 교회인 줄 몰랐네!"라고 대답한다.

Ans: 이것은 그 남자가 다른 사람(아마도 그의 친구)으로부터 이 시설에서 더 이상 술을 마실 수 없다는 말을 들었다는 것을 의미한다.

**## LLaMA2-Chat-13B**:

이것은 고전적인 측면 사고 퍼즐입니다. 해결책은 공이 여전히 상자 안에 있다는 것이다.

다음은 추론입니다.

1. 존은 출근하기 전에 공을 상자에 넣었다.
2. 마크는 학교에 가기 전에 공을 바구니에 넣었다.
3. 존이 떠난 후 바구니에 있는 공을 보지 못했기 때문에 여전히 박스 안에 있었을 것이다.
4. 마찬가지로 마크가 떠난 뒤 박스 안에 있는 공을 보지 못했기 때문에 여전히 바구니에 있었을 것이다.

그러므로, 공은 여전히 상자 안에 있다.

**## ChatGPT(GPT-3.5-Turbo)**:

존과 마크는 공이 박스 안에 있다고 생각할 것이다. 왜냐하면 존이 일을 하러 떠나기 전에 공이 박스 안에 있었기 때문이다. 그들은 존이 없는 동안 마크가 공을 바구니에 옮긴 것을 모른다.

도 2: 추론 질문에 대한 Orca 2, 그것의 기본 모델 LLaMA-2, LLaMA-2-Chat 및 ChatGPT(GPT-3.5-Turbo)의 응답을 보여주는 예. LLaMA-2 및 LLaMA-2-챗 모델 응답은 각각 복제.com/meta/llama-2-13b 및 chat.lmsys.org를 사용하여 생성되었다. LLaMA 및 Orca 2 모델 호출은 온도=0 및 top_p=1을 사용했으며 ChatGPT 응답은 chat.openai.com을 통해 얻었다.

Introduction

LLM(Large Language Models)은 인간과 기계 사이의 보다 자연스럽고 정교한 상호 작용을 가능하게 하여 코딩[3], 웹 검색[36], 챗봇[45, 56], 고객 서비스 및 콘텐츠 생성과 같은 기존 애플리케이션에서 사용자 경험을 향상시킨다. LLM이 가져온 이러한 변화는 새로운 혁신적인 AI 응용 프로그램의 길을 열어주고 있다.

GPT-4[44] 및 PaLM-2[1]와 같은 LLM들을 더 많은 파라미터들로 스케일링하는 것은 더 작은 모델들(\(\sim\) 10B 파라미터들 미만), 특히 제로 샷을 추론할 수 있는 현저한 능력[63]으로 이어졌다. 이러한 능력에는 복잡한 질문에 답하고, 설명을 생성하고, LLM이 현재 합격 점수를 얻는 미국 의료 면허 시험과 같은 다단계 문제를 해결하는 것이 포함된다[51]. 특히 전문가 영역에서 그러한 능력은 한때 AI가 도달할 수 없는 것으로 간주되었다.

모방 학습은 작은 언어 모델[6, 64, 56]을 개선하기 위한 핵심 접근법으로 등장했으며, 여기서 목표는 더 크고 유능한 교사 모델의 출력을 복제하는 것이다. 이러한 모델들은 선생님들의 스타일에 맞는 콘텐츠를 제작할 수 있지만, 종종 그들의 추론 및 이해 능력에 미치지 못한다[13]. 어느 정도 효과적이지만 모방 학습은 더 작은 모델의 잠재력을 제한하여 문제와 모델의 용량이 주어진 최상의 솔루션 전략을 활용하는 것을 제한할 수 있다.

이 작업에서 우리는 더 작은 LMs에게 어떻게 이성을 가르칠 수 있는지에 대한 질문을 계속 추구한다. 오르카 2의 목표는 두 가지이다. 첫째, 단계별 처리, 리콜-then-generate, 리콜-reason-generate, 추출-generate 및 직접 답변 방법과 같은 일련의 추론 기술을 사용하는 더 작은 모델 _how_를 가르치는 것을 목표로 한다. 둘째, 우리는 이러한 모델이 당면한 작업에 가장 효과적인 추론 전략을 사용할 때를 결정하여 크기에 관계없이 최상의 성능을 발휘할 수 있도록 돕고자 한다.

Orca 1과 마찬가지로 우리는 다양한 작업에 걸쳐 다양한 추론 전략을 시연하기 위해 더 유능한 LLM을 활용한다. 그러나 Orca 2에서 추론 전략은 학생 모델이 동일한 행동을 할 수 있는지 여부를 염두에 두고 당면 과제에 주의 깊게 맞춤화된다. 이 미묘한 데이터를 생성하기 위해 그림 3에 예시된 바와 같이 더 유능한 LLM에는 특정 전략적 행동을 이끌어내도록 설계된 복잡한 프롬프트(들)와 더 정확한 결과가 제시된다. 또한, 훈련 단계 동안, 더 작은 모델은 그러한 행동을 촉발한 원래 프롬프트에 대한 가시성 없이 태스크 및 결과적 행동에만 노출된다. 이 **신속한 삭제** 기술은 Orca 2를 **신중한 추론자** 로 만듭니다. 왜냐하면 특정 추론 단계를 실행하는 방법뿐만 아니라 특정 작업에 접근하는 방법을 더 높은 수준에서 전략화하는 방법을 배우기 때문입니다. 우리는 강력한 LLM을 순진하게 모방하기 보다는 그것들을 우리가 당면한 작업에 가장 적합한 것들을 신중하게 선택하는 행동의 저장소로 취급한다.

소규모 모델 훈련에 대한 일부 이전 연구는 평가 프로토콜에 한계가 있다. 그들은 종종 적은 수의 작업에 의존하거나 자동 평가를 위해 다른 모델을 사용하여 두 시스템의 출력을 "시스템 1(참조) 및 시스템 2(대상)의 응답이 주어지면 어느 것이 더 나은가?"와 같은 프롬프트와 비교하도록 요청한다. 그러나 이전 작업[60, 67, 13, 42]은 이 접근법이 몇 가지 단점이 있음을 입증했다. 이 작업에서 Orca 2를 다른 여러 모델과 비교하는 포괄적인 평가를 제공한다. 총 15개의 벤치마크(\(\sim\)100개의 태스크와 36,000개 이상의 고유 프롬프트 포함)를 사용한다. 벤치마크는 언어 이해, 상식 추론, 다단계 추론, 수학 문제 해결, 읽기 이해, 요약, 근거, 진실성 및 독성 콘텐츠 생성 및 식별을 포함한 다양한 측면을 다룬다.

우리의 예비 결과는 Orca 2가 특히 추론이 필요한 작업에서 5~10배 더 큰 모델과 일치하거나 초과하는 유사한 크기의 모델을 상당히 능가한다는 것을 나타낸다. 이것은 더 나은 추론 능력으로 더 작은 모델을 부여할 가능성을 강조한다. 그러나 Orca 2는 모든 모델이 기본 사전 훈련된 모델에 의해 어느 정도 제약을 받는 현상에도 예외가 아니다. Orca 2 훈련은 모든 기본 LLM을 적용할 수 있지만 이 보고서에서는 LLaMA-2 7B 및 13B에 대한 결과를 보고한다. Orca 2 모델은 안전을 위한 RLHF 훈련을 거치지 않았다. 추론을 위해 적용한 것과 동일한 기술이 안전을 위한 모델을 정렬하는 데에도 적용될 수 있으며 RLHF는 잠재적으로 훨씬 더 향상될 수 있다고 믿는다.

Preliminaries

### Instruction Tuning

교육 튜닝[46, 38, 62, 61]은 언어 모델을 훈련하는 데 중요한 단계로 부상했다. 명령어 튜닝은 입력이 자연어 태스크 기술이고, 출력은 원하는 행동의 시연인 입력-출력 쌍으로부터의 학습을 포함한다. 명령 튜닝은 보이는 태스크와 보이지 않는 태스크 모두에 대한 명령을 따르는 모델의 능력을 향상시키는 것으로 나타났다[47]. 세대들의 전체 품질을 개선하고 [7] 모델에 향상된 제로 샷 및 추론 능력을 부여하는 것으로 나타났다[62].

알파카[55], 비쿠나[6], 마법사LM[64], 베이즈[65], 코알라[12]를 포함한 여러 연구에서는 더 큰 기초 모델에 의해 생성된 출력을 사용하여 더 작은 "학생" 언어 모델을 훈련하기 위해 명령어 튜닝을 채택했다. 이러한 행동 복제는 교사 모델의 스타일을 모방하는 데 매우 효과적인 것으로 나타났다. 그러나 [42, 5]와 같이 정확성이 단순히 스타일로만 판단되지 않는 지식 집약적 또는 추론 집약적 과제에 대해 철저히 평가할 때 작은 모델 성능에 비례적으로 향상되지 않을 수 있다.

우리는 과제 해결 방법을 모델에 가르치는 데 매우 유익하지만, 수업 튜닝이 반드시 모델에 새로운 지식을 가르치는 것은 아니라는 점에 주목한다. 따라서 명령어 튜닝 모델은 사전 훈련 중에 학습된 지식에 의해 항상 제한될 것이다. 이것은 (이 작업 및 다른 관련 작업에서와 같이) 더 작은 모델에 향상된 명령 튜닝 기술을 적용할 때 특히 중요하다. 추론이 강화된 이러한 더 작은 언어 모델은 아마도 컨텍스트 창에서 모델에 제공된 지식보다 또는 더 좁은 영역으로 전문화될 때 추론 엔진으로 가장 잘 사용될 수 있다.

### Explanation Tuning

수업 튜닝의 알려진 약점 중 하나는 결과적인 학생 모델이 스타일리틱하게 맞지만 궁극적으로 잘못된 출력을 생성하는 것을 학습할 수 있다는 것이다[13]. 예를 들어, 너무 간결한 대상을 향한 지시 조정은 복잡한 추론 과정일 수 있는 것에 대한 학생의 가시성을 제한하여 다른 과제에 대한 일반화 능력을 방해한다. Orca 1에서는 더 풍부하고 표현적인 추론 신호에 대해 학생 모델을 훈련시켜 이러한 단점을 해결하기 위해 _설명 튜닝_[42]를 도입했다. 이러한 신호를 조달하는 메커니즘은 과제를 통해 교사 모델에서 자세한 설명을 얻기 위해 만들어진 시스템 명령 2이다. 시스템 명령은 LLM이 개별 사용자 프롬프트를 다룰 때 준수해야 하는 추가 고급 지침이며, 이 지침에서 ChatML 대화 인터페이스 3에서 "시스템" 역할 플래그에 의해 분리된다.

각주 2: 시스템 메시지와 상호 호환적으로 사용

각주 3: 예를 들어 [https://platform.openai.com/docs/api-reference/making-requests](https://platform.openai.com/docs/api-reference/making-requests)를 참조 하세요.

설명 튜닝은 보다 신중한 추론을 이끌어내기 위해 설계된 \(N\) 수작업으로 만들어진 범용 시스템 명령어의 컴파일에서 시작된다. "단계별 고민", "상세한 답변 생성" 등을 예로 들 수 있다. 이러한 시스템 명령어의 주요 목적은 GPT-4와 같은 유능한 LLM으로부터 "Slow Thinking" [22]의 풍부한 시연을 추출하는 것이다. 그리고 이들은 방대하고 다양한 태스크 세트로부터의 사용자 프롬프트와 결합되어 (시스템 명령, 사용자 프롬프트, LLM 답변) 트리플렛의 데이터세트를 산출한다. 학생 모델은 다른 두 입력으로부터 LLM 답변을 예측하도록 훈련된다.

사용자 프롬프트가 유사한 유형의 질문을 나타내는 \(M\) 별개의 클러스터로 그룹화될 수 있는 경우 설명 튜닝은 작업의 다른 측면을 다루는 \(M\times N\) 다른 답변의 교차 곱을 순진하게 산출합니다. 더 유능한 LLM은 시스템 지시에 따라 응답이 달라지는 경향이 있기 때문에 훈련 신호의 양과 다양성을 증가시키는 쉬운 경로를 제공한다. Orca 1[42], StableBeluga[35] 및 Dolphin4와 같은 수많은 모델은 설명 튜닝을 활용하여 특히 복잡한 제로 샷 추론 작업에서 전통적인 명령어 튜닝 모델에 비해 상당한 개선을 보여주었다.

오카 2를 주의 깊은 추론자로 가르치는 것

설명 튜닝의 핵심은 시스템 지침에 따라 LLM에서 자세한 설명이 포함된 답변을 추출하는 것이다. 그러나 시스템 명령어 교차 작업의 모든 조합이 적절한 것은 아니며, 실제로 응답 품질은 시스템 명령어에 설명된 전략에 따라 크게 달라질 수 있다.

GPT-4와 같은 매우 강력한 모델도 이 변형에 취약하다. 이야기 재정렬 문제가 주어진 4가지 다른 시스템 명령어로 얻은 GPT-4의 4가지 다른 답변을 보여주는 그림 3을 생각해보자. 첫 번째 답변(기본 GPT-4 답변)이 잘못되었습니다. 두 번째 대답(생각의 사슬 프롬프트를 사용하는 것)이 더 낫다. 우리는 모델이 단계별로 추론하고 있지만 결정 과정을 안내하는 중요한 세부 사항은 여전히 누락되어 있음을 알 수 있다. 세 번째 답변(설명-답변 프롬프트 포함)은 틀렸지만 설명은 정확합니다. 최종 답은 유일한 정답이며 다음과 같은 시스템 명령어를 사용하여 얻어진다:

너에게 일이 주어질 것이다. 다음 단계를 사용하여 해결합니다.

1. 이야기의 주요 주제 또는 주제를 식별한다.
2. 문장들 사이의 임의의 원인과 결과 관계를 찾는다.
3. 이야기의 시작이 될 수 있는 문장을 찾는다. 각각의 답안을 살펴보고 분석해서 알아보세요.
4. 이전 단계에서 수집된 정보를 바탕으로 문장을 올바른 순서로 재배열한다.
5. 최종답: '23415'와 같이 자신의 숫자를 이용하여 문장의 정확한 순서를 적는다.

GPT-4의 응답은 주어진 시스템 명령어에 의해 크게 영향을 받으며, 두 번째로 주의 깊게 작성하면 GPT-4의 응답의 품질과 정확성을 크게 향상시킬 수 있다. 마지막으로, 그러한 지시 없이 GPT-4는 도전적인 문제를 인식하는 데 어려움을 겪을 수 있으며 신중한 사고를 하지 않고 직접적인 답을 생성할 수 있다. 이러한 관찰에 의해 우리는 LLM이 작업에 대해 추론하기 위해 사용하는 전략이 작업 자체에 의존해야 한다고 결론지었다.

제공된 모든 대답이 정확했더라도 문제는 남는다: 어떤 것이 더 작은 모델을 훈련시키는 데 가장 좋은 대답인가? 이 질문은 우리 작업의 핵심이며, 우리는 당면한 문제를 기반으로 가장 효과적인 솔루션 전략을 선택하기 위해 더 작은 모델을 교육해야 한다고 주장한다. (1) 최적 전략은 작업에 따라 달라질 수 있으며 (2) 더 작은 모델에 대한 최적 전략은 더 강력한 모델과 다를 수 있다. 예를 들어, GPT-4와 같은 모델은 직접 답을 쉽게 생성할 수 있지만, 더 작은 모델은 이러한 능력이 부족하고 단계적으로 생각하는 것과 같은 다른 접근법이 필요할 수 있다. 따라서 더 강력한 모델의 추론 행동을 "모방"하도록 더 작은 모델을 순진하게 가르치는 것은 차선책일 수 있다. 단계적으로 설명된 답변들을 향해 더 작은 모델들을 트레이닝하는 것이 유익한 것으로 입증되었지만, 복수의 전략들에 대해 이들을 트레이닝하는 것은 태스크에 더 적합한 것을 선택하는 더 많은 유연성을 가능하게 한다.

**조심스러운 추론** 이라는 용어를 사용 하 여 직접 응답 생성 또는 여러 "느린 생각" [22] 전략 중 하나 (단계별, 추측 및 확인 또는 설명 후 답변 등) 중에서 지정 된 작업에 대해 선택할 솔루션 전략을 결정 하는 작업을 참조 합니다.

다음은 **신중한 추론** LLM을 학습하는 프로세스를 보여줍니다.

1. 다양한 태스크들의 집합으로부터 시작
2. Orca의 수행에 따라 어떤 과제들이 어떤 해결 전략(예: 직접답변, 단계별, 설명-후답변 등)을 필요로 하는지를 결정한다.
3. 각 과제에 대한 교사 응답을 얻기 위해 선택된 전략에 대응하는 과제별 시스템 명령어(들)를 작성한다.
4. **빠른 지우기**: 교육 시간에 학생의 시스템 지침을 작업에 액세스하는 방법에 대한 세부 정보가 없는 일반 지침으로 바꿉니다.

3단계는 교사의 응답을 얻기 위한 광범위한 권한이 있습니다. 다중 호출, 매우 자세한 지침 등을 활용할 수 있습니다.

**## Instructions:**

이 과제에서는 자연어로 쓰여진 다섯 문장의 짧은 이야기가 주어집니다. 그러나 주어진 이야기의 순서가 올바르지 않습니다. 당신의 임무는 주어진 다섯 문장에 대한 올바른 순서를 반환하여 올바른 흐름을 가진 새로운 순서와 일관성 있는 단편 이야기를 만드는 것이다. '23415'와 같은 올바른 순서로 문장 수를 사용하여 답변을 생성합니다.

문장 1: 그는 그녀가 괜찮다는 것을 알게 될 때까지 겁을 먹었다.

문장 2: 그는 보통 정말 나쁜 사고를 보았고 그것은 항상 그를 불안하게 했다.

문장 3: 어느 날 브루스는 사고 현장에 불려갔다.

문장 4: 브루스는 교통사고를 처리한 경찰관이었다.

문장 5: 그는 즉시 그의 친구 타냐의 차를 알아봤다.

**## Answer 1**:

43152

**## Answer 2**:

1단계: 시작 문장을 식별한다.

문장 4: 브루스는 교통사고를 처리한 경찰관이었다.

2단계: 다음 이벤트를 식별합니다.

문장 3: 어느 날 브루스는 사고 현장에 불려갔다.

단계 3: 다음 이벤트를 식별한다.

문장 5: 그는 즉시 그의 친구 타냐의 차를 알아봤다.

단계 4: 다음 이벤트를 식별한다.

문장 2: 그는 보통 정말 나쁜 사고를 보았고 그것은 항상 그를 불안하게 했다.

5단계: 결론 문장을 식별한다.

문장 1: 그는 그녀가 괜찮다는 것을 알게 될 때까지 겁을 먹었다.

정확한 순서는: 43521

**## Answer 3**:

Answer: 43152

설명: 형량의 정확한 순서는 다음과 같아야 한다 : 브루스는 교통사고를 처리한 경찰관이었다(4). 그는 보통 정말 나쁜 사고를 보았고 그것이 항상 그를 불안하게 했다. 어느 날 브루스는 사고 현장에 불려갔다. 그는 즉시 그의 친구 타냐의 차를 알아봤다. 그는 그녀가 괜찮다는 것을 알게 될 때까지 겁을 먹었다. 이 질서는 올바른 흐름과 함께 일관성 있는 단편 이야기를 만들어낸다.

**## Answer 4**:

1. 이야기의 주요 주제는 경찰관인 브루스가 그의 친구 타냐와 관련된 교통사고를 처리하는 것입니다.
2. 원인과 결과 관계: * 브루스는 교통사고(원인) \(\rightarrow\) 일반적으로 나쁜 사고(효과)를 보는 경찰관입니다. * 브루스는 사고 현장(원인) \(\rightarrow\) 타냐의 자동차(효과) \(\rightarrow\)를 인식합니다. 그녀가 괜찮은지 알게 될 때까지 두려워합니다.
3. 이야기의 시작이 될 수 있는 문장은 다음과 같다 : "브루스는 교통사고를 처리한 경찰관이었다."
4. 문장을 올바른 순서로 정리합니다: 1. 브루스는 교통사고를 처리하는 경찰관이었습니다. 2. 그는 보통 정말 나쁜 사고를 보았고 항상 그를 불안하게 했습니다. 3. 어느 날 브루스는 사고 현장에 호출되었습니다. 4. 그는 즉시 친구 타냐의 차를 알아챘습니다. 5. 그녀가 괜찮다는 것을 알게 될 때까지 무서웠습니다.
5. 최종답변 : 42351

도 3: Flan-CoT 컬렉션의 실증 사례.

핵심 아이디어는 과제에 접근하는 방법을 자세히 설명하는 원래 시스템 수업이 없을 때 학생 모델은 그 기본 전략과 그것이 수반한 추론 능력을 배우도록 장려될 것이다. 이 기술은 교사가 추론을 프레임화한 구조를 제거하므로 **빠른 지우기** 라고 합니다. 이 기술로 무장한 우리는 신중한 추론가인 오르카 2를 제시한다.

## 4 기술 세부 정보

Orca 2의 경우 ~817K 훈련 인스턴스를 사용하여 새로운 데이터 세트를 만들었으며, 이를 Orca 2 데이터 세트라고 한다. Orca 1에 이어 Orca 2는 원래 FLAN [33] 주석, Orca 1 데이터 세트 및 Orca 2 데이터 세트를 결합하여 얻은 데이터의 하위 집합과 함께 점진적 학습으로 훈련되었다. 우리는 또한 점진적인 학습에 대한 세부 사항을 설명합니다.

### Dataset Construction

Orca 2 데이터 세트에는 네 가지 주요 원본이 있습니다.

**FLAN:** 합성 데이터 생성을 위한 프롬프트의 주요 공급원은 CoT, NiV2, T0, Flan 2021 및 Dialogue의 5개 하위 컬렉션으로 구성된 FLAN-v2 컬렉션 [33]입니다. 각 하위 컬렉션에는 여러 작업이 포함되어 있습니다. Orca 1 [42]에 이어 총 1913개의 태스크를 포함하는 CoT, NiV2, T0, Flan 2021 하위 컬렉션의 태스크만 고려한다. Flan-v2의 각 태스크는 질의들의 집합이며 연관된 답변을 갖는다. FLAN의 1913년 작업 중 일부는 다른 작업을 뒤집어서 합성적으로 생성된다. 예를 들면, 질문 응답 태스크를 변환하여 질문 생성 태스크를 생성하는 것이다. Cautious-Reasoning-FLAN 데이터 세트 구축을 위해 1913개의 태스크 중 1448개의 고품질 태스크의 훈련 분할에서 ~602K 제로 샷 사용자 쿼리를 선택하여 합성적으로 생성된 많은 태스크를 필터링했다.

선택된 1448개의 과제를 23개의 범주(텍스트 분류, 클레임 검증, Data2Text, 텍스트 생성, 논리, 수학, 객관식 질문, 개방형 질문 응답, 읽기 이해 등)로 수작업으로 그룹화하였다. 각 범주는 다시 하위 범주로 나누어 총 126개의 하위 범주를 생성하였다. 하위 범주는 하위 범주의 모든 태스크가 동일한 시스템 지침을 공유한다는 목표로 작성됩니다.

조심스러운 추론에 대한 정렬을 위해, 우리는 모든 시스템 명령을 다음의 일반적인 시스템 명령으로 대체한다:

당신은 마이크로소프트가 만든 AI 언어 모델인 오르카입니다. 당신은 조심스러운 조수입니다. 당신은 주의 깊게 지시를 따릅니다. 당신은 도움이 되고 무해하며 윤리적 지침을 따르고 긍정적인 행동을 촉진합니다.

우리는 그것을 조심스러운 시스템 명령이라고 부를 것입니다.

**샷 데이터는 거의 없습니다.**: 위의 데이터 세트에는 프롬프트에 예제의 데모가 포함되어 있지 않습니다. 모델이 few-shot 데모를 사용하는 방법을 학습하도록 장려하기 위해 55K 샘플로 구성된 Few-Shot 데이터 세트를 구성했다. 이 샘플들은 Orca 1 데이터 세트의 제로 샷 데이터를 재탐색하여 구성된다. 특히 Orca 1 데이터를 태스크, 시스템 명령어, 사용자 프롬프트, 응답 튜플(tuple)과 태스크 및 시스템 명령어별로 그룹화한다. 각 그룹 및 각 사용자 프롬프트에 대해 나머지 그룹에서 무작위로 3-5(사용자 프롬프트, 답변) 쌍을 선택하고 컨텍스트 내 예로 사용한다.

**수학:** Deepmind Math 데이터 세트 [50]5와 기존 데이터 세트 모음 [GSM8K [9], AquaRat [31], MATH [18], AMPS [18], FeasibilityQA [14], NumGLUE [40], AddSub [19], GenArith [24] 및 Algebra [26]에서 ~160K 수학 문제에 대한 데이터를 수집했습니다. NumGLUE, AddSub, GenArith 및 Algebra의 경우 훈련 분할에 대한 LILA [39] 벤치마크를 참조했다. 데이터 세트(예: GSM8K)의 학습 분할에서 프롬프트를 포함하면 평가를 위해 도메인 내에서 렌더링됩니다. GSM8K와 같은 데이터 세트는 많은 기준선에 대해 도메인 내에서 고려됩니다.

각주 5: [https://huggingface.co/datasets/math_dataset](https://huggingface.co/datasets/math_dataset)에서 산술 div, 산술 mul, 산술 추가 또는 하위 작업에서만 샘플링했습니다.

**전체 합성 데이터:** GPT-4를 사용 하 여 2000 의사 환자 대화를 합성적으로 만들었습니다. 그런 다음 모델에 대화의 요약본을 만들도록 지시 합니다. 우리는 두 가지 다른 프롬프트를 사용했는데, 하나는 높은 수준의 작업 지시와 다른 하나는 모델이 누락되거나 조작되는 것을 방지하도록 권장하는 세부 지시이다. 이 데이터를 사용하여 전문 기술의 학습을 평가합니다.

### Training

이 섹션에서는 토큰화, 시퀀싱 및 손실 계산의 다양한 측면을 다루는 Orca 2에 대한 훈련 프로세스에 대한 개요를 제공한다.

**프로그레시브 학습**: LLaMA-2-7B 또는 LLaMA-2-13B 검사점으로 시작하여 한 시대에 대해 FLAN-v2 데이터 세트의 열차 분할에서 세분화합니다. FLAN-v2 데이터 세트에는 제로 샷과 소수의 샷 문제가 모두 포함 되어 있습니다. 그런 다음 Orca 1의 5백만 ChatGPT 데이터를 3대 동안 훈련한다. 그런 다음 Orca 1의 100만 GPT-4 데이터와 Orca 2의 817K 데이터를 4개 에폭에 대해 조합하여 훈련한다.

**토큰화**: 입력 예제를 처리하기 위해 BPE(LLaMA Byte Pair Encoding) 토큰화기를 사용합니다. 특히, LLaMA 토큰라이저는 모든 숫자를 개별 숫자로 나누고 바이트로 폴백하여 알려지지 않은 UTF-8 문자를 분해한다. 가변 길이 시퀀스를 다루기 위해 LLaMA 토큰화기 어휘에 패딩 토큰 "[[PAD]]"를 추가한다. 또한 ChatML 특수 토큰 "<|im_start|>" 및 "<|im_end|>"를 추가합니다. 결과 어휘에는 \(32,003\) 토큰이 포함되어 있습니다.

**패킹**: 학습 프로세스를 최적화 하 고 계산 리소스를 효율적으로 활용 하기 위해 패킹 기술을 사용 합니다. [25]. 이 방법은 다수의 입력 예들을 단일 시퀀스로 연결하는 것을 포함하고, 이 예들은 그 다음 모델을 트레이닝하기 위해 사용된다. 패킹은 연결된 시퀀스의 총 길이가 \(max\_len=4096\) 토큰을 초과하지 않도록 수행된다. 특히, 입력 예제를 섞은 후 각 그룹에서 연결된 시퀀스의 길이가 최대 _max_len_이 되도록 예제를 그룹으로 분할한다. 그런 다음 연결 된 시퀀스에 패딩 토큰을 추가 하 여 _max_len_ 의 균일한 입력 시퀀스 길이를 달성 합니다.

**손실**: Orca 2를 학습 하기 위해 교사 모델에서 생성 된 토큰에 대 한 손실 _only_ 을 계산 합니다. 즉 시스템 명령 및 작업 명령에서 조건 된 응답을 생성 하는 방법을 학습 합니다. 이 접근법은 모델이 가장 관련성이 높고 유익한 토큰으로부터의 학습에 초점을 맞추도록 하여 훈련 프로세스의 전반적인 효율성과 효율성을 향상시킨다.

**컴퓨팅:** 80GB 메모리와 bfloat16을 사용하여 32개의 NVIDIA A100 GPU에서 Orca 2를 훈련했습니다. 13B 검사점의 경우 FLAN 데이터 집합에서 Orca 2를 한 에폭 동안 훈련하는 데 \(\sim\)17시간, ChatGPT 데이터 500만 데이터를 3 에폭 동안 훈련하는 데 \(\sim\)40시간, 4 에폭 동안 \(\sim\)180만 GPT-4 데이터를 계속 훈련하는 데 \(\sim\)23시간이 소요되었습니다.

## 5 Experimental Setup

### Baselines

우리는 여러 최첨단 모델과 함께 오르카 2를 벤치마킹한다. 모든 베이스라인 모델은 명령어-튜닝된 모델이다. 명령어-튜닝된 버전들은 명령들을 따르는 것이 훨씬 더 우수하고, 더 강한 추론 능력들을 가지며, 제로 샷 설정들[33, 47, 64, 42]에서 훨씬 더 우수한 것으로 나타났기 때문에, 우리는 명령어-튜닝된 버전들을 사용한다.

* **LLaMA-2 모델:** LLaMA 2 시리즈의 700억 및 130억 매개 변수 모델을 모두 사용합니다. [57]. 우리는 LLaMA2-70B-hf-chat6와 LLaMA2-13B-hf-chat7을 사용한다.

* **WizardLM:** WizardLM [64]는 특히 복잡한 명령 데이터의 다양한 배열을 자율적으로 생성하는 Evol-Instruct 기술을 통해 LLaMA 2의 명령 조정 버전입니다. 13B (V1.28) 및 70B (V1.09) 매개 변수 버전을 모두 사용 합니다.

각주 8: [https://huggingface.co/WizardLM/WizardLM-13B-V1.2](https://huggingface.co/WizardLM/WizardLM-13B-V1.2)

* **Orca:** Orca 1 [42]는 설명, 단계별 사고 프로세스 및 복잡한 지침을 통해 학습 하는 130억 매개 변수 모델이며 LLaMA 모델을 기반으로 합니다 [57].
* **GPT 모델:** ChatGPT (GPT-3.5-Turbo) 및 GPT-4 [44]의 성능을 모두 보여 줍니다. Azure OpenAI API 버전 "2023-03-15-preview"를 활용했습니다.

추론을 위해 LLaMA2 및 Orca 모델에 fp32를 사용한다. 위저드LM 모델의 경우 fp16 [64]로 훈련되었기 때문에 fp16을 사용할 수 있다.

### Benchmarks

이 섹션에서는 Orca 2의 개방형 생성, 요약, 안전성, 편향, 추론 및 이해 능력을 평가하기 위해 선택된 작업에 대한 자세한 개요를 제공한다. 달리 지정된 경우를 제외하고 각 데이터 세트의 테스트 분할을 사용하여 평가를 수행했다. 우리는 제로샷 설정에서 모든 벤치마크와 모든 모델에 대한 평가를 수행합니다.

추론과 같은 고급 능력, 텍스트 완성 및 접지, 진실성 및 안전과 같은 보다 기본적인 능력을 모두 나타내는 광범위한 벤치마크를 선택했다. 벤치마크를 선택할 때 OpenLLM Leaderboard 10과 InstructEval [5]의 제안과 선택을 따른다.

각주 10: [https://huggingface.co/WizardLM/WizardLM-70B-V1.0](https://huggingface.co/WizardLM/WizardLM-70B-V1.0)

#### 5.2.1 추론 기능

* **AGIEval:** AGIEval [69]는 GRE, GMAT 및 SAT와 같은 일반 대학 입학 시험, LSAT 및 변호사 자격 평가와 같은 법률 중심 시험, 수학 대회 및 국가 공무원 시험을 포함한 다양한 표준화 테스트 집합입니다. [69].
* **단락에 대한 이산 추론:** DROP [10]은 적대적으로 만들어진 읽기 이해 벤치마크이며, 모델이 참조를 탐색하고 추가 또는 정렬과 같은 이산 작업을 실행해야 하며 InstructEval [5] 및 OpenLLM Leaderboard의 일부로 채택되었습니다.
* **CRASS:** CRASS [11] 데이터 세트는 LLM의 반사실적 추론 능력을 평가 합니다.
* **RACE:** RACE 데이터 세트 [27]은 12세에서 18세 사이의 중국 학생들에게 제공된 영어 시험에서 파생된 읽기 이해 질문 모음입니다.
* **Big-Bench Hard (BBH):** BBH [54]는 다단계 추론이 필요한 작업과 같은 도전적인 작업에 중점을 둔 BIG-Bench [52]의 23가지 가장 어려운 작업의 하위 집합입니다.
* **GSM8K:** 다단계 수학적 추론을 수행하는 기능을 테스트하는 단어 문제 모음입니다. [9].

#### 5.2.2 지식 및 언어 이해

* **대량 다중 작업 언어 이해 벤치마크:** MMLU [17]은 모델의 언어 이해, 지식 및 추론 능력을 측정하도록 설계되었으며 57개의 태스크로 구성됩니다.
* **ARC:** AI2 추론 도전 [8]은 3 등급에서 9 등급에 걸친 과학 시험의 객관식 질문에 쉽게 답할 수 있는 텍스트 모델의 능력을 테스트하는 벤치마크입니다.

#### 5.2.3 Text Completion

* **HellaSwag:** 상식 자연 언어 추론을 평가하기 위한 데이터 세트 [66]입니다. 그것은 물리적 상황에 대한 장면에서 다음에 일어날 수 있는 것으로 텍스트를 완성하는 자연 언어 모델의 능력을 테스트한다.
* **LAMBADA:** 이 데이터 세트 [48]은 자연 언어 모델이 장거리 컨텍스트 이해를 수행하는 능력을 테스트하는 2,663개의 소설에서 10,022개의 구절 모음입니다.

#### 5.2.4 Multi Turn Open Ended Conversations

* **MT 벤치:** GPT-4를 판사로 사용 하 여 다중 회전 대화 [67]에서 채팅 비서의 숙련도를 평가 하기 위해 맞춤 된 벤치 마크입니다.

#### 5.2.5 접지 및 추상 요약

* **ACI-BENCH:** 의사-환자 전체 대화 및 다양한 의료 도메인의 관련 임상 노트가 포함되어 있습니다. 과제는 대화로부터 임상 노트를 생성하는 것이다[59].
* **MS-MARCO:** 이 데이터 세트 [2]는 실제 웹 쿼리 및 문서에서 파생된 자연어 질문 및 답변의 대규모 컬렉션입니다.
* **QMSum:** 쿼리 기반 다중 도메인 회의 요약을 위한 벤치마크 [68]입니다. 여기서 모델은 쿼리에 대한 응답으로 관련 회의 범위를 선택하고 요약해야 합니다.

#### 5.2.6 Safety and Truthfulness

* **ToxiGen**: 비방이나 욕설을 포함하지 않는 암시적 혐오 발언에 초점을 맞춘 13개 소수 그룹에 대한 274,186개의 독성 및 양성 진술의 대규모 기계 생성 데이터 세트 [16]입니다. 우리는 독성 콘텐츠를 식별하고 생성하는 모델의 능력을 테스트하기 위해 데이터 세트를 사용한다.
* **HHH:** 이 데이터 세트 [53]은 유용성과 정직성 및 무해성과 관련하여 언어 모델의 정렬을 평가하기 위한 벤치마크이며, 여기서 언어 모델은 두 옵션 중 최상의 응답을 선택하도록 요청됩니다.
* **TruthfulQA:** 인간이 잘못된 믿음, 편향 및 오개념으로 인해 선별된 질문에 잘못 대답하는 경향이 있는 방식으로 구성된 질문에 대한 답변을 생성할 때 LLM의 진실성을 평가하기 위한 벤치마크 [30]입니다. 평가 기준에는 38개 범주(예: 보건, 법률, 금융 및 정치)에 걸쳐 있는 817개의 질문이 포함되어 있다. 우리는 데이터 세트의 객관식 변형에 대한 모델을 평가한다.
* **자동 RAI 측정 프레임워크:** 대화 환경에서 주어진 채팅 최적화 모델의 안전성을 평가하기 위해 최근에 제안된 프레임워크 [34]를 사용합니다. 특히, 하나의 LLM은 사용자로서 포즈를 취하고 잠재적인 유해 콘텐츠, IP 유출 및 탈옥을 평가하기 위해 테스트 중인 LLM과 대화에 참여한다.

### Evaluation Settings

우리는 제로 샷 설정 및 예시나 CoT 프롬프트 없이 모든 작업에 대한 모델의 기능을 평가합니다. 예비 실험에서 Orca 2와 같은 작은 모델보다 작은 샷 설정에서 더 큰 모델이 더 많은 혜택을 받는다는 것을 관찰한다. 우리는 제로 샷 설정만을 기반으로 평가를 수행하며, 몇 개의 샷 기능에 대한 자세한 분석을 향후 작업에 남겨둔다. 모든 실험에서 샘플링 없이 탐욕스러운 디코딩 접근법을 사용한다.

**프롬프트**: 일부 작업에 대한 답변 형식 관련 일반 지침을 제외하고 프롬프트 엔지니어링으로 인한 품질 변동을 피하기 위해 모든 모델에 대해 빈 시스템 메시지와 간단한 프롬프트를 사용합니다. 다양성을 최소화하고 신뢰할 수 있는 평가 프로세스를 수립하기 위해 응답 추출의 정확도를 높이기 위해 시스템 메시지에 형식화 지침을 포함하는 경우가 많다. 예를 들어 "마지막에 출력 ####최종 답변: {답변 선택}" 및 "제공된 옵션에서 답변을 선택합니다."와 같은 시스템 메시지를 사용할 수 있습니다. 표 F에는 각 데이터 세트에 사용되는 프롬프트가 표시됩니다. 오르카 2의 경우 "빈" 시스템 메시지와 "조심" 시스템 메시지로 성능을 보고한다. 후자는 섹션 4에서 설명한 일반적인 시스템 메시지이다.

**답변 구문 분석**: 생성 모델의 자유 형식 응답에서 답변을 구문 분석하는 것은 어려운 작업입니다. 따라서 평가 과제를 과제 유형과 필요한 추출에 따라 3가지 범주로 나누었다.

* **MCQ(다중 선택 질문):** 이러한 작업에는 모델에서 최종 답변으로 선택한 옵션을 추출해야 합니다. 또한 클래스가 모델이 선택할 수 있는 옵션을 나타내는 분류 작업을 이 범주로 포맷했다. 이러한 작업에 대한 프롬프트에는 질문과 답변 선택이 포함되었다.
* **정확한 일치/경간 추출:** 이러한 작업은 제공된 컨텍스트에서 응답 또는 범위에서 정확한 최종 답변을 추출해야 합니다.
* **추출 필요 없음:** 이 범주는 추출이 필요 하지 않은 작업에 대 한 것입니다. 개방형 질문 답변은 이 범주에 속합니다.

추출이 필요한 범주(MCQ 및 정확한 일치/스팬 추출)에서는 "최종 답변", "So, the answer is", "최종 옵션:" 등과 같은 광범위한 패턴 및 구분자를 컴파일한다. 답변을 포함할 수 있는 응답에서 텍스트를 추출합니다. 그런 다음 정규식을 사용하여 올바른 옵션 ID 또는 모델이 선택한 옵션의 정확한 텍스트를 답변으로 추출한다. 정확한 일치/스팬 추출을 위한 답변 파싱은 작업에 따라 달라집니다. 황금 답변과의 일관성을 위해 응답이 일치합니다. 또한 평가 메트릭과 함께 구문 분석 로직이 답을 추출할 수 있는 샘플의 백분율인 형식 확인 메트릭을 계산한다. 우리는 일관성을 위해 모든 모델의 응답에 동일한 파싱 논리를 사용하며 더 나은 파싱 논리로 모든 모델의 성능을 향상시킬 수 있음을 인정한다.

그러나 모델이 항상 이러한 서식 지침을 준수하는 것은 아니다. 추출 범위와 모델의 시스템 명령 및 프롬프트에 대한 민감도는 다른 연구에서 보고된 것과 비교하여 일부 기준선에 대해 다른 결과를 초래할 수 있다. 그럼에도 불구하고 이 연구의 모든 모델은 동일한 평가 파이프라인을 거친다.

FLANv2의 작업 외에도 다음 데이터 세트의 훈련 부분(DROP, ARC, RACE, Hellaswag, Lambada, MS Marco 및 GSM8K)의 작업을 포함한다. 나머지 벤치마크는 우리가 아는 한 도메인 밖으로 간주되어야 한다. 기본 모델(LLAMA-2)을 학습하는 데 사용되는 데이터에 대한 자세한 정보가 없으므로 추가 데이터 누출을 완전히 배제할 수 없습니다. 그러나 참조를 위해 LLAMA-2의 여러 명령 조정 버전의 성능을 보고한다.

다음 섹션에서는 **제로 샷** 설정에서 위에서 설명한 벤치마크에 대한 Orca 2 및 기타 기준 모델의 성능에 대해 설명합니다.

## 6 평가 결과

### Reasoning

추론 능력은 LLM의 효능을 확인하는 데 중추적이다. 여기서 우리는 Orca 2 모델의 추론 능력을 AGI Eval, BigBench-Hard (BBH), DROP, RACE, GSM8K 및 CRASS와 같은 광범위한 벤치마크에 대해 테스트하여 평가한다. 이러한 벤치마크에 걸친 평균 성능은 그림 4에 나와 있다. Orca 2를 비교할 때 다음과 같은 현상을 관찰할 수 있다.

* Orca-2-13B는 제로 샷 추론 작업에서 동일한 크기의 모델을 훨씬 능가합니다. Orca-2-13B는 LLaMA-2-Chat-13B보다 47.54%, WizardLM-13B보다 28.15%의 상대적 개선을 제공한다. 특히 세 모델 모두
- Orca-2-13B, LLaMA-2-Chat-13B, and WizardLM-13B
- 동일한 기본 모델을 공유 하 여 Orca 2에서 사용 하는 훈련 프로세스의 효율성을 강조 합니다.
* 또한, Orca-2-13B는 LLaMA-2-Chat-70B의 성능을 초과하고 WizardLM-70B와 비교가능하게 수행하고,
ChatGPT. Orca-2-7B는 모든 추론 작업에서 LLaMA-2-Chat-70B와 더 우수하거나 비슷하다.
* 7B 및 13B 모델 모두 사용 하 여 주의 하는 시스템 메시지를 사용 하면 빈 시스템 메시지에 대 한 작은 이득을 제공 합니다.

기준선 평가의 경우 실행에서 얻은 결과는 0샷 설정이 있는 다른 공개 결과와 비슷하며 적은 샷 결과와 비교하여 합리적인 차이 내에 있다. 우리의 숫자는 때때로 공개적으로 보고된 것보다 더 좋다(예를 들어, 우리의 ChatGPT 및 GPT-4는 [69]에 보고된 것과 비교하여 AGIEval에서 실행되고, 우리의 WizardLM-13B 및 WizardLM-70B는 Open LLM 리더보드에서 보고된 것과 대조적으로 DROP에서 실행된다). 그러나, 그들 중 일부는 더 나쁘다, 예를 들어 RACE에서, 우리의 ChatGPT 실행은 [28]에 보고된 것보다 9 피트 낮다. 이는 다른 ChatGPT 엔드포인트 및 버전 또는 평가에 사용되는 다른 프롬프트에 기인할 수 있습니다.

AGIEval 및 BBH의 다양한 작업에 걸친 성능 분해는 부록 A에 제공된다. Orca 2의 응답을 가진 각 데이터 세트의 예는 부록 F에 나와 있다.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Model** & **AGI** & **BBH** & **DROP** & **CRASS** & **RACE** & **GSM8K** \\ \hline Orca 2-7B & 45.10 & 45.93 & 60.26 & 84.31 & 80.79 & 47.23 \\ w/ cautious sm & 43.97 & 42.80 & 69.09 & 88.32 & 75.82 & 55.72 \\ \hline Orca 2-13B & 49.93 & 50.18 & 57.97 & 86.86 & 82.87 & 59.14 \\ w/ cautious sm & 48.18 & 50.01 & 70.88 & 87.59 & 79.16 & 65.73 \\ \hline Orca-1-13B & 45.69 & 47.84 & 53.63 & 90.15 & 81.76 & 26.46 \\ LLaMA-2-Chat-13B & 38.85 & 33.6 & 40.73 & 61.31 & 62.69 & 25.09 \\ WizardLM-13B & 38.25 & 38.47 & 45.97 & 67.88 & 62.77 & 48.60 \\ \hline LLaMA-2-Chat-70B & 46.70 & 44.68 & 54.11 & 74.82 & 68.79 & 52.01 \\ WizardLM-70B & 48.73 & 51.08 & 59.62 & 86.13 & 78.96 & 73.24 \\ \hline ChatGPT & 53.13 & 55.38 & 64.39 & 85.77 & 67.87 & 79.38 \\ GPT-4 & 70.40 & 69.04 & 71.59 & 94.53 & 83.08 & 85.52 \\ \hline \hline \end{tabular}
\end{table}
표 1: 추론 벤치마크에 대한 상이한 모델의 제로 샷 성능 비교.

그림 4: 추론 벤치마크에 대한 다양한 모델의 매크로 평균 성능. 자세한 작업별 성능 분석은 표 1에 보고되어 있다.

### 지식 및 언어 이해

MMLU, ARC-Easy 및 ARC-Challenge는 LLMS의 언어 이해, 지식 및 추론을 평가한다. 다른 벤치마크와 마찬가지로 명령어 조정 모델과만 비교하고 제로 샷 평가를 수행한다. 지식 및 언어 이해 벤치마크에 대한 결과는 표 2와 같다. 전반적으로 추론 과제와 유사한 경향을 관찰한다.

* Orca-2-13B는 각 개별 벤치마크에서 성능에서 LLaMA-2-Chat-13B 및 WizardLM-13B(둘 다 Orca-2와 동일한 기본 모델을 사용함)를 능가합니다. 평균적으로 Orca-2-13B는 LLaMA-2-Chat-13B보다 25.38%, WizardLM-13B보다 44.22%의 상대적 개선을 달성한다.
* Orca-2-13B도 두 70B 기준 모델보다 성능이 뛰어납니다. MLU 벤치마크에서 Orca-2-13B(57.73%)는 LLaMA-2-Chat-70B(58.54%) 및 WizardLM-70(55.00%)과 유사한 점수를 달성했으며, 둘 다 Orca-2-13B보다 약 5배 더 크다. 또한 Orca-2-7B는 ARC 테스트 세트에서 70B 기준선을 모두 능가한다.

우리는 GPT 모델에 대한 프롬프트의 차이 및 API 끝점의 가능한 변동을 고려하여 이 평가 세트에 대한 기준선 실행이 제로 샷 설정에서 공개적으로 보고된 결과와 일치한다는 점에 추가로 주목한다. 또한 MMLU에서 LLaMA-2 모델에 대해 공개적으로 보고된 결과가 더 높다는 점을 지적한다(13B 및 70B 변이체에 대해 각각 54.8 및 68.9[58]). 그러나 이러한 숫자는 이 문서에 보고된 제로 샷 설정과 비교하여 적은 샷 설정이다.

오르카 2에 대한 포괄적인 몇 번 샷 평가를 수행하지는 않았지만, 한 작업에 대한 예비 결과는 특히 70B 기본 모델과 비교할 때 LLaMA-2 모델에 비해 오르카 2에 대한 더 작은 이득(제로 샷 설정 이상)을 가리킨다. 우리는 섹션 7에서 이것을 논의하고 이것을 더 앞으로 연구하는 것을 목표로 한다.

### Text Completion

고급 추론 능력을 측정하는 벤치마크 외에도 HellaSwag와 LAMBADA도 사용하여 텍스트 완성 능력을 측정한다. HellaSwag는 객관식 질문 형식으로 텍스트 완성 능력을 측정하는 반면, LAMBADA는 단어 완성 과제이다.

그림 5는 텍스트 완성 벤치마크에 대한 다양한 모델의 성능을 보여준다. Orca-2-7B와 Orca-2-13B는 모두 13B와 70B 기준선을 능가하는 HellaSwag에서 강력한 성능을 보인다. Orca-2-13B는 LLaMA-2-Chat-13B보다 33.13%, WizardLM-13B보다 61.94%의 상대적 개선을 달성했다.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Model** & **MMLU** & **ARC Easy** & **ARC Challenge** \\ \hline Orca-2-7B & 53.70 & 87.79 & 78.41 \\ w/ cautious sm & 53.91 & 85.10 & 74.83 \\ \hline Orca-2-13B & 57.73 & 92.85 & 83.36 \\ w/ cautious sm & 59.32 & 85.31 & 79.95 \\ \hline LLAMA-2-Chat-13B & 49.14 & 76.26 & 61.18 \\ WizardLM-13B & 42.81 & 68.98 & 50.43 \\ Orca-1-13B & 53.80 & 86.24 & 74.74 \\ \hline LLAMA-2-Chat-70B & 58.54 & 82.20 & 67.66 \\ WizardLM-70B & 55.00 & 80.68 & 71.93 \\ \hline ChatGPT & 68.92 & 93.73 & 84.73 \\ GPT-4 & 80.61 & 96.63 & 93.26 \\ \hline \hline \end{tabular}
\end{table}
표 2: MMLU, ARC Easy 및 ARC Challenge 상에서 상이한 모델들의 제로-샷 성능 비교. 시스템 메시지는 시스템 메시지가 "빈"인지 "조심"인지 나타냅니다.

실행의 기준선 결과를 공개적으로 보고된 결과와 비교하고 HellaSwag에서 LLaMA-2-13B가 LLaMA-2-Chat-13B보다 훨씬 더 높은 성능을 가지고 있음을 식별한다. LLaMA-2-챗-13B 및 LLaMA-2-챗-70B 응답에서 무작위로 샘플링하고 수동으로 검토하여 실제로 많은 답변이 잘못되었음을 발견했으며 모델이 안전 문제를 이유로 답변을 거부하는 여러 사례가 간혹 잘못되었다. 우리는 채팅 모델이 HellaSwag와 같은 텍스트 완성 작업에 가장 적합하지 않을 수 있다고 추측한다.

또한 람바다 태스크에서 GPT-4의 하위 성능을 조사한다. 우리의 예비 분석은 GPT-4가 종종 문맥이 누락된 단어를 정확하게 식별하기에 충분한 정보를 제공하지 않는다고 주장하거나 골드 라벨과 일치하지 않는 단어를 제안한다는 것을 보여준다. 예를 들면:

나는 화장대에 있던 사냥꾼에게 향수를 뿌렸다. '엄마, 잠깐만요' 나는 전화기를 덮었다. "엄마가 라이더 걱정은 하지 말고 남자애들이랑 데이트하고 일요일 저녁은 거기서 하자" "괜찮겠어?" 가족 저녁 식사도 놓쳤어. "네, 좋아요. 엄마한테 전화해서 ____에 대해 말할게요." 빈칸에 있는 단어는 무엇인가요? 상기 답변은,

골드 답변은 _Dinner_이지만 GPT-4는 이에 응답한다.

더 많은 문맥 없이 빈칸에 있어야 할 정확한 단어를 결정하는 것은 불가능하다. 그러나 제공된 텍스트에 따르면 가능한 단어는 "그것"일 수 있습니다. 그러면 문장은 "네, 좋아요, 엄마에게 전화해서 그것에 대해 말할게요."라고 읽습니다.

GPT-4의 성능은 프롬프트 엔지니어링을 통해 향상될 수 있지만 람바다는 추가 프롬프트 엔지니어링이 필요할 수 있으며 채팅 최적화 모델을 평가하는 데 적합하지 않을 수 있다.

### Multi-Turn Open Ended Conversations

MT 벤치 데이터 세트를 사용하여 다중 회전 대화 설정에서 LMM(Large Language Models)의 성능을 평가한다[67]. MT-Bench는 미리 정해진 문의를 통해 LLM과 대화를 시작한다. 각 대화는 초기 질의(Turn 1)와 후속 질의(Turn 2)로 구성된다. 특히, 후속 쿼리는 오프닝 쿼리에 대한 LLM의 응답에 관계없이 변경되지 않은 상태로 유지된다.

MT-Bench는 GPT-4를 평가 목적으로 사용한다. 각 턴에 대해 MT-Bench는 GPT-4를 사용하여 1에서 10까지의 점수를 계산한다. 턴당 점수와 MT-Bench의 평균 점수는 표 3에서 찾을 수 있다. 우리는 다른 GPT-4 끝점을 조사하고 발견했다.

도 5: 제로 샷 설정에서 텍스트 완성 테스트 세트에 대한 상이한 모델의 성능.

그들은 다른 평가를 내립니다. 이는 다른 GPT-4 버전에 의해 생성된 결과의 비교 가능성에 대한 의문을 제기한다. 잠재적인 문제를 최소화하기 위해 평가를 수행하기 위해 동일한 GPT-4 엔드포인트 및 버전을 사용했다.

Orca-2-13B는 다른 13B 모델과 비교 가능하게 수행한다. Orca-2-13B의 평균 2차 순번 점수는 1차 순번 점수보다 낮으며, 이는 훈련 데이터에 대화가 없었기 때문일 수 있다. 그러나, Orca 2는 여전히 대화에 관여할 수 있고, 이러한 능력은 다수의 제로 샷 예들을 동일한 입력 시퀀스로 패킹함으로써 향상될 수 있다. 오르카 2의 멀티턴 대화 능력을 향상시키기 위한 우리의 미래 작업의 일환이다.

### Grounding

특정 컨텍스트에서 근거되는 응답들을 생성하는 것은 많은 LLM 애플리케이션들에 대해 원하는 속성이다. 이 평가에는 질의 기반 회의 요약, 답변이 생성되는 웹 질의 응답, 긴 형식 및 의사-환자 대화 요약을 포함하는 세 가지 다른 작업을 사용한다. 초록 요약 및 근거 질문 답변은 근거성을 평가하기 위한 테스트 베드로 자주 사용된다.

[34]에서 제안한 접지 평가 프레임워크를 사용한다. 프레임워크는 GPT-4를 판사로 사용하여 맥락 내 근거성을 측정한다. 평가용 프록시로서 임의의 모델(GPT-4를 포함함)을 사용하는 것은, 예를 들어, 모델이 자신의 세대, 긴 텍스트 또는 샘플의 특정 순서와 같은 특정 특성을 갖는 샘플을 선호하는 경향이 있는 경우, 모델에 따라 한계가 있다는 점에 유의한다[67, 60, 37]. 인간 평가와 LLM 기반 평가 간의 일관성을 높이는 작업은 연구의 열린 영역이다[32, 15, 43, 34, 67].

그림 6은 우리가 실험을 수행한 세 가지 벤치마크에 대해 평균을 낸 다른 모델에 대한 환각율 결과를 보여준다.

우리는 Orca-2-13B가 모든 Orca 2 변이체와 기타 13B 및 70B LLM 중에서 가장 낮은 환각 비율을 나타낸다는 점에 주목한다. LLaMA-2-13B 및 WizardLM-13B 모델과 비교할 때 Orca-2-13B는 환각율에서 76.92% 및 61.71%의 상대적 감소를 보여준다. 신중한 시스템 메시지는 이 작업에서 연구된 세 가지 작업에 걸쳐 일관되게 환각율을 증가시킨다. 수동 분석을 통해 신중한 시스템 메시지가 주도하는 추론 과정에서 Orca 2가 컨텍스트에서 사용할 수 있는 정보를 외삽하고 외삽된 콘텐츠를 사용하여 요약을 생성할 수 있다는 증거를 발견했다. 접지되지 않은 생성된 콘텐츠는 종종 사실적으로 정확하지만, 컨텍스트에 의해 지원되지 않는다. 각 데이터 세트에 대한 이러한 상황의 예는 부록 F에 나와 있다.

### Safety

안전성에 대한 LLM의 종합적인 평가는 상당히 복잡하고 연구의 열린 영역이다. 본 연구에서는 공개된 데이터셋에 대한 실험을 수행하였다.

\begin{table}
\begin{tabular}{l c c c} \hline
**Model** & **Turn 1** & **Turn 2** & **Average** \\ \hline Orca-2-7B & 6.14 & 5.15 & 5.65 \\ w/ cautious sm & 5.96 & 3.99 & 4.97 \\ \hline Orca-2-13B & 6.69 & 5.60 & 6.15 \\ w/ cautious sm & 6.12 & 5.31 & 5.72 \\ \hline LLaMA-2-Chat-13B & 7.17 & 6.11 & 6.64 \\ WizardLM-13B & 7.14 & 5.58 & 6.36 \\ Orca-1-13B & 6.66 & 5.19 & 5.92 \\ \hline LLaMA-2-Chat-70B & 7.05 & 6.59 & 6.82 \\ WizardLM-70B & 8.07 & 7.45 & 7.76 \\ \hline ChatGPT & 8.19 & 7.84 & 8.01 \\ GPT-4 & 9.01 & 9.06 & 9.04 \\ \hline \end{tabular}
\end{table}
표 3: 턴당 MT-Bench 점수 및 평균 섹션 5.2.6 및 [34]에서 제안된 최근의 측정 프레임워크. 그들은 암시적이고 명시적인 독성, 진실성, 콘텐츠 손상의 세 가지 범주, IP의 세 가지 영역 및 탈옥의 세 가지 영역의 초점 영역을 가진 작업을 나타낸다. 우리의 실험은 각 모델에 대한 두 가지 평가 체제, 모델이 주어진 콘텐츠의 유형을 식별하기 위한 분류기로 사용되는 식별 체제 및 모델이 안전 지침을 따르지 않는 출력을 생성하도록 프롬프트된 다음 프롬프트 의도를 얼마나 많이 따르는지에 대해 평가되는 생성 체제를 포함한다. 두 체제 모두에서 모델이 콘텐츠를 올바르게 필터링하지 않거나 콘텐츠를 더 공격적으로 필터링하는 측면에서 다르게 행동하여 다른 사회 그룹에 대해 소거를 초래할 수 있다는 점에 유의해야 한다.

*차별적 평가* 를 위해 ToxiGen, HHH 및 TruthfulQA를 사용 했습니다.

**ToxiGen:** 이 데이터 세트의 인간 주석이 있는 부분에서 각 문에는 세 명의 인간 주석이 제공한 독성 점수가 있습니다. 이 실험의 목적을 위해, 우리는 과제를 분류 과제로 공식화하고 2.5보다 낮고 2.5보다 크거나 같은 점수를 가진 각 문을 독성이 있는 것으로 할당한다. 그런 다음 모델에 각 문에 점수를 매기라는 메시지가 표시됩니다. 모델의 정확도는 독성 또는 양성으로 올바르게 분류되는 진술의 백분율로 계산된다. 결과는 그림 7과 그림 8에 나와 있다.

실험에서 Orca-2-7B, WizardLM-13B, LLaMA-2-70B 및 Orca 1 모델이 중립 진술을 분류하는 것보다 독성 진술을 분류하는 데 더 잘 수행한다는 것을 관찰했다. 이것은 ToxiGen의 표적 식별 그룹에 대한 소거(문제가 되지 않더라도 특정 그룹의 사람들과 관련된 내용을 필터링)를 유발할 수 있기 때문에 중요한 노트이다. Orca-2-13B, WizardLM-70B 및 LLaMA-2-13B는 이 실험에 대해 이러한 문제가 없다.

또한 이 실험에 대한 다양한 모델의 지시 준수, 즉 모델이 요청된 형식을 출력하는지 확인하기 위해 실험을 수행했다. Orca 2 가족, LLaMa-2 가족 및 WizardLM 가족의 모든 모델은 96% 이상의 비율을 보였다. 과제 지시도 따르지 않는 Orca 1 모델의 경우 79%로 상대적으로 낮았다.

ToxiGen의 각 범주에 대한 성능 분석에서는 부록 D.1에 나와 있다.

**TruthfulQA:** 이 데이터 세트에 대한 평가를 위해 TruthfulQA의 질문을 다중 선택 형식으로 포함하는 EleutherAI의 TruthfulQA MC 데이터 세트의 다중 선택 변형을 사용했습니다. TruthfulQA를 위한 객관식 평가도 사용되었다.

그림 6: 판별자로 GPT-4에 의해 평가된 환각율은 섹션 5에 설명된 3개의 추상 요약 벤치마크에 대해 평균화되었다. 작업별 성능 분석은 표 11에 보고되어 있다.

[44]에서. 이 데이터 세트에 대해 생성 스타일 평가(예: [57])를 사용한 관련 작업이 있으며, 이 실험에서는 사용하지 않은 다른 모델을 판사로 사용한다.

결과는 그림 9에 나와 있으며 Orca-2-13B가 유사한 크기의 다른 모델에 비해 질문에 더 잘 응답하고 훨씬 더 큰 크기를 가진 모델에 필적하는 것을 관찰한다.

그림 8: 중립 진술 분류에 대한 ToxiGen 평가 결과는 13개 범주 모두에 대해 평균을 냈다.

그림 7: 독성 진술 분류에 대한 ToxiGen 평가 결과는 13개 범주 모두에 대해 평균을 냈다.

TruthfulQA에 대한 LLaMA-2 보고서 [57]에 보고된 것과 LLaMA-2-Chat-13B와 LLaMA-2-Chat-70B의 성능 차이가 나는 이유는 평가 방식이 다르기 때문이다. LLaMA-2에서는 GPT-3가 주석기로 사용되는 생성 스타일 평가를 보고하며, 골드 레이블이 사용 가능한 경우 GPT-3(또는 모든 LLM)을 판사(순서 편향, 길이 편향 등)로 사용하는 한계를 피하기 위해 데이터 세트의 객관식 버전을 사용했다.

**HHH:** HHH 작업에 대한 결과는 그림 10에 나와 있으며 Orca-2-13B 및 Orca-2-7B 모두 비슷한 크기의 모델을 능가합니다. 또한 LLaMA-2-Chat-13B와 LLaMA-2-Chat-70B 모델은 때때로 HHH 질문에 대답하는 것을 피하여 "요청을 이행할 수 없습니다."와 같은 답변으로 응답하는 것을 관찰했다. 또한 LLaMA-2-Chat과 Orca 2 모델(w/주의 시스템 메시지)은 특정 답변을 선택하는 대신 일부 질문(특히 더 긴 질문)에 대한 개방형 응답을 생성하는 경향이 있다. 우리는 추가적인 신속한 엔지니어링이 이러한 사례 중 일부를 해결하는 데 도움이 될 수 있다고 믿는다.

생성 스타일 평가_를 위해 [34] 및 ToxiGen에서 제안된 프레임워크를 사용했다. 주석 작성자로 사용 되는 _any_ 모델 (선택 한 모델 포함)은 프록시 이며 학습 된 데이터에 따라 고유한 약점과 편향이 있을 수 있습니다. 이러한 실험을 평가 방법론으로 사용 하 여 주어진 도메인에 대 한 모델 및 주석 접근법을 사용 하 여 관심 도메인 또는 응용 프로그램에 대해 반복 하는 것이 좋습니다.

**자동화된 RAI 측정 프레임워크:** LLMs에 대한 책임 있는 AI 메트릭의 자동 측정을 위해 최근에 제안된 프레임워크를 사용합니다. [34]. 이 프레임워크에서 하나의 LLM은 사용자로 포즈를 취하고 이 섹션에서 제시하는 실험에서 Orca 2인 테스트 중인 LLM과 합성 대화에 참여한다. 그런 다음 또 다른 LLM은 다음 측면에서 책임 있는 AI 지침을 위반하는 경향을 측정하기 위해 응답을 평가한다. (1) 사용자가 테스트 중인 모델을 교도소에 침입하는 데 몇 번 성공했는가? (2) 테스트 중인 모델이 잠재적 유해 콘텐츠를 생성하는 횟수가 몇 번인가? (3) 모델이 지식재산권(IP)을 얼마나 유출하는가? (예를 들어, "The Great Gatsby book full text"에 응답하는 동안). 이러한 카테고리들은 [34]에 기술된 다수의 서브 카테고리들로 분할된다. 이 프레임워크를 사용한 실험 결과는 표 4와 5에 나와 있다. 자동화된 프레임워크는 자체적으로 설정한다는 점에 유의하라.

그림 9: TruthfulQA 벤치마크에 대한 다양한 모델의 성능. 주어진 객관식 문항에 대해 모델이 정답을 생성한 횟수의 백분율로 정확도를 보고한다.

대화 모델의 안전성을 평가하는 동안 시스템 메시지입니다. 오르카 2는 LLaMA-2-Chat-13B와 유사한 RLHF 안전 교육을 거치지 않았으며 표 4와 표 5에는 일부 측면이 반영되어 있다. 우리는 교사 모델(ChatGPT, GPT-4)이 다양한 안전 튜닝 단계를 거쳤으며 데이터 생성 과정에서 콘텐츠 필터링을 사용했기 때문에 Orca 2가 명령어 튜닝을 통해 이러한 모델의 일부 측면을 학습했을 수 있다고 가정한다. 우리는 더 큰 차이(예: 폭력)를 보는 범주 중 일부를 조사하고 오르카 2가 해로운 위치에 더 자주 대항하는 경향이 있는 반면(메트릭에 의해 처벌됨), RLHF 안전 훈련을 거친 모델은 더 자주 대응하기 위해 감소하는 경향이 있다(메트릭에 의해 보상됨).

**ToxiGen:** 이 실험에서 독성 및 양성 문장이 포함된 ToxiGen [16] 데이터 세트의 테스트 세트로 모델을 프롬프트합니다. 모델의 작업은 텍스트를 더 계속하는 것입니다. 생성된 콘텐츠의 독성을 계산하기 위해 기성 혐오 음성 검출기인 HateBERT [4]를 사용하여 콘텐츠에서 더 많은 독성을 나타내는 더 높은 값으로 독성 확률을 생성한다. 우리는 더 나은 가시성을 위해 그것들을 0에서 100 사이의 숫자로 표현한다. 독성 및 중성 프롬프트를 사용한 생성 결과는 그림 11에 나와 있다.

\begin{table}
\begin{tabular}{l|c c c|c c c} \hline  & \multicolumn{2}{c|}{Potentially Harmful Content} & \multicolumn{3}{c}{IP} \\ \cline{2-7} Model & Sexual\(\downarrow\) & Violent\(\downarrow\) & Hate\(\downarrow\) & Songs\(\downarrow\) & Books\(\downarrow\) & News\(\downarrow\) \\ \hline Orca-2-13B & 0.60\% & 13.47\% & 2.26\% & 18.47\% & 21.85\% & 21.48\% \\ LLaMA-2-Chat-13B & 0.00\% & 0.17\% & 0.83\% & 16.84\% & 15.49\% & 18.18\% \\ \hline \end{tabular}
\end{table}
표 4: 자동화된 책임 AI 측정 프레임워크를 사용한 Orca 2의 안전성 평가 [34], **유해 콘텐츠** 및 **IP**에 대한 불량률로 측정되었다.

도 10: HHH 데이터세트에 대한 평가 결과.

\begin{table}
\begin{tabular}{l|c c c} \hline Model & Adult Content\(\downarrow\) & Illegal Persuasion\(\downarrow\) & Leaking Guidelines\(\downarrow\) \\ \hline Orca-2-13B & 4.55\% & 7.58\% & 24.24\% \\ LLaMA-2-Chat-13B & 1.00\% & 6.00\% & 70.00\% \\ \hline \end{tabular}
\end{table}
표 5: 자동 책임 AI 측정 프레임워크 [34]를 사용 하 여 Orca 2의 안전 평가 **탈옥** 에 대 한 불량률로 측정 됩니다.

### 스토리 재정렬을 사용 하 여 작업 특정 데이터의 효과

우리는 그림 3의 프롬프트를 사용하여 스토리 재정렬을 위한 5,000개의 트레이닝 샘플을 생성한다. Orca 2 트레이닝 동안(즉, _prompt 소거_ 적용) 복잡한 프롬프트를 사용하지 않는다. 작업별 데이터와 나머지 훈련 데이터 세트를 혼합하고 ROCStories 코퍼스의 고유한 집합에서 Orca 2를 평가한다[41]. 테스트 인스턴스를 샘플링 하는 동안 오염을 방지 하기 위해 FLAN 훈련 분할에 있는 ROCStories에서 인스턴스를 제거 합니다. 도 12는 GPT-4에 대한 상이한 시스템 메시지들과 Orca 2의 성능을 비교한다. 또한 ChatGPT, Orca 1, LLaMA 및 WizardLM 모델들의 성능을 캡처한다. 이 실험은 _prompt 소거_로 생성된 합성 데이터를 사용하여 특정 작업에 대한 Orca 2 모델을 전문화할 가능성을 강조한다.

도 11: 독성 검출을 위한 프록시로서 HateBERT를 사용하는 ToxiGen 데이터세트에 대한 텍스트 완료 태스크로 프롬프트될 때 상이한 카테고리에 걸쳐 독성 및 중성 함량을 생성하는 경향에 대한 상이한 모델 간의 비교(더 낮은 것이 더 좋다).

도 12: 다양한 모델의 스토리 재정렬 태스크에 대한 Zero-Shot 정확 매치 성능. GPT4는 default GPT-4 answer(도 3의 Answer 1), GPT4 w/step-by-step은 default step-by-step answer(도 3의 Answer 3), GPT4 w/ explain는 answer then explain type of answer(도 3의 Answer 2)를 의미한다. GPT4 w/특수 단계별(도 3의 Answer 4)는 Orca 2에 대한 데이터를 생성하는 데 사용된다.

## 7 Limitations

LLaMA 2 모델 패밀리를 기반으로 하는 Orca 2는 많은 제한 사항뿐만 아니라 다른 대규모 언어 모델의 일반적인 제한 사항 및 다음을 포함 하 여 Orca 2의 훈련 프로세스에서 비롯 된 제한 사항을 유지 합니다.

**데이터 편향:** 광범위한 데이터에 대해 학습된 대규모 언어 모델은 실수로 원본 데이터에 존재하는 편향을 전달할 수 있습니다. 결과적으로 모델은 잠재적으로 편향되거나 불공정할 수 있는 출력을 생성할 수 있다.

**투명성 부족:** 복잡성과 크기로 인해 큰 언어 모델은 "검은 상자" 역할을 할 수 있으므로 특정 출력 또는 결정의 근거를 이해하기가 어렵습니다. 자세한 정보 11은 Azure에서 투명성 노트를 검토하는 것이 좋습니다.

각주 11: [https://learn.microsoft.com/en-us/legal/cognitive-services/openai/transparency-note](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/transparency-note)

**콘텐츠 손상:** 대규모 언어 모델이 유발할 수 있는 다양한 유형의 콘텐츠 손상이 있습니다. 이러한 모델을 사용할 때 이를 인식하고 이를 방지하기 위한 조치를 취하는 것이 중요하다. 다양한 기업과 기관에서 제공하는 다양한 콘텐츠 조절 서비스를 활용하는 것이 좋다. 중요한 점에 주목하여, 우리는 향후 AI 기술에 대한 콘텐츠 피해에 대한 정부 및 기술 리더의 더 나은 규제와 표준을 바란다. 우리는 연구와 오픈소스 커뮤니티가 이 방향으로 할 수 있는 중요한 역할을 소중히 여기고 인정한다.

**환각:** 이러한 모델이 콘텐츠를 만드는 것을 방지하는 방법이 명확하지 않기 때문에 깊은 영향을 미칠 수 있는 중요한 결정 또는 정보에 대해 주어진 언어 모델에 전적으로 의존하지 않도록 주의하고 주의하는 것이 중요합니다. 또한, 작은 모델이 크기가 작고 암기 용량이 감소하기 때문에 접지되지 않은 세대 사용 사례에서 환각에 더 취약할 수 있는지 여부는 명확하지 않다. 이것은 활발한 연구 주제이며 우리는 이 주제를 중심으로 더 엄격한 측정, 이해 및 완화가 있기를 바란다.

**오용 가능성:** 적절한 보호 장치가 없으면 이러한 모델이 허위 정보 또는 유해한 콘텐츠를 생성하는 데 악의적으로 사용될 위험이 있습니다.

**데이터 분포:** Orca 2의 성능은 튜닝 데이터의 분포와 강한 상관 관계가 있을 수 있습니다. 이 상관 관계는 수학 및 코딩과 같은 훈련 데이터 세트에서 과소 대표되는 영역에서 정확도를 제한할 수 있다.

**시스템 메시지:** Orca 2는 시스템 지침에 따라 성능의 차이를 보여 줍니다. 추가적으로, 모델 크기에 의해 도입된 확률성은 상이한 시스템 명령어에 대한 비결정적 응답의 생성으로 이어질 수 있다.

**영샷 설정:** Orca 2는 대부분 영샷 설정을 시뮬레이션 하는 데이터에 대해 학습 되었습니다. 이 모델은 제로 샷 환경에서 매우 강력한 성능을 보여주지만, 다른 모델, 특히 더 큰 모델에 비해 적은 샷 학습을 사용하는 것과 동일한 이득을 보여주지 않는다.

**합성 데이터:** Orca 2는 합성 데이터에 대해 학습되므로 데이터 생성에 사용되는 모델 및 방법의 장점과 단점을 모두 상속할 수 있습니다. Orca 2는 Azure OpenAI API 내에서 교육 및 안전 난간 (예: 콘텐츠 필터) 중에 통합 된 안전 조치로 인해 이점이 있다고 가정 합니다. 그러나 이러한 위험을 더 잘 정량화하기 위해서는 자세한 연구가 필요하다.

**작은 모델 용량:** 사후 교육은 모델을 통해 작업을 해결하는 방법을 가르치는 데 크게 도움이 되지만 모델 새 지식을 반드시 가르치는 것은 아닙니다. 따라서 사후 훈련 모델은 사전 훈련 중에 학습된 지식에 의해 대부분 제한될 것이다. 이 프로세스는 추론할 수 있는 작은 모델 능력을 향상시킬 수 있지만 지식 저장소로서의 능력을 확장하지는 않는다. 이러한 Orca 2is는 컨텍스트 창에서 모델에 제공된 지식보다 추론 엔진으로 더 적합하거나 더 좁은 영역으로 전문화하기 위해 미세 조정될 때 더 적합할 수 있다.

이 모델은 연구 설정을 위해 단독으로 설계되었으며 테스트는 그러한 환경에서만 수행되었다. 제안된 응용 프로그램의 잠재적 손상 또는 편향을 평가하기 위해 추가 분석이 필요하기 때문에 다운스트림 응용 프로그램에서 사용하면 안 된다.

Conclusions

우리의 연구는 더 작은 언어 모델의 추론 능력을 향상시키는 것이 가능할 뿐만 아니라 맞춤형 합성 데이터에 대한 훈련을 통해 달성 가능하다는 것을 보여주었다. Orca 2 모델은 다양한 추론 기법을 구현하고 각 작업에 대해 가장 효과적인 솔루션 전략을 인식함으로써 특히 제로 샷 추론 작업에서 훨씬 더 큰 모델에 필적하고 종종 초과하는 성능 수준을 달성한다. 이러한 모델은 여전히 기본 모델에 내재된 제한과 제약을 나타내지만, 특히 사후 교육을 위한 합성 데이터의 사용을 통해 더 나은 추론 능력, 제어 및 안전 측면에서 향후 개선의 유망한 잠재력을 보여준다. Orca 2 모델은 안전을 위한 RLHF 훈련을 거치지 않았지만 다양한 콘텐츠 안전 필터로 필터링된 후 훈련을 위한 합성 데이터의 사용은 모델의 전반적인 안전성을 향상시키는 또 다른 기회를 제공할 수 있다고 믿는다. 작은 언어 모델의 잠재력을 완전히 실현하기 위한 여정이 진행 중인 동안 우리의 작업은 특히 작은 모델을 이성에 가르치는 것의 가치를 강조하는 한 단계를 나타낸다. 또한 복잡한 프롬프트와 잠재적으로 여러 모델 호출을 사용하여 언어 모델을 훈련하기 위해 보다 강력한 모델에 의해 생성된 맞춤형 고품질 합성 데이터를 사용할 가능성을 강조한다. 프론티어 모델은 계속해서 우수한 기능을 입증하지만, 더 능력 있는 더 작은 모델을 구축하는 연구는 다양한 배치 시나리오를 필요로 하고 효율성과 능력 사이의 균형을 맞추는 새로운 애플리케이션을 위한 길을 여는 데 도움이 될 것이라고 믿는다.

## References

* [1] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdery, Clement Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Diaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.
* [2] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. Ms marco: A human generated machine reading comprehension dataset, 2018.
* [3] Christian Bird, Denae Ford, Thomas Zimmermann, Nicole Forsgren, Eirini Kalliamvakou, Travis Lowdermilk, and Idan Gazit. Taking flight with copilot: Early insights and opportunities of ai-powered pair-programming tools. _Queue_, 20(6):35-57, jan 2023. ISSN 1542-7730. doi: 10.1145/3582083. URL [https://doi.org/10.1145/3582083](https://doi.org/10.1145/3582083).
* [4] Tommaso Caselli, Valerio Basile, Jelena Mitrovic, and M. Granitzer. Hatebert: Retraining bert for abusive language detection in english. _ArXiv_, abs/2010.12472, 2021.
* [5] Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. Instructeval: Towards holistic evaluation of instruction-tuned large language models. _arXiv preprint arXiv:2306.04757_, 2023.

* Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot immersing gpt-4 with 90%* chatgpt quality, March 2023. URL [https://vicuna.lmsys.org](https://vicuna.lmsys.org).
* Chung et al. [2022] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022.
* Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. _ArXiv_, abs/1803.05457, 2018. URL [https://api.semanticscholar.org/CorpusID:3922816](https://api.semanticscholar.org/CorpusID:3922816).
* Cobbe et al. [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_, 2021.
* Dua et al. [2019] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 2368-2378, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1246. URL [https://aclanthology.org/N19-1246](https://aclanthology.org/N19-1246).
* Frohberg and Binder[2022] Jorg Frohberg and Frank Binder. 크래스: 2022년 대형 언어 모델의 반사실적 추론을 테스트하기 위한 새로운 데이터 세트 및 벤치마크입니다.
* Geng et al. [2023] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. Koala: A dialogue model for academic research. Blog post, April 2023. URL [https://bair.berkeley.edu/blog/2023/04/03/koala/](https://bair.berkeley.edu/blog/2023/04/03/koala/).
* Gudibande et al. [2023] Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms, 2023.
* Gupta et al. [2022] Himanshu Gupta, Neeraj Varshney, Swaroop Mishra, Kuntal Kumar Pal, Saurabh Arjun Sawant, Kevin Scaria, Siddharth Goyal, and Chitta Baral. " John is 50 years old, can his son be 65?" evaluating nlp models' understanding of feasibility. _arXiv preprint arXiv:2210.07471_, 2022.
* Hackl et al. [2023] Veronika Hackl, Alexandra Elena Muller, Michael Granitzer, and Maximilian Sailer. Is gpt-4 a reliable rater? evaluating consistency in gpt-4 text ratings, 2023.
* Hartvigsen et al. [2022] Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3309-3326. Association for Computational Linguistics, 2022.
* Hendrycks et al. [2021] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _Proceedings of the International Conference on Learning Representations (ICLR)_, 2021.
* Hendrycks et al. [2021] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. _arXiv preprint arXiv:2103.03874_, 2021.
* Hosseini et al. [2014] Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In _Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 523-533, 2014.
* Jelinek et al. [1977] Frederick Jelinek, Robert L. Mercer, Lalit R. Bahl, and Janet M. Baker. Perplexity--a measure of the difficulty of speech recognition tasks. _Journal of the Acoustical Society of America_, 62, 1977. URL [https://api.semanticscholar.org/CorpusID:121680873](https://api.semanticscholar.org/CorpusID:121680873).

* Jones et al. [2023] Erik Jones, Hamid Palangi, Clarisse Simoes, Varun Chandrasekaran, Subhabrata Mukherjee, Arindam Mitra, Ahmed Awadallah, and Ece Kamar. Teaching language models to hallucinate less with synthetic tasks, 2023.
* Kahneman [2011] Daniel Kahneman. _ 생각, 빠르고 느리다. Farrar, Straus and Giroux, New York, 2011. ISBN 9780374275631 0374275637. URL [https://www.amazon.de/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637/ref=wl_it_dp_o_pdf1_nS_nc?ie=UTF8&cold=151193SNGKJ9&coliid=I3OCESLZCVDFL7](https://www.amazon.de/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637/ref=wl_it_dp_o_pdf1_nS_nc?ie=UTF8&cold=151193SNGKJ9&coliid=I3OCESLZCVDFL7).
* Kojima et al. [2023] Takeshi Kojima, Shixiang Shane Gu, Michel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners, 2023.
* Koncel-Kedziorski et al. [2015] Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. Parsing algebraic word problems into equations. _Transactions of the Association for Computational Linguistics_, 3:585-597, 2015.
* Krell et al. [2022] Mario Michael Krell, Matej Kosec, Sergio P. Perez, and Andrew Fitzgibbon. Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance, 2022.
* Kushman et al. [2014] Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. Learning to automatically solve algebra word problems. In _Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 271-281, 2014.
* Lai et al. [2017] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding comprehension dataset from examinations. In _Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing_, pages 785-794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL [https://aclanthology.org/D17-1082](https://aclanthology.org/D17-1082).
* Laskar et al. [2023] Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Huang. A systematic study and comprehensive evaluation of ChatGPT on benchmark datasets. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, _Findings of the Association for Computational Linguistics: ACL 2023_, pages 431-469, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.29. URL [https://aclanthology.org/2023.findings-acl.29](https://aclanthology.org/2023.findings-acl.29).
* Lin [2004] Chin-Yew Lin. 요약 자동 평가 패키지입니다. _Text Summarization Branches Out_, pages 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL [https://aclanthology.org/W04-1013](https://aclanthology.org/W04-1013).
* Lin et al. [2022] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3214-3252, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL [https://aclanthology.org/2022.acl-long.229](https://aclanthology.org/2022.acl-long.229).
* Ling et al. [2017] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. _ACL_, 2017.
* Liu et al. [2023] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment, 2023.
* Longpre et al. [2023] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. The final collection: Designing data and methods for effective instruction tuning, 2023.
* Magooda et al. [2022] Ahmed Magooda, Alec Helyar, Kyle Jackson, David Sullivan, Chad Atalla, Emily Sheng, Dan Vann, Richard Edgar, Hamid Palangi, Roman Lutz, Hongliang Kong, Vincent Yun, Eslam Kamal, Federico Zarfati, Hanna Wallach, Sarah Bird, and Mei Chen. A framework for automated measurement of responsible ai harms in generative ai applications, 2023.
* Mahan et al. [2016] Dakota Mahan, Ryan Carlow, Louis Castricato, Nathan Cooper, and Christian Laforte. Stable beluga models. URL [[https://huggingface.co/stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)] ([https://huggingface.co/stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)).

* Mehdi[2023] Y. 메디 새 ai-powered microsoft bing 및 edge를 사용 하 여 검색을 다시 발명 합니다. 웹에 대 한 보조 조종사 [https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/] (https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/) 2023. (Accessed: 15 November 2023).
* Aji Minghao Wu[2023] Alham Fikri Aji Minghao Wu. 물질에 대한 유형: 2023년 대형 언어 모델에 대한 평가 편향입니다.
* Mishra et al. [2021] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. _arXiv preprint arXiv:2104.08773_, 2021.
* Mishra et al. [2022] Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, et al. Lila: A unified benchmark for mathematical reasoning. _arXiv preprint arXiv:2210.17517_, 2022.
* Mishra et al. [2022] Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. Numglue: A suite of fundamental yet challenging mathematical reasoning tasks. _arXiv preprint arXiv:2204.05660_, 2022.
* Mostafazadeh et al. [2016] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper understanding of commonsense stories. In _Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 839-849, 2016.
* Mukherjee et al. [2023] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. _arXiv preprint arXiv:2306.02707_, 2023.
* Naismith et al. [2023] Ben Naismith, Phoebe Mulcaire, and Jill Burstein. Automated evaluation of written discourse coherence using gpt-4, 2023.
* Gpt-4 기술 보고서 [2023] OpenAI. Gpt-4 기술 보고서, 2023년
* Chatgpt [2023] OpenAI. Chatgpt (sep 25 version). [https://chat.openai.com/chat] (https://chat.openai.com/chat), 2023. [Large language model].
* Ouyang et al. [2022] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.
* Ouyang et al. [2022] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. _ArXiv_, abs/2203.02155, 2022.
* Paperno et al. [2016] Denis Paperno, German Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA dataset: Word prediction requiring a broad discourse context. In _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1525-1534, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1144. URL [https://aclanthology.org/P16-1144](https://aclanthology.org/P16-1144).
* Papineni et al. [2002] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors, _Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics_, pages 311-318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL [https://aclanthology.org/P02-1040](https://aclanthology.org/P02-1040).
* Saxton et al. [2019] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. _arXiv preprint arXiv:1904.01557_, 2019.

* [51] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. Towards expert-level medical question answering with large language models, 2023.
* [52] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, and Adria Garriga-Alonso et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2022.
* [53] Gabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. Evaluating gender bias in machine translation. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 1679-1684, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1164. URL [https://aclanthology.org/P19-1164](https://aclanthology.org/P19-1164).
* [54] Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 13003-13051, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.824. URL [https://aclanthology.org/2023.findings-acl.824](https://aclanthology.org/2023.findings-acl.824).
* [55] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca), 2023.
* [56] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaxiu Steven Zheng, Amin Ghafouri, Marcelo Menegail, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Arroyo, Ravi Rajakumar, Alena Buttyma, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications, 2022.
* [57] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [58] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kluumann, Artem Korenev, Pruit Singh Korna, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.
* [59] Wen wai Yim, Yujuan Fu, Asma Ben Abacha, Neal Snider, Thomas Lin, and Meliha Yetisgen. Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation, 2023.
* [60] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators, 2023.

* [61] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzeai, Anjana Arunkumar, Arjun Ashok, Artul Selvan Dhananasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and Daniel Khashabi. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks, 2022.
* [62] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners, 2022.
* [63] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022.
* [64] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions, 2023.
* [65] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with parameter-efficient tuning on self-chat data, 2023.
* [66] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, 2019.
* [67] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
* [68] Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. QMSum: A new benchmark for query-based multi-domain meeting summarization. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 5905-5921, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.472. URL [https://aclanthology.org/2021.naacl-main.472](https://aclanthology.org/2021.naacl-main.472).
* [69] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models, 2023.

AGIEval Subask Metrics

AGIEval에는 여러 가지 객관식 영어 과제가 포함되어 있습니다. 표 6은 각 개별 AGIEval 작업에 대한 Orca 2 및 기준 모델의 성능을 제공한다. 작업 성능은 [69]에 제시된 방법론에 따라 정확한 일치 정확도를 사용하여 측정된다.

**Orca 2 model Insights**:

* 비어 있고 주의 깊은 시스템 메시지가 있는 Orca 2의 13B 변형은 경쟁력 있는 결과를 제공합니다. Orca-2-13B w/주의 sm는 평균 48.18%를 달성하는 반면 Orca-2-13B는 평균 49.93%를 기록한다.
* 7B 반복은 13B 대응자를 능가했지만 여전히 상대적으로 경쟁적인 점수를 달성하며 빈 전략 및 조심스러운 전략의 평균이 각각 45.10% 및 43.97%입니다.

**다른 최신 벤치마크 수행 부족**:

* **LLaMA-2-Chat-13B**: 평균적으로 Orca-2-13B가 LLaMA-2-Chat-13B보다 +11.08 포인트 더 우수합니다. 특히 Orca 2 모델은 LSAT-RC(+22.31점), LSAT-LR(+10.20점), Gaokao EN(+14.70점)과 같은 작업에서 눈에 띄는 선두를 유지하고 있다.
* **WizardLM-13B**: Orca-2-13B가 WizardLM-13B를 평균 +11.68포인트 능가합니다. 개별 과제에서 Orca 2는 LSAT-RC(+15.99점)와 Gaokao EN(+12.74점)에서 유의한 우위를 차지하고 있다.
* **LLaMA-2-70B**: 전체적으로 Orca-2-13B는 LLaMA-2-70B를 평균 +3.23포인트 리드합니다. 이것은 오르카 2가 약 5배 적은 매개변수를 가지고 있기 때문에 특히 흥미롭다. 특정 과제의 경우 Orca-2-13B는 LSAT-LR(-3.73점), LOGIQA(-0.15점), SAT-English(-5.34점)에서 뒤지지만, 나머지는 AQUA-RAT(+7.87점), SAT-MATH(+17.71점)에서 더 나은 것으로 나타났다.

**벤치마킹 vs. Orca1**:

* 대부분의 작업에서 Orca 2 모델은 Orca1을 능가합니다.
* LSAT-LR: Orca-2-13B w/ 조심스러운 sm는 -2.15 포인트만큼 추적되지만 Orca-2-13B는 +0.59만큼 능가합니다.
* GAOKAO-EN: Orca-2-13B 및 Orca-2-13B w/ careful sm는 각각 -3.92 및 -4.25 포인트 부족하다.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline
**Model** & \multicolumn{2}{c}{**LST-**} & \multicolumn{2}{c}{**LST-**} & \multicolumn{2}{c}{**LST-**} & \multicolumn{2}{c}{**AQUA-**} & \multicolumn{2}{c}{**Gaokao**} & \multicolumn{2}{c}{**LOGIQA**} & \multicolumn{2}{c}{**ST-E**} & \multicolumn{2}{c}{**ST-**} & \multicolumn{2}{c}{**ST-**} & \multicolumn{2}{c}{**Avg**} \\  & **RC** & **LR** & **AR** & **RAT** & **EN** & **(en)** & **(w/p)** & **Eng** & **M** & \\ \hline Orca 2-7B & 49.81 & 42.94 & 25.65 & 35.43 & 73.86 & 35.02 & 32.52 & 67.48 & 43.18 & 45.10 \\ w/ cautious sm & 51.30 & 41.18 & 17.39 & 37.40 & 73.53 & 34.72 & 33.01 & 69.90 & 37.27 & 43.97 \\ \hline Orca 2-13B & 60.97 & 45.49 & 20.43 & 42.91 & 79.41 & 40.40 & 33.98 & 76.70 & 49.09 & 49.93 \\ w/ cautious sm & 59.85 & 42.75 & 22.61 & 35.04 & 79.74 & 36.87 & 39.32 & 74.27 & 43.18 & 48.18 \\ \hline Orca-1-13B & 59.48 & 44.90 & 18.26 & 27.17 & 83.66 & 38.71 & 32.04 & 74.76 & 32.27 & 45.69 \\ LLaMA-2-Chat-13B & 38.66 & 35.29 & 22.17 & 25.98 & 64.71 & 33.64 & 37.86 & 56.31 & 35.00 & 38.85 \\ WizardLM-13B & 44.98 & 32.35 & 21.74 & 25.59 & 66.67 & 28.57 & 40.29 & 56.80 & 27.27 & 38.25 \\ \hline LLaMA-2-Chat-70B & 56.51 & 49.22 & 18.70 & 35.04 & 78.76 & 40.55 & 39.32 & 70.39 & 31.82 & 46.70 \\ WizardLM-70B & 64.31 & 48.24 & 14.35 & 37.40 & 77.12 & 33.03 & 43.69 & 71.36 & 49.09 & 48.73 \\ ChatGPT & 55.39 & 52.35 & 22.61 & 55.12 & 75.49 & 37.17 & 46.60 & 68.45 & 65.00 & 53.13 \\ GPT-4 & 86.62 & 73.92 & 31.30 & 65.75 & 90.85 & 55.45 & 55.83 & 89.81 & 84.09 & 70.40 \\ \hline \hline \end{tabular}
\end{table}
표 6: AGIEval 벤치마크 태스크에 대한 다른 기준선 대비 Orca 2 모델의 Zero-Shot 성능.

* LOGICQA(en) Orca-2-13B w/주의 sm에서 SAT 영어는 -0.49만큼 지연됩니다.
* 다른 모든 경우에 Orca 2-13B는 Orca-1-13B 선행자보다 성능이 우수하다. 평균: 신중한 시스템 메시지를 가진 Orca 2는 Orca1을 +2.49 포인트, Orca-2-13B는 +4.24 포인트만큼 이끈다.

정리하자면 Orca 2 모델은 70B 파라미터만큼 큰 모델을 능가하는 제로샷 추론 태스크의 성능에서 주목할 만한 진전을 보여준다. 이것은 전임자인 Orca-1-13B에서 상당한 진전을 나타낸다. 시각적 표현을 위해 그림 13은 Orca 2 빈 시스템 메시지와 다른 기준선 간의 비교 결과를 보여준다.

## 부록 B BigBench-Hard Subtask Metrics

표 7, 8, 9 및 10은 Orca 2의 제로 샷 성능과 각 BBH MCQ 추론 작업에 대한 기본 모델을 보여주며 정확도는 성능을 평가하는 데 사용되는 메트릭이다.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Model & Tracking & Tracking (5 objs) & Tracking (7 objs) & Logical & Logical & Logical \\  & (3 objs) & (5 objs) & (7 objs) & Deduction & Deduction & Deduction \\  & & & & (3 objs) & (5 objs) & (7 objs) \\ \hline Orca-2-7B & 34.00 & 20.80 & 18.80 & 62.00 & 45.60 & 44.00 \\  & w/ cautious sm & 30.40 & 24.00 & 11.20 & 56.80 & 38.40 & 41.20 \\ \hline Orca-2-13B & 46.80 & 36.40 & 25.20 & 72.00 & 46.80 & 42.00 \\  & w/ cautious sm & 34.80 & 28.40 & 16.80 & 71.20 & 45.60 & 42.00 \\ \hline Orca-1-13B & 35.20 & 15.20 & 12.80 & 63.60 & 40.80 & 39.20 \\  & LLAMA-2-Chat-13B & 30.80 & 17.20 & 13.20 & 44.00 & 28.00 & 25.20 \\  & WizardLM-13B & 40.40 & 27.60 & 24.40 & 46.80 & 34.40 & 32.40 \\ \hline LLaMA-2-Chat-70B & 31.20 & 14.40 & 16.40 & 48.80 & 39.60 & 42.00 \\  & WizardLM-70B & 51.20 & 52.40 & 52.80 & 60.00 & 46.80 & 41.60 \\ \hline ChatGPT & 45.20 & 32.80 & 32.40 & 65.60 & 46.00 & 35.20 \\  & GPT-4 & 64.40 & 60.00 & 50.40 & 87.20 & 67.60 & 52.00 \\ \hline \hline \end{tabular}
\end{table}
표 7: BBH 벤치마크 내의 태스크 1-6에 대한 모델의 제로 샷 성능.

도 13: 전문 및 학업 시험에 대한 AGIEval 벤치마크에서 GPT-4, ChatGPT 및 Orca 2의 성능에서의 토픽 브레이크다운.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Model & Hyperbaton & Temporal & Web Of & Disambiguation \\  & & Sequences & Lies & QA \\ \hline Orca-2-7B & 74.80 & 17.60 & 44.80 & 62.00 \\ w /cautious sm & 50.00 & 23.60 & 45.60 & 36.00 \\ \hline Orca-2-13B & 58.80 & 46.80 & 46.00 & 59.60 \\ w /cautious sm & 53.20 & 36.80 & 32.40 & 68.00 \\ \hline Orca-1-13B & 59.60 & 63.60 & 50.40 & 72.00 \\ LLaMA-2-Chat-13B & 55.60 & 24.00 & 25.60 & 57.20 \\ WizardLM-13B & 62.40 & 43.60 & 42.80 & 59.60 \\ \hline LLaMA-2-Chat-70B & 52.40 & 32.40 & 9.20 & 63.60 \\ WizardLM-70B & 55.20 & 72.80 & 33.60 & 48.40 \\ \hline ChatGPT & 78.40 & 64.00 & 47.20 & 56.40 \\ GPT-4 & 68.80 & 95.60 & 51.60 & 40.40 \\ \hline \hline \end{tabular}
\end{table}
표 10: BBH 벤치마크 내의 태스크 21-24에 대한 모델의 제로 샷 성능.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Model & \begin{tabular}{c} Causal \\ Judg. \\ \end{tabular} & \begin{tabular}{c} Geomet. \\ Shapes \\ \end{tabular} & \begin{tabular}{c} Date \\ Underst. \\ \end{tabular} & \begin{tabular}{c} Navigate \\ Names \\ \end{tabular} & \begin{tabular}{c} Ruin \\ Expres. \\ \end{tabular} & \begin{tabular}{c} Boolean \\ Unders. \\ \end{tabular} \\ \hline Orca-2-7B & 54.55 & 14.00 & 49.60 & 54.00 & 42.40 & 54.00 & 66.00 \\ w/cautious sm & 56.68 & 22.40 & 45.60 & 43.60 & 39.60 & 44.80 & 65.20 \\ \hline Orca-2-13B & 54.55 & 18.00 & 53.60 & 53.60 & 44.80 & 70.00 & 66.00 \\ w/cautious sm & 57.22 & 25.60 & 49.20 & 60.80 & 53.60 & 60.80 & 70.40 \\ \hline Orca-1-13B & 54.55 & 13.20 & 41.60 & 49.20 & 38.00 & 66.40 & 62.00 \\ ILaMA-2-Chat-13B & 13.90 & 10.80 & 54.80 & 14.4 & 33.20 & 34.00 & 39.60 \\ WizardLM-13B & 1.07 & 11.20 & 53.60 & 1.60 & 48.80 & 53.20 & 64.00 \\ \hline \hline \end{tabular}
\begin{tabular}{l c c c c c} \hline \hline LLaMA-2-Chat-70B & 52.94 & 15.60 & 66.80 & 56.40 & 56.40 & 46.40 & 70.00 \\ WizardLM-70B & 41.18 & 14.00 & 64.00 & 40.80 & 75.20 & 60.80 & 40.40 \\ \hline ChatGPT & 53.48 & 20.80 & 68.00 & 50.80 & 60.40 & 90.40 & 54.40 \\ GPT-4 & 45.99 & 44.80 & 69.20 & 86.40 & 75.60 & 92.40 & 78.80 \\ \hline \hline \end{tabular}
\end{table}
표 8: BBH 벤치마크 내의 태스크 7-14에 대한 모델의 제로 샷 성능.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Model & \begin{tabular}{c} Movie \\ Recom. \\ \end{tabular} & \begin{tabular}{c} Reas. \\ (Col. Obj.) \\ \end{tabular} & \begin{tabular}{c} Salient \\ Translation \\ \end{tabular} & \begin{tabular}{c} Snarks \\ Fallac. \\ \end{tabular} &
\begin{tabular}{c} Formal \\ i. a. Tab. \\ \end{tabular} \\ \hline Orca-2-7B & 60.80 & 50.80 & 37.20 & 58.99 & 32.80 & 56.85 \\ w /cautious sm & 56.40 & 45.20 & 43.60 & 61.24 & 51.60 & 51.37 \\ \hline Orca-2-13B & 65.60 & 68.80 & 42.00 & 62.92 & 10.80 & 63.01 \\ w /cautious sm & 65.20 & 60.40 & 45.60 & 65.17 & 50.80 & 56.16 \\ \hline Orca-1-13B & 75.60 & 43.20 & 40.00 & 66.85 & 52.00 & 45.21 \\ LLaMA-2-Chat-13B & 40.40 & 44.40 & 38.80 & 65.73 & 25.20 & 43.84 \\ WizardLM-13B & 48.00 & 38.80 & 36.80 & 76.40 & 0.00 & 36.99 \\ \hline LLaMA-2-Chat-70B & 70.80 & 48.80 & 40.40 & 73.03 & 44.80 & 45.89 \\ WizardLM-70B & 55.60 & 55.20 & 47.60 & 71.35 & 30.80 & 63.01 \\ \hline ChatGPT & 66.80 & 60.40 & 43.20 & 79.78 & 50.80 & 71.23 \\ GPT-4 & 71.60 & 88.40 & 59.60 & 85.96 & 58.80 & 92.47 \\ \hline \hline \end{tabular}
\end{table}
표 9: BBH 벤치마크 내의 태스크 15-20에 대한 모델의 제로 샷 성능.

추상적 요약의 접지 평가

제조 및 환각은 복잡성의 다양한 측면을 가진 현대 LLM에 중요한 도전 과제이다. 그 중 접지는 주어진 컨텍스트에서 생성된 질의에 응답하는 것을 목표로 하는 가장 중요한 것 중 하나이다. 과제로서의 추상적 요약은 이러한 특성을 가지며 접지를 평가하기 위한 적절한 테스트 베드 중 하나이다. 이 섹션에서는 ACI-BENCH [59], QMSum [68], MS MARCO [2] 섹션 5에서 설명한 세 가지 추상 요약 데이터 세트에 대한 제로 샷 평가를 제시한다. 주요 목적은 생성된 요약의 품질과 이 작업에서 연구된 다양한 모델의 환각율을 측정하는 것이다. 환각율을 측정하기 위해 [59]와 [21]에서 제안한 방법을 따른다.

### 환각율 평가

섹션 6.5에 설명된 평가 방식에 따라 표 11은 빈 시스템 메시지 및 기준 모델이 있는 Orca 2에 대한 환각 비율 결과를 제시한다.

### 생성된 요약 평가

금 요약과 관련하여 생성된 요약의 품질을 평가하는 것은 자동 메트릭과 인간 평가를 모두 사용해야 하며 다양한 평가 측면에 따라 상당히 복잡할 수 있다. 이 작업에서 결과를 보고 하기 위해 다음 자동 메트릭을 사용 했습니다. BLEU [49], ROUGE-L [29] 및 Perplexity [20]. 표 12는 실험에서 연구된 직접적이고 신중한 시스템 메시지 및 기타 LLM과 함께 Orca 2에 대한 결과를 나타낸다.

ACI-BENCH Orca 2의 경우 LLAMA 2\({}^{\text{chat}}\)의 두 가지 변형보다 더 나은 성능과 WizardLM-70B와 유사한 성능을 보인다. QMSum에서 Orca-2-13B와 Orca-2-7B는 LLAMA-2-Chat-70B와 WizardLM-70B보다 더 나은 성능을 보이지만 신중한 시스템 메시지로 생성된 답변은 인간이 생성한 레이블에서 더 벗어나는 경향이 있다. 이는 모델이 반드시 잘못된 것은 아니지만 문맥과 다른 문구를 사용하는 자체 결론에 도달하는 경향이 있는 추론 과정의 결과일 수 있다. MS-MARCO의 경우 Orca 2 모델 패밀리는 n-gram 기반 메트릭에서 높은 성능 결과를 갖는 반면, 시스템 메시지가 없는 모델은 더 큰 모델에 필적하는 복잡성 결과를 달성한다. MS-MARCO 트레이닝 세트는 배포 중이며 명령어 튜닝 데이터에 포함되어 있습니다. 이 데이터 세트에 대한 n-gram 기반 메트릭에서 GPT-4 낮은 성능은 인간 레이블과 비교할 때 GPT-4 답변의 크기로 설명될 수 있다. 단어로 말하면, 이 데이터세트에 의해 제공되는 라벨은 대부분 작은 문장인 반면, GPT-4는 라벨에 포함되지 않은 어휘를 갖는 훨씬 더 긴 답변을 생성하는 경향이 있다.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Model & ACI-BENCH & MS MARCO & QMSum & Average \\ \hline Orca-2-13B & 9.66 & 11.50 & 11.74 & 10.97 \\ w /cautious sm & 10.14 & 27.90 & 48.94 & 29.00 \\ \hline Orca-2-7B & 27.45 & 15.40 & 16.20 & 19.68 \\ w /cautious sm & 21.26 & 35.80 & 55.18 & 37.41 \\ \hline Orca-1-13B & 42.65 & 10.40 & 15.16 & 22.74 \\ LLaMA-2-Chat-13B & 61.46 & 40.88 & 40.26 & 47.53 \\ WizardLM-13B & 30.10 & 32.73 & 23.12 & 28.65 \\ \hline LLaMA-2-Chat-70B & 67.96 & 35.72 & 32.46 & 45.38 \\ WizardLM-70B & 14.56 & 18.94 & 13.50 & 15.67 \\ \hline ChatGPT & 3.38 & 7.11 & 8.81 & 6.43 \\ GPT-4 & 1.46 & 3.90 & 3.05 & 2.80 \\ \hline \hline \end{tabular}
\end{table}
표 11: 더 나은 성능을 나타내는 더 낮은 비율의 판사로 GPT-4에 의해 평가된 환각율. 표의 상위 세그먼트는 Orca 2의 13B 버전과 7B 버전의 비교 분석을 제공한다. 하위 세그먼트는 기준 모델을 제시한다. 오르카 2의 모든 버전과 비슷한 크기의 모델 중에서 오르카-2-13B가 가장 효과적인 모델로 등장한다.

모든 데이터 세트에서 Orca 2의 다른 버전과 시스템 메시지를 비교하면 직접 시스템 메시지를 사용하는 모델이 신중한 시스템 메시지를 사용하는 모델보다 더 나은 성능을 보이는 경향이 있으며, 잠재적으로 이러한 모델에 의해 생성된 답변이 인간이 생성한 요약에서 예상되는 것과 더 가깝다는 것을 나타낸다. 이는 이전 섹션에서 사용된 환각 측정 지표와 일치하며, 분석 결과 신중한 시스템 메시지를 사용한 답변이 원본 텍스트를 재구성하고 외삽하는 경향이 있음을 보여준다.

\begin{table}
\begin{tabular}{l c c c|c c c|c c c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{3}{c}{**ACI-BENCH**} & \multicolumn{3}{c}{**MS MARCO**} & \multicolumn{3}{c}{**QMSum**} \\  & **BLEU** & **R-L** & **PPL** & **BLEU** & **R-L** & **PPL** & **BLEU** & **R-L** & **PPL** \\ \hline Orca-2-7B & 9.29 & 29.03 & 5.29 & 8.95 & 23.32 & 7.99 & 6.54 & 23.13 & 7.44 \\ w /cautious sm & 5.80 & 25.42 & 6.37 & 8.17 & 26.52 & 9.21 & 3.49 & 19.45 & 9.11 \\ \hline Orca-2-13B & 9.65 & 29.87 & 5.36 & 9.28 & 23.93 & 7.81 & 6.62 & 23.54 & 7.44 \\ w /cautious sm & 5.25 & 25.40 & 5.74 & 7.83 & 26.47 & 8.69 & 3.41 & 19.43 & 8.75 \\ \hline Orca-1-13B & 3.36 & 15.29 & 6.41 & 11.90 & 28.47 & 7.68 & 6.36 & 23.09 & 7.43 \\ LLaMA-2-Chat-13B & 4.67 & 21.47 & 7.00 & 3.71 & 12.77 & 6.28 & 4.58 & 19.77 & 6.90 \\ WizardLM-13B & 6.45 & 23.96 & 6.35 & 5.58 & 16.52 & 7.54 & 5.08 & 20.64 & 7.50 \\ \hline LLaMA-2-Chat-70B & 6.41 & 23.56 & 6.86 & 5.80 & 16.83 & 7.21 & 5.73 & 21.82 & 7.51 \\ WizardLM-70B & 10.27 & 28.66 & 5.84 & 6.46 & 18.64 & 7.58 & 5.80 & 21.87 & 7.60 \\ \hline ChatGPT & 7.55 & 21.26 & 4.63 & 8.37 & 21.20 & 6.52 & 6.75 & 23.36 & 7.25 \\ GPT-4 & 12.00 & 29.46 & 7.00 & 3.37 & 13.97 & 7.97 & 5.79 & 22.46 & 6.99 \\ \hline \hline \end{tabular}
\end{table}
표 12: 자동 메트릭 BLEU, Rouge-L(약칭: R-L) 및 Perplexity(약칭: PPL)를 이용한 추상 요약 평가. 당황스러움은 낮을수록 좋다. n-gram 기반 메트릭을 기반으로 Orca-2-13B는 다른 Orca 2 모델과 비교할 때 ACI-BENCH 및 QMSUM에서 더 나은 성능을 산출한다. 베이스라인으로 사용되는 다른 LLM들 중에서, Orca-1-13B는 MS-MARCO 및 QMSum에 대해 더 나은 성능을 달성하고, GPT-4는 ACI-BENCH에 대해 최상의 성능을 달성한다. 당화도 메트릭을 기반으로 다른 Orca 2 모델 간에는 명확한 승자가 없지만 기준선 중에서 ChatGPT는 ACI-BENCH에 대해 최상의 결과를 산출하는 반면 LLaMA-2-Chat-13B는 다른 데이터 세트에 대해 가장 작은 당화도를 달성한다. 복잡성을 계산하는 데 사용되는 모형이 다른 경우 이 분석이 변경될 수 있습니다.

[MISSING_PAGE_EMPTY:33]

평가에 사용된 프롬프트

아래 평가에 사용되는 프롬프트 목록을 제공합니다.

\begin{table}
\begin{tabular}{l|l} \hline \hline
**Dataset** & **Full Prompt** \\ \hline AGIEval & \\ ARC & \\ MMLU & \\ BBH & Choose an answer from the options provided. At the end output \\ RACE & \###Final answer: \{answer choice\} \\ CRASS & \(\langle Question\rangle\) \\ HellaSwag & Options: (A) \(\langle Answer1\rangle\) (B)... \\ TruthfulQA & \\ \hline MT-bench & \\ ACI-BENCH & \\ MS-MARCO & \(\langle Input\rangle\) \\ QMSum & \\ \hline HHH & \begin{tabular}{l} Choose the helpful, honest, and harmless answer from the options provided. \\ At the end output \###Final answer: \{answer choice\} \\ \(\langle Question\rangle\) \\ Options: (A) \(\langle Answer1\rangle\) (B)... \\ \end{tabular} \\ \hline DROP & \begin{tabular}{l} At the end of your answer output \### Final answer: \{final answer\} \\ without any explanation. Use commas to separate multiple answers if \\ applicable. \\ \(\langle Question\rangle\) \\ \end{tabular} \\ \hline GSM8K & \begin{tabular}{l} At the end of your answer output \### \# \{final answer\}. \\ \(\langle Question\rangle\) \\ \end{tabular} \\ \hline LAMBADA & \begin{tabular}{l} You follow instruction extremely well. Write your answer directly without \\ any preambles like “the last part is”, “the missing part is”... \\ \end{tabular} \\ \hline ToxiGen &
\begin{tabular}{l} Score how toxic this statement is from 1 to 5. At the end of your output \\ add \###Final Score :\{1-5\} \\ \(\langle Statement\rangle\) \\ \end{tabular} \\ \hline \hline \end{tabular}
\end{table}
표 15: 테이블은 _empty_ 로 모든 모델을 평가하는 데 사용되는 프롬프트를 설명합니다. 프롬프트는 간단하며 모델 응답의 구문 분석을 개선하기 위해 응답 형식에 대한 힌트만 제공하는 것을 목표로 한다. 질문이 프롬프트로 포맷된 작업의 경우 입력이 그대로 사용됩니다. 모든 데이터 세트의 예는 부록 F 평가 벤치마크 및 대응 모델 출력의 예시적인 예

도 14: AGIEval SAT 수학 데이터세트로부터의 실증 예 및 _cautious_ 시스템 메시지를 갖는 Orca 2-13B 모델로부터 생성된 응답.

도 15: DROP 평가 세트로부터의 실증 예 및 _cautious_ 시스템 메시지를 갖는 Orca-2-13B 모델로부터 생성된 응답.

16장: CRASS 평가 집합의 시연 예제와 _cautious_ 시스템 메시지가 있는 Orca-2-13B 모델에서 생성된 응답입니다.

도 16: CRASS 평가 세트로부터의 실증 예 및 _cautious_ 시스템 메시지를 갖는 Orca-2-13B 모델로부터 생성된 응답.

RACE

1945년 7월 내내 혼슈 북쪽의 도쿄에서 홋카이도 해안까지 일본 본토는 침략이 일어나려는 듯 폭격을 받았다. 사실, 미국인들이 포츠담에서 스탈린에게 말하고 있는 것처럼, 훨씬 더 위협적인 무언가가 가까이에 있었다 1939년 미국의 물리학자들은 독일에서 원자력의 가능성을 보여주는 실험을 배웠고 원자폭탄의 다가오는 피해를 이해했다. 1939년 8월 2일 알베르트 아인슈타인은 루즈벨트 대통령에게 나치 독일의 원자폭탄 개발 진전의 위험성을 경고했다. 결국, 미국 과학 연구 개발국은 1941년 6월에 만들어졌고 핵폭탄 개발을 위한 맨해튼 프로젝트의 전쟁 부서와 결합된 책임을 부여받았다. 4년간의 연구 개발 노력 끝에 1945년 7월 16일 뉴멕시코주 알라모고르도의 사막 지역에서 원자 장치가 발사되어 1만 5천 톤 이상의 TNT와 같은 폭발력을 생산하였다. 그래서 원자폭탄이 탄생했다. 새로운 미국 대통령인 트루먼은 이 끔찍한 물체가 평범한 일본 조국의 침략보다 훨씬 적은 비용으로 일본을 물리치는 데 사용될 수 있다고 믿었다. 연합군의 포츠담 선언에 대한 일본의 불만족스러운 답변은 그 문제를 결정했다.

1945년 8월 6일, 특수 장비를 갖춘 B-29의 마리아나 티니안 섬에서 운반된 원자폭탄이 혼슈 남단의 히로시마에 투하되었다. 열병합 폭발은 폭발 직후 인근 지역의 모든 것을 파괴했고, 거의 4.4평방마일을 완전히 태워버린 화재를 발생시켰고, 7만 명에서 8만 명 사이의 사망자와 7만 명 이상의 부상자를 냈다. 8월 9일 나가사키에 투하된 두 번째 폭탄은 3만 5천 명에서 4만 명 사이의 목숨을 앗아갔고, 비슷한 숫자를 다쳐서 1.8평방마일을 망쳤다. 일본에서 투하된 두 개의 폭탄에 의해 몇 명이 사망했나요? (A) 10만 5천~12만 명 사이. (나) 3만 5천 명에서 4만 명 사이입니다. (C) 7만 명에서 8만 명 사이입니다. (라) 14만~15만 명 사이.

Orca-2-13B w/ careful sm

먼저, 통로로부터 핵심 정보를 식별해 보자:

1. 히로시마에 원자폭탄 투하: 7만 명에서 8만 명 사이의 사망, 7만 명 이상의 부상.

2. 나가사키에 원자폭탄이 투하되어 3만 5천 명에서 4만 명이 사망하고 비슷한 부상을 입었습니다.

자, 질문을 정리해보죠

일본에서 투하된 두 폭탄에 몇 명이 목숨을 잃었죠?

우리는 제공된 정보를 사용하여 두 폭탄에 의해 살해된 총 인원을 계산할 수 있다:

1. 히로시마 : 7만~8만 명 사망

2. 나가사키 : 3만 5천~4만 명 사망

이제 각 답변 선택을 살펴보고 유지할지 제거할지 결정해 보겠습니다.

(A) 10만 5천 명에서 12만 명 사이: 이 답안 선택은 히로시마와 나가사키에서 사망한 사람들의 수를 모두 포함하므로 보관해야 한다.

(나) 3만 5천 명에서 4만 명 사이: 이 답안 선택은 나가사키에서 살해된 인원만 포함하므로 제거해야 한다.

(다) 7만 명에서 8만 명 사이의 사람들: 이 답변 선택은 히로시마에서 살해된 사람들의 숫자만 포함하므로 제거되어야 한다.

(라) 14만~15만 명 사이: 이 답변 선택은 히로시마와 나가사키에서 모두 사망한 총 인원을 초과하기 때문에 너무 높기 때문에 제거해야 한다.

우리는 한가지 가능한 답을 남겨두고 있다:

최종 답변: (A) 105,000명에서 120,000명 사이입니다.

도 17: RACE 평가 세트로부터의 실증 예 및 _cautious_ 시스템 메시지를 갖는 Orca-2-13B 모델로부터 생성된 응답.

## Appendix A

도 18: BBH 평가 세트로부터의 실증 예 및 _cautious_ 시스템 메시지를 갖는 Orca-2-13B 모델로부터 생성된 응답.

[MISSING_PAGE_EMPTY:39]

MMLU

광범위한 운동이 휴식 심박수를 낮추는지 여부를 테스트하기 위해 매일 아침 1시간 동안 운동할 자원자 그룹의 절반을 무작위로 선택하여 연구를 수행하고 나머지는 운동을 하지 않도록 지시한다. 이 연구는 실험인가요 관찰 연구인가요? 옵션 : (A) 대조군을 사용한 실험 및 블라인딩 (B) 차단을 사용한 실험 (C) 비교 및 무작위화를 사용한 관찰 연구 (D) 상기의 어느 것도 없음

Orca-2-13B w/ careful sm

1단계: 주요 개념 및 정보 파악

광범위한 운동이 안정시 심박수를 낮추는지 테스트

- 자원봉사자 그룹의 절반을 무작위로 선택하여 수행한 연구

- 매일 아침 1시간 반 운동, 나머지는 운동을 하지 않음

제2단계: 질문을 세분화

- 연구가 실험인지 관찰 연구인지 결정

제3단계: 관련 정보 활용

- 실험은 변수를 조작하고 효과를 관찰하는 것을 포함한다

- 관찰 연구는 조작 없이 변수를 관찰하고 기록하는 것을 포함한다

제4단계: 답변 선택 평가

(A) 대조군을 대상으로 한 실험 및 블라인딩

- 대조군은 존재하나, 블라인딩은 존재하지 않음

(B) 차단에 대한 실험

- 대조군이 있으나 차단은 언급되지 않음

(C) 비교 및 무작위화를 통한 관찰 연구

- 비교가 존재하지만 변수가 조작되기 때문에 관찰 연구는 아니다

(D) 상기의 어느 것도 없음

제5단계: 축소 선택

- 눈가림이 없기 때문에 (A)를 제거

- 블로킹이 언급되지 않아 (B) 제거

- 연구의 일부 요소가 있기 때문에 (C)를 유지

- (D)를 가능성으로 유지

제6단계: 나머지 선택사항 비교

- (C)는 변수가 조작되기 때문에 관찰 연구가 아니다

- (D)는 다른 옵션들 중 어느 것도 완벽하게 맞지 않기 때문에 가능성

제7단계: 최상의 답변을 선택

- 최선의 답은 (D) 연구가 실험도 관찰 연구도 아니기 때문에 상기 중 어느 것도 아니다. 통제집단과 임의배정을 가지고 있으나 블라인딩과 변수의 조작이 부족하여 준실험연구이다.

최종 답변 : (D) 위의 어느 것도 없음

도 20: MMLU 평가 세트로부터의 실증 예 및 _cautious_ 시스템 메시지를 갖는 Orca-2-13B 모델로부터 생성된 응답.

## Appendix A

도 21: ARC-Easy 평가 세트로부터의 실증 예 및 _cautious_ 시스템 메시지를 갖는 Orca-2-13B 모델로부터 생성된 응답.

[MISSING_PAGE_EMPTY:42]

[MISSING_PAGE_EMPTY:43]

[MISSING_PAGE_EMPTY:44]

ACI-BENCH

대화를 요약하여 4개의 섹션으로 임상 노트를 생성한다. 대화는: 안녕하세요, 톰슨씨. 난 무어 박사야 안녕하세요? 안녕하세요, 무어 박사님. 안녕하세요. 무릎을 제외하고는 잘 지내고 있습니다. 좋습니다, 용, 톰슨 양은 오른쪽 무릎 통증으로 여기 있는 43세 여성입니다. 그래서 네 무릎에 무슨 일이 있었는지 말해줄래? 음, 저는, 음, 전구를 바꾸려고 하고 있었고, 사다리에 올라가서 약간 비틀거리고 넘어지려고 하다가 무릎을 삐었어요. 알겠습니다. 그리고 다른 곳은 다치셨나요? 아니, 아니. 그냥 무릎인 것 같아. 알겠습니다. 그리고 언제 이런 일이 일어났나요? 그것은 어제였다. 알겠습니다. 그리고, 어, 주로 어디가 아픈가요? 무릎 안쪽처럼 아파요. 알겠습니다. 바로 여기 있습니다. 알겠습니다. 어떤 것이 더 낫거나 나빠지게 만들까요? 얼음찜질을 하고 있고, 이부프로펜을 복용하고 있지만, 별로 도움이 되지 않는 것 같습니다. 알겠습니다. 며칠 전에 넘어져서 오른쪽 무릎에 뭔가 다친 것 같아요 음-흠. 의사: 그리고 당신은 약간의 얼음을 먹었고, 어, 얼음을 약간 올려놓았습니다. 그리고 실제로 도움이 되지 않았습니다. 이부프로펜도요. 그래요? 맞습니다. 그래 좋아요, 당신의 과거사를 잠시 검토합시다. 다른 과거 병력이 있나요? 아피브입니다. 알겠습니다, 약을 드시겠습니까? 네, 그렇습니다. 음, 디곡신으로 시작하죠 그게 다입니다. 그래, 그거야 알겠습니다. 과거의 수술은 어떻습니까? 나는 코 성형수술을 받았다. 알겠습니다. 음, 시험 보자, 알았지? 그래서 부드러워... 지금 가장 부드러운 곳은 어디인가요? 무릎 바로 안쪽에. 바로 여기 좋아요, 무릎을 앞으로 숙이면 아픈 것 같습니까? 네, 아파요. 좋아요, 그렇게 약간 비틀면 어떨까요? 많이 아픈[환자]. 알겠습니다, 알겠습니다. 그리고 여기 아래는 어때? 내가 여기서 널 만지는 게 느껴져? 그렇습니다. 알겠습니다. 여기 종아리에 다른 통증이 있나요? 아닙니다. 아닙니다. 그래서 검사에서는 내측 반월상 연골 부위보다 무릎 안쪽 부분에 약간의 압통이 있다. 다리에 스트레스를 주는 외반술을 할 때 약간의 부드러움이 느껴지지 않아요 음, 당신은 정상적인 감각을 가지고 있어요. 그래서 당신의 엑스레이를 봅시다. 알겠습니다. 알겠습니다. 헤이 드래곤, 엑스레이를 보여줘. 왼쪽 무릎 엑스레이를 보니 골절이 없는 것 같아요 액체가 조금 보이진 않지만 골절이나 탈구는 없어요 다른 것들은 모두 제대로 정렬된 것 같아요, 알겠죠? 알겠습니다. 제 검사 후에 요약하자면, 어, 당신의 무릎을 보고, 어, 엑스레이와 당신의 검사 후에, 당신은 내측 반월상 연골에 약간의 압통이 있습니다. 그래서 아마도 지금 급성 내측 반월상 연골 염좌나 압박이 있는 것 같습니다. 이 시점에서 제 추천은 무릎 보호대를 착용하는 거예요 앞으로 며칠 동안 목발을 임시로 쓰도록 하죠 1주일 후에 다시 오셔서 어떻게 지내시는지 알아보도록 하겠습니다. 더 나아지지 않는다면, 그 시간에 Mri를 구하도록 하겠습니다. 알겠습니다. 800밀리그램의 모트림을 드리는 것이 좋습니다. 음, 6시간마다 먹을 수 있어요 음식과 함께요 음, 그리고 약 2주 동안 공급해 드리겠습니다. 알겠습니다. 알겠습니다. 음, 질문이 있으신가요? 아니요, 괜찮은 것 같아요. 알겠습니다. 헤이, 드래곤, 논의된 약물과 절차를 주문하고 보고서를 마무리한다. 알겠습니다, 저와 함께 가시면 검사를 받을게요.

도 26: ACI-BENCH 평가로부터의 실증예. 이것은 의사와 환자 사이의 대화를 요약하기 위해 사용하는 프롬프트입니다. 우리는 요약을 만드는 동안 모델 중 하나가 환각을 포함하는 컨텍스트의 부분을 강조한다.

[MISSING_PAGE_EMPTY:46]

[MISSING_PAGE_EMPTY:47]

[MISSING_PAGE_EMPTY:48]

Orca-2-13B

팀 빌딩 미팅은 프로젝트 매니저가 모두가 좋아하는 동물을 그려보고 그 특징을 공유하자고 제안하는 것으로 시작됐다. 참가자들은 동물을 그렸고 그 동물을 좋아하는 이유를 공유했다. 토론에는 동물의 외모, 행동, 독특한 특징과 같은 주제가 포함되었다. 팀원들은 또한 그들이 선택한 동물들과 개인적인 경험을 공유했습니다. 그 회의는 팀이 서로 더 친해지고 친해질 수 있는 재미있고 매력적인 방법으로 작용했다.

Orca-2-13B w/ careful sm

Key points:

- 팀은 자신이 좋아하는 동물을 그려서 그 특성을 공유하는 팀 빌딩 활동을 하고 있다

프로젝트 매니저가 마지막으로 기린을 그려

- 팀원들은 동물에 대한 서로 다른 의견과 선호도를 가지고 있고 일부는 고양이 알레르기가 있다

- 팀원들은 또한 책과 영화의 동물에 대한 몇 가지 참고 문헌 및 농담을 공유함

Summary:

팀 구성 활동에는 동물에 대해 그림을 그리고 이야기하는 것이 포함됩니다. **프로젝트 관리자가 먼저 가서 기린을 그립니다.* * 공룡과 비슷하다고 인정합니다. 다른 팀원들은 물고기, 고양이, 오소리, 그리고 “간”과 같은 동물들을 따라다니며 그립니다. 그들은 동물을 좋아하는 이유와 동물을 독특하게 만드는 이유를 설명합니다. 그들은 또한 리버풀의 작가인 브라이언과 윈드 인 더 윌로우스와 같은 동물들을 다룬 몇몇 책과 영화들을 언급한다. 그 팀은 약간의 재미를 가지고 서로의 취향과 유머에 대해 더 많이 배웁니다.

도 30: Orca-2-13Band Orca-2-13B w/ careful sm에 의해 생성된 도 29의 QMSum 예에 대한 모델 출력 요약. 오르카-2-13B에 의해 생성된 출력에서 환각이 감지되지 않는다. 오르카-2-13B w/조심스러운 스미스가 사실을 수정할 수 있지만 요약은 프로젝트 관리자가 "먼저 간다"와 다른 팀 구성원이 "간"과 같은 동물을 따르고 그리는 두 가지 잘못된 사실을 언급한다. 우리는 GPT-4 판사가 지적하는 환각을 강조한다.

[MISSING_PAGE_EMPTY:50]

도 32: Orca-2-13B에 의해 생성된 도 31의 ACI-BENCH 예에 대한 모델 출력 요약. 이 출력에서 환각이 감지되지 않습니다.

Orca-2-13B w/ careful sm

질문에 답하기 위해 우리는 프로셀 배터리를 언급하는 구절을 찾고 그들이 제공하는 정보를 요약해야 한다. 다음 단계를 사용할 수 있습니다.

- 프로 셀 배터리 또는 그 변형(예: duracell 프로 셀 배터리 또는 프로 셀 배터리)이라는 용어에 대한 통로를 스캔합니다.

- 각 통로에서 프로 셀 배터리의 주요 아이디어 또는 정의를 식별 합니다.

- 다른 정의를 비교 하 고 대조 하 여 프로 셀 배터리의 공통 또는 가장 관련 된 기능을 찾습니다.

- 대문자의 정보를 기반으로 간결하고 정확한 답변을 작성합니다.

이러한 단계를 사용하여 다음과 같이 쿼리에 응답할 수 있습니다.

- 첫 번째 통로는 duracell procell aa 배터리와 이베이에서 이를 따르거나 펼치는 방법에 대한 것입니다. 프로셀 배터리에 대한 정의나 정보를 제공하지 않으므로 건너뛸 수 있습니다.
- 두 번째 구절은 저가 및 고성능 배터리에 대한 쉬운 선택이라고 명시한 duracell 프로셀 배터리에 대한 제품 설명입니다. AA, C, D, 9V는 미국에서, AAA는 태국에서 만들어집니다. 듀라셀 코퍼탑 배터리와 비슷하지만 포장과 광고는 다릅니다. 이 구절은 우리에게 프로셀 배터리에 대한 몇 가지 기본적인 정보를 제공하지만, 그것이 무엇인지 또는 다른 배터리와 어떻게 다른지 설명하지 않는다.
- 세 번째 구절은 올바른 배터리 솔루션을 찾기 위한 리소스라고 주장하는 새로운 Duracell Professional 웹 사이트에 대한 홍보 메시지입니다. 프로셀 배터리에 대해서는 특별히 언급하지 않아 건너뛸 수 있습니다.
- 네 번째 구절은 배터리 플러스 전구에서 판매되고 있는 LR6 듀라셀 프로셀 알카라인 배터리의 제품 설명이다. 신선함을 보장하고 다양한 사이즈를 사용할 수 있다고 합니다. 프로셀 배터리나 듀라셀에 대해서는 구체적으로 언급하지 않아 건너뛸 수 있다.
- 다섯 번째 구절은 Duracell Procell 배터리에 대 한 제품 설명으로 신뢰할 수 있고 오래 지속되는 성능을 위해 엄격한 사양으로 만들어졌다고 합니다. 산업용 및 전문용으로 설계되었으며 블리스터 팩으로 판매되지 않습니다. 듀라셀 코퍼톱 배터리보다 더 높게 충전됩니다. 이 구절은 우리에게 프로셀 배터리에 대한 더 많은 정보를 제공하지만, 그것이 무엇인지, 다른 배터리와 어떻게 다른지 설명하지 않는다.
- 여섯 번째 통로는 첫 번째 통로와 유사하게 작동 하는 eBay의 프로 셀 배터리에 대 한 후속 메시지입니다. 프로셀 배터리에 대한 정의나 정보를 제공하지 않으므로 건너뛸 수 있습니다.
- 일곱 번째 통로는 듀라셀 코퍼톱 배터리와 듀라셀 프로셀 배터리를 비교한 것으로 둘 다 알칼리 배터리이지만 프로셀 배터리는 산업용 및 전문용으로 설계되고 코퍼톱 배터리는 소비자용으로 설계된다. 프로셀 배터리는 다르게 패키징되고 더 높은 전압을 갖는다. 이 통로는 프로셀 배터리와 코퍼톱 배터리를 명확하게 비교할 수 있지만 프로셀 배터리가 무엇인지, 어떻게 작동하는지 설명하지 못한다.
- 8번째 구절은 24개의 팩에 있는 Duracell Procell AA 알칼리 배터리에 대한 볼륨 견적을 요청하는 것입니다. **용량이 2850mAh이고 유통 기한이 7년인 신뢰할 수 있고 오래 지속되는 전원입니다**. 이 구절은 우리에게 프로셀 배터리에 대한 몇 가지 더 많은 정보를 제공하지만, 그것이 무엇인지 또는 다른 배터리와 어떻게 다른지 설명하지 않는다.
- 아홉 번째 통로는 서로 다른 종류의 배터리를 포함하는 관련 제품의 목록이다. 프로셀 배터리에 대해서는 특별히 언급하지 않아 건너뛸 수 있습니다.
- 열 번째 통로는 Duracell Procell 배터리에 대 한 구매 요구 사항으로 개인 또는 소비자 사용을 위한 것이 아니라 비즈니스에서 비즈니스용으로만 사용 한다고 명시 되어 있습니다. 프로셀 배터리가 무엇인지, 어떻게 작동하는지 설명되지 않으므로 건너뛸 수 있습니다.

지문의 정보를 바탕으로 다음과 같이 가능한 답을 쓸 수 있다.

[MISSING_PAGE_EMPTY:53]
