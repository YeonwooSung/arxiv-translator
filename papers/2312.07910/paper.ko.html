<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# PromptBench: 대용량 언어 모델 평가를 위한 통합 라이브러리\n' +
      '\n' +
      'Kaijie Zhu\\({}^{1,2}\\), Qinlin Zhao\\({}^{1,3*}\\), Hao Chen\\({}^{4}\\), Jindong Wang\\({}^{1}\\), Xing Xie\\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{1}\\)Microsoft Research Asia \\({}^{2}\\)Institute of Automation, Chinese Academy of Sciences\n' +
      '\n' +
      '중국과학기술대학\\({}^{3}\\)카네기멜론대학교\n' +
      '\n' +
      '처음 두 명의 저자는 동등하게 기여했다. Work done at MSRA.Corresponding author: Jinong Wang (jindong.wang@microsoft.com).\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 언어 모델(LLM)의 평가는 성능을 평가하고 잠재적인 보안 위험을 완화하는 데 중요합니다. 본 논문에서는 LLM을 평가하기 위한 통합 라이브러리인 PromptBench를 소개한다. 즉각적인 구축, 신속한 엔지니어링, 데이터셋 및 모델 로딩, 적대적 신속한 공격, 동적 평가 프로토콜, 분석 도구 등 연구자에 의해 쉽게 사용되고 확장되는 여러 주요 구성 요소로 구성된다. 프롬프트벤치는 새로운 벤치마크 생성, 다운스트림 애플리케이션 배포 및 새로운 평가 프로토콜 설계에서 독창적인 연구를 용이하게 할 수 있는 연구 목적을 위한 개방적이고 일반적이며 유연한 코드베이스가 되도록 설계되었다. 이 코드는 [https://github.com/microsoft/promptbench](https://github.com/microsoft/promptbench)에서 사용할 수 있으며 지속적으로 지원됩니다.\n' +
      '\n' +
      '1\n' +
      '각주 1: 처음 두 저자는 동등하게 기여했다. MSRA에서 일했어요\n' +
      '\n' +
      '\\(\\dagger\\). 교신저자: 진동왕(jindong.wang@microsoft.com).\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대형 언어 모델(LLM)은 전례 없이 인간의 삶과 사회의 거의 모든 측면에 영향을 미치면서 수많은 응용 분야에 혁명을 일으키고 있다. 이들은 세심하게 큐레이션된 말뭉치(Brown et al., 2020; Team, 2023; Computer, 2023)의 대규모로 훈련되어, 인간이 자연어를 통해 쉽고 유연하게 그들과 상호작용하여 복잡한 실세계 태스크(OpenAI, 2023b)를 프롬프트를 통해 해결할 수 있게 한다.\n' +
      '\n' +
      '평가는 LLM의 진정한 능력을 이해하고 잠재적인 위험을 완화하며, 결국 사회에 더 많은 이익을 주기 위해 가장 중요하다(Eisenstein, 2023; Chang et al., 2023). 최근의 노력들은 다양한 측면들로부터 LLM들을 평가하였다 (Liang et al., 2022; Zheng et al., 2023; Li et al., 2023; Huang et al., 2023). 발견 중 가장 중요한 것 중 하나는 현재의 LLM이 프롬프트에 민감하고(Wang et al., 2023b), 적대적 프롬프트 공격에 취약하고(Zhu et al., 2023b), 데이터 오염에 노출된다는 것이다(Willig et al., 2023; Zhou et al., 2023b; Zhu et al., 2023a). 이는 심각한 보안 및 프라이버시 문제를 제기한다(Wang et al., 2023a; Simmons, 2022). LLM의 인기가 높아짐에 따라 쉽고 빠르고 유연한 평가가 가능하도록 통일된 코드베이스를 개발하는 것이 필수적이다.\n' +
      '\n' +
      'LlamaIndex(Liu, 2022), semantic kernel(Microsoft, 2023), LangChain(Chase, 2022)과 같은 기존의 라이브러리는 다양한 다운스트림 애플리케이션에 대해 전체 LLM 파이프라인을 제공하지만, 평가 목적으로 본질적으로 구축되지 않는다. LlamaIndex와 LangChain은 데이터베이스와 다양한 데이터 소스를 통합하여 LLM 애플리케이션을 향상시켜 고급 컨텍스트 인식 기능을 가능하게 한다. 시맨틱 커널은 다재다능한 AI 앱 개발을 위해 AI 서비스를 프로그래밍 언어와 융합하는 것을 목표로 한다. 평가-하니스(Gao et al., 2023) 라이브러리는 생성 언어 모델을 평가하기 위한 포괄적인 프레임워크를 제공하지만 적대적 프롬프트 공격, 프롬프트 엔지니어링 및 동적 평가와 같은 다른 평가는 지원하지 않는다. 따라서, 특히 연구 목적을 위해 다양한 측면의 종합적인 평가를 전담하는 통일된 도서관이 절실히 필요하다.\n' +
      '\n' +
      '이 문서에서는 포괄적인 차원에서 LLM을 평가하기 위한 통합 python 라이브러리인 **PromptBench** 를 소개합니다.1 다양한 작업, 평가 프로토콜, 적대적 프롬프트 공격 및 프롬프트 엔지니어링 기술을 포괄하는 광범위한 LLM 및 평가 데이터 세트로 구성됩니다. 또한 전체론적인 라이브러리로서 결과를 해석하기 위한 여러 분석 도구를 지원한다. 저희 라이브러리는 모듈식 방식으로 설계되어 연구자들이 자체 프로젝트에 대한 평가 파이프라인을 쉽게 구축할 수 있습니다. 우리는 쉽고 유연하며 협력적인 평가를 지원하기 위해 포괄적인 문서와 자습서 2가 포함된 오픈 소스 프롬프트벤치를 제공한다. 우리는 프롬프트벤치가 LLM의 능력에 대한 이해를 높이고 커뮤니티의 새로운 연구를 촉진할 수 있다고 믿는다.\n' +
      '\n' +
      '각주 1: "PromptBench"라는 이름은 (Zhu 등, 2023b)와 동일하며, 여기서 우리는 적대적 신속 공격에 대한 견고성만을 평가했다. 우리는 그 이름을 유지하고 그 프로젝트를 크게 확장하기로 결정했다.\n' +
      '\n' +
      '각주 2: [https://promptbench.readthedocs.io/en/latest/](https://promptbench.readthedocs.io/en/latest/)\n' +
      '\n' +
      '## 2 PromptBench\n' +
      '\n' +
      '프롬프트벤치는 pip 설치 프롬프트벤치 또는 git 클론을 통해 쉽게 설치할 수 있습니다. 이 섹션에서는 PromptBench의 구성 요소와 LLMs에 대한 평가 파이프라인 구축에 사용하는 방법을 간략하게 소개한다. PromptBench의 개요는 그림 1에 나와 있다.\n' +
      '\n' +
      '### Components\n' +
      '\n' +
      '**모델.** PromptBench는 오픈 소스 및 독점 LLM을 모두 지원하며 더 추가할 수 있습니다. 현재 Flan-T5-large(Chung et al., 2022), Dolly(Databricks, 2023), Vicuna(Chiang et al., 2023), Llama2(Touvron et al., 2023b), Cerebras-GPT(Dey et al., 2023), GPT-NEOX(Black et al., 2022), Flan-UL2(Brain, 2023), phi-1.5(Li et al.,\n' +
      '\n' +
      '그림 1: PromptBench의 구성 요소와 지원 연구 영역.\n' +
      '\n' +
      '2023c), PaLM2(Anil et al., 2023), ChatGPT(OpenAI, 2023a), GPT-4(OpenAI, 2023b). PromptBench는 지정된 최대 생성 토큰 및 생성 온도를 사용하여 모델을 쉽게 구성하고 추론할 수 있도록 통합된 LLMModel 인터페이스를 제공합니다. 지원되는 모델에 대한 자세한 내용은 부록 A.1에 나와 있다.\n' +
      '\n' +
      '**데이터 세트 및 작업.** 현재 PromptBench는 22개의 공용 데이터 세트가 포함된 12개의 다양한 작업으로 구성되며 자연스럽게 더 많은 작업을 지원합니다. 감성 분석(SST-2(Socher et al., 2013)), 문법 정확성(CoLA(Warstadt et al., 2017)), 중복 문장 검출(QQP(Wang et al., 2017) 및 MRPC(Dolan and Brockett, 2005)), 자연어 추론(MNLI(Williams et al., 2018), QNLI(Wang et al., 2019), RTE(Wang et al., 2019) 및 WNLI(Levesque et al., 2012)), 멀티태스크 지식(MMLU 데이터셋(Hendrycks et al., 2021)), 번역(SQuAD V2 데이터셋(Rajpurkar et al., 2010)), IWSLT 2017(Cettolo et al., 2017)), 수학 문제 해결(Math(Saxton et al., 2019) 및 GSM8K(Cobbe et al., 2021)), 논리 추론(Boolean Expressions(벤치 저자, 2023)), 커먼센스 추론(Commensense QA(Talmor et al., 2019), QASC(Khot et al., 2020), NummerSense(\n' +
      '\n' +
      '**프롬프트 및 프롬프트 엔지니어링.** 프롬프트 벤치는 4가지 고유한 프롬프트 유형의 제품군을 제공하며, 사용자는 프롬프트 인터페이스를 사용하여 사용자 지정 프롬프트를 만들 수 있는 유연성을 제공합니다. 작업 지향 프롬프트는 모델이 예상하는 특정 작업을 명확하게 설명하는 반면 역할 지향 프롬프트는 모델을 전문가, 조언자 또는 번역자와 같이 정의된 역할에 배치하도록 구성됩니다. 이러한 프롬프트 범주는 제로 샷 및 소샷 학습 컨텍스트 모두에 적응할 수 있어 다양한 적용 가능성을 제공한다. 더욱이, PromptBench는 현재 6개의 두드러진 프롬프트 엔지니어링 방법: few-shot Chain-of-Thought(Wei et al., 2023), zero-shot Chain-of-Thought(Kojima et al., 2022), Emotion-Prompt(Li et al., 2023a), Expert Prompting(Xu et al., 2023), Generated Knowledge(Liu et al., 2022), 및 Least to Most(Zhou et al., 2023a)를 포함한다. 본 프레임워크는 프롬프트 엔지니어링 모듈을 통해 이러한 기존 기술을 쉽게 통합할 수 있을 뿐만 아니라 다양한 평가 시나리오에서 적응성을 향상시키면서 광범위한 프롬프트 엔지니어링 방법을 포함하도록 활발하게 진화하고 있다.\n' +
      '\n' +
      '**Adversarial 프롬프트 공격.** 프롬프트에 대한 LLM의 견고성 조사를 용이하게 하기 위해 PromptBench는 7가지 유형의 적대적 프롬프트 공격을 통합합니다 (Zhu et al., 2023b): TextBugger (Li et al., 2019), TextFooler (Jin et al., 2019), BertAttack (Li et al., 2020), DeepWordBug (Gao et al., 2018), Checklist (Ribeiro et al., 2020), StressTest (Naik et al., 2018) 및 semantics (Zhu et al., 2023b). 이러한 공격은 prompt_attack 인터페이스를 통해 쉽게 호출될 수 있다. 또한 LLM의 견고성을 효율적으로 평가하기 위해 선별된 적대적 프롬프트의 사용을 지원한다.\n' +
      '\n' +
      '**다른 평가 프로토콜.** 기본적으로 PromptBench는 표준 프로토콜, 즉 직접 추론을 지원합니다. PromptBench는 테스팅 데이터를 동적으로 생성함으로써 동적(Zhu 등, 2023a) 및 시맨틱(Liu 등, 2023) 평가 프로토콜을 더욱 지원한다. 데이터 오염을 피하기 위해 더 많은 새로운 프로토콜을 통합할 수 있다.\n' +
      '\n' +
      '**분석 도구.** 마지막으로 PromptBench는 연구자가 결과를 분석하는 데 도움이 되는 일련의 분석 도구를 제공합니다. 특히, 벤치마크 결과를 얻기 위해 스윕 실행을 지원한다. 그런 다음 유틸리티 인터페이스를 통해 주의 시각화 분석을 수행할 수 있다. 프롬프트벤치는 단어 수정 도구를 통합해 방어 분석뿐만 아니라 공격에 사용되는 단어를 분석하는 단어 빈도 분석도 지원한다.\n' +
      '\n' +
      '### Evaluation pipeline\n' +
      '\n' +
      'PromptBench는 4단계를 통해 평가 파이프라인을 쉽게 구성할 수 있습니다. 먼저 작업을 지정한 다음 pb.DatasetLoader를 통해 데이터 세트를 로드합니다. PromptBench는 원하는 데이터 세트를 로드하기 위한 간소화된 한 줄 API를 제공합니다. 둘째, 사용자는 허깅페이스에서 구현된 대부분의 LLM과 호환되는 통합 추론 파이프라인을 제공하는 pb.LLM 모델을 사용하여 LLM을 사용자 정의할 수 있다. 셋째, 지정 된 데이터 세트 및 작업에 대 한 프롬프트는 pb. 프롬프트를 통해 정의 됩니다. 사용자는 평가 및 성능 비교를 위한 프롬프트 목록을 입력할 수 있습니다. 프롬프트가 제공되지 않는 경우 데이터 세트에 대한 기본 프롬프트가 사용됩니다. 마지막으로 파이프라인은 pb.utils.dataprocess에 정의된 클래스 InputProcess와 클래스 OutputProcess를 통한 입력 및 출력 처리 함수와 pb.metrics를 통한 평가 함수를 필요로 한다. 구성 요소의 자세한 소개는 부록 A에 나와 있다.\n' +
      '\n' +
      '### 지원되는 연구 토픽\n' +
      '\n' +
      '프롬프트벤치는 주로 연구 목적으로 설계되어 다양한 주제에 대해 쉽게 커스터마이징할 수 있습니다. 그림 1(b)와 같이 벤치마크, 시나리오, 프로토콜 등 연구 커뮤니티와 다른 평가 주제를 지원한다. 벤치마크 연구에서는 표준 자연어 이해, 자연어 생성, 추론 과제를 지원한다. AI 에이전트에 대한 연구와 학제 간 연구를 지원하는 데에도 확장될 수 있다. 시나리오 연구에서는 적대적 평가와 분배 외 평가를 지원하며, 메트릭 및 데이터셋로더 인터페이스를 변경하여 환각 및 편향과 같은 다른 주제도 지원할 수 있다. 프로토콜에서는 표준 및 동적 평가를 자연스럽게 지원하며 측정 이론을 포함하여 추가로 검증할 수 있다.\n' +
      '\n' +
      '프롬프트벤치는 부록 B에서 볼 수 있듯이 적대적 신속 공격, 신속 엔지니어링 및 동적 평가의 세 가지 리더보드를 쉽게 비교할 수 있습니다. 연구자는 당사 플랫폼에 새로운 결과를 제출할 수 있습니다. 확장성은 프레임워크의 편리한 확장을 허용하는 부록 C에 나와 있다.\n' +
      '\n' +
      '## 3 Conclusion\n' +
      '\n' +
      '이 연구는 대규모 언어 모델의 평가를 위한 통합된 프레임워크인 프롬프트벤치를 제시했다. 라이브러리는 모듈식 방식으로 설계되어 사용자가 다양한 모델, 작업 및 프롬프트를 구성하여 평가 파이프라인을 구축할 수 있습니다. 또한 신속한 엔지니어링, 적대적 신속한 공격 및 동적 평가와 같은 여러 연구 방향을 용이하게 한다. 우리는 프롬프트벤치를 진정한 능력을 평가하고 현재 LLM의 경계를 탐색하는 첫 번째 단계로 취급하며 라이브러리의 벤치마크 및 분석 결과가 보다 강력하고 인간 정렬된 모델을 설계하는 데 빛을 발할 수 있다고 믿는다. 프롬프트벤치는 장기 프로젝트이며 연구 커뮤니티의 요구를 충족시키기 위해 적극적으로 업데이트될 것이다. 우리는 모든 잠재적 기여자들의 기여를 환영한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Anil et al. (2023) Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 기술 보고서. _ arXiv preprint arXiv:2305.10403_, 2023.\n' +
      '* BIG bench 저자들(2023) BIG bench 저자들. 모방 게임을 넘어 언어 모델의 기능을 정량화하고 외삽합니다. _ Transactions on Machine Learning Research_, 2023. ISSN 2835-8856. URL [https://openreview.net/forum?id=uyTL5Bvosj](https://openreview.net/forum?id=uyTL5Bvosj).\n' +
      '* Black 등(2022) Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. Gpt-neox-20b: 오픈 소스 자동 회귀 언어 모델, 2022. URL [https://arxiv.org/abs/2204.06745](https://arxiv.org/abs/2204.06745).\n' +
      '* Brain(2023) Google Brain. ul2가 포함된 새로운 오픈 소스 플란 20b, 2023. URL [https://www.gitay.net/blog/flan-ul2-20b](https://www.gitay.net/blog/flan-ul2-20b).\n' +
      '* Brown et al.(2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _ Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* Cettolo et al.(2017) Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian Stuker, Katsuhito Sudoh, Koichiro Yoshino, and Christian Federmann. IWSLT 2017 평가 캠페인의 개요 _Proceedings of the 14th International Conference on Spoken Language Translation_, pages 2-14, Tokyo, Japan, December 14-15. International Workshop on Spoken Language Translation. URL [https://aclanthology.org/2017.iwslt-1.1](https://aclanthology.org/2017.iwslt-1.1).\n' +
      '* Chang et al.(2023) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. arXiv preprint arXiv:2307.03109_, 2023.\n' +
      '* 체이스(2022) Harrison Chase. Langchain. [https://github.com/langchain-ai/langchain] (https://github.com/langchain-ai/langchain), 2022. Date released: 2022-10-17.\n' +
      '* Chiang 등(2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhang, Yonghao Zhang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: 90%의 채팅 품질로 gpt-4를 노출하는 오픈 소스 챗봇, 2023년 3월. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)\n' +
      '* Chung 등(2020)은 형원 정, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V.\n' +
      '\n' +
      '르, 제이슨 웨이 Scaling instruction-finetuned language models, 2022. URL [https://arxiv.org/abs/2210.11416](https://arxiv.org/abs/2210.11416).\n' +
      '* Cobbe et al.(2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 검증자를 학습하여 수학 단어 문제를 해결합니다. _ arXiv preprint arXiv:2110.14168_, 2021.\n' +
      '* Computer(2023) Together Computer. Redpajama: 2023년 대규모 언어 모델을 훈련하기 위한 개방형 데이터 세트입니다. URL [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data).\n' +
      '* Databricks (2023) Databricks. Hello dolly: 오픈 모델로 채팅의 마법을 민주화하는 2023. URL [https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html)입니다.\n' +
      '* Dey 등(2023) Nolan Dey, Gurpreet Gosal, Zhiming, Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, and Joel Hestness. Cerebras-gpt: 2023년 대뇌 웨이퍼 규모 클러스터에서 훈련 된 컴퓨팅 최적 언어 모델을 엽니다.\n' +
      '* Dolan and Brockett (2005) William B. Dolan and Chris Brockett. 센센셜 패러프레이즈 코퍼스를 자동으로 구성합니다. _IWP2005(Third International Workshop on Paraphrasing)_, 2005. URL [https://aclanthology.org/I05-5002](https://aclanthology.org/I05-5002).\n' +
      '* Eisele and Chen (2010) Andreas Eisele and Yu Chen. 다중 UN: 연합 국가 문서에서 나온 다국어 말뭉치. Proceedings of the 7번째 International Conference on Language Resources and Evaluation(LREC\'10)_, Valletta, Malta, May 2010. European Language Resources Association(ELRA). URL [http://www.lrec-conf.org/proceedings/lrec2010/pdf/686_Paper.pdf](http://www.lrec-conf.org/proceedings/lrec2010/pdf/686_Paper.pdf).\n' +
      '* 아이젠슈타인(2023) 마이클 아이젠슈타인. 인공 지능 테스트. _ Nature Outlook: Robotics and artificial intelligence_, 2023.\n' +
      '* Gao 등(2018) J. Gao, J. Lanchantin, M. L. Soffa, and Y. 기 딥러닝 분류기를 회피하기 위한 적대적 텍스트 시퀀스의 블랙박스 생성. _2018 IEEE Security and Privacy Workshops(SPW)_에서, 페이지 50-56, May 2018. doi: 10.1109/SPW.2018.00016.\n' +
      '* Gao 등(2021) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. few-shot 언어 모델 평가를 위한 프레임워크, 12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).\n' +
      '* Hendrycks 등(2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 대규모 다중 작업 언어 이해 측정 _학습 표현에 대 한 국제 회의_ 2021. URL [https://openreview.net/forum?id=d7KBjm13GmQ](https://openreview.net/forum?id=d7KBjm13GmQ)입니다.\n' +
      '* Hoffmann et al. (2022) Jordan Hoffmann et al. Training compute-optimal large language models, 2022.\n' +
      '\n' +
      '* Huang et al. (2023) Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. C-eval: A multi-level multi-discipline Chinese evaluation suite for foundation models. _ arXiv preprint arXiv:2305.08322_, 2023.\n' +
      '* Jin 등(2019) Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits. 버트는 정말 튼튼한가요? 텍스트 분류 및 수반 사항에 대 한 자연어 공격 _ arXiv preprint arXiv:1907.11932_, 2019.\n' +
      '* Khot 등(2020) Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. Qasc: 문장의 구성을 통한 질의응답을 위한 데이터셋, 2020.\n' +
      '* Kojima 등(2022) Takeshi Kojima, Shixiang Shane Gu, Michel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 대형 언어 모델은 제로 샷 추론기입니다. Alice H. Oh, Alekh Agarwal, Danielle Belgrave 및 경현 Cho에서 편집기 _신경 정보 처리 시스템의 Advances_, 2022. URL [https://openreview.net/forum?id=e2TBb5y0yFf](https://openreview.net/forum?id=e2TBb5y0yFf)입니다.\n' +
      '* Levesque 등(2012) Hector Levesque, Ernest Davis, and Leora Morgenstern. 위노그라드 스키마 도전 2012년 _제13회 지식 표현 및 추론 원칙에 관한 국제 회의_에서\n' +
      '* Li 등(2023a) Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, and Xing Xie. 대형 언어 모델은 감정 자극인 2023a에 의해 이해되고 향상될 수 있다.\n' +
      '* Li 등(2019) Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. 텍스트 버거: 실제 응용 프로그램에 대한 적대적 텍스트 생성 _Proceedings 2019 네트워크 및 분산 시스템 보안 심포지엄_ 에서입니다. Internet Society, 2019. doi: 10.14722/ndss.2019.23138. URL [https://doi.org/10.14722%2Fndss.2019.23138](https://doi.org/10.14722%2Fndss.2019.23138)\n' +
      '* Li 등(2020) Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. BERT-ATTACK: BERT를 이용한 BERT에 대한 적대적 공격. _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing(EMNLP)_, pages 6193-6202, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. emnlp-main.500. URL [https://aclanthology.org/2020.emnlp-main.500](https://aclanthology.org/2020.emnlp-main.500)\n' +
      '* Li 등(2023b) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacaeval: 명령어-추종 모델의 자동 평가기. _ Github repository_, 2023b.\n' +
      '* Li 등(2023c) Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 교과서는 ii가 필요한 전부입니다. **phi-1.5** 기술 보고서입니다. _ arXiv preprint arXiv:2309.05463_, 2023c.\n' +
      '* Liang et al.(2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. _ arXiv preprint arXiv:2211.09110_, 2022.\n' +
      '\n' +
      '* Lin 등(2020) Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang Ren. 새들은 네 개의 다리를 가지고 있나요? numersense: 사전 훈련된 언어 모델에 대한 수치적 상식 지식 조사, 2020.\n' +
      '* Liu(2022) Jerry Liu. LlamaIndex, 11 2022. URL [https://github.com/jerrryliu/llama_index](https://github.com/jerrryliu/llama_index)입니다.\n' +
      '* Liu 등(2022) Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and Hannaneh Hajishirzi. 2022년 상식 추론을 촉구하는 지식을 생성했습니다.\n' +
      '* Liu 등(2023) Yachuan Liu, Liang Chen, Jindong Wang, Qiaozhu Mei, and Xing Xie. 대용량 언어 모델의 평가를 위한 메타 시맨틱 템플릿. _ arXiv preprint arXiv:2310.01448_, 2023.\n' +
      '* Microsoft (2023) Microsoft. 시맨틱 커널. [https://github.com/microsoft/semantic-kernel] (https://github.com/microsoft/semantic-kernel), 2023.\n' +
      '* Naik 등(2018) Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, and Graham Neubig. 자연어 추론을 위한 스트레스 테스트 평가 방법. _ACL_에서, 페이지 2340-2353, Santa Fe, New Mexico, USA, August 2018. Association for Computational Linguistics. URL [https://aclanthology.org/C18-1198](https://aclanthology.org/C18-1198).\n' +
      '* OpenAI (2023a) OpenAI. [https://chat.openai.com.chat] (https://chat.openai.com.chat), 2023a.\n' +
      '* OpenAI(2023b) OpenAI. Gpt-4 기술 보고서, 2023b.\n' +
      '* Rajpurkar 등(2018) Pranav Rajpurkar, Robin Jia, and Percy Liang. 네가 모르는 것을 알아라: SQuAD에 대한 답할 수 없는 질문들. _ACL_에서 784-789 페이지, 호주 멜버른, 2018년 7월. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. URL [https://aclanthology.org/P18-2124](https://aclanthology.org/P18-2124)\n' +
      '* Ribeiro 등(2020) Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 정확성을 넘어 CheckList를 사용 하 여 NLP 모델의 행동 테스트입니다. _ACL_에서 4902-4912 페이지, Online, 2020년 7월. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.442. URL [https://aclanthology.org/2020.acl-main.442](https://aclanthology.org/2020.acl-main.442)\n' +
      '* Saxton 등(2019) David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 신경 모델의 수학적 추론 능력을 분석합니다. _ICLR_ 2019. URL [https://openreview.net/forum?id=H1gR5iR5FX](https://openreview.net/forum?id=H1gR5iR5FX)입니다.\n' +
      '* Simmons(2022) Gabriel Simmons. 도덕 모방: 대규모 언어 모델은 정치적 정체성에 맞춘 도덕적 합리화를 생성합니다. _ arXiv preprint arXiv:2209.12106_, 2022.\n' +
      '* Socher 등(2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 감성 트리뱅크에 대한 의미 구성성에 대한 재귀적 심층 모델입니다. _EMNLP_에서, 페이지 1631-1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/D13-1170](https://www.aclweb.org/anthology/D13-1170).\n' +
      '* Talmor 등(2019) Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: 상식 지식을 대상으로 하는 질문 응답 도전, 2019.\n' +
      '* Tafafi 등(2019)* Taori 등(2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: 명령어 추종 llama 모델. [https://github.com/tatsu-lab/stanford_alpaca] (https://github.com/tatsu-lab/stanford_alpaca), 2023.\n' +
      '* Team (2023) InternLM Team. Interlm: 점진적으로 향상된 기능을 가진 다국어 언어 모델입니다. [https://github.com/InternLM/InternLM] (https://github.com/InternLM/InternLM), 2023.\n' +
      '* Touvron 등(2023a) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023a.\n' +
      '* Touvron 등(2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023b.\n' +
      '* Wang 등(2019) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. 보우먼 GLUE: 자연어 이해를 위한 멀티 태스크 벤치마크 및 분석 플랫폼. 2019. In the Proceedings of ICLR.\n' +
      '* Wang and Komatsuzaki (2021) Ben Wang and Aran Komatsuzaki. GPT-J-6B: 60억 Parameter Autoregressive Language Model. [https://github.com/kingoflolz/mesh-transformer-jax] (https://github.com/kingoflolz/mesh-transformer-jax), 5 2021.\n' +
      '* Wang et al.(2023a) Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al. Decodingtrust: gpt 모델에서의 신뢰성에 대한 종합적인 평가. _ arXiv preprint arXiv:2306.11698_, 2023a.\n' +
      '* Wang 등(2023b) Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, et al. On robustness of chatgpt: An adversarial and out-of-distribution perspective. _Trustworthy and Reliable Large-Scale Machine Learning Models_에 관한 국제 학습 표현(ICLR) 워크숍에서, 2023b.\n' +
      '* Wang 등(2017) Zhiguo Wang, Wael Hamza, and Radu Florian. Bilateral multi-perspective matching for natural language sentences, 2017.\n' +
      '* Warstadt et al. (2018) Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. 신경망 허용 여부 판단 _ arXiv preprint arXiv:1805.12471_, 2018.\n' +
      '* Wei 등(2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 생각의 사슬은 2023년 대규모 언어 모델에서 추론을 이끌어낸다.\n' +
      '* Williams 등(2018) Adina Williams, Nikita Nangia, and Samuel Bowman. 추론을 통한 문장 이해를 위한 광범위한 도전 말뭉치입니다. _NAACL HLT_에서 페이지 1112-1122. Computational Linguistics, 2018. URL [http://aclweb.org/anthology/N18-1101](http://aclweb.org/anthology/N18-1101).\n' +
      '\n' +
      '* Willig 등(2023) Moritz Willig, Matej Zecevic, Devendra Singh Dhami, and Kristian Kersting. 인과 앵무새: 대규모 언어 모델은 인과 관계를 말할 수 있지만 인과 관계는 아닙니다. _ Transactions on machine learning research (TMLR)_, 8, 2023.\n' +
      '* Xu 등(2023) Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and Zhendong Mao. 전문가 프롬프트: 대규모 언어 모델을 2023년 저명한 전문가가 되도록 지시합니다.\n' +
      '* Zheng et al.(2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhang, Zhanghao Wu, Yonghao Zhang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arenna. _ arXiv preprint arXiv:2306.05685_, 2023.\n' +
      '* Zhou et al.(2023a) Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. 최소 프롬프트는 2023a의 대규모 언어 모델에서 복잡한 추론을 가능하게 한다.\n' +
      '* Zhou 등(2023b) Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. 평가 기준치를 부정행위자로 만들지 마세요 arXiv preprint arXiv:2311.01964_, 2023b.\n' +
      '* Zhu 등(2023a) Kaijie Zhu, Jiaoa Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, and Xing Xie. Dyval: 대용량 언어 모델의 그래프 정보 동적 평가_ arXiv preprint arXiv:2309.17167_, 2023a.\n' +
      '* Zhu 등(2023b) Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. _ arXiv preprint arXiv:2306.04528_, 2023b.\n' +
      '\n' +
      '## Appendix A Details of PromptBench\n' +
      '\n' +
      '### Models\n' +
      '\n' +
      '이 섹션에서는 프롬프트벤치에 구현된 LLM을 나열한다.\n' +
      '\n' +
      '* **Flan-T5-large (Chung et al., 2022)**: Text-to-Text Transfer Transformer (T5)의 변형인 Google의 Flan-T5-large입니다.\n' +
      '* **Dolly-6B(Databricks, 2023)**: Databricks에서 개발한 Dolly-6B 모델은 60억 매개 변수 인과 언어 모델입니다. 이는 52K 질문/답변 쌍을 포함하는 스탠포드의 알파카(Taori 등, 2023) 데이터세트로 더욱 정제된 EleutherAI의 GPT-J(Wang and Komatsuzaki, 2021)의 확장이다.\n' +
      '* **Vicuna 시리즈(Chiang 등, 2023)**: LLaMA-13B 기본 모델에서 개발된 Vicuna-13B는 ShareGPT.com의 70K 이상의 사용자 공유 대화를 통합하여 데이터 획득을 위한 공용 API를 활용합니다.\n' +
      '* **Cerebras series (Dey et al., 2023)**: GPT-3 아키텍처 상에서 모델링된 Cerebras-13B는 Chinchilla scaling laws (Hoffmann et al., 2022)에 따라 트레이닝되어 계산 효율을 최적화하는 Cerebras-GPT 시리즈의 일부이다.\n' +
      '* **Llama2 시리즈 (Touvron et al., 2023a)**: Meta AI의 FAIR 팀에 의해 엔지니어링 된 Llama2 모델은 변압기 아키텍처를 채택 하는 자기 회귀 언어 모델입니다.\n' +
      '* **GPT-NEOX-20B(Black 등, 2022)**: 광범위한 GPT 모델 시리즈의 일부인 이 변형은 대규모 언어 모델 구현을 예시하는 200억 개의 매개 변수를 특징으로 합니다.\n' +
      '* **Flan-UL2(Brain, 2023)**: 인코더-디코더 모델인 Flan-UL2는 T5 아키텍처에 기반을 두고 있으며 Flan 프롬프트 튜닝 및 데이터 세트 기술로 향상되었습니다.\n' +
      '* **ChatGPT(OpenAI, 2023a) 및 GPT-4(OpenAI, 2023b)**: OpenAI의 ChatGPT 및 GPT-4는 대화형 작업에 맞게 조정된 ChatGPT 및 GPT-4가 시리즈에서 가장 능숙한 GPT 시리즈의 고급 반복입니다.\n' +
      '* **phi-1.5 (Li et al., 2023c)**: phi-1.5는 13억 매개 변수를 가진 변압기 아키텍처이며, 다양한 NLP 합성 텍스트를 추가하여 phi-1에 사용되는 데이터 세트를 기반으로 합니다.\n' +
      '* **PaLM 2 (Anil et al., 2023)** PaLM 2는 다국어 및 추론 기능이 뛰어난 고급 언어 모델로, 이전 모델인 PaLM보다 더 큰 계산 효율성을 제공합니다. 이 트랜스포머 기반 모델은 영어, 다국어 작업 및 추론 과제의 다양한 모델 크기에 걸쳐 성능을 향상시킵니다.\n' +
      '\n' +
      '### 작업 및 데이터 집합\n' +
      '\n' +
      '* **GLUE (Wang et al., 2019)** GLUE 벤치마크 (일반 언어 이해 평가)는 언어를 이해 하는 NLP 모델의 기능을 평가 하는 일련의 작업을 제공 합니다. 이를 위해 Sentiment Analysis (SST-2 (Socher et al., 2013)), Grammar Correctness (CoLA (Warstadt et al., 2018)), Identifying Duplicate Sentences (QQP (Wang et al., 2017), MRPC (Dolan and Brockett, 2005) 및 다양한 Natural Language Inference Task (MNLI (Williams et al., 2018), QNLI (Wang et al., 2019), RTE (Wang et al., 2019), WNLI (Levesque et al., 2012)를 사용하였다.\n' +
      '\n' +
      '* **MMLU(Hendrycks 등, 2021)** MMLU 데이터 세트는 수학, 역사 및 컴퓨터 과학과 같은 분야의 객관식 질문이 있는 57개의 작업을 통해 대규모 언어 모델의 광범위한 지식과 문제 해결 기술을 테스트합니다. 포괄적인 멀티태스크 벤치마크입니다.\n' +
      '* **SQuAD V2(Rajpurkar 등, 2018)** SQuAD v2 데이터 세트는 읽기 이해를 위한 NLP 모델을 학습하고 평가하는 데 중추적입니다. 답할 수 없는 질문을 추가하여 원래 SQuAD를 기반으로 하므로 더 어렵습니다. 모델은 텍스트에서 정답을 식별하거나 질문을 대답할 수 없는 것으로 인식해야 한다.\n' +
      '* **UN Multi (Eisele and Chen, 2010)** 6개의 공식 UN 언어로 된 텍스트를 구성 하는 다중 UN 데이터 세트는 UN 문서의 방대한 병렬 코퍼스입니다. 그러나 공식 텍스트에 대한 초점은 비공식 또는 대화 언어 컨텍스트에서의 사용을 제한할 수 있다.\n' +
      '* **IWSLT 2017 (Cettolo et al., 2017)** 음성 언어 번역 시스템 평가를 위해 설계 된 IWSLT 2017 데이터 세트에는 주로 TED Talks Open Translation Project의 다국어 다중 도메인 텍스트 데이터가 포함 됩니다. 수많은 언어 쌍을 포함하여 번역 작업을 위한 풍부한 리소스를 제공합니다.\n' +
      '* **수학 (Saxton et al., 2019)** DeepMind Mathematics Dataset은 대수에서 미적분까지 다양한 수학 문제를 제기 하 여 AI 모델의 수학적 추론을 평가 합니다. 수학에서 모델의 이해 및 논리적 추론을 테스트 합니다.\n' +
      '* **BigBench(벤치 저자, 2023)** BIG-bench는 대규모 언어 모델의 기능을 평가하고 향후 잠재력을 예측하기 위해 설계된 협업 벤치마크입니다. 132개 기관의 444명의 저자가 기여한 200개 이상의 과제로 구성되어 있으며 언어학, 수학, 상식 추론 등과 같은 광범위한 주제를 다룬다. 이러한 작업은 언어 모델의 현재 능력을 넘어서는 것으로 여겨지는 영역을 조사하기 위한 것이다.\n' +
      '* **GSM8K(Cobbe 등, 2021)** GSM8K 데이터 세트는 8.5K 고품질 언어학적으로 다양한 초등학교 수학 단어 문제의 모음입니다. 인간 문제 집필진이 만든 것으로 7.5K 훈련 문제와 1K 시험 문제로 나뉜다. 2~8단계의 풀이가 필요한 이러한 문제들은 일차적으로 기본적인 산술 연산이 수반되며 밝은 중학생이 풀이할 수 있도록 설계되어 있다.\n' +
      '* **CommonsenseQA(Talmor 등, 2019)** CommonsenseQA 데이터 세트는 도전적인 상식 질문 응답 데이터 세트입니다. 각각 5개의 객관식 답변으로 12,247개의 문항으로 구성되어 있다.\n' +
      '* **QASC(Khot 등, 2020)** QASC(Question Answering via Sentence Composition)는 문장 구성에 중점을 두고 질문 응답 작업을 위해 설계된 전문 컬렉션입니다. 그것은 9,980개의 초등학교 과학에 대한 8방향 객관식 질문으로 구성되어 있으며, 훈련용 8,134개, 개발용 926개, 테스트용 920개로 나뉜다. (평가에서는 개발 부분을 사용한다.) 데이터셋은 각 질문에 답하기 위해 광범위한 코퍼스에서 사실의 검색과 구성이 필요한 멀티홉 추론에 중점을 두고 있다는 점에서 주목할 만하다.\n' +
      '* **NummerSense(Lin 등, 2020)** NumerSense는 3,145개의 마스킹된 단어 예측 프로브를 포함하는 진단 데이터 세트를 특징으로 하는 고유한 숫자 상식 추론 프로빙 작업입니다. 이 데이터 세트에는 지식 베이스 완성 및 개방형 질문 응답과 같은 작업에 응용 프로그램이 있다.\n' +
      '\n' +
      '### Evaluation protocols\n' +
      '\n' +
      'DyVal(Zhu 등, 2023a)은 정적 벤치마크에 의존하는 것과 달리 복잡도 맞춤 평가 샘플을 즉시 생성함으로써 LLM의 _동적 평가_를 위한 접근법이다. DyVal은 (1) 산술 계산과 선형 방정식 해결에 중점을 둔 수학, (2) 부울, 연역 및 귀납 논리를 포함하는 논리 추론, (3) 도달 가능성과 최대 합 경로 문제를 포괄하는 알고리즘 분석을 포함한 7가지 별개의 추론 과제를 합성했다. MSTemp(Liu et al., 2023)는 평가자 LLM과 단어 교체에 의존하여 분포 외 샘플을 생성하는 의미 평가 프로토콜을 의미한다.\n' +
      '\n' +
      '### Prompts\n' +
      '\n' +
      '#### a.4.1 Prompts\n' +
      '\n' +
      '우리의 연구는 의도된 기능과 라벨이 붙은 샘플의 필요한 수에 따라 구별되는 4가지 신속한 범주를 조사한다. 작업 지향 프롬프트는 모델의 작업을 명확하게 정의하도록 설계되어 고유한 사전 교육 지식을 사용하여 작업에 관련된 출력을 생성하도록 프롬프트합니다. 이에 비해 역할 중심은 모델을 전문가, 조언자 또는 번역가와 같이 특정 개체로 위치시켜 가정된 역할을 통해 예상되는 출력 형식과 행동을 미묘하게 안내한다. 두 카테고리 모두 제로-샷 및 소수의-샷 학습 컨텍스트에 대해 적응될 수 있다. 우리는 몇 개의 샷 예제를 형성하기 위해 각 작업에 대해 무작위로 세 개의 트레이닝 세트 예제를 선택한다. 다양한 프롬프트 유형의 예가 표 1에 예시되어 있다.\n' +
      '\n' +
      '#### a.4.2 Prompt Engineering\n' +
      '\n' +
      '프롬프트 엔지니어링은 AI 모델을 효율적으로 사용하기 위해 프롬프트를 구조화하고 최적화하는 과정이다. Methods in prompt engineering, 예를 들어 chain-of-thought(Wei et al., 2023), gener\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline \\multirow{3}{*}{Zero shot} & Task & Determine if the given pair of statements can be considered the same by \\\\  & oriented & responding with ‘equivalent’ or ‘not_equivalent’. \\\\ \\cline{2-3}  & Role & As an instrument for question comparison evaluation, consider the questions \\\\  & oriented & and determine if their meaning is the same, responding with ‘equivalent’ for \\\\  & similar questions or ‘not_equivalent’ for different questions. \\\\ \\hline \\multirow{4}{*}{Few shot} & Task & Review the sentence below and identify whether its grammar is ‘acceptable’ or ‘Unacceptable’: Here are three examples. Sentence: Our friends won’t \\\\  & oriented & buy this analysis, let alone the next one we propose. Answer: acceptable. Sentence: One more pseudo generalization and I’m giving up. Answer: acceptable. Sentence: They drank the pub. Answer: unacceptable. \\\\ \\cline{2-3}  & & Functioning as a grammar evaluation tool, analyze the given sentence and decide \\\\  & & if it is grammatically correct, responding with ‘acceptable’ or ‘unacceptable’: \\\\  & Role & Here are three examples. Sentence: Our friends won’t buy this analysis, \\\\  & oriented & let alone the next one we propose. Answer: acceptable. Sentence: One more \\\\  & & pseudo generalization and I’m giving up. Answer: acceptable. Sentence: They \\\\  & & drank the pub. Answer: unacceptable. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 4종류의 프롬프트의 예.\n' +
      '\n' +
      '(Liu et al., 2022) 등은 인공지능 모델의 추론 능력과 과제 수행 능력을 향상시키는데 도움을 준다. 6 가지 주요 프롬프트 엔지니어링 방법을 구현 합니다.\n' +
      '\n' +
      '* **Chain-of-Thought (Wei et al., 2023)**: 이 방법에는 복잡 하 고 여러 단계 문제를 더 작은 중간 단계로 분해 하 여 모델이 더 복잡한 추론 작업을 처리할 수 있도록 하는 것이 포함 됩니다. 사고의 사슬은 질문과 답을 제공하는 것뿐만 아니라 모델이 최종 답에 도달하기 전에 중간 추론 단계를 생성하도록 유도한다는 점에서 표준 소샷 프롬프트와 다르다.\n' +
      '* **Zero-Shot Chain-of-Thought (Kojima et al., 2022)**: Zero-Shot Chain of Thought는 프롬프트 프로세스를 단순화 하 여 Thought Chain of Thought를 개선 합니다. Zero-Shot Chain-of-Sinking의 핵심 혁신은 질문의 끝에 "단계별로 생각하자"라는 문구를 추가하는 것이다.\n' +
      '* **EmotionPrompt (Li et al., 2023a)**: 인간의 감성 지능에 대한 심리학 및 사회 과학 이론에서 영감을 그리는 이 방법은 기점 프롬프트에 감성 자극을 추가합니다. 예를 들어, "이것은 제 경력에 매우 중요합니다."\n' +
      '* **Expert Prompting (Xu et al., 2023)**: 모델이 역할 수행의 전문가가 되도록 하는 것이 핵심 아이디어입니다. 전문가 아이덴티티를 생성하기 위해 먼저 몇 가지 명령어-전문가 쌍 예제를 제공한 다음 모델이 이 질문의 전문가 아이덴티티를 생성한다. 마지막으로 전문가 신원을 조건으로 한 지침에 대한 답변을 모델에 요청한다.\n' +
      '* **생성 된 지식 (Liu 등, 2022)**: 생성 된 지식 프롬프팅은 모델이 먼저 지식을 생성 한 다음이 생성 된 정보를 추가 입력으로 사용 하 여 질문에 응답 하는 방법입니다. 과제별 지식 통합이나 구조화된 지식 기반을 요구하지 않고 AI에서 상식 추론을 향상시킨다.\n' +
      '* **Least to Most(Zhou 등, 2023a)**: Least to Most는 복잡한 문제를 일련의 더 간단한 하위 문제로 분해한 다음 차례로 해결합니다. 핵심 아이디어는 이전에 해결된 하위 문제에 대한 답을 사용하여 각 하위 문제를 해결하는 것이다. 이 방법은 프롬프트에 표시된 예시보다 더 어려운 문제를 해결해야 하는 작업에 특히 유용하다.\n' +
      '\n' +
      '#### a.4.3 적대적 프롬프트 공격\n' +
      '\n' +
      'Zhu 등(2023b)에 의해 제안된 바와 같은 적대적 신속 공격은 실제 시나리오에서 자연적으로 발생할 수 있는 잠재적 교란을 _시뮬레이션_하는 것을 목표로 한다. 제안된 프롬프트 공격은 사용자가 프롬프트를 입력할 때 종종 오타, 다양한 단어 선택, 다양한 문장 구성과 같은 다양한 실수를 하기 때문에 일반적인 사용자 오류 또는 표현과 유사하도록 의도된다. 상기 프롬프트 공격들은 4개의 별개의 레벨들을 포함한다:\n' +
      '\n' +
      '* **문자 수준:** TextBugger (Li et al., 2019) 및 DeepWordBug (Gao et al., 2018)와 같은 기술이 사용 됩니다. 이러한 방법은 문자를 변경하여 오류나 오타를 단어에 도입합니다.\n' +
      '* **단어 수준:** BertAttack(Li 등, 2020) 및 TextFooler(Jin 등, 2019)와 같은 공격이 활용됩니다. 그들은 단어를 동의어 또는 문맥적으로 유사한 대안으로 대체하는 데 중점을 둡니다.\n' +
      '* **문장 수준:** StressTest (Naik et al., 2018) 및 CheckList (Ribeiro et al., 2020)가 적용 됩니다. 이러한 공격은 프롬프트에 관련 없거나 중복되는 문장을 추가합니다.\n' +
      '\n' +
      '* **의미 수준:** 여러 전역 지역의 언어 스타일을 시뮬레이션합니다.\n' +
      '\n' +
      '### Pipeline\n' +
      '\n' +
      '평가를 위해 프롬프트벤치를 사용하는 전체 파이프라인은 그림 2에 나와 있다.\n' +
      '\n' +
      '## 부록 B 벤치마크 결과\n' +
      '\n' +
      '### Adversarial prompt robustness\n' +
      '\n' +
      '다양한 작업에 대한 다양한 모델의 견고성의 부분 결과는 그림 3에 나와 있다. 모든 모델은 적대적 프롬프트에 대한 취약성을 나타내며 ChatGPT 및 GPT-4가 가장 강력한 견고성을 보여준다.\n' +
      '\n' +
      '### Prompt engineering\n' +
      '\n' +
      '신속한 공학 결과는 그림 4에 나와 있다. 대부분의 방법은 특수 분야에 효과적이므로 이러한 방법이 모든 데이터 세트에서 기준선을 초과할 수 없다.\n' +
      '\n' +
      '### Dynamic evaluation\n' +
      '\n' +
      '그림 5는 다양한 모델과 작업에 걸친 동적 평가의 결과를 보여준다. GPT-4는 상대적으로 성능이 우수하지만, 선형 방정식, 귀납 논리 및 최대합 경로 태스크의 성능 향상 가능성이 남아 있다.\n' +
      '\n' +
      '도 2: LLMs의 평가를 위한 파이프라인.\n' +
      '\n' +
      '그림 4: 다양한 프롬프트 엔지니어링 기술 간의 비교.\n' +
      '\n' +
      '도 5: DyVal 결과.\n' +
      '\n' +
      '도 3: 적대적 프롬프트 강건성 결과.\n' +
      '\n' +
      '## 부록 C 확장성\n' +
      '\n' +
      'PromptBench의 각 모듈을 쉽게 확장할 수 있습니다. 다음에서는 자체 데이터 세트, 모델, 신속한 엔지니어링 방법 및 평가 메트릭을 사용자 지정하기 위한 기본 지침을 제공합니다.\n' +
      '\n' +
      '### 새 데이터 세트 추가\n' +
      '\n' +
      '새 데이터 세트를 추가 하는 데 두 단계가 포함 됩니다.\n' +
      '\n' +
      '1. 새 데이터 세트 클래스 구현: 데이터 세트는 dataloader/dataset.py로 구현되어야 하며 데이터 세트 클래스에서 상속됩니다. 사용자 지정 데이터 세트의 경우 _init_ 메서드를 구현하여 데이터 세트를 로드합니다. 입력 프로세스를 용이하게 하기 위해 데이터 샘플을 사전으로 구성하는 것이 좋습니다.\n' +
      '2. 인터페이스 추가: 데이터 세트 클래스를 사용자 지정 한 후 dataloader.py 내의 DataLoader 클래스에 등록 합니다.\n' +
      '\n' +
      '### 새 모델 추가\n' +
      '\n' +
      '새로운 데이터 세트를 추가하는 것과 유사하게 새로운 모델의 추가도 두 단계로 구성된다.\n' +
      '\n' +
      '1. 새 모델 클래스 구현: 모델은 LLMModel 클래스에서 상속되는 dataloader/model.py로 구현되어야 합니다. 사용자 지정 모델에서 self.tokenizer 및 self.model을 구현해야 합니다. 추론을 위해 자신의 예측 함수를 사용자 정의할 수도 있습니다. 예측 함수가 사용자 지정 되지 않은 경우 LLM 모델에서 상속 된 기본 예측 함수가 사용 됩니다.\n' +
      '2. 인터페이스 추가: 모델 클래스를 사용자 지정 한 후 _init_.py의 클래스 LLMModel 내의 _create_ 모델 함수에 등록 합니다.\n' +
      '\n' +
      '### 새 프롬프트 엔지니어링 메서드 추가\n' +
      '\n' +
      '신속한 공학에서 새로운 방법을 추가하는 것은 C.1 및 C.2의 단계와 유사하다.\n' +
      '\n' +
      '1. 새 메서드 클래스 구현: 메서드는 prompt_engineering 모듈에서 구현되어야 합니다. 먼저 메서드에 대 한 새 py 파일을 만듭니다. 그런 다음 _init_ 및 쿼리의 두 가지 함수를 구현 합니다. 통합 관리를 위해서는 두 가지 점에 유의해야 합니다. 1. 모든 메서드는 프롬프트 엔지니어링 메서드에 대한 공통 코드가 있는 기본 클래스에서 상속되어야 합니다. 2. 메서드에서 사용되는 프롬프트는 프롬프트/method_oriented.py에 저장되어야 합니다.\n' +
      '2. 인터페이스 추가: 새로운 메소드를 구현한 후 메소드 이름을 해당 클래스에 매핑하는 데 사용되는 METHOD_MAP에 등록한다.\n' +
      '\n' +
      '### 새 메트릭 및 입출력 프로세스 함수 추가\n' +
      '\n' +
      '새로운 평가 메트릭은 메트릭 모듈 내의 클래스 Eval에서 정적 함수로 구현되어야 한다. 유사하게, 새로운 입출력 프로세스 함수는 클래스 InputProcess에서 정적 함수, utils 모듈에서 클래스 OutputProcess로 구현되어야 한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>