# 멀티모달 대용량 언어 모델 조사

Shukang Yin\({}^{1}\)

Equal contribution.

Chaoyou Fu\({}^{2}\)\({}^{3}\)

Project leader.

Sirui Zhao\({}^{1}\)

Corresponding author.

Ke Li\({}^{2}\)

Xing Sun\({}^{2}\)

Tong Xu\({}^{1}\)

Enhong Chen\({}^{1}\)

({}^{1}\)CST, USTC & Cognitive Intelligence State Key Laboratory

\({}^{2}\) 텐센트 유튜랩

{xjtupanda,sirui}@mail.ustc.edu.cn, {tongxu,cheneh}@ustc.edu.cn

{bradyfu24}@gmail.com, {tristanli,winfredsun}@tencent.com

Equal contribution.

###### Abstract

최근 멀티모달 대용량 언어 모델(multimodal large language model, MLLM)은 강력한 대용량 언어 모델(large language model, LMM)을 두뇌로 사용하여 멀티모달 태스크를 수행하는 새로운 연구 핫스팟이다. 이미지에 기반한 이야기 쓰기, OCR이 없는 수학 추론과 같은 MLLM의 놀라운 창발 능력은 전통적인 방법에서는 드물며 인공 지능으로의 잠재적 경로를 제안한다. 본 논문에서는 MLLM의 최근 진행 상황을 추적하고 요약하는 것을 목표로 한다. 먼저 MLLM의 공식화를 제시하고 관련 개념을 설명한다. 그런 다음, M-IT(Multimodal Instruction Tuning), M-ICL(Multimodal In-Context Learning), M-CoT(Multimodal Chain of Thought), LAVR(LLM-Aided Visual Reasoning)을 포함한 주요 기술과 응용에 대해 논의한다. 마지막으로, 기존 과제를 논의하고 유망한 연구 방향을 지적한다. MLLM 시대가 이제 막 시작되었다는 점에 비추어, 우리는 이 조사를 계속 업데이트하고 그것이 더 많은 연구에 영감을 줄 수 있기를 바란다. 최신 문서를 수집하는 연결된 GitHub 링크는 [https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)에서 사용할 수 있습니다.

## 1 Introduction

최근 몇 년 동안 대형 언어 모델[1, 2, 3, 4]의 놀라운 발전이 있었다. 데이터 크기 및 모델 크기를 스케일링함으로써, 이러한 LLM은 전형적으로 ICL(In-Context Learning) [5], [4, 6], 및 CoT( Chain of Thought) [7]을 포함하는 놀라운 창발 능력을 높인다. LLM은 대부분의 자연어 처리(NLP) 작업에서 놀라운 제로/퓨샷 추론 성능을 보여주었지만 이산 텍스트만 이해할 수 있기 때문에 본질적으로 시각에 "맹목적인" 것이다. 동시에 큰 비전 기반 모델은 지각에서 빠른 진전을 이루며[8, 9, 10], 텍스트와의 전통적인 결합은 모달리티 정렬[11]과 과제 통합[12]에 더 주목하여 추론에서 천천히 발전한다.

이러한 보완성에 비추어 볼 때, 유니모달 LLM과 비전 모델은 동시에 서로를 향해 달려 결국 MLLM의 새로운 분야로 이어진다. 형식적으로는 멀티모달 정보로 수신하고 추론할 수 있는 능력을 갖춘 LLM 기반 모형을 의미한다. 인공지능(AGI)을 개발하는 관점에서 MLLM은 다음과 같은 이유로 LLM에서 한 걸음 더 나아갈 수 있다. (1) MLLM은 인간이 세상을 인식하는 방식과 더 일치한다. 우리 인간은 종종 보완적이고 협력적인 다중 감각 입력을 자연스럽게 받는다. 따라서, 멀티모달 정보는 MLLM을 보다 지능적으로 만들 것으로 예상된다. (2) MLLM은 보다 사용자 친화적인 인터페이스를 제공한다. 멀티모달 입력의 지원 덕분에, 사용자들은 보다 유연한 방식으로 지능형 어시스턴트와 상호작용하고 통신할 수 있다. (3) MLLM은 보다 균형 잡힌 태스크-해결기이다. LLM들이 통상적으로 NLP 태스크들을 수행할 수 있지만, MLLM들은 일반적으로 더 큰 스펙트럼의 태스크들을 지원할 수 있다.

GPT-4[2]는 그것이 보여주는 놀라운 예들 때문에 MLLM에 대한 연구 광풍을 불러일으킨다 그러나 GPT-4는 멀티모달 인터페이스를 개방하지 않으며, 현재까지 모델에 대한 정보가 공개되지 않았다. 그럼에도 불구하고, 유능하고 오픈소싱된 MLLM을 개발하기 위한 연구 커뮤니티의 많은 노력이 있었고, 이미지를 기반으로 웹사이트 코드를 작성하고[13], 밈의 깊은 의미를 이해하며[14], OCR이 없는 수학 추론[15]과 같은 몇 가지 놀라운 실용적인 능력이 발휘되었다. 우리는 연구자들에게 MLLM의 기본 아이디어, 주요 방법 및 현재 진행 상황을 파악하기 위해 이 설문조사를 작성한다. 우리는 주로 시각 및 언어 양식에 초점을 맞추지만 다른 양식과 관련된 작업도 포함한다는 점에 유의한다. 구체적으로, 기존의 MLLM들을 4가지 타입으로 나누고, 그 중 실시간으로 업데이트될 GitHub 페이지를 연다. 우리가 아는 한, 이것은 MLLM에 대한 첫 번째 조사이다.

## 2 Overview

본 논문은 최근 대표적인 MLLM을 M-IT(Multimodal Instruction Tuning), M-ICL(Multimodal In-Context Learning), M-CoT(Multimodal Chain-of-Thought), LAVR(LLM-Aided Visual Reasoning)의 네 가지 주요 장르로 분류한다. 첫 번째 3개는 MLLM의 기본을 구성하는 반면, 마지막 3개는 LLM을 핵심으로 하는 멀티모달 시스템이다. 상기 세 가지 기법들은 비교적 독립적이며, 조합되어 활용될 수 있다는 점에 유의한다. 따라서 컨셉의 예시는 다른 것도 포함할 수 있습니다.

네 가지 대범주에 따라 설문조사를 정리하고 순차적으로 소개한다. 우리는 LLM이 아키텍처와 데이터라는 두 가지 측면에서 다중 모드에 어떻게 적응될 수 있는지 밝히기 위해 M-IT(SS3.1)의 상세한 소개로 시작한다. 그리고 추론 단계에서 일반적으로 사용되는 효과적인 기법인 M-ICL(SS3.2)을 도입하여 적은 샷의 성능을 향상시킨다. 또 다른 중요한 기술은 M-CoT(SS3.3)이며, 이는 일반적으로 복잡한 추론 태스크에서 사용된다. 그 후, 우리는 LLM이 주로 사용하는 LAVR(SS3.4)에서 세 가지 기술을 자주 포함하는 몇 가지 역할을 추가로 요약한다. 마지막으로 요약 및 잠재적 연구 방향을 통해 설문조사를 마칩니다.

## 3 Method

### 다중 모드 명령어 튜닝

#### 3.1.1 Introduction

명령은 작업에 대한 설명을 나타냅니다. 명령어 튜닝은 명령어-포맷된 데이터세트들의 컬렉션 상에서 미리 트레이닝된 LLM들을 피니튜닝하는 것을 수반하는 기술이다[16]. 이러한 방식으로 튜닝하면, LLM은 새로운 지시에 따라 태스크를 보이지 않도록 일반화하여 제로 샷 성능을 높일 수 있다. 이 간단하면서도 효과적인 아이디어는 ChatGPT[1], InstructGPT[17], FLAN[16, 18], OPT-IML[19]과 같은 NLP 영역에서 후속 작업의 성공을 촉발했다.

지도 튜닝과 관련된 전형적인 학습 패러다임 간의 비교는 그림 1에 나와 있다. 지도 피니튜닝 접근법은 보통 태스크 특정 모델을 훈련시키기 위해 많은 태스크 특정 데이터가 필요하다. 신속한 접근 방식은 대규모 데이터에 대한 의존도를 줄이고 신속한 엔지니어링을 통해 전문화된 작업을 수행할 수 있다. 그러한 경우, 적은-샷 성능이 개선되었지만, 제로-샷 성능은 여전히 상당히 평균적이다[5]. 이와 달리, 지도 튜닝은 두 상대와 같은 특정 과제를 맞추는 것이 아니라 보이지 않는 과제로 일반화하는 방법을 학습한다. 더욱이, 명령어 튜닝은 멀티-태스크 프롬프트[20]와 매우 관련이 있다.

대조적으로, 전통적인 멀티모달 모델은 여전히 제로 샷 능력이 부족한 처음 두 튜닝 패러다임에 국한되어 있다. 따라서 최근 많은 연구[21, 22, 13]는 LLM에서 명령어 튜닝의 성공을 멀티모달리티로 확장하는 것을 탐구했다. 유니모달리티에서 멀티모달리티로 확장하기 위해서는 데이터와 모델 모두에 해당하는 적응이 필요하다. 데이터에 대해 연구자들은 보통 기존 벤치마크 데이터 세트[23, 24, 25, 26, 27, 28]를 적용하거나 자체 지침[21, 29, 13]을 사용하여 M-IT 데이터 세트를 획득한다. 모델과 관련하여 일반적인 접근법은 외국 양식의 정보를 LLM에 주입하고 강력한 추론자로 취급하는 것이다. 관련 작업은 외국 임베딩을 LLM에 직접 정렬하거나[21, 23, 24, 25, 27, 28, 30, 31, 32] 전문가 모델에 의존하여 외국 모달리티를 LLM이 섭취할 수 있는 자연 언어로 번역한다[33, 34]. 이렇게 공식화된 이 작업들은 멀티모달 명령어 튜닝을 통해 LLM을 멀티모달 챗봇[21, 22, 33, 35, 13]과 멀티모달 범용 태스크 해결사[23, 24, 26]로 변환한다.

이 섹션의 다음 부분에서는 먼저 기초 지식(SS3.1.2)을 제공합니다. M-IT의 묘사로 전환하기 전에, M-IT, _i.e._, 정렬 사전 트레이닝(SS3.1.3) 이전에 공통 프로세스를 추가로 도입한다. 그런 다음 그림 1과 같이 나머지 내용을 구조화한다. 2: 먼저 M-IT 데이터가 수집되는 방법을 소개하고(SS3.1.4), 이어서 MLLM에 대한 모델 적응, _i.e._, 상이한 모달리티들 사이의 갭을 브리징하는 다양한 방법들에 대한 상세한 논의가 이어진다(SS3.1.5). 마지막으로 명령어 조정 MLLM(SS3.1.6)을 평가하기 위한 평가 방법을 소개한다.

\begin{table}
\begin{tabular}{|l|} \hline \(<\)BOS\(>\) Below is an instruction that describes a task. \\ Write a response that appropriately completes the \\ request \\
**\#\# Instruction: \(<\)instruction\(>\)** \\
**\#\# Input: \(\{<\)image\(>\), \(<\)text\(>\)** \\
**\#\# Response: \(<\)output\(>\)\(<\)EOS\(>\)** \\ \hline \end{tabular}
\end{table}
표 1: 멀티모달 명령어 데이터를 구조화하기 위한 단순화된 템플릿. \(<\)명령\(>\)은 작업에 대한 텍스트 설명입니다. \ (\{<\)image\(>\), \(<\)text\(>\) 및 \(<\)output\(>\)은 데이터 샘플로부터 입출력된다. 입력에서 \(<\)text\(>\)은 이미지 캡션 데이터 집합과 같은 일부 데이터 집합에 대해 누락될 수 있습니다. \(<\)image\(>\) (<\)BOS\(>\) 및 \(<\)EOS\(>\)은 각각 LLM에 대한 입력의 시작과 끝을 나타내는 토큰이다. 예는 [31]로부터 각색된다.

그림 1: 세 가지 전형적인 학습 패러다임의 비교. 이미지는 [16]의 것이다.

#### 3.1.2 Preliminaries

이 절에서는 멀티모달 명령어 샘플의 일반적인 구조와 M-IT의 공통 프로세스를 간략하게 설명한다.

멀티모달 명령어 샘플은 종종 명령어 및 입력-출력 쌍을 포함한다. 명령어는 전형적으로 "_Describe the image in detail_"와 같이 태스크를 기술하는 자연어 문장이다. 입력은 입력일 수 있다.

도 2: 데이터 구축, 모달리티 브리징, 및 평가로 구성된 M-IT(Multimodal Instruction Tuning)의 분류.

VQA(Visual Question-Answering) 태스크와 같은 이미지-텍스트 쌍[46] 또는 이미지 캡셔닝 태스크와 같은 이미지만 [47]. 출력은 입력에 조건화된 명령어에 대한 답변이다. 명령 템플릿은 유연하고, 표 1에 예시된 바와 같이 수동 설계들[31, 21, 33]의 적용을 받는다. 명령 샘플들은 또한 멀티-라운드 명령들로 일반화될 수 있으며, 여기서 멀티모달 입력들은 공유된다[31, 30, 21, 43].

형식적으로 멀티모달 명령어 샘플은 트리플렛 형태로 표현될 수 있으며, 여기서 \(\mathcal{I},\mathcal{M},\mathcal{R}\)은 명령어, 멀티모달 입력 및 그라운드 트루스 응답을 각각 나타낸다. MLLM은 명령어와 멀티모달 입력이 주어진 답변을 예측한다:

\[\mathcal{A}=f(\mathcal{I},\mathcal{M};\theta) \tag{1}\]

여기서, \(\mathcal{A}\)은 예측된 답을 나타내고, \(\theta\)은 모델의 매개변수이다. 트레이닝 목표는 전형적으로 LLM들[32, 30, 21, 43]을 트레이닝하기 위해 사용되는 원래의 자동-회귀 목표이며, 이에 기초하여 MLLM은 응답의 다음 토큰을 예측하도록 강제된다. 목적은 다음과 같이 표현될 수 있다:

\[\mathcal{L}(\theta)=-\sum_{i=1}^{N}\log p(\mathcal{R}_{i}|\mathcal{I}, \mathcal{R}_{<i};\theta) \tag{2}\]

여기서, \(N\)은 지상-진실 응답의 길이이다.

#### 3.1.3 Modality Alignment

M-IT 이전의 서로 다른 모달리티(25, 35, 38, 29) 간의 정렬을 장려하기 위해 쌍을 이루는 데이터에 대해 대규모(명령어 조정과 비교) 사전 훈련을 수행하는 것이 일반적이다. 정렬 데이터 세트는 전형적으로 이미지-텍스트 쌍[48, 49, 50, 51, 52, 53, 54, 55, 56] 또는 자동 음성 인식(ASR) [57, 58, 59] 데이터 세트이며, 이들은 모두 텍스트를 포함한다. 보다 구체적으로, 이미지-텍스트 쌍들은 자연 언어 문장들의 형태로 이미지들을 기술하는 반면, ASR 데이터세트들은 스피치의 트랜스크립션들을 포함한다. 정렬 사전-훈련을 위한 일반적인 접근법은 사전-훈련된 모듈들(시각 인코더들 및 LLM들)을 동결 상태로 유지하고 학습가능한 인터페이스[37, 38, 21]를 훈련시키는 것이며, 이는 다음 섹션에 예시된다.

#### 3.1.4 Data

멀티모달 명령어 추종 데이터의 수집은 M-IT의 핵심이다. 수집 방법은 크게 벤치마크 적응, 자기 수업[60], 하이브리드 구성으로 분류할 수 있다. 우리는 이 세 가지 방법을 순차적으로 설명한다.

Benchmark AdaptationBenchmark 데이터 세트는 고품질 데이터의 풍부한 소스이다. 따라서 기존의 벤치마크 데이터셋을 활용하여 명령어 형식의 데이터셋을 구축하였다. 예를 들어 VQA 데이터 세트의 변환을 취하면, 원본 샘플은 입력이 이미지와 자연어 질문을 포함하는 입력-아웃 쌍이고 출력은 이미지에서 조건화된 질문에 대한 텍스트 답변이다. 이러한 데이터 세트의 입력-출력 쌍은 명령 샘플의 멀티모달 입력 및 응답을 자연스럽게 포함할 수 있다(SS3.1.2 참조). 작업에 대한 설명인 지침은 수동 설계에서 파생되거나 GPT를 통한 반자동 생성에서 파생될 수 있다. 구체적으로, 일부 작업들[36, 37, 25, 26, 13, 23] 핸드-크래프트는 후보 명령어들의 풀을 작업하고 트레이닝 동안 그들 중 하나를 샘플링한다. 우리는 표 2와 같이 VQA 데이터 세트에 대한 명령어 템플릿의 예를 제공한다. 다른 작업은 몇 가지 시드 명령어를 수동으로 설계하고 이러한 명령어를 사용하여 GPT가 더 많이 생성하도록 프롬프트한다[31, 33, 24].

기존의 VQA 및 캡션 데이터 세트의 답변은 일반적으로 간결하기 때문에 명령어 튜닝을 위해 이러한 데이터 세트를 직접 사용하는 것은 MLLM의 출력 길이를 제한할 수 있다. 이 문제를 해결하기 위한 두 가지 일반적인 전략이 있다. 첫 번째는 지침을 수정하는 것입니다. 예를 들어, ChatBridge[29]는 단답형 데이터에 대해서는 _short_ 및 _brief_를 명시적으로 선언하고, 캡션 데이터에 대해서는 _a sentence_ 및 _single sentence_를 명시적으로 선언한다. 마찬가지로 InstructBLIP [23]은 기본적으로 짧은 응답을 선호 하는 공용 데이터 세트에 대 한 명령 템플릿에 _short_ 및 _brief_ 를 삽입 합니다. 두 번째는 기존의 답변의 길이를 확장하는 것이다[36]. 예를 들어, M\({}^{3}\)IT [36]은 ChatGPT를 원래의 질문, 답변 및 컨텍스트로 프롬프트함으로써 원래의 답변을 재구문화하는 것을 제안한다.

자체 지침 기존 벤치마크 데이터 세트는 풍부한 데이터 소스에 기여할 수 있지만 일반적으로 여러 라운드의 대화와 같은 실제 시나리오에서 인간의 요구를 잘 충족하지 못한다. 이 문제를 해결하기 위해 일부 작업은 자체 명령[60]을 통해 샘플을 수집하며, 이는 LLM을 부트스트랩하여 몇 개의 손으로 주석이 달린 샘플을 사용하여 텍스트 명령 후속 데이터를 생성한다. 구체적으로, 일부 명령어-추종 샘플들은 시드 예들로서 수작업으로 제작되고, 그 후 ChatGPT/GPT-4는 안내로서 시드 샘플들과 함께 더 많은 명령어 샘플들을 생성하도록 프롬프트된다. LLaVA [21]은 이미지를 캡션 및 바운딩 박스의 텍스트로 변환하고, GPT-4가 시드 예제의 맥락에서 새로운 데이터를 생성하도록 유도함으로써 멀티모달 필드에 대한 접근을 확장한다. 이러한 방식으로, LLaVA-Instruct-150k라고 불리는 M-IT 데이터세트가 구축된다. 이 아이디어에 따라 MiniGPT-4 [13], ChatBridge [29], GPT4Tools [34], DetGPT [38]와 같은 후속 작업은 다양한 요구에 맞는 다양한 M-IT 데이터 세트를 개발한다.

M-IT 데이터와는 별도로, 언어 전용 사용자-보조 대화 데이터는 또한 대화 능률 및 명령어-추종 능력을 향상시키기 위해 사용될 수 있다[31, 32, 22, 35]. LaVIN은 언어 전용 데이터와 M-IT 데이터 모두에서 무작위로 샘플링하여 미니배치를 직접 구성한다. 멀티 인스트럭션 [26]은 혼합 명령어 튜닝(데이터 유형과 랜덤 셔플을 모두 조합), 순차 명령어 튜닝(텍스트 데이터에 이어 멀티모달 데이터) 및 어댑터 기반 순차 명령어 튜닝을 포함하는 단일 모달 및 멀티모달 데이터의 융합으로 훈련을 위한 상이한 전략을 탐색한다. 실험 결과는 혼합 명령어 튜닝이 멀티모달 데이터만을 튜닝하는 것보다 적어도 나쁘지 않다는 것을 보여준다.

#### 3.1.5 Modality Bridging

LLM은 텍스트만 인식할 수 있기 때문에 자연어와 다른 양식 사이의 격차를 해소하는 것이 필요하다. 그러나, 큰 멀티모달 모델을 종단간 방식으로 훈련시키는 것은 비용이 많이 들 것이다. 게다가, 그렇게 하는 것은 치명적인 망각의 위험을 감수할 것이다[61]. 따라서, 보다 실용적인 방법은 미리 훈련된 시각적 인코더와 LLM 사이에 학습 가능한 인터페이스를 도입하는 것이다. 다른 접근법은 전문가 모델의 도움으로 이미지를 언어로 번역한 다음 언어를 LLM으로 보내는 것이다.

학습 가능한 인터페이스 학습 가능한 인터페이스는 미리 훈련된 모델의 파라미터를 동결할 때 서로 다른 모달리티를 연결하는 역할을 한다. 문제는 시각적 콘텐츠를 LLM이 이해할 수 있는 텍스트로 효율적으로 번역하는 방법에 있다. 공통적이고 실현가능한 해결책은 질의 기반 방식으로 정보를 추출하기 위해 학습가능한 질의 토큰들의 그룹을 레버리지하는 것이다[62]. 이는 먼저 플라밍고[63] 및 BLIP-2[64]에서 구현되었고, 이후 다양한 작업에 의해 계승된다[42, 23, 25]. 또한, 일부 방법들은 투영 기반 인터페이스를 사용하여 모달리티 갭을 폐쇄한다[38, 21, 30, 43]. 예를 들어, LLaVA[21]은 이미지 특징들을 임베딩하기 위해 단순 선형 레이어를 채택하고 MedVInTE[30]는 브리지로서 2층 다층 퍼셉트론을 사용한다. 또한 매개변수 효율적인 튜닝 방식을 탐색하는 작업도 있다. LLaMA-Adapter [35, 28]는 훈련 중에 Transformer에 경량 어댑터 모듈을 도입한다. LaVIN[32]은 멀티모달 임베딩의 가중치를 동적으로 결정하기 위해 모달리티 혼합물 어댑터를 설계한다.

학습가능한 인터페이스와는 별개로, 이미지 캡셔닝 모델과 같은 전문가 모델을 사용하는 것은 또한 모달리티 갭을 메우기 위한 실현가능한 방법이다[35]. 이와 달리 전문가 모델 이면의 아이디어는 멀티모달 입력을 훈련 없이 언어로 변환하는 것이다. 이러한 방식으로, LLM은 변환된 언어들에 의한 멀티모달리티를 간접적으로 이해할 수 있다. 예를 들어 VideoChat-Text[33]는 미리 훈련된 비전 모델을 사용하여 동작과 같은 시각 정보를 추출하고 음성 인식 모델을 사용하여 설명을 풍부하게 한다. 전문가 모델을 사용하는 것은 간단하지만 학습 가능한 인터페이스를 채택하는 것만큼 유연하지 않을 수 있다. 외국 양식들을 텍스트로 변환하는 것은 전형적으로 정보 손실을 야기할 것이다. VideoChat-Text[33]가 지적하듯이, 비디오를 텍스트 설명으로 변환하는 것은 시공간 관계를 왜곡한다.

#### 3.1.6 Evaluation

M-IT 이후의 모델의 성능을 평가하기 위한 다양한 측정 지표들이 있는데, 이는 질문 장르에 따라 폐쇄 집합과 개방 집합을 포함하여 크게 두 가지 유형으로 분류할 수 있다.

폐쇄-집합 폐쇄-집합 질문은 가능한 답변 옵션이 미리 정의되고 유한 집합으로 제한되는 질문의 유형을 지칭한다. 평가는 일반적으로 벤치마크에 적응된 데이터 세트에서 수행된다. 이 경우, 응답들은 벤치마크 메트릭들에 의해 자연스럽게 판단될 수 있다[21, 23, 25, 26, 28, 29, 32, 35]. 예를 들어, Instruct

\begin{table}
\begin{tabular}{|c|} \hline \hline \(\bullet\) [Question] \\ \(\bullet\) [Question] \\ \(\bullet\) [Question] A short answer to the question is \\ \(\bullet\) [Question] A: \\ \(\bullet\) [Question] Short answer: \\ \(\bullet\) [Question] Given the image, answer the following question with no more than three words. \{Question\} \\ \(\bullet\) [Question] Based on the image, respond to this question with a short answer: \{Question\}. Answer: \\ \(\bullet\) [Question] Use the provided image to answer the question: \{Question\} Provide your answer as short as possible: \\ \(\bullet\) [Question] What is the answer to the following question? \({}^{*}\)[Question]\({}^{*}\) \\ \(\bullet\) [Question]\({}^{*}\) \\ \hline \hline \end{tabular}
\end{table}
표 2: [23]에서 인용 된 VQA 데이터 세트에 대 한 지침 템플릿입니다. \ (\bullet\)Image\(>\)과 \(\{\)Question\(\}\)은 각각 원본 VQA 데이터 세트의 이미지와 질문이다.

BLIP[23]은 ScienceQA[65]에 대한 정확도와 NoCaps[67] 및 Flickr30K[68]에 대한 CIDEr 점수[66]를 보고한다. 평가 설정은 전형적으로 제로 샷[23, 29, 36, 25, 28, 35] 또는 피니튜닝[23, 28, 32, 36, 37, 25, 21, 35]이다. 첫 번째 설정은 종종 서로 다른 일반 작업을 포함하는 광범위한 데이터 세트를 선택하고 보류 및 보류 데이터 세트로 분할한다. 전자를 튜닝한 후, 보이지 않는 데이터 세트 또는 심지어 보이지 않는 태스크를 사용하여 후자에서 제로 샷 성능을 평가한다. 대조적으로, 두 번째 설정은 도메인별 다운스트림 작업의 평가에서 종종 관찰된다. 예를 들어, LLaVA[21] 및 LLaMA-Adapter[28]는 ScienceQA[65]에서 미세 조정된 성능을 보고한다. LLaVA-Med[37]는 생물의학 VQA[69, 70, 71]에 대한 결과를 보고한다.

위의 평가 방법은 일반적으로 포괄적인 정량적 비교가 결여된 선택된 작업 또는 데이터 세트의 작은 범위로 제한된다. 이를 위해 MLLM(39, 72, 40)을 위해 특별히 설계된 새로운 벤치마크를 개발하기 위한 노력이 있었다. 예를 들어, Fu[73]은 총 14개의 인지 및 인지 과제를 포함하는 종합 평가 벤치마크 MME를 구성한다. MME의 모든 명령어-응답 쌍은 데이터 누설을 피하기 위해 수동으로 설계된다. 10개의 고급 MLLM이 자세한 리더보드와 분석을 통해 평가됩니다. LAMM-Benchmark [39]는 다양한 2D/3D 비전 작업에서 MLLM을 정량적으로 평가하기 위해 제안되었다. Video-ChatGPT[40]는 비디오 기반 대화 모델에 대한 정량적 평가 프레임워크를 제안하며, 이는 비디오 기반 생성 성능 평가 및 제로 샷 질문-응답의 두 가지 유형의 평가를 통합한다.

개방형 질문은 폐쇄형 질문과 달리 개방형 질문에 대한 응답이 더 유연할 수 있으며, 여기서 MLLM은 일반적으로 챗봇 역할을 한다. 채팅 내용은 임의적일 수 있기 때문에 폐쇄 출력보다 판단하는 것이 까다로울 것입니다. 기준은 수동 채점, GPT 채점, 사례 연구로 분류할 수 있다. 수동 채점은 인간이 생성된 반응을 평가해야 한다. 이러한 종류의 접근법은 종종 특정 차원을 평가하도록 설계된 수작업 질문을 포함한다. 예를 들어, mPLUG-Owl[22]은 자연 이미지 이해, 다이어그램 및 흐름도 이해와 같은 능력을 판단하기 위해 시각적으로 관련된 평가 세트를 수집한다. 유사하게, GPT4Tools[34]는 각각 피니튜닝 및 제로 샷 수행을 위한 두 세트를 구축하고, 사고, 액션, 주장 및 전체의 관점에서 응답을 평가한다.

수동 평가는 노동 집약적이기 때문에 일부 연구자들은 GPT, 즉 GPT 점수를 사용하여 등급을 탐구했다. 이 접근법은 종종 멀티모달 대화에서 성능을 평가하는 데 사용된다. LLaVA [21]은 유용성과 정확성과 같은 다양한 측면에서 GPT-4를 통해 응답을 점수를 매길 것을 제안한다. 구체적으로, 30개의 이미지가 GPT-4에 대한 자기 지시를 통해 짧은 질문, 상세한 질문 및 복잡한 추론 질문과 각각 연관된 COCO[48] 검증 세트로부터 샘플링된다. MLLM 및 GPT-4 둘 다에 의해 생성된 답변은 비교를 위해 GPT-4로 전송된다. 후속 작업들은 이 아이디어를 따르고 결과를 평가하거나 어느 것이 더 나은지 판단하도록 ChatGPT[22] 또는 GPT-4[36, 37, 25, 32, 29]를 프롬프트한다[37, 22, 29, 32, 25].

GPT-4 기반 점수의 주요 문제는 현재 다중 모드 인터페이스를 공개적으로 사용할 수 없다는 것이다. 결과적으로, GPT-4는 이미지에 액세스하지 않고 캡션들 또는 바운딩 박스 좌표들과 같은 이미지 관련 텍스트 콘텐츠에 기초하여 응답들만을 생성할 수 있다[37]. 따라서 GPT-4를 이 경우의 성능 상한으로 설정하는 것은 의문일 수 있다. 대안적인 접근법은 사례 연구를 통해 MLLM의 다양한 능력을 비교하는 것이다. 예를 들어, mPLUG-Owl은 시각적으로 관련된 농담 이해 사례를 사용하여 GPT-4[2] 및 MM-REACT[14]와 비교한다. 유사하게, Video-LLaMA[42]는 시청각 공동 인식 및 공통-지식 개념 인식과 같은 몇 가지 능력을 입증하기 위한 몇몇 사례를 제공한다.

다른 일부 다른 방법은 MLLM의 특정 측면에 중점을 둔다. 예를 들어, MultiInstruct [26]은 다양한 명령어에 대한 모델의 견고성을 평가하는 민감도라는 메트릭을 제안한다. Li[44]는 객체 환각 문제를 조사하고, 이와 관련하여 성능을 평가하기 위한 쿼리 방법 POPE를 제안한다. Zhao[45]는 안전성 문제를 고려하고 적대적 공격에 대한 MLLM의 견고성을 평가할 것을 제안한다.

### 다중 모드 In-Context 학습

ICL은 LLM의 중요한 비상 능력 중 하나이다. ICL에는 두 가지 좋은 특성이 있는데, (1) 풍부한 데이터로부터 암시적 패턴을 학습하는 전통적인 지도 학습 패러다임과는 다르며, ICL의 핵심은 유추로부터 학습하는 것이다[74]. 구체적으로, ICL 설정에서, LLM은 선택적인 명령어와 함께 몇몇 예들로부터 학습하고 새로운 질문들로 추론함으로써, 복잡하고 보이지 않는 과제들을 몇 샷 방식으로 해결한다[76, 75, 14]. (2) ICL은 보통 트레이닝-프리 방식으로 구현되며[74] 따라서 추론 단계에서 상이한 프레임워크에 유연하게 통합될 수 있다. ICL과 밀접하게 관련된 기술은 명령어 조정(SS3.1 참조)이며, 이는 ICL 능력을 향상시키는 것으로 경험적으로 보여진다[16].

MLLM의 맥락에서 ICL은 더 많은 모달리티로 확장되어 멀티모달 ICL(M-ICL)로 이어졌다. (SS3.1.2)의 설정을 기반으로 추론 시간에 M-ICL은 원본 샘플에 문맥 내 샘플 집합인 데모 세트를 추가하여 구현할 수 있다. 이 경우, 템플릿은 표 3에 예시된 바와 같이 확장될 수 있다. 예시하기 위해 두 개의 문맥 내 예들을 나열하지만, 예들의 수 및 순서는 유연하게 조정될 수 있다는 것에 유의한다. 사실, 모델들은 일반적으로 시연들의 배열에 민감하다[77, 74].

멀티모달리티에서의 애플리케이션 측면에서, M-ICL은

[MISSING_PAGE_FAIL:7]

이 경우, "프레임 단위로 생각하자" 또는 "이 두 키프레임 사이에 무슨 일이 일어났는지"와 같은 설계된 지시를 프롬프트함으로써[85, 86], 모델은 명시적인 안내 없이 내장된 지식과 추론 능력을 활용하는 것을 학습한다. 유사하게, 일부 작업[14, 83]은 복잡한 작업을 하위 작업으로 분해하기 위해 작업 및 도구 사용에 대한 설명과 함께 모델을 프롬프트한다.

#### 3.3.3 Chain Configuration

체인 구성은 추론의 중요한 측면이며 적응적인 형태와 미리 정의된 형태로 분류될 수 있다. 전자의 구성은 추론 체인들을 중지할 때 LLM들이 스스로 결정할 것을 요구하는 반면[14, 65, 76, 82, 83, 75], 후자의 설정은 미리 정의된 길이를 갖는 체인들을 중지한다[81, 84, 85, 86].

#### 3.3.4 생성 패턴

사슬이 어떻게 구성되는지는 연구할 가치가 있는 질문이다. 이를 위해 (1) 채우기 기반 패턴(infilling-based pattern)과 (2) 예측 기반 패턴( prediction-based pattern)으로 요약하였다. 구체적으로, 채움 기반 패턴은 논리적 갭들을 채우기 위해 주변 컨텍스트(이전 및 후속 단계들) 사이의 단계들을 추론할 것을 요구한다[85, 86]. 대조적으로, 예측 기반 패턴은 명령들 및 이전의 추론 이력(14, 65, 75, 82, 83, 76)과 같은 주어진 조건들을 추론 체인들을 확장시키는 것을 필요로 한다. 두 가지 유형의 패턴 공유

도 3: M-CoT(multimodal chain of Thought)의 분류. M-CoT의 주요 측면은 모달리티 브리징, 학습 패러다임, 체인 구성 및 생성 패턴을 포함한다.

생성된 단계가 일관되고 정확해야 한다는 요구 사항.

### LLM 지원 Visual Reasoning

#### 3.4.1 Introduction

공구 증강 LLMs[95, 96, 97, 98]의 성공으로 영감을 받은 일부 연구에서는 시각적 추론 작업을 위해 외부 도구[14, 34, 75, 76] 또는 비전 기반 모델[91, 14, 83, 92, 99, 14]을 호출할 가능성을 탐구했다. LLM들을 상이한 역할들을 갖는 도우미들로서 취하면서, 이러한 작업들은 태스크-특정 [93, 84, 90] 또는 범용 [14, 75, 76, 80, 83] 시각적 추론 시스템들을 구축한다.

기존의 시각적 추론 모델[100, 101, 102]과 비교할 때, 이러한 작업은 (1) 강력한 일반화 능력이라는 몇 가지 좋은 특성을 나타낸다. 대규모 사전 훈련으로부터 학습된 풍부한 개방 세계 지식을 구비한 이들 시스템은 현저한 제로/퓨샷 성능을 갖는 객체 또는 개념을 보이지 않도록 쉽게 일반화할 수 있다[94, 75, 76, 90, 91]. (2) 출현 능력. 이러한 시스템은 강력한 추론 능력과 LLM에 대한 풍부한 지식을 통해 복잡한 작업을 수행할 수 있다. 예를 들어, 이미지가 주어지면, MM-REACT[14]는 밈이 왜 재미있는지를 설명하는 것과 같이 표면 아래의 의미를 해석할 수 있다. (3) 더 나은 상호작용성 및 제어. 전통적인 모델들은 전형적으로 제한된 세트의 제어 메커니즘들을 허용하고 종종 값비싼 큐레이트된 데이터세트들을 수반한다[103, 104]. 대조적으로, LLM 기반 시스템은 사용자 친화적인 인터페이스(_e.g_. 클릭 및 자연 언어 쿼리)에서 미세 제어를 할 수 있는 능력을 갖는다[84].

이 섹션의 다음 부분은 그림 1에 표시된 대로 구성된다. 4: LLM-Aided Visual Reasoning System(SS3.4.2) 구축에 사용되는 다양한 훈련 패러다임을 도입하는 것으로 시작한다. 그 후, 우리는 LLM이 이러한 시스템 내에서 수행하는 주요 역할을 조사한다(SS3.4.3). 마지막으로 다양한 형태의 성과평가로 논의를 마무리한다.

#### 3.4.2 Training Paradigms

학습 패러다임에 따라 LLM-Aided Visual Reasoning 시스템은 학습 프리와 피니튜닝의 두 가지 유형으로 나눌 수 있다.

도 4: LLM-Aided Visual Reasoning(LAVR)의 분류. LAVR의 주요 측면은 훈련 패러다임, LLM의 주요 기능 및 성능 평가를 포함한다.

사전 훈련된 LLM에 저장된 풍부한 사전 지식으로 무훈련, 직관적이고 간단한 방법은 사전 훈련된 모델을 동결하고 LLM이 다양한 요구를 충족하도록 직접 프롬프트하는 것이다. 추론 시스템은 설정에 따라 몇 개의 샷 모델과 제로 샷 모델로 더 분류될 수 있다. 소수 샷 모델[14, 75, 76, 80]은 LLM이 프로그램 또는 실행 단계의 시퀀스를 생성하도록 안내하기 위해 수작업으로 조작된 몇 개의 문맥 내 샘플(SS3.2 참조)을 수반한다. 이들 프로그램들 또는 실행 단계들은 대응하는 기초 모델들 또는 외부 툴들/모듈들에 대한 명령어들로서 기능한다. 제로샷 모델은 LLM의 언어학/의미학 지식이나 추론 능력을 직접 활용하여 한 단계 더 나아간다. 예를 들어, PointCLIP V2[93]는 GPT-3에게 해당 이미지와의 더 나은 정렬을 위해 3D 관련 시맨틱스로 설명을 생성하도록 프롬프트한다. CAT[84]에서 LLM은 사용자 질의에 따라 캡션을 정제하도록 지시받는다.

Finetuning 툴 사용에 관한 계획 능력을 활성화하고 시스템의 명령-추종 능력을 향상시키기 위해, GPT4Tools [34]는 명령-튜닝 접근법을 도입한다(SS3.1 참조). 새로운 도구 관련 명령어 데이터 세트가 수집되고 모델을 조정하기 위해 사용된다.

#### 3.4.3 Functions

LLM-Aided Visual Reasoning 시스템에서 LLM이 정확히 어떤 역할을 하는지 더 조사하기 위해 기존 관련 작업은 세 가지 유형으로 나뉜다.

* 제어기로서의 LLM
* 결정 결정자로서 LLM
* 의미론적 정제기로서의 LLM

처음 두 역할, 즉 제어기와 의사결정자는 CoT와 관련된다(SS3.3 참조). 복잡한 작업을 중간 단계의 간단한 단계로 분해해야 하기 때문에 자주 사용된다. LLM이 컨트롤러 역할을 할 때 시스템은 종종 단일 라운드로 작업을 완료하는 반면 의사결정자의 경우 다중 라운드가 더 일반적이다. 우리는 LLM이 다음 부분에서 이러한 역할을 수행하는 방법을 설명한다.

이 경우 LLM은 (1) 복잡한 작업을 더 간단한 하위 작업/단계로 분해하고 (2) 이러한 작업을 적절한 도구/모듈에 할당하는 중앙 컨트롤러 역할을 한다. 첫 번째 단계는 LLM의 CoT 능력을 활용하여 마무리되는 경우가 많다. 구체적으로, LLM은 태스크 계획[80] 또는 더 직접적으로 호출할 모듈[75, 34, 76]을 출력하도록 명시적으로 프롬프트된다. 예를 들어, VISPROG[76]는 시각적 프로그램을 출력하기 위해 GPT-3를 프롬프트하고, 여기서 각각의 프로그램 라인은 서브-태스크를 수행하기 위해 모듈을 호출한다. 또한 모듈 입력에 대한 인수 이름을 출력하려면 LLM이 필요합니다. 이러한 복잡한 요구 사항을 처리하기 위해, 일부 수작업으로 조작된 인-컨텍스트(SS3.1 참조) 예가 참고문헌[75, 76, 80]으로 사용된다. 이는 추론 체인의 최적화(SS3.3 참조), 또는 더 구체적으로, 가장 적게 프롬프트하는 [105] 기법과 밀접한 관련이 있다. 이렇게 복잡한 문제는 순차적으로 해결되는 하위 문제로 분해된다.

결정자로서의 LLM 이 경우, 복잡한 태스크들은 종종 반복 방식으로, 다라운드 방식으로 해결된다[91]. 결정자들은 종종 다음과 같은 책임을 이행한다 : (1) 현재 컨텍스트 및 이력 정보를 요약하고, 현재 단계에서 이용 가능한 정보가 질문에 답하거나 작업을 완료하기에 충분한지를 결정하며; (2) 사용자 친화적인 방식으로 제시하기 위해 답변을 조직하고 요약한다.

LLM이 Semantics Refiner로 사용될 때, 연구자들은 주로 풍부한 언어학 및 의미학 지식을 활용한다. 구체적으로, LLM은 종종 정보를 일관되고 유창한 자연어 문장으로 통합하거나[94] 상이한 특정 요구에 따라 텍스트를 생성하도록 지시받는다[93, 84, 90].

#### 3.4.4 Evaluation

LLM-지원 시각적 추론 시스템의 성능을 평가하는 두 가지 방법, 즉 벤치마크 메트릭[94, 76, 75, 91, 93]과 수동 평가[92, 92, 34]가 있다.

Benchmark 메트릭 간단한 평가 방법은 메트릭이 모델이 작업을 완료하는 정도를 직접 반영할 수 있기 때문에 기존 벤치마크 데이터 집합에서 시스템을 테스트하는 것입니다. 예를 들어, Chameleon[75]은 ScienceQA[65] 및 TabMWP[106]를 포함하는 복잡한 추론 벤치마크에 대해 평가된다. IdealGPT[91]는 VCR[107] 및 SNLI-VE[108]에 대한 정확도를 보고한다.

수동 평가 일부 작업은 모델의 특정 측면을 평가하기 위해 수동 등급을 채택한다. 예를 들어, Chat-Captioner[92]는 인간 주석자에게 상이한 모델에 의해 생성된 캡션의 존재비 및 정확성을 판단하도록 요청한다. GPT4 도구 [34]는 도구 사용 할당에 대한 모델의 기능을 측정하기 위해 사고, 작업, 인수 및 전체 성공률을 계산합니다. VISPROG[76]는 언어 유도 이미지 편집 작업에서 모델을 평가할 때 정확도를 수동으로 계산한다.

## 4가지 도전과 미래 방향

MLLM의 개발은 아직 초보 단계에 있으므로 개선의 여지가 많이 남는데, 이를 요약하면 다음과 같다.

* 현재의 MLLM은 여전히 지각 능력이 제한되어 불완전하거나 잘못된 시각 정보 획득으로 이어진다[13, 73]. 이는 정보 용량과 계산 부담 사이의 절충 때문일 수 있다. 보다 구체적으로, Q-Former [64]는 이미지를 표현하기 위해 32개의 학습 가능한 토큰만을 사용하는데, 이는 정보 손실을 유발할 수 있다. 그럼에도 불구하고, 토큰 크기를 스케일업하는 것은 입력 길이가 일반적으로 제한되는 LLM들에 더 큰 계산 부담을 가져올 수밖에 없다. 잠재적인 방법은 SAM[8]과 같은 대형 비전 기초 모델을 도입하여 시각 정보를 보다 효율적으로 압축하는 것이다[21, 29].
* MLLM의 추론 체인은 취약할 수 있습니다. 예를 들어, Fu[73]은 수학 계산의 경우, MLLM이 정확한 결과를 계산하지만, 여전히 추론의 깨짐으로 인해 오답을 전달한다는 것을 발견한다. 이는 단봉형 LLM의 추론 능력이 시각적 정보를 수신한 후 LLM의 추론 능력과 동일하지 않을 수 있음을 나타낸다. 멀티모달 추론을 개선하는 주제는 조사할 가치가 있다.
* MLLM의 지시 준수 기능은 업그레이드가 필요합니다. M-IT 이후, 일부 MLLM은 명시적인 지시에도 불구하고 예상된 답변("예" 또는 "아니오")을 생성하지 못한다. "예 또는 아니요로 답변해 주세요."[73]. 이는 명령어 튜닝이 일반화를 개선하기 위해 더 많은 작업을 다루어야 할 수 있음을 시사한다.
* 대상 환각 문제는 널리 퍼져 있습니다 [13, 44]. 이는 MLLM의 신뢰성에 크게 영향을 미칩니다. 이것은 불충분한 정렬 프리트레이닝[13]에 기인할 수 있다. 따라서, 가능한 해결책은 시각적 모달리티와 텍스트 모달리티 사이에서 보다 세밀한 정렬을 수행하는 것이다. 미세 입도는 SAM[21, 29]에 의해 획득될 수 있는 이미지들의 로컬 특징들 및 대응하는 로컬 텍스트 설명들을 지칭한다.
* 매개 변수 효율적인 교육이 필요합니다. 기존의 두 가지 모드 브리징 방식인 학습 가능한 인터페이스와 전문가 모델은 모두 계산 부담을 줄이기 위한 예비 탐색이다. 더 효율적인 트레이닝 방법들은 제한된 계산 자원들을 갖는 MLLM들에서 더 많은 전력을 잠금해제할 수 있다.

## 5 Conclusion

본 논문에서는 기존 MLLM 문헌에 대한 조사를 수행하고 3가지 일반적인 기술(M-IT, M-ICL, M-CoT)과 태스크 해결 시스템(LAVR) 구축을 위한 일반적인 프레임워크를 포함하여 주요 방향을 폭넓게 제공한다. 또한, 우리는 현재 채워져야 할 연구 격차를 강조하고 몇 가지 유망한 연구 방향을 지적한다. 우리는 이 조사가 독자들에게 MLLM의 현재 진행 상황에 대한 명확한 그림을 제공하고 더 많은 작업에 영감을 줄 수 있기를 바란다.

## References

* [1] OpenAI, "Chatgpt: A language model for conversational ai," OpenAI, Tech. Rep., 2023. [Online]. Available: [https://www.openai.com/research/chatgpt](https://www.openai.com/research/chatgpt) 1, 2
* [2] OpenAI, "Gpt-4 technical report," _arXiv:2303.08774_, 2023.
* [3] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez _et al._, "Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality," 2023. [Online]. Available: [https://vicuna.lmsys.org](https://vicuna.lmsys.org) 1
* [4] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar _et al._, "Llama: Open and efficient foundation language models," _arXiv:2302.13971_, 2023.
* [5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell _et al._, "Language models are few-shot learners," _NeurIPS_, 2020.
* [6] B. Peng, C. Li, P. He, M. Galley, and J. Gao, "Instruction tuning with gpt-4," _arXiv:2304.03277_, 2023.
* [7] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou, "Chain of thought prompting elicits reasoning in large language models," _arXiv:2201.11903_, 2022.
* [8] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo _et al._, "Segment anything," _arXiv:2304.02643_, 2023.
* [9] H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. M. Ni, and H.-Y. Shum, "Dino: Detr with improved denoising anchor boxes for end-to-end object detection," _arXiv:2203.03605_, 2022.
* [10] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby _et al._, "Dinov2: Learning robust visual features without supervision," _arXiv:2304.07193_, 2023.
* [11] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark _et al._, "Learning transferable visual models from natural language supervision," in _ICML_, 2021.
* [12] P. Wang, A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou, J. Zhou, and H. Yang, "Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework," in _ICML_, 2022.
* [13] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, "Minigpt-4: Enhancing vision-language understanding with advanced large language models," _arXiv:2304.10592_, 2023.
* [14] Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu, M. Zeng, and L. Wang, "Mm-react: Prompting chatgpt for multimodal reasoning and action," _arXiv:2303.11381_, 2023.
*[*[15] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. 붕태 Yu _et al._, "Palm-e: An embodied multimodal language model," _arXiv:2303.03378_, 2023.
* [16] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, "Finetuned language models are zero-shot learners," _arXiv:2109.01652_, 2021.
* [17] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray _et al._, "Training language models to follow instructions with human feedback," _NeurIPS_, 2022.
* [18] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma _et al._, "Scaling instruction-finetuned language models," _arXiv:2210.11416_, 2022.
* [19] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura _et al._, "Opt-iml: Scaling language model instruction meta learning through the lens of generalization," _arXiv:2212.12017_, 2022.
* [20] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja _et al._, "Multitask prompted training enables zero-shot task generalization," _arXiv:2110.08207_, 2021.
* [21] H. Liu, C. Li, Q. Wu, and Y. J. Lee, "Visual instruction tuning," _arXiv:2304.08485_, 2023.
* [22] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi _et al._, "mplug-owl: Modularization empowers large language models with multimodality," _arXiv:2304.14178_, 2023.
* [23] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi, "Instructblip: Towards general-purpose vision-language models with instruction tuning," _arXiv:2305.06500_, 2023.
* [24] W. Wang, Z. Chen, X. Chen, J. Wu, X. Zhu, G. Zeng, P. Luo, T. Lu, J. Zhou, Y. Qiao _et al._, "Visionllm: Large language model is also an open-ended decoder for vision-centric tasks," _arXiv:2305.11175_, 2023.
* [25] F. Chen, M. Han, H. Zhao, Q. Zhang, J. Shi, S. Xu, and B. Xu, "X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages," _arXiv:2305.04160_, 2023.
* [26] Z. Xu, Y. Shen, and L. Huang, "Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning," _arXiv:2212.10773_, 2022.
* [27] B. Li, Y. Zhang, L. Chen, J. Wang, J. Yang, and Z. Liu, "Otter: A multi-modal model with in-context instruction tuning," _arXiv:2305.03726_, 2023.
* [28] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, and Y. Qiao, "Llama-adapter: Efficient fine-tuning of language models with zero-init attention," _arXiv:2303.16199_, 2023.
* [29] Z. Zhao, L. Guo, T. Yue, S. Chen, S. Shao, X. Zhu, Z. Yuan, and J. Liu, "Chatbridge: Bridging modalities with large language model as a language catalyst," _arXiv:2305.16103_, 2023.
* [30] X. Zhang, C. Wu, Z. Zhao, W. Lin, Y. Zhang, Y. Wang, and W. Xie, "Pmc-vqa: Visual instruction tuning for medical visual question answering," _arXiv:2305.10415_, 2023.
* [31] T. Gong, C. Lyu, S. Zhang, Y. Wang, M. Zheng, Q. Zhao, K. Liu, W. Zhang, P. Luo, and K. Chen, "Multimodal-gpt: A vision and language model for dialogue with humans," _arXiv:2305.04790_, 2023.
* [32] G. Luo, Y. Zhou, T. Ren, S. Chen, X. Sun, and R. Ji, "Cheap and quick: Efficient vision-language instruction tuning for large language models," _arXiv:2305.15023_, 2023.
* [33] K. Li, Y. He, Y. Wang, Y. Li, W. Wang, P. Luo, Y. Wang, L. Wang, and Y. Qiao, "Videochat: Chat-centric video understanding," _arXiv:2305.06355_, 2023.
* [34] R. Yang, L. Song, Y. Li, S. Zhao, Y. Ge, X. Li, and Y. Shan, "Gpt4tools: Teaching large language model to use tools via self-instruction," _arXiv:2305.18752_, 2023.
* [35] P. Gao, J. Han, R. Zhang, Z. Lin, S. Geng, A. Zhou, W. Zhang, P. Lu, C. He, X. Yue _et al._, "Llama-adapter v2: Parameter-efficient visual instruction model," _arXiv:2304.15010_, 2023.
* [36] L. Li, Y. Yin, S. Li, L. Chen, P. Wang, S. Ren, M. Li, Y. Yang, J. Xu, X. Sun, L. Kong, and Q. Liu, "M\({}^{3}\)it: A large-scale dataset towards multi-modal multilingual instruction tuning," _arXiv:2306.04387_, 2023.
* [37] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao, "Llava-med: Training a large language-and-vision assistant for biomedicine in one day," _arXiv:2306.00890_, 2023.
* [38] R. Pi, J. Gao, S. Diao, R. Pan, H. Dong, J. Zhang, L. Yao, J. Han, H. Xu, and L. K. T. Zhang, "Detgpt: Detect what you need via reasoning," _arXiv:2305.14167_, 2023.
* [39] Z. Yin, J. Wang, J. Cao, Z. Shi, D. Liu, M. Li, L. Sheng, L. Bai, X. Huang, Z. Wang _et al._, "Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark," _arXiv:2306.06687_, 2023.
* [40] M. Maaz, H. Rasheed, S. Khan, and F. S. Khan, "Videochatgpt: Towards detailed video understanding via large vision and language models," _arXiv:2306.05424_, 2023.
* [41] C. Lyu, M. Wu, L. Wang, X. Huang, B. Liu, Z. Du, S. Shi, and Z. Tu, "Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration," _arXiv:2306.09093_, 2023.
* [42] H. Zhang, X. Li, and L. Bing, "Video-llama: An instruction-tuned audio-visual language model for video understanding," _arXiv:2306.02858_, 2023.
* [43] Y. Su, T. Lan, H. Li, J. Xu, Y. Wang, and D. Cai, "Pandagpt: One model to instruction-follow them all," _arXiv:2305.16355_, 2023.
* [44] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R. Wen, "Evaluating object hallucination in large vision-language models," _arXiv:2305.10355_, 2023.

* [45] Y. Zhao, T. Pang, C. Du, X. Yang, C. Li, N.-M. Cheung, and M. Lin, "On evaluating adversarial robustness of large vision-language models," _arXiv:2305.16934_, 2023.
* [46] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh, "Vqa: Visual question answering," in _ICCV_, 2015.
* [47] A. Karpathy and L. Fei-Fei, "Deep visual-semantic alignments for generating image descriptions," in _CVPR_, 2015.
* [48] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick, "Microsoft coco: Common objects in context," in _ECCV_, 2014.
* [49] V. Ordonez, G. Kulkarni, and T. Berg, "Im2text: Describing images using 1 million captioned photographs," _NeurIPS_, 2011.
* [50] P. Sharma, N. Ding, S. Goodman, and R. Soricut, "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning," in _ACL_, 2018.
* [51] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut, "Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts," in _CVPR_, 2021.
* [52] C. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullis, A. Katta, T. Coombes, J. Jitsev, and A. Komatsuzaki, "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs," _arXiv:2111.02114_, 2021.
* [53] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma _et al._, "Visual genome: Connecting language and vision using crowdsourced dense image annotations," _IJCV_, 2017.
* [54] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hockenmaier, and S. Lazebnik, "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models," in _ICCV_, 2015.
* [55] J. Wu, H. Zheng, B. Zhao, Y. Li, B. Yan, R. Liang, W. Wang, S. Zhou, G. Lin, Y. Fu _et al._, "Ai challenger: A large-scale dataset for going deeper in image understanding," _arXiv:1711.06475_, 2017.
* [56] J. Gu, X. Meng, G. Lu, L. Hou, N. Minzhe, X. Liang, L. Yao, R. Huang, W. Zhang, X. Jiang _et al._, "Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark," _NeurIPS_, 2022.
* [57] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. D. Plumbley, Y. Zou, and W. Wang, "Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research," _arXiv:2303.17395_, 2023.
* [58] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, "Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline," in _O-COCOSDA_, 2017.
* [59] J. Du, X. Na, X. Liu, and H. Bu, "Aishell-2: Transforming mandarin asr research into industrial scale," _arXiv:1808.10583_, 2018.
* [60] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi, "Self-instruct: Aligning language model with self generated instructions," _arXiv:2212.10560_, 2022.
* [61] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y. Bengio, "An empirical investigation of catastrophic forgetting in gradient-based neural networks," _arXiv:1312.6211_, 2013.
* [62] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, "End-to-end object detection with transformers," in _ECCV_, 2020.
* [63] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds _et al._, "Flamingo: a visual language model for few-shot learning," _NeurIPS_, 2022.
* [64] J. Li, D. Li, S. Savarese, and S. Hoi, "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models," _arXiv:2301.12597_, 2023.
* [65] P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan, "Learn to explain: Multimodal reasoning via thought chains for science question answering," _NeurIPS_, 2022.
* [66] R. Vedantam, C. Lawrence Zitnick, and D. Parikh, "Cider: Consensus-based image description evaluation," in _CVPR_, 2015.
* [67] H. Agrawal, K. Desai, Y. Wang, X. Chen, R. Jain, M. Johnson, D. Batra, D. Parikh, S. Lee, and P. Anderson, "Nocaps: Novel object captioning at scale," in _ICCV_, 2019.
* [68] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions," _TACL_, 2014.
* [69] X. He, Y. Zhang, L. Mou, E. Xing, and P. Xie, "Pathvqa: 30000+ questions for medical visual question answering," _arXiv:2003.10286_, 2020.
* [70] J. J. Lau, S. Gayen, A. Ben Abacha, and D. Demner-Fushman, "A dataset of clinically generated visual questions and answers about radiology images," _Sci. Data_, 2018.
* [71] B. Liu, L.-M. Zhan, L. Xu, L. Ma, Y. Yang, and X.-M. Wu, "Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering," in _ISBI_, 2021.
* [72] P. Xu, W. Shao, K. Zhang, P. Gao, S. Liu, M. Lei, F. Meng, S. Huang, Y. Qiao, and P. Luo, "Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models," _arXiv:2306.09265_, 2023.
* [73] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, Z. Qiu, W. Lin, Z. Qiu, W. Lin _et al._, "Mme: A comprehensive evaluation benchmark for multimodal large language models," _arXiv_, 2023.
* [74] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui, "A survey for in-context learning," _arXiv:2301.00234_, 2022.

* [75] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu, and J. Gao, "Chameleon: Plug-and-play compositional reasoning with large language models," _arXiv:2304.09842_, 2023.
* [76] T. Gupta and A. Kembhavi, "Visual programming: Compositional visual reasoning without training," in _CVPR_, 2023.
* [77] Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp, "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity," _arXiv:2104.08786_, 2021.
* [78] Z. Yang, Z. Gan, J. Wang, X. Hu, Y. Lu, Z. Liu, and L. Wang, "An empirical study of gpt-3 for few-shot knowledge-based vqa," in _AAAI_, 2022.
* [79] M. Tsimpoukelli, J. L. Menick, S. Cabi, S. Eslami, O. Vinyals, and F. Hill, "Multimodal few-shot learning with frozen language models," _NeurIPS_, 2021.
* [80] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface," _arXiv:2303.17580_, 2023.
* [81] J. Ge, H. Luo, S. Qian, Y. Gan, J. Fu, and S. Zhan, "Chain of thought prompt tuning in vision language models," _arXiv:2304.07919_, 2023.
* [82] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, and A. Smola, "Multimodal chain-of-thought reasoning in language models," _arXiv:2302.00923_, 2023.
* [83] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, and N. Duan, "Visual chatgpt: Talking, drawing and editing with visual foundation models," _arXiv:2303.04671_, 2023.
* [84] T. Wang, J. Zhang, J. Fei, Y. Ge, H. Zheng, Y. Tang, Z. Li, M. Gao, S. Zhao, Y. Shan _et al._, "Caption anything: Interactive image description with diverse multimodal controls," _arXiv:2305.02677_, 2023.
* [85] D. Rose, V. Himakunthala, A. Ouyang, R. He, A. Mei, Y. Lu, M. Saxon, C. Sonar, D. Mirza, and W. Y. Wang, "Visual chain of thought: Bridging logical gaps with multimodal infillings," _arXiv:2305.02317_, 2023.
* [86] V. Himakunthala, A. Ouyang, D. Rose, R. He, A. Mei, Y. Lu, C. Sonar, M. Saxon, and W. Y. Wang, "Let's think frame by frame: Evaluating video chain of thought with video infilling and prediction," _arXiv:2305.13903_, 2023.
* [87] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, "Large language models are zero-shot reasoners," _arXiv:2205.11916_, 2022.
* [88] Z. Zhang, A. Zhang, M. Li, and A. Smola, "Automatic chain of thought prompting in large language models," _arXiv:2210.03493_, 2022.
* [89] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need," _NeurIPS_, 2017.
* [90] R. Zhang, X. Hu, B. Li, S. Huang, H. Deng, Y. Qiao, P. Gao, and H. Li, "Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners," in _CVPR_, 2023.
* [91] H. You, R. Sun, Z. Wang, L. Chen, G. Wang, H. A. Ayyubi, K.-W. Chang, and S.-F. Chang, "Idealgpt: Iteratively decomposing vision and language reasoning via large language models," _arXiv:2305.14985_, 2023.
* [92] D. Zhu, J. Chen, K. Haydarov, X. Shen, W. Zhang, and M. Elhoseiny, "Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions," _arXiv:2303.06594_, 2023.
* [93] X. Zhu, R. Zhang, B. He, Z. Zeng, S. Zhang, and P. Gao, "Pointclip v2: Adapting clip for powerful 3d open-world learning," _arXiv:2211.11682_, 2022.
* [94] A. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani, J. Lee, V. Vanhoucke _et al._, "Socratic models: Composing zero-shot multimodal reasoning with language," _arXiv:2204.00598_, 2022.
* [95] A. Parisi, Y. Zhao, and N. Fiedel, "Talm: Tool augmented language models," _arXiv:2205.12255_, 2022.
* [96] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig, "Pal: Program-aided language models," _arXiv:2211.10435_, 2022.
* [97] T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom, "Toolformer: Language models can teach themselves to use tools," _arXiv:2302.04761_, 2023.
* [98] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders _et al._, "Webgpt: Browser-assisted question-answering with human feedback," _arXiv:2112.09332_, 2021.
* [99] A. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani, J. Lee, V. Vanhoucke _et al._, "Socratic models: Composing zero-shot multimodal reasoning with language," _arXiv:2204.00598_, 2022.
* [100] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang, "Bottom-up and top-down attention for image captioning and visual question answering," in _CVPR_, 2018.
* [101] Z. Yu, J. Yu, Y. Cui, D. Tao, and Q. Tian, "Deep modular co-attention networks for visual question answering," in _CVPR_, 2019.
* [102] P. Gao, Z. Jiang, H. You, P. Lu, S. C. Hoi, X. Wang, and H. Li, "Dynamic fusion with intra-and inter-modality attention flow for visual question answering," in _CVPR_, 2019.
* [103] C. Gan, Z. Gan, X. He, J. Gao, and L. Deng, "Stylenet: Generating attractive visual captions with styles," in _CVPR_, 2017.
* [104] A. Mathews, L. Xie, and X. He, "Senticap: Generating image descriptions with sentiments," in _AAAI_, 2016.
* [105] D. Zhou, N. Scharli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, O. Bousquet, Q. Le, and E. Chi, "Least-to-most prompting enables complex reasoning in large language models," _arXiv:2205.10625_, 2022.
* [106] P. Lu, L. Qiu, K.-W. Chang, Y. N. Wu, S.-C. Zhu, T. Rajpurohit, P. Clark, and A. Kalyan, "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning," _arXiv:2209.14610_, 2022.

* [107] R. Zellers, Y. Bisk, A. Farhadi, and Y. Choi, "From recognition to cognition: Visual commonsense reasoning," in _CVPR_, 2019.
* [108] N. Xie, F. Lai, D. Doran, and A. Kadav, "Visual entailment: A novel task for fine-grained image understanding," _arXiv:1901.06706_, 2019.
