<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '대화 참여를 위한 사용자 페르소나 인식 대화 에이전트 설계\n' +
      '\n' +
      ' 덕신 권\\({}^{1,2}\\)선우 이\\({}^{1}\\)기현 김\\({}^{1}\\)서진 이\\({}^{1}\\)태윤 김\\({}^{1}\\)에릭 데이비스\\({}^{1}\\)\n' +
      '\n' +
      '한국 SK 텔레콤, 남부 캘리포니아 대학교\n' +
      '\n' +
      '{ds.kwon, sunwoo.lois, kimkihyun, skt.kaylee, tae.y.kim, eric.davis}@sk.com\n' +
      '\n' +
      'deuksin.kwon@usc.edu\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 논문은 상용 환경에서 자연스러운 응답 생성을 위한 WWH(WHAT, WHEN, How) 문제를 해결하기 위해 개인화된 개방형 대화 시스템을 구축하는 방법을 제안한다. 제안된 방법은 가중 데이터 세트 혼합, 부정적인 페르소나 정보 증강 방법, 개인화된 개방형 대화 시스템에서 WWH의 문제를 해결하기 위한 개인화된 대화 데이터 세트의 설계를 포함한다. 우리의 작업은 대화 유창성과 접지 경향의 균형을 효과적으로 맞추는 동시에 접지된 응답의 제어 가능성과 설명 가능성을 개선하기 위해 응답 유형 라벨을 도입한다. 이러한 방법의 조합은 객관적인 평가뿐만 아니라 주관적인 인간 평가에 의해 입증된 바와 같이 보다 유창한 대화로 이어진다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '개인화된 대화(PD) 시스템은 사용자의 페르소나에 대한 장기 기억에 기초하여 사용자 맞춤형 응답을 생성할 수 있어, 보다 신뢰할 수 있고 매력적인 대화로 이어진다(Ranjbartabar et al., 2021; Xu et al., 2022). 본 연구에서 페르소나 속성은 성격, 행동, 선호도 및 경험과 같은 포괄적인 사용자 관련 정보를 포함한다.\n' +
      '\n' +
      'PD 시스템에서 향상된 사용자 참여의 핵심은 자연스러운 응답을 생성하기 위해 모델이 근거되는 맥락적으로 적절하고 적절한 페르소나를 찾는 데 있다. 그러나, 도 1의 예에 도시된 바와 같이, PD 시스템은 일반적으로 N 페르소나 속성의 주어진 서브세트로부터 관련 페르소나 속성을 선택할 필요가 있으며, 이는 일반적으로 외부 메모리에 의해 제공되거나 사용자 페르소나 풀로부터 검색된다. 에이전트의 응답이 일반적으로 트레이닝 데이터세트에서 연관된 오라클 페르소나 정보와 함께 주석 처리된다는 사실을 고려하면, 모델 추론 동안 각각의 턴에서 어떤 페르소나 속성을 선택할지를 결정하는 것은 자명하지 않은 문제이다. (우리는 이 문제를 이하 "WHAT to ground" 문제로 언급할 것이다.) PD 시스템에서 고려해야 할 또 다른 측면은 특정 대화 컨텍스트 하에서, 보다 자연스러운 상호 작용을 생성하기 위해 검색된 페르소나 속성이 주어진 개인화된 응답을 생성하지 않는 것이 더 낫다는 것이다(도 1의 두 번째 응답). 예에 도시된 바와 같이, 검색 모듈이 응답 생성을 위해 페르소나 정보를 사용할지 여부를 결정하는 것은 보통 어렵다. 또한, 매 회전에 대해 주어진 페르소나 서브세트로 페르소나 정보를 언제 접지할지를 결정하기 위한 모델이 필요하다(이하 이를 "WHEN to Ground" 문제라고 부를 것이다).\n' +
      '\n' +
      '그러한 도전을 감안할 때, 매력적이고 인간-유사 개인화된 응답을 생성할 수 있는 사용자-기반 페르소나-인식 PD 시스템을 설계하는 것은 "WHAT", "WHEN", 및 "HOW"(WWH) 질문을 해결하는 것을 필요로 한다: 1) 대화 컨텍스트가 주어지면 어떤 개인 정보가 근거되어야 하는지, 2) 개인 정보를 사용하여 응답을 생성할 때, 및 3) 자연스럽고 인간-유사 개인화된 응답을 생성하는 방법을 포함한다.\n' +
      '\n' +
      '도 1: 사용자 페르소나에 근거된 개인화된 대화의 샘플. 모든 에이전트 발화에 대해, 응답에 근거할 페르소나 속성들은 검색 모델에 의해 검색된다. 그런 다음 에이전트는 대화 컨텍스트와 검색된 페르소나 서브세트를 사용하여 개인화된 응답을 생성하는 것에 대한 결정을 내린다.\n' +
      '\n' +
      '개인화 대화 시스템에 대한 대부분의 이전 연구는 이상적인 개인화 대화 설정 Liu et al.(2020); Dong et al.(2022); Xu et al.(2022); Fu et al.(2022)에서 자연스러운 응답을 생성하는 데 초점을 맞추었으며, 여기서 캐주얼 대화 전환으로 개인화된 응답을 심하게 인터리빙하는 것과 관련된 문제는 고려되지 않는다. 그러나 우리는 이것이 실제 개인화된 대화 시스템에서 해결해야 할 중요한 문제라고 믿습니다.\n' +
      '\n' +
      'GPT-3과 같은 대규모 언어 모델(LLM: Large-scale Language Models)은 다양한 자연어 이해(NLU: Natural Language Understanding) 작업, 특히 In-context learning Brown 등(2020)에서 뛰어난 성능을 보여주었다. 그러나 실제 서비스 환경에서 WWH 문제를 효과적으로 해결하기에는 LLM의 고유한 능력만으로는 부족하다. 더욱이, 신속하고 신속한 엔지니어링에만 의존하여, 자연스럽고 매력적인 개인화된 응답을 생성하고 다중 턴/세션 시나리오에서 모델의 출력을 정교하게 제어하는 것은 매우 까다롭다.\n' +
      '\n' +
      '연구 격차와 실제 문제를 해결하기 위해 모델의 기울기를 제어하여 개인화된 응답을 생성하는 방법을 제안한다. 본 기법은 개인화된 대화 시스템을 구성하기 위해 개인화된 데이터 세트를 혼합하여 인간과 유사한 자연스러운 대화를 가능하게 한다. 우리의 접근법은 다음의 단계들을 포함한다:\n' +
      '\n' +
      '1) 다중 세션 개인화 대화(MSPC: Multi-Session Personalized Conversation) 데이터셋을 생성하여 개인화된 응답을 위해 제공되는 페르소나 정보를 효과적으로 기반으로 모델을 학습한다. 2) 대화 데이터 세트의 혼합 가중치를 조정하여 모델의 페르소나 접지 수준을 제어한다. 또한 모델 미세 조정을 위해 턴 수준에서 페르소나 하위 집합의 음성 샘플로 데이터 세트를 풍부하게 한다. 3) 세대품질과 페르소나 접지세대의 제어성과 해석성을 높이기 위해 턴 레이블을 사용한다. 이 레이블은 회전이 개인화되었는지 캐주얼한지 여부를 나타내며 입력 중 하나로 사용됩니다. 궁극적으로, 우리는 180억 개의 파라미터 대언어 모델(LLM)을 미세 조정하여 개인화된 대화 시스템을 구축한다. 이 LLM은 대화 이력에 대한 높은 수준의 이해, 고품질 응답을 생성하는 능력 및 사용자의 페르소나를 포함한 주어진 입력에 효과적으로 집중할 수 있는 능력을 가지고 있다.\n' +
      '\n' +
      '또한, _sensibleness_와 _specificity_를 이용하여 주관적인 평가에서 모델의 접지 패턴과 세부 성능을 분석할 수 있는 4가지 접지 유형 분류를 제안하며, 이는 _groundedness_와 _fluency_를 기반으로 객관적인 평가를 보완한다.\n' +
      '\n' +
      '## 2 관련 작업\n' +
      '\n' +
      '**PersonaChat** 데이터 세트 Zhang et al.(2018)의 릴리스 이후 페르소나와 일치하거나 페르소나에 기반을 둔 개인화된 응답을 생성하는 방법은 Lee et al.(2021); Liu et al.(2020); Xu et al.(2021, 2022)이 광범위하게 연구되었습니다. 대부분의 연구는 다양한 모델 아키텍처, 모듈 및 트레이닝 프레임워크 Fu 등(2022); Dong 등(2022)의 사용에 의한 _WHAT_ 및 _HOW_ 과제를 해결하는 것에 초점을 맞춘다.\n' +
      '\n' +
      '상기 _How_와 관련하여, Liu 등(2020)은 상호 페르소나 인식에 대한 보상을 갖는 개인화된 대화를 생성하기 위한 RL 기반 접근법을 제안한다. Wu 등(2019) 및 Fu 등(2022)은 개인화되고 지식가능한 응답 생성을 생성하기 위해 변분 방법을 채용한다. 송송송\n' +
      '\n' +
      '도 2: 제안하는 개인화 대화 시스템의 전체 프레임워크\n' +
      '\n' +
      'et al. (2019)는 메모리로부터 선택된 페르소나를 갖는 CVAE를 통해 페르소나-그라운드된 응답들을 생성함으로써 _WHAT_ 및 _HOW_ 둘 모두를 어드레스한다. Xu 등(2022) 및 Bae 등(2022)도 페르소나 검색 모듈 및 생성기 모듈을 통해 동일한 문제를 해결한다.\n' +
      '\n' +
      '그러나 우리가 아는 한 PD 시스템에서 세 가지 _WWH_ 질문을 모두 해결하는 작업은 없었다. 따라서 본 논문에서는 상용 시스템에서 WWH 문제에 대한 해결의 중요성을 고려하여 상용 시스템에서 임무에 중요한 세 가지 WWH 문제를 모두 해결할 수 있는 새로운 방법을 제안한다.\n' +
      '\n' +
      '## 3 Dataset\n' +
      '\n' +
      'WWH 문제를 해결하는 PD 시스템을 개발하기 위해 MSPD라고 하는 한국어 다중 세션 개인화 대화 데이터 세트를 구성한다. 이 데이터 세트에는 여러 고유한 역할을 수행하는 에이전트가 포함되어 있으며 다른 PD 데이터 세트와 별도로 설정됩니다. 주로, 에이전트는 대화 중에 소개된 임의의 페르소나 속성을 포함하는 사용자 페르소나 속성을 기억하도록 요구된다. 에이전트는 또한 페르소나에 대해 합리적이고 시기 적절하게 근거하는 개인화된 응답을 생성해야 한다. 이 데이터 세트의 목표는 모델이 접지의 _HOW_ 및 _WHEN_ 을 학습할 수 있도록 하는 것입니다. 평균적으로 데이터 세트는 에피소드당 4개의 세션을 포함하며, 각 세션은 사용자와 에이전트 사이의 10-12회 회전으로 구성된다. 이 형식을 통해 에이전트는 세션 내 및 세션 간에 자연스러운 대화 흐름을 유지하는 방법을 배울 수 있습니다. 그림 3의 빨간색과 파란색 텍스트에서 볼 수 있듯이 모델이 기억해야 할 개인 정보를 포함하는 응답 및 사용자 발화에 사용된 페르소나에 주석을 달았다. 특히 데이터 세트 관점에서 _WHEN_ 및 _HOW_ 문제를 해결하기 위해 세션당 개인화된 응답 수를 2개 이하로 제한하고 대화 내에서 개인화된 응답의 품질과 적절성을 보장하기 위해 엄격한 검토를 수행했다. 이 접근법을 통해 총 13,469개의 에피소드를 구성할 수 있다. MSPD 데이터 세트의 통계 및 기타 샘플은 부록 A와 B에서 찾을 수 있다.\n' +
      '\n' +
      'MSPD와 함께 다양한 비공식 대화 데이터셋(D_{casual}\\)을 통합하여 고품질 일상, 지식 기반, 공감 및 개인화된 대화를 생성할 수 있는 보다 균형 잡힌 모델을 훈련한다. \\ (D_{casual}\\)는 약 1,250만 개의 발화에 대한 포괄적인 수집으로 구성된다. NIA(한국정보화진흥원)에서 개발한 온라인1에서 사용할 수 있는 신중하게 큐레이션된 한국어 대화 데이터 세트와 **PersonaChat**, **EmpatheticDialogues** 및 **Wikipedia의 마법사**(Dinan et al., 2018; Rashkin et al., 2018; Zhang et al., 2018)의 한국어 버전을 포함한 크라우드소싱된 대화 데이터 세트를 사용합니다.\n' +
      '\n' +
      '각주 1: [https://aihub.or.kr/](https://aihub.or.kr/)\n' +
      '\n' +
      '## 4 Methodology\n' +
      '\n' +
      '그림 2와 같이 다양한 방법을 사용하여 _WHAT_ 및 _WHEN_ 질문을 해결하기 위해 훈련 단계에서 모델을 훈련한다. 이들을 포함한다.\n' +
      '\n' +
      '그림 3: 제안된 MSPD 데이터 세트의 세션 예제. 왼쪽 그림은 사용자(U)와 에이전트(A) 사이의 대화를 나타내며, 빨간색 텍스트와 대괄호 안의 정보는 에이전트의 개인화된 응답(PR)과 해당 페르소나 속성의 인덱스를 나타낸다. 대괄호 안의 파란색 텍스트와 내용은 사용자의 새 페르소나와 해당 페르소나 색인을 나타냅니다. 오른쪽 그림에는 사용자의 인구 통계 정보를 포함하여 페르소나 속성에 대한 세부 정보가 포함되어 있다.\n' +
      '\n' +
      '다른 유형의 부정적인 페르소나 확장, 데이터 세트 혼합 및 응답 유형 생성입니다. 추론 단계 동안, 대화 컨텍스트 및 페르소나 속성의 서브세트가 주어지면, 모델은 적합한 개인화된 응답을 생성할 수 있다. 이러한 페르소나 서브세트들은 대화의 컨텍스트에 의해 결정된 개별 페르소나 속성들로부터 검색된다. 추가적으로, 모델은 응답 타입 라벨(RTL)을 통해 그 결정에 대한 설명을 제공한다. RTL에 대한 조건화를 통해 개인화된 응답 생성을 명시적으로 제어할 수 있다.\n' +
      '\n' +
      '### Persona-Grounded Generation\n' +
      '\n' +
      '이 연구에서 훈련 데이터셋의 모든 입력은 사용자 인구통계학적 정보 \\(d\\)(예: _gender_, _age_), 페르소나 속성으로 구성된 사용자 페르소나 \\(\\rho^{m}\\), 대화 컨텍스트 \\(c^{m}=[u_{1},a_{1},u_{2},a_{2},\\cdots,u_{m-1},a_{m-1},u_{m}]\\)으로 구성된다. \\ (u\\)와 \\(a\\)는 각각 사용자와 에이전트를 지칭하며, 타겟 응답 \\(y^{m}=[y^{m}_{1},\\cdots,y^{m}_{\\ell}]\\)은 \\(m_{th}\\) 에이전트 응답 \\(a_{m}\\)에 인덱싱된다.\n' +
      '\n' +
      '입력((d,\\rho^{m},c^{m})\\)이 주어졌을 때, 개인화된 응답에 대한 조건부 확률 \\(y^{m}\\)과 다음과 같이 공식화할 수 있는 Negative Log-Likelihood(NLL) 손실이 있는 손실 함수를 통해 모델을 최적화한다.\n' +
      '\n' +
      '\\[P(y^{m}|d,p^{m},c^{m})=\\prod_{t=1}^{\\ell}P(y^{m}_{t}|d,\\rho^{m},c^{m},y^{m}_{< t}) \\tag{1}\\]\n' +
      '\n' +
      '\\[\\mathcal{L}_{NLL}=-\\sum_{t=1}^{\\ell}\\text{log}\\;P(y^{m}_{t}|d,\\rho^{m},c^{m},y^ {m}_{<t}) \\tag{2}\\]\n' +
      '\n' +
      '여기서 \\(\\ell\\)은 대상 응답의 길이입니다.\n' +
      '\n' +
      '### Dataset Blending\n' +
      '\n' +
      '다양한 대화 데이터 세트를 혼합하는 것은 대화 시스템의 다양성, 공감성, 및 지식을 향상시키는 것으로 나타났으며, 이는 스미스 등(2020)이 보다 자연스럽고 매력적인 대화로 이어진다. 개인화된 대화를 위해 맞춤화된 MSPD와 다양한 형태의 캐주얼 대화 데이터 세트 \\(D_{casual}\\)를 혼합함으로써, 모델은 응집적이고 자연스러운 대화에서 보다 균형잡히고 능숙해진다.\n' +
      '\n' +
      '데이터 인스턴스는 섹션 4.1에서 설명한 대로 (\\(c\\), \\(r\\))로 정의하며, 여기서 \\(c\\) 및 \\(r\\)은 각각 대화 컨텍스트 및 대상 응답이다. 각 데이터 세트에 대한 블렌딩 가중치에 따라 인스턴스별로 데이터 세트를 블렌딩한다. 특히, 블렌딩 가중치(\\(w\\))를 갖는 _WWH_ 문제를 미세하게 제어하기 위해 MSPD 데이터 세트는 에이전트의 개인화된 응답(\\(\\mathcal{D}_{\\text{MSPD-PR}}\\))과 개인화되지 않은 응답(\\(\\mathcal{D}_{\\text{MSPD-NPR}}\\))으로 구분된다(예: 그림 3의 에이전트의 빨간색 및 검은색 응답). 최종 학습 데이터 세트는 개별 데이터 세트를 오버 샘플링 또는 언더 샘플링하여 조립된다. 개별 데이터 세트의 트레이닝 데이터 크기는 각 데이터 세트에 대한 데이터 인스턴스의 가중 수에 의해 결정되고, 이에 의해 정의되는,\n' +
      '\n' +
      '\\[\\|\\mathcal{D}_{\\text{i\\;(train)}}\\|=\\frac{w_{i}}{\\sum_{j=1}^{N}w_{j}}\\times\\| \\mathcal{D}\\| \\tag{3}\\]\n' +
      '\n' +
      '여기서, \\(N\\)개의 대화 데이터셋 집합 \\(\\mathcal{D}=\\{\\mathcal{D}_{casual_{1}},\\cdots,\\mathcal{D}_{casual_{k}},\\mathcal{D}_{\\text{MSPD-PR}},\\mathcal{D}_{\\text{MSPD-NPR}}\\}\\), \\(\\mathcal{D}_{i}\\)은 \\(\\mathcal{D}\\)의 \\(i\\)-번째 데이터셋이다.\n' +
      '\n' +
      '### 네거티브 샘플에 의한 _When_ & _What_ 제어\n' +
      '\n' +
      '**시간 제어_** _WHEN_ 문제를 해결 하려면 페르소나 기반 응답을 생성 하는 모델의 성향을 제어 하는 것이 중요 합니다. 페르소나가 주어지면, 에이전트는 일관성 있고 자연스러운 대화를 생성하기 위해 적절한 시간에 개인화된 응답을 생성해야 한다. 페르소나 기반 응답을 너무 자주 생성하는 것은 부자연스러운 대화로 이어진다. 한편, 너무 드물게 개인화된 응답을 생성하는 모델은 에이전트에 대한 사용자의 참여를 충분히 향상시키지 못한다.\n' +
      '\n' +
      '페르소나 서브세트가 각각의 턴에서 검색 모델에 의해 검색되는 특정 상황에서, 모델은 개인화된 응답을 생성하는 대신에 캐주얼 응답을 생성해야 하고, 그 결과 더 자연스러운 흐름을 초래한다. 이 자연스러운 흐름을 학습하기 위해, 우리는 의도적으로 문맥적으로 무관한 모든 페르소나 속성으로 구성된 페르소나 서브세트를 개인화되지 않은 응답에 대한 입력에 포함한다. 우리는 이것을 연구에서 부정적인 페르소나 부분 집합 증가라고 부른다. 이 보강은 모델의 지면 기울기를 너무 자주 "억제"합니다. 그러나 너무 많은 증가는 모델의 접지 능력을 방해할 수 있으므로, 우리는 모든 일상적인 데이터 집합이 아닌 \\(\\mathcal{D}_{\\text{MSPD-NPR}}\\)의 데이터에 대해서만 부정적인 페르소나 부분 집합 증강을 수행한다. \\(D_{casual}\\)\n' +
      '\n' +
      '**What_의 제어** 모델이 페르소나 기반 응답을 생성할 때 _WHAT_, 즉 응답을 기반으로 하는 특정 페르소나 특성을 결정해야 합니다. 그라운드-트루스 페르소나 속성인 응답과 관련이 있는 \\(\\rho_{\\text{pos}}\\)과 목표 응답과 관련이 없는 \\(\\rho_{\\text{neg}1},...,\\rho\\text{neg}_{k-1}\\)을 모두 제공함으로써 모델은 현재 대화 컨텍스트가 주어진 여러 옵션에서 적절한 페르소나 속성을 선택하는 방법을 학습한다. 우리는 그라운드 진리 페르소나에 여러 개의 부정적인 페르소나 속성을 추가하는 과정을 부정적인 페르소나 속성 증대로 지칭한다.\n' +
      '\n' +
      '마지막으로, 반응 유형에 따라 부정적인 페르소나 증대에 대한 페르소나 \\(\\rho\\)의 하위 집합을 (1)에서 변경한다.\n' +
      '\n' +
      '\\[\\rho=\\begin{cases}\\rho_{\\text{npr}}\\text{ for non-personalized response }\\in\\mathcal{D}_{\\text{MSPD-NPR}}\\\\ \\rho_{\\text{pr}}\\text{ for personalized response }\\in\\mathcal{D}_{\\text{MSPD-PR}}\\\\ \\rho_{\\text{c}}\\text{ for casual response }\\in\\mathcal{D}_{\\text{casual}}\\end{cases}\\]\n' +
      '\n' +
      '여기서, \\(\\rho_{\\text{npr}}=\\{\\rho_{\\text{neg}_{1}},\\cdots,\\rho_{\\text{neg}_{k}}\\}\\),\n' +
      '\n' +
      '\\(\\rho_{\\text{pr}}=\\{\\rho_{\\text{pos}},\\rho_{\\text{neg}_{1}},\\cdots,\\rho_{ \\text{neg}_{k-1}}\\}),\\(\\rho_{\\text{c}}=\\phi\\).\n' +
      '\n' +
      '### 응답 형식 레이블을 통한 제어 및 설명\n' +
      '\n' +
      '**제어 가능성** 상업적 환경에서는 비즈니스 논리를 기반으로 개인화된 응답을 생성할지 여부를 결정해야 하는 경우가 많습니다. 예를 들어 에이전트가 사용자에게 메시지를 미리 전송해야 하는 시기를 결정하는 것이 포함될 수 있습니다. 우리는 <RTL>로 표시된 반응 유형 레이블(RTL)을 사용하여 _WHEN_에 관한 모델의 결정을 명시적으로 제어할 수 있다.\n' +
      '\n' +
      '먼저 (1)에서 응답과 대응 RTL 토큰인 \\(P(\\text{<RTL>},y|d,\\rho,c)\\)을 모두 생성하도록 모델을 학습한다. 개인화된 응답 유형 레이블에는 미리 정의된 특수 토큰 <PRTL>과 캐주얼 응답 유형 레이블에는 <CRTL>이 있다. 그런 다음 추론 시간에 RTL을 삽입하여 응답 유형에 해당하는 응답을 생성할 수 있다. \\(y\\sim P_{\\theta}(\\cdot|d,\\rho,c,\\text{<PRTL>})\\) 또는 \\(y\\sim P_{\\theta}(\\cdot|d,\\rho,c,\\text{<CRTL>})\\).\n' +
      '\n' +
      '**설명 가능성** 오류 분석은 신속한 디버깅 및 문제 해결을 위한 상용 시스템에서 중요한 요소입니다. 그러나 이 프로세스는 종종 노동 집약적일 수 있으며, 일반적으로 생성된 개인화된 응답의 품질과 적절성을 평가하기 위해 로그 데이터의 수동 검토를 포함한다. 따라서 제어성을 향상시키는 것 외에도 모델의 생성 반응에 대한 설명성을 향상시키기 위해 반응 유형 레이블(RTL)을 사용한다. 이와 관련하여 RTL이 제공하는 설명 가능성의 수준은 보다 쉽고 효율적인 오류 분석을 용이하게 하여 서비스 운영의 개선으로 이어진다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      'WWH 문제를 해결하는 제어 가능한 개인화 대화(PD: Personalized Dialogue) 시스템을 구축하는 데 있어 제안된 방법의 유효성을 검증하기 위해 여러 모델의 성능을 비교한다. 이들은 데이터 세트 혼합 및 네거티브 샘플링 방법과 같은 미세 조정된 기준 모델로 향상된다. 또한, 서로 다른 블렌딩 가중치로 학습된 모델을 비교하여 블렌딩 가중치가 모델의 접지 성향과 유창성에 미치는 영향을 평가한다. 베이스라인 모델들은 모두 GPT-3 Brown 등(2020)과 동일한 아키텍처를 공유하는 우리의 사내 18B 파라미터 사전 트레이닝 언어 모델로부터 도출된다. 모든 실험은 NVIDIA A100 SXM4 80GB GPU가 탑재된 SKT의 독점 슈퍼컴퓨터인 타이탄에서 진행된다.\n' +
      '\n' +
      '### Evaluation\n' +
      '\n' +
      '**객관적 평가** 복잡성(PPL)을 사용하여 모델에 의해 생성된 응답의 유창성을 측정합니다. 또한 페르소나 속성과 생성된 반응 사이의 F1 점수는 모델의 접지 능력을 평가하는 프록시로 작용한다. 또한, 생성된 응답 Song 등(2019)에 사용자 페르소나가 얼마나 잘 반영되는지 측정하는 P-커버리지 스코어를 계산한다.\n' +
      '\n' +
      '**주관적 평가** 우리는 세션 및 턴 수준 모두에서 주관적인 인간 평가로 객관적인 평가 메트릭을 보완하며, 특히 턴 수준 Adiwardana 등(2020)에서 0 또는 1로 평가된 SS(Sensibleness and Specificity) 점수를 사용합니다. 특히, 턴 레벨에서의 접지 응답의 패턴과 품질을 분석하기 위해 제안된 4가지 접지 유형에 따라 분류하며, 이는 다음과 같다. 먼저 에이전트의 응답인 \\(y\\)이 개인화되었는지 평가한다. 둘째, **접지 수준** 및 **일관성** 두 가지 기준에 따라 \\(y\\)을 분류합니다.\n' +
      '\n' +
      '**접지 수준** 에서 두 가지 하위 범주가 있습니다. 1) _Hard Grounding_ 여기서 \\(y\\)과 페르소나 특성 ( \\(\\rho_{\\text{pos}}\\) 사이에 직접적이고 명시적인 연관성이 있으며 높은 표현 유사성을 특징으로 합니다. 2) _Soft Grounding_ 여기서 \\(y\\)과 \\(\\rho_{\\text{pos}}\\ 사이에 간접적이고 암시적인 연관성이 있으며, 낮은 표현적 유사성으로 표시된다.\n' +
      '\n' +
      '**일관성** 범주 아래에 두 가지 하위 범주가 있습니다. 1) \\(y\\)과 지정 \\(\\rho_{\\text{pos}}\\) 사이에 일관성이 있는 _일관성 접지_ 2) \\(y\\)과 지정 \\(\\rho_{\\text{pos}\\) 사이에 일관성이 없는 _일관성 접지_입니다.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      '#### 5.3.1 부정적 페르소나 속성의 영향\n' +
      '\n' +
      '표 1은 페르소나 기반 응답 생성에 부정적인 페르소나 속성을 도입하는 영향을 보여 줍니다. \\ (Model_{1}\\) 단 하나의 긍정적인 페르소나 속성으로 훈련된 모델이 각각 11.4와 0.28로 가장 높은 F1과 PPL 점수를 보였다. 반면 부정적인 페르소나 속성으로 학습된 \\(Model_{2}\\)은 PPL이 10.97, F1 점수가 10.15로 \\(Model_{1}\\)보다 약간 낮다. 접지 빈도의 감소에도 불구하고, 모델은 대화 맥락을 고려할 때 적절한 페르소나 속성을 합리적으로 선택함으로써 향상된 응답 생성을 보여준다. 주어진 맥락에서 모델이 여러 페르소나 속성 중 가장 적합한 페르소나를 구별하는 방법을 배우기 때문에 PPL이 증가한다고 가정한다. 또한, 지면에 대한 기울기가 감소했음에도 불구하고 모델이 매 회전마다 고품질 개인화된 응답을 생성할 수 있음을 관찰했다.\n' +
      '\n' +
      '#### 5.3.2 Negative Persona Subset의 효과\n' +
      '\n' +
      '표 1은 _WHEN_ 문제를 제어하는 데 있어 부정적인 페르소나 하위 집합의 효과를 보여준다. 부정 페르소나 부분 집합의 적용을 통해, \\(Model_{3}\\)은 페르소나 속성이 주어진 컨텍스트에 적합하지 않을 때 개인화된 응답을 생성하는 것을 자제하는 것을 학습한다. 표 1에서 \\(Model_{3}\\)은 낮은 PPL, F1 및 P-커버 점수(각각 9.37, 01 및 0.05)에서 알 수 있듯이 \\(Model_{2}\\)에 비해 페르소나 접지가 감소하고 유창성이 크게 증가함을 보여준다. 이 향상된 유창성의 주요 이유는 모델이 관련 없는 페르소나 하위 집합에 근거할 필요 없이 테스트 세트에서 개인화되지 않은 턴에 대해 더 빈번하고 자연스러운 일상적인 응답을 생성하기 때문이라고 믿는다.\n' +
      '\n' +
      '#### 5.3.3 Blending Datasets 효과: Model Fluency와 Grounding의 Trade-Off\n' +
      '\n' +
      '<표 2>에서 보는 바와 같이 모형의 유창성과 접지 경향성 간에는 상충 관계가 있다. 부정적인 페르소나 증강을 가진 MSPDNRP 데이터셋의 가중치가 증가함에 따라 F1 점수는 0.14에서 0.06으로 감소하고 P-커버 점수는 0.06에서 0.1 및 0.05로 감소한다. 반대로 PPL은 10.46에서 9.33으로 감소한다. 이는 페르소나 증강된 부정 샘플 수의 증가가 모델 접지를 덜 자주 의미하여 더 나은 품질 응답으로 더 자연스러운 대화 흐름을 유도한다는 것을 의미한다.\n' +
      '\n' +
      '자연스럽고 매력적인 대화를 달성하려면 모델의 지면 성향과 대응 유창성 사이의 균형을 신중하게 고려해야 한다. _WWH_ 균형을 제어 하기 위해 다른 페르소나 증대를 사용 하는 데이터 집합에 대 한 혼합 가중치를 조정 하 고 PPL 및 F1 점수에 적절 한 값을 선택할 수 있습니다. F1 점수가 1 미만인 모델이 대화에서 접지를 시도하는 경우가 거의 없다는 것을 일관되게 관찰했기 때문에 모델의 접지 경향에 대한 최소 임계값으로 F1 점수 \'1\'을 설정했다. 이 접근법은 최적의 PD 시스템이 충분한 양의 근거 응답과 높은 유창성 점수 사이의 균형을 유지하도록 보장한다.\n' +
      '\n' +
      '#### 5.3.4 RTL 생성의 효과: 설명력과 유창성 향상\n' +
      '\n' +
      '표 1에서 볼 수 있듯이 F1 점수와 P-커버를 기반으로 RTL과 개인화된 응답을 모두 생성하도록 훈련된 \\(모델_{4}\\)은 \\(모델_{3}\\)과 비교할 때 접지 경향에 거의 차이가 없음을 보여준다. 반면, PPL 점수는 8.88점으로 감소하였다. 이러한 결과는 Kim et al.(2022)의 연구와 일치하며, 타겟 반응과 관련된 정보가 동시에 생성되었을 때 생성 품질이 향상되었음을 보여주었다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c} \\hline \\hline Model ID & Method & Dataset & \\# Attribute &\n' +
      '\\begin{tabular}{c} Fluency \\\\ \\end{tabular} & Groundness \\\\ \\cline{3-6}  & & & PPL & F1 & P-Cover \\\\ \\hline \\(Model_{1}\\) & Base (Positive Only) & MSPDNRP + \\(\\mathcal{D}_{\\text{consal}}\\) & 1 & 11.4 & 0.28 & 0.12 \\\\ \\(Model_{2}\\) & + Negative Persona Attributes Augmentation & MSPDNRP + \\(\\mathcal{D}_{\\text{consal}}\\) & 5 & 10.97 & 0.15 & 0.07 \\\\ \\(Model_{3}\\) & + Negative Persona Subset Augmentation & MSPDNR + MSPDNR + \\(\\mathcal{D}_{\\text{consal}}\\) & 5 & 9.37 & 0.1 & 0.05 \\\\ \\(Model_{4}\\) & + RTL Generation & MSPDNRP + MSPDNR + \\(\\mathcal{D}_{\\text{consal}}\\) & 5 & 8.88 & 0.1 & 0.046 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 객관적 평가 결과\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline \\multirow{2}{*}{Model} & \\multicolumn{3}{c}{Biencing Weight} & \\multicolumn{3}{c}{Evaluating} \\\\ \\cline{2-7}  & \\(\\mathcal{D}_{\\text{total}}\\) & MSPDNR & MSPDNR & F1 & P-Cover & PPL \\\\ \\hline \\multirow{3}{*}{\\(Model_{3}\\) (Negative Persona Subset Aug. 95)} & 0.94 & 0.5 & 0.1 & 0.14 & 0.06 & 10.46 \\\\  & 0.92 & 0.5 & 0.3 & 0.12 & 0.05 & 10.04 \\\\ \\cline{1-1}  & 0.90 & 0.5 & 0.5 & 0.11 & 0.05 & 9.91 \\\\ \\cline{1-1}  & 0.87 & 0.5 & 0.8 & 0.1 & 0.05 & 9.33 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 블렌딩 가중치가 다른 평가 생성된 반응 유형 레이블(RTL)이 페르소나 접지에 대한 모델의 결정을 정확하게 반영하는지 여부를 분석하여 설명성을 평가했다. 이를 위해 각 응답 유형에 대해 생성된 90개의 응답을 샘플링했다. 캐주얼 응답과 개인화 응답 유형에 대해 생성된 RTL의 정확도는 각각 96.7%와 98.8%이다. 이것은 RTL을 생성하는 것이 _WHEN_ 문제에 대한 모델의 결정에 대한 신뢰할 수 있는 설명을 제공한다는 것을 확인시켜준다.\n' +
      '\n' +
      '#### 5.3.5 주관적 접지 평가\n' +
      '\n' +
      '표 3에서 턴 레벨 및 세션 레벨 모두에 대한 높은 평균(0.8 초과) 점수는 고품질 MSPD 데이터세트에 대해 훈련된 모델이 적절한 응답을 생성할 수 있음을 보여준다. 접지 평가에서는 경질접지와 연질접지의 경우가 대부분으로 페르소나 일치된 결과를 보였다. 두 모델 모두 연약한 접지보다 거의 4배 많은 경질 접지 사례를 보였고 "나쁨 감지" 응답 비율이 더 낮았다. 이는 모델이 문맥을 고려할 때 자연스럽고 명시적인 방식으로 응답에서 지면 페르소나 정보에 강하게 기울어짐을 시사한다. 경질 접지의 "나쁜 감지" 사례를 자세히 조사한 결과 모델이 페르소나 접지에 더 집중할수록 반응이 주어진 맥락에서 간혹 부자연스러울 수 있음을 발견했다. 그러나 "나쁜 합리적" 접지 응답의 비율은 10% 범위였으며, 이는 모델이 일반적으로 고품질 개인화 응답을 생성한다는 것을 확인시켜준다.\n' +
      '\n' +
      'RTL 생성 모델(\\(Model_{4}\\))은 지반에 대한 경사도가 낮지만, 더 나은 나쁨감응비를 보였다. 따라서 객관적인 평가 결과에 따르면 세션 평가 측면에서 유의미한 개선이 없음에도 불구하고 응답과 RTL을 모두 생성하는 것이 유창성에 긍정적인 영향을 미칠 수 있음을 확인할 수 있다.\n' +
      '\n' +
      '#### 5.3.6 객관적 평가와 주관적 평가 간의 상관 관계\n' +
      '\n' +
      '우리는 PPL로 측정한 유창성과 인간의 감각성 판단 사이의 양의 상관관계를 확인했다. \\ (Model_{4}\\)은 \\(Model_{3}\\)에 비해 PPL이 0.49로 감소하여 표 1에서 유창성이 향상되었음을 알 수 있다. 표 3에 제시된 세션 및 턴 평가에서 \\(Model_{4}\\)은 유창성과 관련된 모든 평가 기준에서 \\(Model_{3}\\)에 비해 유의하게 높은 점수를 나타냈다. 또한 턴레벨 접지 평가에서도 개인화/비개인화 턴(각각 0.13, 0.03)에 대해 낮은 불량감도 비율을 나타내어 \\(Model_{4}\\) 응답의 향상된 감도를 확인할 수 있었다. 또한 주관적 평가(즉, 접지 발생량)와 접지 성향을 평가하는 데 사용되는 P-커버리지 메트릭 사이에 양의 상관 관계가 있음을 발견했다. 표 1에서 \\(모델_{4}\\)은 \\(모델_{3}\\)에 비해 P-Coverage가 약간 감소했다. 이는 표 3의 \\(모델_{4}\\)에 의해 생성된 개인화 회전수(약 40개) 감소에 해당하며, 이는 모델의 접지 성향의 실제 감소를 반영한다. 결과적으로 주관적인 평가 비용을 고려할 때 실제 서비스 운영에서 모델의 유창성과 접지 경향을 정확하게 평가하기 위해 객관적인 평가가 가능할 것으로 판단된다.\n' +
      '\n' +
      '## Conclusion\n' +
      '\n' +
      '가중치 데이터 집합 혼합(_WHEN_), 부정 페르소나 부분 집합(_WHEN_), 부정 페르소나 속성(_WHAT_), 고도로 큐레이션된 개인화 대화 데이터 집합(_HOW_) 생성을 통해 자연스럽고 매력적인 대화를 위한 _WWH_ 문제를 해결하는 개인화 개방형 대화 시스템을 구축하는 방법을 제안했다. 또한 응답 레이블(_RTL_)을 생성하는 것이 _WHEN_에 대한 모델 결정의 제어 가능성과 설명 가능성을 모두 향상시킨다는 것을 보여준다. 실험 결과는 주관적인 평가와 객관적인 평가 모두에서 볼 수 있듯이 제안된 방법이 _WWH_ 문제를 해결하고 제어하는 데 효과적임을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Model} & \\multicolumn{4}{c}{**Session / Turn Evaluation**} & \\multicolumn{4}{c}{**Grounding Evaluation**} & \\\\ \\cline{2-10}  & \\multicolumn{2}{c}{Session Score} & \\multicolumn{2}{c}{Session-Benes} & \\multicolumn{2}{c}{Specificity} & \\multicolumn{2}{c}{Hand Grounding} & \\multicolumn{2}{c}{Soft Grounding} & \\multicolumn{2}{c}{Sub Total} & \\multicolumn{2}{c}{Non-personalized} & \\multicolumn{2}{c}{Total} \\\\ \\cline{5-10}  & \\multicolumn{2}{c}{Session Score} & \\multicolumn{2}{c}{(Time-level)} & \\multicolumn{2}{c}{(Time-level)} & \\multicolumn{2}{c}{(Time-level)} & \\multicolumn{2}{c}{Consitect} & \\multicolumn{2}{c}{Inconsistent} & \\multicolumn{2}{c}{Consistent} & \\multicolumn{2}{c}{Inconsistent} \\\\ \\hline \\(Model_{3}\\) & 0.80 & 0.914 & 0.76 & 0.14 (23/162) & NA (00) & 0.23 (9/40) & 1.0 (1/1) & 0.16 (33/203) & 0.03 (10/297) & 0.09 (43/500) \\\\ \\(Model_{4}\\) & 0.885 & 0.939 & 0.875 & 0.13 (16/125) & NA (00) & 0.17(6/39) & NA (00) & 0.13 (22/160) & 0.03 (11/03) & 0.06 (33/543) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 주관적 평가 결과. 1) 세션 & 턴 레벨 평가 및 2) 접지 평가: 5.2에 기술된 각각의 접지 유형의 카운트에 대한 나쁜 감각적 응답의 카운트의 비율. 나쁜 감각적 응답은 응답이 "감응" 평가에서 0을 점수화한 것을 의미한다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '이번 프로젝트 내내 SK텔레콤과 A.Tech 회원님들의 헌신적인 지원에 진심으로 감사드린다. 기술 지원, 의미 있는 토론 및 모델의 성능 향상, 교육 및 배치에 대한 기여에 대해 재단 모델링 팀 구성원에게 특별한 감사를 표합니다. 또한, 모델 훈련 및 개선을 위한 고품질 데이터 세트를 생성하고 평가하는 데 귀중한 기여를 한 대화 PO 팀의 언어학자에게 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* A. Adiwardana, M. 웅동래소 피델 토필란 양안걸쉬레사 Lu, et al. (2020)Towards a human-like open-domain chatbot. arXiv preprint arXiv:2001.09977. Cited by: SS1.\n' +
      '*S. 배동권 강민영 김용 정현근 이원 박태환 성(2022)에게 소식을 전해 주세요! 장기간의 대화에서 기억 관리. arXiv preprint arXiv:2210.08750. Cited by: SS1.\n' +
      '* T. B. Mann, N. 라이더 Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020)Language models are few-shot learners. Neural Information Processing Systems33, pp. 1877-1901. Cited by: SS1.\n' +
      '* E. Dinan, S. 롤러 허스터 Auli, and J. Weston (2018)Wizard of wikipedia: Knowledge-powered conversation agents. arXiv preprint arXiv:1811.01241. Cited by: SS1.\n' +
      '* W. 동석 풍대왕 Zhang(2022)I know you better: user profile aware personalized dialogue generation. In Advanced Data Mining and Applications: 17th International Conference, ADMA 2021, Sydney, NSW, Australia, February 2-4, 2022, Proceedings, Part II, pp. 192-205. Cited by: SS1.\n' +
      '* T. FuX Zhao, C. Tao, J. Wen 및 R. 옌(2022) 천명의 사람들의 눈에 천개의 햄릿이 있다: 개인적인 기억과 함께 지식 기반 대화를 강화한다. arXiv preprint arXiv:2204.02624. Cited by: SS1.\n' +
      '*H. Kim Y. 유락 장세 루다카사비 최모 Sap(2022) 친사회적 대화: 대화 에이전트의 친사회적 백본입니다. arXiv preprint arXiv:2205.12688. Cited by: SS1.\n' +
      '* J. Yang Lee, K. A. Lee, and W. S. Gan (2021) Generating personalized dialogue via multi-task meta-learning. arXiv preprint arXiv:2108.03377. Cited by: SS1.\n' +
      '*Q. 류영 첸, B 첸, J. 루, Z. 첸, B. 저우, 그리고 D. 장 (2020) 당신은 나에게 깊은 인상을 줍니다: 상호 페르소나 인식을 통한 대화 세대입니다. arXiv preprint arXiv:2004.05388. Cited by: SS1.\n' +
      '* H. Ranjbartabar, D. Richards, A. A. Bilgin, and C. Kutay (2021) 물어봐도 될까요? 개인화된 관계형 에이전트 대화에서 콜드 스타트 문제를 해결 In Proceedings of the 21st ACM International Conference on Intelligent Virtual Agents, pp. 167-174. Cited by: SS1.\n' +
      '* H. Rashkin, E. M. Smith, M. Li, Y. Boureau (2018)Towards Empathetic open-domain conversation models: new benchmark and dataset. arXiv preprint arXiv:1811.00207. Cited by: SS1.\n' +
      '* E. M. Smith, M. 윌리엄슨 슈스터, J. 웨스턴, Y. Boureau (2020) 모든 것을 종합해 볼 수 있습니다. 대화형 에이전트의 기술을 혼합 하는 능력을 평가 합니다. arXiv preprint arXiv:2004.08449. Cited by: SS1.\n' +
      '* H. Song, W. 장영 추이왕태 Liu(2019) Exploiting Persona information for various generation of conversation responses. arXiv preprint arXiv:1905.12188. Cited by: SS1.\n' +
      '* B. Wu, M. 이준 왕영 진동웅 Feng, J. Huang, and B. Wang (2019)Guiding Variational Response Generator to exploit persona. arXiv preprint arXiv:1911.02390. Cited by: SS1.\n' +
      '* J. Xu, A. Szlam, and J. Weston (2021)Beyond goldfish memory: long-term open-domain conversation. arXiv preprint arXiv:2107.07567. Cited by: SS1.\n' +
      '*X. 서진 고웅 우진 Niu, H. Wu, H. Wang, and S. 왕(2022) 오랜만이에요! 장기적인 페르소나 기억과 함께 열린 영역 대화. arXiv preprint arXiv:2203.05797. Cited by: SS1.\n' +
      '*S. Zhang, E. Dinan, J. Urbanek, A. Szlam, D. Kiela, and J. Weston (2018) Personalizing dialogue agents: I have a dog, you have pets? arXiv preprint arXiv:1801.07243. Cited by: SS1.\n' +
      '\n' +
      '## 부록 MSPD Dataset의 세부 정보\n' +
      '\n' +
      '### MSPD 통계\n' +
      '\n' +
      '### 모델 학습 설정\n' +
      '\n' +
      '본 연구의 실험을 위해 GPT-3(Brown et al., 2020)과 동일한 아키텍처를 가진 18B 파라미터 모델을 미세 조정했지만 40개의 레이어, 6144개의 숨겨진 크기 및 48개의 주의 헤드를 사용했다. 이 모델은 1.0e-05의 학습률을 사용하여 8의 마이크로 배치 크기를 갖는 단일 에폭에 대해 학습된다. 과적합을 방지하기 위해 0.1의 탈락률과 1.0e-1의 중량 감쇄를 사용한다. 입력 시퀀스 길이는 1024이다. 표 1 1의 모델은 \\(D_{casual}\\) 데이터 세트의 경우 0.85, MSPD\\({}_{PR}\\)의 경우 0.7, MSPD\\({}_{NPR}\\) 데이터 세트의 경우 0.8로 설정된 혼합 가중치로 훈련된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Types & \\\\ \\hline \\# Episodes & 13,469 \\\\ \\# Sessions & 53,880 \\\\ \\# Utterances & 601,062 \\\\ Avg. \\# turns per session & 11.15 \\\\ Avg. \\# personalized response per session & 1.90 \\\\ Avg. \\# user persona per episode & 7.18 \\\\ Avg. \\# newly aggregated persona per episode & 2.18 \\\\ Avg. length of user utterances & 33.72 \\\\ Avg. length of agent response & 28.10 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: MSPD 데이터셋의 통계량\n' +
      '\n' +
      '## Appendix B 예제\n' +
      '\n' +
      '### MSPD 데이터 세트 예제\n' +
      '\n' +
      '도 4: MSPD에서 다중 세션 대화의 샘플\n' +
      '\n' +
      '### Subjective Evaluation\n' +
      '\n' +
      '도 5: 주관적 평가 도구의 스냅샷.\n' +
      '\n' +
      '도 6: 경질 접지에 대한 주관적인 평가를 갖는 우리의 개인화된 대화 모델의 세대들의 예. 푸른색 텍스트는 모델이 응답을 근거하는 페르소나 기반 응답 및 인물 속성이다.\n' +
      '\n' +
      '도 7 : 연약지반의 주관적 평가 및 고장사례 사례\n' +
      '\n' +
      '### 배포 도구: 위생 테스트\n' +
      '\n' +
      '그림 8: 위생 테스트 도구의 스냅샷입니다. 1) 가장 왼쪽 영역은 에이전트와의 대화형 대화를 위한 영역이고, <PL> 및 <DL> 태그는 모델에 의해 생성된 응답 타입을 지칭한다; <PL>은 개인화된 응답이고, <DL>은 비개인화된 응답 타입이다. 2) 센터 창은 사용자 및 현재 턴과 관련된 정보를 보여준다. 그리고 3) 우측의 윈도우는 사용자 페르소나를 표시한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>