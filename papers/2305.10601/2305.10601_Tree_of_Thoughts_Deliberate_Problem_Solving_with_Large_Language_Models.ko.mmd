# Tree of Th thoughts: Deliberate Problem Solving

대용량 언어 모델

 요순유

Princeton University

&Dian Yu

Google DeepMind

&Jeffrey Zhao

Google DeepMind

&Izhak Shafran

Google DeepMind

&Thomas L. 그리피스

Princeton University

&Yuan Cao

Google DeepMind

&Karthik Narasimhan

Princeton University

###### Abstract

언어 모델은 광범위한 작업에 걸쳐 일반적인 문제 해결을 위해 점점 더 많이 배포되고 있지만 여전히 추론 중 토큰 수준의 좌-우 의사 결정 프로세스에 국한되어 있다. 이것은 그들이 탐험, 전략적 모색이 필요한 작업이나 초기 결정이 중추적인 역할을 하는 작업에서 부족할 수 있음을 의미한다. 이러한 문제를 극복하기 위해, 우리는 언어 모델 추론을 위한 새로운 프레임워크인 "Tree of Thoughts"(ToT)를 소개한다. 이 프레임워크는 언어 모델을 프롬프트하기 위해 인기 있는 "Chain of Thought" 접근 방식을 일반화하고 문제 해결을 위한 중간 단계 역할을 하는 일관성 있는 텍스트 단위("사상")를 통한 탐구를 가능하게 한다. ToT는 LMs가 여러 다른 추론 경로를 고려하고 다음 행동 과정을 결정하기 위해 선택을 스스로 평가하고 글로벌 선택을 하기 위해 필요할 때 앞을 내다보거나 역추적하여 의도적인 의사 결정을 수행할 수 있도록 한다. 실험 결과, ToT는 24 게임, 크리에이티브 글쓰기, 미니 십자말풀이의 세 가지 비자명한 계획이나 탐색이 필요한 새로운 작업에서 언어 모델의 문제 해결 능력을 크게 향상시키는 것으로 나타났다. 예를 들어, 게임 24에서는 GPT-4가 4%의 과제만 해결한 반면, 본 방법은 74%의 성공률을 달성하였다. 모든 프롬프트가 있는 코드 리포지토리: [https://github.com/princeton-nlp/tree-of-thought-llm](https://github.com/princeton-nlp/tree-of-thought-llm)입니다.

## 1 Introduction

원래 텍스트를 생성하도록 설계된, GPT[25; 26; 1; 23] 및 PaLM[5]와 같은 언어 모델(LMs)의 스케일-업 버전은 수학적, 상징적, 상식 및 지식 추론을 요구하는 훨씬 더 광범위한 범위의 작업을 수행할 수 있는 것으로 나타났다. 이 모든 진전의 기초가 여전히 토큰 수준의 결정을 하나씩 그리고 좌우 방식으로 내리는 텍스트를 생성하기 위한 원래의 자기 회귀 메커니즘이라는 것은 아마도 놀라운 일이다. LM이 일반적인 문제 해결자를 향해 구축되기에 그러한 간단한 메커니즘은 충분한가? 그렇지 않다면 현재의 패러다임에 도전하는 문제는 무엇이며, 대체 메커니즘은 무엇이어야 하는가?

인간의 인지에 관한 문헌은 이러한 질문에 답할 수 있는 몇 가지 단서를 제공한다. "이중 프로세스" 모델에 대한 연구에 따르면 사람들은 결정에 참여하는 두 가지 모드를 가지고 있다 - 빠르고 자동적이며 무의식적인 모드("시스템 1")와 느리고 고의적이며 의식적인 모드("시스템 2") [30; 31; 16; 15]. 이 두 가지 모드는 이전에 기계 학습에 사용되는 다양한 수학적 모델과 연결되었다. 예를 들어, 인간 및 다른 동물의 강화 학습에 대한 연구는 그들이 연관 "모델 프리" 학습 또는 더 숙의적인 "모델 기반" 계획에 관여하는 상황을 탐구했다[7]. LMs의 단순한 연관 토큰 수준 선택도 "시스템 1"을 연상시키므로 (1) 하나를 선택하는 대신 현재 선택에 대한 다양한 대안을 유지 및 탐색하고 (2) 현재 상태를 평가하고 더 많은 글로벌 결정을 내리기 위해 적극적으로 앞 또는 뒷바퀴를 보는 보다 의도적인 "시스템 2" 계획 프로세스에 의해 보강의 이점을 얻을 수 있다.

이러한 계획 프로세스를 설계하기 위해 우리는 1950년대부터 뉴웰, 쇼, 사이먼이 탐구한 계획 프로세스에서 영감을 끌어내는 인공 지능(및 인지 과학)의 기원으로 돌아간다[21; 22]. 뉴웰과 동료들은 문제 풀이[21]를 나무로 표현된 조합적 문제 공간을 통한 탐색으로 특성화했다. 따라서 본 논문에서는 언어 모델을 이용한 일반적인 문제 해결을 위한 T(Tree of Thoughts) 프레임워크를 제안한다. 도 1에 도시된 바와 같이, 기존의 방법들(이하에서 상세히 설명됨)이 문제 해결을 위한 연속 언어 시퀀스들을 샘플링하는 반면, ToT는 생각 트리를 능동적으로 유지하며, 여기서 각각의 _생각_은 문제 해결을 위한 중간 단계 역할을 하는 일관성 있는 언어 시퀀스이다(표 1). 이러한 높은 수준의 의미 단위는 LM이 언어에서도 인스턴스화되는 의도적인 추론 과정을 통해 문제를 해결하기 위한 서로 다른 중간 사고가 하는 진행 상황을 스스로 평가할 수 있게 한다(도 2,4,6). LM 자체 평가 및 심의를 통한 검색 휴리스틱의 이러한 구현은 이전의 검색 휴리스틱이 프로그래밍되거나 학습되기 때문에 참신하다. 마지막으로, 다양한 생각을 생성하고 평가할 수 있는 언어 기반 기능을 탐색 알고리즘인 BFS(breadth-first search) 또는 DFS(depth-first search)와 결합하여 룩어헤드(lookahead)와 역추적(backtracking)을 통해 생각 트리를 체계적으로 탐색할 수 있다.

실증적으로, 우리는 최첨단 언어 모델인 GPT-4[23]: 게임 오브 24, 크리에이티브 글쓰기, 크로스워드(표 1)로도 기존의 LM 추론 방법에 도전하는 세 가지 새로운 문제를 제안한다. 이러한 과제는 연역적, 수학적, 상식적, 어휘적 추론 능력과 체계적인 계획이나 탐색을 통합하는 방법을 필요로 한다. 우리는 ToT가 다른 수준의 생각, 생각을 생성하고 평가하는 다른 방법, 다른 문제의 특성에 적응하는 다른 검색 알고리즘을 지원하기에 충분히 일반적이고 유연하여 세 가지 과제 모두에서 우수한 결과를 얻음을 보여준다. 또한 이러한 선택이 체계적인 삭제를 통해 모델 성능에 어떤 영향을 미치는지 분석하고 LM을 더 잘 훈련하고 사용하기 위한 향후 방향에 대해 논의한다.

## 2 Background

우리는 먼저 문제 해결을 위해 큰 언어 모델을 사용하는 기존의 몇 가지 방법을 공식화하는데, 이는 우리의 접근법이 영감을 받고 나중에 비교된다. \(p_{\theta}\)를 사용 하 여 매개 변수 \(\theta\)를 사용 하 고 **소문자** \(x,y,z,s,\cdots\)** 는 언어 시퀀스를 나타냅니다. 즉, \(x=(x[1],\cdots,x[n])\입니다. 여기서 각 \(x[i]\)는 토큰이므로 \(p_{\theta}(x)=\prod_{i=1}^{n}p_{\theta}(x[i]|x[1...i])\. 우리는 언어 시퀀스의 모음을 나타내기 위해 대문자 \(S,\cdots\)를 사용한다.

**입력 출력 프롬프트** 는 LM을 사용 하 여 문제 입력 \(x\)을 출력 \(y\)으로 전환 하는 가장 일반적인 방법입니다. \(y\sim p_{\theta}(y|\texttt{prompt}_{IO}(x))\), \(\texttt{prompt}_{IO}(x)\) 작업 명령 및/또는 소수의 샷 입력 출력 예제를 사용 하 여 입력 \(x\)을 래핑 합니다. 단순화를 위해 \(p_{\theta}^{\text{prompt}}(\texttt{output}\mid\texttt{input})=p_{\theta}(\texttt{output}\mid\texttt{prompt}(\texttt{input}))\)로 표현하여 IO 프롬프팅이 \(y\sim p_{\theta}^{IO}(y|x)\으로 공식화될 수 있도록 한다.

도 1: LLM을 사용한 문제 해결에 대한 다양한 접근법을 예시하는 개략도. 각 사각형 상자는 문제 해결을 위한 중간 단계 역할을 하는 일관성 있는 언어 시퀀스인 _생각_ 을 나타냅니다. 도 2,4,6에서 생각이 어떻게 생성되고 평가되고 검색되는지에 대한 구체적인 예를 참조한다.

**CoT(Chain-of-though) 프롬프트**[38]는 입력 \(x\)과 출력 \(y\)의 매핑이 사소하지 않은 경우(예: \(x\)이 수학 질문이고 \(y\)이 최종 숫자 답인 경우)를 해결하기 위해 제안되었습니다. 핵심 아이디어는 _thoughts_\(z_{1},\cdots,z_{n}\)의 연쇄를 도입하여 \(x\)와 \(y\)을 연결하는 것인데, 여기서 각 \(z_{i}\)은 문제 해결을 위한 의미 있는 중간 단계 역할을 하는 일관성 있는 언어 시퀀스이다(예: \(z_{i}\)는 수학 QA의 중간 방정식이 될 수 있다). CoT 문제를 해결하기 위해, 각각의 생각 \(z_{i}\sim p_{\theta}^{CoT}(z_{i}\mid x,z_{1\cdots i-1})\)을 순차적으로 샘플링하고 출력 \(y\sim p_{\theta}^{CoT}(y|x,z_{1\cdots n})\)을 계산한다. 실제로, \([z_{1\cdots n},y]\sim p_{\theta}^{CoT}(z_{1\cdots n},y|x)\)는 연속적인 언어 시퀀스로 샘플링되고, 생각의 **분해**(예: 각 \(z_{i}\) 구, 문장 또는 단락)는 모호하게 남겨진다.

**CoT와의 자기 일관성(CoT-SC)**[36]은 \(k\) i.i.d. 생각의 사슬: \([z_{1\cdots n}^{(i)},y^{(i)}]\sim p_{\theta}^{CoT}(z_{1\cdots n},y|x)\)\((i=1\cdots k)\))을 샘플링한 다음 가장 빈번한 출력을 반환합니다. \(\operatorname*{arg\,max}_{y}\#\{i\mid y^{(i)}=y\}\). CoT-SC는 일반적으로 동일한 문제에 대해 다른 사고 과정(예: 동일한 정리를 증명하는 다른 방법)이 있고 더 풍부한 사고 세트를 탐색함으로써 출력 결정이 더 충실할 수 있기 때문에 CoT에 따라 개선된다. 그러나, 각각의 체인 내에서 상이한 사고 단계들에 대한 로컬 탐색은 없으며, "가장 빈번한" 휴리스틱은 출력 공간이 제한된 경우에만 적용된다(예를 들어, 다중 선택 QA).

## 3 Tree of Thoughts: Deliberated Problem Solving with LM

_진정한 문제 해결 프로세스_는 탐색을 시작하기 위해 사용 가능한 정보를 반복적으로 사용하는 것을 포함하며, 이는 솔루션을 달성하는 방법이 최종적으로 발견될 때까지 차례로 더 많은 정보를 공개합니다.--- - Newell et al. [21]

인간의 문제 해결에 대한 연구는 사람들이 조합적 문제 공간을 통해 탐색한다는 것을 제시한다. 즉, 노드는 부분적인 해를 나타내는 트리이며, 브랜치는 이를 수정하는 연산자에 해당한다[21, 22]. 어떤 분기를 취할지는 문제 공간을 탐색하고 문제 해결자를 해결책을 향해 안내하는 데 도움이 되는 휴리스틱에 의해 결정된다. 이 관점은 일반적인 문제를 해결하기 위해 LM을 사용하는 기존 접근법의 두 가지 주요 단점을 강조한다. 2) 전 세계적으로, 이들은 이러한 상이한 옵션들 - 인간의 문제 해결의 특징처럼 보이는 휴리스틱-유도 탐색의 유형 - 을 평가하는 것을 돕기 위해 어떠한 유형의 계획, 룩어헤드 또는 역추적도 통합하지 않는다.

이러한 단점을 해결하기 위해, 우리는 LM이 사고를 통해 여러 추론 경로를 탐색할 수 있는 패러다임인 _T(Tree of Thoughts)_를 소개한다(그림 1(c)). ToT는 모든 문제를 트리를 통한 검색으로 프레임화 합니다. 여기서 각 노드는 **상태**\(s=[x,z_{1\cdots i}]\)입니다. ToT의 특정 인스턴스화는 4가지 질문에 응답하는 것을 포함합니다. 1. 중간 프로세스를 생각 단계로 **분해** 하는 방법; 2. 각 상태에서 잠재적인 생각을 **생성** 하는 방법; 3. 상태를 휴리스틱하게 **평가** 하는 방법; 4. 사용할 **검색** 알고리즘입니다.

**1. 생각 분해.** CoT는 명시적 분해 없이 일관성 있게 생각을 샘플링 하는 동안 ToT는 문제 속성을 활용 하 여 중간 생각 단계를 설계 하 고 분해 합니다. 표 1에서 알 수 있듯이 서로 다른 문제에 따라 생각은 몇 개의 단어(크로스워드), 방정식 선(게임 24) 또는 쓰기 계획의 전체 단락(창의적 쓰기)일 수 있다. 일반적으로, 생각은 LM들이 유망하고 다양한 샘플들을 생성할 수 있도록 충분히 "작음"이어야 한다(예를 들어, 전체 책을 생성하는 것은 일반적으로 일관성 있게 하기에는 너무 "큰"이다), 그러나 LM들이 문제 해결에 대한 전망을 평가할 수 있도록 충분히 "큰"이어야 한다(예를 들어, 하나의 토큰을 생성하는 것은 일반적으로 평가하기에는 너무 "작은"이다).

**2. 생각 생성기 \(G(p_{\theta},s,k)\).** 트리 상태 \(s=[x,z_{1\cdots i}]\)가 주어지면 다음 생각 단계에 대 한 \(k\) 후보를 생성 하는 두 가지 전략을 고려 합니다.

1. **샘플** i.i.d. CoT 프롬프트(Creative Writing, 그림 4): \(z^{(j)}\sim p_{\theta}^{CoT}(z_{i+1}|s)=p_{\theta}^{CoT}(z_{i+1}|x,z_{1\cdots i })\)\((j=1\cdots k)\). 이것은 사고 공간이 풍부할 때(예를 들어, 각각의 사상은 단락임), i.i.d. 샘플이 다양성으로 이어질 때 더 잘 작동한다;
2. **제안** 생각을 "제안 프롬프트"(게임 24, 그림 2; 교차 단어, 그림 6): \([z^{(1)},\cdots,z^{(k)}]\sim p_{\theta}^{propose}(z_{i+1}^{(1\cdots k)}\mid s)\)를 사용하여 순차적으로 생각합니다. 이는 사고 공간이 더 제약될 때(예를 들어, 각각의 사고는 단지 단어 또는 선일 뿐임) 더 잘 작동하므로, 동일한 맥락에서 상이한 생각을 제안하는 것은 중복을 회피한다.

**3. 상태 평가기 \(V(p_{\theta},S)\).** 다른 상태의 프런티어가 주어지면 상태 평가기는 문제를 해결 하기 위해 수행 하는 진행 상황을 평가 하 여 탐색을 계속할 상태 및 순서를 결정 하는 검색 알고리즘에 대 한 _휴리스틱_ 역할을 합니다. 휴리스틱이 검색 문제를 해결하기 위한 표준 접근법이지만, 이들은 전형적으로 프로그래밍(예를 들어, DeepBlue[3])되거나 학습(예를 들어, AlphaGo[29])된다. 우리는 LM을 사용하여 주에 대해 의도적으로 추론함으로써 세 번째 대안을 제안한다. 적용 가능한 경우, 그러한 고의적 휴리스틱은 프로그래밍된 규칙보다 더 유연할 수 있고, 학습된 모델보다 더 샘플 효율적일 수 있다. 생각 생성기와 유사하게 우리는 상태를 독립적으로 또는 함께 평가하는 두 가지 전략을 고려한다.

1. **값** 각 상태는 독립적으로 \(V(p_{\theta},S)(s)\sim p_{\theta}^{value}(v|s)\)\(\forall s\in S\)입니다. 여기서 값은 상태 \(s\)에 대한 이유를 프롬프트하여 스칼라 값 \(v\)(예: 1-10) 또는 휴리스틱하게 값으로 변환될 수 있는 분류(예: 확실/가능/불가능)를 생성합니다. 이러한 평가적 추론의 기초는 문제와 사고 단계에 따라 달라질 수 있다. 이 작업에서 우리는 몇 가지 _lookahead_ 시뮬레이션을 통해 평가를 탐색한다(예: 5, 5, 14가 5 + 5 + 14를 통해 24에 도달할 수 있는지 빠르게 확인하거나 "hot_l"이 "-"에 "e"를 채우는 것을 통해 "inn"을 의미할 수 있음). + 상식(예: 12 3은 24에 도달하기에는 너무 작거나 "tzxc"로 시작할 수 있는 단어가 없다. 전자가 "좋은" 상태를 촉진할 수 있지만 후자는 "나쁜" 상태를 제거하는 데 도움이 될 수 있다. 그러한 평가는 완벽할 필요는 없고, 단지 의사 결정에 대략적으로 도움이 될 필요가 있다.
2. 상태 간 **투표**: \(V(p_{\theta},S)(s)=\mathds{1}[s=s^{*}]\), 여기서 "좋은" 상태 \(s^{*}\sim p_{\theta}^{vote}(s^{*}|S)\)는 투표 프롬프트에서 \(S\)의 다른 상태를 의도적으로 비교하는 것을 기반으로 투표됩니다. 문제 성공이 직접적으로 가치를 매기기 어려울 때(예: 통과 일관성) 다른 부분 해결책을 대신 비교하고 가장 유망한 해결책에 투표하는 것은 당연하다. 이것은 "단계적" 자기 일관성 전략, 즉 다중 선택 QA로 "탐구할 상태"를 캐스팅하고 LM 샘플을 사용하여 투표하는 것과 정신적으로 유사하다.

두 전략 모두에 대해 우리는 LM이 가치를 집계하거나 투표 결과를 더 충실한/강력한 휴리스틱을 위해 시간/자원/비용을 교환하도록 여러 번 프롬프트할 수 있다.

```
입력 \(x\), LM \(p_{\theta}\), 사고 발생기 \(G()\) & 크기 제한 \(k\), 상태 평가기 \(V()\), 단계 제한 \(T\), 폭 제한 \(b\). \ (S_{0}\leftarrow\{x\}\) for\(t=1,\cdots,T\)do \(S^{\prime}_{t}\leftarrow\{[s,z]\mid s\in S_{t-1},z_{t}\in\mathrm{G}(p_{\theta},s,k)\}\) \(V_{t}\gets V(p_{\theta},S^{\prime}_{t)\) \(S_{t}\leftarrow\operatorname*{arg\,max}_{S\subset S^{\prime}_{t},|S|=b} \sum_{s\in S}V_{t}(s)\) endfor return\(G(p_{\theta},\operatorname*{arg\,max}_{s\in S_{T}}V_{T}(s),1)\)
```

**알고리즘 1** ToT-BFS(\(x,p_{\theta},G,k,V,T,b\))

**알고리즘 2** ToT-DFS(\(s,t,p_{\theta},G,k,V,T,v_{th}\))

**4. 검색 알고리즘.** 마지막으로 ToT 프레임워크 내에서 트리 구조에 따라 다른 검색 알고리즘을 플러그 앤 플레이할 수 있습니다. 비교적 간단한 두 가지 검색 알고리즘을 탐색 하 고 향후 작업을 위해 고급 알고리즘 (예: A* [11], MCTS [2])을 남깁니다.

1. **BFS (Breadth-first search)** (알고리즘 1)는 단계당 \(b\) 가장 유망한 상태 집합을 유지 관리 합니다. 이것은 나무 깊이가 제한(\(T\leq 3\))인 24 게임과 창의적 글쓰기에 사용되며, 초기 사고 단계는 작은 집합(\(b\leq 5\))으로 평가 및 프루닝될 수 있다.
2. **깊이 우선 검색 (DFS)** (알고리즘 2)은 최종 출력에 도달할 때까지 가장 유망한 상태를 먼저 탐색 합니다 (\(t>T\)) 또는 상태 평가자는 현재 \(s\)(값 임계값 \(v_{th}\))에서 문제를 해결할 수 없다고 간주 합니다 (\(V(p_{\theta},\{s\})(s)\leq v_{th}\). 후자의 경우 \(s\)의 하위 트리는 개발을 위한 무역 탐색으로 프루닝된다. 두 경우 모두 DFS _backtracks_ 를 \(s\)의 부모 상태로 이동하여 탐색을 계속합니다.

개념적으로 ToT는 LMs로 일반적인 문제 해결을 위한 방법으로서 몇 가지 이점을 가지고 있다. (1) _일반성_ IO, CoT, CoT-SC, self-refinement는 ToT의 특수한 경우(즉, 깊이와 폭이 제한된 나무; 그림 1)로 볼 수 있다. (2) _Modularity_. 기반 LM과 사고 분해, 생성, 분석, 탐색 과정은 모두 독립적으로 달라질 수 있다. (3) _적응성_. 상이한 문제 속성들, LM 능력들 및 자원 제약들이 수용될 수 있다. (4) _편리성_. 추가 훈련은 필요하지 않으며 사전 훈련된 LM만으로도 충분하다. 다음 절에서는 이러한 개념적 이익이 서로 다른 문제에서 강력한 경험적 성과로 어떻게 해석되는지를 보여줄 것이다.

## 4 Experiments

표준 IO 프롬프트 또는 CoT 프롬프팅을 사용하여 최신 언어 모델인 GPT-4 [23]에서 샘플링하더라도 어려운 세 가지 작업을 제안한다. 우리는 생각의 나무(ToT)에서 의도적인 탐색이 더 나은 결과를 생성하는 방법을 보여주며, 더 중요하게는 검색이나 계획이 필요한 문제를 해결하기 위해 언어 모델을 사용하는 흥미롭고 유망한 새로운 방법을 보여준다. 달리 명시되지 않는 한 샘플링 온도가 0.7인 채팅 완료 모드 GPT-41을 사용하여 실험을 수행한다.

각주 1: 실험은 2023년 5월 5일부터 16일 사이에 수행되었다.

### 24 게임

24의 게임은 수학적 추론 도전이며, 여기서 목표는 24를 얻기 위해 4개의 숫자와 기본적인 산술 연산(+-*/)을 사용하는 것이다. 예를 들어, 입력 "4 9 10 13"이 주어지면, 솔루션 출력은 "(10 - 4) * (13 - 9) = 24)일 수 있다.

**작업 설정.** 4nums.com의 데이터를 긁어냅니다. 4nums.com은 인간의 해결 시간에 따라 쉬운 게임부터 어려운 게임까지 정렬된 1,362개의 게임을 가지고 있으며 상대적으로 어려운 게임의 하위 집합을 테스트에 901-1,000으로 인덱싱합니다. 각 과제에 대해 우리는 24와 같은 유효한 방정식이고 입력 숫자를 각각 정확히 한 번 사용하는 경우 출력을 성공으로 간주한다. 100개 게임의 성공률을 미터법으로 보고합니다.

**기준.** 5개의 컨텍스트 내 예제와 함께 표준 IO(입력 출력) 프롬프트를 사용합니다. CoT(chain-of-thought) 프롬프팅을 위해, 우리는 각각의 입력-출력 쌍을 3개의 중간 방정식으로 증분하고, 각각의 중간 방정식은 두 개의 나머지 숫자로 동작한다. 예를 들어, 입력 "4 9 10 13"이 주어지면, 생각들은 "13 - 9 = 4 (왼쪽: 4 10); 10 - 4 = 6 (왼쪽: 4 6); 4 * 6 = 24 (왼쪽: 24)"일 수 있다. 각 게임에 대해 평균 성능을 위해 IO 및 CoT 프롬프트를 100회 샘플링합니다. 또한 100개의 CoT 샘플에서 대부분의 출력을 취하는 CoT 자기 일관성 기준선과 최대 \(10\) 반복 동안 IO 샘플 위에 반복 정제 접근법을 고려한다. 각 반복에서 LM은 모든 이전 이력을 조건으로 하여 출력이 잘못된 경우 "실수를 반영하고 정제된 답변을 생성"한다. 방정식의 정확성에 대한 접지진 피드백 신호를 사용합니다.

**ToT 설정.** 24의 게임을 ToT로 프레임화 하려면 생각을 각 중간 방정식의 3 단계로 분해 하는 것이 당연 합니다. 그림 2(a)와 같이 각 트리 노드에서 나머지 숫자를 정확하게 지정하고 LM이 몇 가지 가능한 다음 단계를 제안하도록 프롬프트한다. 동일한 "프로포즈 프롬프트"가 3개의 사고 단계 모두에 사용되지만 4개의 입력 숫자로 하나의 예만 있다. ToT에서 폭 우선 탐색(BFS: breadth-first search)을 수행하며, 각 단계에서 최적의 후보 \(b=5\)을 유지한다. 그림 2(b)와 같이 ToT에서 고의적인 BFS를 수행하기 위해 LM이 각 사고 후보를 24에 도달하는 것과 관련하여 "확신/가능/불가능"으로 평가하도록 촉구한다. 목표는 몇 번의 시도 내에서 검증될 수 있는 올바른 부분 솔루션을 촉진하고 "너무 크거나 작은" 상식을 기반으로 불가능한 부분 솔루션을 제거하고 나머지는 "가능"을 유지하는 것이다. 우리는 각 사고에 대한 값을 \(3\)회 샘플링한다.

\begin{table}
\begin{tabular}{l|l l l} \hline \hline  & **Game of 24** & **Creative Writing** & **5x5 Crosswords** \\ \hline
**Input** & 4 numbers (4 9 10 13) & 4 random sentences & 10 clues (h1.presented;..) \\ \hline
**Output** & An equation to reach 24 (13-9)*(10-4)=24 & A passage of 4 paragraphs ending in the 4 sentences & 5x5 letters: SHOWN; WIRRA; AVAIL;... \\ \hline
**Thoughs** & 3 intermediate equations (13-9=4 (left 4,4,10); 10-4=6 (left 4,6); 4*6=24) & A short writing plan (1. Introduce a book that connects...) & Words to fill in for clues: (h1.shown; v5.naled;...) \\ \hline
**\#ToT steps** & 3 & 1 & 5-10 (variable) \\ \hline \hline \end{tabular}
\end{table}
표 1: 작업 개요. 입력, 출력, 생각 예는 파란색입니다.

도 2: 24의 게임에서의 ToT. LM은 (a) 사상 생성 및 (b) 가치 평가를 촉구한다.

**결과.** 표 2에서 볼 수 있듯이 IO, CoT 및 CoT-SC 프롬프트 방법은 작업을 잘못 수행 하 여 7.3%, 4.0% 및 9.0%의 성공률만 달성 합니다. 반면, 넓이가 \(b=1\)인 ToT는 이미 \(45\%\)의 성공률을 달성하였고, \(b=5\)는 \(74\%\)의 성공률을 달성하였다. 또한, \(k\)개의 샘플(\(1\leq k\leq 100\))을 이용하여 성공률을 계산하여 IO/CoT를 위한 오라클 설정을 고려한다. IO/CoT (best of k)와 ToT를 비교하기 위해 \(b=1\cdots 5\)에 걸쳐 ToT에서 작업당 방문한 트리 노드를 계산하는 것을 고려하고 그림 3(a)에서 IO/CoT (best of \(k\))를 산적에서 \(k\) 노드를 방문하는 것으로 간주하여 5개의 성공률을 매핑한다. 물론 CoT는 IO보다 더 잘 확장되며 100개의 CoT 샘플 중 가장 좋은 것은 \(49\%\)의 성공률을 달성하지만 ToT에서 더 많은 노드를 탐색하는 것보다 훨씬 더 나쁘다(\(b>1\)).

**오류 분석.** 그림 3(b)는 CoT 및 ToT 샘플이 작업에 실패 하는 단계, 즉 생각 (CoT 내) 또는 모든 \(b\) 생각 (ToT 내)이 유효 하지 않거나 24에 도달 하는 것이 불가능 합니다. 특히 CoT 샘플의 약 60%가 첫 번째 단계를 생성 한 후 작업을 이미 실패 했습니다 (예: "\(4+9\)"). 이것은 직접적인 좌-우 디코딩의 문제들을 강조한다.

### Creative writing

다음으로, 4개의 무작위 문장이 입력되고 4개의 입력 문장에서 각각 끝나는 4개의 단락으로 일관성이 있는 구절이 출력되어야 하는 창의적 쓰기 과제를 발명한다. 이러한 과제는 개방적이고 탐구적이며, 높은 수준의 계획뿐만 아니라 창의적인 사고에 도전한다.

**작업 설정.** 무작위 단어 생성자.com의 무작위 문장을 샘플링하여 100개의 입력을 형성하며, 각 입력 제약 조건에 대한 그라운드 트루스가 없습니다. GPT-4가 대부분의 시간 동안 입력 제약을 따를 수 있음을 발견함에 따라 GPT-4 제로 샷 프롬프트를 사용하여 1-10 스칼라 점수를 제공하거나 인간 판단을 사용하여 다른 방법의 출력 쌍을 비교하는 두 가지 방법으로 통과 일관성을 평가하는 데 중점을 둔다. 전자의 경우 5개의 점수를 샘플링하고 각 작업 출력에 대해 평균을 내며 이 5개의 점수는 보통 일치하며 출력 전체에 걸쳐 평균 약 \(0.56\)의 표준 편차를 발견한다. 후자의 경우 CoT 대 CoT의 일관성을 비교하기 위해 블라인드 연구에서 저자의 하위 집합을 사용한다. ToT 생성 통로 쌍은 통로의 순서가 100개의 입력에 대해 무작위로 뒤집힌다.

**기준.** 작업의 창의적 특성을 고려할 때 IO 및 CoT 프롬프트는 모두 제로 샷입니다. 전자는 입력 제약 조건이 주어진 일관된 패시지를 직접 생성하도록 LM에게 프롬프트하는 반면, 후자는 먼저 간략한 계획을 만든 다음 패시지를 작성하도록 LM에게 프롬프트한다. 즉, 계획은 중간 사고 단계 역할을 한다. 작업당 10개의 IO 및 CoT 샘플을 생성합니다. 또한 각 태스크에 대한 랜덤 IO 샘플 위에 반복 정제(\(k\leq 5\)) 방법을 고려하며, 여기서 LM은 입력 제약 조건에 따라 조정되고 마지막으로 생성된 계대는 계대가 이미 "완벽하게 일관성"인지, 정제된 계대는 생성되지 않는지를 결정한다.

**ToT 설정.** 깊이 2 (및 중간 생각 단계 1 개만)를 사용 하 여 ToT를 빌드합니다. - LM은 먼저 \(k=5\) 계획을 생성 하 고 최상의 계획에 대해 투표 합니다 (그림 4). 그런 다음 유사하게 최상의 계획에 따라 \(k=5\) 구절을 생성 하 고 최상의 계획에 대해 투표 합니다. 여기서 폭 제한 \(b=1\)은 단계당 하나의 선택만 유지되기 때문이다. 간단한 제로 샷 투표 프롬프트("아래에서 선택들을 분석하고, 이어서 지시사항에 대해 가장 유망한 것을 결론내림")는 두 단계 모두에서 5표를 샘플링하는 데 사용된다.

**결과.** 그림 5(a)는 100개 작업에 대한 평균 GPT-4 점수를 보여줍니다. 여기서 ToT(7.56)는 IO(6.19) 및 CoT(6.93)보다 더 일관성 있는 패시지를 생성하는 것으로 간주됩니다. 이러한 자동 메트릭은 시끄러울 수 있지만 그림 5(b)는 인간이 100개의 통로 쌍 중 41개에서 CoT보다 ToT를 선호하는 반면 21개에서는 ToT보다 CoT를 선호한다는 것을 보여줌으로써 발견을 확인한다(다른 38개 쌍은 "유사하게 일관성 있는" 것으로 발견됨). 마지막으로, 반복-정제는 이 자연 언어 작업에 더 효과적이며, 여기서 반복-정제는 이 자연 언어 작업에 더 효과적이다.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Method** & **Success** \\ \hline IO prompt & 7.3\% \\ CoT prompt & 4.0\% \\ CoT-SC (k=100) & 9.0\% \\ ToT (ours) (b=1) & 45\% \\ ToT (ours) (b=5) & **74\%** \\ \hline IO + Refine (k=10) & 27\% \\ IO (best of 100) & 33\% \\ CoT (best of 100) & 49\% \\ \hline \hline \end{tabular}
\end{table}
표 2: 24결과의 게임. 도 3 : 게임 24(a)척도분석 & (b)오차분석.

이는 IO 일관성 점수를 6.19에서 7.67로, ToT 일관성 점수를 7.56에서 7.91로 향상시킨다. 우리는 I.I.D. 대신 오래된 생각을 정제하거나 순차적으로 생성함으로써 새로운 생각이 발생할 수 있는 ToT 프레임워크에서 사고 생성에 대한 세 번째 접근법으로 생각할 수 있다고 믿는다.

### Mini crosswords

24 게임과 창작 글쓰기에서 ToT는 상대적으로 얕다. - 최종 산출물에 도달하려면 최대 3개의 사고 단계가 필요하다. 여기에서 우리는 자연어를 포함하는 더 어려운 검색 문제로 \(5\times 5\) 미니 십자어를 탐구한다. 다시 말하지만, LM 대신 대규모 검색을 활용하는 전문 NLP 파이프라인[34]으로 보다 일반적인 십자말풀이를 쉽게 해결할 수 있기 때문에 목표는 단순히 과제를 해결하는 것이 아니다. 오히려 자기 생각을 탐색하고, 의도적 추론을 휴리스틱으로 자기 탐구를 안내하는 일반적인 문제 해결자로서 LM의 한계를 탐색하는 것을 목표로 한다.

**작업 설정.** 156게임의 \(5\times 5\) 미니 크로스워드를 포함하는 GooBix에서 데이터를 긁어냅니다. 인접 게임들이 유사한 단서들을 포함하고 있음을 관찰하여, 테스트에는 인덱스 \(1,6,\cdots,91,96\), 프롬프트에는 게임 \(136,141,146,151,156\)이 포함된 20개의 게임을 사용하였다. 각 과제마다 입력은 5개의 가로 단서와 5개의 세로 단서를 기술하며, 출력은 십자말풀이를 위해 \(5\times 5=25\) 글자의 보드가 되어야 한다. 평가를 위해 올바른 글자의 비율(게임당 25개), 단어(게임당 10개), 게임의 세 가지 성공 수준을 고려한다.

**기준.** IO 프롬프트에 5개의 예제 입력-출력 쌍을 제공하고 CoT 프롬프트에는 h1..5 다음 v1..5 순서로 중간 단어가 추가로 포함됩니다. 각 프롬프트를 10개 샘플에 대해 실행하고 결과를 평균합니다.

**ToT 설정.** 상태가 더 이상 유망하지 않을 때까지 가장 유망한 후속 단어 단서를 계속 탐색한 다음 부모 상태로 백트랙하여 대체 생각을 탐색하는 깊이 우선 검색(알고리즘 2)을 활용합니다. 탐색을 다루기 쉽게 만들기 위해 후속 생각은 채워진 단어나 글자를 변경하지 않도록 제한되므로 ToT는 최대 10개의 중간 단계를 갖는다. 생각 생성을 위해 각 상태에서 기존 모든 생각 (예: 그림 6(a)의 상태에 대 한 "h2. 모터; h1. 작업")을 나머지 단서에 대 한 문자 제약 사항 (예: "v1.To heap: tm_,...;...")으로 변환 하 고 다음 단어를 채울 위치와 항목에 대 한 후보를 작성 하도록 제안 프롬프트 \(5\) 회를 프롬프트 합니다. 중요한 것은 LM이 다양한 생각에 대한 신뢰 수준을 제공하도록 유도하고 집계한다.

그림 4: 무작위로 선정된 창의적 글쓰기 과제에서 고의적으로 탐색하는 단계이다. 입력을 감안할 때 LM은 5개의 다른 계획을 샘플링한 다음 5번 투표하여 어떤 계획이 가장 좋은지 결정한다. 다수결 선택은 결과적으로 동일한 표본 투표 절차로 출력 패시지를 작성하는 데 사용된다.

도 5: 창의적 글쓰기 결과.

\begin{table}
\begin{tabular}{l|l l l} \hline \hline
**Method** & \multicolumn{3}{l}{**Success Rate (\%)**} \\  & **Letter Word** & **Game** \\ \hline IO & 38.7 & 14 & 0 \\ CoT & 40.6 & 15.6 & 1 \\ ToT (ours) & **78** & **60** & **20** \\ \hline +best state & 82.4 & 67.5 & 35 \\ -prune & 65.4 & 41.5 & 5 \\ -backtrack & 54.6 & 20 & 5 \\ \hline \hline \end{tabular}
\end{table}
표 3: 미니 크로스워드 결과.

이것은 탐색할 다음 생각의 정렬된 목록을 얻기 위한 제안 전반에 걸쳐 있다(그림 6(a)). 상태 평가를 위해 우리는 유사하게 각 상태를 나머지 단서에 대한 문자 제약으로 번역한 다음 제약 조건이 주어진 경우 채울 수 있는지 각 단서에 대해 평가한다. 나머지 단서가 채울 수 없는 것으로 간주되는 경우(예: "v1. 힙: tm_s_") 주의 하위 트리에 대한 탐색은 가지치기되고 DFS는 다음 유망한 생각을 탐색하기 위해 부모에게 역추적한다. 우리는 DFS 탐색 단계를 100으로 제한하고 가장 깊이 탐색된 상태(다중인 경우 처음 탐색된 상태)를 최종 출력으로 간단히 렌더링한다.

**결과.** 표 3에서 볼 수 있듯이 IO 및 CoT 프롬프트 방법은 단어 수준 성공률 \(16\%\) 미만으로 성능이 좋지 않은 반면 ToT는 모든 메트릭을 크게 향상시켜 단어 수준 성공률 \(60\%\)을 달성하고 20개 게임 중 4개를 해결합니다. IO 및 CoT가 다른 단서를 시도하거나 결정을 변경하거나 백트랙하는 메커니즘이 부족하기 때문에 이러한 개선은 놀라운 일이 아니다.

**오라클 및 절제 연구.** 작업당 오라클 최상의 DFS 상태(휴리스틱하게 결정된 최상의 상태 대신)에서 출력할 때 ToT 성능은 훨씬 더 높고 실제로 7/20 게임(표 3, "+최상의 상태")을 해결하여 간단한 출력 휴리스틱을 쉽게 개선할 수 있음을 나타냅니다. 흥미롭게도 크로스워드 게임이 실제로 해결되었을 때 상태 평가자는 여전히 일부 단어를 "불가능"으로 간주하고 프루닝을 할 수 있다. 아마도 디자인에 의한 \(5\times 5\) 크로스워드에는 GPT-4가 인식할 수 없는 희귀하거나 쓸모없는 단어가 있기 때문일 것이다. 프루닝 휴리스틱으로 상태 평가가 불완전하다면 프루닝 제거도 탐색하고 일반적으로 성능이 더 나쁘다는 것을 발견한다(표 3, "-프루닝"). 그러나 실제로 4/20 게임(휴리스틱을 통해 1개만 출력하지만)에 대한 정확한 해를 찾을 수 있었으며, 이 중 ToT+pruning은 100단계 내에서 해결할 수 없는 게임이다. 따라서 이 경우 DFS 프루닝에 대한 더 나은 휴리스틱이 문제 해결에 중요하다. 마지막으로, 최대 20단계 동안 가장 유망한 단서를 계속 채워 덮어쓰기를 허용하는 절제를 실행하여 역추적의 중요성을 확인한다. 이것은 폭 한계 \(b=1\)인 "탐욕스러운" BFS 검색과 유사하며, 단지 \(20\%\)의 단어 레벨 성공으로 저조한 성능을 수행한다(표 3, "-백트랙").

각주 2: 예를 들어, "에이전트"는 "에이전텀"의 구식 형태이지만, GPT-4는 "아젠다"에 대한 오타로 간주한다. 외부 검색이나 웹 상호 작용은 지식 불확실성 하에서 문제 해결을 위한 LM을 증가시킬 수 있다.

## 5 관련 작업

**계획 및 의사 결정** 스마트 계획 및 의사 결정은 미리 정의된 목표를 달성하는 데 중요합니다. 그들은 방대한 세계 지식과 인간의 예에 대해 훈련받았기 때문에 LM은 이미 문제 설정 및 환경 상태에 조건화된 합리적인 계획을 제안할 수 있는 풍부한 상식을 흡수한 것으로 알려져 있다[12; 42; 37; 13; 35; 41; 40]. 제안된 ToT 접근법은 각 문제 해결 단계에서 잠재적으로 실현 가능한 여러 계획을 동시에 고려하고 가장 유망한 계획을 진행함으로써 기존 계획 공식을 확장한다. 사고 샘플링과 가치 피드백 간의 통합은 계획과 의사 결정 메커니즘을 유기적으로 통합하여 솔루션 트리 내에서 효과적인 검색을 가능하게 한다. 한편, 전통적인 의사 결정 절차는 보통 강화 학습에서와 같이 훈련 전용 보상 및 정책 모델을 필요로 하는 반면(예를 들어, CHAI[33]) 우리는 의사 결정을 위한 가치 추정치를 제공하기 위해 LM 자체를 사용한다. RAP[9]는 언어 모델을 다루는 동시 작업이다.

도 6: 미니 크로스워드들에서, (a) 깊이-우선 탐색(DFS)을 위한 우선순위 큐에서 생각들이 제안되고 집계되는 방법, 및 (b) 각각의 남은 단어 단서에 채워질 가능성을 기초로 상태가 평가되고, 임의의 남은 단서가 LM에 의해 채워질 수 없다고 간주되면 프루닝되는 방법. 그런 다음 DFS는 부모 상태로 역추적하고 단서를 위한 다음 유망한 생각을 탐구한다.

추론은 내부 세계 모델을 사용하여 계획하는 것으로 추론하고 ToT와 유사한 MCTS 기반 방법을 제안한다. 그러나 그 작업은 우리보다 간단하고 프레임워크는 다양한 트리 검색 알고리즘을 통합할 수 있는 모듈성이 부족하다.

**자기 반성.** LLM을 사용하여 자체 예측의 실행 가능성을 평가하는 것은 문제 해결에서 점점 더 중요한 절차가 되고 있습니다. [28; 20; 24]는 LM들이 그들의 세대 후보들에게 피드백을 제공하는 "자기-반사" 메커니즘을 도입하였다. [4] LM이 자체적으로 생성한 피드백 메시지를 자신의 코드 실행 결과를 기반으로 주입함으로써 LMs 코드 생성 정확도를 향상시킨다. 유사하게, [17]은 또한 동작들 및 상태들에 대한 "비판적" 또는 검토 단계들을 도입하며, 컴퓨터 동작 태스크들을 풀 때 취할 다음 동작을 결정한다. 우리와 매우 관련이 있는 또 다른 최근 작업은 "자체 평가 안내 디코딩"이다[39]. 우리의 방법과 유사하게 자체 평가 디코딩은 확률적 빔 탐색 디코딩에서 샘플링된 잎을 사용하여 트리 탐색 절차를 따르며, 그런 다음 신중하게 준비된 자체 평가 프롬프트를 사용하여 LLM 자체에 의해 평가된다. 그러나 그들의 접근 방식은 생각을 코드로 나타내는 PAL 공식 [8]을 사용하므로 본 논문에서 고려하는 창의적인 글쓰기와 같은 도전 과제를 해결하기 어렵다. 따라서 우리의 사고 트리 공식은 더 다양하며 GPT-4가 표준 프롬프트와 함께 매우 낮은 정확도만을 달성하는 도전적인 작업을 처리한다.

**프로그램 가이드 LLM 생성.** 우리의 제안은 체계적인 절차 [14; 44; 6; 43] 또는 기호 프로그램 지침으로 LM의 동작을 조직하는 최근 발전과 관련이 있습니다. 예를 들어, Schlag 등[27]은 질의 응답과 같은 문제를 단계적으로 해결하는 것을 돕기 위해 알고리즘 검색 절차에 LM들을 임베딩하고, 여기서 검색 트리들은 답변을 제공할 수 있는 관련 단락들에 의해 확장된다. 그러나 이 접근법은 LM 자신의 생각 대신 외부 단락을 샘플링하여 나무를 확장하고 반영이나 투표 단계가 없다는 점에서 우리와 다르다. 또 다른 접근법인 LLM+P [18]은 한 걸음 더 나아가 실제 계획 과정을 고전적인 계획자에게 위임한다.

**고전적 검색 방법.** 마지막으로 이 방법은 문제 해결을 위한 고전적 검색 방법의 현대판으로 취급할 수 있습니다. 예를 들어, A*[10]과 같은 휴리스틱 탐색 알고리즘으로 간주될 수 있는데, 여기서 각 탐색 노드에서의 휴리스틱은 LM의 자체 평가에 의해 제공된다. 이러한 관점에서, 우리의 방법은 A* 탐색에서 영감을 얻었지만 LM이 빔 탐색 또는 top-k 샘플링 디코딩을 개선하기 위해 효율적인 룩-어헤드 휴리스틱을 도입하는 NeuroLogic A*esque 디코딩 [19]과도 관련이 있다. 그러나 이 방법은 문장제 생성 작업에 제약을 받는 반면, 본 프레임워크는 가치 피드백에 의해 보호되는 복잡하고 다단계 문제 해결을 위해 설계되었다.

## 6 Discussion

**제한 사항 및 향후 방향.** GPT-4가 이미 뛰어난 기존 작업에 ToT와 같은 심의 검색이 필요하지 않을 수 있습니다 (부록 B.1 참조). 이 작업은 초기 단계로 GPT-4에 도전하는 비교적 간단한 세 가지 작업(일부 GPT-3.5 실험 결과에 대한 부록 B.2 참조) 및 LM과 통합된 더 나은 검색 및 계획 능력의 호출만 탐구합니다. 그러나 더 많은 실제 의사 결정 응용 프로그램(예: 코딩, 데이터 분석, 로봇 공학 등)을 위해 LM을 배치하기 시작함에 따라 더 복잡한 작업이 나타나고 이러한 연구 질문을 연구할 수 있는 새로운 기회를 제시할 수 있다. 또한, ToT와 같은 검색 방법은 태스크 성능을 향상시키기 위해 샘플링 방법보다 더 많은 리소스(예를 들어, GPT-4 API 비용)를 필요로 하지만, ToT의 모듈식 유연성은 사용자가 이러한 성능-비용 절충을 맞춤화할 수 있게 하며, 지속적인 오픈 소스 노력[32]은 가까운 장래에 그러한 비용을 쉽게 감소시켜야 한다. 비용과 효율성에 대한 자세한 내용은 부록 B.3에 있다. 마지막으로 본 연구는 기성 LM을 사용하는 것에 초점을 맞추고 있으며, ToT 스타일의 높은 수준의 반사실적 의사 결정(예: 다음 토큰을 예측하는 대신 다음 단락에 대한 잠재적 선택을 심의하는 것)을 사용하여 LM을 미세 조정하면 LM의 문제 해결 능력을 향상시킬 수 있는 기회를 제공할 수 있다.

**결론.** LM의 연관 "시스템 1"은 문제에 대 한 솔루션에 대 한 가능한 경로의 트리를 검색 하는 것을 기반으로 "시스템 2"에 의해 유익 하 게 확장 될 수 있습니다. 사상의 나무 프레임워크는 문제 해결에 대한 고전적인 통찰력을 현대 LM을 위한 실행 가능한 방법으로 변환하는 방법을 제공한다. 동시에 LMs는 이러한 고전적 방법의 약점을 해결하여 창의적인 글쓰기와 같이 쉽게 형식화되지 않는 복잡한 문제를 해결할 수 있는 방법을 제공한다. 우리는 인공지능에 대한 고전적인 접근법과 LM의 교차점을 흥미로운 방향으로 본다.

### Broader Impact

ToT는 LMs가 보다 자율적이고 지능적으로 의사결정을 내리고 문제를 해결할 수 있도록 권한을 부여하는 프레임워크이다. 현재 작업은 추론 및 검색 문제로 제한되지만 외부 환경 또는 인간과의 상호 작용을 포함하는 미래 애플리케이션은 잠재적인 위험을 초래할 수 있으며, 예를 들어 LM의 유해한 사용을 촉진할 수 있다. 반면에 ToT는 암시적이고 낮은 수준의 토큰 값 대신 결과 표현이 읽을 수 있고 높은 수준의 언어 추론이기 때문에 모델 결정의 해석 가능성과 인간 정렬 기회를 향상시킨다.

### Acknowledgements

SY와 KN은 오라클 협력 연구상과 보조금 2239363호에 따른 국립 과학 재단의 지원을 인정한다. 이 자료에서 표현된 의견, 결과, 결론 또는 권장 사항은 저자의 것이며 국립 과학 재단의 견해를 반드시 반영하는 것은 아니다. SY는 또한 해롤드 W에 의해 지원된다. 프린스턴에서 온 도즈 펠로우십

## References

* [1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [2] C. Browne, E. J. Powley, D. Whitehouse, S. M. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener, D. P. Liebana, S. Samothrakis, and S. Colton. A survey of monte carlo tree search methods. _IEEE Transactions on Computational Intelligence and AI in Games_, 4:1-43, 2012.
* [3] M. Campbell, A. J. Hoane Jr, and F.-h. Hsu. Deep blue. _Artificial intelligence_, 134(1-2):57-83, 2002.
* [4] X. Chen, M. Lin, N. Scharli, and D. Zhou. Teaching large language models to self-debug, 2023.
* [5] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.
* [6] A. Creswell and M. Shanahan. Faithful reasoning using large language models. _arXiv preprint arXiv:2208.14271_, 2022.
* [7] N. D. Daw, Y. Niv, and P. Dayan. Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control. _Nature neuroscience_, 8(12):1704-1711, 2005.
* [8] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. Pal: Program-aided language models, 2023.
* [9] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu. Reasoning with language model is planning with world model. _arXiv preprint arXiv:2305.14992_, 2023.
* [10] P. E. Hart, N. J. Nilsson, and B. Raphael. A formal basis for the heuristic determination of minimum cost paths. _IEEE Transactions on Systems Science and Cybernetics_, 4(2):100-107, 1968. doi: 10.1109/TSSC.1968.300136.
* [11] P. E. Hart, N. J. Nilsson, and B. Raphael. A formal basis for the heuristic determination of minimum cost paths. _IEEE transactions on Systems Science and Cybernetics_, 4(2):100-107, 1968.
* [12] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents, 2022.
* [13] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. _arXiv preprint arXiv:2207.05608_, 2022.

* [14] J. Jung, L. Qin, S. Welleck, F. Brahman, C. Bhagavatula, R. L. Bras, and Y. Choi. Maieutic prompting: Logically consistent reasoning with recursive explanations. _arXiv preprint arXiv:2205.11822_, 2022.
* [15] D. Kahneman. _Thinking, fast and slow_. Macmillan, 2011.
* [16] D. Kahneman, S. Frederick, et al. Representativeness revisited: Attribute substitution in intuitive judgment. _Heuristics and biases: The psychology of intuitive judgment_, 49(49-81):74, 2002.
* [17] G. Kim, P. Baldi, and S. McAleer. Language models can solve computer tasks, 2023.
* [18] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone. Llm+p: Empowering large language models with optimal planning proficiency, 2023.
* [19] X. Lu, S. Welleck, P. West, L. Jiang, J. Kasai, D. Khashabi, R. L. Bras, L. Qin, Y. Yu, R. Zellers, N. A. Smith, and Y. Choi. Neurologic a*esque decoding: Constrained text generation with lookahead heuristics. In _North American Chapter of the Association for Computational Linguistics_, 2021.
* [20] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Welleck, B. P. Majumder, S. Gupta, A. Yazdanbakhsh, and P. Clark. Self-refine: Iterative refinement with self-feedback, 2023.
* [21] A. Newell, J. C. Shaw, and H. A. Simon. Report on a general problem solving program. In _IFIP congress_, volume 256, page 64. Pittsburgh, PA, 1959.
* [22] A. Newell, H. A. Simon, et al. _Human problem solving_. Prentice-Hall, 1972.
* [23] OpenAI. Gpt-4 technical report. _ArXiv_, abs/2303.08774, 2023.
* [24] D. Paul, M. Ismayilzada, M. Peyrard, B. Borges, A. Bosselut, R. West, and B. Faltings. Refiner: Reasoning feedback on intermediate representations, 2023.
* [25] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by generative pre-training. _OpenAI blog_, 2018.
* [26] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* [27] I. Schlag, S. Sukhbaatar, A. Celikyilmaz, W. tau Yih, J. Weston, J. Schmidhuber, and X. Li. Large language model programs, 2023.
* [28] N. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection, 2023.
* [29] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. _nature_, 550 (7676):354-359, 2017.
* [30] S. A. Sloman. The empirical case for two systems of reasoning. _Psychological bulletin_, 119(1): 3, 1996.
* [31] K. E. Stanovich. _Who is rational? Studies of individual differences in reasoning_. Psychology Press, 1999.
* [32] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [33] S. Verma, J. Fu, S. Yang, and S. Levine. Chai: A chatbot ai for task-oriented dialogue with offline reinforcement learning. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 4471-4491, 2022.

* Wallace et al. [2022] E. Wallace, N. Tomlin, A. Xu, K. Yang, E. Pathak, M. Ginsberg, and D. Klein. Automated crossword solving. _arXiv preprint arXiv:2205.09665_, 2022.
* Wang et al. [2023] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K.-W. Lee, and E.-P. Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models, 2023.
* Wang et al. [2022] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. _arXiv preprint arXiv:2203.11171_, 2022.
* Wang et al. [2023] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents, 2023.
* Wei et al. [2022] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint arXiv:2201.11903_, 2022.
* Xie et al. [2023] Y. Xie, K. Kawaguchi, Y. Zhao, X. Zhao, M.-Y. Kan, J. He, and Q. Xie. Decomposition enhances reasoning via self-evaluation guided decoding, 2023.
* Yang et al. [2023] S. Yang, O. Nachum, Y. Du, J. Wei, P. Abbeel, and D. Schuurmans. Foundation models for decision making: Problems, methods, and opportunities, 2023.
* Yao et al. [2022] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing reasoning and acting in language models. _arXiv preprint arXiv:2210.03629_, 2022.
* Zhang et al. [2023] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan. Planning with large language models for code generation. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=Lr8c0UtYbRL](https://openreview.net/forum?id=Lr8c0UtYbRL).
* Zhou et al. [2022] D. Zhou, N. Scharli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet, Q. Le, et al. Least-to-most prompting enables complex reasoning in large language models. _arXiv preprint arXiv:2205.10625_, 2022.
* Zhu et al. [2022] X. Zhu, J. Wang, L. Zhang, Y. Zhang, R. Gan, J. Zhang, and Y. Yang. Solving math word problem via cooperative reasoning induced language models. _arXiv preprint arXiv:2210.16257_, 2022.

코드, 프롬프트, 궤적

모든 코드는 [https://github.com/princeton-nlp/tree-of-thought-llm](https://github.com/princeton-nlp/tree-of-thought-llm)에서 사용할 수 있습니다.

모든 프롬프트는 [https://github.com/princeton-nlp/tree-of-thought-llm/tree/master/src/tot/prompts](https://github.com/princeton-nlp/tree-of-thought-llm/tree/master/src/tot/prompts)에서 사용할 수 있습니다.

궤적은 [https://github.com/princeton-nlp/tree-of-thought-llm/tree/master/logs](https://github.com/princeton-nlp/tree-of-thought-llm/tree/master/logs)에서 사용할 수 있습니다.

## 부록 B 추가 실험 결과

언어 모델의 능력 프론티어를 탐색하고 확장하는 동기를 감안할 때, 본 논문의 실험은 최첨단 언어 모델(GPT-4)과의 설정과 이에 도전하기 위해 고안된 세 가지 어려운 작업에 초점을 맞추었다. 여기서는 LLM이 약하거나 더 쉬운 작업에 대한 추가 실험을 보고하고 비용과 효율성에 대해 논의한다.

### Zero-shot ToT를 사용 하 여 새 작업 (GSM8k, StrategyQA)으로 확장

더 일반적인 NLP 작업은 GPT-4에 너무 쉽고 ToT가 필요하지 않을 수 있지만(이것이 우리가 더 어려운 새로운 작업을 고려했던 이유이다), ToT를 새로운 작업에 적용하는 것이 간단할 수 있다고 믿는다. 예를 들어, GSM8K 및 StrategyQA에 대한 창의적 쓰기(샘플 5 문제 해결 전략을 기반으로 한 샘플 5 솔루션과 가장 좋은 솔루션에 투표)와 유사한 단순하고 일반적인 제로 샷 ToT-BFS를 구현했으며 추가 코드 라인이 거의 없다.

# 새 작업의 답변 형식 정의

gsm8k_format = ''the answer is n'' where n is the number'

strategyqa_format = ''either ''the answer is yes'' or ''the answer is no''

# define zero-shot io prompting

standard_prompt = ''Answer the following question with {format}: {input}'

# define thought format for zero-shot cot and zero-shot tot

cot_prompt = ''Answer the following question: {input}

전략을 세우고 글을 쓰세요. 출력 형식이 다음 형식이어야 합니다.

Strategy:

질문에 답하는 방법에 대한 당신의 전략.

Answer:

질문에 대한 너의 대답. {format}로 끝나야 한다.

''

# define zero-shot voting used for zero-shot tot

vote_prompt = '''' 명령어 및 몇 가지 선택이 주어지면,

어떤 선택이 가장 유망한지 결정하다.

각 선택사항을 자세히 분석한 다음 마지막 줄에서 마무리

"최상의 선택은 {s}", 여기서 s는 선택의 정수 id이다.

''

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & **GSM8K** & **StrategyQA** & & **GPT-4** & **GPT-3.5** & & **GPT-4** & **GPT-3.5** \\ \hline IO & 51 & 73 & IO & 7.3\% & 6\% & IO & 6.19 & 4.47 \\ CoT & 86 & 82 & CoT & 4.0\% & 3\% & CoT & 6.93 & 5.16 \\ ToT & **90** & **83** & ToT & **74\%** & **19\%** & ToT & **7.56** & **6.62** \\ \hline \hline \end{tabular}
\end{table}
표 4: 제로 샷 ToT 및 GPT-4를 갖는 새로운 작업.

100개의 무작위 GSM8K 테스트 및 StrategyQA 개발 질문의 하위 집합에 대해 평가했다. 표 4 및 예상된 바와 같이, ToT는 두 태스크 모두에서 CoT보다 개선된다(그러나 약간만, GPT-4 + CoT가 이미 그러한 태스크에서 매우 우수하고, StrategyQA의 병목 현상은 추론이 아닌 외부 지식이다). 계산 비용을 고려할 때 전통적인 NLP 작업의 경우 더 작은 LLMs + ToT를 시도하거나 GPT-4 + CoT의 추론에 도전하는 하드 작업의 경우 GPT-4 + ToT를 시도하는 것이 더 적합하다.

### 새 LMs(GPT-3.5)로 확장

ToT가 다른 LLM과 어떻게 작동하는지 이해하기 위해 GPT-3.5-turbo for Creative Writing(표 6)과 Game of 24(표 5)도 실행했다. 두 작업 모두에서 "ToT \(>\) CoT \(>\) IO"는 GPT-3.5에 대해 여전히 유효하다. 크리에이티브 글쓰기에서는 GPT-3.5+ToT가 GPT-4+IO를 능가하며, GPT-4+CoT와 유사하여 ToT가 더 약한 언어 모델에서도 잘 작동할 수 있음을 시사한다.

24게임에서는 GPT-3.5+ToT의 19%가 GPT-4+ToT의 74%보다 훨씬 나쁘다. 세대 대 세대의 중요성을 더 이해하기 위해. 평가는 GPT-4 세대 + GPT-3.5 평가(64%)와 GPT-3.5 세대 + GPT-4 평가(31%)를 실시하였다. 이는 게임의 병목 현상이 사고 생성이며, 다양한 생성/평가 언어 모델이 비용을 절감하면서 적절한 결과를 얻을 수 있음을 시사한다.

### 비용 및 효율성

ToT를 실행하려면 IO 또는 CoT 프롬프트보다 훨씬 더 많은 계산이 필요합니다. 예를 들어, 24의 게임(아래 표 7)에서 ToT로 문제를 해결하려면 5.5k 완료 토큰이 필요하며, 이는 100 CoT 시행(6.7k 토큰)에 가깝다. 그러나 ToT의 성능은 100개의 독립적인 CoT 시험보다 우수하다.

크리에이티브 글쓰기(아래 표 8)에서 ToT가 약 5배 완료 토큰과 화폐 비용을 취한다는 것을 발견했는데, 이는 \(b=5\)으로 직관적이며 대부분의 토큰은 생성된 통로이다.

따라서 게임 24와 크리에이티브 글쓰기의 주요 ToT 실험을 완료하는 비용은 약 \(0.74\times 100+0.32\times 100=106\)이다. 교차 단어의 DFS 실험도 \(100\) 달러 이내여야 합니다. 일반적으로 ToT의 비용 및 효율성은 사용되는 프롬프트 및 검색 알고리즘에 크게 의존하며 CoT보다 5-100배 더 많은 생성된 토큰을 요구할 수 있다. 몇 가지 실행 가능한 통찰력입니다.

* CoT가 고군분투하는 의도적인 추론이 필요한 작업에 ToT를 사용하는 것이 좋습니다.
* ToT의 유연성은 BFS에서 빔 크기 또는 투표 수를 변경 하는 것과 같은 몇 가지 성능-비용 절충을 허용 합니다. 0샷 프롬프트, GPT-3.5 vs. GPT-4 등. 일부 리소스 제약 사항 또는 성능 목표에 따라 설정을 구성할 수 있습니다.
* 효율성을 개선하기 위한 많은 공간이 있습니다. 예를 들어 BFS는 솔루션이 발견되면 조기에 중지하거나 일부 생각이 "불가능"할 때까지 빔 크기를 트리밍할 수 있습니다.
* 모델이 더 강력한 인텔리전스를 달성하려면 실제로 더 많은 계산이 필요하며, 이는 장기적으로와 같이 차단 문제가 되지 않아야 하며, (오픈 소스) LM은 훨씬 저렴하고 효율적일 것이라고 믿습니다. 그것은 또한 사고 생성 및/또는 평가를 위해 LM을 더 잘 훈련/피네튜닝하는 좋은 방향이다.

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**Game of 24** & **Generate/Prompt tokens** & **Cost per case** & **Success** \\ \hline IO (best of 100) & 1.8k / 1.0k & $0.13 & 33\% \\ CoT (best of 100) & 6.7k / 2.2k & $0.47 & 49\% \\ ToT & 5.5k / 1.4k & $0.74 & 74\% \\ \hline \hline \end{tabular}
\end{table}
표 7: 24의 게임에 대한 비용 분석.

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Creative Writing** & **Generate/Prompt tokens** & **Cost per case** \\ \hline IO & 0.9k / 0.4k & $0.06 \\ CoT & 0.9k / 0.4k & $0.07 \\ ToT & 4k / 2.9k & $0.32 \\ \hline \hline \end{tabular}
\end{table}
표 8: 24의 게임에 대한 비용 분석.
