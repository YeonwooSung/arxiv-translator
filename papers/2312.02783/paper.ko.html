<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 그래프에 대한 대용량 언어 모델: 종합 설문 조사\n' +
      '\n' +
      '보웬진({}^{\\star}\\), 강류({}^{\\star}\\), 치한({}^{\\star}\\), 멍장, 행지, 지아웨이한\n' +
      '\n' +
      '\\({}^{\\star}\\)_처음 세 명의 저자는 이 작업에 동등하게 기여했다. Bowen Jin, Chi Han, Heng Ji, Jiawei Han: University of Illinois at Urbrun-Champaign. {bowen4, Chihan3, hengji, hanr}@illinois.edu Gang Liu, Meng Jiang: University of Notre Dame. {gliiu7, mjiang2@}@nd.edu_\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'ChatGPT 및 LLMa와 같은 대형 언어 모델(LLM)은 강력한 텍스트 인코딩/디코딩 능력과 새로 발견된 창발적 능력(_e.g._, 추론)으로 인해 자연어 처리에서 상당한 발전을 이루고 있다. LLM은 주로 순수한 텍스트를 처리하도록 설계되지만, 텍스트 데이터가 그래프(_e.g._, 학술 네트워크 및 전자 상거래 네트워크) 형태의 풍부한 구조 정보와 연관되는 많은 실제 시나리오 또는 그래프 데이터가 풍부한 텍스트 정보와 쌍을 이루는 시나리오(_e.g._, 설명이 있는 분자)가 있다. 또한, LLM은 순수한 텍스트 기반 추론 능력을 보여주었지만 그러한 능력이 그래프 시나리오(_i.e._, 그래프 기반 추론)로 일반화될 수 있는지 여부는 불분명하다. 본 논문에서는 그래프에 대한 대규모 언어 모델과 관련된 시나리오 및 기술에 대한 체계적인 검토를 제공한다. 먼저 그래프에 LLM을 채택하는 잠재적 시나리오를 순수 그래프, 텍스트가 풍부한 그래프 및 텍스트 쌍을 이루는 그래프의 세 가지 범주로 요약한다. 그런 다음 LLM을 예측기로, LLM을 인코더로, LLM을 얼라이너로 포함한 그래프에서 LLM을 활용하는 세부 기술에 대해 논의하고 다양한 모델 학교의 장단점을 비교한다. 또한, 이러한 방법의 실제 응용에 대해 언급하고 오픈 소스 코드와 벤치마크 데이터 세트를 요약한다. 마지막으로, 이 빠르게 성장하는 분야의 잠재적인 미래 연구 방향을 결론짓는다. 관련 원본은 [https://github.com/PeterGriffin.Jin/Awesome-Language-Model-on-Graphs](https://github.com/PeterGriffin.Jin/Awesome-Language-Model-on-Graphs)에서 찾을 수 있습니다.\n' +
      '\n' +
      ' 대용량 언어 모델, 그래프 신경망, 자연어 처리, 그래프 표현 학습\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '초대형 텍스트 코퍼스에서 사전 훈련된 대용량 언어 모델(LLM)(_e.g._, BERT[22], T5[30], LLMa[119])은 질의 응답[1], 텍스트 생성[2], 문서 이해[3]를 포함하는 자연어 처리(NLP) 태스크를 해결하는 데 매우 강력한 것으로 입증되었다. 초기의 LLMs(_e.g._, BERT[22], RoBERTa[23])는 인코더 전용 아키텍처를 채택하여 주로 텍스트 표현 학습[4] 및 자연어 이해[3]에 적용된다. 최근, 디코더 전용 아키텍처[119] 또는 인코더-디코더 아키텍처[30]에 더 많은 초점이 주어지고 있다. 모델 크기가 증가함에 따라 이러한 LLM은 추론 능력과 훨씬 더 진보된 창발 능력을 보여 인공 지능(AGI)의 강력한 잠재력을 노출시켰다.\n' +
      '\n' +
      '순수한 텍스트를 처리하기 위해 LLM이 광범위하게 적용되는 반면, 텍스트 데이터가 그래프 형태로 표현되는 구조 정보와 연관되는 애플리케이션이 증가하고 있다. 도면에 제시된 바와 같이. 1, 학술 네트워크에서 논문(제목과 설명이 있는)과 저자(프로필 텍스트가 있는)는 저자 관계와 상호 연결되어 있다. 이러한 그래프에 대한 저자/논문의 텍스트 정보와 저자-논문의 구조 정보를 모두 이해하는 것은 진보된 저자/논문의 모델링 및 협업을 위한 정확한 추천에 기여할 수 있다; 과학 영역에서 분자는 그래프로 표현되고 종종 그들의 기본 정보(_e.g._, 독성)를 기술하는 텍스트와 쌍을 이룬다. 분자 구조(그래프)와 관련 풍부한 지식(텍스트) 모두의 공동 모델링은 더 깊은 분자 이해를 위해 중요하다. LLM은 주로 순차적인 방식으로 놓여 있는 텍스트를 모델링하기 위해 제안되기 때문에 위에서 언급한 시나리오는 포즈를 취한다.\n' +
      '\n' +
      '도. 1: 그래프와 텍스트의 관계에 따라, 그래프 시나리오 상에서 세 개의 LLM을 분류한다. LLM의 역할에 따라 3가지 LLM 온 그래프 기술을 요약한다. 예측자로서의 LLM은 LLM이 최종 답을 예측할 책임이 있는 곳이다. ‘LLM as Aligner’는 입력-출력 쌍들을 GNN들의 쌍들과 정렬할 것이다. \'LLM as Encoder\'는 특징 벡터를 인코딩하고 얻기 위해 LLM을 사용하는 것을 말한다.\n' +
      '\n' +
      'LLM이 그래프의 구조 정보를 인코딩할 수 있도록 하는 방법에 대한 새로운 과제입니다. 또한, LLM은 뛰어난 텍스트 기반 추론 능력을 입증했기 때문에 순수한 그래프에서 근본적인 그래프 추론 문제를 해결할 가능성이 있는지 여부를 탐구할 가능성이 있다. 이러한 그래프 추론 작업에는 연결성 추론[6], 최단 경로[7], 서브 그래프 매칭[8]이 포함된다.\n' +
      '\n' +
      '최근, 그래프 기반 애플리케이션(도 1에서 요약됨)을 위해 LLM을 확장하는 것에 대한 관심이 증가하고 있다. 그림에 제시된 그래프와 텍스트의 관계에 따라. 1, 애플리케이션 시나리오는 순수 그래프, 텍스트가 풍부한 그래프 및 텍스트 쌍을 이루는 그래프로 분류할 수 있습니다. LLM의 역할과 그래프 신경망(GNN)과의 상호 작용에 따라, 그래프 기법 상의 LLM은 LLM을 태스크 예측기로서 처리하는 것(LLM as Predictor), LLM을 GNN의 특징 인코더로서 처리하는 것(LLM as Encoder), LLM을 GNN과 정렬하는 것(LLM as Aligner)으로 분류될 수 있다.\n' +
      '\n' +
      'LLM과 그래프의 교차점을 탐색하는 기존 조사는 한정적이다. 그래프에 대한 딥 러닝과 관련하여, Wu 등[17]은 순환 그래프 신경망, 콘볼루션 그래프 신경망, 그래프 오토인코더 및 공간-시간 그래프 신경망에 대한 상세한 설명과 함께 그래프 신경망(GNN)의 포괄적인 개요를 제공한다. Liu 등[18]은 그들의 백본 아키텍처들, 사전 트레이닝 방법들, 및 적응 기법들을 포함하는 그래프들 상에서 사전 트레이닝된 기초 모델들을 논의한다. Pan 등[19]은 특히 KG들이 어떻게 LLM 트레이닝 및 추론을 향상시킬 수 있는지, 그리고 LLM들이 어떻게 KG 구성 및 추론을 용이하게 할 수 있는지에 대해 LLM들과 지식 그래프들(KG들) 사이의 연결을 검토한다. 요약하면, 기존 조사는 LLM보다 GNN에 더 초점을 맞추거나 그림 1과 같이 다양한 그래프 시나리오에서 적용에 대한 체계적인 관점을 제공하지 못한다. 본 논문은 빠르게 발전하는 이 분야에 진출하려는 컴퓨터 과학 및 기계 학습 커뮤니티 외에도 다양한 배경의 광범위한 연구자를 위해 그래프에 대한 LLM에 대한 포괄적인 검토를 제공한다.\n' +
      '\n' +
      '**우리의 기여** 본 논문의 주목할 만한 기여는 다음과 같이 요약됩니다.\n' +
      '\n' +
      '* **그래프 시나리오의 범주화.** 언어 모델을 채택할 수 있는 그래프 시나리오를 순수 그래프, 텍스트가 풍부한 그래프 및 텍스트 쌍을 이루는 그래프로 체계적으로 요약합니다.\n' +
      '* **기술에 대한 체계적인 검토.** 그래프 기술에 대한 언어 모델에 대한 가장 포괄적인 개요를 제공합니다. 다양한 그래프 시나리오에 대해 대표 모델을 요약하고 각각에 대한 자세한 설명을 제공하고 필요한 비교를 한다.\n' +
      '* **풍부한 리소스.** 벤치마크 데이터 세트, 오픈 소스 코드 베이스 및 실제 응용 프로그램을 포함 하 여 그래프의 언어 모델에 대 한 풍부한 리소스를 수집 합니다.\n' +
      '* **미래 방향.** 그래프에서 언어 모델의 기본 원칙을 조사하고 향후 탐색을 위한 6가지 예상 경로를 제안합니다.\n' +
      '\n' +
      '**조사 조직.** 이 조사의 나머지는 다음과 같이 구성됩니다. 섹션 2에서는 LLM과 GNN의 배경을 소개하고 일반적으로 사용되는 표기법을 나열하고 관련 개념을 정의한다. 섹션 3에서는 LLM을 채택할 수 있는 그래프 시나리오를 분류하고 그래프 기법에서 LLM을 요약한다. 섹션 4-6은 다양한 그래프 시나리오에 대한 LLM 방법론에 대한 자세한 설명을 제공한다. 섹션 7은 다양한 도메인에 걸쳐 사용 가능한 데이터 세트, 오픈 소스 코드 베이스 및 애플리케이션 컬렉션을 제공합니다. 섹션 8에서는 몇 가지 잠재적인 미래 방향을 소개한다. 섹션 9는 논문을 요약한다.\n' +
      '\n' +
      '## 2 Definition & Background\n' +
      '\n' +
      '### _Definitions_\n' +
      '\n' +
      '본 절에서는 다양한 형태의 그래프에 대한 정의를 제시하고 본 논문에서 사용한 표기법(표 Ⅰ)을 소개한다.\n' +
      '\n' +
      '_정의 1(Graph):_ 그래프는 \\(\\mathcal{G}=(\\mathcal{V},\\mathcal{E})\\)로 정의될 수 있다). 여기서 \\(\\mathcal{V}\\)은 노드들의 집합을 의미하고, \\(E\\)은 에지들의 집합을 의미한다. 특정 노드는 \\(v_{i}\\in\\mathcal{V}\\)으로 표현될 수 있고, 노드 \\(v_{j}\\)에서 \\(v_{i}\\)으로 향하는 에지는 \\(e_{ij}=(v_{i},v_{j})\\in\\mathcal{E}\\)으로 표현될 수 있다. 특정 노드 \\(v\\)에 인접한 노드 집합은 \\(N(v)=\\{u\\in\\mathcal{V}|(v,u)\\in\\mathcal{E}\\}\\)로 표현된다.\n' +
      '\n' +
      '노드 유형 집합 \\(\\mathcal{A}\\) 및 에지 유형 집합 \\(\\mathcal{R}\\)을 포함하는 그래프입니다. 여기서 \\(|\\mathcal{A}|+|\\mathcal{R}|>2\\)을 _이질 그래프_라고 합니다. 이질 그래프는 노드 유형 매핑 함수 \\(\\phi:\\mathcal{V}\\rightarrow\\mathcal{A}\\)와 에지 유형 매핑 함수 \\(\\psi:\\mathcal{E}\\rightarrow\\mathcal{R}\\)과도 연결된다.\n' +
      '\n' +
      '_정의 2(노드-레벨 텍스트 정보를 갖는 그래프):_노드-레벨 텍스트 정보를 갖는 그래프는 \\(\\mathcal{G}=(\\mathcal{V},\\mathcal{E},\\mathcal{D})\\), 여기서 \\(\\mathcal{V}\\), \\(\\mathcal{E}\\) 및 \\(\\mathcal{D}\\)은 각각 노드 세트, 에지 세트 및 텍스트 세트이다. 각 \\(v_{i}\\in\\mathcal{V}\\)은 일부와 연결됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Notations** & **Descriptions** \\\\ \\hline \\(|\\cdot|\\) & The length of a set. \\\\ \\hline \\(|\\mathbf{A},\\mathbf{B}|\\) & The concatenation of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\). \\\\ \\hline \\(\\|\\) & Concatenate operation. \\\\ \\hline \\(\\mathcal{G}\\) & A graph. \\\\ \\hline \\(\\mathcal{V}\\) & The set of nodes in a graph. \\\\ \\hline \\(v\\) & A node \\(v\\in\\mathcal{V}\\). \\\\ \\hline \\(\\mathcal{E}\\) & The set of edges in a graph. \\\\ \\hline \\(e\\) & An edge \\(e\\in\\mathcal{E}\\). \\\\ \\hline \\(\\mathcal{G}_{v}\\) & The ego graph associated with \\(v\\) in \\(\\mathcal{G}\\). \\\\ \\hline \\(N(v)\\) & The neighbors of a node \\(v\\). \\\\ \\hline \\(M\\) & A meta-path or a meta-graph. \\\\ \\hline \\(N_{M}(v)\\) & The nodes which are reachable from \\\\  & node \\(v\\) with meta-path or meta-graph \\(M\\). \\\\ \\hline \\(\\mathcal{D}\\) & The text set. \\\\ \\hline \\(s\\in\\mathcal{S}\\) & The text token in a text sentence \\(\\mathcal{S}\\). \\\\ \\hline \\(d_{v_{i}}\\) & The text associated with the node \\(v_{i}\\). \\\\ \\hline \\(d_{e_{ij}}\\) & The text associated with the edge \\(e_{ij}\\). \\\\ \\hline \\(d_{Q}\\) & The text associated with the graph \\(\\mathcal{G}\\). \\\\ \\hline \\(n\\) & The number of nodes, \\(n=|\\mathcal{V}|\\). \\\\ \\hline \\(b\\) & The dimension of a node hidden state. \\\\ \\hline \\(\\mathbf{x}_{v_{i}}\\in\\mathbf{R}^{d}\\) & The initial feature vector of the node \\(v_{i}\\). \\\\ \\hline \\(\\mathbf{H}_{v}\\in\\mathbf{R}^{n\\times b}\\) & The node hidden feature matrix. \\\\ \\hline \\(\\mathbf{h}_{v_{i}}\\in\\mathbf{R}^{b}\\) & The hidden representation of node \\(v_{i}\\). \\\\ \\hline \\(\\mathbf{h}_{Q}\\in\\mathbf{R}^{b}\\) & The hidden representation of a graph \\(\\mathcal{G}\\). \\\\ \\hline \\(\\mathbf{h}_{d_{v}}\\in\\mathbf{R}^{b}\\) & The representation of text \\(d_{v}\\). \\\\ \\hline \\(\\mathbf{H}_{d_{v}}\\in\\mathbf{R}^{d_{v}|\\times b}\\) & The hidden states of tokens in \\(d_{v}\\). \\\\ \\hline \\(\\mathbf{W},\\mathbf{\\Theta},w,\\theta\\) & Learnable model parameters. \\\\ \\hline LLM\\((\\cdot)\\) & Large Language model. \\\\ \\hline GNN\\((\\cdot)\\) & Graph neural network. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE I: Notations of Concepts.\n' +
      '\n' +
      '텍스트 정보 \\(d_{v_{i}}\\in\\mathcal{D}\\) 예를 들어, 학술 인용 네트워크에서는 \\(v\\in\\mathcal{V}\\)을 학술논문으로, \\(e\\in\\mathcal{E}\\)을 학술논문의 인용 연결로, \\(d\\in\\mathcal{D}\\)을 학술논문의 텍스트 내용으로 해석할 수 있다. 노드 레벨의 텍스트 정보를 갖는 그래프는 텍스트 풍부 그래프[32], 텍스트 분산 그래프[62], 또는 텍스트 그래프[73]로도 불린다.\n' +
      '\n' +
      '**정의 3** (에지 수준 텍스트 정보가 있는 그래프): _노드 수준 텍스트 정보가 있는 그래프는 \\(\\mathcal{G}=(\\mathcal{V},\\mathcal{E},\\mathcal{D})\\), 여기서 \\(\\mathcal{V}\\), \\(\\mathcal{E}\\) 및 \\(\\mathcal{D}\\)는 각각 노드 집합, 에지 집합 및 텍스트 집합입니다. 각 \\(e_{ij}\\in\\mathcal{E}\\)은 일부 텍스트 정보와 연관되어 있다 \\(d_{e_{ij}\\in\\mathcal{D}\\) 예를 들어 소셜 네트워크에서 \\(v\\in\\mathcal{V}\\)을 사용자로, \\(e\\in\\mathcal{E}\\)을 사용자 간의 상호 작용으로, \\(d\\in\\mathcal{D}\\)을 사용자 간에 보낸 메시지의 텍스트 내용으로 해석할 수 있습니다._\n' +
      '\n' +
      '**정의 4** (그래프 수준 텍스트 정보가 있는 그래프): _그래프 수준 텍스트 정보가 있는 그래프 데이터 개체는 쌍 \\((\\mathcal{G},d_{\\mathcal{G}})\\)으로 표시될 수 있습니다. 여기서 \\(\\mathcal{G}=(\\mathcal{V},\\mathcal{E})\\). \\ (\\mathcal{V}\\) 및 \\(\\mathcal{E}\\)는 노드 집합 및 에지 집합입니다. \\ (d_{\\mathcal{G}}\\)는 그래프 \\(\\mathcal{G}\\)에 쌍을 이루는 텍스트 집합입니다. 예를 들어, 분자 그래프에서 \\(\\mathcal{G}\\), \\(v\\in\\mathcal{V}\\)은 원자를 나타내고, \\(e\\in\\mathcal{E}\\)은 분자를 하나로 묶는 강한 인력 또는 화학적 결합을 나타내며, \\(d_{\\mathcal{G}}\\)은 분자의 텍스트 설명을 나타낸다._\n' +
      '\n' +
      '### _Background_\n' +
      '\n' +
      '**(대형) 언어 모델.** 언어 모델(LMs) 또는 언어 모델링은 텍스트 배포에서 이해 및 생성에 대한 자연어 처리(NLP) 분야입니다. 최근, 대형 언어 모델(LLM)은 기계 번역, 텍스트 요약, 및 질의 응답과 같은 태스크에서 인상적인 능력을 입증했다[110, 111, 25, 44, 112, 113].\n' +
      '\n' +
      '대형 언어 모델은 시간이 지남에 따라 크게 발전했습니다. 초기에, Word2Vec[114] 및 GloVe[115]와 같은 단어 벡터는 의미적으로 유사한 단어들이 서로 가깝게 매핑된 연속 벡터 공간 내의 단어들을 나타낸다. 이러한 임베딩(\\mathbf{w}\\in\\mathbb{R}^{d}\\)은 일반적으로 스킵그램의 조건부 단어 확률이나 CBOW(Continuous bag-of-words)와 같은 말뭉치의 단어 수준 상관 관계를 모델링하며, 단어 유추 및 단어 유사도와 같은 예비 작업에 유용하다.\n' +
      '\n' +
      'BERT[22]의 등장은 언어 모델링과 표현의 상당한 진전을 보여준다. BERT는 양방향 문맥이 주어진 단어의 조건부 확률을 모델링하며, MMLM(Masked Language Modeling) 목적이라고도 한다. 그것의 트레이닝은 문장에서 단어들의 랜덤 서브세트를 마스킹하고 나머지 단어들에 기초하여 단어들을 예측하는 것을 포함한다. 직관적인 이해를 위해, 하나의 마스킹된 단어만을 갖는 경우로 단순화하면, 목적은 다음 식에 대응한다:\n' +
      '\n' +
      '\\[\\mathbb{E}_{\\mathcal{S}\\sim\\mathcal{D}}\\left[\\sum_{s_{i}\\in\\mathcal{S}}\\log p (s_{i}|s_{1},\\ldots,s_{i-1},s_{i+1},\\ldots,s_{N_{\\mathcal{S}}})\\right], \\tag{1}\\\n' +
      '\n' +
      '여기서, \\(\\mathcal{S}\\)은 말뭉치에서 샘플링된 문장 \\(\\mathcal{D}\\), \\(s_{i}\\)은 문장 내 \\(i\\)번째 단어, \\(N_{\\mathcal{S}}\\)은 문장의 길이이다. BERT는 주목 메커니즘과 함께 트랜스포머 아키텍처를 핵심 빌딩 블록으로 활용한다. 바닐라 트랜스포머에서, 주의 메커니즘은 다음과 같이 정의된다:\n' +
      '\n' +
      '\\[\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_{k}}}\\right)V, \\tag{2}\\]\n' +
      '\n' +
      '여기서 \\(Q,K,V\\in\\mathbb{R}^{N_{\\mathcal{S}}\\times d_{k}}\\)은 문장의 각 단어에 대한 질의, 키 및 값 벡터이다. 주의 메커니즘은 유연한 방식으로 문장에서 단어 간의 의존성을 포착하도록 설계되었으며, 그래프와 같은 다른 입력 형식과 결합하는 데에도 잠재적으로 유용한 이점이 있다. BERT는 입력 텍스트 \\(\\mathbf{h}_{\\mathcal{S}}\\in\\mathbb{R}^{d}\\)의 표현을 마지막 계층에서 출력하는 텍스트 표현 모델로 유용하다. BERT에 이어서, 텍스트 표현의 유사한 아키텍처 및 목적을 갖는 RoBERTa[23], ALBERT[116], 및 ELECTRA[117]와 같은 많은 다른 마스킹된 언어 모델이 제안된다. 이러한 유형의 모델은 사전 훈련된 언어 모델(PLM)이라고도 불린다.\n' +
      '\n' +
      '원본 트랜스포머 논문[94]이 기계 번역에 대해 실험되었지만 GPT-2[113]가 출시될 때까지 인과 언어 모델링(즉, 텍스트 생성)이 다운스트림 작업에 영향을 미쳤다. 인과 언어 모델링은 문장에서 이전 단어가 주어졌을 때 다음 단어를 예측하는 작업이다. 인과적 언어 모델링의 목적은 다음과 같이 정의된다.\n' +
      '\n' +
      '\\[\\mathbb{E}_{\\mathcal{S}\\sim\\mathcal{D}}\\left[\\sum_{s_{i}\\in\\mathcal{S}}\\log p (s_{i}|s_{1},\\ldots,s_{i-1})\\right]. \\tag{3}\\\n' +
      '\n' +
      '트랜스포머를 인과적 언어 모델링에 적용하기 위해 미래 단어에 대한 주의 가중치를 마스킹하는 인과적 주의를 소개한다. GPT-3 [25], GPT-4 [118], LLaMA [119], LLaMA [121], Mistral 7B [120], T5 [30]와 같은 단순하지만 강력한 후속 모델은 소수의 샷 학습, 연쇄 사고 추론 및 프로그래밍과 같은 인상적인 비상 기능을 보여준다. 언어 모델을 시각[97, 121] 및 생화학 구조[122, 48, 123]와 같은 다른 양식과 결합하려는 노력도 이루어졌다. 그래프와의 결합은 우리가 현재 논문에서 논의할 또 다른 흥미로운 주제이다.\n' +
      '\n' +
      'LLM에서 "크다"라는 단어는 언어 모델을 나누는 명확하고 정적인 임계값과 관련이 없다는 점을 지적하고자 한다. 우리가 "크다"라고 부르는 현재 모델은 결국 미래 모델에 비해 작게 나타날 것이며, 개발 당시에는 더 작은 모델이 많았다. "라지"는 실제로 언어 모델이 필연적으로 진화하는 방향을 가리킨다. 더 중요한 것은 최근 연구자들이 발견한 경험적 규칙을 나타낸다. 더 큰 기반 모델은 훨씬 더 많은 대표성과 일반화 능력을 보유하는 경향이 있다. 따라서 [19]에서 제안한 대로 BERT와 같은 중규모 PLM과 GPT-4와 같은 대규모 LM을 모두 포함하도록 LLM을 정의한다.\n' +
      '\n' +
      '**Graph Neural Networks & Graph Transformers.** GNN은 그래프 데이터에 대한 딥 러닝 아키텍처로 제안됩니다. GCN[85], GraphSAGE[86] 및 GAT[87]을 포함하는 일차 GNN들은 노드-레벨 태스크들을 해결하기 위해 설계된다. 이들은 주로 노드 표현을 획득하기 위해 전파-집계 패러다임을 채택한다:\n' +
      '\n' +
      '\\[\\mathbf{a}_{v_{i}v_{i}}^{(l-1)} =\\mathrm{PROP}^{(l)}\\left(\\mathbf{h}_{v_{i}}^{(l-1)},\\mathbf{h}_{v_{i}}^{(l-1)},\\mathbf{h}_{v_{i}}^{(l-1)}\\right),\\left(\\forall v_{j}\\in\\mathcal{N}(v_{i})\\right); \\tag{4}\\] \\[\\mathbf{h}_{v_{i}}^{(l)} =\\mathrm{AGG}^{(l)}\\left(\\mathbf{h}_{v_{i}}^{(l-1)},\\{\\mathbf{a}_{v_{i}v_{i}}^{(l-1)}|v_{j}\\in\\mathcal{N}(v_{i})\\}\\right). \\tag{5}\\]\n' +
      '\n' +
      'GNN[205]과 같은 이후의 작업들은 그래프-레벨 태스크들을 해결하기 위한 GNN들을 탐색한다. 노드 표현에 READOUT 함수를 채택 하 여 그래프 표현을 얻습니다.\n' +
      '\n' +
      '\\[\\mathbf{h}_{\\mathcal{G}}=\\text{READOUT}(\\{\\mathbf{h}_{v_{i}}|v_{i}\\in\\mathcal{G}\\}). \\tag{6}\\] READOUT 함수는 평균 풀링, 최대 풀링 등을 포함한다. GNN에 대한 후속 작업은 과평활[137], 과분할[138], 해석 가능성[147], 편향[143]의 문제를 다룬다. 메시지 전달 기반 GNN은 향상된 구조 인코딩 능력을 보여주었지만, 연구자들은 트랜스포머(_i.e._, 그래프 트랜스포머)를 사용하여 표현성을 더욱 향상시키는 방법을 모색하고 있다. 그래프 트랜스포머는 각 그래프 인코딩 계층의 수용 필드를 확장하기 위해 글로벌 멀티헤드 어텐션 메커니즘을 이용한다[141]. 그들은 그래프의 귀납적 편향들을 위치 인코딩, 구조적 인코딩, 메시지 전달 계층들과 주의 계층들의 조합, 또는 큰 그래프들 상에서 주의의 효율성을 개선함으로써 모델에 통합한다[142]. 그래프 트랜스포머는 많은 순수 그래프 문제에 대한 최첨단 솔루션으로 입증되었다. 우리는 GT의 가장 최근의 발전에 대해 독자들을 참조한다.\n' +
      '\n' +
      '**언어 모델 대** 그래프 트랜스포머.** 최신 언어 모델과 그래프 트랜스포머는 모두 트랜스포머 [94]를 기본 모델 아키텍처로 사용합니다. 이것은 특히 언어 모델이 그래프 응용 프로그램에서 채택될 때 두 개념을 구별하기 어렵게 만든다. 본 논문에서 "트랜스포머"는 일반적으로 단순화를 위해 Transformer 언어 모델을 말한다. 여기서, 우리는 그것들을 구별하는 데 도움이 되는 세 가지 점을 제공한다: 1) _Tokens_ (단어 토큰 대 노드 토큰): Transformer가 토큰 시퀀스를 입력으로 하는 언어 모델의 경우 토큰은 단어 토큰이고, 그래프 트랜스포머의 경우 토큰은 노드 토큰이다. 백본 트랜스포머가 텍스트 코퍼스에서 사전 훈련된 경우(_e.g._, BERT [22] 및 LLaMA [119])에는 토큰과 노드 토큰을 모두 포함하는 경우를 "언어 모델"이라고 부르기로 한다. 2) _Positional Encoding_ (서열 대 그래프): 언어 모델은 일반적으로 시퀀스 내 단어 토큰의 위치를 고려한 절대 또는 상대 위치 인코딩을 채택하고, 그래프 트랜스포머는 그래프 내 노드의 거리를 고려하기 위해 최단 경로 거리[141], 랜덤 워크 거리, 그래프의 고유값 Laplacian [142]을 채택한다. 3) _Goal_ (텍스트 대 그래프): 언어 모델은 원래 텍스트 인코딩 및 생성을 위해 제안되고, 그래프 트랜스포머는 노드 인코딩 또는 그래프 인코딩을 위해 제안되며, 백본 트랜스포머가 텍스트 코퍼스에서 사전 훈련된 경우 그래프에서 텍스트가 노드/엣지로 제공되는 경우를 "언어 모델"이라고 부르기로\n' +
      '\n' +
      '## 3 범주화 및 프레임워크\n' +
      '\n' +
      '이 섹션에서는 먼저 언어 모델을 채택할 수 있는 그래프 시나리오의 범주화를 소개한다. 그런 다음 그래프 기술에 대한 LLM의 범주화에 대해 논의한다. 마지막으로 그래프에 대한 언어 모델의 학습 및 추론 프레임워크를 요약한다.\n' +
      '\n' +
      '### _언어 모델을 사용 하 여 그래프 시나리오의 범주화_\n' +
      '\n' +
      '**텍스트 정보가 없는 순수 그래프** 는 텍스트 정보가 없거나 의미적으로 풍부한 텍스트 정보가 없는 그래프입니다. 그러한 그래프들의 예들은 트래픽 그래프들 및 전력 전송 그래프들을 포함한다. 이러한 그래프는 종종 큰 언어 모델의 그래프 추론 능력을 테스트하기 위한 컨텍스트 역할을 하거나(그래프 이론 문제를 해결하거나) 큰 언어 모델을 향상시키기 위한 지식 소스 역할을 한다(환각 완화).\n' +
      '\n' +
      '**텍스트 풍부 그래프** 는 노드 또는 에지가 의미적으로 풍부한 텍스트 정보와 연결된 그래프를 나타냅니다. 이러한 그래프는 텍스트 풍부 네트워크[32], 텍스트 분산 그래프[62], 텍스트 그래프[73] 또는 텍스트 에지 네트워크[75]라고도 한다. 실제 사례로는 학술 네트워크, 전자상거래 네트워크, 소셜 네트워크, 법률 사례 네트워크 등이 있다. 이러한 그래프들에서, 사람들은 텍스트 정보 및 구조 정보 둘 다를 갖는 노드들 또는 에지들에 대한 표현들을 학습하는 것에 관심이 있다[73][75].\n' +
      '\n' +
      '**텍스트 쌍 그래프** 는 전체 그래프 구조에서 텍스트 설명이 정의 된 그래프입니다. 이러한 그래프는 노드들이 원자를 나타내고 에지가 화학적 결합을 나타내는 분자 또는 단백질을 포함한다. 텍스트 설명은 분자 캡션 또는 단백질 텍스트 특징일 수 있다. 그래프 구조가 분자 특성에 영향을 미치는 가장 중요한 요소이지만 분자의 텍스트 설명은 분자를 이해하는 데 도움이 되는 보완 지식 소스 역할을 할 수 있다[148]. 그래프 시나리오는 그림 1에서 찾을 수 있다.\n' +
      '\n' +
      '### _Graph Techniques_ 에서 LLM의 범주화_\n' +
      '\n' +
      'LLM의 역할과 그래프 관련 문제를 해결하기 위한 최종 구성요소는 무엇인가에 따라 그래프 기법에서 LLM을 크게 세 가지 범주로 분류한다.\n' +
      '\n' +
      '**LLM as Predictor**. 이 범주의 방법은 표현 또는 예측을 출력하는 최종 구성요소로서 LLM을 제공한다. GNN으로 강화될 수 있고, 그래프 정보가 LLM에 어떻게 주입되는지에 따라 분류될 수 있다: 1) _Graph as Sequence_: 이러한 유형의 방법은 LLM 아키텍처에 변화를 주지 않지만, "그래프 토큰 시퀀스"를 입력으로 하여 그래프 구조를 인식하게 한다. "그래프 토큰 시퀀스"는 그래프 또는 그래프 인코더들에 의해 출력된 숨겨진 표현들에 대한 자연 언어 설명들일 수 있다. 2) _Graph-Empoured LLM_: 이러한 유형의 방법은 LLM 기본 모델(_i.e._, Transformers)의 아키텍처를 수정하고 그들의 아키텍처 내부에서 공동 텍스트 및 그래프 인코딩을 수행할 수 있게 한다. 3) _Graph-Aware LLM Finetuning_: 이러한 유형의 방법은 LLM들 또는 LLM 아키텍처들의 입력을 변경하지 않고, 단지 그래프로부터의 감독으로 LLM들을 미세 조정한다.\n' +
      '\n' +
      '**인코더로서의 LLM.** 이 메서드는 노드 또는 에지가 텍스트 정보와 연결된 그래프(노드 수준 또는 에지 수준 작업 해결)에 주로 사용됩니다. GNN은 최종 구성 요소이며 LLM을 초기 텍스트 인코더로 채택한다. 구체적으로, LLM들은 노드들/에지들과 연관된 텍스트를 인코딩하기 위해 먼저 활용된다. LLM에 의해 출력된 특징 벡터는 그래프 구조 인코딩을 위한 GNN에 대한 입력 임베딩 역할을 한다. GNN의 출력 임베딩은 다운스트림 작업에 대한 최종 노드/에지 표현으로 채택된다. 그러나 이러한 방법들은 최적화, 데이터 증강 및 지식 증류 관점에서 솔루션을 요약하는 수렴 문제, 희박한 데이터 문제 및 비효율적인 문제로 인해 어려움을 겪는다.\n' +
      '\n' +
      '**LLM as Aligner.** 이 범주의 메서드는 텍스트 인코딩 구성 요소로 LLM을 채택하고 그래프 구조 인코딩 구성 요소 역할을 하는 GNN과 정렬합니다. 과제 해결을 위한 최종 구성 요소로 LMM과 GNN을 함께 채택하였다. 구체적으로, LLM들과 GNN들 사이의 정렬은 1) 하나의 모달리티로부터 생성된 의사 라벨들이 반복 학습 방식으로 다른 모달리티에 대한 트레이닝에 활용되는 _예측 정렬_ 및 2) 대조적 학습이 LLM들에 의해 생성된 텍스트 임베딩들 및 GNN들에 의해 생성된 그래프 임베딩들을 정렬하기 위해 채택되는 _잠재 공간 정렬_로 분류될 수 있다.\n' +
      '\n' +
      '### _Training & Inference Framework with LLMs_\n' +
      '\n' +
      '그래프에 언어 모델을 적용하기 위한 두 가지 전형적인 훈련 및 추론 패러다임이 있다: 1) 사전 훈련-then-finetuning: 일반적으로 중규모 대형 언어 모델에 채택되고, 2) 사전 훈련-then-prompting: 일반적으로 대규모 대형 언어 모델에 채택된다.\n' +
      '\n' +
      '**사전 훈련** 은 감독 되지 않은 목표를 사용 하 여 언어 모델을 학습 하 여 다운스트림 작업에 대 한 언어 이해 및 추론 능력으로 초기화하는 것을 나타냅니다. 순수 텍스트에 대한 전형적인 사전 트레이닝 목표는 마스킹된 언어 모델링[22], 자동-회귀 인과 언어 모델링[25], 부패-재구성 언어 모델링[29] 및 텍스트-대-텍스트 전이 모델링[30]을 포함한다. 그래프 도메인에서 확장될 때, 언어 모델 사전 트레이닝 전략들은 문서 관계 예측[31], 네트워크-맥락화 마스킹된 언어 모델링[32], 대조적 소셜 예측[33] 및 컨텍스트 그래프 예측[34]을 포함한다.\n' +
      '\n' +
      '**Finetuning** 은 다운스트림 작업에 대해 레이블이 지정된 데이터로 언어 모델을 학습하는 프로세스를 나타냅니다. 언어 모델 미세 조정 방법론은 완전 미세 조정, 효율적인 미세 조정 및 명령어 튜닝으로 더 분류될 수 있다.\n' +
      '\n' +
      '* **전체 Finetuning** 은 언어 모델 내부의 모든 매개 변수를 업데이트 하는 것을 의미 합니다. 다운스트림 작업에 대한 언어 모델의 잠재력을 충분히 자극하는 가장 일반적으로 사용되는 미세 조정 방법이지만, 과도한 계산 과부하를 겪을 수 있고 과적합 문제를 초래할 수 있다[37].\n' +
      '* **효율적인 Finetuning** 은 언어 모델 내부의 매개 변수 하위 집합을 미세 조정 하는 것만을 참조 합니다. 순수 텍스트에 대한 효율적인 튜닝 방법에는 프롬프트 튜닝[38], 프리픽스 튜닝[39], 어댑터[40] 및 LoRA[41]가 있다. 그래프 데이터에 대해 특히 설계된 효율적인 언어 모델 미세 조정 방법에는 그래프 신경 프롬프트[42] 및 그래프 강화 프리픽스[43]가 포함된다.\n' +
      '* **명령어 조정** 은 추론에서 태스크를 보기 위해 모델 일반화를 권장 하는 다운스트림 작업 명령 [44][45]을 사용 하 여 언어 모델을 미세 조정 합니다. 완전 미세 조정과 효율적인 미세 조정을 갖는 직교 개념이며, 즉 명령어 튜닝을 위해 완전 미세 조정과 효율적인 미세 조정을 모두 선택할 수 있다. 명령 튜닝은 노드 분류[46], 링크 예측[47], 및 그래프-레벨 태스크[48]를 위해 그래프 도메인에서 채택된다.\n' +
      '\n' +
      '**Prompting** 은 언어 모델을 적용 하는 기술입니다.\n' +
      '\n' +
      '도. 2: 대표적인 예가 있는 그래프 시나리오 및 기술에 대한 LLM의 분류.\n' +
      '\n' +
      '모델 매개변수를 업데이트하지 않고 다운스트림 작업을 해결합니다. 테스트 샘플을 자연어 시퀀스로 공식화하고 언어 모델에 문맥 내 데모를 기반으로 직접 추론을 수행하도록 요청할 필요가 있다. 이것은 대규모 자기회귀 언어 모델에 특히 인기 있는 기법이다. 직접 프롬프팅과는 별도로 후속 작업에서는 체인 오브 사상 프롬프팅[49], 트리 오브 사상 프롬프팅[50], 그래프 오브 사상 프롬프팅[51]을 제안한다.\n' +
      '\n' +
      '다음 섹션에서는 섹션 3의 범주화를 따르고 각 그래프 시나리오에 대한 자세한 방법론을 논의한다.\n' +
      '\n' +
      '## 4 pure Graphs\n' +
      '\n' +
      '순수 그래프에 대한 문제는 LLM이 그래프 관련 추론 문제에 도입되는 이유와 방법에 대한 근본적인 동기를 제공한다. 그래프 이론에서 철저히 조사한 순수한 그래프는 컴퓨터 과학의 모든 관점에서 광범위한 고전적 알고리즘 문제에 대한 보편적인 표현 형식 역할을 한다. 최단 경로들, 특정 서브-그래프들, 및 흐름 네트워크들과 같은 많은 그래프 기반 개념들은 실세계 애플리케이션들과 강한 연결들을 갖는다[133, 134, 135]. 따라서 순수 그래프 기반 추론은 실제 응용에 기반을 둔 추론 문제에 대한 이론적 솔루션과 통찰력을 제공하는 데 필수적이다.\n' +
      '\n' +
      '그럼에도 불구하고, 많은 추론 태스크들은 전통적인 GNN들을 넘어서는 계산 용량을 필요로 한다. GNN은 일반적으로 그래프 크기가 주어진 제한된 수의 연산을 수행하도록 설계된다. 이와 달리 그래프 추론 문제는 과제의 성격에 따라 최대 무기한 복잡성을 요구할 수 있다. 일반적인 추론 집약적인 문제에 대해 기존의 GNN을 훈련하는 것은 사전 가정과 전문화된 모델 설계 없이는 어렵다. 이러한 근본적인 격차는 연구자들이 그래프 문제에 LLM을 통합하려는 동기를 부여한다. 한편, LLM은 최근 뛰어난 창발적 추론 능력[110, 49, 111]을 보여준다. 이것은 부분적으로 주의 깊은 프롬프트 또는 트레이닝으로 중간 단계들의 무한 시퀀스들을 컴퓨팅하는 것을 가능하게 하는 그들의 자기회귀 메커니즘 때문이다[49, 50].\n' +
      '\n' +
      '다음의 하위 섹션에서는 LLM을 순수한 그래프 추론 문제에 통합하려는 시도를 논의한다. 이러한 작품들의 과제, 한계점, 연구 결과에 대해서도 논의할 것이다. 표 II는 이러한 노력의 대략적인 분류를 나열한다. 보통, 입력 그래프들은 그래프 구조[124, 125, 126, 126, 127, 128, 129, 130, 126]를 구두화함으로써 또는 그래프 구조를 암시적 특징 시퀀스들로 인코딩함으로써 입력 시퀀스의 일부로서 직렬화된다[43]. 연구된 추론 문제는 연결성, 최단 경로, 주기 검출과 같은 단순한 문제에서 최대 흐름 및 해밀턴 경로 탐색과 같은 더 단단한 문제(NP-완전 문제)에 이르기까지 다양하다. 연구된 문제의 포괄적인 목록은 표 IV에 나열되어 있다. 여기서 대표적인 문제만 나열합니다. 이 테이블은 [128]의 공간-시간 추론 문제와 같은 더 많은 도메인-특정 문제를 포함하지 않는다.\n' +
      '\n' +
      '### _Direct Answering_\n' +
      '\n' +
      '그래프 기반 추론 문제는 일반적으로 복잡한 계산을 포함하지만, 연구자들은 부분적으로 접근법의 단순성 때문에 그리고 부분적으로 LLM의 다른 창발 능력에 경외심을 가지고 연속된 입력 그래프에서 답을 시작점 또는 기준선으로 직접 생성하도록 여전히 시도한다. 이 접근법은 LLM에 대한 그래프 이해의 프로브(그래프 "추론"과는 대조적으로)로 볼 수 있으며, 이는 LLM이 LLM이 답변을 직접 "추측"하기에 내부적으로 충분한 표현을 획득하는지 테스트한다. 입력 시퀀스에서 그래프가 어떻게 제시되는지 최적화하려는 다양한 시도가 있었지만, 유한한 시퀀스 길이와 계산 연산에 의해 제한되는 다음 섹션에서 논의할 것이지만 NP-완전한 문제와 같은 복잡한 추론 문제를 해결하는 데는 이 접근법의 근본적인 한계가 있다. 당연히, 대부분의 연구들은 LLM들이 예비 그래프 이해 능력을 가지고 있지만, 더 복잡한 문제들 또는 더 큰 그래프들[124, 125, 126, 128, 131, 43]에서 성능이 덜 만족스럽다는 것을 발견한다. 이하에서는 이들의 투입 표현 방법과 주요 차이점을 바탕으로 이들 연구의 세부 사항에 대해 논의하고자 한다.\n' +
      '\n' +
      '**일반적으로 그래프를 언어화 합니다.* * 그래프 구조를 자연어로 언어화하는 것은 그래프를 나타내는 가장 간단한 방법입니다. 대표적인 접근법들은 [124, 125, 131, 128]에서 널리 연구된 에지 및 인접 리스트를 기술하는 것을 포함한다. 예를 들어, 세 개의 노드를 갖는 삼각형 그래프에 대해, 에지 리스트는 "_(0, 1), (1, 2), (2, 0)]으로 기입될 수 있다. 노드 0은 노드 1에 연결되고, 노드 1은 노드 2에 연결되며, 노드 2는 노드 0에 연결된다. 또한, "_노드 0과 노드 1 사이에 엣지가 있음, 노드 1과 노드 2 사이에 엣지가 있음, 노드 2와 노드 0 사이에 엣지가 있음"과 같은 자연어로 작성될 수 있다._ 한편, 노드들의 관점에서 인접 리스트를 기술할 수 있다. 예를 들어, 동일한 삼각형 그래프에 대해, 인접 리스트는 _"노드 0은 노드 1 및 노드 2에 연결됨. 노드 1은 노드 0 및 노드 2에 연결됨. 노드 2는 노드 0 및 노드 1에 연결됨."_ 이 입력들에서, 제로-샷 또는 소수-샷(맥락 학습 내) 설정에서 질문에 답하도록 LLM들을 프롬프트할 수 있으며, 전자의 것은 그래프 구조가 주어진 질문을 직접 하는 것이고, 후자의 것은 질문 및 답변의 몇 가지 예를 제공한 후에 그래프 구조에 대한 질문을 하는 것이다. [124, 125, 126] LLM은 연결성, 이웃 식별 및 그래프 크기 카운팅과 같은 더 쉬운 질문에 답할 수 있지만 주기 감지 및 해밀턴 경로 찾기와 같은 더 복잡한 질문에 답하지 못한다는 것을 확인한다. 그들의 결과는 또한 적은 샷 설정에서 더 많은 예를 제공하는 것이 여전히 만족스럽지는 않지만 특히 더 쉬운 문제에서 성능을 증가시킨다는 것을 보여준다.\n' +
      '\n' +
      '**패러프레이징 그래프.** 언어화된 그래프는 사람에게도 길고, 구조화되지 않고, 읽기 복잡할 수 있으므로 LLM이 답변을 추론하는 데 가장 적합한 입력 형식이 아닐 수 있습니다. 이를 위해, 연구자도 그래프 구조를 좀 더 자연스럽거나 간결한 문장으로 패러프레이징하려고 한다. [126] LLM들이 자신에 대한 원시 그래프 입력들에 대한 포맷 설명을 생성하도록 프롬프트함으로써(_Format-Explanation_), 또는 자연 태스크에서 역할을 하는 척하도록 프롬프트함으로써(_Role Prompting_), 일부 문제들에 대한 성능은 향상될 수 있지만 체계적이지는 않다는 것을 발견한다([131] 소셜 네트워크, 우정 그래프 또는 공저자 그래프와 같은 실제 시나리오에서 순수 그래프를 접지하는 효과를 탐구한다. 이러한 그래프에서 노드는 사람으로 설명되고, 에지는 사람 간의 관계이다. 결과는 실제 시나리오에서 인코딩이 일부 문제에 대한 성능을 향상시킬 수 있지만 여전히 일관되지 않음을 나타낸다.\n' +
      '\n' +
      '**그래프를 암시적 특징 시퀀스로 인코딩** 합니다. 마지막으로, 연구자들은 또한 그래프 구조를 입력 시퀀스의 일부로서 암시적 특징 시퀀스들로 인코딩하려고 시도한다[43]. 이전의 언어화 접근법들과는 달리, 이것은 일반적으로 그래프 구조를 특징들의 시퀀스로 인코딩하기 위해 그래프 인코더를 트레이닝하고 새로운 입력 포맷에 적응하기 위해 LLM들을 미세 조정하는 것을 포함한다. [43] 부분 구조 카운팅, 최대 삼중항 합, 최단 경로 및 이분 매칭을 포함한 문제에 대한 급격한 성능 향상을 보여주며, 미세 조정 LLM이 특정 작업 분포에 큰 적합력을 가지고 있다는 증거이다.\n' +
      '\n' +
      '### _Heuristic Reasoning_\n' +
      '\n' +
      '출력에 대한 직접 매핑은 LLM의 강력한 표현력을 활용하여 답을 "추측"하지만 복잡한 추론 문제를 해결하는 데 필수적인 LLM의 인상적인 창발적 추론 능력을 충분히 활용하지는 못한다. 이를 위해, LLM이 그래프에 대해 휴리스틱 추론을 수행하도록 하는 시도가 있었다. 이 접근법은 LLM이 휴리스틱하게 정답으로 이어질 수 있는 일련의 중간 추론 단계를 수행하도록 권장한다.\n' +
      '\n' +
      '**단계별 추론**. CoT(chain-of-thought) 추론[49, 111]의 성공에 힘입어 연구자들은 LLM이 그래프에 대해 단계적으로 추론을 수행하도록 시도한다. 사슬 사상(Chain-of-thought)은 LLM이 문제를 해결하기 위해 일련의 추론 단계를 내놓도록 장려하는데, 이는 인간이 문제를 해결하는 방식과 유사하다. 그것은 일반적으로 추론 과정을 안내하기 위해 몇 가지 예시적인 예를 포함한다. 제로-샷 CoT는 어떠한 예도 요구하지 않는 유사한 접근법이다. 이러한 기술들은 [128, 124, 43, 125, 126, 131, 43]에서 연구된다. 결과적으로 CoT-스타일 추론은 사이클 검출 및 최단 경로와 같은 더 간단한 문제에 대한 성능을 향상시킬 수 있음을 나타낸다. 그러나 해밀턴 경로 탐색 및 위상 분류와 같은 더 복잡한 문제에서는 개선이 일관되지 않거나 감소한다.\n' +
      '\n' +
      '**서브그래프를 증거로 검색** 합니다. 노드 차수 계산 및 이웃 검출과 같은 많은 그래프 추론 문제는 전체 그래프의 하위 그래프에서만 추론을 포함한다. 이러한 속성을 통해 연구자들은 LLM이 증거로서 하위 그래프를 검색하고 하위 그래프에 대한 추론을 수행할 수 있다. 빌드-아-그래프 프롬프트 [124]는 LLM들이 관련 그래프 구조들을 질문들에 재구성하고 그들에 대한 추론을 수행하도록 장려한다. 이 방법은 전체 그래프에서 추론을 필요로 하는 악명 높은 까다로운 문제인 해밀턴 경로 탐색을 제외한 문제에 대해 유망한 결과를 보여준다. 또 다른 접근법인 Context-Summarization[126]은 LLM들이 키 노드들, 에지들, 또는 서브-그래프들을 요약하고 추론을 수행하도록 장려한다. 그들은 노드 분류에 대해서만 평가하며, 결과는 노드 분류 문제의 지역적 특성을 고려한 직관적인 결과인 CoT 스타일 추론과 결합될 때 개선을 보여준다.\n' +
      '\n' +
      '**그래프에서 검색** 입니다. 이러한 추론은 그래프에 대한 탐색 알고리즘인 폭 우선 탐색(BFS: breadth-first search)과 깊이 우선 탐색(DFS: depth-first search)과 관련이 있다. 일반적으로 적용되지는 않지만, BFS와 DFS는 그래프 추론 문제를 해결하는 가장 직관적이고 효과적인 방법이다. 특히 지식 그래프 질문에서 검색 기반 추론을 시뮬레이션하기 위해 수많은 탐색이 이루어졌다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{113.8pt} p{113.8pt} p{113.8pt} p{113.8pt} p{113.8pt}} \\hline \\hline Method & Graph Format or Encoding & Reasoning Process & Reasoning Category & Papers \\\\ \\hline Zero-Shot & Verbaliized edge or adjacency list. & Directly answering. & Direct Answering & [124, 125, 126, 128, 131] \\\\ \\hline Role Promping & Verbaliized edge or adjacency list. & Directly answering by designating a specific role to the LLM. & Direct Answering & [126] \\\\ \\hline Format Explanation & Verbaliized edge or adjacency list. & Encouraging the LLM to explain the input graph format first. & Direct Answering & [126] \\\\ \\hline GraphLLM & Prefix tokens encoded by a graph encoder. & Directly answering. & Direct Answering & [43] \\\\ \\hline Few-Shot (In-Context Learning) & Verbaliized edge or adjacency lists preceded with a few demonstrative examples. & Directly answering by following the examples. & Direct Answering & [124, 125, 128, 131] \\\\ \\hline Chain-of-Thought & Verbaliized edge or adjacency lists preceded with a few demonstrative examples. & Reasoning through a series of intermediate reasoning steps in the generation following the examples. & Heuristic Reasoning & [124, 125, 126, 128, 131, 132] \\\\ \\hline Self-Consistency & Verbaliized edge or adjacency lists preceded with a few demonstrative examples. & Reasoning through a series of intermediate reasoning steps in generation, and then selecting the most consistent answer. & Heuristic Reasoning & [124] \\\\ \\hline Build-a-Graph list. & Verbaliized edge or adjacency list. & Reconstructing the graph in output, and then reasoning on the graph. & Heuristic Reasoning & [124, 131] \\\\ \\hline Context-Summarization & Verbaliized edge or adjacency list. & Directly answering by first summarizing the key elements in the graph. & Heuristic Reasoning & [126] \\\\ \\hline Reasoning-on-Graph & Retrieved paths from external graphs. & First, plan the reasoning process in the form of paths to be retrieved and then infer on the retrieved paths. & Heuristic Reasoning & [129] \\\\ \\hline Iterative Reading-then- Reasoning & Retrieved neighboring edges or nodes from external graphs. & Iteratively retrieving neighboring edges or nodes from external graphs and inferring from the retrieved information. & Heuristic Reasoning & [130, 132] \\\\ \\hline Algorithmic Reasoning & Verbaliized edge or adjacency list. & Simulating the reasoning process of a relevant algorithm in a generation. & Algorithmic Reasoning & [124] \\\\ \\hline Calling APIs & External Knowledge Base. & Generate the reasoning process as (probably nested) API calls to be executed externally on the knowledge base. & Algorithmic Reasoning & [132, 127] \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE II: A collection of LLM reasoning methods on pure graph discussed in Section 4. We do not include the backbone models used in these methods studied in the original papers, as these methods generally apply to any LLMs. The “Papers” column lists the papers that study the specific methods.\n' +
      '\n' +
      '답변 이 접근법은 답 외에 해석 가능한 증거를 제공한다는 이점을 누리고 있다. 추론-온-그래프(RoG)[129]는 LLM들이 계획으로서 여러 관계 경로들을 생성하도록 프롬프트하고, 그 후 지식 그래프(KG)로부터 검색되어 질문에 답하기 위한 증거로서 사용되는 대표적인 접근법이다. 또 다른 접근법은 동적 검색 프로세스를 시뮬레이션하는 KG[130, 132]로부터 서브그래프 상에서 반복적으로 검색 및 추론하는 것이다. 각 단계에서 LLM은 현재 노드의 이웃을 검색한 다음 질문에 답하거나 다음 검색 단계를 계속하기로 결정한다.\n' +
      '\n' +
      '### _Algorithmic Reasoning_\n' +
      '\n' +
      '앞의 두 가지 접근법은 휴리스틱이며, 이는 추론 과정이 인간의 직관과 일치하지만 정답으로 이어지지는 않는다는 것을 의미한다. 대조적으로, 이러한 문제는 보통 컴퓨터 과학의 알고리즘에 의해 해결된다. 따라서, 연구원들은 또한 LLM들이 그래프들에 대한 알고리즘 추론을 수행하게 하려고 시도한다. [124] 제안된 "_Algorithmic Prompting_"은 LLM이 질문과 관련된 알고리즘을 상기한 다음 알고리즘에 따라 단계적으로 추론을 수행하도록 프롬프트하는 "_Algorithmic Prompting_"이다. 그러나 그들의 결과는 휴리스틱 추론 접근법에 비해 일관된 개선을 보여주지 않는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{56.9pt} p{113.8pt} p{113.8pt} p{113.8pt} p{113.8pt}} \\hline \\hline\n' +
      '**Problem** & **Definition** & **Applications** & **Typical Complexity** & **Papers** \\\\ \\hline Connectivity & Given a graph \\(\\mathcal{G}\\) and two nodes \\(u\\) and \\(v\\), tell if they are connected by a path. & Relationship Detection, Link Prediction & \\(O(|E|)\\) or \\(O(V^{2})\\) & [124, 125] \\\\ \\hline Neighbor Detection & Given a graph \\(\\mathcal{G}\\) and a node \\(v\\), find the nodes \\(v\\). & Recommendation, Knowledge QA & \\(O(\\min(|E|,|V|))\\) & [126] \\\\ \\hline Node Degree & Given a graph \\(\\mathcal{G}\\) and a node \\(v\\), find the number of edges connected to \\(v\\). & Entity Popularity, Importance Ranking & \\(O(\\min(|E|,|V|))\\) & [125, 126] \\\\ \\hline Attribute Retrieval & Given a graph \\(\\mathcal{G}\\) with node-level information and a node \\(v\\), return the attribute of \\(v\\). & Recommendation, Node Classification, Node QA & \\(O(1)\\) & [126] \\\\ \\hline Graph Size & Given a graph \\(\\mathcal{G}\\), find the number of nodes and edges. & Graph-level Classification & \\(O(|V|+|E|)\\) & [126] \\\\ \\hline Cycle Detection & Given a graph \\(\\mathcal{G}\\), tell if it contains a cycle. & Loop Elimination, Program Loop Detection & \\(O(|V|)\\) & [124] \\\\ \\hline Diameter & Given a graph \\(\\mathcal{G}\\), find the diameter of \\(\\mathcal{G}\\). & Graph-level Classification, Clustering & \\(O(|V^{\\mathcal{G}})\\) or \\(O(|V|^{2}\\log|V|+|V|E|)\\) & [126] \\\\ \\hline Topological Sort & Given a directed acyclic graph \\(\\mathcal{G}\\), find a topological ordering of its vertices so that for every edge \\((u,v)\\), \\(u\\) comes before \\(v\\) in the ordering. & Timeline Generation, Dependency Parsing, Scheduling & \\(O(|V|+|E|)\\) & [124] \\\\ \\hline Wedge or Triangle Detection & Given a graph \\(\\mathcal{G}\\) and a vertex \\(v\\), identify if there is a wedge or triangle centered at \\(v\\). & Relationship Detection, Link Prediction & \\(O(|V|+|E|)\\) & [125] \\\\ \\hline Maximum Triplet Sum & Given a graph \\(\\mathcal{G}\\), find the maximum sum of the weights of three vertices that are connected. & Community Detection & \\(O(|V|^{\\mathcal{G}})\\) & [43] \\\\ \\hline Shortest Path & Given a graph \\(\\mathcal{G}\\) and two nodes \\(u\\) and \\(v\\), find the shortest path between \\(u\\) and \\(v\\). & Navigation, Planning & \\(O(|E|)\\) or \\(O(V^{2})\\) & [124, 125] \\\\ \\hline Maximum Flow & Given a directed graph \\(\\mathcal{G}\\) with a source node \\(s\\) and a sink node \\(t\\), find the maximum flow from \\(s\\) to \\(t\\). & Transportation Planning, Network Design & \\(O(|V|E|^{2})\\), \\(O(|E|V|\\log|V|)\\) or \\(O(|V|^{3})\\) & [124] \\\\ \\hline Bipartite Graph Matching & Given a bipartite graph \\(\\mathcal{G}\\) with two disjoint sets of vertices \\(v_{1}\\) and \\(v_{2}\\), find a matching between \\(V_{1}\\) and \\(v_{2}\\) that maximizes the number of matched pairs. & Recommendation, Resource Allocation, Scheduling & \\(O(|E|\\sqrt{|V|})\\) & [124, 43] \\\\ \\hline Graph Neural Networks & Given a graph \\(\\mathcal{G}\\) with node features \\(\\mathbf{X}\\) of dimension \\(\\delta\\), simulate a graph neural networks with \\(\\mathcal{G}\\) layers and return the encoded node features & Node Classification, Graph-level Classification & \\(O(|V|^{2})\\) & [124] \\\\ \\hline Clustering Coefficient & Given a graph \\(\\mathcal{G}\\), find the clustering coefficient of \\(\\mathcal{G}\\). & Community Detection, Node Clustering & \\(O(|V|^{\\mathcal{G}})\\) & [126] \\\\ \\hline Substructure Counting & Given a graph \\(\\mathcal{G}\\) and a subgraph \\(\\mathcal{G}^{\\prime}\\), count the number of occurrences of \\(\\mathcal{G}^{\\prime}\\) in \\(\\mathcal{G}\\). & Pattern Matching, Subgraph Detection, Abnormality Detection & NP-Complete & [43] \\\\ \\hline Hamilton Path & Given a graph \\(\\mathcal{G}\\), find a path that visits every vertex exactly once. & Route Planning, Drilling Machine Planning, DNA Sequencing & NP-Complete & [124] \\\\ \\hline (Knowledge) Graph QA & Given a (knowledge) graph \\(\\mathcal{G}\\) and a question \\(\\mathcal{G}\\), find the answer to \\(\\mathcal{G}\\). & Dialogue System, Smart Assistant, Recommendation & -- & [126, 129, 130, 131, 132] \\\\ \\hline Graph Query Language Generation & Given a graph \\(\\mathcal{G}\\) and a query \\(\\mathcal{G}\\), generate a query language that can be used to query \\(\\mathcal{G}\\). & Graph Summarization, FAQ Generation, Query Suggestions & [126] \\\\ \\hline Node Classification & Given a graph \\(\\mathcal{G}\\), predict the class of a node \\(v\\). & Recommendation, User Profile, Algorithmy Detection & -- & [126, 127] \\\\ \\hline Graph Classification & Given a graph \\(\\mathcal{G}\\), predict the class of \\(\\mathcal{G}\\). & Molecule Property Prediction, Molecule QA, Graph QA & [127, 128] \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE III: A collection of pure graph reasoning problems studied in Section 4. \\(\\mathcal{G}=(V,\\mathcal{E})\\) denotes a graph with vertices \\(\\mathcal{G}\\) and edges \\(\\mathcal{E}\\). \\(v\\) and \\(c\\) denote individual vertices and edges, respectively. The “Papers” column lists the time complexity of standard algorithms for the problem, ignoring more advanced but complex algorithms that are not comparable to LLMs’ reasoning processes.\n' +
      '\n' +
      '동일한 논문에서 제안된 BaG 프롬프트와 같다. 이는 알고리즘 추론 접근법이 더 신중한 기술 없이 시뮬레이션하기에는 여전히 복잡하기 때문일 수 있다. 보다 직접적인 접근 방식인 Graph-ToolFormer [127]을 통해 LLM은 명시적 추론 단계로 API 호출을 생성할 수 있습니다. 그런 다음 외부 그래프에서 답변을 얻기 위해 이러한 API 호출이 외부에서 실행됩니다. 이 접근법은 실제 작업에 기반을 둔 작업을 순수한 그래프 추론 문제로 변환하기에 적합하며, 지식 그래프, 소셜 네트워크 및 추천 시스템과 같은 다양한 응용 프로그램에서 효과를 보여준다.\n' +
      '\n' +
      '### _Discussion_\n' +
      '\n' +
      '상기 접근법들은 상호 배타적이지 않으며, 더 나은 성능을 달성하기 위해 결합될 수 있다. 더욱이 엄밀히 말하면, 휴리스틱 추론은 직접 답변도 할 수 있는 반면, 알고리즘 추론은 특별한 경우로서 휴리스틱 추론의 역량을 내포하고 있다. 연구자들은 특정 문제에 가장 적합한 접근법을 선택하는 것이 좋다. 예를 들어, 직접 응답은 해결하기 쉽고 사전-트레이닝 데이터세트가 공통 엔티티 분류 및 관계 검출과 같은 양호한 추측을 위한 충분한 바이어스를 제공하는 문제에 적합하다. 발견적 추론은 명시적으로 다루기 어려운 문제에 적합하지만 직관은 그래프 기반 질문 응답 및 지식 그래프 추론과 같은 몇 가지 지침을 제공할 수 있다. 알고리즘 추론은 해결하기 어렵지만 경로 계획 및 패턴 매칭과 같이 알고리즘 솔루션이 잘 정의된 문제에 적합하다.\n' +
      '\n' +
      '## 5 텍스트 리치 그래프입니다.\n' +
      '\n' +
      '노드/에지 수준의 텍스트 정보를 가진 그래프(텍스트가 풍부한 그래프)는 현실 세계, _예:_, 학술 네트워크, 소셜 네트워크 및 법률 사례 네트워크에 도처에 존재한다. 이러한 네트워크에서의 학습은 모델이 노드/에지와 관련된 텍스트 정보와 입력 그래프 내부에 놓여 있는 구조 정보를 모두 인코딩해야 한다. LLM의 역할에 따라 기존 저작물은 LLM을 Predictor로, LLM을 Encoder로, LLM을 Aligner로 세 가지로 분류할 수 있다. 조사된 모든 방법을 표 V에 요약한다.\n' +
      '\n' +
      '### _LLM as Predictor_\n' +
      '\n' +
      '이러한 방법들은 텍스트 정보와 그래프 구조 정보를 모두 포착하기 위한 주요 모델 아키텍처로서 언어 모델을 제공한다. 그래프의 구조 정보가 언어 모델에 주입되는 방식(입력 대 아키텍처 대 손실)에 따라 _Graph as Sequence 메서드_, _Graph-Empowered LLMs_ 및 _Graph-Aware LLM finetuning 메서드_의 세 가지 유형으로 분류할 수 있습니다. _Graph as Sequence methods_에서 그래프는 입력으로부터 텍스트와 함께 언어 모델에 의해 이해될 수 있는 시퀀스로 변환된다. _Graph-Empowered LLMs_에서 사람들은 텍스트와 그래프 구조를 동시에 인코딩할 수 있도록 트랜스포머의 아키텍처(LLM의 기본 아키텍처)를 수정합니다. Graph-Aware LLM 피니튜닝 방법_에서 LLM은 그래프 구조 감독으로 미세 조정되며 그래프 맥락화 표현을 생성할 수 있습니다.\n' +
      '\n' +
      '#### 5.1.1 Graph as Sequence.\n' +
      '\n' +
      '이들 방법에서, 그래프 정보는 주로 "입력"측으로부터 LLM으로 인코딩된다. 노드/에지와 관련된 에고 그래프는 수열 \\(\\mathbf{H}_{\\mathcal{G}_{v}}\\)로 직렬화되며, 이 수열은 텍스트 \\(d_{v}\\):와 함께 LLM에 입력될 수 있다.\n' +
      '\n' +
      '\\[\\mathbf{H}_{\\mathcal{G}_{v}}=\\text{Graph2Seq}(\\mathcal{G}_{v}), \\tag{7}\\] \\[\\mathbf{h}_{v}=\\text{LLM}([\\mathbf{H}_{\\mathcal{G}_{v}},d_{v}]). \\tag{8}\\]\n' +
      '\n' +
      '이 방법은 \\(\\text{Graph2Seq}(\\cdot)\\) 함수의 선택에 따라 규칙 기반 방법과 GNN 기반 방법으로 분류할 수 있다. 범주의 삽화는 그림 3에서 찾을 수 있다.\n' +
      '\n' +
      '**규칙 기반: 규칙을 사용하여 그래프를 텍스트 시퀀스로 선형화합니다.* * 이러한 방법은 자연어로 구조를 설명하는 규칙을 설계하고 텍스트 프롬프트 템플릿을 \\(\\text{Graph2Seq}(\\cdot)\\)로 채택합니다. 예를 들어, 논문 노드 \\(v_{i}\\)의 에고그래프 \\(\\mathcal{G}_{v_{i}\\)가 저자 노드 \\(v_{j}\\)와 \\(v_{k}\\)와 장소 노드 \\(v_{t}\\)와 \\(v_{s}\\), \\(\\mathbf{H}_{\\mathcal{G}_{v_{i}}}=\\text{Graph2Seq}(\\mathcal{G}_{v_{i}})=\\)_"중심 논문 노드는 \\(v_{i}\\)이고, 저자 이웃 노드는 \\(v_{j}\\)와 \\(v_{k}\\)이고 장소 이웃 노드는 \\(v_{t}\\)와 \\(v_{s}\\)"_이다. 이것은 그래프 구조를 언어 모델로 인코딩하기 위해 (추가 모델 매개 변수를 도입하지 않고) 가장 간단하고 쉬운 방법이다. 이 라인을 따라, InstructGLM[47]은 각 노드에 대한 로컬 에고-그래프 구조(최대 3-홉 연결)를 기술하기 위한 템플릿을 설계하고 노드 분류 및 링크 예측을 위한 명령어 튜닝을 수행한다. GraphText[66]는 텍스트 시퀀스로 구조화하기 위해 구문 트리 기반 방법을 추가로 제안한다. 연구자들 [84] 또한 그래프에 선형화된 구조 정보가 노드 분류에 대한 LLM의 성능을 향상시킬 수 있는 시기와 이유를 연구하고 노드와 관련된 텍스트 정보가 부족할 때 구조 정보가 유익하다는 것을 발견한다.\n' +
      '\n' +
      '**GNN 기반: GNN을 사용하여 그래프를 특수 토큰으로 인코딩합니다.* * 자연어 프롬프트를 사용하여 그래프를 시퀀스로 선형화하는 규칙 기반 메서드와는 달리 GNN 기반 메서드는 그래프 인코더 모델(_i.e._, GNN)을 사용하여 노드와 연결된 에고 그래프를 순수 텍스트 정보와 연결된 특수 토큰 표현으로 언어 모델로 인코딩합니다.\n' +
      '\n' +
      '\\[\\mathbf{H}_{\\mathcal{G}_{v}}=\\text{Graph2Seq}(\\mathcal{G}_{v})=\\text{Graph Enc}(\\mathcal{G}_{v}). \\tag{9}\\]\n' +
      '\n' +
      '이러한 방법의 장점은 강력한 그래프 인코더를 사용하여 유용한 구조 정보의 숨겨진 표현을 캡처할 수 있다는 것이며, 문제는 그래프 모달리티와 텍스트 모달리티 사이의 간격을 메우는 방법이다. GNP[42]는 LLaVA[92]의 유사한 철학을 채택하는데, 여기서 이들은 GNN을 사용하여 그래프 토큰을 생성한 다음, 학습가능한 프로젝션 매트릭스를 갖는 텍스트 토큰 공간에 그래프 토큰을 프로젝션한다. 투영된 그래프 토큰은 텍스트 토큰과 연결되어 언어 모델에 공급된다. GraphGPT[46]는 텍스트 인코더 및 대조적 학습으로 프로젝션에 대한 텍스트-접지 GNN을 트레이닝하는 것을 추가로 제안한다. DGTL[77]은 엉킴 없는 그래프 학습을 도입하고, 그래프 표현을 위치 인코딩으로서 서비스하고, 이들을 텍스트 시퀀스에 추가한다. METERN[76]은 그래프 상의 텍스트 기반 다중 표현 학습을 위해 노드 텍스트 시퀀스에 학습 가능한 관계 임베딩을 추가한다[93].\n' +
      '\n' +
      '#### 5.1.2 Graph-Empowered LLMs.\n' +
      '\n' +
      '이러한 방법들에서, 연구자들은 그들의 모델 아키텍처 내에서 공동 텍스트와 그래프 인코딩을 수행할 수 있는 고급 LLM 아키텍처(_i.e._, Graph-Empowered LLMs)를 설계한다. 트랜스포머[44]는 오늘날 사전 훈련된 LMs[22] 및 LLMs[37]의 기본 모델 역할을 한다. 그러나, 이들은 자연어(시퀀스) 인코딩을 위해 설계되고 비순차적 구조 정보를 고려하지 않는다. 이를 위해 Graph-Empowered LLMs를 제안한다. 각 변압기 계층에 가상 구조 토큰 \\(\\mathbf{H}_{\\mathcal{G}_{v}}\\)을 도입 하는 공유 철학을 가지고 있습니다.\n' +
      '\n' +
      '\\[\\widetilde{\\mathbf{H}}_{d_{v}}^{(l)}=[\\mathbf{H}_{\\mathcal{G}_{v}}^{(l)},\\mathbf{H}_{d_{v}}^{(l)}] \\tag{10}\\]\n' +
      '\n' +
      '여기서 \\(\\mathbf{H}_{\\mathcal{G}_{v}}\\)은 그래프 인코더로부터 학습 가능한 임베딩 또는 출력 가능하다. 그런 다음, 트랜스포머들에서의 오리지널 멀티헤드 어텐션(MHA)은 구조 토큰들을 고려하기 위해 비대칭 MHA로 수정된다:\n' +
      '\n' +
      '\\[\\begin{MHA}_{asy}(\\mathbf{H}_{d_{v}}^{(l)},\\widetilde{\\mathbf{H}_{d_{v}}^{(l)})&=\\mathrm{head}_{u}(\\mathbf{H}_{d_{v}}^{(l)},\\widetilde{\\mathbf{H}_{u}^{(l)}}{\\sqrt{d/U}}\\bigg{(}\\frac{\\mathbf{Q}_{u}^{(l)}=\\mathbf{H}_{d_{v}}^{(l)}\\mathbf{W}_{Q,u}^{(l)},& \\widetilde{\\mathbf{K}_{u}^{(l)}=\\widetilde{\\mathbf{H}}_{d_{v}}^{(l)}\\mathbf{W}_{k,u}^{(l)}\\mathbf{W}\n' +
      '\n' +
      '비대칭 MHA 메커니즘으로, \\((l+1)\\)-번째 레이어의 노드 인코딩 프로세스는 다음과 같을 것이다:\n' +
      '\n' +
      '\\[\\begin{split}\\widetilde{\\mathbf{H}}_{d_{v}}^{(l)}=\\mathrm{ Normalize}(\\mathbf{H}_{d_{v}}^{(l)}+\\mathrm{MHA}_{asy}(\\widetilde{\\mathbf{H}}_{d_{v}}^{(l)}, \\mathbf{H}_{d_{v}}^{(l)})),\\\\ \\mathbf{H}_{d_{v}}^{(l+1)}=\\mathrm{Normalize}(\\widetilde{\\mathbf{H}}_{d_{v}}^{(l)^{\\prime}}+\\mathrm{MLP}(\\widetilde{\\mathbf{H}}_{d_{v}}^{(l)^{\\prime})). \\end{split} \\tag{12}\\]\n' +
      '\n' +
      '이러한 작업 라인을 따라, GreaseLM[68]은 각각의 계층에 언어 인코딩 컴포넌트 및 그래프 인코딩 컴포넌트를 갖는 것을 제안한다. 두 컴포넌트는 MInt 계층을 통해 상호 작용하며, 여기서 특수 구조 토큰은 텍스트 Transformer 입력에 추가되고, 특수 노드는 그래프 인코딩 계층에 추가된다. DRAGON [83]은 감독되지 않은 신호로 그리스LM을 사전 훈련하는 전략을 추가로 제안한다. GraphFormers[73]는 이웃 문서들의 현재 계층 [CLS] 토큰 은닉 상태들이 집계되고 현재 계층 중심 노드 텍스트 인코딩 상에 새로운 토큰으로서 추가되는 동종 텍스트-분산 네트워크들 상에서 노드 표현 학습을 위해 설계된다. Patton [32]은 네트워크-맥락화된 마스킹된 언어 모델링 및 마스킹된 노드 예측의 두 가지 새로운 전략으로 그래프포머를 사전 트레이닝하는 것을 추가로 제안한다. 헤터형성기[74]는 일부 노드가 텍스트와 연관되고(텍스트가 풍부하고) 다른 노드가 그렇지 않은(텍스트가 없는) 이종 텍스트-분산 네트워크 상의 표현들을 학습하기 위해 도입된다. 텍스트가 풍부한 이웃과 텍스트가 없는 이웃에 대한 가상 이웃 토큰은 원본 텍스트 토큰과 연결되어 각 트랜스포머 계층에 입력된다. 에지가 풍부한 텍스트 정보와 연관되는 텍스트-에지 네트워크 상의 표현 학습을 위해 에지형성기[75]가 제안된다. 에지 인코딩을 수행할 때, 가상 노드 토큰들은 조인트 인코딩을 위해 원래의 에지 텍스트 토큰들 상에 연결될 것이다.\n' +
      '\n' +
      '#### 5.1.3 Graph-Aware LLM finetuning.\n' +
      '\n' +
      '이러한 방법들에서 그래프 정보는 주로 "그래프에 대한 미세 조정"에 의해 LLM에 주입된다. 연구자들은 그래프의 구조가 어떤 문서가 다른 문서와 "의미적으로 유사하다"는 힌트를 제공할 수 있다고 가정한다. 예를 들어, 학술 그래프에서 서로 인용하는 논문은 유사한 주제일 수 있으며, 전자 상거래 그래프에서 많은 사용자가 공동 구매하는 항목은 관련 기능일 수 있다. 이러한 방법들은 텍스트를 입력으로 하는 바닐라 언어 모델들(_e.g._, BERT[22] 및 SciBERT[24])을 기본 모델로 채택하고 그래프 상의 구조 신호들로 이들을 미세 조정한다. 그 후, LLM은 텍스트 관점에서 그래프를 호모필리적으로 캡처하는 노드/에지 표현을 학습할 것이다.\n' +
      '\n' +
      '대부분의 방법들은 투-타워 인코딩 및 트레이닝 파이프라인을 채택하며, 여기서 각 노드의 표현은 개별적으로 획득된다:\n' +
      '\n' +
      '\\[\\mathbf{h}_{v_{i}}=\\mathrm{LLM}_{\\theta}(d_{v_{i}}), \\tag{13}\\]\n' +
      '\n' +
      '에 의해 최적화되고, 상기 모델은\n' +
      '\n' +
      '\\[\\min_{\\theta}f(\\mathbf{h}_{v_{i}},\\{\\mathbf{h}_{v_{i}^{+}}\\},\\{\\mathbf{h}_{v_{i}^{-}}\\}). \\tag{14}\\}\n' +
      '\n' +
      '여기서 \\(v_{i}^{+}\\)은 양의 노드에서 \\(v_{i}\\), \\(v_{i}^{-}\\)은 음의 노드에서 \\(v_{i}\\), \\(f(\\cdot)\\)는 쌍별 훈련 목표를 나타낸다. 훈련목표에 따라 \\(v_{i}^{-}\\)와 \\(v_{i}^{-}\\)의 전략이 다르다. SPECTER[52]는 인용 관계를 갖는 양의 텍스트/노드 쌍을 구성하고, 랜덤 네거티브 및 구조 하드 네거티브를 탐색하고, 삼중항 손실을 갖는 미세-튜닝 SciBERT[24]를 구성한다. SciNCL[53]은 그래프에 대해 훈련된 임베딩을 기반으로 한 보다 진보된 포지티브 및 네거티브 샘플링 방법을 도입함으로써 SPECTER을 확장한다. Touchup-G [55]는 그래프에서 특징 호모필리의 측정을 제안하고 이진 교차 엔트로피 미세 조정 목표를 제기한다. TwHIN-BERT [57]은 기성 이종 정보 네트워크 임베딩과 양의 노드 쌍을 채굴하고 대조적인 사회적 손실로 모델을 훈련한다. MICoL[60] 발견\n' +
      '\n' +
      '도. 3: (a) Rule-based Graph As Sequence, (b) GNN-based Graph As Sequence, (c) Graph-Empowered LLMs를 포함하는 Predictor 방법으로서 다양한 LLM의 예시.\n' +
      '\n' +
      '의미적으로 긍정적인 노드 쌍은 메타-경로[91]와 쌍을 이루며 InfoNCE 목표를 채택한다. E2EG[61]은 GIANT[59]의 유사한 철학을 이용하고 다운스트림 태스크 목적과는 별도로 이웃 예측 목적을 추가한다. 2-타워 그래프 중심 LLM 미세 조정 목표의 요약은 표 IV에서 찾을 수 있다.\n' +
      '\n' +
      '원-타워 파이프라인을 사용하는 다른 방법들이 있으며, 여기서 노드 쌍들은 함께 연결 및 인코딩된다:\n' +
      '\n' +
      '\\[\\mathbf{h}_{v_{i},v_{j}}=\\text{LLM}_{\\theta}(d_{v_{i}},d_{v_{j}}), \\tag{15}\\] \\[\\min_{\\theta}f(\\mathbf{h}_{v_{i},v_{j}}) \\tag{16}\\]\n' +
      '\n' +
      'LinkBERT[31]는 연속, 랜덤, 링크된 두 노드 텍스트 쌍의 관계를 분류하는 것을 목표로 하는 문서 관계 예측 목표(BERT[22]에서 다음 문장 예측의 확장)를 제안한다. MICoL[60]은 노드 쌍의 바이너리 메타 경로 또는 메타 그래프가 원타워 언어 모델과의 관계를 나타내는 것을 예측하는 것을 탐구한다.\n' +
      '\n' +
      '#### 5.1.4 Discussion\n' +
      '\n' +
      '1) 표현 학습(Representation Learning: Graph-Aware LLM 미세 조정(더 효율적이지만 덜 효과적) 및 Graph-Empowered LLMs(덜 효율적이지만 더 효과적) 2) 생성: 규칙 기반 그래프 As 시퀀스(제로 샷, 제한된 표현성 지원) 및 GNN 기반 그래프 As 시퀀스(필요한 훈련, 더 강력한 표현성 지원)를 제공한다. 공동체가 좋은 성과를 내고 있지만 여전히 풀어야 할 몇 가지 열린 질문이 있다.\n' +
      '\n' +
      '**코드 시퀀스로 그래프.** 시퀀스 메서드로 기존 그래프는 주로 규칙 기반 또는 GNN 기반입니다. 전자는 구조 데이터에 대해 자연스럽지 않은 그래프를 설명하기 위해 자연 언어에 의존하는 반면 후자는 훈련해야 하는 GNN 구성 요소를 가지고 있다. 보다 유망한 방법은 제로 샷 추론을 지원할 수 있는 그래프에 대한 구조 인식 시퀀스를 얻는 것이다. 잠재적인 해결책은 그래프를 설명하고 코드 LLMs[21]를 활용하기 위해 (구조를 캡처할 수 있는) 코드를 채택하는 것이다.\n' +
      '\n' +
      '**고급 Graph-Empowered LLM 기술.** Graph-empowered LLM은 그래프에 대한 기본 모델을 달성하는 유망한 방향입니다. 그러나 기존 작업은 1) _작업_ 과제와 거리가 멀습니다. 기존의 방법은 생성 작업에 적용하기 어려운 표현 학습(인코더 전용 LLM 사용)을 위해 주로 설계되었으며, 잠재적인 해결책은 디코더 전용 또는 인코더-디코더 LLM을 기본 구조로 하는 Graph-Empowered LLM을 설계하는 것이다. 2) _Pretraining_. 사전 훈련은 상황화된 데이터 이해 능력을 가진 LLM을 가능하게 하는 데 중요하며, 이는 다른 작업으로 일반화될 수 있다. 그러나 기존의 연구들은 주로 동질적인 텍스트가 풍부한 네트워크에서 LLM을 사전 훈련하는 것에 초점을 맞추고 있다. 이질적인 텍스트가 풍부한 네트워크[74], 동적 텍스트가 풍부한 네트워크[128], 텍스트에지 네트워크[75]를 포함한 보다 다양한 실제 시나리오에서 LLM 사전 교육을 탐구하려면 향후 연구가 필요하다.\n' +
      '\n' +
      '### _LLM as Encoder_\n' +
      '\n' +
      'LLM은 GNN에 대한 초기 노드 특징 벡터 역할을 하기 위해 텍스트 특징을 추출하고, 이를 통해 노드/에지 표현을 생성하고 예측을 수행한다. 이러한 방법들은 일반적으로 노드 \\(v_{i}\\)에 대한 최종 표현 \\(\\mathbf{h}_{v_{i}}\\)을 얻기 위해 LLM-GNN 캐스케이드 구조를 채택한다 \\(v_{i}\\):\n' +
      '\n' +
      '\\[\\mathbf{x}_{v_{i}}=\\text{LLM}(d_{v_{i}}) \\tag{17}\\] \\[\\mathbf{h}_{v_{i}}=\\text{GNN}(\\mathbf{X}_{v},\\mathcal{G}). \\tag{18}\\]\n' +
      '\n' +
      '여기서 \\(\\mathbf{x}_{v_{i}}\\)은 \\(v_{i}\\)과 연관된 텍스트 정보 \\(d_{v_{i}}\\)을 포착하는 특징 벡터이다. 최종 표현 \\(\\mathbf{h}_{v_{i}}\\)은 텍스트 정보와 \\(v_{i}\\)의 구조 정보를 모두 포함하고, 다운스트림 작업에 사용될 수 있다. 다음 섹션에서는 이러한 모델의 최적화, 증강 및 증류에 대해 논의한다. 이러한 기술에 대한 그림은 그림 4에서 찾을 수 있다.\n' +
      '\n' +
      '#### 5.2.1 Optimization\n' +
      '\n' +
      '**원 단계 학습** 은 다운스트림 작업에 대 한 계단식 아키텍처에서 LLM 및 GNN을 함께 학습 하는 것을 참조 합니다. TextGNN[79]은 GCN[85], GraphSAGE[86], GAT[87]을 기본 GNN 아키텍처로서 탐색하고, LLM 출력과 GNN 출력 사이에 스킵 연결을 추가하고, 스폰서된 검색 작업을 위해 전체 아키텍처를 최적화한다. AdsGNN[80]은 에지 레벨 정보 집계를 제안함으로써 TextGNN을 더 확장한다. GNN-LM[67]은 바닐라 언어 모델이 언어 모델링을 위해 코퍼스에서 유사한 컨텍스트를 참조할 수 있도록 GNN 계층을 추가한다. 케스케이드 파이프라인에서의 공동 트레이닝 LLM들 및 GNN들은 간단하지만 효율성[69]을 겪을 수 있다(단지 샘플링을 지원함).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Method & positive \\(v_{i}^{+}\\) & negative \\(v_{i}^{-}\\) & Objective \\(f(\\cdot)\\) \\\\ \\hline SPECTER [52] & \\((v_{i},v_{i}^{+})\\in\\mathcal{E}\\) & \\((v_{i},v_{i}^{-})\\notin\\mathcal{E}\\); & \\(\\max\\{\\|\\mathbf{h}_{v_{i}}-\\mathbf{h}_{v_{i}^{-}}\\|_{2}-\\|\\mathbf{h}_{v_{i}^{-}}- \\mathbf{h}_{v_{i}^{-}}\\|_{2}+m,0\\}\\) \\\\ \\hline SciNCL [53] & \\(\\|\\mathbf{h}_{v_{i}}-\\mathbf{h}_{v_{i}^{-}}\\|_{2}\\in(\\hat{k}^{+}-e^{+};k^{+}]\\) & \\(\\|\\mathbf{h}_{v_{i}}-\\mathbf{h}_{v_{i}^{-}}\\|_{2}\\in(\\hat{k}^{-}_{hard}-e^{-}_{ hard};k^{-}_{hard})\\) & \\(\\max\\{\\|\\mathbf{h}_{v_{i}}-\\mathbf{h}_{v_{i}^{-}}\\|_{2}-\\|\\mathbf{h}_{v_{i}^{-}}\\|_ {2}+m,0\\}\\) \\\\ \\hline Touchup-G [55] & \\((v_{i},v_{i}^{+})\\in\\mathcal{E}\\) & \\((v_{i},v_{i}^{-})\\notin\\mathcal{E}\\) & \\(\\log(\\mathbf{h}_{v_{i}}\\cdot\\mathbf{h}_{v_{i}^{+}})+\\log(1-\\mathbf{h}_{v_{i}} \\cdot\\mathbf{h}_{v_{i}^{-}})\\) \\\\ \\hline TwHN-BERT [57] & \\(\\cos(\\mathbf{x}_{v_{i}},\\mathbf{x}_{v_{i}^{-}})<k\\) & in-batch random & \\(-\\log\\frac{\\exp((\\mathbf{h}_{v_{i}}\\cdot\\mathbf{h}_{v_{i}^{-}})/\\eta)}{\\sum_{v_{i} ^{-}}\\exp((\\mathbf{h}_{v_{i}}\\cdot\\mathbf{h}_{v_{i}^{-}})/\\eta)}\\) \\\\ \\hline MICoL [60] & \\(v_{i}^{+}\\in N_{M}(v_{i})\\) & in-batch random & \\(-\\log\\frac{\\exp((\\mathbf{h}_{v_{i}}\\cdot\\mathbf{h}_{v_{i}^{-}})/\\eta)}{\\sum_{v_{i} ^{-}}\\exp((\\mathbf{h}_{v_{i}}\\cdot\\mathbf{h}_{v_{i}^{-}})/\\eta)}\\) \\\\ \\hline E2EG [61] & \\((v_{i},v_{i}^{+})\\in\\mathcal{E}\\) & \\((v_{i},v_{i}^{-})\\notin\\mathcal{E}\\) & \\(\\log(\\mathbf{h}_{v_{i}}\\cdot\\mathbf{h}_{v_{i}^{+}})+\\sum_{v_{i}^{-}}\\log(1- \\mathbf{h}_{v_{i}}\\cdot\\mathbf{h}_{v_{i}^{-}})\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE IV: A summarization of Graph-Aware LLM finentuning objectives on text-rich graphs. \\(v_{i}^{+}\\) and \\(v_{i}^{-}\\) denote a positive training node and a negative training node to \\(v_{i}\\) respectively.\n' +
      '\n' +
      '메모리 복잡도와 로컬 최소 [36] (데이터에 적합한 LMM) 문제에 관한 단일 홉 이웃을few.\n' +
      '\n' +
      '**2단계 훈련** 은 먼저 그래프에 LLM을 적용 한 다음 전체 LLM-GNN 계단식 파이프라인을 조정 하는 것을 의미 합니다. 이 전략은 더 높은 텍스트 표현 품질에 기여하는 LLM의 불충분한 훈련을 효과적으로 완화할 수 있다. GIANT[59]는 XR-트랜스포머[81]를 사용하여 이웃 예측을 수행할 것을 제안하고, 노드 분류를 위해 백-오브-워드 및 바닐라 BERT[22] 임베딩보다 더 나은 특징 벡터를 출력할 수 있는 LLM을 생성한다. LM-GNN [69]는 전체 LLM-GNN 파이프라인을 피니튜닝하기 전에 주어진 그래프 상에서 LLM을 워밍업하기 위해 그래프 인식 사전-파인-튜닝을 도입하고 상당한 성능 이득을 입증한다. SimToG[36]는 먼저 다운스트림 태스크 상에서 LLM들을 트레이닝한 다음 LLM들을 고정하고 GNN들을 트레이닝하는 간단한 프레임워크가 우수한 성능을 가져올 수 있다는 것을 발견한다. 그들은 또한 LLM을 튜닝하기 위해 효율적인 미세 조정 방법, _예를 들어_, LoRA[41]을 사용하는 것이 과적합 문제를 완화할 수 있다는 것을 발견한다. GaLM [82]는 LLM-GNN 캐스케이드 아키텍처를 사전 트레이닝하는 방법을 탐색한다.\n' +
      '\n' +
      '#### 5.2.2 Data Augmentation\n' +
      '\n' +
      '그것의 입증된 제로-샷 능력[44]으로, LLM은 LLM-GNN 캐스케이드 아키텍처에 대한 추가 텍스트 데이터를 생성하기 위한 데이터 증강에 사용될 수 있다. 의사 데이터를 생성하기 위해 LLM을 사용하는 철학은 NLP[90]에서 널리 탐구된다. LLM-GNN[64]은 텍스트 분산 네트워크에서 소수의 노드를 레이블링하고 의사 레이블을 사용하여 GNN을 미세 조정함으로써 제로 샷 노드 분류를 수행할 것을 제안한다. TAPE[71]는 LLM을 사용하여 예측 텍스트 및 설명 텍스트를 생성하는 방법을 제시하며, 이는 원본 텍스트 데이터와 비교하여 증강 텍스트 데이터의 역할을 한다. GNN에 입력하기 전에 증강 텍스트 및 원본 텍스트에 대한 텍스트 및 출력 특징을 각각 인코딩하기 위해 다음과 같은 중간 규모의 언어 모델이 채택된다. ENG[72]는 각 카테고리에 대해 라벨링된 노드들을 생성하고, 라벨링된 노드들과 다른 노드들 사이에 에지를 추가하고, 노드 분류를 위한 준지도 GNN 학습을 수행하는 아이디어를 제시한다.\n' +
      '\n' +
      '#### 5.2.3 Knowledge Distillation\n' +
      '\n' +
      'LLM-GNN 캐스케이드 파이프라인은 텍스트 정보와 구조 정보를 모두 캡처할 수 있다. 그러나 GNN은 이웃 샘플링을 수행해야 하고 LLM은 중심 노드와 이웃 노드 모두와 연관된 텍스트를 인코딩해야 하기 때문에 파이프라인은 추론 중에 시간 복잡성 문제를 겪는다. 간단한 해결책은 LLM-GNN 캐스케이드 파이프라인을 교사 모델로 서비스하고 이를 학생 모델로 LLM으로 증류하는 것이다. 이 경우, 추론 동안, 모델(순수한 LLM임)은 단지 중심 노드 상의 텍스트를 인코딩하고 시간 소모적인 이웃 샘플링을 피하면 된다. AdsGNN[80]은 교사 모델이 트레이닝된 후에 학생 모델의 출력들이 토폴로지를 보존하도록 강제하는 L2-손실을 제안한다. GraD[70]는 교사 모델을 최적화하고 학생 모델에 대한 능력을 증류하기 위해 증류 목표와 과제 목표를 포함한 세 가지 전략을 소개한다.\n' +
      '\n' +
      '#### 5.2.4 Discussion\n' +
      '\n' +
      'GNN이 그래프 인코딩에서 강력한 모델로 입증된다는 점을 감안할 때 "인코더로서의 LMM"은 그래프에서 LLM을 활용하는 가장 간단한 방법인 것 같다. "인코더로서의 LMs"에 대한 많은 연구가 논의되었지만 여전히 풀어야 할 열린 질문이 있다.\n' +
      '\n' +
      '**제한된 작업: 표현 학습을 넘어갑니다.** 현재 "인코더로서의 LMM" 메서드 또는 LLM-GNN 계단식 아키텍처는 GNN의 단일 임베딩 전파-집계 메커니즘을 고려할 때 주로 표현 학습에 중점을 두고 있으며, 이는 생성 작업(_e.g._, 노드/텍스트 생성)에 채택되지 않도록 합니다. 이 챌린지에 대한 잠재적인 해결책은 LLM 출력 토큰-레벨 표현들에 대한 GNN 인코딩을 수행하고, LLM-GNN 캐스케이드 모델 출력들에 기초하여 생성을 수행할 수 있는 적절한 디코더들을 설계하는 것일 수 있다.\n' +
      '\n' +
      '**낮은 효율성: 고급 지식 증류.** LLM-GNN 캐스케이드 파이프라인은 모델이 이웃 샘플링을 수행한 다음 각 이웃 노드에 대해 인코딩을 임베딩해야 하기 때문에 시간 복잡성 문제가 발생합니다. 학습된 LLM-GNN 모델을 빠른 추론을 위해 LLM 모델로 증류하는 방법을 탐색하는 방법이 있지만, LLM의 추론 자체가 시간이 많이 걸린다는 점을 감안할 때 충분하지 않다. 잠재적인 해결책은 모델을 훨씬 더 작은 LM 또는 MLP로 증류하는 것이다. 유사한 방법 [88]은 MLP 증류에 대한 GNN에서 효과적인 것으로 입증되었으며 LLM-GNN 캐스케이드 파이프라인에 대해서도 탐색할 가치가 있다.\n' +
      '\n' +
      '### _LLM as Aligner_\n' +
      '\n' +
      '이러한 방법들은 텍스트 인코딩을 위한 LLM 컴포넌트와 구조 인코딩을 위한 GNN 컴포넌트를 포함한다. 상기 두 컴포넌트는 동일하게 서비스되고 반복적으로 또는\n' +
      '\n' +
      '도. 4: (a) One-step Training, (b) Two-step Training, (c) Data Augmentation, (d) Knowledge Distillation을 포함하는 Encoder로서 LLM과 관련된 다양한 기법들의 예시.\n' +
      '\n' +
      '병렬로 형성하는 것을 특징으로 하는 반도체 소자의 제조 방법. LLM들은 텍스트 신호들을 GNN들에 제공할 수 있는 반면, GNN들은 구조 정보를 LLM들에 전달할 수 있기 때문에, LLM들과 GNN들은 서로 강화될 수 있다. LLM과 GNN이 상호 작용하는 방식에 따라 이러한 방법은 LLM-GNN 예측 정렬과 LLM-GNN 잠재 공간 정렬로 더 분류할 수 있다. 두 가지 범주의 방법에 대한 그림은 그림 5에서 찾을 수 있다.\n' +
      '\n' +
      '#### 5.3.1 LLM-GNN Prediction Alignment\n' +
      '\n' +
      '이는 그래프 상의 텍스트 데이터로 LLM을 트레이닝하고 그래프 상의 구조 데이터로 GNN을 반복적으로 트레이닝하는 것을 지칭한다. LLM은 텍스트 관점에서 노드들에 대한 라벨들을 생성하고 GNN 트레이닝을 위한 의사 라벨들로서 서비스할 것이고, GNN은 구조 관점에서 노드들에 대한 라벨들을 생성하고 LLM 트레이닝을 위한 의사 라벨들로서 서비스할 것이다.\n' +
      '\n' +
      '이러한 설계에 의해, 두 모달리티 인코더는 서로로부터 학습할 수 있고 최종 조인트 텍스트 및 그래프 인코딩에 기여할 수 있다. 이러한 방향으로, LTRN[58]은 언어 모델로서 BERT[22]를 채택하면서 구조 인코딩을 위한 개인화된 PageRank[95] 및 주의 메커니즘을 갖는 새로운 GNN 아키텍처를 제안한다. LLM과 GNN에 의해 생성된 의사 라벨은 훈련의 다음 반복을 위해 병합된다. GLEM[62]은 반복 트레이닝 프로세스를 의사-우도 변분 프레임워크로 공식화하고, 여기서 E-스텝은 LLM을 최적화하기 위한 것이고, M-스텝은 GNN을 트레이닝하기 위한 것이다.\n' +
      '\n' +
      '#### 5.3.2 LLM-GNN Latent Space Alignment\n' +
      '\n' +
      '이는 텍스트 인코딩(LLM)과 구조 인코딩(GNN)을 교차-모달리티 대조 학습과 연결하는 것을 의미한다[96]:\n' +
      '\n' +
      '\\\\[\\mathbf{h}_{d_{v_{i}}=\\text{LLM}(d_{v_{i}}),\\mathbf{h}_{v_{i}}= \\text{GNN}(\\mathcal{G}_{v}), \\tag{19}\\] \\[l(\\mathbf{h}_{d_{v_{i}},\\mathbf{h}_{v_{i}})=\\frac{\\text{Sim}(\\mathbf{h}_{d_{v_{i}},\\mathbf{h}_{v_{i}})}{\\sum_{j\\neq i}\\text{Sim}(\\mathbf{h}_{d_{v_{i}},\\mathbf{h}_{v_{i}})}\\] (20) \\[\\mathcal{L}=\\sum_{v_{i}\\in\\mathcal{G}}\\frac{1}(l( \\mathbf{h}_{d_{v_{i}},\\mathbf{h}\n' +
      '\n' +
      '유사한 철학이 비전-언어 공동 모달리티 학습에서 널리 사용된다[97]. 이러한 작업 라인을 따라 ConGrat[54]는 그래프 인코더로서 GAT[87]을 채택하고 언어 모델 인코더로서 MPNet[35] 및 DistillGPT[20]을 시도한다. 그들은 텍스트가 가져오는 노드와 노드가 생성하는 텍스트에 대한 가장 가능성 있는 두 번째, 세 번째 및 추가 선택에 대한 그래프 특정 요소를 추가하여 원래 InfoNCE 손실을 확장한다. GRENADE [56]은 노드 수준의 다중 모달리티 대비 목적 외에도 LLM과 GNN 사이에 계산된 이웃 유사성 분포를 최소화하는 KL-divergence 기반 이웃 수준 지식 정렬을 제안한다. G2P2[63]는 텍스트-요약 상호작용 및 노드-요약 상호작용을 추가함으로써 노드-텍스트 대비 학습을 더욱 확장한다. 그리고 제로 샷 분류를 위해 레이블 텍스트를 텍스트 모달리티에 사용하고, 소수의 쇼 분류를 위해 소프트 프롬프트를 사용한다. THLM[34]은 이질적인 텍스트-분산 네트워크 상에서 이질적인 GNN과의 대조적 학습에 의해 언어 모델을 사전 트레이닝하는 것을 제안한다. 사전 훈련된 LLM은 다운스트림 태스크에서 미세 조정될 수 있다.\n' +
      '\n' +
      '#### 5.3.3 Discussion.\n' +
      '\n' +
      '"LLMs as Aligners" 방법에서, 대부분의 연구는 반복 트레이닝(_i.e._, 예측 정렬) 또는 대조 트레이닝(_i.e._, 잠재 공간 정렬)을 통해 LLMs와 정렬되는 그래프 인코더로서 _hallow_ GNNs(_e.g._, GCN, GAT, 수천 개의 파라미터를 포함)를 채택하고 있다. LLM들(수백만 또는 수십억 개의 파라미터들을 갖는)은 강한 표현 능력을 갖지만, 얕은 GNN들(제한된 대표 능력을 갖는)은 LLM들과 GNN들 사이의 상호 학습 효과를 제약할 수 있다. 잠재적인 해결책은 확장될 수 있는 GNN을 채택하는 것이다[89]. 또한 LLM-GNN 상호 향상 프레임워크에서 LLM과 GNN에 대한 최상의 모델 크기 조합이 무엇인지 탐구하기 위한 심층 연구는 매우 중요하다.\n' +
      '\n' +
      '## 6 텍스트 쌍 그래프\n' +
      '\n' +
      '그래프는 화학 정보학[194], 물질 정보학[190], 생물 정보학[149], 컴퓨터 비전[150], 양자 컴퓨팅[151]과 같은 과학 분야에서 널리 퍼진 데이터 객체이다. 이러한 다양한 분야에서 그래프는 종종 중요한 그래프 수준의 텍스트 정보와 쌍을 이룬다. 예를 들어, 케민포매틱스의 분자 그래프는 독성, 수용성 및 투과성 특성과 같은 텍스트 특성으로 주석이 달린다[194, 190, 214]. 이러한 그래프에 대한 연구(과학적인 발견)는 텍스트 정보와 LLM의 채택에 의해 가속화될 수 있다. 이 섹션에서는 분자 그래프를 중심으로 그래프 캡션 그래프에 LLM의 적용을 검토한다. 섹션 3.2의 기술 분류에 따라 LLM을 예측자로 활용하는 방법을 조사하는 것으로 시작한다. 그런 다음 GNN을 LLM과 정렬하는 방법에 대해 논의한다. 조사된 모든 방법을 표 VI에 요약한다.\n' +
      '\n' +
      '### _LLM as Predictor_\n' +
      '\n' +
      '이 하위 섹션에서는 그래프 수준 작업에 대해 "LLM as Predictor"를 수행하는 방법을 검토한다. 기존 방법은 Graph as Sequence(그래프 데이터를 시퀀스 입력으로 처리) 및 Graph-Empowered LLMs(그래프를 인코딩하기 위한 모델 아키텍처 설계)로 분류할 수 있습니다.\n' +
      '\n' +
      '#### 6.1.1 Graph as Sequence\n' +
      '\n' +
      '텍스트 쌍을 이루는 그래프의 경우 그래프 입력에 기존 LLM을 활용하는 세 가지 단계가 있다. **1단계**: 규칙 기반 메서드를 사용하여 그래프를 순서대로 선형화합니다. **2단계**: 선형화된 시퀀스를 토큰화합니다. **3단계**: 특정 작업에 대해 서로 다른 LLM(_e.g._, Encoder-only, Encoder-Decoder, Decoder-only)을 트레인/파인튜닝합니다. 각 단계에 대해 다음과 같이 논의하겠습니다.\n' +
      '\n' +
      '도. 5: (a) LLM-GNN 예측 정렬 및 (b) LLM-GNN 잠재 공간 정렬을 포함하는 정렬기 방법으로서 LLM의 예시.\n' +
      '\n' +
      '**1단계: 규칙 기반 그래프 선형화.** 규칙 기반 선형화는 분자 그래프를 LLM에서 처리할 수 있는 텍스트 시퀀스로 변환합니다. 이를 달성하기 위해, 연구자들은 라인 표기의 형태로 인간의 전문지식을 기반으로 사양을 개발한다[152]. 예를 들어, SMILES(Simplified Molecular-Input Line-Entry System)[152]는 분자 그래프의 깊이-첫 번째 순회 동안 마주치는 노드들의 심볼들을 기록한다. IUPAC(International Union of Pure and Applied Chemistry)에 의해 생성된 InChI(International Chemical Identifier)[153]는 분자 구조를 보다 계층적인 정보를 갖는 고유한 문자열 텍스트로 인코딩한다. 표준화 알고리즘은 종종 표준 SMILES라고 하는 각 분자에 대해 고유한 SMILES를 생성한다. 그러나, 단일 분자에 대응하는 하나 이상의 SMILES가 존재하고 SMILES는 때때로 무효 분자를 나타낸다; 이러한 선형화된 시퀀스로부터 학습된 LLM은 구문 오류로 인해 무효 분자(_예를 들어,_ 부정확한 링 클로저 심볼 및 매칭되지 않는 괄호)를 쉽게 생성할 수 있다. 이를 위해 DeepSMILES[154]를 제안한다. 대부분의 경우 이 문제를 완화할 수 있지만 100% 견고성을 보장하지는 않습니다. 선형화된 문자열은 여전히 기본적인 물리적 제약을 위반할 수 있다. 이 문제를 완전히 해결하기 위해 유효한 분자 그래프를 일관되게 생성하는 SELFIES [155]가 도입되었다.\n' +
      '\n' +
      '**2단계: 토큰화.** 이러한 선형화된 시퀀스에 대한 토큰화 접근법은 일반적으로 언어와 무관합니다. 문장을 기반으로 문자 수준[176, 187]과 부분 문자열 수준[171, 178, 182, 183, 184, 185] 모두에서 작동한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l} \\hline \\hline Approach & Time & Category & Role of LM & LM Size & Focus & Task \\\\ \\hline GNN-LM [67] & 2021.10 & LLM as Encoder & TE & 237M & Task & LM \\\\ \\hline GLANT [59] & 2021.11 & LLM as Encoder & TE & 110M & Task & NC \\\\ \\hline TextGNN [79] & 2022.1 & LLM as Encoder & TE & 110M & Task & Search \\\\ \\hline AdSRNN [80] & 2022.4 & LLM as Encoder & TE & 110M & Task & Search \\\\ \\hline LM-GNN [69] & 2022.6 & LLM as Encoder & TE & 110M & Efficiency & NC, LP, EC \\\\ \\hline GraD [70] & 2023.4 & LLM as Encoder & TE & 110M/66M & Efficiency & LP, NC \\\\ \\hline TAPE [71] & 2023.5 & LLM as Encoder & TE, AUG & 129M/GPT-3.5 & Task & NC \\\\ \\hline SimToG [36] & 2023.8 & LLM as Encoder & TE & 80M/355M & Task & NC, LP \\\\ \\hline LLM-GNN [64] & 2023.10 & LLM as Encoder & ANN & GPT-3.5 & Task & NC \\\\ \\hline ENG [72] & 2023.10 & LLM as Encoder & TE, AUG & 80M/GPT-3.5 & Task & NC \\\\ \\hline SPECTER [52] & 2020.4 & LLM as Predictor & TE & 110M & Representation & NC, UAP, LP, Rec \\\\ \\hline GraphFormers [73] & 2021.5 & LLM as Predictor & TE, SE & 110M & Representation & LP \\\\ \\hline GreaseLM [68] & 2022.1 & LLM as Predictor & TE, SE & 355M & Task & QA \\\\ \\hline SciNCL [53] & 2022.2 & LLM as Predictor & TE & 110M & Representation & NC, UAP, LP, Rec \\\\ \\hline MICoL [60] & 2022.2 & LLM as Predictor & TE & 110M & Supervision & NC \\\\ \\hline LinkBERT [31] & 2022.3 & LLM as Predictor & TE & 110M & Pretraining & QA, NLU \\\\ \\hline Heterformer [74] & 2022.5 & LLM as Predictor & TE, SE & 110M & Representation & NC, LP \\\\ \\hline E2EG [61] & 2022.8 & LLM as Predictor & TE & 66M & Task & NC \\\\ \\hline TwHN-BERT [57] & 2022.9 & LLM as Predictor & TE & 110M/355M & Pretraining & NC, LP \\\\ \\hline Edgeformers [75] & 2023.1 & LLM as Predictor & TE, SE & 110M & Representation & NC, LP, EC \\\\ \\hline Patton [32] & 2023.5 & LLM as Predictor & TE, RE & 110M & Pretraining & NC, LP, Search \\\\ \\hline InstructGLM [47] & 2023.8 & LLM as Predictor & TE, SE & 250M/7B & Generalization & NC, LP \\\\ \\hline GNP [42] & 2023.9 & LLM as Predictor & TE, SE & 3B/11B & Task & QA \\\\ \\hline Touchp-G [55] & 2023.9 & LLM as Predictor & TE & 110M & Representation & NC, LP \\\\ \\hline DGTL [77] & 2023.10 & LLM as Predictor & TE, SE & 13B & Task & NC \\\\ \\hline GraphText [66] & 2023.10 & LLM as Predictor & TE, SE & GPT-3.5/4 & Task & NC \\\\ \\hline GraphGPT [46] & 2023.10 & LLM as Predictor & TE, SE & 7B & Generalization & NC \\\\ \\hline METERN [76] & 2023.10 & LLM as Predictor & TE, RE & 110M & Representation & NC, LP, Rec, RG \\\\ \\hline LTRN [58] & 2021.2 & LLM as Aligner & TE & 110M & Supervision & NC \\\\ \\hline GLEM [62] & 2023.1 & LLM as Aligner & TE & 110M & Task & NC \\\\ \\hline G2P2 [63] & 2023.5 & LLM as Aligner & TE & 110M & Supervision & NC \\\\ \\hline ConGat [54] & 2023.5 & LLM as Aligner & TE & 110M/82M & Representation & LP, LM, NC \\\\ \\hline GRENADE [56] & 2023.10 & LLM as Aligner & TE & 110M & Representation & NC, LP \\\\ \\hline THLM [34] & 2023.10 & LLM as Aligner & TE & 110B & Pretraining & NC, LP \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE V: Summary of large language models on text-rich graphs. Role of LM: “TE”, “SE”, “ANN” and “AUG” denote text encoder, structure encoder, annotator (labeling the node/edges), and augmentator (conduct data augmentation). Task: “NC”, “UAP”, “LP”, “Rec”, “QA”, “NLU”, “EC”, “LM”, “RG” denote node classification, user activity prediction, link prediction, recommendation, question answering, natural language understanding, edge classification, language modeling, and regression task.\n' +
      '\n' +
      '피스[163] 또는 BPE[162]. 추가적으로, RT[173]는 LM 트랜스포머 내의 회귀 태스크들을 취급하기 용이하게 하는 토큰화 접근법을 제안한다.\n' +
      '\n' +
      '**3단계: 선형화된 그래프를 LLM으로 인코딩합니다**. _ 인코더 전용 LLMs_입니다. 이전에 SciBERT[24] 및 BioBERT[189]와 같은 LLM은 분자와 관련된 자연어 설명을 이해하도록 과학 문헌에 대해 훈련되었지만 분자 그래프 구조를 이해할 수 없다. 이를 위해, 선형화된 SMILES 스트링을 갖는 분자 그래프 분류를 위해 SMILES-BERT[188] 및 MFBERT[185]가 제안된다. 과학적 자연 언어 기술에는 분자 그래프 구조에 대한 보충 역할을 할 수 있는 인간의 전문 지식이 포함되어 있기 때문에, 최근의 진보들은 이들에 대한 공동 이해를 강조한다[184, 168]: 선형화된 그래프 시퀀스는 원시 자연 언어 데이터에 연결되어 LLM에 입력된다. 구체적으로, KV-PLM[184]는 BERT[22]를 기반으로 구축되어 생의학적 맥락에서 분자 구조를 이해한다. RoBERTa[23]에서 개발된 CatBERTa[168]은 분자 그래프에 대한 촉매 특성 예측을 전문으로 한다.\n' +
      '\n' +
      '_Encoder-Decoder LLMs_. 인코더 전용 LLM은 생성 작업에 대한 기능이 부족할 수 있습니다. 본 절에서는 인코더-디코더 구조를 갖는 LLM에 대해 논의한다. 예를 들어, Chemformer[164]는 BART[29]와 유사한 아키텍처를 사용한다. 인코더로부터의 표현은 속성 예측 작업에 사용될 수 있고, 전체 인코더-디코더 아키텍처는\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l} \\hline \\hline \\multicolumn{1}{c}{Model} & \\multicolumn{1}{c}{Time} & \\multicolumn{1}{c}{LM Encoder} & \\multicolumn{1}{c}{Graph Encoder} & \\multicolumn{1}{c}{Gen. Decoder} & \\multicolumn{1}{c}{LM Size} & \\multicolumn{1}{c}{Task} \\\\ \\hline SMILES-BERT [188] & 2019.09 & Transformer [94] & Linearized & N.A. & 30M-563M & Classification \\\\ \\hline Text2Mol [122] & 2021.11 & SciBERT [24] & GCN & Transformer [94] & \\(\\geq\\)110M & Retrieval \\\\ \\hline MoICPT [186] & 2021.10 & N.A. & Linearized & GPT & 6M & Generation \\\\ \\hline Chemformer [164] & 2022.01 & BART [29] & Linearized & BART [29] & 45M-230M & Regression, Gen. \\\\ \\hline \\multirow{7}{*}{Constration} & KV-PLM [184] & 2022.02 & BERT [22] & Linearized & N.A & 110M-340M & Classifari, NER, Retrieval \\\\ \\cline{2-8}  & MFBERT [185] & 2022.06 & RoBERTa [23] & Linearized & N.A & 110M-340M & Classification \\\\ \\cline{2-8}  & MFBERT [185] & 2022.06 & RoBERTa [23] & Linearized & N.A & 110M-340M & Classification \\\\ \\cline{2-8}  & Galatica [187] & 2022.11 & N.A. & Linearized & Transformer [94] & 125M-120B & Classification \\\\ \\cline{2-8}  & MoITS [123] & 2022.12 & T5.1.1 [30] & Linearized & Transformer & 80M-780M & Gen., Cap. \\\\ \\cline{2-8}  & Text-Chem T5 [180] & 2023.05 & T5 [30] & Linearized & T5 [30] & 80M-780M & Classifari, Gen., Caption \\\\ \\cline{2-8}  & LLM-ICL [177] & 2023.05 & N.A. & Linearized & \\begin{tabular}{l} GPT-3.5/4 \\\\ LLaMA2 [19] \\\\ Galactica [187] \\\\ \\end{tabular} & \\(\\geq\\) 780M & \\begin{tabular}{l} Classification \\\\ Generation \\\\ Caption \\\\ \\end{tabular} \\\\ \\cline{2-8}  & GMLET [48] & 2023.05 & T5 [30] & GT & T5 [30] & 80M-780M & Classifari, Reg. \\\\ \\hline \\multirow{7}{*}{Constration} & MoXPT [187] & 2023.05 & N.A. & Linearized & GPT-2 & 350M & Classifari, Gen., Cap \\\\ \\cline{2-8}  & ChatMol [175] & 2023.06 & T5 [30] & Linearized & T5 [30] & 80M-780M & Gen., Cap. \\\\ \\cline{2-8}  & MoIEcPT [174] & 2023.06 & N.A. & Linearized & GPT-3.5 & N.A. & Gen., Cap. \\\\ \\cline{2-8}  & RT [173] & 2023.06 & N.A. & Linearized & XLNet [26] & 27M & Reg. Gen. \\\\ \\cline{2-8}  & LLM4Mol [172] & 2023.07 & RoBERTa [23] & Linearized & GPT-3.5 & N.A. & Classifari, Reg. \\\\ \\cline{2-8}  & LLMa-Mol [169] & 2023.07 & N.A. & Linearized & LLaMA [119] & 7B & Reg., Gene. \\\\ \\cline{2-8}  & LLMa-Mol [169] & 2023.07 & N.A. & Linearized & LLaMA [119] & 7B & Reg., Gene. \\\\ \\cline{2-8}  & LLMa-Mol [169] & 2023.07 & N.A. & Linearized & LLaMA [119] & 7B & Reg., Gene. \\\\ \\cline{2-8}  & ProJ2Text [170] & 2023.07 & N.A. & GNN & GPT-2 & 256M-760M & Caption \\\\ \\cline{2-8}  & ProJ2Text [170] & 2023.07 & N.A. & GNN & GPT-2 & 256M-760M & Caption \\\\ \\cline{2-8}  & CatBERTa [168] & 2023.10 & N.A. & Linearized & RoBERTa [23] & N.A. & Regression \\\\ \\cline{2-8}  & ReLU [166] & 2023.10 & N.A. & GNN & GPT-3.5 & N.A. & Classification \\\\ \\hline \\multirow{7}{*}{Constration} & MoMoMo [183] & 2022.12 & \\begin{tabular}{l} SelfBERT [24] \\\\ KV-PLM [184] \\\\ \\end{tabular} & \\begin{tabular}{l} MoT5 [123] \\\\ MGFlow [220] \\\\ \\end{tabular} & \\begin{tabular}{l} MoT5 [123] \\\\ \\end{tabular} & \\begin{tabular}{l} 82M-782M \\\\ Caption, Ret. \\\\ \\end{tabular} \\\\ \\cline{2-8}  & MoleculeLSTM [181] & 2022.12 & BART [29] & GIN & Linearized & BART [29] & 45M-230M & \\begin{tabular}{l} Classifari, Gen., \\\\ Caption \\\\ \\end{tabular} \\\\ \\cline{2-8}  & CLAMP [179] & 2023.05 & \\begin{tabular}{l} BioBERT [189], GNN \\\\ CLIP [97], T5 [30] \\\\ \\end{tabular} & \\begin{tabular}{l} GNN \\\\ Lin., Vec. \\\\ \\end{tabular} & N.A. & \\(\\leq\\)11B & \\begin{tabular}{l} Classification \\\\ Retrieval \\\\ \\end{tabular} \\\\ \\cline{2-8}  & MoFM [171] & 2023.06 & BERT [22] & GIN & MoITS [123] & 61.8M & \\begin{tabular}{l} Classifari, Gen. \\\\ Caption, Ret. \\\\ \\end{tabular} \\\\ \\cline{2-8}  & MoMoMo+v2 [182] & 2023.07 & SciBERT [24] & GIN & N.A & 82M-782M & Classification \\\\ \\cline{2-8}  & GIT-Mol [167] & 2023.08 & SciBERT [24] & \\begin{tabular}{l} GIN \\\\ Linearized \\\\ \\end{tabular} & MoITS [123] & 190M-890M & \\begin{tabular}{l} Classifari, Gen. \\\\ Caption \\\\ \\end{tabular} \\\\ \\cline{2-8}  & MoICA [176] & 2023.10 & Galactica [187] & GIN & N.A & 100M-877M &\n' +
      '\\begin{tabular}{l} Classifari, Reg., \\\\ Retrieval \\\\ \\end{tabular} \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VI: Model collection in Section 6 for text-captioned graphs. “In.” and “Vec.” represent Linearized Graph Encoding and Vectorized Graph Encoding. “Classif.”, “Reg.”, “NER”, “RE, “Retr.”, “Gen.”, “Cap.” represent classification, regression, named entity recognition, relation extraction, (molecule) graph retrieval, (molecule) graph generation, (molecule) graph captioning.\n' +
      '\n' +
      '분자 생성에 최적화될 수 있다. 다른 작업은 (분자로부터 텍스트 설명을 생성하는 것을 포함하는) 분자 캡셔닝 및 텍스트 기반 분자 생성(분자 그래프 구조가 자연 설명으로부터 생성되는 경우)에 초점을 맞춘다. 구체적으로, MoI75[123]은 이 두 작업에 적합한 T5[30]을 기반으로 개발된다. 다국어 문제로 분자-텍스트 변환을 공식화하고 T5 체크포인트를 사용하여 모델을 초기화한다. 모델은 두 개의 단일설 말뭉치, 즉 자연 언어 모달리티의 경우 거대 클린 크롤드 말뭉치(C4)[30], 분자 모달리티의 경우 100만 SMILES[164]에 대해 사전 훈련되었다. Text+Chem T5[180]는 입력 및 출력 도메인을 SMILES와 텍스트 모두를 포함하도록 확장하여, 텍스트 또는 반응 생성과 같은 더 많은 생성 기능을 위해 LLM을 잠금 해제한다. ChatMol[175]은 LLM의 상호 작용 능력을 이용하고 T5와의 멀티-턴 다이얼로그를 통해 분자 구조를 설계하는 것을 제안한다.\n' +
      '\n' +
      '_Decoder-only LLMs._ 디코더 전용 아키텍처는 발전 능력으로 인해 최근 LLM에 채택되었다. MoI6PT[186] 및 MoIXPT[178]은 분자 분류 및 생성에 사용되는 GPT-스타일 모델이다. 구체적으로, MoI GPT [186]은 스캐폴드를 사용하는 조건부 분자 생성 태스크에 초점을 맞추는 반면, MolXPT [178]은 분류 태스크를 예 또는 아니오 응답을 갖는 질문-답변 문제로 공식화한다. RT [173]은 XLNet [26]을 채택하여 분자 회귀 작업에 초점을 맞추고 있다. 그것은 회귀를 조건부 시퀀스 모델링 문제로 프레임화한다. Galactica [187]은 PubChem [194]에서 200만 개의 화합물에 대해 사전 훈련된 최대 1,200억 개의 매개변수를 가진 LLM 세트이다. 따라서 Galactica는 SMILES를 통해 분자 그래프 구조를 이해할 수 있었다. 지도 튜닝 데이터와 도메인 지식을 사용하여 연구자들은 LLaMA와 같은 일반 도메인 LLM을 적용하여 분자 그래프 구조를 인식하고 분자 작업을 해결한다[169]. 최근의 연구들은 또한 그래프에 대한 LLM의 문맥 내 학습 능력을 탐구한다. LLM-ICL [177]은 특성 분류에서 분자-텍스트 번역에 이르기까지 분자 도메인의 8개 작업에 걸쳐 LLM의 성능을 평가한다. MolReGPT[174]는 컨텍스트 학습을 개선하기 위해 유사한 구조 및 설명을 갖는 분자를 검색하는 방법을 제안한다. LLM4Mol[172]은 특징 추출기로서 LLM의 요약 능력을 활용하고 특정 예측 작업에 대해 더 작고 튜닝 가능한 LLM과 결합한다.\n' +
      '\n' +
      '#### 6.1.2 Graph-Empowered LLMs\n' +
      '\n' +
      '원래의 LLM 아키텍처(_i.e.,_ Transformers)를 채택하고 그래프를 LLM에 시퀀스로서 입력하는 방법과는 달리, 그래프로 구동되는 LLM은 텍스트 및 그래프 구조의 공동 인코딩을 수행할 수 있는 LLM 아키텍처를 설계하려고 시도한다. 일부 작업은 트랜스포머의 위치 인코딩을 수정합니다. 예를 들어 GIMLET [48]은 그래프의 노드를 토큰으로 처리합니다. 단일 Transformer를 사용하여 그래프 구조와 텍스트 시퀀스 \\([v_{1},v_{2},\\dots,v_{|\\mathcal{V}|},s_{|\\mathcal{V}|+1},\\dots,s_{|\\mathcal{V} |+|d_{\\mathcal{G}}|}]\\)을 모두 관리합니다. 여기서 \\(v\\in\\mathcal{V}\\)은 노드이고 \\(s\\in d_{\\mathcal{G}}\\)은 \\(\\mathcal{G}\\)과 연결된 텍스트의 토큰입니다. 다양한 데이터 양식과 상호 작용을 충족시키기 위한 위치 인코딩을 위한 세 가지 하위 인코딩 접근법이 있었다. 구체적으로, Graph Transformer로부터의 구조적 위치 인코딩(PE)을 채택하고 토큰 \\(i\\)과 \\(j\\) 사이의 상대적 거리를 다음과 같이 정의한다:\n' +
      '\n' +
      '\\[\\mathrm{PE}(i,j)=\\begin{cases}i-j&\\text{if }i,j\\in d_{\\mathcal{G}},\\\\ \\mathrm{GSD}(i,j)+\\mathrm{Mean}_{e_{k}\\in\\mathrm{SP}(i,j)}\\mathbf{x}_{e_{k}}&\\text{if }i,j\\in\\mathcal{V},\\\\ -\\infty&\\text{if }i\\in\\mathcal{V},j\\in d_{\\mathcal{G}},\\\\ 0&\\text{if }i\\in d_{\\mathcal{G}},j\\in\\mathcal{V}\\end{cases} \\tag{22}\\]\n' +
      '\n' +
      '여기서, \\(\\mathrm{GSD}\\)은 두 노드 사이의 그래프 최단 거리를 나타내고, \\(\\mathrm{Mean}_{k\\in\\mathrm{SP}(i,j)}\\)은 노드와 노드 사이의 최단 경로 \\(\\mathrm{SP}(i,j)\\)를 따라 에지 특징들 \\(\\mathbf{x}_{e_{k}}\\)의 평균 풀링을 나타낸다. GIMLET[48]은 노드 토큰들에 대해 양방향 주의력을 적응시키고 텍스트들이 노드들에 선택적으로 참석할 수 있게 한다. 이러한 설계들은 그래프 부분을 처리하는 트랜스포머의 서브모듈을 그래프 트랜스포머와 등가적으로 렌더링한다[141].\n' +
      '\n' +
      '그래프 및 텍스트 표현 사이의 상호 작용을 용이하게 하기 위해 교차 주의 모듈을 수정하는 다른 작업이 있다. 그래프 히든 상태 \\(\\mathbf{h}_{\\mathcal{G}}\\), 그것의 노드 레벨 히든 상태 \\(\\mathbf{H}_{v}\\) 및 텍스트 히든 상태 \\(\\mathbf{H}_{d_{\\mathcal{G}}}\\), Text2Mol[122]은 인코더의 히든 레이어에서 표현들 간의 상호작용을 구현하였고, Prot2Text[170]은 인코더와 디코더 사이의 레이어 내에서 이러한 상호작용을 구현하였다:\n' +
      '\n' +
      '\\[\\mathbf{H}_{d_{\\mathcal{G}}}=\\text{softmax}\\left(\\frac{\\mathbf{W}_{Q}\\mathbf{H}_{d_{\\mathcal{G}}}\\cdot(\\mathbf{W}_{K}\\mathbf{H}_{v})^{T}}{\\sqrt{d_{k}}} \\right)\\cdot\\mathbf{W}_{V}\\mathbf{H}_{v}, \\tag{23}\\]\n' +
      '\n' +
      '여기서 \\(\\mathbf{W}_{Q},\\mathbf{W}_{K},\\mathbf{W}_{V}\\)는 질의 모달리티(예: 시퀀스)와 키/값 모달리티(예: 그래프)를 주의 공간으로 변환하는 훈련 가능한 파라미터이다. 또한 Prot2Text [170]은 두 개의 훈련 가능한 매개 변수 행렬 \\(\\mathbf{W}_{1}\\)과 \\(\\mathbf{W}_{2}\\)을 사용하여 그래프 표현을 시퀀스 표현에 통합한다.\n' +
      '\n' +
      '\\[\\mathbf{H}_{d_{\\mathcal{G}}}=\\left(\\mathbf{H}_{d_{\\mathcal{G}}}+\\mathbf{1}_{|d _{\\mathcal{G}}|}\\mathbf{h}_{\\mathcal{G}}\\mathbf{W}_{1}\\right)\\mathbf{W}_{2}. \\tag{24}\\]\n' +
      '\n' +
      '#### 6.1.3 Discussion\n' +
      '\n' +
      '**시퀀스 Prior가 있는 LMM 입력**_첫 번째 과제는 고급 선형화 방법의 진전이 LLMs._의 발전과 함께 진행되지 않았다는 것입니다._ 2020년경, SELFIES와 같은 분자 그래프에 대한 선형화 방법은 상당한 문법적인 이점을 제공하지만, 그래프 기계 학습 및 언어 모델 커뮤니티의 고급 LM 및 LLM은 이러한 인코딩된 결과가 제안 전에 말뭉치를 사전 훈련하는 부분이 아니기 때문에 이를 완전히 활용하지 못할 수 있다. 결과적으로, 최근 연구[177]는 GPT-3.5/4와 같은 LLM이 SMILES에 비해 SELFIES 사용에 덜 능숙할 수 있음을 나타낸다. 따라서, LM-전용 및 LLM-전용 방법의 성능은 LLM들의 학습 파이프라인 동안 이러한 하드-코딩된 규칙들을 최적화할 방법이 없기 때문에, 오래된 선형화 방법들의 표현성에 의해 제한될 수 있다. 그러나 두 번째 도전은 선형화에 의해 그래프의 유도성 바이어스가 끊어질 수 있기 때문에 남아 있다._ 규칙 기반 선형화 방법은 서열 모델링을 위한 귀납적 편향을 도입하여 분자 그래프에 내재된 순열 불변 가정을 깨뜨린다. 검색 공간을 줄이기 위해 시퀀스 순서를 도입하여 태스크 난이도를 줄일 수 있다. 그러나, 모델 일반화를 의미하는 것은 아니다. 구체적으로, 단일 또는 상이한 접근법들로부터 단일 그래프에 대한 다수의 스트링 기반 표현들이 있을 수 있다. 수많은 연구[156, 157, 158, 159]는 이러한 데이터 증강 접근법이 그래프의 순열-불변 특성을 유지하기 위해 관리하기 때문에 동일한 분자의 서로 다른 문자열 기반 뷰에 대한 훈련이 순차 모델의 성능을 향상시킬 수 있음을 보여주었다. 이러한 장점은 또한 순열 불변 GNN으로 달성할 수 있으며, 복잡한 문자열 기반 데이터 증강 설계의 필요성을 줄임으로써 모델을 잠재적으로 단순화할 수 있다.\n' +
      '\n' +
      '**그래프 이전을 사용 하는 LLM 입력** 규칙 기반 선형화는 풍부한 노드 특성, 에지 특성 및 인접 행렬을 사용 하는 직접 그래프 표현에 비해 표현력이 낮고 일반화할 수 있는 것으로 간주 될 수 있습니다. [203]. 다양한 원자 특징은 원자 번호, 키랄성, 정도, 공식 전하, 수소 원자 수, 라디칼 전자 수, 혼성화 상태, 방향족성 및 고리 내 존재를 포함한다. 결합 특징은 결합의 유형(예를 들어, 단일, 이중, 또는 삼중), 결합의 입체화학(예를 들어, E/Z 또는 시스/트랜스) 및 결합이 결합되었는지 여부를 포함한다[204]. 각 특징은 분자 모델링 및 화학 정보학에 중요한 원자 특성과 구조에 대한 구체적인 정보를 제공한다. 분자 그래프 구조를 이진 벡터로 **벡터화** 한 다음, 이러한 벡터 맨 위에 매개 변수화된 다층 퍼셉트론 (MLP)을 적용 하 여 그래프 표현을 얻을 수 있습니다. 이러한 벡터화 접근법은 MACCS[200], ECFP[201], CDK 지문[202]과 같은 지문으로도 알려져 있으며, 인간 정의 규칙에 기초한다. 이 규칙들은 분자의 입력을 받아 0/1 비트로 구성된 벡터를 출력한다. 각 비트는 다양한 특성 예측에 사용될 수 있는 작용기와 관련된 특정 유형의 하부 구조를 나타낸다. 지문은 원자와 구조를 고려하지만 여전히 원시 그래프 구조에서 자동으로 학습하는 데 부족하다. GNN은 지문을 대체하거나 강화하기 위한 자동 특징 추출기 역할을 할 수 있다. 섹션 6.1.2에서 일부 특정 방법을 탐구하는 반면, 그래프 라플라시안 및 랜덤 워크 사전의 고유 벡터와 같은 다른 그래프 사전도 사용할 수 있다[142].\n' +
      '\n' +
      '**LLM 출력 for Prediction.** KV-PLM [184], SMILES-BERT [188], MFBERT [185], Chemformer [164]와 같은 LM은 마지막 계층의 출력 벡터에 예측 헤드를 사용합니다. 이러한 모델은 표준 분류 및 회귀 손실로 미세 조정되지만 완전한 아키텍처의 모든 매개변수 및 이점을 완전히 활용하지는 못할 수 있다. 이에 반해, 텍스트 생성 태스크로서 RT[173], MolXPT[178], Text+Chem T5[180] 프레임 예측과 같은 모델들이 있다. 이러한 모델들은 마스킹된 언어 모델링 또는 자기회귀 타겟으로 트레이닝되며, 이는 텍스트 내의 문맥 단어들에 대한 세심한 설계를 필요로 한다[173]. 구체적으로, LLM들의 인-컨텍스트 학습 능력을 활성화하기 위해 도메인 지식 명령들이 필요할 수 있고, 이에 의해 이들을 도메인 전문가들로 만들 수 있다[177]. 예를 들어, 가능한 템플릿은 {일반 설명}{태스크-특정 설명}{질문-답변 예시}{테스트 질문}의 네 부분으로 분할될 수 있다.\n' +
      '\n' +
      '**추론을 위한 LLM 출력** 분자 그래프의 문자열 표현은 일반적으로 LLM의 지식을 넘어서는 새롭고 심층적인 도메인 지식을 전달하므로 최근 작업 [174, 148, 147]도 분자 그래프의 특성을 예측하기 위한 지식 원본으로 사용하는 대신 LLM의 추론 능력을 활용하려고 시도합니다. ReLM[166]은 GNN을 활용하여 top-k 후보를 제시한 후, 문맥 내 학습을 위한 객관식 답변을 구성하는 데 사용되었다. ChemCrow[148]는 다양한 화학 도구를 구현하기 위해 화학제로서 LLM을 설계한다. 전문성이 집약된 영역에서 직접적인 추론을 피했다.\n' +
      '\n' +
      '### _LLM as Aligner_\n' +
      '\n' +
      '#### 6.2.1 Latent Space Alignment\n' +
      '\n' +
      '대비 학습 및 예측 정규화를 통해 GNN 및 LLM의 잠재 공간을 직접 정렬할 수 있다. 전형적으로, GNN으로부터의 그래프 표현은 모든 노드-레벨 표현들을 요약함으로써 판독될 수 있고, 시퀀스 표현은 [CLS] 토큰으로부터 획득될 수 있다. 먼저, GNN과 LLM으로부터 분리된 표현벡터를 \\(\\mathbf{h}_{\\mathcal{G}}\\)과 \\(\\mathbf{h}_{d_{\\mathcal{G}}}\\)으로 통일된 공간에 매핑하기 위해 일반적으로 MLP인 두 개의 프로젝션 헤드를 사용하여 이 공간 내에서 정렬한다. 구체적으로, MoMu[183] 및 MoMu-v2[182]는 각각의 분자 그래프에 대한 코퍼스로부터 두 문장을 검색한다. 학습하는 동안 그래프 데이터 증강을 분자 그래프에 적용하여 두 개의 증강된 뷰를 생성하였다. 결과적으로 \\(\\mathcal{G}\\)과 \\(d_{\\mathcal{G}}\\)의 4쌍이 존재한다. 각 쌍에 대해, 공간 정렬에 대한 대비 손실은 다음과 같다.\n' +
      '\n' +
      '\\[\\ell_{\\text{MoMu}}=-\\log\\frac{\\exp\\left(\\cos\\left(\\mathbf{h}_{\\mathcal{G}}, \\mathbf{h}_{d_{\\mathcal{G}}\\right)/\\tau\\right)}{\\sum_{\\tilde{d}_{\\mathcal{G}} \\neq d_{\\mathcal{G}}\\exp\\left(\\cos\\left(\\mathbf{h}_{\\tilde{\\mathcal{G}}}, \\mathbf{h}_{\\tilde{d}_{\\mathcal{G}}}\\right)/\\tau\\right)}, \\tag{25}\\]\n' +
      '\n' +
      '여기서 \\(\\tau\\)은 온도 하이퍼-파라미터이고 \\(\\tilde{d_{\\mathcal{G}}}\\)은 그래프와 쌍을 이루지 않는 수열을 나타낸다 \\(\\mathcal{G}\\). 분자STM [181]은 또한 분자 그래프 \\(\\mathcal{G}\\)과 해당 텍스트 \\(d_{\\mathcal{G}}\\) 사이의 표현 거리를 최소화하면서 분자와 관련 없는 설명 사이의 거리를 최대화하기 위해 대조적 학습을 적용한다. 구체적으로, EBM-NCE와 InfoNCE의 두 가지 대조적 학습 전략을 고려한다:\n' +
      '\n' +
      '\\begin{split}\\ell_{\\text{STM-EBM}}&=-\\frac{1}{2} \\left(\\mathbb{E}_{\\mathbf{h}_{\\mathcal{G}},\\mathbf{h}_{d_{\\mathcal{G}}}\\left[ \\log\\sigma\\left(\\mathbf{h}_{\\mathcal{G}}^{\\top}\\mathbf{h}_{d_{\\mathcal{G}} \\right)\\right]\\right.\\\\ &\\\\ left.+\\ mathbb{E}_{\\mathbf{h}_{\\mathcal{G}},\\mathbf{h}_{d_{ \\mathcal{G}}}\\left[\\log\\left(1-\\sigma\\left(\\mathbf{h}_{\\mathcal{G}}^{\\top} \\mathbf{h}_{d_{\\mathcal{G}}\\right)\\right]\\right)\\\\ &-\\frac{1}{2}\\left(\\mathbb{E}_{\\mathbf{h}_{\\mathcal{G}}^{\\top} \\mathbf{h}_{d_{\\mathcal{G}}\\right)\\left[\\log\\sigma\\left(\\mathbf{h}_{\\mathcal{G}}^{\\top} \\mathbf{G}}_{d_{\\mathcal{G}}\\right)\\right]\\right.\\\\ &-\\frac{1}{2}\\left(\\mathbf{h}_{\\mathcal{G}}^{\\top} \\mathbf{ left.+\\ mathbb{E}_{\\mathbf{h}_{\\mathcal{G}},\\mathbf{h}_{d_{ \\mathcal{G}}}\\left[\\log\\left(1-\\sigma\\left(\\mathbf{h}_{\\mathcal{G}}^{\\top} \\mathbf{h}_{d_{\\mathcal{G}}\\right)\\right)\\right]\\right)\\end{split} \\tag{26}\\]\n' +
      '\n' +
      '\\[\\begin{split}\\ell_{\\text{STM-Info}}=-\\frac{1}{2}\\mathbb{E}_{ \\mathbf{h}_{\\mathcal{G}},\\mathbf{h}_{d_{\\mathcal{G}}}}&\\left[ \\log\\frac{\\exp\\left(\\mathbf{h}_{\\mathcal{G}}^{\\top}\\mathbf{h}_{d_{\\mathcal{G}}\\right)}{\\sum_{\\tilde{d}_{\\mathcal{G}}}\\neq d_{\\mathcal{G}}\\neq d_{\\mathcal{G}}\\right)}\\right.\\\\ &\\left\\log\\frac{\\exp\\left(\\mathbf{h}_{\\mathcal{G}}^{\\top} \\mathbf{h}_{d_{\\mathcal{G}}\\right)}{\\sum_{\\tilde{\\mathcal{G}}\\neq\\tilde{\\mathcal{G\n' +
      '\n' +
      '분자STM[181]은 음수 그래프 또는 텍스트를 무작위로 샘플링하여 \\((\\mathcal{G},\\tilde{d})\\) 및 \\((\\mathcal{G},d)\\)의 음수 쌍을 구성한다. 유사하게, MolFM[171] 및 GIT-Mol[167]은 식과 같이 상호 정보 및 음성 샘플링으로 대조 손실을 구현한다. (27). 이 두 방법은 또한 교차 엔트로피를 사용하여 무작위로 순열된 그래프와 텍스트 입력이 동일한 분자에서 유래한 경우 예측할 수 있다는 가정으로 통일된 공간을 정규화한다. 그러나, 전술한 방법들은 태스크 라벨들을 레버리지할 수 없다. 분류 레이블 \\(y\\)이 주어지면 CLAMP [179]는 활성 분자를 매핑하여 각 분자 그래프 \\(\\mathcal{G}\\)에 대한 해당 분석 설명과 일치하도록 학습한다.\n' +
      '\n' +
      '\\[\\begin{split}\\ell_{\\text{CLAMP}}=& y\\log\\left(\\sigma\\left(\\tau^{-1}\\mathbf{h}_{\\mathcal{G}}^{T} \\mathbf{h}_{d_{\\mathcal{G}}\\right)\\right)\\\\ &+(1-y)\\log\\left(1-\\sigma\\left(\\tau^{-1}\\mathbf{h}_{\\mathcal{G}}^{T} \\mathbf{h}_{d_{\\mathcal{G}}\\right)\\right),\\end{split} \\tag{28}\\]CLAMP [179]에는 활성 분자와 해당 텍스트 설명이 잠재 공간에 함께 군집되도록 권장하는 레이블이 필요합니다. 두 모달리티 사이의 정렬을 발전시키기 위해, MolCA[176]는 분자-텍스트 투영 및 대비 정렬을 위해 쿼리 트랜스포머(Q-Former)[207]를 훈련시킨다. Q-former는 학습 가능한 질의 토큰 \\(\\{\\mathbf{q}_{k}\\}_{k=1}^{N_{q}}\\)을 초기화한다. 이러한 질의 토큰은 자기 주의로 갱신되고 교차 주의를 통해 GNN의 출력과 상호 작용하여 \\(k\\)-번째 질의 분자 표현 벡터 \\((\\mathbf{h}_{\\mathcal{G}})_{k}:=\\text{Q-Former}(\\mathbf{q}_{k})\\)를 얻는다. 쿼리 토큰은 텍스트와 동일한 자체 주의 모듈을 공유하지만 다른 MLP를 사용하여 Q-Former를 사용하여 텍스트 시퀀스 \\(\\mathbf{h}_{d_{\\mathcal{G}}}:=\\text{Q-Former}([\\text{CLSI}])\\)의 표현을 얻을 수 있습니다.\n' +
      '\n' +
      '\\log\\frac{\\exp\\left(\\max_{k}\\cos\\left((\\mathbf{ h}_{\\mathcal{G}})_{k},\\mathbf{h}_{d_{\\mathcal{G}}\\right)/\\tau\\right)}{\\sum_{ \\tilde{d}_{\\mathcal{G}}}\\neq(\\max_{k}\\cos\\left(\\mathbf{ h}_{d_{\\mathcal{G}}\\right)/\\tau\\right)} \\tag{29}\\] \\[\\ell_{\\text{t2g}=\\log\\frac{\\exp\\left(\\max_{k}\\cos\\left(\\mathbf{h}_{\\mathcal{G}})_{k}\\right)/\\tau\\right)}{\\sum_{ \\tilde{\\mathcal{G}}\\neq\\mathcal{G}}\\neq\\mathcal{G}}\\exp\\left(\\max_{k}\\cos\\left(\\mathbf{h}_{d_{\n' +
      '\n' +
      '#### 6.2.2 Discussion\n' +
      '\n' +
      '**더 큰 규모의 GNN.** GNN은 분자 표현 학습을 위해 원자 및 그래프 구조적 특징을 통합합니다 [147]. 구체적으로, Text2Mol[122]은 GCN[85]을 그의 그래프 인코더로서 활용하고, Morgan fingerprint에 기초하여 노드 특징들에 대한 고유 식별자들을 추출한다[201]. MoMu[183], MoMu-v2[182], MolFM[171], GIT-Mol[167], MolCA[176]는 GIN[205]을 백본으로 선호하는데, GIN이 Weisfeiler-Lehman 그래프 동형 테스트[206]만큼 표현적이고 강력한 것으로 입증되었기 때문이다. 2.2절에서 설명한 바와 같이 2016년 GCN[85], 2018년 GIN[205] 제안 이후 GNN을 더 깊고 일반화할 수 있으며 더 강력하게 만드는 데 주목할 만한 진전이 있었다. 그러나 대부분의 검토된 작업 [167, 171, 168, 183, 182]는 GIN[205]를 접근법의 개념 증명으로 사용하여 개발되었다. 이러한 사전 훈련된 GIN은 5개의 레이어와 300개의 숨겨진 차원을 특징으로 한다. GNN의 규모는 표현 벡터에서 의미 의미를 학습하는 데 병목 현상이 될 수 있으며 한 모달리티에 과도하게 의존하여 다른 모달리티를 무시하는 위험이 있다. 따라서, LLM에 필적하는 미래의 대규모 GNN 설계에 대해, 치수 크기를 스케일링하고 최근의 물질 발견 작업[231]에서 효과적인 것으로 입증된 더 깊은 레이어를 추가하는 것이 고려될 수 있다. 최근의 연구[139, 140, 142]는 또한 트랜스포머 인코더 계층들로 진보된 GNN들을 설계하는 것이 GNN들의 표현력을 향상시킬 수 있음을 시사한다. 따라서 앞서 언급한 방법을 결합할 때 실세계 및 대규모 GNN의 배치가 유망해야 한다.\n' +
      '\n' +
      '**GNN을 사용 하는 생성 디코더.** 흐름 기반 MoFlow 방법 [220]을 분자 그래프 생성기로 사용 하는 MoMu [183]을 제외 하 고 정렬 된 GNN은 종종 생성을 위한 디코더가 아닌 인코더로 사용 됩니다. 일반적인 디코더 구조는 대부분 텍스트 기반으로 SMILES와 같은 선형화된 그래프 구조를 생성한다. 이러한 접근법은 섹션 6.1.3에서 논의된 바와 같이 순열 불변 그래프의 특성을 거의 활용하지 못할 수 있다. 그래프 [219, 232]에 대한 생성 확산 모델의 최근 발전은 GNN을 사용하여 생성기를 설계하는 향후 작업에 활용될 수 있다.\n' +
      '\n' +
      '## 7 Applications\n' +
      '\n' +
      '### _Datasets, Splitting and Evaluation_\n' +
      '\n' +
      '세 가지 시나리오(즉, 순수 그래프, 텍스트가 풍부한 그래프 및 텍스트 쌍을 이루는 그래프)에 대한 데이터 세트를 요약하고 각각 표 IV, 표 VII 및 표 VIII에 보여준다.\n' +
      '\n' +
      '#### 7.1.1 Pure Graphs\n' +
      '\n' +
      '표 IV에서는 4절에서 논의된 순수한 그래프 추론 문제를 요약한다. 많은 문제가 일반성으로 인해 서로 다른 데이터 세트에서 공유되거나 재방문된다. NLGraph[124], LLMtoGraph[125] 및 GUC[126]는 연결성, 최단 경로 및 그래프 직경을 포함하는 일련의 표준 그래프 추론 문제를 연구한다. GraphQA [131]은 유사한 문제 세트를 벤치마킹하지만, 그래프 접지의 효과에 따라 실제 시나리오에서의 그래프를 추가로 설명한다. LM4DyG[128]는 시간적으로 진화하는 그래프에 대한 추론 작업에 초점을 맞춘다. 정확도는 주로 그래프 질문-답변 태스크로 공식화되기 때문에 가장 일반적인 평가 메트릭이다.\n' +
      '\n' +
      '#### 7.1.2 Text-Rich Graphs\n' +
      '\n' +
      '우리는 표 VII에서 텍스트가 풍부한 그래프에 대한 모델을 평가하기 위한 유명한 데이터 세트를 요약한다. 데이터 세트는 대부분 학술, 전자 상거래, 책, 소셜 미디어 및 위키피디아 도메인에서 왔습니다. 이러한 데이터 세트에 대한 모델을 평가하는 인기 있는 작업에는 노드 분류, 링크 예측, 에지 분류, 회귀 및 권장 사항이 포함된다. 노드/에지 분류를 위한 평가 메트릭은 Accuracy, Macro-F1, Micro-F1이 있다. 링크 예측 및 추천 평가를 위해 일반적으로 평균 상호 순위(Mean Reciprocal Rank, MRR), 정규 할인 누적 이득(Normalized Discounted Cumulative Gain, NDCG), 적중률(Hit Ratio, Hit)이 메트릭 역할을 한다. 회귀 작업에 대한 모델 성능을 평가하는 동안 사람들은 평균 절대 오차(MAE) 또는 평균 제곱근 오차(RMSE)를 채택하는 경향이 있다.\n' +
      '\n' +
      '#### 7.1.3 Text-Paired Graphs\n' +
      '\n' +
      '표 VIII는 텍스트-쌍 그래프 데이터세트(텍스트-이용가능 및 그래프-전용 데이터세트를 포함함)를 나타낸다. _데이터 분할_의 경우 옵션에는 임의 분할, 원본 기반 분할, 활동 절벽 [221] 및 스캐폴드 [222], 데이터 분산 [143]이 포함됩니다. 그래프 분류는 일반적으로 AUC[204]를 메트릭으로 채택하고, 회귀는 MAE, RMSE 및 R\\({}^{2}\\)[147]을 사용한다. 텍스트 생성 평가를 위해 사람들은 BLEU(Bilingual Evaluation Understudy) 점수를 사용하는 경향이 있으며, 분자 생성 평가를 위해 휴리스틱 평가 방법(유효성, 새로움 및 고유성을 포함한 요인에 기반함)이 채택된다. 그러나 BLEU 점수는 효율적이지만 정확성이 떨어지는 반면 휴리스틱 평가 방법은 [223]의 탄소 원자의 불필요한 추가와 같은 의도하지 않은 모드의 영향을 받는 문제가 있다는 점에 주목할 필요가 있다.\n' +
      '\n' +
      '### _Open-source Implementations_\n' +
      '\n' +
      '**포옹 얼굴.** HF Transformer는 Transformer 기반 언어 모델에 가장 인기 있는 Python 라이브러리입니다. 또한 데이터 세트에 쉽게 액세스하고 공유하기 위한 데이터 세트와 머신 러닝 모델 및 데이터 세트를 쉽게 평가하기 위한 평가라는 두 가지 추가 패키지도 제공합니다.\n' +
      '\n' +
      '**Fairseq.** Fairseq는 Transformers 기반 언어 모델을 위한 또 다른 오픈 소스 Python 라이브러리입니다.\n' +
      '\n' +
      '**PyTorch Geometric.** PyG는 그래프 기계 학습을 위한 오픈 소스 Python 라이브러리입니다. 다양한 집성 및 풀링 계층과 결합된 60가지 이상의 GNN 계층을 패키징합니다.\n' +
      '\n' +
      '**Deep Graph 라이브러리.** DGL은 그래프 기계 학습을 위한 또 다른 오픈 소스 Python 라이브러리입니다.\n' +
      '\n' +
      '**RDKit.** RDKit은 분자 그래프에 대 한 다양한 작업 및 시각화를 용이하게 하는 가장 인기 있는 오픈 소스 Chminformatics 소프트웨어 프로그램 중 하나입니다. 분자 그래프에 대한 선형화 구현과 같은 많은 유용한 API를 제공하여 쉽게 저장된 SMILES로 변환하고 이러한 SMILES를 다시 그래프로 변환한다.\n' +
      '\n' +
      '### _Practical applications_\n' +
      '\n' +
      '#### 7.3.1 Scientific Discovery\n' +
      '\n' +
      '**가상 스크리닝.** 약물 및 재료 설계를 위한 수많은 레이블이 지정되지 않은 분자 후보가 있을 수 있지만 화학자는 종종 화학 공간의 특정 영역에 있는 일부에만 관심이 있습니다. [226]. 기계 학습 모델은 연구자들이 사소한 후보를 자동으로 선별하는 데 도움이 될 수 있습니다. 그러나, 라벨링된 분자 데이터 세트는 종종 작은 크기와 불균형한 데이터 분포를 갖기 때문에 정확한 모델을 훈련시키는 것은 쉬운 작업이 아니다[143]. 데이터 희소성에 대해 GNN을 개선하려는 많은 노력이 있다[143, 147, 229]. 그러나 모델이 한 번도 훈련된 적이 없는 심층 도메인 지식을 일반화하고 이해하는 것은 불가능하지는 않더라도 어렵다. 따라서 텍스트는 지식의 보완적 원천이 될 수 있다. 대규모 과학 논문에서 과제 관련 내용을 발견하여 지침으로 사용하는 것은 정확한 가상 스크리닝 작업에서 GNN을 개선할 수 있는 큰 잠재력을 가지고 있다[48].\n' +
      '\n' +
      '**과학 가설 최적화** 분자 생성 및 최적화는 약물 및 물질 발견을 위한 화학 과학의 기본 목표 중 하나를 나타냅니다. [227]. 복합 분자와 같은 과학적 가설[228]은 GNN과 LLM의 공동 공간에서 표현될 수 있다. 그런 다음 텍스트 설명(인간 요구 사항)과 일치하고 화학적 타당성과 같은 구조적 제약을 준수하는 더 나은 가설을 잠재 공간에서 검색할 수 있다. 화학적 공간에는 \\(10^{60}\\) 이상의 분자[225]가 포함되어 있는 것으로 밝혀졌으며, 이는 습식 실험실 실험에서 탐사의 능력을 넘어서는 것이다. 가장 큰 도전 중 하나는 관련 없는 부분 공간에서 후보를 무작위로 생성하는 것이 아니라 고품질 후보를 생성하는 것이다. 다중 조건(텍스트, 숫자, 범주형)을 갖는 분자 생성은 이 문제를 해결할 가능성을 보여준다.\n' +
      '\n' +
      '**합성 계획.** 합성 디자인은 사용 가능한 분자에서 시작되며 일련의 반응을 통해 원하는 화학 화합물을 최종적으로 생성할 수 있는 일련의 단계를 계획하는 것을 포함합니다. [228]. 이 절차는 일련의 반응물 분자 및 반응 조건을 포함한다. 이 과정에서 그래프와 텍스트 모두 중요한 역할을 한다. 예를 들어, 그래프는 분자의 기본 구조를 나타낼 수 있는 반면, 텍스트는 반응 조건, 첨가제 및 용매를 설명할 수 있다. LLM은 또한 가능한 합성 경로를 직접 제안하거나 기존의 계획 툴 상에서 동작하기 위한 에이전트로서 기능함으로써 계획을 보조할 수 있다[148].\n' +
      '\n' +
      '#### 7.3.2 Computational Social Science\n' +
      '\n' +
      '컴퓨팅 사회과학에서 연구자들은 사람/사용자의 행동을 모델링하고 미래를 예측하는 데 활용할 수 있는 새로운 지식을 발견하는 데 관심이 있다. 사용자의 행동들 및 사용자들 사이의 상호작용들은 그래프들로서 모델링될 수 있으며, 여기서 노드들은 풍부한 텍스트 정보(_e.g._, 사용자 프로파일, 메시지들, 이메일들)와 연관된다. 아래 두 가지 예제 시나리오를 보여드리겠습니다.\n' +
      '\n' +
      '**전자 상거래.** 전자 상거래 플랫폼에는 사용자와 제품 간에 많은 상호 작용(예:_e.g._, 구매, 보기)이 있습니다. 예를 들어, 사용자는 제품을 보거나, 카트하거나, 구매할 수 있습니다. 또한, 사용자들, 제품들 및 이들의 상호작용들은 풍부한 텍스트 정보와 연관된다. 예를 들어, 제품에는 제목/설명이 있으며 사용자는 제품에 대한 리뷰를 남길 수 있습니다. 이 경우 노드는 사용자와 제품이고 에지는 상호 작용인 그래프를 구성할 수 있다. 노드와 모서리는 모두 텍스트와 연결됩니다. 텍스트 정보와 그래프 구조 정보(사용자 행동)를 모두 활용하여 사용자와 아이템을 모델링하고 복잡한 다운스트림 태스크(_e.g._, 아이템 추천[104], 번들 추천[105], 상품 이해[106])를 해결하는 것이 중요하다.\n' +
      '\n' +
      '**소셜 미디어.** 소셜 미디어 플랫폼에는 많은 사용자가 있으며 메시지를 통해 서로 상호 작용합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l} \\hline \\hline Text. & Data & Year & Task & \\# Nodes & \\# Edges & Domain & Source \\& Notes \\\\ \\hline \\multirow{8}{*}{\\begin{tabular}{} \\end{tabular} } & ogb-arxiv & 2020.5 & NC & 169.343 & 1,166,243 & Academic & OGB [204] \\\\  & ogb-products & 2020.5 & NC & 2,449,029 & 61,859,140 & E-commerce & OGB [204] \\\\  & ogb-papers110M & 2020.5 & NC & 111,059,956 & 1,615,685,872 & Academic & OGB [204] \\\\  & ogb-citation2 & 2020.5 & LP & 2,927,963 & 30,561,187 & Academic & OGBs [204] \\\\  & Cora & 2000 & NC & 2,708 & 5,429 & Academic & [9] \\\\  & Citeseer & 1998 & NC & 3,312 & 4,732 & Academic & [10] \\\\  & DBLP & 2023.1 & NC, LP & 5,259,858 & 36,630,661 & Academic & www.aminer.org/citation \\\\  & MAG & 2020 & NC, LP, Rec RG & \\(\\sim 10\\)M & \\(\\sim 50\\)M & Academic & multiple domains [11][12] \\\\  & Goodreads-books & 2018 & NC, LP & \\(\\sim 2\\)M & \\(\\sim 20\\)M & Books & multiple domains [13] \\\\  & Amazon-items & 2018 & NC, LP, Rec & \\(\\sim 15.5\\)M & \\(\\sim 100\\)M & E-commerce & multiple domains [14] \\\\  & ScIDocs & 2020 & NC, UAP, LP, Rec & - & - & Academic & [52] \\\\  & PubMed & 2020 & NC & 19,717 & 44,338 & Academic & [15] \\\\  & Wikidata5M & 2021 & LP & \\(\\sim 4\\)M & \\(\\sim 20\\)M & Wikipedia & [16] \\\\  & Twitter & 2023 & NC, LP & 176,279 & 2,373,956 & Social & [54] \\\\ \\hline \\multirow{4}{*}{\n' +
      '\\begin{tabular}{} \\end{tabular} } & Goodreads-reviews & 2018 & EC, LP & \\(\\sim 3\\)M & \\(\\sim 100\\)M & Books & multiple domains [13] \\\\  & Amazon-reviews & 2018 & EC, LP & \\(\\sim 15.5\\)M & \\(\\sim 200\\)M & E-commerce & multiple domains [14] \\\\ \\cline{1-1}  & Stackoverflow & 2023 & EC, LP & 129,322 & 281,657 & Social & [75] \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VII: Data collection in Section 5 for text-rich graphs. Task: “NC, “UAP,”, “LP”, “Rec”, “EC,” “RG” denote node classification, user activity prediction, link prediction, recommendation, edge classification, and regression task.\n' +
      '\n' +
      '이메일 등등. 이 경우, 우리는 노드들이 사용자이고 에지가 사용자들 사이의 상호작용인 그래프를 구축할 수 있다. 노드들(_e.g._, 사용자 프로파일) 및 에지들(_e.g._, 메시지들)과 연관된 텍스트가 있을 것이다. 흥미로운 연구 질문은 친구 추천[107], 사용자 분석[108], 커뮤니티 탐지[109]를 위해 사용자를 깊이 이해하기 위해 공동 텍스트 및 그래프 구조 모델링을 수행하는 방법이다.\n' +
      '\n' +
      '#### 7.3.3 특정 도메인\n' +
      '\n' +
      '많은 특정 도메인에서 텍스트 데이터는 상호 연결되어 그래프 형식으로 놓여 있다. 그래프의 구조 정보는 텍스트 단위를 더 잘 이해하고 고급 문제 해결에 기여할 수 있도록 활용될 수 있다.\n' +
      '\n' +
      '**학술 도메인.** 학술 도메인에서 네트워크 [11]은 논문을 노드로 구성 하 고 그 관계 (예: _, 인용, 저자 등)를 에지로 구성 합니다. 이러한 네트워크 상의 논문들에 대해 학습된 표현은 논문 추천[101], 논문 분류[102], 및 저자 식별[103]에 활용될 수 있다.\n' +
      '\n' +
      '**법적 도메인.** 법적 도메인에서 판사가 제공한 의견은 항상 이전 사례에 대해 제공된 의견에 대한 참조를 포함합니다. 이러한 시나리오에서 사람들은 의견들 간의 인용 관계에 기초하여 오피니언 네트워크를 구성할 수 있다[98]. 텍스트 및 구조 정보를 모두 갖는 이러한 네트워크 상에서 학습된 표현들은 절 분류[99] 및 의견 추천[100]에 활용될 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline  & Data & Date & Task & Size & Tested Model & Source \\& Notes \\\\ \\hline \\multirow{9}{*}{\\begin{tabular}{} \\end{tabular} } & CheShML2023 [196] & 20223 & Various & 2.487\\({}^{\\text{2}}\\)20;387 & Various & Drug-like \\\\ \\cline{2-6}  & PatChem [194] & 2019 & Various & 964\\({}^{\\text{2}}\\)20;387 & Various & Biomedical \\\\ \\cline{2-6}  & PC32MR [176] & 2023 & PT & \\multirow{2}{*}{30/M2} & \\multirow{2}{*}{MoCA [176]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & MoMCP-FP [178] & 2023 & PT & \\multirow{2}{*}{30/M2} & \\multirow{2}{*}{MoMCP [178]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & Che-bio [48] & 2023 & PT & \\multirow{2}{*}{30/M2} & \\multirow{2}{*}{MoMCP [178]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & Che-bio [48] & 2023 & PT & \\multirow{2}{*}{30/M2} & \\multirow{2}{*}{MoMCP [178]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & Che-bio [48] & 2023 & PT & \\multirow{2}{*}{30/M2} & \\multirow{2}{*}{MoMCP [178]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & Che-bio [48] & 2023 & PT & \\multirow{2}{*}{30/M2} & \\multirow{2}{*}{MoMCP [178]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & Che-bio [48] & 2023 & PT & \\multirow{2}{*}{30/M2} & \\multirow{2}{*}{MoMCP [178]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & Che-bio [48] & 2023 & PT & \\multirow{2}{*}{30/M2} & \\multirow{2}{*}{MoMCP [178]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & PC22MR [179] & 2023 & PT & \\multirow{2}{*}{223/M2} & \\multirow{2}{*}{CLAMP [179]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & PCSTM [181] & 2022 & PT & \\multirow{2}{*}{281/K1} & \\multirow{2}{*}{MoleculeSTM [181]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & PCdes [194] & 2022 & \\multirow{2}{*}{202} & \\multirow{2}{*}{Exp.} & \\multirow{2}{*}{18/K1} & \\multirow{2}{*}{K1} & \\multirow{2}{*}{K1/K1} \\\\  & & & & & & \\\\ \\cline{2-6}  & PCdes [194] & 2022 & \\multirow{2}{*}{Exp.} & \\multirow{2}{*}{18/K1} & \\multirow{2}{*}{K1/K1} & \\multirow{2}{*}{K1/K1} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & CheB8-20 [122] & 2021 & \\multirow{2}{*}{Exp.} & \\multirow{2}{*}{30/M2} & \\multirow{2}{*}{MoMCP [178]} & \\multirow{2}{*}{MoMCP [178]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & & & & & & \\\\ \\cline{2-6}  & Cip. & & & & & \\\\ \\hline \\multirow{9}{*}{\\begin{tabular}{} \\end{tabular} } & ZINC15 [197] & 2015 & PT & 120M2 & Various & \\multirow{2}{*}{Twilogs [183]} & \\multirow{2}{*}{Drop-like} \\\\ \\cline{2-6}  & CDB13 [198] & 2009 & PT & 977M2 & MFBERT [188] & & Drug-like \\\\ \\cline{2-6}  & PS-Mdd [192] & 2021 & CC & 27,0652 & CLAMP [179] & & ChEMRRL [199] \\\\ \\cline{2-6}  & PCBA [208] & 2018 & CC & 439,8632 & SMILES-BERT [188], CMLET [48] & PubChem-Bio [194] \\\\ \\cline{2-6}  & MUV [208] & 2018 & CC & 93,1272 & Modus [183], CMLET [48], MoMFM [171] & PubChem-Bio [194] \\\\ \\cline{2-6}  & & & & & & \\\\ \\cline{2-6}  & HV [208] & 2018 & CC & 41,9132 & \\multirow{2}{*}{MoMCP [183], CMLET [48], MoMFM [171]} & \\multirow{2}{*}{Due Therapous [194]} \\\\  & & & & & & & \\\\ \\cline{2-6}  & & & & & & \\\\ \\cline{2-6}  & HV [208] & 2018 & CC & 16,8962 & CMLET [48] & & Pharmacicinete \\\\ \\cline{2-6}  & & & & & & & \\\\ \\cline{2-6}  & & & & & & & \\\\ \\cline{2-6}  & & & & & & & \\\\ \\cline{2-6}  & & & & & & & \\\\ \\cline{2-6}  & BACE [208] & 2018 & CC & 1,8222 & \\multirow{2}{*}{ChEMT [48], CMLET [48], MMFM [171],} & \\multirow{2}{*}{Bioting for} \\\\  & & & & & & & \\\\ \\cline{2-6}  & & & & & & & \\\\ \\cline{2-6}  & & & & & & & \\\\ \\cline{2-6}  & & & & & & & \\\\ \\cline{2-6}  & & & & & & & \\\\ \\cline{2-6}  & & & & & & & \\\\ \\hline \\multirow{9}{*}{*}{\n' +
      '\\begin{tabular}{} \\end{tabular} } & BBBP [208] & 2018 & CC & 2,0832 & \\multirow{2}{*}{2,0832} & \\multirow{2}{*}{MoM2} & \\multirow{2}{*}{1,1,124, 17, 17, 17, 18, 19, 1\n' +
      '\n' +
      '## 8 Future direction\n' +
      '\n' +
      '**더 나은 벤치마크 데이터 세트.** 순수 그래프의 경우 대부분의 기존 벤치마크는 이질적인 그래프 또는 시공간 그래프에 대한 평가가 부족하지만 동질적인 그래프에서 LLM의 추론 능력을 평가하기 위해 구성됩니다. 텍스트가 풍부한 그래프의 경우 표 VII에 요약된 바와 같이 대부분의 벤치마크 데이터 세트는 학술 도메인과 전자 상거래 도메인에서 가져온 것이다. 그러나 현실 세계에서 텍스트가 풍부한 그래프는 여러 도메인(예: 법적 및 건강)에 걸쳐 편재한다. 실제 시나리오에서 LLM을 종합적으로 평가하기 위해서는 보다 다양한 데이터 세트가 필요하다. 텍스트 쌍을 이루는 그래프의 경우 표 VIII에 요약된 바와 같이 화학에서 다양한 기계 학습 작업을 다루는 포괄적인 데이터 세트가 부족하다. 많은 수의 과학 논문을 사용할 수 있지만 즉시 사용할 수 있는 형식으로 전처리하고 특정 분자 그래프 데이터 포인트와 페어링하는 것은 번거롭고 어려운 작업으로 남아 있다.\n' +
      '\n' +
      '**LLM이 있는 광범위한 작업 공간.** 그래프 작업에 대 한 LLM의 성능에 대 한 보다 포괄적인 연구는 미래에 대 한 가능성을 보유 합니다. 인코더 접근법으로서의 LLM은 텍스트가 풍부한 그래프에 대해 탐구되었지만 텍스트가 포함된 분자 그래프에 대한 적용은 아직 확장되지 않았다. 유망한 방향에는 데이터 증강 및 지식 증류를 위해 LLM을 사용하여 다양한 텍스트 쌍을 이루는 그래프 작업에 대한 도메인별 GNN을 설계하는 것이 포함된다. 또한 그래프 생성은 텍스트 쌍을 이루는 그래프에서 접근되었지만 텍스트가 풍부한 그래프(_i.e._, 공동 텍스트 및 그래프 구조 생성 방법)에 대해서는 여전히 열려 있는 문제로 남아 있다.\n' +
      '\n' +
      '**Multi-Modal Foundation Models.** 한 가지 열린 질문은 "하나의 Foundation 모델을 사용하여 다른 모달리티를 통합해야 하며 어떻게 해야 하나요?"입니다. 모달리티에는 텍스트, 그래프 및 이미지도 포함될 수 있습니다. 예를 들어, 분자는 그래프로 표현되고, 텍스트로 설명되고, 이미지로 촬영될 수 있고, 생성물은 그래프의 노드로서 취급되고, 제목/묘사와 연관되고, 이미지와 결합될 수 있다. 모든 모달리티에 대해 공동 인코딩을 수행할 수 있는 모델을 설계하는 것은 유용하지만 도전적일 것이다. 이 문제는 그래프 양식의 데이터가 매우 다양하다는 점을 감안할 때 훨씬 더 어렵다(예: 분자 그래프는 소셜 네트워크와 상당히 다르다). 또한, 획일적인 기반 모델을 구축하는 노력과 다양한 도메인에 대한 모델 아키텍처를 맞춤화하는 노력 사이에는 항상 긴장이 있었다. 따라서 통합된 아키텍처가 다른 데이터 유형에 적합한지 또는 도메인에 따라 모델 설계를 맞춤화할 필요가 있는지 묻는 것은 흥미롭다. 이 질문에 올바르게 대답하면 불필요한 시도로부터 경제적, 지적 자원을 절약할 수 있으며 그래프 관련 과제에 대한 더 깊은 이해도를 밝힐 수 있다.\n' +
      '\n' +
      '**그래프에서 효율적인 LLMs.** LLM은 그래프에서 학습할 수 있는 강력한 기능을 보여주었지만 그래프 선형화 및 모델 최적화 측면에서 비효율적인 문제를 겪습니다. 한편으로 섹션 5.1.1 및 6.1.1에서 논의된 바와 같이, 많은 방법은 그래프를 LLM에 입력될 수 있는 시퀀스로 전달하는 데 의존한다. 그러나, 전달된 시퀀스의 길이는 그래프의 크기가 증가함에 따라 상당히 증가할 것이다. LLM은 항상 최대 시퀀스 입력 길이를 가지며 긴 입력 시퀀스는 더 높은 시간 및 메모리 복잡성으로 이어질 것이기 때문에 이것은 과제를 제기한다. 한편, LLMs 자체를 최적화하는 것은 계산적으로 비용이 많이 든다. LoRA와 같은 일반적인 효율적인 튜닝 방법이 제안되었지만 그래프 인식 LLM 효율적인 튜닝 방법에 대한 논의는 부족하다.\n' +
      '\n' +
      '**그래프에서 일반화 가능하고 강력한 LLMs.** 또 다른 흥미로운 방향은 그래프에서 LLM의 일반화 가능성과 견고성을 탐구하는 것입니다. 일반화 가능성은 하나의 도메인 그래프에서 학습된 지식을 다른 도메인으로 전달하는 능력을 갖는 것을 의미하고, 견고성은 난독화 및 공격에 대해 일관된 예측을 갖는 것을 의미한다. LLM은 텍스트 처리에서 강력한 일반화 가능성과 견고성을 입증했지만 이러한 능력이 그래프 데이터에 대해 존재하는지 여부는 여전히 열려 있는 문제이다.\n' +
      '\n' +
      '**그래프에서 동적 에이전트로서의 LLM.** LLM은 텍스트를 생성하는 데 있어 고급 기능을 보여주었지만 정확한 매개 변수 지식의 부족으로 인해 LLM의 원패스 생성은 환각 및 잘못된 정보 문제를 겪습니다. 단순히 컨텍스트에서 검색된 지식을 늘리는 것은 검색기의 용량으로 인해 병목 현상도 발생한다. 많은 실제 시나리오에서 학술 네트워크 및 위키피디아와 같은 그래프는 지식 유도 추론을 위해 인간이 동적으로 찾는다. 이러한 동적 에이전트의 역할을 시뮬레이션하면 LLM이 다중 홉 추론을 통해 관련 정보를 보다 정확하게 검색하여 답변을 수정하고 환각을 완화하는 데 도움이 될 수 있다.\n' +
      '\n' +
      '## 9 Conclusion\n' +
      '\n' +
      '본 논문에서는 그래프에 대한 대규모 언어 모델에 대한 포괄적인 검토를 제공한다. 먼저 LMs가 채택될 수 있는 그래프 시나리오를 분류하고 그래프 기법에 대한 대규모 언어 모델을 요약한다. 그런 다음 각 시나리오 내에서 방법에 대한 철저한 검토, 분석 및 비교를 제공한다. 또한 사용 가능한 데이터 세트, 오픈 소스 코드 베이스 및 여러 응용 프로그램을 요약합니다. 마지막으로, 그래프 상의 대규모 언어 모델에 대한 향후 방향을 제시한다.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      '이것은 이 기사에 대한 귀하의 작업을 지원한 개인과 기관에 감사하기 위해 참고 문헌 앞의 간단한 단락일 것이다.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      '* [9] McCallum, A.K., Nigam, K., Rennie, J. and Seymore, K., "Automating the construction of internet portals with machine learning," in _Information Retrieval_, _3, pp.127-163, 2003.\n' +
      '* [10] Giles, C.L., Bollacker, K.D. and Lawrence, S., "CiteSeer: An automatic citation indexing system," in _Proceedings of the third ACM conference on Digital libraries ( pp. 89-98)_, 1998.\n' +
      '* [11] Wang, K., Shen, Z., Huang, C., Wu, C.H., Dong, Y. and Kanakia, A., "Microsoft academic graph: When experts are not enough," in _Quantitative Science Studies, 1(1), pp.396-413_, 2020.\n' +
      '* [12] Zhang, Y., Jin, B., Zhu, Q., Meng, Y. and Han, J., "The Effect of Metadata on Scientific Literature Tagging: A Cross-Field Cross-Model Study," in _WWW_, 2023.\n' +
      '* [13] Wan, M. and McAuley, J., "Item recommendation on monotonic behavior chains," in _Proceedings of the 12th ACM conference on recommender systems_, 2018.\n' +
      '* [14] Ni, J., Li, J. and McAuley, J., "Justifying recommendations using distantly-labeled reviews and fine-grained aspects," in _EMNLP-IJCNLP_, 2019.\n' +
      '* [15] Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B. and Eliassi-Rad, T., "Collective classification in network data," in _AI magazine, 29(3), pp.93-93_, 2008.\n' +
      '* [16] Wang, X., Gao, T., Zhu, Z., Zhang, Z., Liu, Z., Li, J. and Tang, J., "KEPLER: A unified model for knowledge embedding and pre-trained language representation," in _TACL_, 2021.\n' +
      '* [17] Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., & Philip, S. Y., "A comprehensive survey on graph neural networks," in _IEEE transactions on neural networks and learning systems, 32(1), 4-24_, 2020.\n' +
      '* [18] Liu, J., Yang, C., Lu, Z., Chen, J., Li, Y., Zhang, M., Bai, T., Fang, Y., Sun, L., Yu, P.S. and Shi, C., "Towards Graph Foundation Models: A Survey and Beyond," in _arXiv preprint arXiv:2310.11829_, 2023.\n' +
      '* [19] Pan, S., Luo, L., Wang, Y., Chen, C., Wang, J. and Wu, X., "Unifying Large Language Models and Knowledge Graphs: A Roadmap," in _arXiv preprint arXiv:2306.08302_, 2023.\n' +
      '* [20] Senh, V., Debut, L., Chaumond, J. and Wolf, T., "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.," in _arXiv preprint arXiv:1910.01108_, 2019.\n' +
      '* [21] Wang, Y., Le, H., Gontmez, A.D., Bui, N.D., Li, J. and Hoi, S.C., "Coded5+: Open code large language models for code understanding and generation.," in _arXiv preprint arXiv:2305.07922_, 2023.\n' +
      '* [22] Devlin, J., Chang, M.W., Lee, K. and Toutanova, K., "Bert: Pre-training of deep bidirectional transformers for language understanding," in _NAACL_, 2019.\n' +
      '* [23] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. and Stoyanov, V., "Roberta: A robustly optimized bert pretraining approach," in _arXiv preprint arXiv:1907.11692_, 2019.\n' +
      '* [24] Beltagy, I., Lo, K. and Cohan, A., "SciBERT: A pretrained language model for scientific text," in _arXiv preprint arXiv:1903.10676_, 2019.\n' +
      '* [25] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantam, A., Shyam, P., Sastry, G., Askell, A., and Agarwal, "Language models are few-shot learners," in _NeurIPS_, 2020.\n' +
      '* [26] Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R. and Le, Q.V., "Mnet: Generalized autoregressive pretraining for language understanding," in _NeurIPS_, 2019.\n' +
      '* [27] Clark, K., Luong, M.T., Le, Q.V. and Manning, C.D., "Electra: Pre-training text encoders as discriminators rather than generators," in _ICLR_, 2020.\n' +
      '* [28] Meng, Y., Xiong, C., Bajaj, P., Bennett, P., Han, J. and Song, X., "Coco-lm: Correcting and contrasting text sequences for language model pretraining," in _NeurIPS_, 2021.\n' +
      '* [29] Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V. and Zettlemoyer, L., "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension," in _ACL_, 2020.\n' +
      '* [30] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W. and Liu, P.J., "Exploring the limits of transfer learning with a unified text-to-text transformer," in _JMLR_, 2020.\n' +
      '* [31] Yasunaga, M., Leskovec, J. and Liang, P., "LinkBERT: Pretraining Language Models with Document Links," in _ACL_, 2022.\n' +
      '* [32] Jin, B., Zhang, W., Zhang, Y., Meng, Y., Zhang, X., Zhu, Q. and Han, J., "Patton: Language Model Pretraining on Text-Rich Networks," in _ACL_, 2023.\n' +
      '* [33] Zhang, X., Malkov, Y., Florez, O., Park, S., McWilliams, B., Han, J. and El-Kishky, A., "TwHIN-BERT: a socially-enriched pre-trained language model for multilingual Tweet representations," in _KDD_, 2023.\n' +
      '* [34] Zou, T., Yu, L., Huang, Y., Sun, L. and Du, B., "Pretraining Language Models with Text-Attributed Heterogeneous Graphs," in _arXiv preprint arXiv:2310.12580_, 2023.\n' +
      '* [35] Song, K., Tan, X., Qin, T., Lu, J. and Liu, T.Y., "Mpnnet: Masked and permuted pre-training for language understanding," in _NeurIPS_., 2020.\n' +
      '* [36] Duan, K., Liu, Q., Chua, T.S., Yan, S., Ooi, W.T., Xie, Q. and He, J., "Simteg: A frustratingly simple approach improves textual graph learning," in _arXiv preprint arXiv:2308.02565._, 2023.\n' +
      '* [37] Kasnecl, E., Sessler, U., Groh, G., Gunnemann, S., Hullermeier, E. and Krusche, S., "ChatGPT for good? On opportunities and challenges of large language models for education," in _Learning and individual differences_, 103., 2023.\n' +
      '* [38] Lester, B., Al-Rfou, R. and Constant, N., "The power of scale for parameter-efficient prompt tuning," in _EMNLP_, 2021.\n' +
      '* [39] Li, X.L. and Liang, P., "Prefix-tuning: Optimizing continuous prompts for generation," in _ACL_, 2021.\n' +
      '* [40] Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M. and Gelly, S., "Parameter-efficient transfer learning for NLP," in _ICML_, 2019.\n' +
      '* [41] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. and Chen, W., "Lora: Low-rank adaptation of large language models," in _ICLR_, 2022.\n' +
      '* [42] Tian, Y., Song, H., Wang, Z., Wang, H., Hu, Z., Wang, F., Chawla, N.V. and Xu, P., "Graph Neural Prompting with Large Language Models," in _arXiv preprint arXiv:2309.15427._, 2023.\n' +
      '* [43] Chai, Z., Zhang, T., Wu, L., Han, K., Hu, X., Huang, X. and Yang, Y., "GraphLLM: Boosting Graph Reasoning Ability of Large Language Model," in _arXiv preprint arXiv:2310.05845._, 2023.\n' +
      '* [44] Wei, J., Bosma, M., Zhao, V.Y., Guu, K., Yu, A.W., Lester, B., Du, N., Dai, A.M. and Le Q.V., "Finetuned language models are zero-shot learners," in _ICLR_, 2022.\n' +
      '* [45] Sanh, V., Webson, A., Raffel, C., Bach, S.H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T.L., Raja, A. and Dey, M., "Multitask prompted training enables zero-shot task generalization," in _ICLR_, 2022.\n' +
      '* [46] Tang, J., Yang, Y., Wei, W., Shi, L., Su, L., Cheng, S., Yin, D. and Huang, C., "GraphGPT: GraphTraction Turning for Large Language Models," in _arXiv preprint arXiv:2310.13023_, 2023.\n' +
      '* [47] Ye, R., Zhang, C., Wang, R., Xu, S. and Zhang, Y., "Natural language is all a graph needs," in _arXiv preprint arXiv:2308.07134._, 2023.\n' +
      '* [48] Zhao, H., Liu, S., Ma, C., Xu, H., Fu, J., Deng, Z.H., Kong, L. and Liu, Q., "GIMET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning," in _bioRxiv, pp. 2023-05_, 2023.\n' +
      '* [49] Wei, J., Wang, X., Schummans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V. and Zhou, D., "Chain-of-thought prompting elicits reasoning in large language models," in _NeurIPS_, 2022.\n' +
      '* [50] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.L., Cao, Y. and Narasimhan, K., "Tree of thoughts: Delbreatable problem solving with large language models," in _arXiv preprint arXiv:2305.10601._, 2023.\n' +
      '* [51] Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gianinazzi, L., Gajda, J., Lehmann, T., Podstawski, M., Niewiadomski, H., Nyczyk, P. and Hoefler, T., "Graph of thoughts: Solving elaborate problems with large language models," in _arXiv preprint arXiv:2308.09687._, 2023.\n' +
      '* [52] Cohan, A., Feldman, S., Beltagy, I., Downey, D. and Weld, D.S., "Specter: Document-level representation learning using citation-informed transformers," in _ACL_, 2020.\n' +
      '* [53] Ostendorff, M., Reth* [58] Zhang, X., Zhang, C., Dong, X.L., Shang, J. and Han, J., "Minimally-supervised structure-rich text categorization via learning on text-rich networks," in _WWW_, 2021.\n' +
      '* [59] Chien, E., Chang, W.C., Hsieh, C.J., Yu, H.F., Zhang, J., Milenkovic, O., and Dhillon, I.S., "Node feature extraction by self-supervised multi-scale neighborhood prediction," in _ICLR_, 2022.\n' +
      '* [60] Zhang, Y., Shen, Z., Wu, C.H., Xie, B., Hao, J., Wang, Y.Y., Wang, K. and Han, J., "Metadata-induced contrastive learning for zero-shot multi-label text classification," in _WWW_, 2022.\n' +
      '* [61] Dinh, T.A., Boer, J.D., Cornless, J. and Groth, P., "E2EG: End-to-End Node Classification Using Graph Topology and Text-based Node Attributes," in _arXiv preprint arXiv:2208.04609._, 2022.\n' +
      '* [62] Zhao, J., Qu, M., Li, C., Yan, H., Liu, Q., Li, R., Xie, X. and Tang, J., "Learning on large-scale text-attributed graphs via variational inference," in _ICLR_, 2023.\n' +
      '* [63] Wen, Z. and Fang Y., "Augmenting Low-Resource Text Classification with Graph-Grunded Pre-training and Prompting," in _SIGIR_, 2023.\n' +
      '* [64] Chen, Z., Mao, H., Wen, H., Han, H., Jin, W., Zhang, H., Liu, H. and Tang, J., "Label-free Node Classification on Graphs with Large Language Models (LLMS)," in _arXiv preprint arXiv:2310.04668_, 2023.\n' +
      '* [65] Huang, X., Han, K., Bao, D., Tao, Q., Zhang, Z., Yang, Y. and Zhu, Q., "Prompt-based Node Feature Extractor for Few-shot Learning on Text-Attributed Graphs," in _arXiv preprint arXiv:2309.02848_, 2023.\n' +
      '* [66] Zhao, J., Zhuo, L., Shen, Y., Qu, M., Liu, K., Bronstein, M., Zhu, Z. and Tang, J., "Graphlet: Graph reasoning in text space," in _arXiv preprint arXiv:2310.01089_, 2023.\n' +
      '* [67] Meng, Y., Zong, S., Li, X., Sun, X., Zhang, T., Wu, F. and Li, J., "Gnn-lm: Language modeling based on global contexts via gnn," in _ICLR_, 2022.\n' +
      '* [68] Zhang, X., Bosselut, A., Yasunaga, M., Ren, H., Liang, P., Manning, C.D. and Leskovec, J., "Greaselm: Graph reasoning enhanced language models for question answering," in _ICLR_, 2022.\n' +
      '* [69] Ioannidis, V.N., Song, X., Zheng, D., Zhang, H., Ma, J., Xu, Y., Zeng, B., Chilimbt, T. and Karypis, G., "Efficient and effective training of language and graph neural network models," in _AAAI_, 2023.\n' +
      '* [70] Mavromatis, C., Ioannidis, V.N., Wang, S., Zheng, D., Adeshina, S., Ma, J., Zhao, H., Faloutsos, C. and Karypis, G., "Train Your Own GNN Teacher: Graph-Aware Distillation on Textual Graphs," in _PKDD_, 2023.\n' +
      '* [71] He, X., Bresson, X., Laurent, T. and Hooi, B., "Explanations as Features: LLM-Based Features for Text-Attributed Graphs," in _arXiv preprint arXiv:2305.19523_, 2023.\n' +
      '* [72] Yu, J., Ren, Y., Gong, C., Tan, J., Li, X. and Zhang, X., "Empower Text-Attributed Graphs Learning with Large Language Models (LLMs)," in _arXiv preprint arXiv:2310.09872_, 2023.\n' +
      '* [73] Yang, J., Liu, Z., Xiao, S., Li, C., Lian, D., Agrawal, S., Singh, A., Sun, C. and Xie, X., "GraphFormers: GNN-nested transformers for representation learning on textual graph," in _NeurIPS_, 2021.\n' +
      '* [74] Jin, B., Zhang, Y., Zhu, Q. and Han, J., "Heterformer: Transformer-based deep node representation learning on heterogeneous text-rich networks," in _KDD_, 2023.\n' +
      '* [75] Jin, B., Zhang, Y., Meng, Y. and Han, J., "Edgeformers: Graph-Empowered Transformers for Representation Learning on Textual-Edge Networks," in _ICLR_, 2023.\n' +
      '* [76] Jin, B., Zhang, W., Zhang, Y., Meng, Y., Zhao, H. and Han, J., "Learning Multiplex Embeddings on Text-rich Networks with One Text Encoder," in _arXiv preprint arXiv:2310.06684_, 2023.\n' +
      '* [77] Qin, Y., Wang, X., Zhang, Z. and Zhu, W., "Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs," in _arXiv preprint arXiv:2310.18152_, 2023.\n' +
      '* [78] Zhang, Y., Shen, Z., Dong, Y., Wang, K. and Han, J., "MATCH: Metadata-aware text classification in a large hierarchy," in _WWW_, 2021.\n' +
      '* [79] Zhu, J., Cui, Y., Liu, Y., Sun, H., Li, X., Pelegr, M., Yang, T., Zhang, L., Zhang, R. and Zhao, H., "Textgnm: Improving text encoder via graph neural network in sponsored search," in _WWW_, 2021.\n' +
      '* [80] Li, C., Pang, B., Liu, Y., Sun, H., Liu, Z., Xie, X., Yang, T., Cui, Y., Zhang, L. and Zhang, Q., "Adsgnn: Behavior-graph augmented relevance modeling in sponsored search," in _SIGIR_, 2021.\n' +
      '* [81] Zhang, J., Chang, W.C., Yu, H.F. and Dhillon, I., "Fast multi-resolution transformer fine-tuning for extreme multi-label text classification," in _NeurIPS_, 2021.\n' +
      '* [82] Xie, H., Zheng, D., Ma, J., Zhang, H., Ioannidis, V.N., Song, X., Ping, Q., Wang, S., Yang, C., Xu, Y. and Zeng, B., "Graph-Aware Language Model Pre-Training on a Large Graph Corpus Can Help Multiple Graph Applications," in _KDD_, 2023.\n' +
      '* [83] Yasunaga, M., Bosselut, A., Ren, H., Zhang, X., Manning, C.D., Liang, P.S. and Leskovec, J., "Deep bidirectional language-knowledge graph pretraining," in _NeurIPS_, 2021.\n' +
      '* [84] Huang, J., Zhang, X., Mei, Q. and Ma, J., "CAN LLMS EF-FECTIVELY LEVERAGE GRAPH STRUCTURAL INFORMATION: WHEN AND WHY," in _arXiv preprint arXiv:2309.16595._, 2023.\n' +
      '* [85] Kipf, T.N. and Welling, M., "Semi-supervised classification with graph convolutional networks," in _ICLR_, 2017.\n' +
      '* [86] Hamilton, W., Ying, Z. and Leskovec, J., "Inductive representation learning on large graphs," in _NeurIPS_, 2017.\n' +
      '* [87] Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Lio, P. and Bengio, Y., "Graph attention networks," in _ICLR_, 2018.\n' +
      '* [88] Zhang, S., Liu, Y., Sun, Y. and Shah, N., "Graph-less Neural Networks: Teaching Old MLPs New Tricks Via Distillation," in _ICLR_, 2022.\n' +
      '* [89] Liu, M., Gao, H. and Ji, S., "Towards deeper graph neural networks," in _KDD_, 2020.\n' +
      '* [90] Meng, Y., Huang, J., Zhang, Y. and Han, J., "Generating training data with language models: Towards zero-shot language understanding," in _NeurIPS_, 2022.\n' +
      '* [91] Sun, Y., Han, J., Yan, X., Yu, P.S. and Wu, T., "Pathsim: Meta path-based top-k similarity search in heterogeneous information networks," in _VLDB_, 2011.\n' +
      '* [92] Liu, H., Li, C., Wu, Q. and Lee, Y.J., "Visual instruction tuning," in _NeurIPS_, 2023.\n' +
      '* [93] Park, C., Kim, D., Han, J. and Yu, H., "Unsupervised attributed multiplex network embedding," in _AAAI_, 2020.\n' +
      '* [94] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L. and Polosukhin, I., "Attention is all you need," in _NeurIPS_, 2017.\n' +
      '* [95] Haveliwala, T.H., "Topic-sensitive pagerank," in _WWW_, 2002.\n' +
      '* [96] Oord, A.V.D., Li, Y. and Vinyals, O., "Representation learning with contrastive predictive coding," in _arXiv preprint arXiv:1807.03748_, 2018.\n' +
      '* [97] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J. and Krueger, G., "Learning transferable visual models from natural language supervision," in _ICML_, 2021.\n' +
      '* [98] Whalen, R., "Legal networks: The promises and challenges of legal network analysis," in _Mich. St. L. Ren._, 2016.\n' +
      '* [99] Friedrich, A. and Palmer, A. and Pinkal, M., "Situation entity types: automatic classification of clause-level aspect," in _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2016.\n' +
      '* [100] Guha, N., Nyarko, J., Ho, D.E., Re, C., Chilton, A., Narayana, A., Chohla-Nood, A., Peters, A., Waldon, B., Rockmore, D.N. and Zambrano, D., "Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models," in _arXiv preprint arXiv:2308.11462_, 2023.\n' +
      '* [101] Bai, X., Wang, M., Lee, I., Yang, Z., Kong, X. and Xia, F., "Scientific paper recommendation: A survey," in _Ieee Access_, _7, pp.9324-9339, 2019.\n' +
      '* [102] Chowdhury, S. and Schoen, M.P., "Research paper classification using supervised machine learning techniques," in _Intermountain Engineering, Technology and Computing_, 2020.\n' +
      '* [103] Madigan, D., Genkin, A., Lewis, D.D., Argamon, S., Fradkin, D. and Ye, L., "Author identification on the large scale," in _Proceedings of the 2005 Meeting of the Classification Society of North America (CSAN)_, 2005.\n' +
      '* [104] He, X., Deng, K., Wang, X., Li, Y., Zhang, Y. and Wang, M., "Lightgcn: Simplifying and powering graph convolution network for recommendation," in _SIGIR_, 2020.\n' +
      '* [105] Chang, J., Gao, C., He, X., Jin, D. and Li, Y., "Bundle recommendation with graph convolutional networks," in _SIGIR_, 2020.\n' +
      '* [106] Xu, H., Liu, B., Shu, L. and Yu, P.,"Emergent Abilities of Large Language Models" in _Transactions on Machine Learning Research_, 2022.\n' +
      '* [11] Kojima, T., Gu, S.S., Reid, M., Matsuo, Y. and Iwasawa, Y., 2022. "Large language models are zero-shot reasoners" in _Advances in neural information processing systems_, 35, pp.22199-22213.\n' +
      '* [12] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V. and Zhou, D., 2022. "Chain-of-thought prompting elicits reasoning in large language models" in _Advances in Neural Information Processing Systems_, 35, pp.24824-24837.\n' +
      '* [13] Radford, A., 2019. "Language Models are Unsupervised Multitask Learners" in _OpenAI blog_, 2019.\n' +
      '* [14] Mikolov, T., Chen, K., Corrado, G. and Dean, J., 2013. "Efficient estimation of word representations in vector space" in _arXiv preprint arXiv:1301.3781_.\n' +
      '* [15] Pennington, J., Socher, R. and Manning, C.D., 2014, October. "Glove: Global vectors for word representation" in _Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)_ (pp. 1532-1543).\n' +
      '* [16] Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P. and Soricut, R., 2019, September. "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations" in _International Conference on Learning Representations_.\n' +
      '* [17] Clark, K., Luong, M.T., Le, Q.V. and Manning, C.D., 2019, September. "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators" in _International Conference on Learning Representations_.\n' +
      '* [18] Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y.T., Li, Y., Lundberg, S. and Nori, H., 2023. "Sparks of artificial general intelligence: Early experiments with gpt-at" in _arXiv preprint arXiv:2303.12712_.\n' +
      '* [19] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S. and Bikel, D., 2023. "Llama 2: Open foundation and fine-tuned chat models" in _arXiv preprint arXiv:2307.09288_.\n' +
      '* [20] Jiang, A.Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.S., Casas, D.D.L., Bressand, F., Lengyel, G., Lample, G., Saulnier, L. and Lavaud, L.R., 2023. "Mistral 7B" in _arXiv preprint arXiv:2310.06825_.\n' +
      '* [21] Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M. and Ring, R., 2022. "Planning: a visual language model for few-shot learning" in _Advances in Neural Information Processing Systems_ (pp. 23716-23736).\n' +
      '* [22] Edwards, C., Zhai, C. and Ji, H., 2021, November. "Text2mol: Cross-modal molecule retrieval with natural language queries" in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_ (pp. 595-607).\n' +
      '* [23] Edwards, C., Lai, T., Ros, K., Honke, G., Cho, K. and Ji, H., 2022, December. "Translation between Molecules and Natural Language" in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_ (pp. 375-413).\n' +
      '* [24] Wang, H., Feng, S., He, T., Tan, Z., Han, X. and Tsvetkov, Y., 2023. "Can Language Models Solve Graph Problems in Natural Language" in _arXiv preprint arXiv:2305.10037._, 2023.\n' +
      '* [25] Liu, C. and Wu, B., 2023. "Evaluating large language models on graphs: Performance insights and comparative analysis" in _arXiv preprint arXiv:2308.11224_, 2023.\n' +
      '* [26] Guo, J., Du, L. and Liu, H., 2023. "GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking" in _arXiv preprint arXiv:2305.15066_, 2023.\n' +
      '* [27] Zhang, J., 2023. "Graph-TooFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT" in _arXiv preprint arXiv:2304.11116_, 2023.\n' +
      '* [28] Zhang, Z., Wang, X., Zhang, Z., Li, H., Qin, Y., Wu, S. and Zhu, W., 2023. "LMLQ3: Can Large Language Models Solve Problems on Dynamic Graphs" in _arXiv preprint arXiv:2310.17110_, 2023.\n' +
      '* [29] Luo, L., Li, Y.F., Haffari, G. and Pan, S., 2023. "Reasoning on graphs: Faithful and interpretable large language model reasoning" in _arXiv preprint arXiv:2310.01061_, 2023.\n' +
      '* [30] Jiang, J., Zhou, K., Dong, Z., Ye, K., Zhao, W.X. and Wen, J.R., 2023. "Structgpt: A general framework for large language model to reason over structured data" in _arXiv preprint arXiv:2305.09645_, 2023.\n' +
      '* [31] Fatemi, B., Halcrow, J. and Perozzi, B., 2023. "Talk like a graph: Encoding graphs for large language models" in _arXiv preprint arXiv:2310.04560_, 2023.\n' +
      '* [32] Sun, J., Xu, C., Tang, L., Wang, S., Lin, C., Gong, Y., Shum, H.Y. and Guo, J., 2023. "Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph" in _arXiv preprint arXiv:2307.07697_, 2023.\n' +
      '* [33] Danny Z. Chen. 1996. "Developing algorithms and software for geometric path planning problems" in _ACM Comput. Surv. 28, 4es (Dec. 1996), 18-es. [https://doi.org/10.1145/42224.242246_](https://doi.org/10.1145/42224.242246_), 1996.\n' +
      '* [34] Iqbal A., Hossain Md., Ebna A. (2018). "Airline Scheduling with Max Flow algorithm" in _International Journal of Computer Applications_, 2018.\n' +
      '* [35] Li Jiang, Xiaoning Zang, Ibrahim I.Y. Alghoul, Xiang Fang, Junfeng Dong, Changyong Liang, 2022. "Scheduling the covering delivery problem in last mile delivery" in _Expert Systems with Applications_, 2022.\n' +
      '* [36] Zhang, X., Wang, L., Helwig, J., Luo, Y., Fu, C., Xie, Y.,... & Ji, S. (2023). Artificial intelligence for science in quantum, atomistic, and continuum systems, _arXiv preprint arXiv:2307.08423_.\n' +
      '* [37] Rusch, T. K., Bronstein, M. M., & Mishra, S. (2023). A survey on oversmoothing in graph neural networks. arXiv preprint arXiv:2303.10993.\n' +
      '* [38] Topping, J., Di Giovanni, F., Chamberlain, B. P., Dong, X., & Bronstein, M. M. (2021). Understanding over-squashing and bottlenecks on graphs via curvature. arXiv preprint arXiv:2111.14522.\n' +
      '* [39] Zhang, B., Luo, S., Wang, L., & He, D. (2023). Rethinking the expressive power of gnns via graph biconnectivity. arXiv preprint arXiv:2301.099505.\n' +
      '* [40] Muller L, Galkin M, Morris C, Rampasek L. Attending to graph transformers. arXiv preprint arXiv:2302.04181. 2023 Feb 8.\n' +
      '* [41] Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D.,... & Liu, T. Y. (2021). Do transformers really perform badly for graph representation? Advances in Neural Information Processing Systems, 34, 28877-28888.\n' +
      '* [42] Rampasek, L., Galkin, M., Dwivedi, V. P., Luu, A. T., Wolf, G., & Beaini, D. (2022). Recipe for a general, powerful, scalable graph transformer. Advances in Neural Information Processing Systems, 35, 14501-14515.\n' +
      '* [43] Liu, G., Zhao, T., Inae, E., Luo, T., & Jiang, M. (2023). Semi-Supervised Graph Imbalanced Regression. arXiv preprint arXiv:2305.12087.\n' +
      '* [44] Wu Q. Zhao W, Li Z, Wipf DP, Yan J. Nodeformer: A scalable graph structure learning transformer for node classification. Advances in Neural Information Processing Systems. 2022 pc 6.3527387-401.\n' +
      '* [45] Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., & Liu, Y. (2021). Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864.\n' +
      '* [46] Balaban, A. T., Applications of graph theory in chemistry. _Journal of chemical information and computer sciences_, 25(3), 334-343, 1985.\n' +
      '* [47] Liu, G., Zhao, T., Xu, J., Luo, T., & Jiang, M., Graph rationalization with environment-based augmentations. In _ACM SIGKDD_, 2022.\n' +
      '* [48] Bran, A. M., Cox, S., White, A. D., & Schwaller, P., ChemCrow: Augmenting large-language models with chemistry tools, _arXiv preprint arXiv:2304.05376_, 2023.\n' +
      '* [49] Borgwardt, K. M., Ong, C. S., Schonauer, S., Vishwanathan, S. V. N., Smola, A. J., & Kriegel, H. P., Protein function prediction via graph kernels. _Bioinformatics_, 21, 47-fe6, 2005.\n' +
      '* [50] Riesen, K., & Bunke, H., IAM graph database repository for graph based pattern recognition and machine learning. In _Structural, Syntactic, and Statistical Pattern Recognition: Joint IAPR International Workshop, SSRP & SPR 2008. Orlando, USA_, December 4-6, 2008. Proceedings (pp. 287-297). Springer Berlin Heidelberg.\n' +
      '* [51] Jain, N., Coyle, B., Kashefi, E., & Kumar, N., Graph neural network initialisation of quantum approximate optimisation. Quantum, 6, 861, 2022.\n' +
      '* [52] Weininger, D., SMILES, a chemical language and information system. I. Introduction to methodology and encoding rules. _Journal of chemical information and computer sciences_, 28(1), 31-36, 1988\n' +
      '* [53] Heller S, McNaught A, Stein S, Tchekhovskoi D, Pleture I. InChI-the worldwide chemical structure identifier standard. _Journal of cheminformatics_, 2013 Dec5(1)-19.\n' +
      '* [54] O\'Boyle, N., & Dalke, A., DeepSMILES: an adaptation of SMILES for use in machine-learning of chemical structures, 2018.\n' +
      '* [55] Krenn, M., Hase, F., Nigam, A., Friederich, P., & Aspuru* [157] Arais-Pous, J., Johansson, S. V., Prykhodko, O., Bjerrum, E. J., Tyrchan, C., Reymond, J. L.,... & Engkvist, O. (2019). Randomized SMILES strings improve the quality of molecular generative models. Journal of cheminformatics, 11(1), 1-13.\n' +
      '* [158] Tetko IV, Karpov P, Bruno E, Kimber TB, Godin G. Augmentation is what you need!. InInternational Conference on Artificial Neural Networks 2019 Sep 9 (pp. 831-835). Cham: Springer International Publishing.\n' +
      '* [159] van Deursen R, Ertl P, Tetko IV, Godin G. GEN: highly efficient SMILES explore using autodidactic generative examination networks. Journal of Cheminformatics, 2020 Deci-12(1):1-4.\n' +
      '* [160] Schwaller, P., Gaudin, T., Lanyi, D., Bekas, C., & Laino, T., "Found in Translation", predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models. _Chemical science_, 9(28), 6091-6098, 2018.\n' +
      '* [161] Morgan, H. L., The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service. _Journal of chemical documentation_, 5(2), 107-113, 1965.\n' +
      '* [162] Sennrich, R., Haddow, B., & Birch, A. Neural machine translation of rare words with subword units, in _ACL_, 2016.\n' +
      '* [163] Kudo, T., & Richardson, J., Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In _EMNLP_, 2018.\n' +
      '* [164] Irwin, R., Dimitriadis, S., He, J., & Bjerrum, E. J. (2022). Chemformer: a pre-trained transformer for computational chemistry. _Machine Learning: Science and Technology_, 3(1), 015022.\n' +
      '* [165] Zhao WX, Zhou K, Li J, Tang T, Wang X, Hou Y, Min Y, Zhang B, Zhang J, Dong Z, Du Y. A survey of large language models. _arXiv preprint_ arXiv:2303.18223. 2023 Mar 31.\n' +
      '* [166] Shi, Y., Zhang, A., Zhang, E., Liu, Z., & Wang, X., ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction, in _EMNLP_, 2023.\n' +
      '* [167] Liu P, Ren Y, Ren Z., Git-mol: A multi-modal large language model for molecular science with graph, image, and text, _arXiv preprint arXiv:2308.06911_, 2023\n' +
      '* [168] Ock J, Guntuboina C, Farimani AB. Catalyst Property Prediction with CatBFTA: Unveiling Feature Exploration Strategies through Large Language Models. _arXiv preprint_ arXiv:2309.00563, 2023.\n' +
      '* [169] Fang Y, Liang X, Zhang N, Liu K, Huang R, Chen Z, Fan X, Chen H., Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. _arXiv preprint_ arXiv:2306.08018, 2023.\n' +
      '* [170] Abbdine H, Chatzianassis M, Boquivioulos C, Vazirgiannis M., Prot2Text: Multimodal Protein\'s Function Generation with GNNs and Transformers, _arXiv preprint_ arXiv:2307.14367, 2023.\n' +
      '* [171] Luo Y, Yang K, Hong M, Liu X, Nie Z., MolFM: A Multimodal Molecular Foundation Model, _arXiv preprint_ arXiv:2307.09484, 2023.\n' +
      '* [172] Qian, C., Tang, H., Yang, Z., Liang, H., & Liu, Y., Can large language models empower molecular property prediction? _arXiv preprint_ arXiv:2307.07443, 2023.\n' +
      '* [173] Born, J., & Manica, M., Regression Transformer enables concurrent sequence regression and generation for molecular language modelling. _Nature Machine Intelligence_, 5(4), 432-444, 2023.\n' +
      '* [174] Li J, Liu Y, Fan W, Wei XY, Liu H, Tang J, Li Q, Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective. _arXiv preprint_ arXiv:2306.06615, 2023.\n' +
      '* [175] Zeng, Z., Yin, B., Wang, S., Liu, J., Yang C., Yao, H.,... & Liu, Z., Interactive Molecular Discovery with Natural Language. arXiv preprint arXiv:2306.11976, 2023.\n' +
      '* [176] Liu Z, Li S, Luo Y, Fei H, Cao Y, Kawaguchi K, Wang X, Chua TS., MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter, in _EMNLP_, 2023.\n' +
      '* [177] Guo T, Guo K, Liang Z, Guo Z, Chawla NV, Wiest O, Zhang X. What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks. In _NeurIPS_, 2023.\n' +
      '* [178] Liu Z, Zhang W, Xia Y, Wu L, Xie S, Qin T, Zhang M, Liu TY., Molybdenum: Wrapping Molecules with Text for Generative Pre-training. In _ACL_, 2023.\n' +
      '* [179] Seidl, P., Vall, A., Hochreiter, S., & Klambauer, G., Enhancing activity prediction models in drug discovery with the ability to understand human language, in _ICML_, 2023.\n' +
      '* [180] Christofields, D., Giamone, G., Born, J., Winther, O., Laino, T., & Manica, M., Unifying molecular and textual representations via multi-task language modelling. in _ICML_, 2023.\n' +
      '* [181] Liu, S., Nie, W., Wang, C., Lu, J., Qiao, Z., Liu, L.,... & Anandkumar, A. Multi-modal molecule structure-text model for text-based retrieval and editing. _Nature Machine Intelligence_, 2023.\n' +
      '* [182] Lacombe, R., Gaut, A., He, J., Ludeke, D., & Pistunova, K., Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning. _ICML Workshop on Computational Biology_, 2023.\n' +
      '* [183] Su, B., Du, D., Yang, Z., Zhou, Y., Li, J., Rao, A.,... & Wen, J. R., A molecular multimodal foundation model associating molecule graphs with natural language, _arXiv preprint_ arXiv:2209.05481. 2022.\n' +
      '* [184] Zeng, Z., Yao, Y., Liu, Z., & Sun, M., A deep-learning system bridging molecule structure and biomedical text with compression comparable to human professionals, _Nature communications_, 13(1), 862.\n' +
      '* [185] Iwayama, M., Wu, S., Liu, C., & Yoshida, R., Functional Output Regression for Machine Learning in Materials Science. _Journal of Chemical Information and Modeling_, 62(2020), 4837-4851, 2022.\n' +
      '* [186] Bagal V, Aggarwal R, Vinod PK, Priyakumar U. MolCPT: molecular generation using a transformer-decoder model. _Journal of Chemical Information and Modeling_. 2021 Oct 256(29):2064-76.\n' +
      '* [187] Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hathshorn, A., Saravia, E.,... & Stojnic, R., Galactica: A large language model for science. _arXiv preprint_ arXiv:2211.09085, 2022.\n' +
      '* [188] Wang, S., Guo, Y., Wang, Y., Sun, H., & Huang, J., Smiles-bert: large scale unsupervised pre-training for molecular property prediction. In _BCB_, 2019\n' +
      '* [189] Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H., & Kang, J., BioBERT: a pre-trained biomedical language representation model for biomedical text mining. _Bioinformatics_, 36(4), 1234-1240, 2020.\n' +
      '* [190] Ma, R., & Luo, T. (2020). PIM: a benchmark database for polymer informatics. Journal of Chemical Information and Modeling, 60(10), 4684-4690.\n' +
      '* [191] Li, X., Xu, Y., Lai, L., & Pei, J. Prediction of human cytochrome P450 inhibition using a multiscale deep autoencoder neural network. _Molecular pharmacimetrics_, 15(10), 4363-4345, 2020.\n' +
      '* [192] Stanley M, Bronskill JF, Maziarz K, Miszeta H, Lanini J, Segler M, Schneider N, Brockschmidt M. FS-mol: A few-shot learning dataset of molecules, in _NeurIPS_, 2021.\n' +
      '* [193] Hastings, J., Owen, G., Dekker, A., Ennis, M., Kale, N., Muthukrishnan, V.,... & Steinbeck, C., CheB1 in 2016: Improved services and an expanding collection of metabolites. _Nucleic acids research_, 44(D1), D1214-D1219, 2016.\n' +
      '* [194] Kim, S., Chen, J., Cheng, T., Gindulyte, A., He, J., He, S.,... & Bolton, E. E., PubChem 2019 update: improved access to chemical data, _Nucleic acids research_, 47(D1), D1012-D1109, 2019.\n' +
      '* [195] Gaulton, A., Bellis, L. J., Bento, A. P., Chambers, J., Davies, M., Hersey, A.,... & Overington, J. P., ChEMBL: a large-scale bioactivity database for drug discovery. _Nucleic acids research_, 40(D1), D1100-D1107. 2012.\n' +
      '* [196] Zdrazil B, Felix E, Hunter F, Manners EJ, Blackshaw J, Corbett S, de Veij M, Ioannidis H, Lopez DM, Mosquera JF, Magarinos MP. The ChEMBL Database in 2023: a drug discovery platform spanning multiple bioactivity data types and time periods. Nucleic Acids Research. 2023 Nov 2:gkad1004.\n' +
      '* [197] Sterling, T. and Irwin, J.J., ZINC 15-ligand discovery for everyone. _Journal of chemical information and modeling_, 55(11), pp. 2324-2337, 2015.\n' +
      '* [198] Blum LC, Raymond JL. 970 million druglike small molecules for virtual screening in the chemical universe database CBB-13. Journal of the American Chemical Society. 2009 Jul 1:131(25):8732-3.\n' +
      '* [199] Mellor, C. L., Robinson, R. M., Benigni, R., Ebbrell, D., Enoch, S. J., Firman, J. W.,... & Cronin, M. T. D. (2019). Molecular fingerprint-derived similarity measures for toxicological read-across: Recommendations for optimal use. _Regulatory Toxicology and Pharmacology_, 101, 121-134.\n' +
      '* [200] Maggioro, G., Vogt, M., Stumpfe, D., & Bajorath, J. (2014). Molecular similarity in medicinal chemistry: minipperspective. _Journal of medicinal chemistry_, 57(8), 3186-3204.\n' +
      '* [21] Rogers, D., & Hahn, M. (2010). Extended-connectivity fingerprints. Journal of chemical information and modeling, 50(5), 742-754.\n' +
      '* [22] Beisken, S., Meinl, T., Wiswedel, B., de Figueiredo, L. F., Berthold, M., & Steinbeck, C. (2013). KNIME-CDK: Workflow-driven cheminformatics. _BMC bioinformatics_, 14: 1-4.\n' +
      '* [23] Krenn, M., Ai, Q., Barthel, S., Carson, N., Frei, A., Frey, N. C.,... & Aspuru-Guzik, A. (2022). SELFI* [206] Leman, A. A., & Weisfeiler, B. (1968). A reduction of a graph to a canonical form and an algebra arising during this reduction. _Nuncino-Technischekar Informatsiya_, 2(9), 12:16.\n' +
      '* [207] Li, J., Li, D., Savarese, S., & Hoi, S., Blip-2: Bootstraping language-image pre-training with frozen image encoders and large language models. _arXiv preprint_ arXiv:2301.12597.\n' +
      '* [208] Wu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Geniesse, C., Pappu, A. S.,... & Pande, V. (2018). MoleculeNet: a benchmark for molecular machine learning. Chemical science, 9(2), 513-530.\n' +
      '* [209] AIDS Antiviral Screen Data. [https://wiki.nichin.gov/display/NCIDTPdata/AIDS+Antiviral+Screen+Data](https://wiki.nichin.gov/display/NCIDTPdata/AIDS+Antiviral+Screen+Data), Accessed: 2017-09-27\n' +
      '* [210] Subramanian, G., Ramsundar, B., Pande, V., & Denny, R. A. (2016). Computational modeling of \\(\\beta\\)-secretase 1 (BACE-1) inhibitors using ligand based approaches. Journal of chemical information and modeling, 56(10), 1936-1949.\n' +
      '* [211] Martins, I. F., Teixeira, A. L., Pinheiro, L., & Falcao, A. O. (2012). A Bayesian approach to in silico blood-brain barrier penetration modeling. Journal of chemical information and modeling, 52(6), 1686-1697.\n' +
      '* [212] Tox21 Challenge. [https://tripod.nih.gov/tox21/challenge/](https://tripod.nih.gov/tox21/challenge/), Accessed: 2017-09-27\n' +
      '* [213] Altae-Tran H, Ramsundar B, Pappu AS, Pande V. Low data drug discovery with one-shot learning. ACS central science. 2017 Apr 26:34(8):283-39.\n' +
      '* [214] Novick PA, Ortiz OF, Poelman J, Abdulhay AY, Pande VS. SWEETLEAD: an in silico database of approved drugs, regulated chemicals, and herbal isolates for computer-aided drug discovery. PloS one. 2013 Nov 18(11)e79568.\n' +
      '* [215] Aggregate Analysis of ClinicalTrials.gov (AACT) Database. [https://www.ctti-clinicaltrials.org/aact-database](https://www.ctti-clinicaltrials.org/aact-database), Accessed: 2017-09-27.\n' +
      '* [216] Mobley DL, Guthrie JP. FreeSolv: a database of experimental and calculated hydration free energies, with input files. Journal of computer-aided molecular design. 2014 Jul;28:711-20.\n' +
      '* [217] Delaney, J. S. (2004). ESOL: estimating aqueous solubility directly from molecular structure. Journal of chemical information and computer sciences, 44(3), 1000-1005.\n' +
      '* [218] Zhao, T., Liu, G., Wang, D., Yu, W., & Jiang, M. (2022, June). Learning from counterfactual links for link prediction. In International Conference on Machine Learning (pp. 26911-26926). PMLR.\n' +
      '* [219] Vignac, C., Krawczuk, I., Siraudin, A., Wang, B., Cevher, V., & Frossard, P. (2022). Digress: Discrete density diffusion for graph generation. arXiv preprint arXiv:2209.14734.\n' +
      '* [220] Zang, C., & Wang, F. Mollow: an invertible flow model for generating molecular graphs. In _ACM SIGKDD_, 2020.\n' +
      '* [221] Deng, J., Yang, Z., Wang, H., Ojima, I., Samaras, D., & Wang, F. (2023). A systematic study of key elements underlying molecular property prediction. Nature Communications, 14(1), 6395.\n' +
      '* [222] Bohm, H. J., Flohr, A., & Stahl, M. (2004). Scaffold hopping. Drug discovery today: Technologies, 1(3), 217-224.\n' +
      '* [223] Renz, P., Van Rompaey, D., Wegner, J. K., Hochreiter, S., & Klambauer, G. (2019). On failure modes in molecule generation and optimization. Drug Discovery Today: Technologies, 32, 55-63.\n' +
      '* [224] Polykovskiy, D., Zhebrak, A., Sanchez-Lengeling, B., Golovanov, S., Tatonov, O., Belyaev, S.,... & Zharovonkov, A. (2020). Molecular sets (MOSES): a benchmarking platform for molecular generation models. Frontiers in pharmacology, 11, 565644.\n' +
      '* [225] Reymond, J. L. (2015). The chemical space project. Accounts of Chemical Research, 48(3), 722-730.\n' +
      '* [226] Lin, A., Horvath, D., Afonina, V., Marcou, G., Reymond, J. L., & Varnek, A. (2018). Mapping of the Available Chemical Space versus the Chemical Universe of Lead-Like Compounds. ChemMedChem, 13(6), 540-554.\n' +
      '* [227] Gao, W., Fu, T., Sun, J., & Coley, C. (2022). Sample efficiency matters: a benchmark for practical molecular optimization. Advances in Neural Information Processing Systems, 35, 21342-21357.\n' +
      '* [228] Wang, H., Fu, T., Du, Y., Gao, W., Huang, K., Liu, Z.,... & Zitnik, M. (2023). Scientific discovery in the age of artificial intelligence. Nature, 620(7972), 47-60.\n' +
      '* [229] Liu, G., Hae, E., Zhao, T., Xu, J., Luo, T., & Jiang, M. (2023). Data-Centric Learning from Unlabeled Graphs with Diffusion Model. arXiv preprint arXiv:2303.10108.\n' +
      '* [230][https://pracaticalcheminformatics.blogspot.com/2023/08/we-need-better-benchmarks-for-machine.html?m=1](https://pracaticalcheminformatics.blogspot.com/2023/08/we-need-better-benchmarks-for-machine.html?m=1)\n' +
      '* [231] Merchant, A., Batzner, S., Schoenholz, S. S., Aykol, M., Cheon, G., & Cubuk, E. D. (2023). Scaling deep learning for materials discovery. Nature, 1-6.\n' +
      '* [232] Jo, J., Lee, S., & Hwang, S. J. (2022, June). Score-based generative modeling of graphs via the system of stochastic differential equations. In International Conference on Machine Learning (pp. 10362-10383). PMLR.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>