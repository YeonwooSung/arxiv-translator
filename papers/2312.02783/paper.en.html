<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Large Language Models on Graphs: A Comprehensive Survey\n' +
      '\n' +
      'Bowen Jin\\({}^{\\star}\\), Gang Liu\\({}^{\\star}\\), Chi Han\\({}^{\\star}\\), Meng Jiang, Heng Ji, Jiawei Han\n' +
      '\n' +
      '\\({}^{\\star}\\)_The first three authors contributed equally to this work. Bowen Jin, Chi Han, Heng Ji, Jiawei Han: University of Illinois at Urbrun-Champaign. {bowen4, chihan3, hengji, hanr}@illinois.edu Gang Liu, Meng Jiang: University of Notre Dame. {gliiu7, mjiang2@}@nd.edu_\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Large language models (LLMs), such as ChatGPT and LLMa, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (_e.g._, reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data are associated with rich structure information in the form of graphs (_e.g._, academic networks, and e-commerce networks) or scenarios where graph data are paired with rich textual information (_e.g._, molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graph scenarios (_i.e._, graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-rich graphs, and text-paired graphs. We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we mention the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field. The related source can be found at [https://github.com/PeterGriffin.Jin/Awesome-Language-Model-on-Graphs](https://github.com/PeterGriffin.Jin/Awesome-Language-Model-on-Graphs).\n' +
      '\n' +
      ' Large Language Models, Graph Neural Networks, Natural Language Processing, Graph Representation Learning\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Large language models (LLMs) (_e.g._, BERT [22], T5 [30], LLMa [119]) which are pretrained on very large text corpus has been demonstrated to be very powerful in solving natural language processing (NLP) tasks, including question answering [1], text generation [2] and document understanding [3]. Early LLMs (_e.g._, BERT [22], RoBERTa [23]) adopt an encoder-only architecture and are mainly applied for text representation learning [4] and natural language understanding [3]. In recent years, more focus has been given to decoder-only architectures [119] or encoder-decoder architectures [30]. As the model size scales up, such LLMs have also shown reasoning ability and even more advanced emergent ability [5], exposing a strong potential for Artificial General Intelligence (AGI).\n' +
      '\n' +
      'While LLMs are extensively applied to process pure texts, there is an increasing number of applications where the text data are associated with structure information which are represented in the form of graphs. As presented in Fig. 1, in academic networks, papers (with title and description) and authors (with profile text), are interconnected with authorship relationships. Understanding both the author/paper\'s text information and author-paper structure information on such graphs can contribute to advanced author/paper modeling and accurate recommendations for collaboration; In the scientific domain, molecules are represented as graphs and are often paired with text that describes their basic information (_e.g._, toxicity). Joint modeling of both the molecule structure (graph) and the associated rich knowledge (text) is important for deeper molecule understanding. Since LLMs are mainly proposed for modeling texts that lie in a sequential fashion, those scenarios mentioned above pose\n' +
      '\n' +
      'Fig. 1: According to the relationship between graph and text, we categorize three LLM on graph scenarios. Depending on the role of LLM, we summarize three LLM-on-graph techniques. ‘LLM as Predictor’ is where LLMs are responsible for predicting the final answer. ‘LLM as Aligner’ will align the inputs-output pairs with those of GNNs. ‘LLM as Encoder’ refers to using LLMs to encode and obtain feature vectors.\n' +
      '\n' +
      'new challenges on how to enable LLMs to encode the structure information on graphs. In addition, since LLMs have demonstrated their superb text-based reasoning ability, it is promising to explore whether they have the potential to address fundamental graph reasoning problems on pure graphs. These graph reasoning tasks include inferring connectivity [6], shortest path [7], and subgraph matching [8].\n' +
      '\n' +
      'Recently, there has been an increasing interest in extending LLMs for graph-based applications (summarized in Fig. 1). According to the relationship between graph and text presented in Fig. 1, the application scenarios can be categorized into pure graphs, text-rich graphs, and text-paired graphs. Depending on the role of LLMs and their interaction with graph neural networks (GNNs), the LLM on graphs techniques can be classified into treating LLMs as the task predictor (LLM as Predictor), treating LLMs as the feature encoder for GNNs (LLM as Encoder), and align LLMs with GNNs (LLM as Aligner).\n' +
      '\n' +
      'There are a limited number of existing surveys exploring the intersection between LLMs and graphs. Related to deep learning on graphs, Wu et al. [17] give a comprehensive overview of graph neural networks (GNNs) with detailed illustrations on recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. Liu et al. [18] discuss pretrained foundation models on graphs, including their backbone architectures, pretraining methods, and adaptation techniques. Pan et al. [19] review the connection between LLMs and knowledge graphs (KGs) especially on how KGs can enhance LLMs training and inference, and how LLMs can facilitate KG construction and reasoning. In summary, existing surveys either focus more on GNNs rather than LLMs or fail to provide a systematic perspective on their applications in various graph scenarios as in Fig. 1. Our paper provides a comprehensive review of the LLMs on graphs, for broader researchers from diverse backgrounds besides the computer science and machine learning community who want to enter this rapidly developing field.\n' +
      '\n' +
      '**Our Contributions.** The notable contributions of our paper are summarized as follows:\n' +
      '\n' +
      '* **Categorization of Graph Scenarios.** We systematically summarize the graph scenarios where language models can be adopted into: pure graphs, text-rich graphs, and text-paired graphs.\n' +
      '* **Systematic Review of Techniques.** We provide the most comprehensive overview of language models on graph techniques. For different graph scenarios, we summarize the representative models, provide detailed illustrations of each of them, and make necessary comparisons.\n' +
      '* **Abundant Resources.** We collect abundant resources on language models on graphs, including benchmark datasets, open-source codebases, and practical applications.\n' +
      '* **Future Directions.** We delve into the foundational principles of language models on graphs and propose six prospective avenues for future exploration.\n' +
      '\n' +
      '**Organization of Survey.** The rest of this survey is organized as follows. Section 2 introduces the background of LLMs and GNNs, lists commonly used notations, and defines related concepts. Section 3 categorizes graph scenarios where LLMs can be adopted and summarizes LLMs on graph techniques. Section 4-6 provides a detailed illustration of LLM methodologies for different graph scenarios. Section 7 delivers available datasets, opensource codebases, and a collection of applications across various domains. Section 8 introduces some potential future directions. Section 9 summarizes the paper.\n' +
      '\n' +
      '## 2 Definitions & Background\n' +
      '\n' +
      '### _Definitions_\n' +
      '\n' +
      'In this section, we provide definitions of various types of graphs and introduce the notations (as shown in Table I) used in this paper.\n' +
      '\n' +
      '_Definition 1 (Graph):_ A graph can be defined as \\(\\mathcal{G}=(\\mathcal{V},\\mathcal{E})\\). Here \\(\\mathcal{V}\\) signifies the set of nodes, while \\(E\\) denotes the set of edges. A specific node can be represented by \\(v_{i}\\in\\mathcal{V}\\), and an edge directed from node \\(v_{j}\\) to \\(v_{i}\\) can be expressed as \\(e_{ij}=(v_{i},v_{j})\\in\\mathcal{E}\\). The set of nodes adjacent to a particular node \\(v\\) is articulated as \\(N(v)=\\{u\\in\\mathcal{V}|(v,u)\\in\\mathcal{E}\\}\\).\n' +
      '\n' +
      'A graph containing a node type set \\(\\mathcal{A}\\) and an edge type set \\(\\mathcal{R}\\), where \\(|\\mathcal{A}|+|\\mathcal{R}|>2\\), is called a _heterogeneous graph_. A heterogeneous graph is also associated with a node type mapping function \\(\\phi:\\mathcal{V}\\rightarrow\\mathcal{A}\\) and an edge type mapping function \\(\\psi:\\mathcal{E}\\rightarrow\\mathcal{R}\\).\n' +
      '\n' +
      '_Definition 2 (Graph with node-level textual information):_ A graph with node-level textual information can be denoted as \\(\\mathcal{G}=(\\mathcal{V},\\mathcal{E},\\mathcal{D})\\), where \\(\\mathcal{V}\\), \\(\\mathcal{E}\\) and \\(\\mathcal{D}\\) are node set, edge set, and text set, respectively. Each \\(v_{i}\\in\\mathcal{V}\\) is associated with some\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Notations** & **Descriptions** \\\\ \\hline \\(|\\cdot|\\) & The length of a set. \\\\ \\hline \\(|\\mathbf{A},\\mathbf{B}|\\) & The concatenation of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\). \\\\ \\hline \\(\\|\\) & Concatenate operation. \\\\ \\hline \\(\\mathcal{G}\\) & A graph. \\\\ \\hline \\(\\mathcal{V}\\) & The set of nodes in a graph. \\\\ \\hline \\(v\\) & A node \\(v\\in\\mathcal{V}\\). \\\\ \\hline \\(\\mathcal{E}\\) & The set of edges in a graph. \\\\ \\hline \\(e\\) & An edge \\(e\\in\\mathcal{E}\\). \\\\ \\hline \\(\\mathcal{G}_{v}\\) & The ego graph associated with \\(v\\) in \\(\\mathcal{G}\\). \\\\ \\hline \\(N(v)\\) & The neighbors of a node \\(v\\). \\\\ \\hline \\(M\\) & A meta-path or a meta-graph. \\\\ \\hline \\(N_{M}(v)\\) & The nodes which are reachable from \\\\  & node \\(v\\) with meta-path or meta-graph \\(M\\). \\\\ \\hline \\(\\mathcal{D}\\) & The text set. \\\\ \\hline \\(s\\in\\mathcal{S}\\) & The text token in a text sentence \\(\\mathcal{S}\\). \\\\ \\hline \\(d_{v_{i}}\\) & The text associated with the node \\(v_{i}\\). \\\\ \\hline \\(d_{e_{ij}}\\) & The text associated with the edge \\(e_{ij}\\). \\\\ \\hline \\(d_{Q}\\) & The text associated with the graph \\(\\mathcal{G}\\). \\\\ \\hline \\(n\\) & The number of nodes, \\(n=|\\mathcal{V}|\\). \\\\ \\hline \\(b\\) & The dimension of a node hidden state. \\\\ \\hline \\(\\mathbf{x}_{v_{i}}\\in\\mathbf{R}^{d}\\) & The initial feature vector of the node \\(v_{i}\\). \\\\ \\hline \\(\\mathbf{H}_{v}\\in\\mathbf{R}^{n\\times b}\\) & The node hidden feature matrix. \\\\ \\hline \\(\\mathbf{h}_{v_{i}}\\in\\mathbf{R}^{b}\\) & The hidden representation of node \\(v_{i}\\). \\\\ \\hline \\(\\mathbf{h}_{Q}\\in\\mathbf{R}^{b}\\) & The hidden representation of a graph \\(\\mathcal{G}\\). \\\\ \\hline \\(\\mathbf{h}_{d_{v}}\\in\\mathbf{R}^{b}\\) & The representation of text \\(d_{v}\\). \\\\ \\hline \\(\\mathbf{H}_{d_{v}}\\in\\mathbf{R}^{d_{v}|\\times b}\\) & The hidden states of tokens in \\(d_{v}\\). \\\\ \\hline \\(\\mathbf{W},\\mathbf{\\Theta},w,\\theta\\) & Learnable model parameters. \\\\ \\hline LLM\\((\\cdot)\\) & Large Language model. \\\\ \\hline GNN\\((\\cdot)\\) & Graph neural network. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE I: Notations of Concepts.\n' +
      '\n' +
      'textual information \\(d_{v_{i}}\\in\\mathcal{D}\\). For instance, in an academic citation network, one can interpret \\(v\\in\\mathcal{V}\\) as the scholarly articles, \\(e\\in\\mathcal{E}\\) as the citation links between them, and \\(d\\in\\mathcal{D}\\) as the textual content of these articles. A graph with node-level textual information is also called a text-rich graph [32], a text-attributed graph [62], or a textual graph [73].\n' +
      '\n' +
      '**Definition 3** (Graph with edge-level textual information): _A graph with node-level textual information can be denoted as \\(\\mathcal{G}=(\\mathcal{V},\\mathcal{E},\\mathcal{D})\\), where \\(\\mathcal{V}\\), \\(\\mathcal{E}\\) and \\(\\mathcal{D}\\) are node set, edge set, and text set, respectively. Each \\(e_{ij}\\in\\mathcal{E}\\) is associated with some textual information \\(d_{e_{ij}}\\in\\mathcal{D}\\). For example, in a social network, one can interpret \\(v\\in\\mathcal{V}\\) as the users, \\(e\\in\\mathcal{E}\\) as the interaction between the users, and \\(d\\in\\mathcal{D}\\) as the textual content of the messages sent between the users._\n' +
      '\n' +
      '**Definition 4** (Graph with graph-level textual information): _A graph data object with graph-level textual information can be denoted as the pair \\((\\mathcal{G},d_{\\mathcal{G}})\\), where \\(\\mathcal{G}=(\\mathcal{V},\\mathcal{E})\\). \\(\\mathcal{V}\\) and \\(\\mathcal{E}\\) are node set and edge set. \\(d_{\\mathcal{G}}\\) is the text set paired to the graph \\(\\mathcal{G}\\). For instance, in a molecular graph \\(\\mathcal{G}\\), \\(v\\in\\mathcal{V}\\) denotes an atom, \\(e\\in\\mathcal{E}\\) represents the strong attractive forces or chemical bonds that hold molecules together, and \\(d_{\\mathcal{G}}\\) represents the textual description of the molecule._\n' +
      '\n' +
      '### _Background_\n' +
      '\n' +
      '**(Large) Language Models.** Language Models (LMs), or language modeling, is a field of natural language processing (NLP) on understanding and generation from text distributions. In recent years, large language models (LLMs) have demonstrated impressive capabilities in tasks such as machine translation, text summarization, and question answering [110, 111, 25, 44, 112, 113].\n' +
      '\n' +
      'Large Language models have evolved significantly over time. Initially, word vectors such as Word2Vec [114] and GloVe [115] represent words in a continuous vector space where semantically similar words were mapped close together. These embeddings \\(\\mathbf{w}\\in\\mathbb{R}^{d}\\) usually model the word-level correlations in a corpus, such as the conditional word probability on skip-grams or continuous bag-of-words (CBOW), and are useful for preliminary tasks like word analogy and word similarity.\n' +
      '\n' +
      'The advent of BERT [22] marks significant progress in language modeling and representation. BERT models the conditional probability of a word given its bidirectional context, also named masked language modeling (MLM) objective. Its training involves masking a random subset of words in a sentence and predicting them based on the remaining words. For an intuitive understanding, if simplified to the case with only one masked word, the objective corresponds to the following equation:\n' +
      '\n' +
      '\\[\\mathbb{E}_{\\mathcal{S}\\sim\\mathcal{D}}\\left[\\sum_{s_{i}\\in\\mathcal{S}}\\log p (s_{i}|s_{1},\\ldots,s_{i-1},s_{i+1},\\ldots,s_{N_{\\mathcal{S}}})\\right], \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\mathcal{S}\\) is a sentence sampled from the corpus \\(\\mathcal{D}\\), \\(s_{i}\\) is the \\(i\\)-th word in the sentence, and \\(N_{\\mathcal{S}}\\) is the length of the sentence. BERT utilizes the Transformer architecture with attention mechanisms as the core building block. In the vanilla Transformer, the attention mechanism is defined as:\n' +
      '\n' +
      '\\[\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_{k}}}\\right)V, \\tag{2}\\]\n' +
      '\n' +
      'where \\(Q,K,V\\in\\mathbb{R}^{N_{\\mathcal{S}}\\times d_{k}}\\) are the query, key, and value vectors for each word in the sentence, respectively. The attention mechanism is designed to capture the dependencies between words in a sentence in a flexible way, an advantage also potentially useful for combining with other input formats like graphs. BERT is useful as a text representation model, where the last layer outputs the representation of the input text \\(\\mathbf{h}_{\\mathcal{S}}\\in\\mathbb{R}^{d}\\). Following BERT, many other masked language models are proposed, such as RoBERTa [23], ALBERT [116], and ELECTRA [117], with similar architectures and objectives of text representation. This type of model is also called the pretrained language model (PLM).\n' +
      '\n' +
      'Although the original Transformer paper [94] was experimented on machine translation, it was not until the release of GPT-2 [113] that causal language modeling (i.e., text generation) became impactful on downstream tasks. Causal language modeling is the task of predicting the next word given the previous words in a sentence. The objective of causal language modeling is defined as:\n' +
      '\n' +
      '\\[\\mathbb{E}_{\\mathcal{S}\\sim\\mathcal{D}}\\left[\\sum_{s_{i}\\in\\mathcal{S}}\\log p (s_{i}|s_{1},\\ldots,s_{i-1})\\right]. \\tag{3}\\]\n' +
      '\n' +
      'To adapt the Transformer to causal language modeling, causal attention is introduced which masks the attention weights for the future words. Simple but powerful, subsequent models like GPT-3 [25], GPT-4 [118], LLaMA [119], LLaMA [121], Mistral 7B [120], and T5 [30] show impressive emergent capabilities such as few-shot learning, chain-of-thought reasoning, and even programming. Efforts have also been made to combine language models with other modalities such as vision [97, 121] and biochemical structures [122, 48, 123]. Its combination with graphs is another exciting topic we will discuss in the current paper.\n' +
      '\n' +
      'We would like to point out that the word "large" in LLM is not associated with a clear and static threshold to divide language models: current models that we call "large" will eventually appear small compared to future models, and many smaller models were large at the time they were developed. "Large" actually refers to a direction in which language models are inevitably evolving. More importantly, it represents an empirical rule that researchers discovered recently: larger foundational models tend to possess significantly more representation and generalization power. Hence, we define LLMs to encompass both medium-scale PLMs, such as BERT, and large-scale LMs, like GPT-4, as suggested by [19].\n' +
      '\n' +
      '**Graph Neural Networks & Graph Transformers.** GNN is proposed as a deep learning architecture for graph data. Primary GNNs including GCN [85], GraphSAGE [86] and, GAT [87] are designed for solving node-level tasks. They mainly adopt a propagation-aggregation paradigm to obtain node representations:\n' +
      '\n' +
      '\\[\\mathbf{a}_{v_{i}v_{i}}^{(l-1)} =\\mathrm{PROP}^{(l)}\\left(\\mathbf{h}_{v_{i}}^{(l-1)},\\mathbf{h}_{v_{i}}^{ (l-1)}\\right),\\left(\\forall v_{j}\\in\\mathcal{N}(v_{i})\\right); \\tag{4}\\] \\[\\mathbf{h}_{v_{i}}^{(l)} =\\mathrm{AGG}^{(l)}\\left(\\mathbf{h}_{v_{i}}^{(l-1)},\\{\\mathbf{a}_{v_{i}v_{ i}}^{(l-1)}|v_{j}\\in\\mathcal{N}(v_{i})\\}\\right). \\tag{5}\\]\n' +
      '\n' +
      'Later works such as GNN [205] explore GNNs for solving graph-level tasks. They obtain graph representations by adopting a READOUT function on node representations:\n' +
      '\n' +
      '\\[\\mathbf{h}_{\\mathcal{G}}=\\text{READOUT}(\\{\\mathbf{h}_{v_{i}}|v_{i}\\in\\mathcal{G}\\}). \\tag{6}\\]The READOUT functions include mean pooling, max pooling, and so on. Subsequent work on GNN tackles the issues of over-smoothing [137], over-squashing [138], interpretability [147], and bias [143]. While message-passing-based GNNs have demonstrated advanced structure encoding capability, researchers are exploring further enhancing its expressiveness with Transformers (_i.e._, graph Transformers). Graph Transformers utilize a global multi-head attention mechanism to expand the receptive field of each graph encoding layer [141]. They integrate the inductive biases of graphs into the model by positional encoding, structural encoding, the combination of message-passing layers with attention layers [142], or improving the efficiency of attention on large graphs [144]. Graph Transformers have been proven as the state-of-the-art solution for many pure graph problems. We refer readers to [140] for the most recent advances in GTs.\n' +
      '\n' +
      '**Language Models vs. Graph Transformers.** Modern language models and graph Transformers both use Transformers [94] as the base model architecture. This makes the two concepts hard to distinguish, especially when the language models are adopted on graph applications. In this paper, Transformers" typically refers to Transformer language models for simplicity. Here, we provide three points to help distinguish them: 1) _Tokens_ (word token vs. node token): Transformers take a token sequence as inputs. For language models, the tokens are word tokens; while for graph Transformers, the tokens are node tokens. In those cases where tokens include both word tokens and node tokens if the backbone Transformers is pretrained on text corpus (_e.g._, BERT [22] and LLaMA [119]), we will call it a "language model". 2) _Positional Encoding_ (sequence vs. graph): language models typically adopt the absolute or relative positional encoding considering the position of the word token in the sequence, while graph Transformers adopt shortest path distance [141], random walk distance, the eigenvalues of the graph Laplacian [142] to consider the distance of nodes in the graph. 3) _Goal_ (text vs. graph): The language models are originally proposed for text encoding and generation; while graph Transformers are proposed for node encoding or graph encoding. In those cases where texts are served as nodes/edges on the graph if the backbone Transformers is pretrained on text corpus, we will call it a "language model".\n' +
      '\n' +
      '## 3 Categorization and Framework\n' +
      '\n' +
      'In this section, we first introduce our categorization of graph scenarios where language models can be adopted. Then we discuss the categorization of LLM on graph techniques. Finally, we summarize the training & inference framework for language models on graphs.\n' +
      '\n' +
      '### _Categorization of Graph Scenarios with Language Models._\n' +
      '\n' +
      '**Pure Graphs without Textual Information** are graphs with no text information or no semantically rich text information. Examples of those graphs include traffic graphs and power transmission graphs. Those graphs often serve as context to test the graph reasoning ability of large language models (solve graph theory problems) or serve as knowledge sources to enhance the large language models (alleviate hallucination).\n' +
      '\n' +
      '**Text-Rich Graphs** refers to graphs where nodes or edges are associated with semantically rich text information. Such graphs are also called text-rich networks [32], text-attributed graphs [62], textual graphs [73] or textual-edge networks [75]. Real-world examples include academic networks, e-commerce networks, social networks, and legal case networks. On these graphs, people are interested in learning representations for nodes or edges with both textual information and structure information [73][75].\n' +
      '\n' +
      '**Text-Paired Graphs** are graphs where the textual description is defined on the whole graph structure. Such graphs include molecules or proteins where nodes represent atoms and edges represent chemical bonds. The text description can be molecule captions or protein textual features. Although the graph structure is the most significant factor influencing molecular properties, text descriptions of molecules can serve as a complementary knowledge source to help understand molecules [148]. The graph scenarios can be found in Fig. 1.\n' +
      '\n' +
      '### _Categorization of LLM on Graph Techniques_\n' +
      '\n' +
      'According to the roles of LLMs and what are the final components for solving graph-related problems, we classify LLM on graph techniques into three main categories:\n' +
      '\n' +
      '**LLM as Predictor**. This category of methods serves LLM as the final component to output representations or predictions. It can be enhanced with GNNs and can be classified depending on how the graph information is injected into LLM: 1) _Graph as Sequence_: This type of method makes no changes to the LLM architecture, but makes it be aware of graph structure by taking a "graph token sequence" as input. The "graph token sequence" can be natural language descriptions for a graph or hidden representations outputed by graph encoders. 2) _Graph-Empoured LLM_: This type of method modifies the architecture of the LLM base model (_i.e._, Transformers) and enables it to conduct joint text and graph encoding inside their architecture. 3) _Graph-Aware LLM Finetuning_: This type of method makes no changes to the input of the LLMs or LLM architectures, but only fine-tunes the LLMs with supervision from the graph.\n' +
      '\n' +
      '**LLM as Encoder.** This method is mostly utilized for graphs where nodes or edges are associated with text information (solving node-level or edge-level tasks). GNNs are the final components and we adopt LLM as the initial text encoder. To be specific, LLMs are first utilized to encode the text associated with the nodes/edges. The outputted feature vectors by LLMs then serve as input embeddings for GNNs for graph structure encoding. The output embeddings from the GNNs are adopted as final node/edge representations for downstream tasks. However, these methods suffer from convergence issues, sparse data issues, and inefficient issues, where we summarize solutions from optimization, data augmentation, and knowledge distillation perspectives.\n' +
      '\n' +
      '**LLM as Aligner.** This category of methods adopts LLMs as text-encoding components and aligns them with GNNs which serve as graph structure encoding components. LLMs and GNNs are adopted together as the final components for task solving. To be specific, the alignment between LLMs and GNNs can be categorized into 1) _Prediction Alignment_ where the generated pseudo labels from one modality are utilized for training on the other modality in an iterative learning fashion and 2) _Latent Space Alignment_ where contrastive learning is adopted to align text embeddings generated by LLMs and graph embeddings generated by GNNs.\n' +
      '\n' +
      '### _Training & Inference Framework with LLMs_\n' +
      '\n' +
      'There are two typical training and inference paradigms to apply language models on graphs: 1) Pretraining-then-finetuning: typically adopted for medium-scale large language models; and 2) Pretraining-then-prompting: typically adopted for large-scale large language models.\n' +
      '\n' +
      '**Pretraining** denotes training the language model with unsupervised objectives to initialize them with language understanding and inference ability for downstream tasks. Typical pretraining objectives for pure text include masked language modeling [22], auto-regressive causal language modeling [25], corruption-reconstruction language modeling [29] and text-to-text transfer modeling [30]. When extended in the graph domain, language model pretraining strategies include document relation prediction [31], network-contextualized masked language modeling [32], contrastive social prediction [33] and context graph prediction [34].\n' +
      '\n' +
      '**Finetuning** refers to the process of training the language model with labeled data for the downstream tasks. Language model fine-tuning methodology can be further categorized into fully fine-tuning, efficient fine-tuning, and instruction tuning.\n' +
      '\n' +
      '* **Full Finetuning** means updating all the parameters inside the language model. It is the most commonly used fine-tuning method that fully stimulates the language model\'s potential for downstream tasks, but can suffer from heavy computational overload [37] and result in overfitting issues [36].\n' +
      '* **Efficient Finetuning** refers to only fine-tuning a subset of parameters inside the language model. Efficient tuning methods for pure text include prompt tuning [38], prefix tuning [39], adapter [40] and LoRA [41]. Efficient language model fine-tuning methods particularly designed for graph data include graph neural prompt [42] and graph-enhanced prefix [43].\n' +
      '* **Instruction Tuning** denotes fine-tuning language model with downstream task instructions [44][45] to encourage model generalization to unseen tasks in inference. It is an orthogonal concept with full fine-tuning and efficient fine-tuning, in other words, one can choose both full fine-tuning and efficient fine-tuning for instruction tuning. Instruction tuning is adopted in the graph domain for node classification [46], link prediction [47], and graph-level tasks [48].\n' +
      '\n' +
      '**Prompting** is a technique to apply language model for\n' +
      '\n' +
      'Fig. 2: A taxonomy of LLM on graph scenarios and techniques with representative examples.\n' +
      '\n' +
      'downstream task solving without updating the model parameters. One needs to formulate the test samples into natural language sequences and ask the language model to directly conduct inference based on the in-context demonstrations. This is a technique particularly popular for large-scale autoregressive language models. Apart from direct prompting, following-up works propose chain-of-thought prompting [49], tree-of-thought prompting [50], and graph-of-thought prompting [51].\n' +
      '\n' +
      'In the following sections, we will follow our categorization in Section 3 and discuss detailed methodologies for each graph scenario.\n' +
      '\n' +
      '## 4 Pure Graphs\n' +
      '\n' +
      'Problems on pure graphs provide a fundamental motivation for why and how LLMs are introduced into graph-related reasoning problems. Investigated thoroughly in graph theory, pure graphs serve as a universal representation format for a wide range of classical algorithmic problems in all perspectives in computer science. Many graph-based concepts, such as shortest paths, particular sub-graphs, and flow networks, have strong connections with real-world applications [133, 134, 135]. Therefore, pure graph-based reasoning is vital in providing theoretical solutions and insights for reasoning problems grounded in real-world applications.\n' +
      '\n' +
      'Nevertheless, many reasoning tasks require a computation capacity beyond traditional GNNs. GNNs are typically designed to carry out a bounded number of operations given a graph size. In contrast, graph reasoning problems can require up to indefinite complexity depending on the task\'s nature. Training conventional GNNs on general reasoning-intensive problems is challenging without prior assumptions and specialized model design. This fundamental gap motivates researchers to seek to incorporate LLMs in graph problems. On the other hand, LLMs demonstrate excellent emergent reasoning ability [110, 49, 111] recently. This is partially due to their autoregressive mechanism, which enables computing indefinite sequences of intermediate steps with careful prompting or training [49, 50].\n' +
      '\n' +
      'The following subsections discuss the attempts to incorporate LLMs into pure graph reasoning problems. We will also discuss these works\' challenges, limitations, and findings. Table II lists a rough categorization of these efforts. Usually, input graphs are serialized as part of the input sequence, either by verbalizing the graph structure [124, 125, 126, 126, 127, 128, 129, 130, 126] or by encoding the graph structure into implicit feature sequences [43]. The studied reasoning problems range from simpler ones like connectivity, shortest paths, and cycle detection to harder ones like maximum flow and Hamiltonian pathfinding (an NP-complete problem). A comprehensive list of the studied problems is listed in Table IV. Note that we only list representative problems here. This table does not include more domain-specific problems, such as the spatial-temporal reasoning problems in [128].\n' +
      '\n' +
      '### _Direct Answering_\n' +
      '\n' +
      'Although graph-based reasoning problems usually involve complex computation, researchers still attempt to let language models directly generate answers from the serialized input graphs as a starting point or a baseline, partially because of the simplicity of the approach and partially in awe of other emergent abilities of LLMs. This approach can be viewed as a probe of graph understanding of LLMs (in contrast to graph "reasoning"), which tests if LLMs acquire a good enough representation internally for LLMs to directly "guess" the answers. Although various attempts have been made to optimize how graphs are presented in the input sequence, which we will discuss in the following sections, bounded by the finite sequence length and computational operations, there is a fundamental limitation of this approach to solving complex reasoning problems such as NP-complete ones. Unsurprisingly, most studies find that LLMs possess preliminary graph understanding ability, but the performance is less satisfactory on more complex problems or larger graphs [124, 125, 126, 128, 131, 43]. In the following, we will discuss the details of these studies based on their input representation methods and their main difference.\n' +
      '\n' +
      '**Plainly Verbalizing Graphs.** Verbalizing the graph structure in natural language is the most straightforward way of representing graphs. Representative approaches include describing the edge and adjacency lists, widely studied in [124, 125, 131, 128]. For example, for a triangle graph with three nodes, the edge list can be written as "_(0, 1), (1, 2), (2, 0)]_", which means node 0 is connected to node 1, node 1 is connected to node 2, node 2 is connected to node 0. It can also be written in natural language such as "_There is an edge between node 0 and node 1, an edge between node 1 and node 2, and an edge between node 2 and node 0."_ On the other hand, we can describe the adjacency list from the nodes\' perspective. For example, for the same triangle graph, the adjacency list can be written as _"Node 0 is connected to node 1 and node 2. Node 1 is connected to node 0 and node 2. Node 2 is connected to node 0 and node 1."_ On these inputs, one can prompt LLMs to answer questions either in zero-shot or few-shot (in-context learning) settings, the former of which is to directly ask questions given the graph structure, while the latter is to ask questions about the graph structure after providing a few examples of questions and answers. [124, 125, 126] do confirm that LLMs can answer easier questions such as connectivity, neighbor identification, and graph size counting but fail to answer more complex questions such as cycle detection and Hamiltonian pathfinding. Their results also reveal that providing more examples in the few-shot setting increases the performance, especially on easier problems, although it is still not satisfactory.\n' +
      '\n' +
      '**Paraphrasing Graphs.** The verbalized graphs can be lengthy, unstructured, and complicated to read, even for humans, so they might not be the best input format for LLMs to infer the answers. To this end, researchers also attempt to paraphrase the graph structure into more natural or concise sentences. [126] find that by prompting LLMs to generate a format explanation of the raw graph inputs for itself (_Format-Explanation_) or to pretend to play a role in a natural task (_Role Prompting_), the performance on some problems can be improved but not systematically. [131] explores the effect of grounding the pure graph in a real-world scenario, such as social networks, friendship graphs, or co-authorship graphs. In such graphs, nodes are described as people, and edges are relationships between people. Results indicate that encoding in real-world scenarios can improve the performance on some problems, but still not consistently.\n' +
      '\n' +
      '**Encoding Graphs Into Implicit Feature Sequences**. Finally, researchers also attempt to encode the graph structure into implicit feature sequences as part of the input sequence [43]. Unlike the previous verbalizing approaches, this usually involves training a graph encoder to encode the graph structure into a sequence of features and fine-tuning the LLMs to adapt to the new input format. [43] demonstrates drastic performance improvement on problems including substructure counting, maximum triplet sum, shortest path, and bipartite matching, evidence that fine-tuning LLMs has great fitting power on a specific task distribution.\n' +
      '\n' +
      '### _Heuristic Reasoning_\n' +
      '\n' +
      'Direct mapping to the output leverages the LLMs\' powerful representation power to "guess" the answers. Still, it does not fully utilize the LLMs\' impressive emergent reasoning ability, which is essential for solving complex reasoning problems. To this end, attempts have been made to let LLMs perform heuristic reasoning on graphs. This approach encourages LLMs to perform a series of intermediate reasoning steps that might heuristically lead to the correct answer.\n' +
      '\n' +
      '**Reasoning Step by Step**. Encouraged by the success of chain-of-thought (CoT) reasoning [49, 111], researchers also attempt to let LLMs perform reasoning step by step on graphs. Chain-of-thought encourages LLMs to roll out a sequence of reasoning steps to solve a problem, similar to how humans solve problems. It usually incorporates a few demonstrative examples to guide the reasoning process. Zero-shot CoT is a similar approach that does not require any examples. These techniques are studied in [128, 124, 43, 125, 126, 131, 43]. Results indicate that CoT-style reasoning can improve the performance on simpler problems, such as cycle detection and shortest path. Still, the improvement is inconsistent or diminishes on more complex problems, such as Hamiltonian path finding and topological sorting.\n' +
      '\n' +
      '**Retrieving Subgraphs as Evidence**. Many graph reasoning problems, such as node degree counting and neighborhood detection, only involve reasoning on a subgraph of the whole graph. Such properties allow researchers to let LLMs retrieve the subgraphs as evidence and perform reasoning on the subgraphs. Build-a-Graph prompting [124] encourages LLMs to reconstruct the relevant graph structures to the questions and then perform reasoning on them. This method demonstrates promising results on problems except for Hamiltonian pathfinding, a notoriously tricky problem requiring reasoning on the whole graph. Another approach, Context-Summarization [126], encourages LLMs to summarize the key nodes, edges, or sub-graphs and perform reasoning. They evaluate only on node classification, and results show improvement when combined with CoT-style reasoning, an intuitive outcome considering the local nature of the node classification problem.\n' +
      '\n' +
      '**Searching on Graphs**. This kind of reasoning is related to the search algorithms on graphs, such as breadth-first search (BFS) and depth-first search (DFS) Although not universally applicable, BFS and DFS are the most intuitive and effective ways to solve some graph reasoning problems. Numerous explorations have been made to simulate searching-based reasoning, especially on knowledge-graph question\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{113.8pt} p{113.8pt} p{113.8pt} p{113.8pt} p{113.8pt}} \\hline \\hline Method & Graph Format or Encoding & Reasoning Process & Reasoning Category & Papers \\\\ \\hline Zero-Shot & Verbaliized edge or adjacency list. & Directly answering. & Direct Answering & [124, 125, 126, 128, 131] \\\\ \\hline Role Promping & Verbaliized edge or adjacency list. & Directly answering by designating a specific role to the LLM. & Direct Answering & [126] \\\\ \\hline Format Explanation & Verbaliized edge or adjacency list. & Encouraging the LLM to explain the input graph format first. & Direct Answering & [126] \\\\ \\hline GraphLLM & Prefix tokens encoded by a graph encoder. & Directly answering. & Direct Answering & [43] \\\\ \\hline Few-Shot (In-Context Learning) & Verbaliized edge or adjacency lists preceded with a few demonstrative examples. & Directly answering by following the examples. & Direct Answering & [124, 125, 128, 131] \\\\ \\hline Chain-of-Thought & Verbaliized edge or adjacency lists preceded with a few demonstrative examples. & Reasoning through a series of intermediate reasoning steps in the generation following the examples. & Heuristic Reasoning & [124, 125, 126, 128, 131, 132] \\\\ \\hline Self-Consistency & Verbaliized edge or adjacency lists preceded with a few demonstrative examples. & Reasoning through a series of intermediate reasoning steps in generation, and then selecting the most consistent answer. & Heuristic Reasoning & [124] \\\\ \\hline Build-a-Graph list. & Verbaliized edge or adjacency list. & Reconstructing the graph in output, and then reasoning on the graph. & Heuristic Reasoning & [124, 131] \\\\ \\hline Context-Summarization & Verbaliized edge or adjacency list. & Directly answering by first summarizing the key elements in the graph. & Heuristic Reasoning & [126] \\\\ \\hline Reasoning-on-Graph & Retrieved paths from external graphs. & First, plan the reasoning process in the form of paths to be retrieved and then infer on the retrieved paths. & Heuristic Reasoning & [129] \\\\ \\hline Iterative Reading-then- Reasoning & Retrieved neighboring edges or nodes from external graphs. & Iteratively retrieving neighboring edges or nodes from external graphs and inferring from the retrieved information. & Heuristic Reasoning & [130, 132] \\\\ \\hline Algorithmic Reasoning & Verbaliized edge or adjacency list. & Simulating the reasoning process of a relevant algorithm in a generation. & Algorithmic Reasoning & [124] \\\\ \\hline Calling APIs & External Knowledge Base. & Generate the reasoning process as (probably nested) API calls to be executed externally on the knowledge base. & Algorithmic Reasoning & [132, 127] \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE II: A collection of LLM reasoning methods on pure graph discussed in Section 4. We do not include the backbone models used in these methods studied in the original papers, as these methods generally apply to any LLMs. The “Papers” column lists the papers that study the specific methods.\n' +
      '\n' +
      'answering. This approach enjoys the advantage of providing interpretable evidence besides the answer. Reasoning-on-Graphs (RoG) [129] is a representative approach that prompts LLMs to generate several relation paths as plans, which are then retrieved from the knowledge graph (KG) and used as evidence to answer the questions. Another approach is to iteratively retrieve and reason on the subgraphs from KG [130, 132], simulating a dynamic searching process. At each step, the LLMs retrieve neighbors of the current nodes and then decide to answer the question or continue the next search step.\n' +
      '\n' +
      '### _Algorithmic Reasoning_\n' +
      '\n' +
      'The previous two approaches are heuristic, which means that the reasoning process accords with human intuition but is not guaranteed to lead to the correct answer. In contrast, these problems are usually solved by algorithms in computer science. Therefore, researchers also attempt to let LLMs perform algorithmic reasoning on graphs. [124] proposed "_Algorithmic Prompting_", which prompts the LLMs to recall the algorithms that are relevant to the questions and then perform reasoning step by step according to the algorithms. Their results, however, do not show consistent improvement over the heuristic reasoning approach, such\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{56.9pt} p{113.8pt} p{113.8pt} p{113.8pt} p{113.8pt}} \\hline \\hline\n' +
      '**Problem** & **Definition** & **Applications** & **Typical Complexity** & **Papers** \\\\ \\hline Connectivity & Given a graph \\(\\mathcal{G}\\) and two nodes \\(u\\) and \\(v\\), tell if they are connected by a path. & Relationship Detection, Link Prediction & \\(O(|E|)\\) or \\(O(V^{2})\\) & [124, 125] \\\\ \\hline Neighbor Detection & Given a graph \\(\\mathcal{G}\\) and a node \\(v\\), find the nodes \\(v\\). & Recommendation, Knowledge QA & \\(O(\\min(|E|,|V|))\\) & [126] \\\\ \\hline Node Degree & Given a graph \\(\\mathcal{G}\\) and a node \\(v\\), find the number of edges connected to \\(v\\). & Entity Popularity, Importance Ranking & \\(O(\\min(|E|,|V|))\\) & [125, 126] \\\\ \\hline Attribute Retrieval & Given a graph \\(\\mathcal{G}\\) with node-level information and a node \\(v\\), return the attribute of \\(v\\). & Recommendation, Node Classification, Node QA & \\(O(1)\\) & [126] \\\\ \\hline Graph Size & Given a graph \\(\\mathcal{G}\\), find the number of nodes and edges. & Graph-level Classification & \\(O(|V|+|E|)\\) & [126] \\\\ \\hline Cycle Detection & Given a graph \\(\\mathcal{G}\\), tell if it contains a cycle. & Loop Elimination, Program Loop Detection & \\(O(|V|)\\) & [124] \\\\ \\hline Diameter & Given a graph \\(\\mathcal{G}\\), find the diameter of \\(\\mathcal{G}\\). & Graph-level Classification, Clustering & \\(O(|V^{\\mathcal{G}})\\) or \\(O(|V|^{2}\\log|V|+|V|E|)\\) & [126] \\\\ \\hline Topological Sort & Given a directed acyclic graph \\(\\mathcal{G}\\), find a topological ordering of its vertices so that for every edge \\((u,v)\\), \\(u\\) comes before \\(v\\) in the ordering. & Timeline Generation, Dependency Parsing, Scheduling & \\(O(|V|+|E|)\\) & [124] \\\\ \\hline Wedge or Triangle Detection & Given a graph \\(\\mathcal{G}\\) and a vertex \\(v\\), identify if there is a wedge or triangle centered at \\(v\\). & Relationship Detection, Link Prediction & \\(O(|V|+|E|)\\) & [125] \\\\ \\hline Maximum Triplet Sum & Given a graph \\(\\mathcal{G}\\), find the maximum sum of the weights of three vertices that are connected. & Community Detection & \\(O(|V|^{\\mathcal{G}})\\) & [43] \\\\ \\hline Shortest Path & Given a graph \\(\\mathcal{G}\\) and two nodes \\(u\\) and \\(v\\), find the shortest path between \\(u\\) and \\(v\\). & Navigation, Planning & \\(O(|E|)\\) or \\(O(V^{2})\\) & [124, 125] \\\\ \\hline Maximum Flow & Given a directed graph \\(\\mathcal{G}\\) with a source node \\(s\\) and a sink node \\(t\\), find the maximum flow from \\(s\\) to \\(t\\). & Transportation Planning, Network Design & \\(O(|V|E|^{2})\\), \\(O(|E|V|\\log|V|)\\) or \\(O(|V|^{3})\\) & [124] \\\\ \\hline Bipartite Graph Matching & Given a bipartite graph \\(\\mathcal{G}\\) with two disjoint sets of vertices \\(v_{1}\\) and \\(v_{2}\\), find a matching between \\(V_{1}\\) and \\(v_{2}\\) that maximizes the number of matched pairs. & Recommendation, Resource Allocation, Scheduling & \\(O(|E|\\sqrt{|V|})\\) & [124, 43] \\\\ \\hline Graph Neural Networks & Given a graph \\(\\mathcal{G}\\) with node features \\(\\mathbf{X}\\) of dimension \\(\\delta\\), simulate a graph neural networks with \\(\\mathcal{G}\\) layers and return the encoded node features & Node Classification, Graph-level Classification & \\(O(|V|^{2})\\) & [124] \\\\ \\hline Clustering Coefficient & Given a graph \\(\\mathcal{G}\\), find the clustering coefficient of \\(\\mathcal{G}\\). & Community Detection, Node Clustering & \\(O(|V|^{\\mathcal{G}})\\) & [126] \\\\ \\hline Substructure Counting & Given a graph \\(\\mathcal{G}\\) and a subgraph \\(\\mathcal{G}^{\\prime}\\), count the number of occurrences of \\(\\mathcal{G}^{\\prime}\\) in \\(\\mathcal{G}\\). & Pattern Matching, Subgraph Detection, Abnormality Detection & NP-Complete & [43] \\\\ \\hline Hamilton Path & Given a graph \\(\\mathcal{G}\\), find a path that visits every vertex exactly once. & Route Planning, Drilling Machine Planning, DNA Sequencing & NP-Complete & [124] \\\\ \\hline (Knowledge) Graph QA & Given a (knowledge) graph \\(\\mathcal{G}\\) and a question \\(\\mathcal{G}\\), find the answer to \\(\\mathcal{G}\\). & Dialogue System, Smart Assistant, Recommendation & -- & [126, 129, 130, 131, 132] \\\\ \\hline Graph Query Language Generation & Given a graph \\(\\mathcal{G}\\) and a query \\(\\mathcal{G}\\), generate a query language that can be used to query \\(\\mathcal{G}\\). & Graph Summarization, FAQ Generation, Query Suggestions & [126] \\\\ \\hline Node Classification & Given a graph \\(\\mathcal{G}\\), predict the class of a node \\(v\\). & Recommendation, User Profile, Algorithmy Detection & -- & [126, 127] \\\\ \\hline Graph Classification & Given a graph \\(\\mathcal{G}\\), predict the class of \\(\\mathcal{G}\\). & Molecule Property Prediction, Molecule QA, Graph QA & [127, 128] \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE III: A collection of pure graph reasoning problems studied in Section 4. \\(\\mathcal{G}=(V,\\mathcal{E})\\) denotes a graph with vertices \\(\\mathcal{G}\\) and edges \\(\\mathcal{E}\\). \\(v\\) and \\(c\\) denote individual vertices and edges, respectively. The “Papers” column lists the time complexity of standard algorithms for the problem, ignoring more advanced but complex algorithms that are not comparable to LLMs’ reasoning processes.\n' +
      '\n' +
      'as the BaG prompting proposed in the same paper. This might be because the algorithmic reasoning approach is still complex to simulate without more careful techniques. A more direct approach, Graph-ToolFormer [127], lets LLMs generate API calls as explicit reasoning steps. These API calls are then executed externally to acquire answers on an external graph. This approach is suitable for converting tasks grounded in real tasks into pure graph reasoning problems, demonstrating efficacy on various applications such as knowledge graphs, social networks, and recommender systems.\n' +
      '\n' +
      '### _Discussion_\n' +
      '\n' +
      'The above approaches are not mutually exclusive, and they can be combined to achieve better performance. Moreover, strictly speaking, heuristic reasoning can also conduct direct answering, while algorithmic reasoning contains the capacity of heuristic reasoning as a special case. Researchers are advised to select the most suitable approach for a specific problem. For example, direct answering is suitable for problems that are easy to solve and where the pre-training dataset provides sufficient bias for a good guess, such as common entity classification and relationship detection. Heuristic reasoning is suitable for problems that are hard to deal with explicitly, but intuition can provide some guidance, such as graph-based question answering and knowledge graph reasoning. Algorithmic reasoning is suitable for problems that are hard to solve but where the algorithmic solution is well-defined, such as route planning and pattern matching.\n' +
      '\n' +
      '## 5 Text-Rich Graphs.\n' +
      '\n' +
      'Graphs with node/edge-level textual information (text-rich graphs) exist ubiquitously in the real world, _e.g._, academic networks, social networks, and legal case networks. Learning on such networks requires the model to encode both the textual information associated with the nodes/edges and the structure information lying inside the input graph. Depending on the role of LLM, existing works can be categorized into three types: LLM as Predictor, LLM as Encoder, and LLM as Aligner. We summarize all surveyed methods in Table V.\n' +
      '\n' +
      '### _LLM as Predictor_\n' +
      '\n' +
      'These methods serve the language model as the main model architecture to capture both the text information and graph structure information. They can be categorized into three types: _Graph as Sequence methods_, _Graph-Empowered LLMs_, and _Graph-Aware LLM finetuning methods_, depending on how structure information in graphs is injected into language models (input vs. architecture vs. loss). In the _Graph as Sequence methods_, graphs are converted into sequences that can be understood by language models together with texts from the inputs. In the _Graph-Empowered LLMs_, people modify the architecture of Transformers (which is the base architecture for LLMs) to enable it to encode text and graph structure simultaneously. In the _Graph-Aware LLM finetuning methods_, LLM is fine-tuned with graph structure supervision and can generate graph-contextualized representations.\n' +
      '\n' +
      '#### 5.1.1 Graph as Sequence.\n' +
      '\n' +
      'In these methods, the graph information is mainly encoded into the LLM from the "input" side. The ego-graphs associated with nodes/edges are serialized into a sequence \\(\\mathbf{H}_{\\mathcal{G}_{v}}\\), which can be fed into the LLM together with the texts \\(d_{v}\\):\n' +
      '\n' +
      '\\[\\mathbf{H}_{\\mathcal{G}_{v}}=\\text{Graph2Seq}(\\mathcal{G}_{v}), \\tag{7}\\] \\[\\mathbf{h}_{v}=\\text{LLM}([\\mathbf{H}_{\\mathcal{G}_{v}},d_{v}]). \\tag{8}\\]\n' +
      '\n' +
      'Depending on the choice of \\(\\text{Graph2Seq}(\\cdot)\\) function, the methods can be further categorized into rule-based methods and GNN-based methods. The illustration of the categories can be found in Fig. 3.\n' +
      '\n' +
      '**Rule-based: Linearizing Graphs into Text Sequence with Rules.** These methods design rules to describe the structure with natural language and adopt a text prompt template as \\(\\text{Graph2Seq}(\\cdot)\\). For example, given an ego-graph \\(\\mathcal{G}_{v_{i}}\\) of the paper node \\(v_{i}\\) connecting to author nodes \\(v_{j}\\) and \\(v_{k}\\) and venue nodes \\(v_{t}\\) and \\(v_{s}\\), \\(\\mathbf{H}_{\\mathcal{G}_{v_{i}}}=\\text{Graph2Seq}(\\mathcal{G}_{v_{i}})=\\)_"The center paper node is \\(v_{i}\\). Its author neighbor nodes are \\(v_{j}\\) and \\(v_{k}\\) and its venue neighbor nodes are \\(v_{t}\\) and \\(v_{s}\\)"_. This is the most straightforward and easiest way (without introducing extra model parameters) to encode graph structures into language models. Along this line, InstructGLM [47] designs templates to describe local ego-graph structure (maximum 3-hop connection) for each node and conduct instruction tuning for node classification and link prediction. GraphText [66] further proposes a syntax tree-based method to structure into text sequence. Researchers [84] also study when and why the linearized structure information on graphs can improve the performance of LLM on node classification and find that the structure information is beneficial when the textual information associated with the node is scarce.\n' +
      '\n' +
      '**GNN-based: Encoding Graphs into Special Tokens with GNNs.** Different from rule-based methods which use natural language prompts to linearize graphs into sequences, GNN-based methods adopt graph encoder models (_i.e._, GNN) to encode the ego-graph associated with nodes into special token representations which are concatenated with the pure text information into the language model:\n' +
      '\n' +
      '\\[\\mathbf{H}_{\\mathcal{G}_{v}}=\\text{Graph2Seq}(\\mathcal{G}_{v})=\\text{Graph Enc}(\\mathcal{G}_{v}). \\tag{9}\\]\n' +
      '\n' +
      'The strength of these methods is that they can capture the hidden representations of useful structure information with a strong graph encoder, while the challenge is how to fill the gap between graph modality and text modality. GNP [42] adopts a similar philosophy from LLaVA [92], where they utilize GNN to generate graph tokens and then project the graph tokens into the text token space with learnable projection matrices. The projected graph tokens are concatenated with text tokens and fed into the language model. GraphGPT [46] further proposes to train a text-grounded GNN for the projection with a text encoder and contrastive learning. DGTL [77] introduces disentangled graph learning, serves graph representations as positional encoding, and adds them to the text sequence. METERN [76] adds learnable relation embeddings to node textual sequences for text-based multiplex representation learning on graphs [93].\n' +
      '\n' +
      '#### 5.1.2 Graph-Empowered LLMs.\n' +
      '\n' +
      'In these methods, researchers design advanced LLM architecture (_i.e._, Graph-Empowered LLMs) which can conduct joint text and graph encoding inside their model architecture. Transformers [44] serve as the base model for nowadays pre-trained LMs [22] and LLMs [37]. However, they are designed for natural language (sequence) encoding and do not take non-sequential structure information into consideration. To this end, Graph-Empowered LLMs are proposed. They have a shared philosophy of introducing virtual structure tokens \\(\\mathbf{H}_{\\mathcal{G}_{v}}\\) inside each Transformer layer:\n' +
      '\n' +
      '\\[\\widetilde{\\mathbf{H}}_{d_{v}}^{(l)}=[\\mathbf{H}_{\\mathcal{G}_{v}}^{(l)},\\mathbf{H}_{d_{v }}^{(l)}] \\tag{10}\\]\n' +
      '\n' +
      'where \\(\\mathbf{H}_{\\mathcal{G}_{v}}\\) can be learnable embeddings or output from graph encoders. Then the original multi-head attention (MHA) in Transformers is modified into an asymmetric MHA to take the structure tokens into consideration:\n' +
      '\n' +
      '\\[\\begin{split}\\mathrm{MHA}_{asy}(\\mathbf{H}_{d_{v}}^{(l)},\\widetilde{ \\mathbf{H}}_{d_{v}}^{(l)})&=\\|_{u=1}^{U}\\ \\mathrm{head}_{u}(\\mathbf{H}_{d_{v}}^{(l)},\\widetilde{\\mathbf{H}}_{d_{v}}^{(l)}),\\\\ \\text{where}\\ \\ \\ \\mathrm{head}_{u}(\\mathbf{H}_{d_{v}}^{(l)},\\widetilde{ \\mathbf{H}}_{d_{v}}^{(l)})&=\\mathrm{softmax}\\bigg{(}\\frac{\\mathbf{Q}_{u}^{(l) }\\widetilde{\\mathbf{K}}_{u}^{(l)\\top}}{\\sqrt{d/U}}\\bigg{)}\\cdot\\widetilde{\\mathbf{V}}_ {u}^{(l)},\\\\ \\mathbf{Q}_{u}^{(l)}=\\mathbf{H}_{d_{v}}^{(l)}\\mathbf{W}_{Q,u}^{(l)},& \\ \\widetilde{\\mathbf{K}}_{u}^{(l)}=\\widetilde{\\mathbf{H}}_{d_{v}}^{(l)}\\mathbf{W}_{k,u}^{(l)}, \\ \\ \\widetilde{\\mathbf{V}}_{u}^{(l)}=\\widetilde{\\mathbf{H}}_{d_{v}}^{(l)}\\mathbf{W}_{V,u}^{(l)}. \\end{split} \\tag{11}\\]\n' +
      '\n' +
      'With the asymmetric MHA mechanism, the node encoding process of the \\((l+1)\\)-th layer will be:\n' +
      '\n' +
      '\\[\\begin{split}\\widetilde{\\mathbf{H}}_{d_{v}}^{(l)^{\\prime}}=\\mathrm{ Normalize}(\\mathbf{H}_{d_{v}}^{(l)}+\\mathrm{MHA}_{asy}(\\widetilde{\\mathbf{H}}_{d_{v}}^{(l)}, \\mathbf{H}_{d_{v}}^{(l)})),\\\\ \\mathbf{H}_{d_{v}}^{(l+1)}=\\mathrm{Normalize}(\\widetilde{\\mathbf{H}}_{d_ {v}}^{(l)^{\\prime}}+\\mathrm{MLP}(\\widetilde{\\mathbf{H}}_{d_{v}}^{(l)^{\\prime}})). \\end{split} \\tag{12}\\]\n' +
      '\n' +
      'Along this line of work, GreaseLM [68] proposes to have a language encoding component and a graph encoding component in each layer. The two components interact through an MInt layer, where a special structure token is added to the text Transformer input, and a special node is added to the graph encoding layer. DRAGON [83] further proposes strategies to pretrain GreaseLM with unsupervised signals. GraphFormers [73] are designed for node representation learning on homogeneous text-attributed networks where the current layer [CLS] token hidden states of neighboring documents are aggregated and added as a new token on the current layer center node text encoding. Patton [32] further proposes to pretrain GraphFormers with two novel strategies: network-contextualized masked language modeling and masked node prediction. Heterformer [74] is introduced for learning representations on heterogeneous text-attributed networks where some nodes are associated with text (text-rich) and others are not (textless). Virtual neighbor tokens for text-rich neighbors and textless neighbors are concatenated with the original text tokens and inputted into each Transformer layer. Edgeformers [75] are proposed for representation learning on textual-edge networks where edges are associated with rich textual information. When conducting edge encoding, virtual node tokens will be concatenated onto the original edge text tokens for joint encoding.\n' +
      '\n' +
      '#### 5.1.3 Graph-Aware LLM finetuning.\n' +
      '\n' +
      'In these methods, the graph information is mainly injected into the LLM by "fine-tuning on graphs". Researchers assume that the structure of graphs can provide hints on what documents are " semantically similar" to what other documents. For example, papers citing each other in an academic graph can be of similar topics; items co-purchased by many users in an e-commerce graph can be of related functions. These methods adopt vanilla language models that take text as input (_e.g._, BERT [22] and SciBERT [24]) as the base model and fine-tune them with structure signals on the graph. After that, the LLMs will learn node/edge representations that capture the graph homophily from the text perspective.\n' +
      '\n' +
      'Most methods adopt the two-tower encoding and training pipeline, where the representation of each node is obtained separately:\n' +
      '\n' +
      '\\[\\mathbf{h}_{v_{i}}=\\mathrm{LLM}_{\\theta}(d_{v_{i}}), \\tag{13}\\]\n' +
      '\n' +
      'and the model is optimized by\n' +
      '\n' +
      '\\[\\min_{\\theta}f(\\mathbf{h}_{v_{i}},\\{\\mathbf{h}_{v_{i}^{+}}\\},\\{\\mathbf{h}_{v_ {i}^{-}}\\}). \\tag{14}\\]\n' +
      '\n' +
      'Here \\(v_{i}^{+}\\) represents the positive nodes to \\(v_{i}\\), \\(v_{i}^{-}\\) represents the negative nodes to \\(v_{i}\\) and \\(f(\\cdot)\\) denotes the pairwise training objective. Different methods have different strategies for \\(v_{i}^{-}\\) and \\(v_{i}^{-}\\) with different training objectives \\(f(\\cdot)\\). SPECTER [52] constructs the positive text/node pairs with the citation relation, explores random negatives and structure hard negatives, and fine-tunes SciBERT [24] with the triplet loss. SciNCL [53] extends SPECTER by introducing more advanced positive and negative sampling methods based on embeddings trained on graphs. Touchup-G [55] proposes the measurement of feature homophily on graphs and brings up a binary cross-entropy fine-tuning objective. TwHIN-BERT [57] mines positive node pairs with off-the-shelf heterogeneous information network embeddings and trains the model with a contrastive social loss. MICoL [60] discovers\n' +
      '\n' +
      'Fig. 3: The illustration of various LLM as Predictor methods, including (a) Rule-based Graph As Sequence, (b) GNN-based Graph As Sequence, (c) Graph-Empowered LLMs.\n' +
      '\n' +
      'semantically positive node pairs with meta-path [91] and adopts the InfoNCE objective. E2EG [61] utilizes a similar philosophy from GIANT [59] and adds a neighbor prediction objective apart from the downstream task objective. A summarization of the two-tower graph-centric LLM fine-tuning objectives can be found in Table IV.\n' +
      '\n' +
      'There are other methods using the one-tower pipeline, where node pairs are concatenated and encoded together:\n' +
      '\n' +
      '\\[\\mathbf{h}_{v_{i},v_{j}}=\\text{LLM}_{\\theta}(d_{v_{i}},d_{v_{j}}), \\tag{15}\\] \\[\\min_{\\theta}f(\\mathbf{h}_{v_{i},v_{j}}). \\tag{16}\\]\n' +
      '\n' +
      'LinkBERT [31] proposes a document relation prediction objective (an extension of next sentence prediction in BERT [22]) which aims to classify the relation of two node text pairs from contiguous, random, and linked. MICoL [60] explores predicting the node pairs\' binary meta-path or meta-graph indicated relation with the one-tower language model.\n' +
      '\n' +
      '#### 5.1.4 Discussion\n' +
      '\n' +
      'We provide simple guidance on the selection of the methods above when facing real-world problems based on your tasks: 1) Representation Learning: Graph-Aware LLM fine-tuning (more efficient but less effective) and Graph-Empowered LLMs (less efficient but more effective); 2) Generation: Rule-based Graph As Sequence (support zero-shot, limited expressiveness) and GNN-based Graph As Sequence (need training, stronger expressiveness). Although the community is making good progress, there are still some open questions to be solved.\n' +
      '\n' +
      '**Graph as Code Sequence.** Existing graphs as sequence methods are mainly rule-based or GNN-based. The former relies on natural language to describe the graphs which is not natural for structure data, while the latter has a GNN component that needs to be trained. A more promising way is to obtain a structure-aware sequence for graphs that can support zero-shot inference. A potential solution is to adopt codes (that can capture structures) to describe the graphs and utilize code LLMs [21].\n' +
      '\n' +
      '**Advanced Graph-Empowered LLM techniques.** Graph-empowered LLM is a promising direction to achieve foundational models for graphs. However, existing works are far from enough: 1) _Task_. Existing methods are mainly designed for representation learning (with encoder-only LLMs) which are hard to adopt for generation tasks. A potential solution is to design Graph-Empowered LLMs with decoder-only or encoder-decoder LLMs as the base architecture. 2) _Pretraining_. Pretraining is important to enable LLMs with contextualized data understanding capability, which can be generalized to other tasks. However, existing works mainly focus on pretraining LLMs on homogeneous text-rich networks. Future studies are needed to explore LLM pretraining in more diverse real-world scenarios including heterogeneous text-rich networks [74], dynamic text-rich networks [128], and textual-edge networks [75].\n' +
      '\n' +
      '### _LLM as Encoder_\n' +
      '\n' +
      'LLMs extract textual features to serve as initial node feature vectors for GNNs, which then generate node/edge representations and make predictions. These methods typically adopt an LLM-GNN cascaded architecture to obtain the final representation \\(\\mathbf{h}_{v_{i}}\\) for node \\(v_{i}\\):\n' +
      '\n' +
      '\\[\\mathbf{x}_{v_{i}}=\\text{LLM}(d_{v_{i}}) \\tag{17}\\] \\[\\mathbf{h}_{v_{i}}=\\text{GNN}(\\mathbf{X}_{v},\\mathcal{G}). \\tag{18}\\]\n' +
      '\n' +
      'Here \\(\\mathbf{x}_{v_{i}}\\) is the feature vector that captures the textual information \\(d_{v_{i}}\\) associated with \\(v_{i}\\). The final representation \\(\\mathbf{h}_{v_{i}}\\) will contain both textual information and structure information of \\(v_{i}\\) and can be used for downstream tasks. In the following sections, we will discuss the optimization, augmentation, and distillation of such models. The figures for these techniques can be found in Fig. 4.\n' +
      '\n' +
      '#### 5.2.1 Optimization\n' +
      '\n' +
      '**One-step training** refers to training the LLM and GNN together in the cascaded architecture for the downstream tasks. TextGNN [79] explores GCN [85], GraphSAGE [86], GAT [87] as the base GNN architecture, add skip connection between LLM output and GNN output, and optimize the whole architecture for sponsored search task. AdsGNN [80] further extends TextGNN by proposing edge-level information aggregation. GNN-LM [67] adds GNN layers to enable the vanilla language model to reference similar contexts in the corpus for language modeling. Joint training LLMs and GNNs in a cascaded pipeline is straightforward but may suffer from efficiency [69] (only support sampling a\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Method & positive \\(v_{i}^{+}\\) & negative \\(v_{i}^{-}\\) & Objective \\(f(\\cdot)\\) \\\\ \\hline SPECTER [52] & \\((v_{i},v_{i}^{+})\\in\\mathcal{E}\\) & \\((v_{i},v_{i}^{-})\\notin\\mathcal{E}\\); & \\(\\max\\{\\|\\mathbf{h}_{v_{i}}-\\mathbf{h}_{v_{i}^{-}}\\|_{2}-\\|\\mathbf{h}_{v_{i}^{-}}- \\mathbf{h}_{v_{i}^{-}}\\|_{2}+m,0\\}\\) \\\\ \\hline SciNCL [53] & \\(\\|\\mathbf{h}_{v_{i}}-\\mathbf{h}_{v_{i}^{-}}\\|_{2}\\in(\\hat{k}^{+}-e^{+};k^{+}]\\) & \\(\\|\\mathbf{h}_{v_{i}}-\\mathbf{h}_{v_{i}^{-}}\\|_{2}\\in(\\hat{k}^{-}_{hard}-e^{-}_{ hard};k^{-}_{hard})\\) & \\(\\max\\{\\|\\mathbf{h}_{v_{i}}-\\mathbf{h}_{v_{i}^{-}}\\|_{2}-\\|\\mathbf{h}_{v_{i}^{-}}\\|_ {2}+m,0\\}\\) \\\\ \\hline Touchup-G [55] & \\((v_{i},v_{i}^{+})\\in\\mathcal{E}\\) & \\((v_{i},v_{i}^{-})\\notin\\mathcal{E}\\) & \\(\\log(\\mathbf{h}_{v_{i}}\\cdot\\mathbf{h}_{v_{i}^{+}})+\\log(1-\\mathbf{h}_{v_{i}} \\cdot\\mathbf{h}_{v_{i}^{-}})\\) \\\\ \\hline TwHN-BERT [57] & \\(\\cos(\\mathbf{x}_{v_{i}},\\mathbf{x}_{v_{i}^{-}})<k\\) & in-batch random & \\(-\\log\\frac{\\exp((\\mathbf{h}_{v_{i}}\\cdot\\mathbf{h}_{v_{i}^{-}})/\\eta)}{\\sum_{v_{i} ^{-}}\\exp((\\mathbf{h}_{v_{i}}\\cdot\\mathbf{h}_{v_{i}^{-}})/\\eta)}\\) \\\\ \\hline MICoL [60] & \\(v_{i}^{+}\\in N_{M}(v_{i})\\) & in-batch random & \\(-\\log\\frac{\\exp((\\mathbf{h}_{v_{i}}\\cdot\\mathbf{h}_{v_{i}^{-}})/\\eta)}{\\sum_{v_{i} ^{-}}\\exp((\\mathbf{h}_{v_{i}}\\cdot\\mathbf{h}_{v_{i}^{-}})/\\eta)}\\) \\\\ \\hline E2EG [61] & \\((v_{i},v_{i}^{+})\\in\\mathcal{E}\\) & \\((v_{i},v_{i}^{-})\\notin\\mathcal{E}\\) & \\(\\log(\\mathbf{h}_{v_{i}}\\cdot\\mathbf{h}_{v_{i}^{+}})+\\sum_{v_{i}^{-}}\\log(1- \\mathbf{h}_{v_{i}}\\cdot\\mathbf{h}_{v_{i}^{-}})\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE IV: A summarization of Graph-Aware LLM finentuning objectives on text-rich graphs. \\(v_{i}^{+}\\) and \\(v_{i}^{-}\\) denote a positive training node and a negative training node to \\(v_{i}\\) respectively.\n' +
      '\n' +
      'few one-hop neighbors regarding memory complexity) and local minimal [36] (LLM underfits the data) issues.\n' +
      '\n' +
      '**Two-step training** means first adapting LLMs to the graph, and then finetuning the whole LLM-GNN cascaded pipeline. This strategy can effectively alleviate the insufficient training of the LLM which contributes to higher text representation quality. GIANT [59] proposes to conduct neighborhood prediction with the use of XR-Transformers [81] and results in an LLM that can output better feature vectors than bag-of-words and vanilla BERT [22] embedding for node classification. LM-GNN [69] introduces graph-aware pre-fine-tuning to warm up the LLM on the given graph before finetuning the whole LLM-GNN pipeline and demonstrating significant performance gain. SimToG [36] finds that the simple framework of first training the LLMs on the downstream task and then fixing the LLMs and training the GNNs can result in outstanding performance. They further find that using the efficient fine-tuning method, _e.g._, LoRA [41] to tune the LLM can alleviate overfitting issues. GaLM [82] explores ways to pretrain the LLM-GNN cascaded architecture.\n' +
      '\n' +
      '#### 5.2.2 Data Augmentation\n' +
      '\n' +
      'With its demonstrated zero-shot capability [44], LLMs can be used for data augmentation to generate additional text data for the LLM-GNN cascaded architecture. The philosophy of using LLM to generate pseudo data is widely explored in NLP [90]. LLM-GNN [64] proposes to conduct zero-shot node classification on text-attributed networks by labeling a few nodes and using the pseudo labels to fine-tune GNNs. TAPE [71] presents a method that uses LLM to generate prediction text and explanation text, which serve as augmented text data compared with the original text data. A following medium-scale language model is adopted to encode the texts and output features for augmented texts and original text respectively before feeding into GNNs. ENG [72] brings forward the idea of generating labeled nodes for each category, adding edges between labeled nodes and other nodes, and conducting semi-supervised GNN learning for node classification.\n' +
      '\n' +
      '#### 5.2.3 Knowledge Distillation\n' +
      '\n' +
      'LLM-GNN cascaded pipeline is capable of capturing both text information and structure information. However, the pipeline suffers from time complexity issues during inference, since GNNs need to conduct neighbor sampling and LLMs need to encode the text associated with both the center node and its neighbors. A straightforward solution is to serve the LLM-GNN cascade pipeline as the teacher model and distill it into an LLM as the student model. In this case, during inference, the model (which is a pure LLM) only needs to encode the text on the center node and avoid time-consuming neighbor sampling. AdsGNN [80] proposes an L2-loss to force the outputs of the student model to preserve topology after the teacher model is trained. GraD [70] introduces three strategies including the distillation objective and task objective to optimize the teacher model and distill its capability to the student model.\n' +
      '\n' +
      '#### 5.2.4 Discussion\n' +
      '\n' +
      'Given that GNNs are demonstrated as powerful models in encoding graphs, "LLMs as encoders" seems to be the most straightforward way to utilize LLMs on graphs. Although we have discussed much research on "LLMs as encoders", there are still open questions to be solved.\n' +
      '\n' +
      '**Limited Task: Go Beyond Representation Learning.** Current "LLMs as encoders" methods or LLM-GNN cascaded architectures are mainly focusing on representation learning, given the single embedding propagation-aggregation mechanism of GNNs, which prevents it from being adopted to generation tasks (_e.g._, node/text generation). A potential solution to this challenge can be to conduct GNN encoding for LLM outputted token-level representations and to design proper decoders that can perform generation based on the LLM-GNN cascaded model outputs.\n' +
      '\n' +
      '**Low Efficiency: Advanced Knowledge Distillation.** The LLM-GNN cascaded pipeline suffers from time complexity issues since the model needs to conduct neighbor sampling and then embedding encoding for each neighboring node. Although there are methods that explore distilling the learned LLM-GNN model into an LLM model for fast inference, they are far from enough given that the inference of LLM itself is time-consuming. A potential solution is to distill the model into a much smaller LM or even an MLP. Similar methods [88] have been proven effective in GNN to MLP distillation and are worth exploring for the LLM-GNN cascaded pipeline as well.\n' +
      '\n' +
      '### _LLM as Aligner_\n' +
      '\n' +
      'These methods contain an LLM component for text encoding and a GNN component for structure encoding. The two components are served equally and trained iteratively or\n' +
      '\n' +
      'Fig. 4: The illustration of various techniques related to LLM as Encoder, including (a) One-step Training, (b) Two-step Training, (c) Data Augmentation, and (d) Knowledge Distillation.\n' +
      '\n' +
      'parallelly. LLMs and GNNs can mutually enhance each other since the LLMs can provide textual signals to GNNs, while the GNNs can deliver structure information to LLMs. According to how the LLM and the GNN interact, these methods can be further categorized into: LLM-GNN Prediction Alignment and LLM-GNN Latent Space Alignment. The illustration of the two categories of method can be found in Fig. 5.\n' +
      '\n' +
      '#### 5.3.1 LLM-GNN Prediction Alignment\n' +
      '\n' +
      'This refers to training the LLM with the text data on a graph and training the GNN with the structure data on a graph iteratively. LLM will generate labels for nodes from the text perspective and serve them as pseudo-labels for GNN training, while GNN will generate labels for nodes from the structure perspective and serve them as pseudo-labels for LLM training.\n' +
      '\n' +
      'By this design, the two modality encoders can learn from each other and contribute to a final joint text and graph encoding. In this direction, LTRN [58] proposes a novel GNN architecture with personalized PageRank [95] and attention mechanism for structure encoding while adopting BERT [22] as the language model. The pseudo labels generated by LLM and GNN are merged for the next iteration of training. GLEM [62] formulates the iterative training process into a pseudo-likelihood variational framework, where the E-step is to optimize LLM and the M-step is to train the GNN.\n' +
      '\n' +
      '#### 5.3.2 LLM-GNN Latent Space Alignment\n' +
      '\n' +
      'This denotes connecting text encoding (LLM) and structure encoding (GNN) with cross-modality contrastive learning [96]:\n' +
      '\n' +
      '\\[\\mathbf{h}_{d_{v_{i}}}=\\text{LLM}(d_{v_{i}}),\\mathbf{h}_{v_{i}}= \\text{GNN}(\\mathcal{G}_{v}), \\tag{19}\\] \\[l(\\mathbf{h}_{d_{v_{i}}},\\mathbf{h}_{v_{i}})=\\frac{\\text{Sim}( \\mathbf{h}_{d_{v_{i}}},\\mathbf{h}_{v_{i}})}{\\sum_{j\\neq i}\\text{Sim}(\\mathbf{ h}_{d_{v_{i}}},\\mathbf{h}_{v_{j}})}\\] (20) \\[\\mathcal{L}=\\sum_{v_{i}\\in\\mathcal{G}}\\frac{1}{2|\\mathcal{G}|}(l( \\mathbf{h}_{d_{v_{i}}},\\mathbf{h}_{v_{i}})+l(\\mathbf{h}_{v_{i}},\\mathbf{h}_{d _{v_{i}}})) \\tag{21}\\]\n' +
      '\n' +
      'A similar philosophy is widely used in vision-language joint modality learning [97]. Along this line of works, ConGrat [54] adopts GAT [87] as the graph encoder and tries MPNet [35] and DistillGPT [20] as the language model encoder. They extend the original InfoNCE loss by adding graph-specific elements about the most likely second, third, and further choices for the nodes a text comes from and the texts a node produces. In addition to the node-level multi-modality contrastive objective, GRENADE [56] proposes KL-divergence-based neighbor-level knowledge alignment: minimize the neighborhood similarity distribution calculated between LLM and GNN. G2P2 [63] further extends node-text contrastive learning by adding text-summary interaction and node-summary interaction. Then, they introduce using label texts in the text modality for zero-shot classification, and using soft prompts for few-show classification. THLM [34] proposes to pretrain the language model by contrastive learning with a heterogeneous GNN on heterogeneous text-attributed networks. The pretrained LLM can be fine-tuned on downstream tasks.\n' +
      '\n' +
      '#### 5.3.3 Discussion.\n' +
      '\n' +
      'In "LLMs as Aligners" methods, most research is adopting _shallow_ GNNs (_e.g._, GCN, GAT, with thousands of parameters) to be the graph encoders that are aligned with LLMs through iterative training (_i.e._, prediction alignment) or contrastive training (_i.e._, latent space alignment). Although LLMs (with millions or billions of parameters) have strong expressive capability, the shallow GNNs (with limited representative capability) can constrain the mutual learning effectiveness between LLMs and GNNs. A potential solution is to adopt GNNs which can be scaled up [89]. Furthermore, deeper research to explore what is the best model size combination for LLMs and GNNs in such "LLMs as Aligners" LLM-GNN mutual enhancement framework is very important.\n' +
      '\n' +
      '## 6 Text-Paired Graphs\n' +
      '\n' +
      'Graphs are prevalent data objects in scientific disciplines such as cheminformatics [194], material informatics [190], bioinformatics [149], computer vision [150], and quantum computing [151]. Within these diverse fields, graphs frequently come paired with critical graph-level text information. For instance, molecular graphs in cheminformatics are annotated with text properties such as toxicity, water solubility, and permeability properties [194, 190, 214]. Research on such graphs (scientific discovery) could be accelerated by the text information and the adoption of LLMs. In this section, we review the application of LLMs on graph-captioned graphs with a focus on molecular graphs. According to the technique categorization in Section 3.2, we begin by investigating methods that utilize LLMs as Predictor. Then, we discuss methods that align GNNs with LLMs. We summarize all surveyed methods in Table VI.\n' +
      '\n' +
      '### _LLM as Predictor_\n' +
      '\n' +
      'In this subsection, we review how to conduct "LLM as Predictor" for graph-level tasks. Existing methods can be categorized into Graph as Sequence (treat graph data as sequence input) and Graph-Empowered LLMs (design model architecture to encode graphs).\n' +
      '\n' +
      '#### 6.1.1 Graph as Sequence\n' +
      '\n' +
      'For text-paired graphs, we have three steps to utilize existing LLM for graph inputs. **Step 1**: Linearize graphs into sequence with rule-based methods. **Step 2**: Tokenize the linearized sequence. **Step 3**: Train/Finetune different LLMs (_e.g._, Encoder-only, Encoder-Decoder, Decoder-only) for specific tasks. We will discuss each step as follows.\n' +
      '\n' +
      'Fig. 5: The illustration of LLM as Aligner methods, including (a) LLM-GNN Prediction Alignment and (b) LLM-GNN Latent Space Alignment.\n' +
      '\n' +
      '**Step 1: Rule-based Graph Linearization.** Rule-based linearization converts molecular graphs into text sequences that can be processed by LLMs. To achieve this, researchers develop specifications based on human expertise in the form of line notations [152]. For example, the Simplified Molecular-Input Line-Entry System (SMILES) [152] records the symbols of nodes encountered during a depth-first traversal of a molecular graph. The International Chemical Identifier (InChI) [153], created by the International Union of Pure and Applied Chemistry (IUPAC), encodes molecular structures into unique string texts with more hierarchical information. Canonicalization algorithms produce unique SMILES for each molecule, often referred to as canonical SMILES. However, there are more than one SMILES corresponding to a single molecule and SMILES sometimes represent invalid molecules; LLMs learned from these linearized sequences can easily generate invalid molecules (_e.g.,_ incorrect ring closure symbols and unmatched parentheses) due to syntactical errors. To this end, DeepSMILES [154] is proposed. It can alleviate this issue in most cases but does not guarantee 100% robustness. The linearized string could still violate basic physical constraints. To fully address this problem, SELFIES [155] is introduced which consistently yields valid molecular graphs.\n' +
      '\n' +
      '**Step 2: Tokenization.** The tokenization approaches for these linearized sequences are typically language-independent. They operate at both the character level [176, 187] and the substring level [171, 178, 182, 183, 184, 185], based on Sentence\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l} \\hline \\hline Approach & Time & Category & Role of LM & LM Size & Focus & Task \\\\ \\hline GNN-LM [67] & 2021.10 & LLM as Encoder & TE & 237M & Task & LM \\\\ \\hline GLANT [59] & 2021.11 & LLM as Encoder & TE & 110M & Task & NC \\\\ \\hline TextGNN [79] & 2022.1 & LLM as Encoder & TE & 110M & Task & Search \\\\ \\hline AdSRNN [80] & 2022.4 & LLM as Encoder & TE & 110M & Task & Search \\\\ \\hline LM-GNN [69] & 2022.6 & LLM as Encoder & TE & 110M & Efficiency & NC, LP, EC \\\\ \\hline GraD [70] & 2023.4 & LLM as Encoder & TE & 110M/66M & Efficiency & LP, NC \\\\ \\hline TAPE [71] & 2023.5 & LLM as Encoder & TE, AUG & 129M/GPT-3.5 & Task & NC \\\\ \\hline SimToG [36] & 2023.8 & LLM as Encoder & TE & 80M/355M & Task & NC, LP \\\\ \\hline LLM-GNN [64] & 2023.10 & LLM as Encoder & ANN & GPT-3.5 & Task & NC \\\\ \\hline ENG [72] & 2023.10 & LLM as Encoder & TE, AUG & 80M/GPT-3.5 & Task & NC \\\\ \\hline SPECTER [52] & 2020.4 & LLM as Predictor & TE & 110M & Representation & NC, UAP, LP, Rec \\\\ \\hline GraphFormers [73] & 2021.5 & LLM as Predictor & TE, SE & 110M & Representation & LP \\\\ \\hline GreaseLM [68] & 2022.1 & LLM as Predictor & TE, SE & 355M & Task & QA \\\\ \\hline SciNCL [53] & 2022.2 & LLM as Predictor & TE & 110M & Representation & NC, UAP, LP, Rec \\\\ \\hline MICoL [60] & 2022.2 & LLM as Predictor & TE & 110M & Supervision & NC \\\\ \\hline LinkBERT [31] & 2022.3 & LLM as Predictor & TE & 110M & Pretraining & QA, NLU \\\\ \\hline Heterformer [74] & 2022.5 & LLM as Predictor & TE, SE & 110M & Representation & NC, LP \\\\ \\hline E2EG [61] & 2022.8 & LLM as Predictor & TE & 66M & Task & NC \\\\ \\hline TwHN-BERT [57] & 2022.9 & LLM as Predictor & TE & 110M/355M & Pretraining & NC, LP \\\\ \\hline Edgeformers [75] & 2023.1 & LLM as Predictor & TE, SE & 110M & Representation & NC, LP, EC \\\\ \\hline Patton [32] & 2023.5 & LLM as Predictor & TE, RE & 110M & Pretraining & NC, LP, Search \\\\ \\hline InstructGLM [47] & 2023.8 & LLM as Predictor & TE, SE & 250M/7B & Generalization & NC, LP \\\\ \\hline GNP [42] & 2023.9 & LLM as Predictor & TE, SE & 3B/11B & Task & QA \\\\ \\hline Touchp-G [55] & 2023.9 & LLM as Predictor & TE & 110M & Representation & NC, LP \\\\ \\hline DGTL [77] & 2023.10 & LLM as Predictor & TE, SE & 13B & Task & NC \\\\ \\hline GraphText [66] & 2023.10 & LLM as Predictor & TE, SE & GPT-3.5/4 & Task & NC \\\\ \\hline GraphGPT [46] & 2023.10 & LLM as Predictor & TE, SE & 7B & Generalization & NC \\\\ \\hline METERN [76] & 2023.10 & LLM as Predictor & TE, RE & 110M & Representation & NC, LP, Rec, RG \\\\ \\hline LTRN [58] & 2021.2 & LLM as Aligner & TE & 110M & Supervision & NC \\\\ \\hline GLEM [62] & 2023.1 & LLM as Aligner & TE & 110M & Task & NC \\\\ \\hline G2P2 [63] & 2023.5 & LLM as Aligner & TE & 110M & Supervision & NC \\\\ \\hline ConGat [54] & 2023.5 & LLM as Aligner & TE & 110M/82M & Representation & LP, LM, NC \\\\ \\hline GRENADE [56] & 2023.10 & LLM as Aligner & TE & 110M & Representation & NC, LP \\\\ \\hline THLM [34] & 2023.10 & LLM as Aligner & TE & 110B & Pretraining & NC, LP \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE V: Summary of large language models on text-rich graphs. Role of LM: “TE”, “SE”, “ANN” and “AUG” denote text encoder, structure encoder, annotator (labeling the node/edges), and augmentator (conduct data augmentation). Task: “NC”, “UAP”, “LP”, “Rec”, “QA”, “NLU”, “EC”, “LM”, “RG” denote node classification, user activity prediction, link prediction, recommendation, question answering, natural language understanding, edge classification, language modeling, and regression task.\n' +
      '\n' +
      'Piece [163] or BPE [162]. Additionally, RT [173] proposes a tokenization approach that facilitates handling regression tasks within LM Transformers.\n' +
      '\n' +
      '**Step 3: Encoding the Linearized Graph with LLMs**. _Encoder-only LLMs_. Earlier LLMs like SciBERT [24] and BioBERT [189] are trained on scientific literature to understand natural language descriptions related to molecules but are not capable of comprehending molecular graph structures. To this end, SMILES-BERT [188] and MFBERT [185] are proposed for molecular graph classification with linearized SMILES strings. Since scientific natural language descriptions contain human expertise which can serve as a supplement for molecular graph structures, recent advances emphasize the joint understanding of them [184, 168]: The linearized graph sequence is concatenated on the raw natural language data and then input into the LLMs. Specifically, KV-PLM [184] is built based on BERT [22] to understand the molecular structure in a biomedical context. CatBERTa [168], as developed from RoBERTa [23], specializes in the prediction of catalyst properties for molecular graphs.\n' +
      '\n' +
      '_Encoder-Decoder LLMs_. Encoder-only LLMs may lack the capability for generation tasks. In this section, we discuss LLMs with encoder-decoder architectures. For example, Chemformer [164] uses a similar architecture as BART [29]. The representation from the encoder can be used for property prediction tasks, and the whole encoder-decoder architecture\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l} \\hline \\hline \\multicolumn{1}{c}{Model} & \\multicolumn{1}{c}{Time} & \\multicolumn{1}{c}{LM Encoder} & \\multicolumn{1}{c}{Graph Encoder} & \\multicolumn{1}{c}{Gen. Decoder} & \\multicolumn{1}{c}{LM Size} & \\multicolumn{1}{c}{Task} \\\\ \\hline SMILES-BERT [188] & 2019.09 & Transformer [94] & Linearized & N.A. & 30M-563M & Classification \\\\ \\hline Text2Mol [122] & 2021.11 & SciBERT [24] & GCN & Transformer [94] & \\(\\geq\\)110M & Retrieval \\\\ \\hline MoICPT [186] & 2021.10 & N.A. & Linearized & GPT & 6M & Generation \\\\ \\hline Chemformer [164] & 2022.01 & BART [29] & Linearized & BART [29] & 45M-230M & Regression, Gen. \\\\ \\hline \\multirow{7}{*}{Constration} & KV-PLM [184] & 2022.02 & BERT [22] & Linearized & N.A & 110M-340M & Classifari, NER, Retrieval \\\\ \\cline{2-8}  & MFBERT [185] & 2022.06 & RoBERTa [23] & Linearized & N.A & 110M-340M & Classification \\\\ \\cline{2-8}  & MFBERT [185] & 2022.06 & RoBERTa [23] & Linearized & N.A & 110M-340M & Classification \\\\ \\cline{2-8}  & Galatica [187] & 2022.11 & N.A. & Linearized & Transformer [94] & 125M-120B & Classification \\\\ \\cline{2-8}  & MoITS [123] & 2022.12 & T5.1.1 [30] & Linearized & Transformer & 80M-780M & Gen., Cap. \\\\ \\cline{2-8}  & Text-Chem T5 [180] & 2023.05 & T5 [30] & Linearized & T5 [30] & 80M-780M & Classifari, Gen., Caption \\\\ \\cline{2-8}  & LLM-ICL [177] & 2023.05 & N.A. & Linearized & \\begin{tabular}{l} GPT-3.5/4 \\\\ LLaMA2 [19] \\\\ Galactica [187] \\\\ \\end{tabular} & \\(\\geq\\) 780M & \\begin{tabular}{l} Classification \\\\ Generation \\\\ Caption \\\\ \\end{tabular} \\\\ \\cline{2-8}  & GMLET [48] & 2023.05 & T5 [30] & GT & T5 [30] & 80M-780M & Classifari, Reg. \\\\ \\hline \\multirow{7}{*}{Constration} & MoXPT [187] & 2023.05 & N.A. & Linearized & GPT-2 & 350M & Classifari, Gen., Cap \\\\ \\cline{2-8}  & ChatMol [175] & 2023.06 & T5 [30] & Linearized & T5 [30] & 80M-780M & Gen., Cap. \\\\ \\cline{2-8}  & MoIEcPT [174] & 2023.06 & N.A. & Linearized & GPT-3.5 & N.A. & Gen., Cap. \\\\ \\cline{2-8}  & RT [173] & 2023.06 & N.A. & Linearized & XLNet [26] & 27M & Reg. Gen. \\\\ \\cline{2-8}  & LLM4Mol [172] & 2023.07 & RoBERTa [23] & Linearized & GPT-3.5 & N.A. & Classifari, Reg. \\\\ \\cline{2-8}  & LLMa-Mol [169] & 2023.07 & N.A. & Linearized & LLaMA [119] & 7B & Reg., Gene. \\\\ \\cline{2-8}  & LLMa-Mol [169] & 2023.07 & N.A. & Linearized & LLaMA [119] & 7B & Reg., Gene. \\\\ \\cline{2-8}  & LLMa-Mol [169] & 2023.07 & N.A. & Linearized & LLaMA [119] & 7B & Reg., Gene. \\\\ \\cline{2-8}  & ProJ2Text [170] & 2023.07 & N.A. & GNN & GPT-2 & 256M-760M & Caption \\\\ \\cline{2-8}  & ProJ2Text [170] & 2023.07 & N.A. & GNN & GPT-2 & 256M-760M & Caption \\\\ \\cline{2-8}  & CatBERTa [168] & 2023.10 & N.A. & Linearized & RoBERTa [23] & N.A. & Regression \\\\ \\cline{2-8}  & ReLU [166] & 2023.10 & N.A. & GNN & GPT-3.5 & N.A. & Classification \\\\ \\hline \\multirow{7}{*}{Constration} & MoMoMo [183] & 2022.12 & \\begin{tabular}{l} SelfBERT [24] \\\\ KV-PLM [184] \\\\ \\end{tabular} & \\begin{tabular}{l} MoT5 [123] \\\\ MGFlow [220] \\\\ \\end{tabular} & \\begin{tabular}{l} MoT5 [123] \\\\ \\end{tabular} & \\begin{tabular}{l} 82M-782M \\\\ Caption, Ret. \\\\ \\end{tabular} \\\\ \\cline{2-8}  & MoleculeLSTM [181] & 2022.12 & BART [29] & GIN & Linearized & BART [29] & 45M-230M & \\begin{tabular}{l} Classifari, Gen., \\\\ Caption \\\\ \\end{tabular} \\\\ \\cline{2-8}  & CLAMP [179] & 2023.05 & \\begin{tabular}{l} BioBERT [189], GNN \\\\ CLIP [97], T5 [30] \\\\ \\end{tabular} & \\begin{tabular}{l} GNN \\\\ Lin., Vec. \\\\ \\end{tabular} & N.A. & \\(\\leq\\)11B & \\begin{tabular}{l} Classification \\\\ Retrieval \\\\ \\end{tabular} \\\\ \\cline{2-8}  & MoFM [171] & 2023.06 & BERT [22] & GIN & MoITS [123] & 61.8M & \\begin{tabular}{l} Classifari, Gen. \\\\ Caption, Ret. \\\\ \\end{tabular} \\\\ \\cline{2-8}  & MoMoMo+v2 [182] & 2023.07 & SciBERT [24] & GIN & N.A & 82M-782M & Classification \\\\ \\cline{2-8}  & GIT-Mol [167] & 2023.08 & SciBERT [24] & \\begin{tabular}{l} GIN \\\\ Linearized \\\\ \\end{tabular} & MoITS [123] & 190M-890M & \\begin{tabular}{l} Classifari, Gen. \\\\ Caption \\\\ \\end{tabular} \\\\ \\cline{2-8}  & MoICA [176] & 2023.10 & Galactica [187] & GIN & N.A & 100M-877M & \n' +
      '\\begin{tabular}{l} Classifari, Reg., \\\\ Retrieval \\\\ \\end{tabular} \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VI: Model collection in Section 6 for text-captioned graphs. “In.” and “Vec.” represent Linearized Graph Encoding and Vectorized Graph Encoding. “Classif.”, “Reg.”, “NER”, “RE, “Retr.”, “Gen.”, “Cap.” represent classification, regression, named entity recognition, relation extraction, (molecule) graph retrieval, (molecule) graph generation, (molecule) graph captioning.\n' +
      '\n' +
      'can be optimized for molecule generation. Other works focus on molecule captioning (which involves generating textual descriptions from a molecule) and text-based molecular generation (where a molecular graph structure is generated from a natural description). Specifically, MoI75 [123] is developed based on the T5 [30], suitable for these two tasks. It formulates molecule-text translation as a multilingual problem and initializes the model using the T5 checkpoint. The model was pre-trained on two monolingual corpora: the Colossal Clean Crawled Corpus (C4) [30] for the natural language modality and one million SMILES [164] for the molecule modality. Text+Chem T5 [180] extends the input and output domains to include both SMILES and texts, unlocking LLMs for more generation functions such as text or reaction generation. ChatMol [175] exploits the interactive capabilities of LLMs and proposes designing molecule structures through multi-turn dialogs with T5.\n' +
      '\n' +
      '_Decoder-only LLMs._ Decoder-only architectures have been adopted for recent LLMs due to their advanced generation ability. MoI6PT [186] and MoIXPT [178] are GPT-style models used for molecule classification and generation. Specifically, MoI GPT [186] focuses on conditional molecule generation tasks using scaffolds, while MolXPT [178] formulates the classification task as a question-answering problem with yes or no responses. RT [173] adopts XLNet [26] and focuses on molecular regression tasks. It frames the regression as a conditional sequence modeling problem. Galactica [187] is a set of LLMs with a maximum of 120 billion parameters, which is pretrained on two million compounds from PubChem [194]. Therefore, Galactica could understand molecular graph structures through SMILES. With instruction tuning data and domain knowledge, researchers also adapt general-domain LLMs such as LLaMA to recognize molecular graph structures and solve molecule tasks [169]. Recent studies also explore the in-context learning capabilities of LLMs on graphs. LLM-ICL [177] assesses the performance of LLMs across eight tasks in the molecular domain, ranging from property classification to molecule-text translation. MolReGPT [174] proposes a method to retrieve molecules with similar structures and descriptions to improve context learning. LLM4Mol [172] utilizes the summarization capability of LLMs as a feature extractor and combines it with a smaller, tunable LLM for specific prediction tasks.\n' +
      '\n' +
      '#### 6.1.2 Graph-Empowered LLMs\n' +
      '\n' +
      'Different from the methods that adopt the original LLM architecture (_i.e.,_ Transformers) and input the graphs as sequences to LLMs, graph-empowered LLMs attempt to design LLM architectures that can conduct joint encoding of text and graph structures. Some works modify the positional encoding of Transformers. For instance, GIMLET [48] treats nodes in a graph as tokens. It uses a single Transformer to manage both the graph structure and text sequence \\([v_{1},v_{2},\\dots,v_{|\\mathcal{V}|},s_{|\\mathcal{V}|+1},\\dots,s_{|\\mathcal{V} |+|d_{\\mathcal{G}}|}]\\), where \\(v\\in\\mathcal{V}\\) is a node and \\(s\\in d_{\\mathcal{G}}\\) is a token in the text associated with \\(\\mathcal{G}\\). It had three sub-encoding approaches for positional encodings to cater to different data modalities and their interactions. Specifically, it adopted the structural position encoding (PE) from the Graph Transformer and defines the relative distance between tokens \\(i\\) and \\(j\\) as follows:\n' +
      '\n' +
      '\\[\\mathrm{PE}(i,j)=\\begin{cases}i-j&\\text{if }i,j\\in d_{\\mathcal{G}},\\\\ \\mathrm{GSD}(i,j)+\\mathrm{Mean}_{e_{k}\\in\\mathrm{SP}(i,j)}\\mathbf{x}_{e_{k}}& \\text{if }i,j\\in\\mathcal{V},\\\\ -\\infty&\\text{if }i\\in\\mathcal{V},j\\in d_{\\mathcal{G}},\\\\ 0&\\text{if }i\\in d_{\\mathcal{G}},j\\in\\mathcal{V}.\\end{cases} \\tag{22}\\]\n' +
      '\n' +
      'Here, \\(\\mathrm{GSD}\\) denotes the graph shortest distance between two nodes, and \\(\\mathrm{Mean}_{k\\in\\mathrm{SP}(i,j)}\\) represents the mean pooling of the edge features \\(\\mathbf{x}_{e_{k}}\\) along the shortest path \\(\\mathrm{SP}(i,j)\\) between nodes \\(i\\) and \\(j\\). GIMLET [48] adapts bi-directional attention for node tokens and enables texts to selectively attend to nodes. These designs render the Transformer\'s submodule, which handles the graph part, equivalent to a Graph Transformer [141].\n' +
      '\n' +
      'There are other works that modify cross-attention modules to facilitate interaction between graph and text representations. Given the graph hidden state \\(\\mathbf{h}_{\\mathcal{G}}\\), its node-level hidden state \\(\\mathbf{H}_{v}\\) and text hidden state \\(\\mathbf{H}_{d_{\\mathcal{G}}}\\), Text2Mol [122] implemented interaction between representations in the hidden layers of encoders, while Prot2Text [170] implemented this interaction within the layers of between encoder and decoder:\n' +
      '\n' +
      '\\[\\mathbf{H}_{d_{\\mathcal{G}}}=\\text{softmax}\\left(\\frac{\\mathbf{W}_{Q}\\mathbf{H }_{d_{\\mathcal{G}}}\\cdot(\\mathbf{W}_{K}\\mathbf{H}_{v})^{T}}{\\sqrt{d_{k}}} \\right)\\cdot\\mathbf{W}_{V}\\mathbf{H}_{v}, \\tag{23}\\]\n' +
      '\n' +
      'Where \\(\\mathbf{W}_{Q},\\mathbf{W}_{K},\\mathbf{W}_{V}\\) are trainable parameters that transform the query modality (e.g., sequences) and the key/value modality (e.g., graphs) into the attention space. Furthermore, Prot2Text [170] utilizes two trainable parameter matrices \\(\\mathbf{W}_{1}\\) and \\(\\mathbf{W}_{2}\\) to integrate the graph representation into the sequence representation.\n' +
      '\n' +
      '\\[\\mathbf{H}_{d_{\\mathcal{G}}}=\\left(\\mathbf{H}_{d_{\\mathcal{G}}}+\\mathbf{1}_{|d _{\\mathcal{G}}|}\\mathbf{h}_{\\mathcal{G}}\\mathbf{W}_{1}\\right)\\mathbf{W}_{2}. \\tag{24}\\]\n' +
      '\n' +
      '#### 6.1.3 Discussion\n' +
      '\n' +
      '**LLM Inputs with Sequence Prior.**_The first challenge is that the progress in advanced linearization methods has not progressed in tandem with the development of LLMs._ Emerging around 2020, linearization methods for molecular graphs like SELFIES offer significant grammatical advantages, yet advanced LMs and LLMs from graph machine learning and language model communities might not fully utilize these, as these encoded results are not part of pretraining corpora prior to their proposal. Consequently, recent studies [177] indicate that LLMs, such as GPT-3.5/4, may be less adept at using SELFIES compared to SMILES. Therefore, the performance of LM-only and LLM-only methods may be limited by the expressiveness of older linearization methods, as there is no way to optimize these hard-coded rules during the learning pipeline of LLMs. However, the second challenge remains as the inductive bias of graphs may be broken by linearization._ Rule-based linearization methods introduce inductive biases for sequence modeling, thereby breaking the permutation invariance assumption inherent in molecular graphs. It may reduce task difficulty by introducing sequence order to reduce the search space. However, it does not mean model generalization. Specifically, there could be multiple string-based representations for a single graph from single or different approaches. Numerous studies [156, 157, 158, 159] have shown that training on different string-based views of the same molecule can improve the sequential model\'s performance, as these data augmentation approaches manage to retain the permutation-invariance nature of graphs. These advantages are also achievable with a permutation-invariant GNN, potentially simplifying the model by reducing the need for complex, string-based data augmentation design.\n' +
      '\n' +
      '**LLM Inputs with Graph Prior.** Rule-based linearization may be considered less expressive and generalizable compared to the direct graph representation with rich node features, edge features, and the adjacency matrix [203]. Various atomic features include atomic number, chirality, degree, formal charge, number of hydrogen atoms, number of radical electrons, hybridization state, aromaticity, and presence in a ring. Bond features encompass the bond\'s type (e.g., single, double, or triple), the bond\'s stereochemistry (e.g., E/Z or cis/trans), and whether the bond is conjugated [204]. Each feature provides specific information about atomic properties and structure, crucial for molecular modeling and cheminformatics. One may directly **vectorize** the molecular graph structure into binary vectors [199] and then apply parameterized Multilayer Perceptrons (MLPs) on the top of these vectors to get the graph representation. These vectorization approaches are also known as fingerprints such as MACCS [200], ECFP [201], and CDK fingerprints [202] and are based on human-defined rules. These rules take inputs of a molecule and output a vector consisting of 0/1 bits. Each bit denotes a specific type of substructure related to functional groups that could be used for various property predictions. Fingerprints consider atoms and structures, but they still fall short of automatically learning from the raw graph structure. GNNs could serve as automatic feature extractors to replace or enhance fingerprints. Some specific methods are explored in Section 6.1.2, while the other graph prior such as the eigenvectors of a graph Laplacian and the random walk prior could also be used [142].\n' +
      '\n' +
      '**LLM Outputs for Prediction.** LMs like KV-PLM [184], SMILES-BERT [188], MFBERT [185], and Chemformer [164] use a prediction head on the output vector of the last layer. These models are finetuned with standard classification and regression losses but may not fully utilize all the parameters and advantages of the complete architecture. In contrast, models like RT [173], MolXPT [178], and Text+Chem T5 [180] frame prediction as a text generation task. These models are trained with either masked language modeling or autoregressive targets, which requires a meticulous design of the context words in the text [173]. Specifically, domain knowledge instructions may be necessary to activate the in-context learning ability of LLMs, thereby making them domain experts [177]. For example, a possible template could be divided into four parts: {General Description}{Task-Specific Description}{Question-Answer Examples}{Test Question}.\n' +
      '\n' +
      '**LLM Outputs for Reasoning.** Since string representations of molecular graphs usually carry new and in-depth domain knowledge, which is beyond the knowledge of LLMs, recent work [174, 148, 147] also attempts to utilize the reasoning ability of LLMs, instead of using them as a knowledge source for predicting the property of molecular graphs. ReLM [166] utilizes GNNs to suggest top-k candidates, which were then used to construct multiple-choice answers for in-context learning. ChemCrow [148] designs the LLMs as the chemical agent to implement various chemical tools. It avoided direct inference in an expertise-intensive domain.\n' +
      '\n' +
      '### _LLM as Aligner_\n' +
      '\n' +
      '#### 6.2.1 Latent Space Alignment\n' +
      '\n' +
      'One may directly align the latent spaces of the GNN and LLM through contrastive learning and predictive regularization. Typically, a graph representation from a GNN can be read out by summarizing all node-level representations, and a sequence representation can be obtained from the [CLS] token. We first use two projection heads, which are usually MLPs, to map the separate representation vectors from the GNN and LLM into a unified space as \\(\\mathbf{h}_{\\mathcal{G}}\\) and \\(\\mathbf{h}_{d_{\\mathcal{G}}}\\), and then align them within this space. Specifically, MoMu [183] and MoMu-v2 [182] retrieve two sentences from the corpus for each molecular graph. During training, graph data augmentation was applied to molecular graphs, creating two augmented views. Consequently, there are four pairs of \\(\\mathcal{G}\\) and \\(d_{\\mathcal{G}}\\). For each pair, the contrastive loss for space alignment is as follows.\n' +
      '\n' +
      '\\[\\ell_{\\text{MoMu}}=-\\log\\frac{\\exp\\left(\\cos\\left(\\mathbf{h}_{\\mathcal{G}}, \\mathbf{h}_{d_{\\mathcal{G}}}\\right)/\\tau\\right)}{\\sum_{\\tilde{d}_{\\mathcal{G}} \\neq d_{\\mathcal{G}}}\\exp\\left(\\cos\\left(\\mathbf{h}_{\\tilde{\\mathcal{G}}}, \\mathbf{h}_{\\tilde{d}_{\\mathcal{G}}}\\right)/\\tau\\right)}, \\tag{25}\\]\n' +
      '\n' +
      'where \\(\\tau\\) is the temperature hyper-parameter and \\(\\tilde{d_{\\mathcal{G}}}\\) denotes the sequence not paired to the graph \\(\\mathcal{G}\\). MoleculeSTM [181] also applies contrastive learning to minimize the representation distance between a molecular graph \\(\\mathcal{G}\\) and its corresponding texts \\(d_{\\mathcal{G}}\\), while maximizing the distance between the molecule and unrelated descriptions. Specifically, it considers two contrastive learning strategies: EBM-NCE and InfoNCE:\n' +
      '\n' +
      '\\[\\begin{split}\\ell_{\\text{STM-EBM}}&=-\\frac{1}{2} \\left(\\mathbb{E}_{\\mathbf{h}_{\\mathcal{G}},\\mathbf{h}_{d_{\\mathcal{G}}}}\\left[ \\log\\sigma\\left(\\mathbf{h}_{\\mathcal{G}}^{\\top}\\mathbf{h}_{d_{\\mathcal{G}}} \\right)\\right]\\right.\\\\ &\\left.+\\mathbb{E}_{\\mathbf{h}_{\\mathcal{G}},\\mathbf{h}_{d_{ \\mathcal{G}}}}\\left[\\log\\left(1-\\sigma\\left(\\mathbf{h}_{\\mathcal{G}}^{\\top} \\mathbf{h}_{d_{\\mathcal{G}}}\\right)\\right)\\right]\\right)\\\\ &-\\frac{1}{2}\\left(\\mathbb{E}_{\\mathbf{h}_{\\mathcal{G}},\\mathbf{h} _{d_{\\mathcal{G}}}}\\left[\\log\\sigma\\left(\\mathbf{h}_{\\mathcal{G}}^{\\top} \\mathbf{h}_{d_{\\mathcal{G}}}\\right)\\right]\\right.\\\\ &\\left.+\\mathbb{E}_{\\mathbf{h}_{\\mathcal{G}},\\mathbf{h}_{d_{ \\mathcal{G}}}}\\left[\\log\\left(1-\\sigma\\left(\\mathbf{h}_{\\mathcal{G}}^{\\top} \\mathbf{h}_{d_{\\mathcal{G}}}\\right)\\right)\\right]\\right)\\end{split} \\tag{26}\\]\n' +
      '\n' +
      '\\[\\begin{split}\\ell_{\\text{STM-Info}}=-\\frac{1}{2}\\mathbb{E}_{ \\mathbf{h}_{\\mathcal{G}},\\mathbf{h}_{d_{\\mathcal{G}}}}&\\left[ \\log\\frac{\\exp\\left(\\mathbf{h}_{\\mathcal{G}}^{\\top}\\mathbf{h}_{d_{\\mathcal{G}}} \\right)}{\\sum_{\\tilde{d}_{\\mathcal{G}}\\neq d_{\\mathcal{G}}}\\exp\\left( \\mathbf{h}_{\\mathcal{G}}^{\\top}\\mathbf{h}_{d_{\\mathcal{G}}}\\right)}\\right.\\\\ &\\left.+\\log\\frac{\\exp\\left(\\mathbf{h}_{\\mathcal{G}}^{\\top} \\mathbf{h}_{d_{\\mathcal{G}}}\\right)}{\\sum_{\\tilde{\\mathcal{G}}\\neq\\tilde{ \\mathcal{G}}}\\exp\\left(\\mathbf{h}_{\\mathcal{G}}^{\\top}\\mathbf{h}_{d_{\\mathcal{G}}} \\right)}\\right]\\end{split} \\tag{27}\\]\n' +
      '\n' +
      'MoleculeSTM [181] randomly samples negative graphs or texts to construct negative pairs of \\((\\mathcal{G},\\tilde{d})\\) and \\((\\mathcal{G},d)\\). Similarly, MolFM [171] and GIT-Mol [167] implement the contrastive loss with mutual information and negative sampling, as shown in Eq. (27). These two methods also use cross-entropy to regularize the unified space with the assumption that randomly permuted graph and text inputs are predictable if they originate from the same molecule. However, the aforementioned methods cannot leverage task labels. Given a classification label \\(y\\), CLAMP [179] learns to map active molecules (\\(y=1\\)) so that they align with the corresponding assay description for each molecular graph \\(\\mathcal{G}\\).\n' +
      '\n' +
      '\\[\\begin{split}\\ell_{\\text{CLAMP}}=& y\\log\\left(\\sigma\\left(\\tau^{-1}\\mathbf{h}_{\\mathcal{G}}^{T} \\mathbf{h}_{d_{\\mathcal{G}}}\\right)\\right)\\\\ &+(1-y)\\log\\left(1-\\sigma\\left(\\tau^{-1}\\mathbf{h}_{\\mathcal{G}}^{T} \\mathbf{h}_{d_{\\mathcal{G}}}\\right)\\right),\\end{split} \\tag{28}\\]CLAMP [179] requires labels to encourage that active molecules and their corresponding text descriptions are clustered together in the latent space. To advance the alignment between two modalities, MolCA [176] trains the Query Transformer (Q-Former) [207] for molecule-text projecting and contrastive alignment. Q-former initializes \\(N_{q}\\) learnable query tokens \\(\\{\\mathbf{q}_{k}\\}_{k=1}^{N_{q}}\\). These query tokens are updated with self-attention and interact with the output of GNNs through cross-attention to obtain the \\(k\\)-th queried molecular representation vector \\((\\mathbf{h}_{\\mathcal{G}})_{k}:=\\text{Q-Former}(\\mathbf{q}_{k})\\). The query tokens share the same self-attention modules with the texts, but use different MLPs, allowing the Q-Former to be used for obtaining the representation of text sequence \\(\\mathbf{h}_{d_{\\mathcal{G}}}:=\\text{Q-Former}([\\text{CLSI}])\\).\n' +
      '\n' +
      '\\[\\ell_{\\text{g2t}}=\\log\\frac{\\exp\\left(\\max_{k}\\cos\\left((\\mathbf{ h}_{\\mathcal{G}})_{k},\\mathbf{h}_{d_{\\mathcal{G}}}\\right)/\\tau\\right)}{\\sum_{ \\tilde{d}_{\\mathcal{G}}\\neq d_{\\mathcal{G}}}\\exp\\left(\\max_{k}\\cos\\left(( \\mathbf{h}_{\\mathcal{G}})_{k},\\mathbf{h}_{\\tilde{d}_{\\mathcal{G}}}\\right)/ \\tau\\right)} \\tag{29}\\] \\[\\ell_{\\text{t2g}}=\\log\\frac{\\exp\\left(\\max_{k}\\cos\\left(\\mathbf{ h}_{d_{\\mathcal{G}}},(\\mathbf{h}_{\\mathcal{G}})_{k}\\right)/\\tau\\right)}{\\sum_{ \\tilde{\\mathcal{G}}\\neq\\mathcal{G}}\\exp\\left(\\max_{k}\\cos\\left(\\mathbf{h}_{d_{ \\mathcal{G}}},(\\mathbf{h}_{\\tilde{\\mathcal{G}}})_{k}\\right)/\\tau\\right)}\\] \\[\\ell_{\\text{MolCA}}=-\\ell_{\\text{g2t}}-\\ell_{\\text{t2g}}\\]\n' +
      '\n' +
      '#### 6.2.2 Discussion\n' +
      '\n' +
      '**Larger-Scale GNNs.** GNNs integrate atomic and graph structural features for molecular representation learning [147]. Specifically, Text2Mol [122] utilizes the GCN [85] as its graph encoder and extracts unique identifiers for node features based on Morgan fingerprints [201]. MoMu [183], MoMu-v2 [182], MolFM [171], GIT-Mol [167], and MolCA [176] prefer GIN [205] as the backbone, as GIN has been proven to be as expressive and powerful as the Weisfeiler-Lehman graph isomorphism test [206]. As described in Section 2.2, there has been notable progress in making GNNs deeper, more generalizable, and more powerful since the proposal of the GCN [85] in 2016 and the GIN [205] in 2018. However, most reviewed works [167, 171, 168, 183, 182] are developed using the GIN [205] as a proof of concept for their approaches. These pretrained GINs feature five layers and 300 hidden dimensions. The scale of GNNs could be a bottleneck in learning semantic meanings in their representation vectors and there is a risk of over-reliance on one modality, neglecting the other. Therefore, for future large-scale GNN designs comparable to LLMs, scaling up the dimension size and adding deeper layers, which have proven effective in recent material discovery tasks [231], may be considered. Recent studies [139, 140, 142] also suggest that designing advanced GNNs with Transformer encoder layers may improve the expressive power of GNNs. Therefore, the deployment of real-world and large-scale GNNs should be promising when combining the aforementioned methods.\n' +
      '\n' +
      '**Generation Decoder with GNNs.** Except for MoMu [183], which utilizes the flow-based MoFlow method [220] as the molecular graph generator, aligned GNNs are often used as encoders, not as decoders for generation. The prevalent decoder architecture is mostly text-based, generating linearized graph structures such as SMILES. These approaches may hardly utilize the characteristic of permutation-invariant graphs, as discussed in Section 6.1.3. Recent advances in generative diffusion models on graphs [219, 232] could be utilized in future work to design generators with GNNs.\n' +
      '\n' +
      '## 7 Applications\n' +
      '\n' +
      '### _Datasets, Splitting and Evaluation_\n' +
      '\n' +
      'We summarize the datasets for the three scenarios (namely pure graphs, text-rich graphs, and text-paired graphs) and show them in Table IV, Table VII, and Table VIII respectively.\n' +
      '\n' +
      '#### 7.1.1 Pure Graphs\n' +
      '\n' +
      'In Table IV, we summarize the pure graph reasoning problems discussed in Section 4. Many problems are shared or revisited in different datasets due to their generality. NLGraph [124], LLMtoGraph [125] and GUC [126] study a set of standard graph reasoning problems, including connectivity, shortest path, and graph diameter. GraphQA [131] benchmarks a similar set of problems but additionally describes the graphs in real-world scenarios to the effect of graph grounding. LM4DyG [128] focuses on reasoning tasks on temporally evolving graphs. Accuracy is the most common evaluation metric as they are primarily formulated as graph question-answering tasks.\n' +
      '\n' +
      '#### 7.1.2 Text-Rich Graphs\n' +
      '\n' +
      'We summarize the famous datasets for evaluating models on text-rich graphs in Table VII. The datasets are mostly from the academic, e-commerce, book, social media, and Wikipedia domains. The popular tasks to evaluate models on those datasets include node classification, link prediction, edge classification, regression, and recommendation. The evaluation metrics for node/edge classification include Accuracy, Macro-F1, and Micro-F1. For link prediction and recommendation evaluation, Mean Reciprocal Rank (MRR), Normalized Discounted Cumulative Gain (NDCG), and Hit Ratio (Hit) usually serve as metrics. While evaluating model performance on regression tasks, people tend to adopt mean absolute errors (MAE) or root mean square error (RMSE).\n' +
      '\n' +
      '#### 7.1.3 Text-Paired Graphs\n' +
      '\n' +
      'Table VIII shows text-paired graph datasets (including text-available and graph-only datasets). For _Data Splitting_, options include random splitting, source-based splitting, activity cliffs [221] and scaffolds [222], and data balancing [143]. Graph classification usually adopts AUC [204] as the metrics, while regression uses MAE, RMSE, and R\\({}^{2}\\)[147]. For text generation evaluation, people tend to use the Bilingual Evaluation Understudy (BLEU) score; while for molecule generation evaluation, heuristic evaluation methods (based on factors including validity, novelty, and uniqueness) are adopted. However, it is worth noted that BLEU score is efficient but less accurate, while heuristic evaluation methods are problematic subject to unintended modes, such as the superfluous addition of carbon atoms in [223].\n' +
      '\n' +
      '### _Open-source Implementations_\n' +
      '\n' +
      '**Hugging Face.** HF Transformers is the most popular Python library for Transformers-based language models. Besides, it also provides two additional packages: Datasets for easily accessing and sharing datasets and Evaluate for easily evaluating machine learning models and datasets.\n' +
      '\n' +
      '**Fairseq.** Fairseq is another open-source Python library for Transformers-based language models.\n' +
      '\n' +
      '**PyTorch Geometric.** PyG is an open-source Python library for graph machine learning. It packages more than 60 types of GNN layers, combined with various aggregation and pooling layers.\n' +
      '\n' +
      '**Deep Graph Library.** DGL is another open-source Python library for graph machine learning.\n' +
      '\n' +
      '**RDKit.** RDKit is one of the most popular open-source cheminformatics software programs that facilitates various operations and visualizations for molecular graphs. It offers many useful APIs, such as the linearization implementation for molecular graphs, to convert them into easily stored SMILES and to convert these SMILES back into graphs.\n' +
      '\n' +
      '### _Practical applications_\n' +
      '\n' +
      '#### 7.3.1 Scientific Discovery\n' +
      '\n' +
      '**Virtual Screening.** While we may have numerous unlabeled molecule candidates for drug and material design, chemists are often interested in only a small portion of them that are located in a specific area of chemical space [226]. Machine learning models could help researchers automatically screen out trivial candidates. However, training accurate models is not an easy task because labeled molecule datasets often have small sizes and imbalanced data distribution [143]. There are many efforts to improve GNNs against data sparsity [143, 147, 229]. However, it is difficult, if not impossible, for a model to generalize and understand in-depth domain knowledge that it has never been trained on. Texts, therefore, could be complementary sources of knowledge. Discovering task-related content from massive scientific papers and using them as instructions has great potential to improve GNNs in accurate virtual screening tasks [48].\n' +
      '\n' +
      '**Optimizing Scientific Hypotheses.** Molecular generation and optimization represent one of the fundamental goals in chemical science for drug and material discovery [227]. Scientific hypotheses, such as the complex molecules [228], can be represented in the joint space of GNNs and LLMs. Then, one may search in the latent space for a better hypothesis that aligns with the text description (human requirements) and adheres to structural constraints like chemical validity. Chemical space has been found to contain more than \\(10^{60}\\) molecules [225], which is beyond the capacity of exploration in wet lab experiments. One of the biggest challenges lies in generating high-quality candidates, rather than randomly producing candidates in irrelevant subspaces. Molecular generation with multiple conditions (textual, numerical, categorical) shows promise to solve this problem.\n' +
      '\n' +
      '**Synthesis Planning.** Synthesis designs start from available molecules and involve planning a sequence of steps that can finally produce a desired chemical compound through a series of reactions [228]. This procedure includes a sequence of reactant molecules and reaction conditions. Both graphs and texts play important roles in this process. For example, graphs may represent the fundamental structure of molecules, while texts may describe the reaction conditions, additives, and solvents. LLMs can also assist in the planning by suggesting possible synthesis paths directly or by serving as agents to operate on existing planning tools [148].\n' +
      '\n' +
      '#### 7.3.2 Computational Social Science\n' +
      '\n' +
      'In computational social science, researchers are interested in modeling the behavior of people/users and discovering new knowledge that can be utilized to forecast the future. The behaviors of users and interactions between users can be modeled as graphs, where the nodes are associated with rich text information (_e.g._, user profile, messages, emails). We will show two example scenarios below.\n' +
      '\n' +
      '**E-commerce.** In E-commerce platforms, there are many interactions (_e.g._, purchase, view) between users and products. For example, users can view, cart, or purchase products. In addition, the users, products, and their interactions are associated with rich text information. For instance, products have titles/descriptions and users can leave a review of products. In this case, we can construct a graph where nodes are users and products, while edges are their interactions. Both nodes and edges are associated with text. It is important to utilize both the text information and the graph structure information (user behavior) to model users and items and solve complex downstream tasks (_e.g._, item recommendation [104], bundle recommendation [105], and product understanding [106]).\n' +
      '\n' +
      '**Social Media.** In social media platforms, there are many users and they interact with each other through messages,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l} \\hline \\hline Text. & Data & Year & Task & \\# Nodes & \\# Edges & Domain & Source \\& Notes \\\\ \\hline \\multirow{8}{*}{\\begin{tabular}{} \\end{tabular} } & ogb-arxiv & 2020.5 & NC & 169.343 & 1,166,243 & Academic & OGB [204] \\\\  & ogb-products & 2020.5 & NC & 2,449,029 & 61,859,140 & E-commerce & OGB [204] \\\\  & ogb-papers110M & 2020.5 & NC & 111,059,956 & 1,615,685,872 & Academic & OGB [204] \\\\  & ogb-citation2 & 2020.5 & LP & 2,927,963 & 30,561,187 & Academic & OGBs [204] \\\\  & Cora & 2000 & NC & 2,708 & 5,429 & Academic & [9] \\\\  & Citeseer & 1998 & NC & 3,312 & 4,732 & Academic & [10] \\\\  & DBLP & 2023.1 & NC, LP & 5,259,858 & 36,630,661 & Academic & www.aminer.org/citation \\\\  & MAG & 2020 & NC, LP, Rec RG & \\(\\sim 10\\)M & \\(\\sim 50\\)M & Academic & multiple domains [11][12] \\\\  & Goodreads-books & 2018 & NC, LP & \\(\\sim 2\\)M & \\(\\sim 20\\)M & Books & multiple domains [13] \\\\  & Amazon-items & 2018 & NC, LP, Rec & \\(\\sim 15.5\\)M & \\(\\sim 100\\)M & E-commerce & multiple domains [14] \\\\  & ScIDocs & 2020 & NC, UAP, LP, Rec & - & - & Academic & [52] \\\\  & PubMed & 2020 & NC & 19,717 & 44,338 & Academic & [15] \\\\  & Wikidata5M & 2021 & LP & \\(\\sim 4\\)M & \\(\\sim 20\\)M & Wikipedia & [16] \\\\  & Twitter & 2023 & NC, LP & 176,279 & 2,373,956 & Social & [54] \\\\ \\hline \\multirow{4}{*}{\n' +
      '\\begin{tabular}{} \\end{tabular} } & Goodreads-reviews & 2018 & EC, LP & \\(\\sim 3\\)M & \\(\\sim 100\\)M & Books & multiple domains [13] \\\\  & Amazon-reviews & 2018 & EC, LP & \\(\\sim 15.5\\)M & \\(\\sim 200\\)M & E-commerce & multiple domains [14] \\\\ \\cline{1-1}  & Stackoverflow & 2023 & EC, LP & 129,322 & 281,657 & Social & [75] \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VII: Data collection in Section 5 for text-rich graphs. Task: “NC, “UAP,”, “LP”, “Rec”, “EC,” “RG” denote node classification, user activity prediction, link prediction, recommendation, edge classification, and regression task.\n' +
      '\n' +
      'emails, and so on. In this case, we can build a graph where nodes are users and edges are the interaction between users. There will be text associated with nodes (_e.g._, user profile) and edges (_e.g._, messages). Interesting research questions will be how to do joint text and graph structure modeling to deeply understand the users for friend recommendation [107], user analysis [108], and community detection [109].\n' +
      '\n' +
      '#### 7.3.3 Specific Domains\n' +
      '\n' +
      'In many specific domains, text data are interconnected and lie in the format of graphs. The structure information on the graphs can be utilized to better understand the text unit and contribute to advanced problem-solving.\n' +
      '\n' +
      '**Academic Domain.** In the academic domain, networks [11] are constructed with papers as nodes and their relations (_e.g._, citation, authorship, etc) as edges. The representation learned for papers on such networks can be utilized for paper recommendation [101], paper classification [102], and author identification [103].\n' +
      '\n' +
      '**Legal Domain.** In the legal domain, opinions given by the judges always contain references to opinions given for previous cases. In such a scenario, people can construct an opinion network [98] based on the citation relations between opinions. The representations learned on such a network with both text and structure information can be utilized for clause classification [99] and opinion recommendation [100].\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline  & Data & Date & Task & Size & Tested Model & Source \\& Notes \\\\ \\hline \\multirow{9}{*}{\\begin{tabular}{} \\end{tabular} } & CheShML2023 [196] & 20223 & Various & 2.487\\({}^{\\text{2}}\\)20;387 & Various & Drug-like \\\\ \\cline{2-6}  & PatChem [194] & 2019 & Various & 964\\({}^{\\text{2}}\\)20;387 & Various & Biomedical \\\\ \\cline{2-6}  & PC32MR [176] & 2023 & PT & \\multirow{2}{*}{30/M2} & \\multirow{2}{*}{MoCA [176]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & MoMCP-FP [178] & 2023 & PT & \\multirow{2}{*}{30/M2} & \\multirow{2}{*}{MoMCP [178]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & Che-bio [48] & 2023 & PT & \\multirow{2}{*}{30/M2} & \\multirow{2}{*}{MoMCP [178]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & Che-bio [48] & 2023 & PT & \\multirow{2}{*}{30/M2} & \\multirow{2}{*}{MoMCP [178]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & Che-bio [48] & 2023 & PT & \\multirow{2}{*}{30/M2} & \\multirow{2}{*}{MoMCP [178]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & Che-bio [48] & 2023 & PT & \\multirow{2}{*}{30/M2} & \\multirow{2}{*}{MoMCP [178]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & Che-bio [48] & 2023 & PT & \\multirow{2}{*}{30/M2} & \\multirow{2}{*}{MoMCP [178]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & Che-bio [48] & 2023 & PT & \\multirow{2}{*}{30/M2} & \\multirow{2}{*}{MoMCP [178]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & PC22MR [179] & 2023 & PT & \\multirow{2}{*}{223/M2} & \\multirow{2}{*}{CLAMP [179]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & PCSTM [181] & 2022 & PT & \\multirow{2}{*}{281/K1} & \\multirow{2}{*}{MoleculeSTM [181]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & PCdes [194] & 2022 & \\multirow{2}{*}{202} & \\multirow{2}{*}{Exp.} & \\multirow{2}{*}{18/K1} & \\multirow{2}{*}{K1} & \\multirow{2}{*}{K1/K1} \\\\  & & & & & & \\\\ \\cline{2-6}  & PCdes [194] & 2022 & \\multirow{2}{*}{Exp.} & \\multirow{2}{*}{18/K1} & \\multirow{2}{*}{K1/K1} & \\multirow{2}{*}{K1/K1} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & CheB8-20 [122] & 2021 & \\multirow{2}{*}{Exp.} & \\multirow{2}{*}{30/M2} & \\multirow{2}{*}{MoMCP [178]} & \\multirow{2}{*}{MoMCP [178]} & \\multirow{2}{*}{PubChem [194]} \\\\  & & & & & & \\\\ \\cline{2-6}  & & & & & & \\\\ \\cline{2-6}  & Cip. & & & & & \\\\ \\hline \\multirow{9}{*}{\\begin{tabular}{} \\end{tabular} } & ZINC15 [197] & 2015 & PT & 120M2 & Various & \\multirow{2}{*}{Twilogs [183]} & \\multirow{2}{*}{Drop-like} \\\\ \\cline{2-6}  & CDB13 [198] & 2009 & PT & 977M2 & MFBERT [188] & & Drug-like \\\\ \\cline{2-6}  & PS-Mdd [192] & 2021 & CC & 27,0652 & CLAMP [179] & & ChEMRRL [199] \\\\ \\cline{2-6}  & PCBA [208] & 2018 & CC & 439,8632 & SMILES-BERT [188], CMLET [48] & PubChem-Bio [194] \\\\ \\cline{2-6}  & MUV [208] & 2018 & CC & 93,1272 & Modus [183], CMLET [48], MoMFM [171] & PubChem-Bio [194] \\\\ \\cline{2-6}  & & & & & & \\\\ \\cline{2-6}  & HV [208] & 2018 & CC & 41,9132 & \\multirow{2}{*}{MoMCP [183], CMLET [48], MoMFM [171]} & \\multirow{2}{*}{Due Therapous [194]} \\\\  & & & & & & & \\\\ \\cline{2-6}  & & & & & & \\\\ \\cline{2-6}  & HV [208] & 2018 & CC & 16,8962 & CMLET [48] & & Pharmacicinete \\\\ \\cline{2-6}  & & & & & & & \\\\ \\cline{2-6}  & & & & & & & \\\\ \\cline{2-6}  & & & & & & & \\\\ \\cline{2-6}  & & & & & & & \\\\ \\cline{2-6}  & BACE [208] & 2018 & CC & 1,8222 & \\multirow{2}{*}{ChEMT [48], CMLET [48], MMFM [171],} & \\multirow{2}{*}{Bioting for} \\\\  & & & & & & & \\\\ \\cline{2-6}  & & & & & & & \\\\ \\cline{2-6}  & & & & & & & \\\\ \\cline{2-6}  & & & & & & & \\\\ \\cline{2-6}  & & & & & & & \\\\ \\cline{2-6}  & & & & & & & \\\\ \\hline \\multirow{9}{*}{*}{\n' +
      '\\begin{tabular}{} \\end{tabular} } & BBBP [208] & 2018 & CC & 2,0832 & \\multirow{2}{*}{2,0832} & \\multirow{2}{*}{MoM2} & \\multirow{2}{*}{1,1,124, 17, 17, 17, 18, 19, 1\n' +
      '\n' +
      '## 8 Future directions\n' +
      '\n' +
      '**Better Benchmark Datasets.** For pure graphs, most existing benchmarks are constructed for evaluating the reasoning ability of LLM on homogeneous graphs while lacking the evaluation on heterogeneous graphs or spatial-temporal graphs. For text-rich graphs, as summarized in Table VII, most benchmark datasets are from academic domains and e-commerce domains. However, in the real world, text-rich graphs are ubiquitous across multiple domains (_e.g._, legal and health). More diverse datasets are needed to comprehensively evaluate LLMs on real-world scenarios. For text-paired graphs, as summarized in Table VIII, there is a lack of comprehensive datasets covering various machine learning tasks in chemistry. Although a massive number of scientific papers are available, preprocessing them into a ready-to-use format and pairing them with specific molecular graph data points of interest remains a cumbersome and challenging task.\n' +
      '\n' +
      '**Broader Task Space with LLMs.** More comprehensive studies on the performance of LLMs for graph tasks hold promise for the future. While LLMs as encoder approaches have been explored for text-rich graphs, their application to text-captioned molecular graphs remains underexplored. Promising directions include using LLMs for data augmentation and knowledge distillation to design domain-specific GNNs for various text-paired graph tasks. Furthermore, although graph generation has been approached in text-paired graphs, it remains an open problem for text-rich graphs (_i.e._, how to conduct joint text and graph structure generation)\n' +
      '\n' +
      '**Multi-Modal Foundation Models.** One open question is, "Should we use one foundation model to unify different modalities, and how?" The modalities can include texts, graphs, and even images. For instance, molecules can be represented as graphs, described as texts, and photographed as images; products can be treated as nodes in a graph, associated with a title/description, and combined with an image. Designing a model that can conduct joint encoding for all modalities will be useful but challenging. This problem is even harder given that data from graph modalities are quite diverse (_e.g._, molecular graphs are quite different from social networks). Furthermore, there has always been a tension between efforts in building a uniform foundational model and in customizing model architectures for different domains. It is thus intriguing to ask whether a unified architecture will suit different data types, or if tailoring model designs according to domains will be necessary. Correctly answering this question can save economic and intellectual resources from unnecessary attempts and also shed light on a deeper understanding of graph-related tasks.\n' +
      '\n' +
      '**Efficient LLMs on Graphs.** While LLMs have shown a strong capability to learn on graphs, they suffer from inefficiency issues in terms of graph linearization and model optimization. On one hand, as discussed in Section 5.1.1 and 6.1.1, many methods rely on transferring graphs into sequences that can be inputted into LLMs. However, the length of the transferred sequence will increase significantly as the size of the graph increases. This poses challenges since LLMs always have a maximum sequence input length and a long input sequence will lead to higher time and memory complexity. On the other hand, optimizing LLMs itself is computationally expensive. Although some general efficient tuning methods such as LoRA are proposed, there is a lack of discussion on graph-aware LLM efficient tuning methods.\n' +
      '\n' +
      '**Generalizable and Robust LLMs on Graphs.** Another interesting direction is to explore the generalizability and robustness of LLMs on graphs. Generalizability refers to having the ability to transfer the knowledge learned from one domain graph to another; while robustness denotes having consistent prediction regarding to obfuscations and attacks. Although LLMs have demonstrated their strong generalizability and robustness in processing text, it is still an open problem whether these abilities exist for graph data.\n' +
      '\n' +
      '**LLM as Dynamic Agents on Graphs.** Although LLMs have shown their advanced capability in generating text, one-pass generation of LLMs suffers from hallucination and misinformation issues due to the lack of accurate parametric knowledge. Simply augmenting retrieved knowledge in context is also bottlenecked by the capacity of the retriever. In many real-world scenarios, graphs such as academic networks, and Wikipedia are dynamically looked up by humans for knowledge-guided reasoning. Simulating such a role of dynamic agents can help LLMs more accurately retrieve relevant information via multi-hop reasoning, thereby correcting their answers and alleviating hallucinations.\n' +
      '\n' +
      '## 9 Conclusion\n' +
      '\n' +
      'In this paper, we provide a comprehensive review of large language models on graphs. We first categorize graph scenarios where LMs can be adopted and summarize the large language models on graph techniques. We then provide a thorough review, analysis, and comparison of methods within each scenario. Furthermore, we summarize available datasets, open-source codebases, and multiple applications. Finally, we suggest future directions for large language models on graphs.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      'This should be a simple paragraph before the References to thank those individuals and institutions who have supported your work on this article.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      '* [9] McCallum, A.K., Nigam, K., Rennie, J. and Seymore, K., "Automating the construction of internet portals with machine learning," in _Information Retrieval_, _3, pp.127-163, 2003.\n' +
      '* [10] Giles, C.L., Bollacker, K.D. and Lawrence, S., "CiteSeer: An automatic citation indexing system," in _Proceedings of the third ACM conference on Digital libraries ( pp. 89-98)_, 1998.\n' +
      '* [11] Wang, K., Shen, Z., Huang, C., Wu, C.H., Dong, Y. and Kanakia, A., "Microsoft academic graph: When experts are not enough," in _Quantitative Science Studies, 1(1), pp.396-413_, 2020.\n' +
      '* [12] Zhang, Y., Jin, B., Zhu, Q., Meng, Y. and Han, J., "The Effect of Metadata on Scientific Literature Tagging: A Cross-Field Cross-Model Study," in _WWW_, 2023.\n' +
      '* [13] Wan, M. and McAuley, J., "Item recommendation on monotonic behavior chains," in _Proceedings of the 12th ACM conference on recommender systems_, 2018.\n' +
      '* [14] Ni, J., Li, J. and McAuley, J., "Justifying recommendations using distantly-labeled reviews and fine-grained aspects," in _EMNLP-IJCNLP_, 2019.\n' +
      '* [15] Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B. and Eliassi-Rad, T., "Collective classification in network data," in _AI magazine, 29(3), pp.93-93_, 2008.\n' +
      '* [16] Wang, X., Gao, T., Zhu, Z., Zhang, Z., Liu, Z., Li, J. and Tang, J., "KEPLER: A unified model for knowledge embedding and pre-trained language representation," in _TACL_, 2021.\n' +
      '* [17] Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., & Philip, S. Y., "A comprehensive survey on graph neural networks," in _IEEE transactions on neural networks and learning systems, 32(1), 4-24_, 2020.\n' +
      '* [18] Liu, J., Yang, C., Lu, Z., Chen, J., Li, Y., Zhang, M., Bai, T., Fang, Y., Sun, L., Yu, P.S. and Shi, C., "Towards Graph Foundation Models: A Survey and Beyond," in _arXiv preprint arXiv:2310.11829_, 2023.\n' +
      '* [19] Pan, S., Luo, L., Wang, Y., Chen, C., Wang, J. and Wu, X., "Unifying Large Language Models and Knowledge Graphs: A Roadmap," in _arXiv preprint arXiv:2306.08302_, 2023.\n' +
      '* [20] Senh, V., Debut, L., Chaumond, J. and Wolf, T., "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.," in _arXiv preprint arXiv:1910.01108_, 2019.\n' +
      '* [21] Wang, Y., Le, H., Gontmez, A.D., Bui, N.D., Li, J. and Hoi, S.C., "Coded5+: Open code large language models for code understanding and generation.," in _arXiv preprint arXiv:2305.07922_, 2023.\n' +
      '* [22] Devlin, J., Chang, M.W., Lee, K. and Toutanova, K., "Bert: Pre-training of deep bidirectional transformers for language understanding," in _NAACL_, 2019.\n' +
      '* [23] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. and Stoyanov, V., "Roberta: A robustly optimized bert pretraining approach," in _arXiv preprint arXiv:1907.11692_, 2019.\n' +
      '* [24] Beltagy, I., Lo, K. and Cohan, A., "SciBERT: A pretrained language model for scientific text," in _arXiv preprint arXiv:1903.10676_, 2019.\n' +
      '* [25] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantam, A., Shyam, P., Sastry, G., Askell, A., and Agarwal, "Language models are few-shot learners," in _NeurIPS_, 2020.\n' +
      '* [26] Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R. and Le, Q.V., "Mnet: Generalized autoregressive pretraining for language understanding," in _NeurIPS_, 2019.\n' +
      '* [27] Clark, K., Luong, M.T., Le, Q.V. and Manning, C.D., "Electra: Pre-training text encoders as discriminators rather than generators," in _ICLR_, 2020.\n' +
      '* [28] Meng, Y., Xiong, C., Bajaj, P., Bennett, P., Han, J. and Song, X., "Coco-lm: Correcting and contrasting text sequences for language model pretraining," in _NeurIPS_, 2021.\n' +
      '* [29] Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V. and Zettlemoyer, L., "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension," in _ACL_, 2020.\n' +
      '* [30] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W. and Liu, P.J., "Exploring the limits of transfer learning with a unified text-to-text transformer," in _JMLR_, 2020.\n' +
      '* [31] Yasunaga, M., Leskovec, J. and Liang, P., "LinkBERT: Pretraining Language Models with Document Links," in _ACL_, 2022.\n' +
      '* [32] Jin, B., Zhang, W., Zhang, Y., Meng, Y., Zhang, X., Zhu, Q. and Han, J., "Patton: Language Model Pretraining on Text-Rich Networks," in _ACL_, 2023.\n' +
      '* [33] Zhang, X., Malkov, Y., Florez, O., Park, S., McWilliams, B., Han, J. and El-Kishky, A., "TwHIN-BERT: a socially-enriched pre-trained language model for multilingual Tweet representations," in _KDD_, 2023.\n' +
      '* [34] Zou, T., Yu, L., Huang, Y., Sun, L. and Du, B., "Pretraining Language Models with Text-Attributed Heterogeneous Graphs," in _arXiv preprint arXiv:2310.12580_, 2023.\n' +
      '* [35] Song, K., Tan, X., Qin, T., Lu, J. and Liu, T.Y., "Mpnnet: Masked and permuted pre-training for language understanding," in _NeurIPS_., 2020.\n' +
      '* [36] Duan, K., Liu, Q., Chua, T.S., Yan, S., Ooi, W.T., Xie, Q. and He, J., "Simteg: A frustratingly simple approach improves textual graph learning," in _arXiv preprint arXiv:2308.02565._, 2023.\n' +
      '* [37] Kasnecl, E., Sessler, U., Groh, G., Gunnemann, S., Hullermeier, E. and Krusche, S., "ChatGPT for good? On opportunities and challenges of large language models for education," in _Learning and individual differences_, 103., 2023.\n' +
      '* [38] Lester, B., Al-Rfou, R. and Constant, N., "The power of scale for parameter-efficient prompt tuning," in _EMNLP_, 2021.\n' +
      '* [39] Li, X.L. and Liang, P., "Prefix-tuning: Optimizing continuous prompts for generation," in _ACL_, 2021.\n' +
      '* [40] Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M. and Gelly, S., "Parameter-efficient transfer learning for NLP," in _ICML_, 2019.\n' +
      '* [41] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. and Chen, W., "Lora: Low-rank adaptation of large language models," in _ICLR_, 2022.\n' +
      '* [42] Tian, Y., Song, H., Wang, Z., Wang, H., Hu, Z., Wang, F., Chawla, N.V. and Xu, P., "Graph Neural Prompting with Large Language Models," in _arXiv preprint arXiv:2309.15427._, 2023.\n' +
      '* [43] Chai, Z., Zhang, T., Wu, L., Han, K., Hu, X., Huang, X. and Yang, Y., "GraphLLM: Boosting Graph Reasoning Ability of Large Language Model," in _arXiv preprint arXiv:2310.05845._, 2023.\n' +
      '* [44] Wei, J., Bosma, M., Zhao, V.Y., Guu, K., Yu, A.W., Lester, B., Du, N., Dai, A.M. and Le Q.V., "Finetuned language models are zero-shot learners," in _ICLR_, 2022.\n' +
      '* [45] Sanh, V., Webson, A., Raffel, C., Bach, S.H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T.L., Raja, A. and Dey, M., "Multitask prompted training enables zero-shot task generalization," in _ICLR_, 2022.\n' +
      '* [46] Tang, J., Yang, Y., Wei, W., Shi, L., Su, L., Cheng, S., Yin, D. and Huang, C., "GraphGPT: GraphTraction Turning for Large Language Models," in _arXiv preprint arXiv:2310.13023_, 2023.\n' +
      '* [47] Ye, R., Zhang, C., Wang, R., Xu, S. and Zhang, Y., "Natural language is all a graph needs," in _arXiv preprint arXiv:2308.07134._, 2023.\n' +
      '* [48] Zhao, H., Liu, S., Ma, C., Xu, H., Fu, J., Deng, Z.H., Kong, L. and Liu, Q., "GIMET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning," in _bioRxiv, pp. 2023-05_, 2023.\n' +
      '* [49] Wei, J., Wang, X., Schummans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V. and Zhou, D., "Chain-of-thought prompting elicits reasoning in large language models," in _NeurIPS_, 2022.\n' +
      '* [50] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.L., Cao, Y. and Narasimhan, K., "Tree of thoughts: Delbreatable problem solving with large language models," in _arXiv preprint arXiv:2305.10601._, 2023.\n' +
      '* [51] Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gianinazzi, L., Gajda, J., Lehmann, T., Podstawski, M., Niewiadomski, H., Nyczyk, P. and Hoefler, T., "Graph of thoughts: Solving elaborate problems with large language models," in _arXiv preprint arXiv:2308.09687._, 2023.\n' +
      '* [52] Cohan, A., Feldman, S., Beltagy, I., Downey, D. and Weld, D.S., "Specter: Document-level representation learning using citation-informed transformers," in _ACL_, 2020.\n' +
      '* [53] Ostendorff, M., Reth* [58] Zhang, X., Zhang, C., Dong, X.L., Shang, J. and Han, J., "Minimally-supervised structure-rich text categorization via learning on text-rich networks," in _WWW_, 2021.\n' +
      '* [59] Chien, E., Chang, W.C., Hsieh, C.J., Yu, H.F., Zhang, J., Milenkovic, O., and Dhillon, I.S., "Node feature extraction by self-supervised multi-scale neighborhood prediction," in _ICLR_, 2022.\n' +
      '* [60] Zhang, Y., Shen, Z., Wu, C.H., Xie, B., Hao, J., Wang, Y.Y., Wang, K. and Han, J., "Metadata-induced contrastive learning for zero-shot multi-label text classification," in _WWW_, 2022.\n' +
      '* [61] Dinh, T.A., Boer, J.D., Cornless, J. and Groth, P., "E2EG: End-to-End Node Classification Using Graph Topology and Text-based Node Attributes," in _arXiv preprint arXiv:2208.04609._, 2022.\n' +
      '* [62] Zhao, J., Qu, M., Li, C., Yan, H., Liu, Q., Li, R., Xie, X. and Tang, J., "Learning on large-scale text-attributed graphs via variational inference," in _ICLR_, 2023.\n' +
      '* [63] Wen, Z. and Fang Y., "Augmenting Low-Resource Text Classification with Graph-Grunded Pre-training and Prompting," in _SIGIR_, 2023.\n' +
      '* [64] Chen, Z., Mao, H., Wen, H., Han, H., Jin, W., Zhang, H., Liu, H. and Tang, J., "Label-free Node Classification on Graphs with Large Language Models (LLMS)," in _arXiv preprint arXiv:2310.04668_, 2023.\n' +
      '* [65] Huang, X., Han, K., Bao, D., Tao, Q., Zhang, Z., Yang, Y. and Zhu, Q., "Prompt-based Node Feature Extractor for Few-shot Learning on Text-Attributed Graphs," in _arXiv preprint arXiv:2309.02848_, 2023.\n' +
      '* [66] Zhao, J., Zhuo, L., Shen, Y., Qu, M., Liu, K., Bronstein, M., Zhu, Z. and Tang, J., "Graphlet: Graph reasoning in text space," in _arXiv preprint arXiv:2310.01089_, 2023.\n' +
      '* [67] Meng, Y., Zong, S., Li, X., Sun, X., Zhang, T., Wu, F. and Li, J., "Gnn-lm: Language modeling based on global contexts via gnn," in _ICLR_, 2022.\n' +
      '* [68] Zhang, X., Bosselut, A., Yasunaga, M., Ren, H., Liang, P., Manning, C.D. and Leskovec, J., "Greaselm: Graph reasoning enhanced language models for question answering," in _ICLR_, 2022.\n' +
      '* [69] Ioannidis, V.N., Song, X., Zheng, D., Zhang, H., Ma, J., Xu, Y., Zeng, B., Chilimbt, T. and Karypis, G., "Efficient and effective training of language and graph neural network models," in _AAAI_, 2023.\n' +
      '* [70] Mavromatis, C., Ioannidis, V.N., Wang, S., Zheng, D., Adeshina, S., Ma, J., Zhao, H., Faloutsos, C. and Karypis, G., "Train Your Own GNN Teacher: Graph-Aware Distillation on Textual Graphs," in _PKDD_, 2023.\n' +
      '* [71] He, X., Bresson, X., Laurent, T. and Hooi, B., "Explanations as Features: LLM-Based Features for Text-Attributed Graphs," in _arXiv preprint arXiv:2305.19523_, 2023.\n' +
      '* [72] Yu, J., Ren, Y., Gong, C., Tan, J., Li, X. and Zhang, X., "Empower Text-Attributed Graphs Learning with Large Language Models (LLMs)," in _arXiv preprint arXiv:2310.09872_, 2023.\n' +
      '* [73] Yang, J., Liu, Z., Xiao, S., Li, C., Lian, D., Agrawal, S., Singh, A., Sun, C. and Xie, X., "GraphFormers: GNN-nested transformers for representation learning on textual graph," in _NeurIPS_, 2021.\n' +
      '* [74] Jin, B., Zhang, Y., Zhu, Q. and Han, J., "Heterformer: Transformer-based deep node representation learning on heterogeneous text-rich networks," in _KDD_, 2023.\n' +
      '* [75] Jin, B., Zhang, Y., Meng, Y. and Han, J., "Edgeformers: Graph-Empowered Transformers for Representation Learning on Textual-Edge Networks," in _ICLR_, 2023.\n' +
      '* [76] Jin, B., Zhang, W., Zhang, Y., Meng, Y., Zhao, H. and Han, J., "Learning Multiplex Embeddings on Text-rich Networks with One Text Encoder," in _arXiv preprint arXiv:2310.06684_, 2023.\n' +
      '* [77] Qin, Y., Wang, X., Zhang, Z. and Zhu, W., "Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs," in _arXiv preprint arXiv:2310.18152_, 2023.\n' +
      '* [78] Zhang, Y., Shen, Z., Dong, Y., Wang, K. and Han, J., "MATCH: Metadata-aware text classification in a large hierarchy," in _WWW_, 2021.\n' +
      '* [79] Zhu, J., Cui, Y., Liu, Y., Sun, H., Li, X., Pelegr, M., Yang, T., Zhang, L., Zhang, R. and Zhao, H., "Textgnm: Improving text encoder via graph neural network in sponsored search," in _WWW_, 2021.\n' +
      '* [80] Li, C., Pang, B., Liu, Y., Sun, H., Liu, Z., Xie, X., Yang, T., Cui, Y., Zhang, L. and Zhang, Q., "Adsgnn: Behavior-graph augmented relevance modeling in sponsored search," in _SIGIR_, 2021.\n' +
      '* [81] Zhang, J., Chang, W.C., Yu, H.F. and Dhillon, I., "Fast multi-resolution transformer fine-tuning for extreme multi-label text classification," in _NeurIPS_, 2021.\n' +
      '* [82] Xie, H., Zheng, D., Ma, J., Zhang, H., Ioannidis, V.N., Song, X., Ping, Q., Wang, S., Yang, C., Xu, Y. and Zeng, B., "Graph-Aware Language Model Pre-Training on a Large Graph Corpus Can Help Multiple Graph Applications," in _KDD_, 2023.\n' +
      '* [83] Yasunaga, M., Bosselut, A., Ren, H., Zhang, X., Manning, C.D., Liang, P.S. and Leskovec, J., "Deep bidirectional language-knowledge graph pretraining," in _NeurIPS_, 2021.\n' +
      '* [84] Huang, J., Zhang, X., Mei, Q. and Ma, J., "CAN LLMS EF-FECTIVELY LEVERAGE GRAPH STRUCTURAL INFORMATION: WHEN AND WHY," in _arXiv preprint arXiv:2309.16595._, 2023.\n' +
      '* [85] Kipf, T.N. and Welling, M., "Semi-supervised classification with graph convolutional networks," in _ICLR_, 2017.\n' +
      '* [86] Hamilton, W., Ying, Z. and Leskovec, J., "Inductive representation learning on large graphs," in _NeurIPS_, 2017.\n' +
      '* [87] Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Lio, P. and Bengio, Y., "Graph attention networks," in _ICLR_, 2018.\n' +
      '* [88] Zhang, S., Liu, Y., Sun, Y. and Shah, N., "Graph-less Neural Networks: Teaching Old MLPs New Tricks Via Distillation," in _ICLR_, 2022.\n' +
      '* [89] Liu, M., Gao, H. and Ji, S., "Towards deeper graph neural networks," in _KDD_, 2020.\n' +
      '* [90] Meng, Y., Huang, J., Zhang, Y. and Han, J., "Generating training data with language models: Towards zero-shot language understanding," in _NeurIPS_, 2022.\n' +
      '* [91] Sun, Y., Han, J., Yan, X., Yu, P.S. and Wu, T., "Pathsim: Meta path-based top-k similarity search in heterogeneous information networks," in _VLDB_, 2011.\n' +
      '* [92] Liu, H., Li, C., Wu, Q. and Lee, Y.J., "Visual instruction tuning," in _NeurIPS_, 2023.\n' +
      '* [93] Park, C., Kim, D., Han, J. and Yu, H., "Unsupervised attributed multiplex network embedding," in _AAAI_, 2020.\n' +
      '* [94] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L. and Polosukhin, I., "Attention is all you need," in _NeurIPS_, 2017.\n' +
      '* [95] Haveliwala, T.H., "Topic-sensitive pagerank," in _WWW_, 2002.\n' +
      '* [96] Oord, A.V.D., Li, Y. and Vinyals, O., "Representation learning with contrastive predictive coding," in _arXiv preprint arXiv:1807.03748_, 2018.\n' +
      '* [97] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J. and Krueger, G., "Learning transferable visual models from natural language supervision," in _ICML_, 2021.\n' +
      '* [98] Whalen, R., "Legal networks: The promises and challenges of legal network analysis," in _Mich. St. L. Ren._, 2016.\n' +
      '* [99] Friedrich, A. and Palmer, A. and Pinkal, M., "Situation entity types: automatic classification of clause-level aspect," in _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2016.\n' +
      '* [100] Guha, N., Nyarko, J., Ho, D.E., Re, C., Chilton, A., Narayana, A., Chohla-Nood, A., Peters, A., Waldon, B., Rockmore, D.N. and Zambrano, D., "Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models," in _arXiv preprint arXiv:2308.11462_, 2023.\n' +
      '* [101] Bai, X., Wang, M., Lee, I., Yang, Z., Kong, X. and Xia, F., "Scientific paper recommendation: A survey," in _Ieee Access_, _7, pp.9324-9339, 2019.\n' +
      '* [102] Chowdhury, S. and Schoen, M.P., "Research paper classification using supervised machine learning techniques," in _Intermountain Engineering, Technology and Computing_, 2020.\n' +
      '* [103] Madigan, D., Genkin, A., Lewis, D.D., Argamon, S., Fradkin, D. and Ye, L., "Author identification on the large scale," in _Proceedings of the 2005 Meeting of the Classification Society of North America (CSAN)_, 2005.\n' +
      '* [104] He, X., Deng, K., Wang, X., Li, Y., Zhang, Y. and Wang, M., "Lightgcn: Simplifying and powering graph convolution network for recommendation," in _SIGIR_, 2020.\n' +
      '* [105] Chang, J., Gao, C., He, X., Jin, D. and Li, Y., "Bundle recommendation with graph convolutional networks," in _SIGIR_, 2020.\n' +
      '* [106] Xu, H., Liu, B., Shu, L. and Yu, P.,"Emergent Abilities of Large Language Models" in _Transactions on Machine Learning Research_, 2022.\n' +
      '* [11] Kojima, T., Gu, S.S., Reid, M., Matsuo, Y. and Iwasawa, Y., 2022. "Large language models are zero-shot reasoners" in _Advances in neural information processing systems_, 35, pp.22199-22213.\n' +
      '* [12] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V. and Zhou, D., 2022. "Chain-of-thought prompting elicits reasoning in large language models" in _Advances in Neural Information Processing Systems_, 35, pp.24824-24837.\n' +
      '* [13] Radford, A., 2019. "Language Models are Unsupervised Multitask Learners" in _OpenAI blog_, 2019.\n' +
      '* [14] Mikolov, T., Chen, K., Corrado, G. and Dean, J., 2013. "Efficient estimation of word representations in vector space" in _arXiv preprint arXiv:1301.3781_.\n' +
      '* [15] Pennington, J., Socher, R. and Manning, C.D., 2014, October. "Glove: Global vectors for word representation" in _Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)_ (pp. 1532-1543).\n' +
      '* [16] Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P. and Soricut, R., 2019, September. "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations" in _International Conference on Learning Representations_.\n' +
      '* [17] Clark, K., Luong, M.T., Le, Q.V. and Manning, C.D., 2019, September. "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators" in _International Conference on Learning Representations_.\n' +
      '* [18] Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y.T., Li, Y., Lundberg, S. and Nori, H., 2023. "Sparks of artificial general intelligence: Early experiments with gpt-at" in _arXiv preprint arXiv:2303.12712_.\n' +
      '* [19] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S. and Bikel, D., 2023. "Llama 2: Open foundation and fine-tuned chat models" in _arXiv preprint arXiv:2307.09288_.\n' +
      '* [20] Jiang, A.Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.S., Casas, D.D.L., Bressand, F., Lengyel, G., Lample, G., Saulnier, L. and Lavaud, L.R., 2023. "Mistral 7B" in _arXiv preprint arXiv:2310.06825_.\n' +
      '* [21] Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M. and Ring, R., 2022. "Planning: a visual language model for few-shot learning" in _Advances in Neural Information Processing Systems_ (pp. 23716-23736).\n' +
      '* [22] Edwards, C., Zhai, C. and Ji, H., 2021, November. "Text2mol: Cross-modal molecule retrieval with natural language queries" in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_ (pp. 595-607).\n' +
      '* [23] Edwards, C., Lai, T., Ros, K., Honke, G., Cho, K. and Ji, H., 2022, December. "Translation between Molecules and Natural Language" in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_ (pp. 375-413).\n' +
      '* [24] Wang, H., Feng, S., He, T., Tan, Z., Han, X. and Tsvetkov, Y., 2023. "Can Language Models Solve Graph Problems in Natural Language" in _arXiv preprint arXiv:2305.10037._, 2023.\n' +
      '* [25] Liu, C. and Wu, B., 2023. "Evaluating large language models on graphs: Performance insights and comparative analysis" in _arXiv preprint arXiv:2308.11224_, 2023.\n' +
      '* [26] Guo, J., Du, L. and Liu, H., 2023. "GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking" in _arXiv preprint arXiv:2305.15066_, 2023.\n' +
      '* [27] Zhang, J., 2023. "Graph-TooFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT" in _arXiv preprint arXiv:2304.11116_, 2023.\n' +
      '* [28] Zhang, Z., Wang, X., Zhang, Z., Li, H., Qin, Y., Wu, S. and Zhu, W., 2023. "LMLQ3: Can Large Language Models Solve Problems on Dynamic Graphs" in _arXiv preprint arXiv:2310.17110_, 2023.\n' +
      '* [29] Luo, L., Li, Y.F., Haffari, G. and Pan, S., 2023. "Reasoning on graphs: Faithful and interpretable large language model reasoning" in _arXiv preprint arXiv:2310.01061_, 2023.\n' +
      '* [30] Jiang, J., Zhou, K., Dong, Z., Ye, K., Zhao, W.X. and Wen, J.R., 2023. "Structgpt: A general framework for large language model to reason over structured data" in _arXiv preprint arXiv:2305.09645_, 2023.\n' +
      '* [31] Fatemi, B., Halcrow, J. and Perozzi, B., 2023. "Talk like a graph: Encoding graphs for large language models" in _arXiv preprint arXiv:2310.04560_, 2023.\n' +
      '* [32] Sun, J., Xu, C., Tang, L., Wang, S., Lin, C., Gong, Y., Shum, H.Y. and Guo, J., 2023. "Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph" in _arXiv preprint arXiv:2307.07697_, 2023.\n' +
      '* [33] Danny Z. Chen. 1996. "Developing algorithms and software for geometric path planning problems" in _ACM Comput. Surv. 28, 4es (Dec. 1996), 18-es. [https://doi.org/10.1145/42224.242246_](https://doi.org/10.1145/42224.242246_), 1996.\n' +
      '* [34] Iqbal A., Hossain Md., Ebna A. (2018). "Airline Scheduling with Max Flow algorithm" in _International Journal of Computer Applications_, 2018.\n' +
      '* [35] Li Jiang, Xiaoning Zang, Ibrahim I.Y. Alghoul, Xiang Fang, Junfeng Dong, Changyong Liang, 2022. "Scheduling the covering delivery problem in last mile delivery" in _Expert Systems with Applications_, 2022.\n' +
      '* [36] Zhang, X., Wang, L., Helwig, J., Luo, Y., Fu, C., Xie, Y.,... & Ji, S. (2023). Artificial intelligence for science in quantum, atomistic, and continuum systems, _arXiv preprint arXiv:2307.08423_.\n' +
      '* [37] Rusch, T. K., Bronstein, M. M., & Mishra, S. (2023). A survey on oversmoothing in graph neural networks. arXiv preprint arXiv:2303.10993.\n' +
      '* [38] Topping, J., Di Giovanni, F., Chamberlain, B. P., Dong, X., & Bronstein, M. M. (2021). Understanding over-squashing and bottlenecks on graphs via curvature. arXiv preprint arXiv:2111.14522.\n' +
      '* [39] Zhang, B., Luo, S., Wang, L., & He, D. (2023). Rethinking the expressive power of gnns via graph biconnectivity. arXiv preprint arXiv:2301.099505.\n' +
      '* [40] Muller L, Galkin M, Morris C, Rampasek L. Attending to graph transformers. arXiv preprint arXiv:2302.04181. 2023 Feb 8.\n' +
      '* [41] Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D.,... & Liu, T. Y. (2021). Do transformers really perform badly for graph representation? Advances in Neural Information Processing Systems, 34, 28877-28888.\n' +
      '* [42] Rampasek, L., Galkin, M., Dwivedi, V. P., Luu, A. T., Wolf, G., & Beaini, D. (2022). Recipe for a general, powerful, scalable graph transformer. Advances in Neural Information Processing Systems, 35, 14501-14515.\n' +
      '* [43] Liu, G., Zhao, T., Inae, E., Luo, T., & Jiang, M. (2023). Semi-Supervised Graph Imbalanced Regression. arXiv preprint arXiv:2305.12087.\n' +
      '* [44] Wu Q. Zhao W, Li Z, Wipf DP, Yan J. Nodeformer: A scalable graph structure learning transformer for node classification. Advances in Neural Information Processing Systems. 2022 pc 6.3527387-401.\n' +
      '* [45] Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., & Liu, Y. (2021). Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864.\n' +
      '* [46] Balaban, A. T., Applications of graph theory in chemistry. _Journal of chemical information and computer sciences_, 25(3), 334-343, 1985.\n' +
      '* [47] Liu, G., Zhao, T., Xu, J., Luo, T., & Jiang, M., Graph rationalization with environment-based augmentations. In _ACM SIGKDD_, 2022.\n' +
      '* [48] Bran, A. M., Cox, S., White, A. D., & Schwaller, P., ChemCrow: Augmenting large-language models with chemistry tools, _arXiv preprint arXiv:2304.05376_, 2023.\n' +
      '* [49] Borgwardt, K. M., Ong, C. S., Schonauer, S., Vishwanathan, S. V. N., Smola, A. J., & Kriegel, H. P., Protein function prediction via graph kernels. _Bioinformatics_, 21, 47-fe6, 2005.\n' +
      '* [50] Riesen, K., & Bunke, H., IAM graph database repository for graph based pattern recognition and machine learning. In _Structural, Syntactic, and Statistical Pattern Recognition: Joint IAPR International Workshop, SSRP & SPR 2008. Orlando, USA_, December 4-6, 2008. Proceedings (pp. 287-297). Springer Berlin Heidelberg.\n' +
      '* [51] Jain, N., Coyle, B., Kashefi, E., & Kumar, N., Graph neural network initialisation of quantum approximate optimisation. Quantum, 6, 861, 2022.\n' +
      '* [52] Weininger, D., SMILES, a chemical language and information system. I. Introduction to methodology and encoding rules. _Journal of chemical information and computer sciences_, 28(1), 31-36, 1988\n' +
      '* [53] Heller S, McNaught A, Stein S, Tchekhovskoi D, Pleture I. InChI-the worldwide chemical structure identifier standard. _Journal of cheminformatics_, 2013 Dec5(1)-19.\n' +
      '* [54] O\'Boyle, N., & Dalke, A., DeepSMILES: an adaptation of SMILES for use in machine-learning of chemical structures, 2018.\n' +
      '* [55] Krenn, M., Hase, F., Nigam, A., Friederich, P., & Aspuru* [157] Arais-Pous, J., Johansson, S. V., Prykhodko, O., Bjerrum, E. J., Tyrchan, C., Reymond, J. L.,... & Engkvist, O. (2019). Randomized SMILES strings improve the quality of molecular generative models. Journal of cheminformatics, 11(1), 1-13.\n' +
      '* [158] Tetko IV, Karpov P, Bruno E, Kimber TB, Godin G. Augmentation is what you need!. InInternational Conference on Artificial Neural Networks 2019 Sep 9 (pp. 831-835). Cham: Springer International Publishing.\n' +
      '* [159] van Deursen R, Ertl P, Tetko IV, Godin G. GEN: highly efficient SMILES explore using autodidactic generative examination networks. Journal of Cheminformatics, 2020 Deci-12(1):1-4.\n' +
      '* [160] Schwaller, P., Gaudin, T., Lanyi, D., Bekas, C., & Laino, T., "Found in Translation", predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models. _Chemical science_, 9(28), 6091-6098, 2018.\n' +
      '* [161] Morgan, H. L., The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service. _Journal of chemical documentation_, 5(2), 107-113, 1965.\n' +
      '* [162] Sennrich, R., Haddow, B., & Birch, A. Neural machine translation of rare words with subword units, in _ACL_, 2016.\n' +
      '* [163] Kudo, T., & Richardson, J., Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In _EMNLP_, 2018.\n' +
      '* [164] Irwin, R., Dimitriadis, S., He, J., & Bjerrum, E. J. (2022). Chemformer: a pre-trained transformer for computational chemistry. _Machine Learning: Science and Technology_, 3(1), 015022.\n' +
      '* [165] Zhao WX, Zhou K, Li J, Tang T, Wang X, Hou Y, Min Y, Zhang B, Zhang J, Dong Z, Du Y. A survey of large language models. _arXiv preprint_ arXiv:2303.18223. 2023 Mar 31.\n' +
      '* [166] Shi, Y., Zhang, A., Zhang, E., Liu, Z., & Wang, X., ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction, in _EMNLP_, 2023.\n' +
      '* [167] Liu P, Ren Y, Ren Z., Git-mol: A multi-modal large language model for molecular science with graph, image, and text, _arXiv preprint arXiv:2308.06911_, 2023\n' +
      '* [168] Ock J, Guntuboina C, Farimani AB. Catalyst Property Prediction with CatBFTA: Unveiling Feature Exploration Strategies through Large Language Models. _arXiv preprint_ arXiv:2309.00563, 2023.\n' +
      '* [169] Fang Y, Liang X, Zhang N, Liu K, Huang R, Chen Z, Fan X, Chen H., Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. _arXiv preprint_ arXiv:2306.08018, 2023.\n' +
      '* [170] Abbdine H, Chatzianassis M, Boquivioulos C, Vazirgiannis M., Prot2Text: Multimodal Protein\'s Function Generation with GNNs and Transformers, _arXiv preprint_ arXiv:2307.14367, 2023.\n' +
      '* [171] Luo Y, Yang K, Hong M, Liu X, Nie Z., MolFM: A Multimodal Molecular Foundation Model, _arXiv preprint_ arXiv:2307.09484, 2023.\n' +
      '* [172] Qian, C., Tang, H., Yang, Z., Liang, H., & Liu, Y., Can large language models empower molecular property prediction? _arXiv preprint_ arXiv:2307.07443, 2023.\n' +
      '* [173] Born, J., & Manica, M., Regression Transformer enables concurrent sequence regression and generation for molecular language modelling. _Nature Machine Intelligence_, 5(4), 432-444, 2023.\n' +
      '* [174] Li J, Liu Y, Fan W, Wei XY, Liu H, Tang J, Li Q, Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective. _arXiv preprint_ arXiv:2306.06615, 2023.\n' +
      '* [175] Zeng, Z., Yin, B., Wang, S., Liu, J., Yang C., Yao, H.,... & Liu, Z., Interactive Molecular Discovery with Natural Language. arXiv preprint arXiv:2306.11976, 2023.\n' +
      '* [176] Liu Z, Li S, Luo Y, Fei H, Cao Y, Kawaguchi K, Wang X, Chua TS., MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter, in _EMNLP_, 2023.\n' +
      '* [177] Guo T, Guo K, Liang Z, Guo Z, Chawla NV, Wiest O, Zhang X. What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks. In _NeurIPS_, 2023.\n' +
      '* [178] Liu Z, Zhang W, Xia Y, Wu L, Xie S, Qin T, Zhang M, Liu TY., Molybdenum: Wrapping Molecules with Text for Generative Pre-training. In _ACL_, 2023.\n' +
      '* [179] Seidl, P., Vall, A., Hochreiter, S., & Klambauer, G., Enhancing activity prediction models in drug discovery with the ability to understand human language, in _ICML_, 2023.\n' +
      '* [180] Christofields, D., Giamone, G., Born, J., Winther, O., Laino, T., & Manica, M., Unifying molecular and textual representations via multi-task language modelling. in _ICML_, 2023.\n' +
      '* [181] Liu, S., Nie, W., Wang, C., Lu, J., Qiao, Z., Liu, L.,... & Anandkumar, A. Multi-modal molecule structure-text model for text-based retrieval and editing. _Nature Machine Intelligence_, 2023.\n' +
      '* [182] Lacombe, R., Gaut, A., He, J., Ludeke, D., & Pistunova, K., Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning. _ICML Workshop on Computational Biology_, 2023.\n' +
      '* [183] Su, B., Du, D., Yang, Z., Zhou, Y., Li, J., Rao, A.,... & Wen, J. R., A molecular multimodal foundation model associating molecule graphs with natural language, _arXiv preprint_ arXiv:2209.05481. 2022.\n' +
      '* [184] Zeng, Z., Yao, Y., Liu, Z., & Sun, M., A deep-learning system bridging molecule structure and biomedical text with compression comparable to human professionals, _Nature communications_, 13(1), 862.\n' +
      '* [185] Iwayama, M., Wu, S., Liu, C., & Yoshida, R., Functional Output Regression for Machine Learning in Materials Science. _Journal of Chemical Information and Modeling_, 62(2020), 4837-4851, 2022.\n' +
      '* [186] Bagal V, Aggarwal R, Vinod PK, Priyakumar U. MolCPT: molecular generation using a transformer-decoder model. _Journal of Chemical Information and Modeling_. 2021 Oct 256(29):2064-76.\n' +
      '* [187] Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hathshorn, A., Saravia, E.,... & Stojnic, R., Galactica: A large language model for science. _arXiv preprint_ arXiv:2211.09085, 2022.\n' +
      '* [188] Wang, S., Guo, Y., Wang, Y., Sun, H., & Huang, J., Smiles-bert: large scale unsupervised pre-training for molecular property prediction. In _BCB_, 2019\n' +
      '* [189] Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H., & Kang, J., BioBERT: a pre-trained biomedical language representation model for biomedical text mining. _Bioinformatics_, 36(4), 1234-1240, 2020.\n' +
      '* [190] Ma, R., & Luo, T. (2020). PIM: a benchmark database for polymer informatics. Journal of Chemical Information and Modeling, 60(10), 4684-4690.\n' +
      '* [191] Li, X., Xu, Y., Lai, L., & Pei, J. Prediction of human cytochrome P450 inhibition using a multiscale deep autoencoder neural network. _Molecular pharmacimetrics_, 15(10), 4363-4345, 2020.\n' +
      '* [192] Stanley M, Bronskill JF, Maziarz K, Miszeta H, Lanini J, Segler M, Schneider N, Brockschmidt M. FS-mol: A few-shot learning dataset of molecules, in _NeurIPS_, 2021.\n' +
      '* [193] Hastings, J., Owen, G., Dekker, A., Ennis, M., Kale, N., Muthukrishnan, V.,... & Steinbeck, C., CheB1 in 2016: Improved services and an expanding collection of metabolites. _Nucleic acids research_, 44(D1), D1214-D1219, 2016.\n' +
      '* [194] Kim, S., Chen, J., Cheng, T., Gindulyte, A., He, J., He, S.,... & Bolton, E. E., PubChem 2019 update: improved access to chemical data, _Nucleic acids research_, 47(D1), D1012-D1109, 2019.\n' +
      '* [195] Gaulton, A., Bellis, L. J., Bento, A. P., Chambers, J., Davies, M., Hersey, A.,... & Overington, J. P., ChEMBL: a large-scale bioactivity database for drug discovery. _Nucleic acids research_, 40(D1), D1100-D1107. 2012.\n' +
      '* [196] Zdrazil B, Felix E, Hunter F, Manners EJ, Blackshaw J, Corbett S, de Veij M, Ioannidis H, Lopez DM, Mosquera JF, Magarinos MP. The ChEMBL Database in 2023: a drug discovery platform spanning multiple bioactivity data types and time periods. Nucleic Acids Research. 2023 Nov 2:gkad1004.\n' +
      '* [197] Sterling, T. and Irwin, J.J., ZINC 15-ligand discovery for everyone. _Journal of chemical information and modeling_, 55(11), pp. 2324-2337, 2015.\n' +
      '* [198] Blum LC, Raymond JL. 970 million druglike small molecules for virtual screening in the chemical universe database CBB-13. Journal of the American Chemical Society. 2009 Jul 1:131(25):8732-3.\n' +
      '* [199] Mellor, C. L., Robinson, R. M., Benigni, R., Ebbrell, D., Enoch, S. J., Firman, J. W.,... & Cronin, M. T. D. (2019). Molecular fingerprint-derived similarity measures for toxicological read-across: Recommendations for optimal use. _Regulatory Toxicology and Pharmacology_, 101, 121-134.\n' +
      '* [200] Maggioro, G., Vogt, M., Stumpfe, D., & Bajorath, J. (2014). Molecular similarity in medicinal chemistry: minipperspective. _Journal of medicinal chemistry_, 57(8), 3186-3204.\n' +
      '* [21] Rogers, D., & Hahn, M. (2010). Extended-connectivity fingerprints. Journal of chemical information and modeling, 50(5), 742-754.\n' +
      '* [22] Beisken, S., Meinl, T., Wiswedel, B., de Figueiredo, L. F., Berthold, M., & Steinbeck, C. (2013). KNIME-CDK: Workflow-driven cheminformatics. _BMC bioinformatics_, 14: 1-4.\n' +
      '* [23] Krenn, M., Ai, Q., Barthel, S., Carson, N., Frei, A., Frey, N. C.,... & Aspuru-Guzik, A. (2022). SELFI* [206] Leman, A. A., & Weisfeiler, B. (1968). A reduction of a graph to a canonical form and an algebra arising during this reduction. _Nuncino-Technischekar Informatsiya_, 2(9), 12:16.\n' +
      '* [207] Li, J., Li, D., Savarese, S., & Hoi, S., Blip-2: Bootstraping language-image pre-training with frozen image encoders and large language models. _arXiv preprint_ arXiv:2301.12597.\n' +
      '* [208] Wu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Geniesse, C., Pappu, A. S.,... & Pande, V. (2018). MoleculeNet: a benchmark for molecular machine learning. Chemical science, 9(2), 513-530.\n' +
      '* [209] AIDS Antiviral Screen Data. [https://wiki.nichin.gov/display/NCIDTPdata/AIDS+Antiviral+Screen+Data](https://wiki.nichin.gov/display/NCIDTPdata/AIDS+Antiviral+Screen+Data), Accessed: 2017-09-27\n' +
      '* [210] Subramanian, G., Ramsundar, B., Pande, V., & Denny, R. A. (2016). Computational modeling of \\(\\beta\\)-secretase 1 (BACE-1) inhibitors using ligand based approaches. Journal of chemical information and modeling, 56(10), 1936-1949.\n' +
      '* [211] Martins, I. F., Teixeira, A. L., Pinheiro, L., & Falcao, A. O. (2012). A Bayesian approach to in silico blood-brain barrier penetration modeling. Journal of chemical information and modeling, 52(6), 1686-1697.\n' +
      '* [212] Tox21 Challenge. [https://tripod.nih.gov/tox21/challenge/](https://tripod.nih.gov/tox21/challenge/), Accessed: 2017-09-27\n' +
      '* [213] Altae-Tran H, Ramsundar B, Pappu AS, Pande V. Low data drug discovery with one-shot learning. ACS central science. 2017 Apr 26:34(8):283-39.\n' +
      '* [214] Novick PA, Ortiz OF, Poelman J, Abdulhay AY, Pande VS. SWEETLEAD: an in silico database of approved drugs, regulated chemicals, and herbal isolates for computer-aided drug discovery. PloS one. 2013 Nov 18(11)e79568.\n' +
      '* [215] Aggregate Analysis of ClinicalTrials.gov (AACT) Database. [https://www.ctti-clinicaltrials.org/aact-database](https://www.ctti-clinicaltrials.org/aact-database), Accessed: 2017-09-27.\n' +
      '* [216] Mobley DL, Guthrie JP. FreeSolv: a database of experimental and calculated hydration free energies, with input files. Journal of computer-aided molecular design. 2014 Jul;28:711-20.\n' +
      '* [217] Delaney, J. S. (2004). ESOL: estimating aqueous solubility directly from molecular structure. Journal of chemical information and computer sciences, 44(3), 1000-1005.\n' +
      '* [218] Zhao, T., Liu, G., Wang, D., Yu, W., & Jiang, M. (2022, June). Learning from counterfactual links for link prediction. In International Conference on Machine Learning (pp. 26911-26926). PMLR.\n' +
      '* [219] Vignac, C., Krawczuk, I., Siraudin, A., Wang, B., Cevher, V., & Frossard, P. (2022). Digress: Discrete density diffusion for graph generation. arXiv preprint arXiv:2209.14734.\n' +
      '* [220] Zang, C., & Wang, F. Mollow: an invertible flow model for generating molecular graphs. In _ACM SIGKDD_, 2020.\n' +
      '* [221] Deng, J., Yang, Z., Wang, H., Ojima, I., Samaras, D., & Wang, F. (2023). A systematic study of key elements underlying molecular property prediction. Nature Communications, 14(1), 6395.\n' +
      '* [222] Bohm, H. J., Flohr, A., & Stahl, M. (2004). Scaffold hopping. Drug discovery today: Technologies, 1(3), 217-224.\n' +
      '* [223] Renz, P., Van Rompaey, D., Wegner, J. K., Hochreiter, S., & Klambauer, G. (2019). On failure modes in molecule generation and optimization. Drug Discovery Today: Technologies, 32, 55-63.\n' +
      '* [224] Polykovskiy, D., Zhebrak, A., Sanchez-Lengeling, B., Golovanov, S., Tatonov, O., Belyaev, S.,... & Zharovonkov, A. (2020). Molecular sets (MOSES): a benchmarking platform for molecular generation models. Frontiers in pharmacology, 11, 565644.\n' +
      '* [225] Reymond, J. L. (2015). The chemical space project. Accounts of Chemical Research, 48(3), 722-730.\n' +
      '* [226] Lin, A., Horvath, D., Afonina, V., Marcou, G., Reymond, J. L., & Varnek, A. (2018). Mapping of the Available Chemical Space versus the Chemical Universe of Lead-Like Compounds. ChemMedChem, 13(6), 540-554.\n' +
      '* [227] Gao, W., Fu, T., Sun, J., & Coley, C. (2022). Sample efficiency matters: a benchmark for practical molecular optimization. Advances in Neural Information Processing Systems, 35, 21342-21357.\n' +
      '* [228] Wang, H., Fu, T., Du, Y., Gao, W., Huang, K., Liu, Z.,... & Zitnik, M. (2023). Scientific discovery in the age of artificial intelligence. Nature, 620(7972), 47-60.\n' +
      '* [229] Liu, G., Hae, E., Zhao, T., Xu, J., Luo, T., & Jiang, M. (2023). Data-Centric Learning from Unlabeled Graphs with Diffusion Model. arXiv preprint arXiv:2303.10108.\n' +
      '* [230][https://pracaticalcheminformatics.blogspot.com/2023/08/we-need-better-benchmarks-for-machine.html?m=1](https://pracaticalcheminformatics.blogspot.com/2023/08/we-need-better-benchmarks-for-machine.html?m=1)\n' +
      '* [231] Merchant, A., Batzner, S., Schoenholz, S. S., Aykol, M., Cheon, G., & Cubuk, E. D. (2023). Scaling deep learning for materials discovery. Nature, 1-6.\n' +
      '* [232] Jo, J., Lee, S., & Hwang, S. J. (2022, June). Score-based generative modeling of graphs via the system of stochastic differential equations. In International Conference on Machine Learning (pp. 10362-10383). PMLR.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>