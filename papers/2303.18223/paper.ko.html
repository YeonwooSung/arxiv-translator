<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 대규모 언어 모델 조사\n' +
      '\n' +
      'Wayne Xin Zhao, Kun Zhou\\({}^{\\star}\\), Junyi Li\\({}^{\\star}\\), Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie and Jiong Wen\n' +
      '\n' +
      '* _Version: v13 (major update on November 23, 2023).__ GitHub 링크: [https://github.com/RICAIBox/LMSurvey_Chinese](https://github.com/RICAIBox/LMSurvey_Chinese) 버전 링크: [https://github.com/RICAIBox/LMSurvey/blob/unit/assets/LLM_Survey_Chinese.pdf_](https://github.com/RICAIBox/LMSurvey/blob/unit/assets/LLM_Survey_Chinese.pdf_)* _K. Zhou와 J. Li는 이 작업에 동등하게 기여했다. _ 저자는 주로 중국 베이징에 있는 중국 인민대학교의 Gaoling College of Artificial Intelligence and School of Information, Jian-Yun Nie는 캐나다 몬트리올 대학의 DIRO와 함께 있다. Contact e-mail: bitmantly@gmail.com_이 조사 논문의 저자들은 수치/하블의 모든 저작권을 보유하며, 출판 목적을 위해 이러한 자료의 어떠한 사용도 조사 저자들에 의해 공식적으로 부여되어야 한다._\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '튜링 테스트가 1950년대에 제안된 이후로, 인간은 기계에 의한 언어 지능의 숙달을 탐구해 왔다. 언어는 본질적으로 문법 규칙에 의해 지배되는 복잡하고 복잡한 인간 표현의 체계이다. 언어를 이해하고 이해하기 위한 유능한 인공지능(AI) 알고리즘을 개발하는 것은 중요한 도전 과제이다. 주요 접근법으로서, _언어 모델링_은 지난 20년 동안 언어 이해 및 생성을 위해 널리 연구되어 왔으며, 통계 언어 모델에서 신경 언어 모델로 진화했다. 최근 대규모 말뭉치를 대상으로 Transformer 모델을 사전 학습하여 사전 학습된 언어 모델(PLMs)이 제안되어 다양한 자연어 처리(NLP) 작업을 해결하는 데 강력한 성능을 보이고 있다. 연구자들은 모델 스케일링이 개선된 모델 용량으로 이어질 수 있다는 것을 발견했기 때문에, 그들은 파라미터 스케일을 훨씬 더 큰 크기로 증가시켜 스케일링 효과를 추가로 조사한다. 흥미롭게도, 파라미터 척도가 일정 수준을 초과할 때, 이러한 확대된 언어 모델들은 상당한 성능 향상을 달성할 뿐만 아니라, 소규모 언어 모델들(_e.g.,_ BERT)에는 존재하지 않는 일부 특별한 능력들(_e.g.,_ 문맥내 학습)을 나타낸다. 서로 다른 파라미터 척도에서 언어 모델을 구별하기 위해, 연구 커뮤니티는 상당한 크기(예를 들어, 수십 또는 수천억 개의 파라미터를 포함하는)의 PLM에 대한 용어 _대형 언어 모델(LLM)_을 만들었다. 최근 LIM에 대한 연구는 학계와 산업계 모두에 의해 크게 진전되었으며, 괄목할 만한 진전은 ChatGPT(LIM을 기반으로 개발된 강력한 AI 챗봇)의 출시로 사회의 광범위한 관심을 받고 있다. LIM의 기술적 진화는 AI 알고리즘을 개발하고 사용하는 방식에 혁명을 일으킬 전체 AI 커뮤니티에 중요한 영향을 미치고 있다. 이러한 급속한 기술적 진보를 고려하여 본 조사에서는 배경, 주요 연구 결과 및 주류 기술을 도입하여 LIM의 최근 발전을 검토한다. 특히 LIM의 4가지 주요 측면, 즉 사전 훈련, 적응 튜닝, 활용도, 역량 평가에 초점을 맞춘다. 또한 LIM을 개발하기 위한 가용 자원을 요약하고 향후 방향에 대한 나머지 문제를 논의한다. 이 조사는 연구자와 엔지니어 모두에게 유용한 자원이 될 수 있는 LIM에 대한 최신 문헌 검토를 제공한다.\n' +
      '\n' +
      ' 대형 언어 모델; 출현 능력; 적응 튜닝; 활용; 정렬; 용량 평가\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '_"내 언어의 한계는 내 세계의 한계를 의미한다._\n' +
      '\n' +
      '_--Ludwig Wittgenstein_\n' +
      '\n' +
      '언어는 인간이 표현하고 의사소통하는 능력이 두드러진 것으로, 유아기에 발달하여 일생에 걸쳐 진화한다[3, 4]. 기계는 그러나 강력한 인공지능(AI) 알고리즘을 탑재하지 않으면 인간의 언어 형태로 이해하고 소통하는 능력을 자연스럽게 파악할 수 없다. 이 목표를 달성하고 기계가 인간처럼 읽고 쓰고 소통할 수 있도록 하는 것은 오랜 연구 과제였다[5].\n' +
      '\n' +
      '기술적으로, _언어 모델링(LM)_은 기계의 언어 지능을 발전시키기 위한 주요 접근법 중 하나이다. 일반적으로, LM은 미래의 (또는 누락된) 토큰의 확률을 예측하기 위해, 단어 시퀀스의 생성 가능성을 모델링하는 것을 목표로 한다. LM에 대한 연구는 문헌에서 광범위한 관심을 받았으며, 이는 크게 네 가지 발전 단계로 나눌 수 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_SLM(통계 언어 모델)_입니다. SLM[6, 7, 8, 9]은 1990년대에 상승한 _통계적 학습_ 방법을 기반으로 개발되었다. 기본 아이디어는 마르코프 가정에 기초하여 단어 예측 모델을 구축하는 것이고, _예를 들어_ 가장 최근의 문맥에 기초하여 다음 단어를 예측하는 것이다. 고정된 컨텍스트 길이 \\(n\\)를 갖는 SLM은 \\(n\\)-그램 언어 모델, 즉 _예를 들어_ 빅램 및 트리그램 언어 모델이라고도 한다. SLM은 정보 검색(IR; Information Retrieval)[10, 11] 및 자연어 처리(NLP; Natural Language Processing)[12, 13, 14]에서 태스크 성능을 향상시키기 위해 널리 적용되어 왔다. 그러나 이들은 종종 차원성의 저주를 겪는데, 기하급수적인 수의 전이 확률을 추정해야 하기 때문에 고차 언어 모델을 정확하게 추정하기 어렵다. 따라서 데이터 희소성 문제를 완화하기 위해 백오프 추정[15], Good-Turing 추정[16]과 같은 특별히 설계된 평활화 전략이 도입되었다.\n' +
      '\n' +
      '\\(\\bullet\\)_신경망 언어 모델(NLM)_입니다. NLMs [17, 18, 1]은 신경망, _예를 들어, 다층 퍼셉트론(MLP) 및 순환 신경망(RNN)에 의한 단어 시퀀스의 확률을 특성화한다. 주목할 만한 기여로서, [1]의 작업은 단어들의 _분산 표현_ 개념을 도입하고, 집합된 문맥 특징들(_i.,_분산된 단어 벡터들)에 조건화된 단어 예측 함수를 구축했다. 텍스트 데이터에 대한 효과적인 특징을 학습하는 아이디어를 확장하여 다양한 NLP 작업에 대한 통합된 엔드 투 엔드 솔루션을 구축하기 위한 일반적인 신경망 접근법을 개발했다[2]. 또한, 분산 단어 표현을 학습하기 위한 단순화된 얕은 신경망을 구축하기 위해 word2vec[19, 20]이 제안되었으며, 이는 다양한 NLP 작업에 걸쳐 매우 효과적인 것으로 입증되었다. 이러한 연구들은 NLP 분야에 중요한 영향을 미치는 표현 학습을 위한 언어 모델의 사용을 시작했다.\n' +
      '\n' +
      '\\(\\bullet\\)_Pre-trained language models (PLM)_. 초기 시도로서, ELMo[21]는 먼저 양방향 LSTM(biLSTM) 네트워크를 사전 트레이닝(고정 단어 표현을 학습하는 대신)한 다음 특정 다운스트림 태스크에 따라 biLSTM 네트워크를 미세 조정함으로써 컨텍스트 인식 단어 표현을 캡처하도록 제안되었다. 또한, 자체 주의 메커니즘을 가진 고도로 병렬화 가능한 트랜스포머 아키텍처 [22]를 기반으로, BERT [23]은 레이블이 없는 대규모 코퍼스에서 특별히 설계된 사전 훈련 작업을 사용하여 양방향 언어 모델을 사전 훈련함으로써 제안되었다. 이러한 사전 훈련된 문맥 인식 단어 표현은 범용 의미 자질로서 매우 효과적이며, 이는 NLP 작업의 성능 기준을 크게 높였다. 이 연구는 _"사전 훈련"_과 _미세 조정"_ 학습 패러다임을 설정하는 많은 후속 작업에 영감을 주었다. 이러한 패러다임에 따라, PLM에 대한 많은 연구가 개발되었으며, 상이한 아키텍처[24, 25](_e.g._, GPT-2[26] 및 BART[24]) 또는 개선된 사전 트레이닝 전략[27, 28, 29]을 도입하였다. 이러한 패러다임에서 다양한 다운스트림 작업에 적응하기 위해 PLM을 미세 조정해야 하는 경우가 많다.\n' +
      '\n' +
      '\\(\\bullet\\)_Large language models (LLM)_. 연구자들은 PLM(_e.g._, 스케일링 모델 크기 또는 데이터 크기)을 스케일링하는 것이 다운스트림 태스크에서 종종 개선된 모델 용량으로 이어진다는 것을 발견한다\n' +
      '\n' +
      '도. 1: 키프레이즈 _“언어 모델”_(2018년 6월 이후) 및 _“대형 언어 모델”_(2019년 10월 이후)을 각각 포함하는 arXiv 논문의 누적 개수 추이. 통계는 제목 또는 초록의 키프레이즈를 월별로 쿼리하여 정확한 일치를 사용하여 계산된다. 우리는 "언어 모델"이 더 일찍 탐구되었기 때문에 두 키프레이즈에 대해 서로 다른 x축 범위를 설정했다. LLM의 연구 진행 과정에서 중요한 랜드마크에 해당하는 지점에 레이블을 붙인다. ChatGPT의 출시 후에 급격한 증가가 발생하는데, _“큰 언어 모델”_을 제목 또는 초록에 포함하는 발표된 arXiv 논문의 평균 수는 하루 0.40개에서 하루 8.58개로 증가한다(도 1(b)).\n' +
      '\n' +
      '도. 2: 과제 해결 능력의 관점에서 4세대 언어 모델(LM)의 진화 과정. 각 단계에 대한 기간은 매우 정확하지 않을 수 있으며 각 단계에서 가장 대표적인 연구의 출판 날짜에 따라 주로 시간을 설정했다. 신경 언어 모델의 경우, NPLM[1](_"A neural probabilistic language model"_) 및 NLPS[2](_"Natural language processing(거의 처음부터)"_)의 두 가지 접근법의 이름을 지정하기 위해 두 개의 대표적인 연구의 논문 제목을 축약한다. 공간적 한계로 인해 우리는 이 그림에서 대표적인 연구들을 모두 나열하지는 않는다.\n' +
      '\n' +
      '(_i.e._, 후속 스케일링 법칙 [30]). 많은 연구들이 훨씬 더 큰 PLM(예를 들어, 175B-파라미터 GPT-3 및 540B-파라미터 PaLM)을 트레이닝함으로써 성능 한계를 탐구했다. 스케일링은 주로 모델 크기(유사한 아키텍처 및 사전 트레이닝 태스크를 갖는)에서 수행되지만, 이러한 대형 PLM은 더 작은 PLM(_예를 들어,_330M-파라미터 BERT 및 1.5B-파라미터 GPT-2)과 상이한 거동을 나타내고 일련의 복잡한 태스크를 해결하는 데 놀라운 능력(_응급 능력_[31])을 나타낸다. 예를 들어, GPT-3은 _in-context learning_을 통해 소수의 샷 태스크를 해결할 수 있는 반면, GPT-2는 잘 할 수 없다. 따라서, 연구 커뮤니티는 이러한 대형 PLM[32, 33, 34, 35]에 대한 용어 _"대형 언어 모델(LLM)"1_을 코인하며, 이는 증가하는 연구 관심을 끈다(도 1 참조). LLM의 놀라운 적용은 인간과의 놀라운 대화 능력을 나타내는 GPT 시리즈의 LLM을 대화에 적응시키는 _ChatGPT2_이다. 우리는 그림 1에서 ChatGPT의 출시 후 LLM과 관련된 arXiv 논문의 급격한 증가를 관찰할 수 있다.\n' +
      '\n' +
      '각주 1: LLM이 반드시 작은 PLM보다 더 능력 있는 것은 아니며, 일부 LLM에서는 비상 능력이 발생하지 않을 수 있다는 점에 유의한다.\n' +
      '\n' +
      '각주 2: [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/)\n' +
      '\n' +
      '앞서 살펴본 바와 같이 언어 모델은 LLM에 특화된 새로운 기술적 개념이 아니라 수십 년에 걸쳐 인공지능의 발전과 함께 진화해 왔다. 초기 언어 모델은 주로 텍스트 데이터를 모델링하고 생성하는 것을 목표로 하는 반면, 최신 언어 모델(예: GPT-4)은 복잡한 작업 해결에 중점을 둔다. 언어 모델링_에서 과제 해결_에 이르기까지 과학적 사고의 중요한 도약이며, 이는 연구사에서 언어 모델의 개발을 이해하는 열쇠이다. 과제 해결의 관점에서 4세대 언어 모델은 서로 다른 수준의 모델 역량을 보여주었다. <그림 2>에서는 과제 해결력 측면에서 언어 모델의 진화 과정을 설명한다. 처음에, 통계적 언어 모델들은 주로 몇몇 특정 태스크들(_예를 들어,_검색 또는 음성 태스크들)을 보조했는데, 여기서 예측되거나 추정된 확률들은 태스크-특정 접근법들의 성능을 향상시킬 수 있다. 이어서, 신경 언어 모델들은 인간 특징 공학에 대한 노력을 감소시키는 것을 목표로, 태스크-불가지론 표현들(_예를 들어,_특징들)을 학습하는 것에 초점을 맞추었다. 또한 사전 학습된 언어 모델은 다운스트림 작업에 따라 최적화될 수 있는 상황 인식 표현을 학습하였다. 최신 언어 모델의 경우 범용 태스크 해결자로 간주될 수 있는 모델 용량에 대한 스케일링 효과를 탐색하여 LLM을 향상시킨다. 요약하면, 진화 과정에서 언어 모델이 해결할 수 있는 과업 범위가 크게 확장되었고, 언어 모델이 달성한 과업 성능이 크게 향상되었다.\n' +
      '\n' +
      '기존 문헌에서 PLM은 널리 논의되고 조사되었지만[36, 37, 38, 39], LLM은 체계적인 방식으로 검토되는 경우가 거의 없다. 설문 조사에 동기를 부여하기 위해 먼저 LLM과 PLM 간의 세 가지 주요 차이점을 강조한다. 첫째, LLM은 이전의 더 작은 PLM에서 관찰되지 않을 수 있는 몇 가지 놀라운 비상 능력을 보여준다. 이러한 능력은 복잡한 작업에 대한 언어 모델의 성능에 핵심이며, AI 알고리즘을 탁월하게 강력하고 효과적으로 만든다. 둘째, LLM은 인간이 AI 알고리즘을 개발하고 사용하는 방식에 혁명을 일으킬 것이다. 소형 PLM들과 달리, LLM들에 액세스하기 위한 주요 접근법은 프롬프팅 인터페이스(_예를 들어,_GPT-4 API)를 통한 것이다. 인간은 LLM이 어떻게 작동하는지 이해하고 LLM이 따를 수 있는 방식으로 작업을 포맷해야 한다. 셋째, LLM의 개발은 더 이상 연구와 공학의 명확한 구분을 이끌어내지 못한다. LLM의 훈련은 대규모 데이터 처리 및 분산 병렬 훈련에 대한 광범위한 실무 경험이 필요하다. 유능한 LLM을 개발하기 위해 연구자들은 엔지니어와 함께 작업하거나 엔지니어가 되는 복잡한 엔지니어링 문제를 해결해야 한다.\n' +
      '\n' +
      '오늘날 LLM은 AI 커뮤니티에 상당한 영향을 미치고 있으며 ChatGPT와 GPT-4의 등장은 인공지능(AGI)의 가능성을 재고하게 한다. OpenAI는 AGI에 접근하기 위한 단기 및 장기 계획을 논의하는 "AGI 및 그 이상의 계획"이라는 제목의 기술 기사를 발표했으며, 보다 최근의 논문은 GPT-4가 AGI 시스템의 초기 버전으로 간주될 수 있다고 주장했다. LLM의 급속한 발전으로 AI의 연구 분야가 혁명을 일으키고 있다. NLP 분야에서 LLM은 범용 언어 과제 해결자 역할을 할 수 있으며(어느 정도), 연구 패러다임이 LLM의 사용으로 전환되고 있다. IR 분야에서, 전통적인 검색 엔진은 AI 챗봇(_i.,_ ChatGPT)을 통한 새로운 정보 탐색 방식에 도전을 받고, _New Bing3_는 LLMs에 기초하여 검색 결과를 향상시키는 초기 시도를 제시한다. CV 분야에서 연구자들은 멀티모달 대화(42, 43, 44, 45)를 더 잘 제공할 수 있는 ChatGPT와 같은 비전 언어 모델을 개발하려고 시도했으며 GPT-4[46]는 시각 정보를 통합하여 멀티모달 입력을 지원했다. 이러한 새로운 기술의 흐름은 잠재적으로 LLM을 기반으로 한 실제 응용 프로그램의 번영하는 생태계로 이어질 것이다. 예를 들어 Microsoft 365는 사무 작업을 자동화 하기 위해 LLM (_i.,_ Copilot)에 의해 권한을 부여 받고 있으며 OpenAI는 특수 기능을 구현 하기 위해 ChatGPT에서 플러그 인 사용을 지원 합니다.\n' +
      '\n' +
      '각주 3: [https://www.bing.com/new](https://www.bing.com/new)\n' +
      '\n' +
      '진전과 영향에도 불구하고 LLM의 기본 원칙은 여전히 잘 탐구되지 않았다. 첫째, 더 작은 PLM 대신 LLM에서 창발적 능력이 발생하는 이유는 불가사의하다. 보다 일반적인 문제로서 LLM의 우수한 능력에 기여하는 핵심 요인에 대한 깊고 자세한 조사가 부족하다. LLM이 언제 어떻게 그러한 능력을 얻는지 연구하는 것이 중요하다[47]. 이 문제에 대한 몇 가지 의미 있는 논의[47, 31]가 있지만 LLM의 _"비밀"_을 밝히기 위해서는 보다 원론적인 조사가 필요하다. 둘째, 연구 커뮤니티가 유능한 LLM을 훈련시키는 것은 어렵다. 계산 자원의 엄청난 수요로 인해 LLM을 훈련하기 위한 다양한 전략의 효과를 조사하기 위한 연구를 반복하는 것은 매우 비용이 많이 든다. 실제로, LLM은 주로 산업별로 훈련되며, 여기서 많은 중요한 훈련 세부사항(예를 들어, 데이터 수집 및 청소)이 대중에게 드러나지 않는다. 셋째, LLM을 인간의 가치나 선호도와 일치시키는 것은 어렵다. 용량에도 불구하고 LLM은 독성, 허구 또는 유해한 내용물을 생성할 가능성이 있다. LLM 사용의 잠재적 위험을 제거하기 위한 효과적이고 효율적인 제어 접근법이 필요하다[46].\n' +
      '\n' +
      '기회와 도전 모두에 직면한 LLM의 연구 및 개발에 더 많은 관심이 필요하다. LLM에 대한 기본적인 이해를 돕기 위해, 본 조사는 LLM의 최근 발전에 대한 문헌 검토를 수행하는데, _pre-training_ (유능한 LLM을 사전 훈련하는 방법), _adaptation_ (사전 훈련된 LLM을 더 잘 사용하기 위해 효과적으로 적응하는 방법), _utilization_ (다양한 다운스트림 태스크를 해결하기 위해 LLM을 사용하는 방법), _capability evaluation_ (LLM의 능력을 평가하는 방법 및 기존 경험적 발견)을 포함한다. 우리는 문헌을 철저히 검토하고 LLM의 주요 발견, 기술 및 방법을 요약한다. 이 조사를 위해 [https://github.com/RUCAlBox/LLMSurvey](https://github.com/RUCAlBox/LLMSurvey) 링크에서 LLMs에 대 한 지원 리소스를 수집 하 여 GitHub 프로젝트 웹 사이트를 만듭니다. 우리는 또한 PLM 또는 LLM에 대한 여러 관련 리뷰 기사[32, 36, 38, 39, 43, 48, 49, 50, 51, 52, 53, 54]를 알고 있다. 이 논문은 PLM 또는 LLM의 일부 특정(또는 일반) 측면에 대해 논의한다. 이와 비교하여 LLM을 개발하고 사용하는 기술과 방법에 초점을 맞추고 LLM의 중요한 측면에 대한 비교적 포괄적인 참조를 제공한다.\n' +
      '\n' +
      '이 조사의 나머지 부분은 다음과 같이 구성된다. 섹션 2는 LLM의 배경과 GPT 시리즈 모델의 진화를 소개하고 섹션 3에서 LLM 개발을 위한 가용 자원을 요약한다. 섹션 4, 5, 6 및 7은 각각 사전 훈련, 적응, 활용 및 용량 평가의 네 가지 측면에서 최근 진행 상황을 검토하고 요약한다. 그런 다음 섹션 8에서는 신속한 설계를 위한 실용적인 가이드에 대해 논의하고 섹션 9에서는 여러 대표적인 도메인에서 LLM의 적용을 검토한다. 마지막으로 주요 연구 결과를 요약하여 섹션 10에서 조사를 마무리하고 향후 작업을 위한 나머지 문제에 대해 논의한다.\n' +
      '\n' +
      '## 2 Overview\n' +
      '\n' +
      '이 섹션에서는 LLM의 배경에 대한 개요를 제시한 다음 GPT 시리즈 모델의 기술적 진화를 요약한다.\n' +
      '\n' +
      '### _Background for LLMs_\n' +
      '\n' +
      '통상적으로 _large language models_(LLMs)는 GPT-3[55], PaLM[56], Galactica[35], LLMA[57]와 같이 방대한 텍스트 데이터[32]에 대해 훈련되는 수천억(또는 그 이상)의 파라미터 4를 포함하는 Transformer 언어 모델을 의미한다. LLM은 (텍스트 생성을 통해) 자연어를 이해하고 복잡한 작업을 해결하는 강력한 능력을 나타낸다. LLM이 어떻게 작동하는지 빠르게 이해하기 위해 이 부분에서는 스케일링 법칙, 비상 능력 및 주요 기술을 포함하여 LLM의 기본 배경을 소개한다.\n' +
      '\n' +
      '각주 4: 기존 문헌에서는 모델 용량이 데이터 크기 및 총 계산과 관련이 있기 때문에 LLM에 대한 최소 매개변수 척도에 대한 공식적인 합의가 없다. 이 조사에서 LLM에 대한 약간 느슨한 정의를 취하며 주로 10B보다 큰 모델 크기를 가진 언어 모델에 대해 논의하는 데 중점을 둔다.\n' +
      '\n' +
      '**LLM에 대 한 크기 조정 규칙 공식** 입니다. 현재 LLM은 주로 트랜스포머 아키텍처[22]를 기반으로 하며, 여기서 다중 헤드 주의 계층은 매우 깊은 신경망에 쌓여 있다. 기존의 LLM은 작은 언어 모델로서 유사한 Transformer 아키텍처 및 사전 훈련 목표(_e.g._, 언어 모델링)를 채택한다. 그러나 LLM은 모델 크기, 데이터 크기 및 총 계산(배율 순서)을 크게 확장합니다. 광범위한 연구에 따르면 스케일링은 LLM의 모델 용량을 크게 향상시킬 수 있다[56, 55, 26]. 따라서, 스케일링 효과를 특성화하기 위한 정량적 접근법을 확립하는 것이 유용하다. 다음으로, 트랜스포머 언어 모델에 대한 두 가지 대표적인 스케일링 법칙을 소개한다[30, 34].\n' +
      '\n' +
      '\\(\\bullet\\)_KM scaling law5_. 2020년 Kaplan et al. [30] (the OpenAI team)은 신경망 언어 모델에 대해 모델 크기(\\(N\\)), 데이터 세트 크기(\\(D\\)), 훈련 계산량(\\(C\\)의 세 가지 주요 요인으로 모델 성능의 멱법칙 관계를 모델링하는 것을 처음으로 제안했다. 계산 예산 \\(c\\)이 주어지면 스케일링 법칙 6에 대한 세 가지 기본 공식을 경험적으로 제시합니다.\n' +
      '\n' +
      '각주 5: 원래 논문에서 이 법칙을 따라 훈련된 모델이 없었기 때문에 두 명의 공동 첫 번째 저자의 성을 사용하여 이 스케일링 법칙의 이름을 지정했다.\n' +
      '\n' +
      '각주 6: 여기서, \\(N_{c}\\)\\(D_{c}\\) 및 \\(C_{c}\\)은 각각 비-임베딩 파라미터의 수, 트레이닝 토큰의 수 및 FP-일의 수로 측정된다. 원 논문 [30]에 따르면, \\(C_{c}\\)과 \\(C\\)은 계산의 최적 사용에 해당하는 \\(C_{c}^{min}\\)과 \\(C_{min}\\으로 표시되어야 한다. 우리는 논의를 쉽게 하기 위해 단순화된 표기법을 사용한다.\n' +
      '\n' +
      '\\[L(N) = \\left(\\frac{N_{c}}{N}\\right)^{\\alpha_{N}},\\\\\\alpha_{N}\\sim 0.076,N_{c}\\sim 8.8 \\times 10^{13} \\tag{1}\\] \\[L(D) = \\left(\\frac{D_{c}}{D}\\right)^{\\alpha_{D}},\\\\\\alpha_{D}\\sim 0.095,D_{c}\\sim 5.4 \\times 10^{13}\\] \\[L(C) = \\left(\\frac{C_{c}}{C}\\right)^{\\alpha_{C}},\\\\\\alpha_{C}\\sim 0.050,C_{c}\\sim 3.1 \\times 10^{8}\\]\n' +
      '\n' +
      '여기서 \\(L(\\cdot)\\)은 nats의 교차 엔트로피 손실을 나타내며 OpenAI의 후속 연구 [58]은 언어 모델링 손실이 _irreducible loss_ (실제 데이터 분포의 엔트로피)와 _reducible loss_ (실제 분포와 모델 분포 사이의 KL 발산 추정치)의 두 부분으로 분해될 수 있음을 보여주었다. 3가지 법칙은 다양한 데이터 크기(22M~23B 토큰), 모델 크기(768M~1.5B 비임베딩 매개변수) 및 훈련 계산으로 모델 성능을 피팅하여 파생되었으며, 일부 가정(예: 한 요인의 분석에서는 다른 두 요인에 의해 병목되지 않아야 한다. 그들은 모델 성능이 세 가지 요인에 대해 강한 의존 관계를 가지고 있음을 보여주었다.\n' +
      '\n' +
      '\\(\\bullet\\)_Chinchilla scaling law_. 또 다른 대표적인 연구로서, Hoffmann 등[34](Google DeepMind team)은 LLMs에 대한 계산-최적의 훈련을 지시하기 위해 스케일링 법칙을 위한 대안적인 형태를 제안했다. 그들은 더 큰 범위의 모델 크기(70M에서 16B)와 데이터 크기(5B에서 500B 토큰)를 변경하여 엄격한 실험을 수행했으며 아래 [34]와 같이 계수가 다른 유사한 스케일링 법칙에 적합했다.\n' +
      '\n' +
      '\\[L(N,D)=E+\\frac{A}{N^{\\alpha}}+\\frac{B}{D^{\\beta}}, \\tag{2}\\]\n' +
      '\n' +
      '여기서 \\(E=1.69,A=406.4,B=410.7\\), \\(\\alpha=0.34\\), \\(\\beta=0.28\\). 제약조건 \\(C\\approx 6ND\\) 하에서 손실 \\(L(N,D)\\)을 최적화함으로써 모델 크기 및 데이터 크기에 대한 컴퓨팅 예산의 최적 할당이 다음과 같이 도출될 수 있음을 보여주었다.\n' +
      '\n' +
      '\\[N_{opt}(C)=G\\bigg{(}\\frac{C}{6}\\bigg{)}^{a},\\ \\ \\ D_{opt}(C)=G^{-1}\\bigg{(}\\frac{C}{6} \\bigg{)}^{b}, \\tag{3}\\]\n' +
      '\n' +
      '여기서 \\(a=\\frac{\\alpha}{\\alpha+\\beta}\\), \\(b=\\frac{\\beta}{\\alpha+\\beta}\\) 및 \\(G\\)는 \\(A\\), \\(B\\), \\(\\alpha\\) 및 \\(\\beta\\)에 의해 계산될 수 있는 스케일링 계수이다. [34]에서 분석한 바와 같이, 계산 예산의 증가를 감안할 때 KM 스케일링 법칙은 데이터 크기보다 모델 크기에서 더 큰 예산 할당을 선호하는 반면, Chinchilla 스케일링 법칙은 식 (3)의 \\(a\\) 및 \\(b\\)에 대해 유사한 값을 갖는 동일한 규모 _i.e._에서 두 크기를 증가시켜야 한다고 주장한다.\n' +
      '\n' +
      '**스케일링 규칙에 대한 논의**. 공식을 도입한 후 이해도를 높이기 위해 다음 두 가지 측면에서 스케일링 법칙에 대해 계속 논의한다.\n' +
      '\n' +
      '\\(\\bullet\\)_예측 가능한 크기 조정_입니다. 실제로 스케일링 법칙은 LLM의 훈련을 지시하는 데 사용될 수 있으며, _예측 가능한 스케일링_[46]이라고 하는 더 작은 모델의 성능을 기반으로 더 큰 모델의 성능을 안정적으로 추정하는 것이 실현 가능하다는 것이 입증되었다. LLM 훈련을 위한 예측 가능한 스케일링의 이점은 주로 두 가지이다. 첫째, 대형 모델의 경우 다양한 훈련 트릭이나 변형을 엄격하게 조사하는 것이 불가능하며, 소형 모델에서 얻은 경험이 대형 모델에도 적용될 수 있다면 매우 도움이 될 것이다. 예를 들어, 작은 프록시 모델들은 큰 모델들에 대한 데이터 혼합물의 최적 스케줄을 찾도록 훈련될 수 있다[59]. 둘째, 대규모 모델의 훈련은 종종 훈련 손실 스파이크와 같은 문제로 고통받는 오랜 시간이 소요되며, 스케일링 법칙을 사용하여 LLM의 훈련 상태를 모니터링하여 조기에 비정상 성능을 식별할 수 있다. 크기 조정 법칙은 성능 증가(또는 손실 감소)의 원활한 추세를 특징짓지만 _diminishing returns7_이 모델 크기 조정으로 발생할 수 있음을 나타냅니다. OpenAI 팀의 실증적 연구[58]는 표현 품질 또는 의미 콘텐츠가 수익 감소 지점(_i.e._, 기약 손실 접근)에 접근하더라도 여전히 효과적으로 개선될 수 있음을 보여주었다[58]. 이 발견은 대규모 모델을 훈련하는 것이 다운스트림 작업의 성능을 향상시키는 데 유망하다는 것을 시사한다. 스케일링 효과를 더 탐구하기 위해 잠재적인 문제는 LLM을 훈련하는 데 사용할 수 있는 데이터의 양이 실제로 제한적이라는 것이다. 계속 증가하는 모델 스케일로, 공공 텍스트 데이터는 LLMs[60]에 대해 곧 "소진"될 것이다. 따라서 데이터 반복 또는 증대가 데이터 부족을 완화하는 데 유용할 수 있는 데이터 제한 체제[61]에 스케일링 법칙이 어떻게 적용되는지 연구하는 것은 의미가 있을 것이다.\n' +
      '\n' +
      '각주 7: [https://en.wikipedia.org/wiki/Diminishing_returns](https://en.wikipedia.org/wiki/Diminishing_returns)\n' +
      '\n' +
      '\\(\\bullet\\)_Task-level predictability_. 스케일링 법칙에 대한 기존 연구는 대부분 언어 모델링 손실(_e.g._, nats [30]의 토큰당 교차 엔트로피 손실) 측면에서 수행되지만 실제로는 실제 작업에 대한 LLM의 성능에 더 관심이 있다. 따라서, 기본적인 문제는 언어 모델링 손실의 감소가 과제 수행의 향상으로 어떻게 변환되는가이다[58]. 직관적으로, 언어 모델링 손실이 더 작은 모델은 전체 모델 용량의 일반적인 척도로 간주될 수 있기 때문에 다운스트림 작업에서 더 나은 성능을 산출하는 경향이 있다. GPT-4[46]은 일부 능력들(_e.g._, 코딩 능력)이 스케일링 법칙을 통해 정확하게 예측될 수 있다고 보고했다. 그럼에도 불구하고, 독자들은 언어 모델링 손실의 직접적인 감소가 항상 다운스트림 작업에 대한 모델 성능의 향상을 나타내는 것은 아니라는 것을 인식해야 한다. 특히, _역스케일링_ 현상은 일부 태스크에 대해 발생할 것이며, 여기서 태스크 성능은 놀랍게도 언어 모델링 손실이 감소함에 따라 악화된다[62]. 전반적으로, 태스크-레벨 스케일링 법칙은 또한 태스크 관련 정보(태스크 메트릭, 태스크 난이도 등)에 의존할 수 있기 때문에 탐색하고 특성화하는 것이 더 어렵다. 더 나아가, 일부 용량(_e.g._, 인-컨텍스트 학습[55])은 스케일링 법칙에 따라 예측할 수 없으며, 이는 모델 크기가 특정 수준을 초과할 때(아래에서 논의되는 바와 같이)에만 관찰될 수 있다.\n' +
      '\n' +
      '**LLM의 최신 기능**. 문헌 [31]에서 LLM의 _에너지 능력_은 공식적으로 "작은 모델에는 존재하지 않지만 큰 모델에서는 발생하는 능력"으로 정의되며, 이는 LLM을 이전 PLM과 구별하는 가장 두드러진 특징 중 하나이다. 창발적 능력이 발생할 때 주목할 만한 특징을 더 도입한다[31]: 규모가 일정 수준에 도달하면 성능이 무작위보다 크게 상승한다. 유추하자면, 그러한 창발적 패턴은 물리학[31, 63]에서 _위상 전이_ 현상과 밀접한 관련이 있다. 창발 능력은 원칙적으로 일부 복잡한 과제와 관련하여 정의될 수 있는 반면[31, 64], 우리는 다양한 과제를 해결하기 위해 적용될 수 있는 일반적인 능력에 더 관심이 있다. 여기서는 LLM에 대한 세 가지 전형적인 비상 능력과 그러한 능력 8을 소유한 대표적인 모델을 간략하게 소개한다.\n' +
      '\n' +
      '각주 8: 다른 모델이나 작업에 따라 달라질 수 있기 때문에 LLM의 비상 능력에 대한 임계 크기(_i.e._, 능력을 보유하기 위한 최소 크기)를 정확하게 조사하는 것은 어렵다. 또한 기존 연구에서는 특정 LLM에 대해 매우 제한된 모델 크기에 대한 비상 능력을 테스트하는 경우가 많다. 예를 들어, PaLM은 종종 8B, 62B 및 540B의 세 가지 크기로 테스트된다. 테스트되지 않은 크기의 모델 성능에 대해서는 불분명하다.\n' +
      '\n' +
      '\\(\\bullet\\)_In-context learning_. ICL(in-context learning) 능력은 GPT-3[55]: 언어 모델이 자연어 명령 및/또는 여러 태스크 데모를 제공받았다고 가정하면, 추가적인 트레이닝 또는 그래디언트 업데이트9를 필요로 하지 않고, 입력 텍스트의 단어 시퀀스를 완료함으로써 테스트 인스턴스에 대한 예상 출력을 생성할 수 있다. GPT 시리즈 모델 중 175B GPT-3 모델은 일반적으로 강한 ICL 능력을 나타냈지만, GPT-1 및 GPT-2 모델은 그렇지 않았다. 그러한 능력은 또한 특정 다운스트림 작업에 의존한다. 예를 들어, ICL 능력은 13B GPT-3에 대한 산술 작업들(_e.g._, 3자리 덧셈 및 뺄셈) 상에서 나타날 수 있지만, 175B GPT-3은 페르시아 QA 작업에서도 잘 작동하지 않는다[31].\n' +
      '\n' +
      '각주 9: 최근 연구[65]에서도 맥락 내 학습이 주의 메커니즘을 통해 메타 최적화를 암묵적으로 수행한다는 것을 보여준다.\n' +
      '\n' +
      '\\(\\bullet\\)_Instruction following_. 자연어 설명들을 통해 포맷된 멀티-작업 데이터세트들의 혼합물로 미세 조정(명령어 튜닝_이라 함)함으로써, LLM들은 명령어의 형태로도 설명된 보이지 않는 작업들에 대해 잘 수행하는 것으로 도시된다[28, 66, 67]. 명령어 튜닝을 통해 LLM은 명시적인 예제를 사용하지 않고 새로운 작업에 대한 작업 지시를 따를 수 있게 되어 일반화 능력이 향상된다. [67]의 실험에 따르면, 명령어-튜닝된 LaMDA-PT [68]은 모델 크기가 68B에 도달했을 때 보이지 않는 태스크에서 튜닝되지 않은 태스크를 상당히 능가하기 시작했지만, 8B 또는 더 작은 모델 크기에서는 그렇지 않았다. 최근 연구[69]는 PaLM이 4개의 평가 벤치마크(_i.e._, MMLU, BBH, TyDiQA 및 MGSM)에서 다양한 작업에 대해 잘 수행하려면 적어도 62B의 모델 크기가 필요하다는 것을 발견했지만, 일부 특정 작업(_e.g._, MMLU)에는 훨씬 작은 크기가 충분할 수 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_Step-by-step reasoning_. 작은 언어 모델의 경우 일반적으로 여러 추론 단계, 예를 들어 수학 단어 문제를 포함하는 복잡한 작업을 해결하는 것이 어렵다. 대조적으로, CoT(chain-of-thought) 프롬프트 전략[33]으로, LLM은 최종 답변을 도출하기 위한 중간 추론 단계들을 수반하는 프롬프트 메커니즘을 활용함으로써 그러한 태스크들을 해결할 수 있다. 이 능력은 코드에 대한 훈련에 의해 잠재적으로 획득되는 것으로 추측된다[33, 47]. 경험적 연구 [33]은 CoT 프롬프트가 모델 크기가 60B보다 큰 PaLM 및 LaMDA 변형에 적용될 때 성능 향상(산술 추론 벤치마크에 대한)을 가져올 수 있는 반면 모델 크기가 100B를 초과할 때 표준 프롬프트에 대한 이점이 더 분명해지는 것으로 나타났다. 또한 CoT 프롬프팅에 의한 성능 향상은 PaLM [33]에 대한 GSM8K \\(>\\) MAWPS \\(>\\) SWAMP와 같은 다른 작업에 대해서도 달라지는 것으로 보인다.\n' +
      '\n' +
      '**출현 기능이 크기 조정 규칙과 관련 되는 방법**. 기존 문헌[30, 31, 34]에서 스케일링 법칙과 창발 능력은 작은 모델보다 큰 모델의 이점을 이해하는 두 가지 관점을 제공한다. 일반적으로 스케일링 법칙(종종 _언어 모델링 손실_로 측정됨)은 수익 감소의 잠재적 효과와 예측 가능한 성능 관계를 설명하는 반면, 비상 능력(종종 _작업 성능_으로 측정됨)은 예측할 수 없지만 그러한 능력이 실제로 나타나면 매우 수익성이 있다. 두 관점은 서로 다른 성능 추세(지속적인 개선 _v.s._ 급격한 성능 도약)를 반영하므로 잘못된 결과 또는 관찰로 이어질 수 있습니다. 창발적 능력의 합리성에 대한 광범위한 논쟁도 있다. 일반적인 추측은 비상 능력이 특수 작업에 대한 평가 설정(예: 불연속 평가 메트릭)에 부분적으로 기인할 수 있다는 것입니다. [70, 71]: 평가 메트릭이 적절하게 변경되면 비상 능력 곡선의 선명도가 사라집니다. 그러나 대부분의 태스크에 대한 LLM의 성능은 사용자에게 불연속적인 방식으로 자연스럽게 인식된다. 예를 들어, 최종 사용자는 테스트 케이스를 성공적으로 통과할 수 있는 LLM에 의해 생성된 신뢰할 수 있는 코드를 선호하지만 실패한 두 코드 사이에 오류가 적은 더 나은 코드를 선택하는 데 관심이 적다. 보다 최근에, 연구[72]는 태스크 메트릭의 해상도를 확대하여 태스크 성능을 보다 예측 가능하게 할 수 있는 새로운 평가 설정을 제안한다. 이러한 노력에도 불구하고 LLM의 작동 메커니즘에 대한 보다 근본적인 연구(예: grokking10)는 여전히 특정 능력의 출현을 이해해야 한다. 스케일링 법칙과 창발 능력 사이의 미묘한 관계는 인간 11의 능력 습득과 유사하게 설명할 수 있다. 말하기 능력을 예로 들어보자. 아동의 경우 언어발달(특히 유아)도 \'출현적 능력\'이 발생하는 다단계 과정으로 볼 수 있다. 특히, 언어 능력은 시간 간격 내에서 비교적 안정적일 것이지만, 질적인 변화는 다른 능력 레벨(예를 들어, 간단한 단어를 말하는 것부터 간단한 문장을 말하는 것)로 진화할 때만 발생한다. 이러한 학습 과정은 본질적으로 _smooth_ 및 _stable_(즉, 언어 능력은 시간이 지남에 따라 일정한 속도로 발달하지 않음)가 아니지만, 실제로 어린이는 매일 성장한다. 어린 부모들은 아기들이 보여주는 말하기 능력의 예상치 못한 진전에 종종 놀랄 것이라는 것이 흥미롭다.\n' +
      '\n' +
      '각주 10: Grokking은 원래 논문[73]에서 인용한 "데이터의 패턴, 무작위 우연 수준에서 완벽한 일반화로 일반화 성능을 향상시키는 것"을 말한다.\n' +
      '\n' +
      '각주 11: 이 설명은 이해의 편의를 위한 것일 뿐 두 점을 연결시킬 직접적인 증거는 없다.\n' +
      '\n' +
      '**LLM에 대 한 주요 기술**. LLM이 현재 상태: _일반_ 및 _가능_ 학습자로 진화하는 것은 먼 길이었다. 개발 과정에서 LLM의 용량을 크게 향상시키는 여러 가지 중요한 기술이 제안된다. 여기서, (잠재적으로) LLM의 성공으로 이어지는 몇 가지 중요한 기술을 간단히 열거하면 다음과 같다.\n' +
      '\n' +
      '\\(\\bullet\\)_Scaling_입니다. 이전 부분들에서 논의된 바와 같이, 트랜스포머 언어 모델들에서 명백한 스케일링 효과가 존재한다: 더 큰 모델/데이터 크기들 및 더 많은 트레이닝 컴퓨트들은 전형적으로 개선된 모델 용량으로 이어진다[30, 34]. 두 가지 대표적인 모델로서 GPT-3과 PaLM은 모델 크기를 각각 175B와 540B로 증가시켜 스케일링 한계를 탐구했다. 컴퓨팅 예산은 일반적으로 제한되기 때문에, 스케일링 법칙들은 컴퓨팅 리소스들의 더 컴퓨팅-효율적인 할당을 수행하기 위해 추가로 채용될 수 있다. 예를 들어, Chinchilla(더 많은 트레이닝 토큰을 갖는)는 동일한 컴퓨트 예산으로 데이터 스케일을 증가시킴으로써 상대 모델 Gopher(더 큰 모델 크기를 갖는)보다 성능이 우수하다[34]. 또한, 사전 훈련 데이터의 품질이 모델 용량에 중요한 역할을 하기 때문에 데이터 스케일링은 세심한 청소 프로세스를 수행해야 한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Training_입니다. 모델 크기가 크기 때문에 유능한 LLM을 성공적으로 훈련하는 것은 매우 어렵다. 다양한 병렬 전략이 공동으로 활용되는 LLM의 네트워크 파라미터를 학습하기 위해서는 분산 학습 알고리즘이 필요하다. 분산 트레이닝을 지원하기 위해, DeepSpeed[74] 및 Megatron-LM[75, 76, 77]과 같은 병렬 알고리즘의 구현 및 배치를 용이하게 하기 위해 여러 최적화 프레임워크가 출시되었다. 또한, 최적화 트릭은 트레이닝 안정성 및 모델 성능, 예를 들어, 트레이닝 손실 스파이크[56] 및 혼합 정밀 트레이닝[78]을 극복하기 위한 재시작에도 중요하다. 보다 최근에, GPT-4[46]는 훨씬 더 작은 모델들을 갖는 큰 모델들의 성능을 신뢰성 있게 예측하는 특별한 인프라 및 최적화 방법들을 개발할 것을 제안한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Ability eliciting_. 대규모 말뭉치에서 사전 훈련된 후 LLM은 범용 작업 해결사로서 잠재적인 능력을 부여받는다. LLM이 특정 작업을 수행할 때 이러한 능력을 명시적으로 나타내지 않을 수 있다. 기술적 접근으로는 그러한 능력을 이끌어내기 위해 적절한 과제 지시나 특정 맥락 내 학습 전략을 설계하는 것이 유용하다. 예를 들어, 연쇄 사고 프롬프트는 중간 추론 단계를 포함하여 복잡한 추론 작업을 해결하는 데 유용한 것으로 나타났다. 또한, 보이지 않는 작업에 대한 LLM의 일반화 가능성을 향상시키기 위해 자연어로 표현된 작업 설명으로 LLM에 대한 명령어 튜닝을 수행할 수 있다. 이러한 유도 기술은 주로 LLM의 창발적 능력에 해당하며, 이는 작은 언어 모델에 동일한 효과를 나타내지 않을 수 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_Alignment tuning_. LLM은 사전 훈련 말뭉치(고품질 및 저품질 데이터를 모두 포함)의 데이터 특성을 캡처하도록 훈련되기 때문에 인간에게 독성, 편향 또는 심지어 유해한 콘텐츠를 생성할 가능성이 있다. LLM을 인간 값(예: 도움됨, 정직 및 무해함)과 일치시켜야 합니다. 이를 위해, InstructGPT[66]는 LLM들이 예상된 지시들을 따를 수 있게 하는 효과적인 튜닝 접근법을 설계하는데, 이는 _인간 피드백을 갖는 강화 학습_[66, 79]의 기술을 활용한다. 정교하게 설계된 라벨링 전략으로 훈련 루프에 사람을 통합합니다. ChatGPT는 실제로 InstructGPT와 유사한 기술로 개발되었으며, 이는 모욕적인 질문에 답하기 위해 거부하고 고품질 무해한 응답을 생성하는 데 강력한 정렬 능력을 보여준다.\n' +
      '\n' +
      '\\(\\bullet\\)_도구 조작_입니다. 본질적으로, LLM은 방대한 평문 말뭉치 위에 텍스트 생성기로서 트레이닝되며, 따라서 텍스트의 형태로 가장 잘 표현되지 않는 태스크들(_e.g._, 수치 계산)에 대해 덜 잘 수행한다. 또한, 그들의 용량은 또한 사전 트레이닝 데이터, _예를 들어_ 최신 정보를 캡처할 수 없는 것으로 제한된다. 이러한 문제를 해결하기 위해 최근에 제안된 기술은 LLM의 결함을 보완하기 위해 외부 도구를 사용하는 것이다[80, 81]. 예를 들어, LLM은 정확한 계산을 위해 계산기를 활용할 수 있고[80], 알려지지 않은 정보를 검색하기 위해 검색 엔진을 채용할 수 있다[81]. 보다 최근에 ChatGPT는 LLM의 _"눈 및 안"_과 유사한 외부 플러그인(기존 또는 새로 생성된 앱)12를 사용하는 메커니즘을 가능하게 했다. 이러한 메커니즘은 LLM의 용량 범위를 광범위하게 확장할 수 있다.\n' +
      '\n' +
      '각주 12: [https://openai.com/blog/chatgpt-plugins](https://openai.com/blog/chatgpt-plugins)\n' +
      '\n' +
      '또한, 많은 다른 요소들(_e.g._, 하드웨어의 업그레이드)도 LLM의 성공에 기여한다. 현재, 우리는 LLM을 개발하기 위한 주요 기술적 접근법과 주요 발견으로 논의를 제한한다.\n' +
      '\n' +
      '### _GPT 시리즈 Models_의 기술 진화_\n' +
      '\n' +
      '챗GPT는 인간과 소통하는 능력이 뛰어나 출시 이후 AI 커뮤니티의 설렘에 불을 붙였다. ChatGPT는 특별히 최적화된 대화 용량을 가진 강력한 GPT 모델을 기반으로 개발되었다. 챗GPT 및 GPT 모델에 대한 관심이 지속적으로 증가하고 있는 점을 고려하여 GPT 시리즈 모델의 기술적 진화에 대한 특별한 논의를 추가하여 지난 몇 년 동안 어떻게 발전했는지 간략하게 요약한다. 한편, 우리는 그림 4에서 GPT 시리즈 모델의 기술적 진화를 묘사하는 개략도를 그렸다. GPT 모델의 기본 원리는 언어 모델링에 의해 세계 지식을 디코더 전용 트랜스포머 모델로 압축하여 세계 지식의 의미를 복구(또는 암기)하고 범용 태스크 해결기의 역할을 하는 것이다. 성공의 두 가지 핵심은 (I) 다음 단어를 정확하게 예측할 수 있는 디코더 전용 트랜스포머 언어 모델 훈련과 (II) 언어 모델의 크기를 스케일업하는 것이다. 종합적으로 LLM에 대한 OpenAI의 연구는 다음과 같은 13단계로 대별할 수 있다.\n' +
      '\n' +
      '각주 13: 이 부분에 대한 논의는 다소 주관적일 수 있다는 점에 유의한다. 전체적인 관점과 요약은 OpenAI가 발표한 논문, 블로그 기사, 인터뷰 보고서 및 API를 읽음으로써 설문 작성자의 이해를 바탕으로 이루어진다.\n' +
      '\n' +
      '**초기 탐색** 입니다. 일리야 서츠케버14(오픈아이의 공동 설립자이자 수석 과학자)와의 한 인터뷰에 따르면, 지능 시스템을 언어 모델로 접근하는 아이디어는 오픈아이 초기 이미 탐구된 반면, 순환 신경망(RNN)(121)으로 시도되었다. 트랜스포머의 등장으로 OpenAI는 두 개의 초기 GPT 모델, 즉 GPT-1 [122] 및 GPT-2 [26]을 개발했으며, 이는 후속적으로 더 강력한 모델 _i.e._, GPT-3 및 GPT-4의 기반으로 간주될 수 있다.\n' +
      '\n' +
      '각주 14: [https://hackernoon.com/an-interview-with-ilya-sutskever-co-founder-of-openai](https://hackernoon.com/an-interview-with-ilya-sutskever-co-founder-of-openai)\n' +
      '\n' +
      '\\(\\bullet\\)_GPT-1_. 2017년 구글에 의해 트랜스포머 모델[22]이 도입되었고 OpenAI 팀은 언어 모델링 작업을 이 새로운 신경망 아키텍처에 빠르게 적응시켰다. 그들은 2018년에 첫 번째 GPT 모델인 _i.e._, GPT-1[122]을 출시하고 _생성 사전 훈련_을 대신하여 약어 _GPT_를 모델 이름으로 만들었다. GPT-1은 생성 디코더 전용 트랜스포머 아키텍처를 기반으로 개발되었으며, 비지도 사전 훈련과 감독된 미세 조정의 하이브리드 접근법을 채택했다. GPT-1은 GPT 시리즈 모델의 핵심 아키텍처를 설정하고 다음 단어를 예측하는 자연어 텍스트 _i.e._를 모델링하기 위한 기본 원리를 확립했다.\n' +
      '\n' +
      '\\(\\bullet\\)_GPT-2_. GPT-1의 유사한 아키텍처에 이어, GPT-2[26]은 파라미터 스케일을 1.5B로 증가시켰으며, 이는 큰 웹페이지 데이터세트 WebText로 트레이닝되었다. GPT-2의 논문에서 주장한 바와 같이 라벨링된 데이터를 사용하여 명시적인 미세 조정 없이 감독되지 않은 언어 모델링을 통해 작업을 수행하려고 했다. 접근에 대한 동기를 부여하기 위해, 그들은 다중 태스크 해결을 위한 확률적 형태인 _i.e._, \\(p(output|input,task)\\)([123]에서 유사한 접근 방식이 채택됨)을 도입했으며, 이는 입력 및 태스크 정보에 조건화된 출력을 예측한다. 이 조건부 확률을 모델링하기 위해 언어 텍스트는 입력, 출력 및 작업 정보의 형식을 지정하는 통합 방식으로 자연스럽게 사용될 수 있다. 이와 같이 과제를 해결하는 과정을 풀이 텍스트를 생성하기 위한 단어 예측 문제로 캐스팅할 수 있다. 또한, 이들은 이 아이디어에 대한 보다 공식적인 주장을 소개하였다 : "(태스크-특정) 감독된 목적은 비감독된 (언어 모델링) 목적과 동일하지만 시퀀스의 서브세트에 대해서만 평가되기 때문에, 비감독된 목적 중 전역 최소값은 또한 (다양한 태스크에 대해) 감독된 목적 중 전역 최소값이다."[26]15. 이 주장에 대한 기본적인 이해는 각각의 (NLP) 태스크가 세계 텍스트의 서브세트에 기초하여 단어 예측 문제로서 고려될 수 있다는 것이다. 따라서, 비지도 언어 모델링은 세계 텍스트를 복구하는 데 충분한 능력을 갖도록 훈련된다면 다양한 작업을 해결할 수 있다. GPT-2 논문의 이러한 초기 논의는 Jensen Huang의 일리야 Sutskever의 인터뷰에서 반향을 불러일으켰다: "신경망이 학습하는 것은 텍스트를 생성한 프로세스의 일부 표현이다. 이 텍스트는 실제로 세계의 투영이다. 다음 단어를 더 정확하게 예측할수록 충실도가 높아지고 이 프로세스에서 더 많은 해상도를 얻을 수 있다..."16.\n' +
      '\n' +
      '각주 15: 이 문장을 더 잘 이해하기 위해 괄호 안에 몇 가지 설명 단어를 넣는다.\n' +
      '\n' +
      '**용량 증가**. GPT-2는 "감독되지 않은 멀티태스킹 학습자"로 의도되어 있지만, 감독된 미세 조정 최신 방법에 비해 전반적으로 성능이 떨어진다. 모델 크기가 상대적으로 작기 때문에 다운스트림 작업, 특히 대화 작업[124, 125]에서 광범위하게 미세 조정되었다. GPT-2를 기준으로, GPT-3\n' +
      '\n' +
      '(거의 동일한) 생성 사전-훈련 아키텍처의 스케일링에 의해 핵심 용량 도약을 입증한다.\n' +
      '\n' +
      '\\(\\bullet\\)_GPT-3_. GPT-3 [55]는 2020년에 출시되었으며, 이는 모델 매개변수를 175B의 더 큰 크기로 확장했다. 본 논문에서는 GPT-3의 논문에 LLM을 활용하여 자연어 텍스트 형태로 작업을 이해할 수 있도록 하는 인-컨텍스트 학습(in-context learning, ICL)17의 개념을 공식적으로 소개하였다. ICL은 LLM의 사전 학습과 활용은 동일한 언어 모델링 패러다임으로 수렴한다. 사전 훈련은 문맥을 조건으로 하는 다음 텍스트 시퀀스를 예측하는 반면, ICL은 태스크 설명과 시연을 고려할 때 텍스트 시퀀스로 형식화될 수 있는 올바른 태스크 솔루션을 예측한다. GPT-3은 다양한 NLP 태스크에서 매우 우수한 성능을 보일 뿐만 아니라 추론이나 도메인 적응 능력을 필요로 하는 여러 특수 설계된 태스크에서도 매우 우수한 성능을 보인다. GPT-3의 논문은 LLM의 창발적 능력에 대해 명시적으로 논의하지는 않지만 기본 스케일링 법칙을 초월할 수 있는 큰 성능 비약을 관찰할 수 있다[30]. 전반적으로 GPT-3는 PLM에서 LLM으로 진화하는 여정에서 주목할 만한 랜드마크로 볼 수 있다. 신경망을 상당한 크기로 스케일링하면 모델 용량이 크게 증가할 수 있음을 경험적으로 입증했다.\n' +
      '\n' +
      '각주 17: GPT-2는 비지도 과제 학습을 위해 ICL을 기본적으로 사용했지만 당시에는 ICL이라고 불리지 않았다.\n' +
      '\n' +
      '**용량 향상** 강력한 용량으로 인해 GPT-3는 훨씬 더 능력 있는 개발을 위한 기본 모델이었습니다.\n' +
      '\n' +
      '도. 4: GPT-시리즈 모델의 기술적 진화를 위한 간략한 예시. 우리는 주로 OpenAI의 논문, 블로그 기사 및 공식 API를 기반으로 이 그림을 그린다. 여기서, _실선_은 두 모델 사이의 진화 경로에 대한 명시적 증거(_예를 들어,_ 기본 모델을 기반으로 새로운 모델이 개발된다는 공식 진술)가 존재하는 반면, _점선_은 상대적으로 약한 진화 관계를 나타낸다.\n' +
      '\n' +
      '도. 3: 최근 몇 년 동안(10B보다 큰 크기를 갖는) 기존의 대형 언어 모델들의 타임라인. 타임라인은 모델에 대한 기술 논문의 출시 날짜(_예: arXiv로의 제출 날짜)에 따라 주로 설정되었다. 해당 논문이 없는 경우 모델의 날짜를 공개 또는 발표의 가장 빠른 시간으로 설정했다. LLM에 공개적으로 사용 가능한 모델 체크포인트를 노란색으로 표시합니다. 그림의 공간 한계로 인해 공개적으로 보고된 평가 결과가 있는 LLM만 포함한다.\n' +
      '\n' +
      '오픈아이의 LMs. 전반적으로 OpenAI는 GPT-3 모델을 더욱 개선하기 위한 두 가지 주요 접근법인 _i.e._, 코드 데이터에 대한 훈련 및 인간 선호도와의 정렬을 탐구했으며, 이는 다음과 같다.\n' +
      '\n' +
      '\\(\\bullet\\)_Training on code data.__ 원래의 GPT-3 모델(평문 상에서 사전 트레이닝된)의 주요 한계는 복잡한 태스크들, _예를 들어_에 대한 추론 능력 부족, 코드 완성 및 수학 문제 해결에 있다. 이 능력을 높이기 위해 코덱스[105]는 2021년 7월 오픈에이이에 의해 도입되었으며, 이는 GitHub 코드의 대규모 코퍼스에 미세 조정된 GPT 모델이었다. 코덱스는 매우 어려운 프로그래밍 문제를 해결할 수 있고 수학 문제를 해결하는 데 상당한 성능 향상으로 이어질 수 있음을 입증했다[126]. 또한, 훈련 텍스트 및 코드 임베딩에 대한 대조적 접근[127]이 2022년 1월에 보고되었으며, 이는 일련의 관련 작업(_i.e._, 선형 프로브 분류, 텍스트 검색 및 코드 검색)을 개선하는 것으로 나타났다. 실제로 GPT-3.5 모델은 코드 기반 GPT 모델(_i.e._, code-davinci-002)을 기반으로 개발되었으며, 이는 코드 데이터에 대한 학습이 GPT 모델의 모델 용량, 특히 추론 능력을 향상시키는 데 매우 유용한 연습임을 나타낸다. 또한 코드 데이터에 대한 훈련은 LLM[47]의 연쇄 사고 능력을 크게 증가시킬 수 있지만 더 철저한 검증으로 더 조사할 가치가 있다는 추측도 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_Human alignment_. 인간 정렬에 대한 관련 연구는 OpenAI에 대해 2017년(또는 이전)으로 거슬러 올라갈 수 있다: "인간 선호도로부터 학습"18이라는 제목의 블로그 기사가 OpenAI 블로그에 게시되어 인간에 의해 주석이 달린 _선호도 비교_로부터 학습하기 위해 강화 학습(RL)을 적용한 작업을 기술했다[79]. (도 12의 InstructGPT의 정렬 알고리즘에서의 _보상 훈련_ 단계와 유사하다). 이 RL 논문[79]이 발표된 직후, 2017년 7월에 Proximal Policy Optimization(PPO)[128] 논문이 발표되었으며, 이는 현재 인간의 선호로부터 학습을 위한 기초 RL 알고리즘이다[66]. 이후 2020년 1월, GPT-2는 앞서 언급한 RL 알고리즘[79, 128]을 사용하여 미세 조정되었으며, 이는 NLP 작업에서 GPT-2의 용량을 개선하기 위해 인간의 선호도를 활용했다. 같은 해에, 다른 작업[129]은 유사한 방식으로 인간의 선호도를 최적화하기 위한 요약 모델을 훈련시켰다. 이러한 선행 연구를 바탕으로 2022년 1월 인간 정렬을 위한 GPT-3 모델을 개선하기 위해 InstructGPT[66]이 제안되었으며, 이는 공식적으로 인간 피드백으로부터 3단계 강화 학습(RLHF) 알고리즘을 확립했다. "_명령 튜닝_"의 작업은 OpenAI의 논문 및 문서에서 거의 사용되지 않은 것으로 보이며, 이는 _인간 시연에 대한 감독된 미세 조정_(_i.e._, RLHF 알고리즘 [66])의 첫 번째 단계로 대체된다. 명령어 추종 능력을 향상시키는 것 외에도, RLHF 알고리즘은 LLM에 대한 유해 또는 독성 콘텐츠를 생성하는 문제를 완화하는데 특히 유용하며, 이는 실제로 LLM의 안전한 배치의 핵심이다. OpenAI는 정렬 연구에 대한 그들의 접근법을 기술 기사 [130]에서 설명하는데, 이는 "인간 피드백을 사용하고 인간 평가를 지원하고 정렬 연구를 수행하기 위해 AI 시스템을 훈련"이라는 세 가지 유망한 방향을 요약했다.\n' +
      '\n' +
      '각주 18: [https://openai.com/research/learning-from-human-preferences](https://openai.com/research/learning-from-human-preferences)\n' +
      '\n' +
      '이러한 향상 기술은 더 강한 용량을 갖는 개선된 GPT-3 모델로 이어지며, 이를 OpenAI에 의한 GPT-3.5 모델이라고 한다(섹션 3.1의 OpenAI API에 대한 논의를 참조).\n' +
      '\n' +
      '**언어 모델의 마일스톤** 입니다. 모든 탐사 노력을 바탕으로 OpenAI에 의해 두 가지 주요 이정표, 즉 ChatGPT[131]와 GPT-4[46]가 달성되었으며, 이는 기존 AI 시스템의 용량 바를 크게 높였다.\n' +
      '\n' +
      '\\(\\bullet\\)_ChatGPT_. 오픈AI는 2022년 11월 GPT 모델(GPT-3.5, GPT-4)을 기반으로 대화 모델 ChatGPT를 출시했다. 공식 블로그 기사 [131]이 소개한 바와 같이 ChatGPT는 대화에 특별히 최적화되면서 InstructGPT(원래 게시글에서 "InstructGPT에 형제 모델"이라고 함)와 유사한 방식으로 훈련되었다. 그들은 데이터 수집 설정에서 ChatGPT와 InstructGPT의 훈련 사이의 차이를 보고했다: 인간 생성 대화(사용자 및 AI의 역할을 모두 수행하는)는 ChatGPT 훈련을 위한 대화 형식으로 InstructGPT 데이터 세트와 결합된다. ChatGPT는 방대한 지식의 저장소 보유, 수학적 문제에 대한 추론 기술, 멀티턴 대화에서 맥락을 정확하게 추적하고 안전한 사용을 위해 인간의 가치와 잘 일치시키는 우수한 능력을 보였다. 나중에 플러그인 메커니즘은 ChatGPT에서 지원되었으며, 이는 기존 도구 또는 앱으로 ChatGPT의 용량을 더욱 확장한다. 지금까지 AI 역사상 가장 강력한 챗봇인 것 같습니다. 챗GPT의 출시는 인간과 유사한 AI 시스템의 탐구를 조명하는 향후 AI 연구에 큰 영향을 미친다.\n' +
      '\n' +
      '\\(\\bullet\\)_GPT-4_. 또 다른 괄목할 만한 진전으로 2023년 3월 GPT-4[46]가 출시되어 텍스트 입력을 멀티모달 신호로 확장하였다. 전반적으로 GPT-4는 GPT-3.5보다 복잡한 과제 해결 능력이 강하여 많은 평가 과제에서 큰 성능 향상을 보였다. 최근 연구[41]는 인간이 생성한 문제에 대해 다양한 범위의 어려운 작업에 걸쳐 정성적 테스트를 수행하여 GPT-4의 능력을 조사했으며 GPT-4가 ChatGPT와 같은 이전 GPT 모델보다 더 우수한 성능을 달성할 수 있음을 보여주었다. 또한, GPT-4는 6개월 반복 정렬(RLHF 훈련에서 추가 안전 보상 신호 포함)로 인해 악의적이거나 자극적인 쿼리에 보다 안전하게 응답한다. 기술 보고서에서 OpenAI는 GPT-4를 안전하게 개발하는 방법을 강조했으며 환각, 개인 정보 보호 및 과잉 의존과 같은 LLM의 가능한 문제를 완화하기 위해 여러 개입 전략을 적용했다. 예를 들어, 그들은 유해 또는 독성 함량 생성을 줄이기 위해 _적색 학습_[132]라는 메커니즘을 도입했다. 또 다른 중요한 측면으로 GPT-4는 개선된 최적화 방법을 사용하여 잘 확립된 딥 러닝 인프라에서 개발되었다. 그들은 모델 훈련 동안 작은 비율의 계산으로 최종 성능을 정확하게 예측할 수 있는 _예측 가능한 스케일링_이라는 새로운 메커니즘을 도입했다.\n' +
      '\n' +
      '\\(\\bullet\\)_GPT-4V, GPT-4 터보, beyond_. GPT-4[46]에 대해 수행된 작업을 기반으로, OpenAI는 GPT-4의 비전 능력의 안전한 배치에 초점을 맞춘 GPT-4V를 2023년 9월에 추가로 출시하였다. GPT-4V의 시스템 카드[133]에서는 시각적으로 증강된 입력과 관련된 위험의 평가 및 완화에 대해 광범위하게 논의하였다. 특히 GPT-4V는 다양한 응용 시나리오에서 강력한 비전 능력을 나타내어 강력한 멀티모달 학습 시스템으로서의 큰 잠재력을 보여주었다. 보다 최근에, 2023년 11월, 오픈AI는 일련의 기술적 개선과 함께 _GPT-4 터보_라는 이름의 DevDay에서 업그레이드된 세대의 GPT-4 모델을 출시했다. GPT-4 Turbo는 개선된 모델 용량(GPT-4보다 더 가능), 확장된 지식 소스(2023년 4월까지), 긴 컨텍스트 윈도우(최대 128k 토큰), 최적화된 모델 성능(저렴한 가격), 및 기타 유용한 기능 업데이트(기능 호출, 재현 가능한 출력 등)에 의해 특징지어진다. 동시에, 어시스턴트 API는 에이전트-유사 어시스턴트의 신속한 개발을 용이하게 하기 위해 개시되었다. 이 API를 사용하여 개발자는 특정 명령, 추가 지식 및 도구 사용을 활용하여 애플리케이션 내에서 목표 지향 비서를 쉽게 만들 수 있습니다. 또한, GPT-4 Turbo with vision, DALL-E 3, Text-to-speech (TTS) 및 Listen to voice 샘플로 지원되는 이 새로운 릴리스에서도 멀티모달 용량(참조, 듣기 및 말하기)이 향상되었다. 이러한 개선은 용량 범위를 크게 확장하고 GPT 모델의 작업 성능을 향상시켰다. 더 중요한 것은 개선된 모델, API 및 기능에서 기술 업그레이드로 애플리케이션 생태계가 크게 강화된다는 것이다.\n' +
      '\n' +
      '엄청난 발전에도 불구하고, 이러한 우수한 LLM에는 여전히 한계가 있으며, 예를 들어, 일부 특정 컨텍스트 내에서 사실적 오류 또는 잠재적으로 위험한 응답을 갖는 환각을 생성한다[46]. LLM의 더 많은 한계 또는 문제는 섹션 7에서 논의될 것이다. 더 유능하고 안전한 LLM을 개발하기 위한 오랜 연구 과제를 제기한다. 엔지니어링 관점에서 OpenAI는 5단계 개발 및 배포 라이프 사이클을 따라 모델과 제품을 개발하는 반복적인 배포 전략[134]을 채택했으며, 이는 모델 사용의 잠재적 위험을 효과적으로 줄이는 것을 목표로 한다. 이하에서는 그들이 어떻게 개발되었는지에 대한 구체적인 이해를 돕기 위해 기술적 세부 사항에 대해 살펴보기로 한다.\n' +
      '\n' +
      '## 3 Resources of LLMs\n' +
      '\n' +
      '어려운 기술적 문제와 막대한 계산 자원 수요를 고려할 때 LLM을 개발하거나 재현하는 것은 결코 쉬운 일이 아니다. 실행 가능한 방법은 기존 LLM에서 경험을 배우고 점진적 개발 또는 실험 연구를 위해 공개적으로 사용 가능한 리소스를 재사용하는 것이다. 이 섹션에서는 모델 체크포인트(또는 API), 말뭉치 및 라이브러리를 포함하여 LLM 개발을 위해 공개적으로 사용할 수 있는 리소스를 간략하게 요약한다.\n' +
      '\n' +
      '### _Publicly Available Model Checkpoints or APIs_\n' +
      '\n' +
      '모델 사전 훈련의 막대한 비용을 감안할 때 잘 훈련된 모델 체크포인트는 연구 커뮤니티를 위한 LLM의 연구 및 개발에 매우 중요하다. 파라미터 스케일은 LLM을 사용하기 위해 고려해야 할 핵심 요소이기 때문에, 우리는 이러한 공개 모델을 두 가지 스케일 레벨(즉, 수백억 개의 파라미터_ 및 수천억 개의 파라미터_)로 분류하며, 이는 사용자가 리소스 예산에 따라 적합한 리소스를 식별하는 데 유용하다. 또한 추론을 위해 모델을 로컬로 실행하지 않고 공개 API를 직접 사용하여 작업을 수행할 수 있습니다. 다음으로 공개적으로 사용 가능한 모델 체크포인트 및 API를 소개합니다.\n' +
      '\n' +
      '**매개 변수가 수십 개 있는 모델** 입니다. 이 범주의 대부분의 모델은 LLaMA[57] 및 LLaMA2[99](최대 버전에서 70B 매개변수 포함), NLLB[91](최대 버전에서 54.5B 매개변수 포함), 팔콘[135](최대 버전에서 40B 매개변수 포함)를 제외하고 10B에서 20B 범위의 매개변수 척도를 가지고 있다. 이 범위 내의 다른 모델로는 mT5[83], PanGu-\\(\\alpha\\)[84], T0[28], GPT-NeoX-20B[87], CodeGen[86], UL2[89], Flan-T5[69], 및 mT0[94]가 있다. 이 중 Flan-T5(11B 버전)는 3가지 측면에서 명령어 튜닝을 탐구하기 때문에 명령어 튜닝 연구의 기본 모델 역할을 할 수 있다[69]: 태스크 수 증가, 모델 크기 조정, 체인 오브 생각 프롬프트 데이터로 미세 조정. 또한 코드 생성을 위해 설계된 자기회귀 언어 모델인 CodeGen(11B 버전)은 코드 생성 능력을 탐색하기 위한 좋은 후보로 간주될 수 있다. 또한 115개의 전문가 생성 문제로 구성된 멀티턴 프로그램 합성을 위한 새로운 벤치마크 MTPB[86]를 소개한다. 이러한 문제를 해결하기 위해 LLM은 충분한 프로그래밍 지식(예: 수학, 배열 연산 및 알고리즘)을 획득해야 한다. 보다 최근에, 모델 아키텍처, 학습 알고리즘 및 데이터 분포에서의 선택이 모델에 미치는 영향을 탐구하기 위해 CodeGen2[97]가 출시되었다. 코딩 능력에 특화된 또 다른 LLM으로서 스타코더[98]도 우수한 성과를 거두었다. 다국어 태스크의 경우, mT0(13B 버전)은 다국어 프롬프트가 있는 다국어 태스크에서 미세 조정된 좋은 후보 모델이 될 수 있다. 또한, PanGu-\\(\\alpha\\)[84]는 딥 러닝 프레임워크 마인드스포어(MindSpore, 136)를 기반으로 개발된 제로샷 또는 소샷 설정에서 중국 다운스트림 태스크에서 우수한 성능을 보인다. PanGu-\\(\\alpha\\)[84]는 여러 버전의 모델(최대 200B 매개변수)을 보유하고 있는 반면, 가장 큰 공개 버전에는 13B 매개변수가 있다. 일반적인 LLM으로서, 다른 모델보다 약 5배 많은 파라미터를 포함하는 LLaMA(65B 버전)[57]는 명령어 추종 관련 태스크에서 우수한 성능을 보였다. LLaMA에 비해 LLaMA2 [99]는 인간 피드백(RLHF)으로부터 강화 학습에서 더 많은 탐구를 하고 _LLaMA-chat_라는 채팅 지향 버전을 개발했으며, 이는 일반적으로 다양한 유용성과 안전 벤치마크에 걸쳐 기존 오픈 소스 모델보다 성능이 우수하다. 개방성과 효과성으로 인해 LLaMA는 연구 커뮤니티의 상당한 관심을 받았으며 많은 노력[137, 138, 139, 140]이 새로운 모델 또는 도구를 구현하기 위해 다양한 모델 버전을 미세 조정하거나 지속적으로 사전 훈련하는 데 전념했다. 최근에는 또 다른 오픈 소스 LLM인 Falcon [135]도 오픈 벤치마크에서 매우 우수한 성능을 달성했다. 사전 학습 데이터(공개 공유 데이터 세트 _RefinedWeb [141]_ 포함)를 준비하기 위해 보다 세심한 데이터 정리 프로세스가 특징입니다. 일반적으로 이 규모의 사전 훈련 모델은 수백 개 또는 수천 개의 GPU 또는 TPU를 필요로 한다. 예를 들어, GPT-NeoX-20B는 각각 8개의 NVIDIA A100-SXM4-40GB GPU가 장착된 12개의 초마이크로 서버를 사용하는 반면 LLaMA는 원래 간행물에 보고된 대로 2,048개의 A100-80G GPU를 사용한다. 필요한 계산 자원을 정확하게 추정하기 위해, _FLOPS_(_i.,_ FLoating point number Operations Per Second)[30]와 같은 관련된 계산의 수를 측정하는 메트릭을 사용하는 것이 제안된다.\n' +
      '\n' +
      '**수백 개의 매개 변수가 있는 모델** 입니다. 이 범주에 속하는 모델의 경우 소수의 모델만 공개적으로 공개되었다. 예를 들어, OPT[90], OPT-IML[95], BLOOM[78] 및 BLOOMZ[94]는 GPT-3(175B 버전)과 거의 동일한 수의 파라미터를 갖는 반면, GLM[93] 및 Galactica[35]는 각각 130B 및 120B 파라미터를 갖는다. 이 중 OPT(175B 버전)는 명령어 조정 버전 OPT-IML과 함께 개방형 공유를 위해 특별히 동기를 부여했으며, 이는 연구자가 대규모로 재현 가능한 연구를 수행할 수 있도록 하는 것을 목표로 한다. 언어 간 일반화 연구를 위해 BLOOM(176B 버전)과 BLOOMZ(176B 버전)는 다국어 언어 모델링 작업에 대한 능력으로 인해 기본 모델로 사용할 수 있다. 이중 언어 LLM으로서, GLM은 또한 효율성과 용량(_e.g._, 양자화, 32K-길이 컨텍스트, 빠른 추론 속도)에서 많은 개선을 특징으로 하는 인기 있는 소형 중국어 채팅 모델 ChatGLM2-6B(ChatGLM-6B에 대한 업데이트된 버전)를 제공하였다. 이 규모의 모델에는 일반적으로 수천 개의 GPU 또는 TPU가 교육 되어야 합니다. 예를 들어, OPT(175B 버전)는 992개의 A100-80GB GPU를 사용했고, GLM(130B 버전)은 96개의 NVIDIA DGX-A100(8x40G) GPU 노드의 클러스터를 사용했다.\n' +
      '\n' +
      '**LLaMA 모델 패밀리** 입니다. LLaMA 모델 모음 [57]은 2023년 2월 Meta AI에 의해 소개되었으며 4가지 크기(7B, 13B, 30B 및 65B)로 구성되었다. 출시 이후 LLaMA는 연구 및 산업 커뮤니티 모두에서 광범위한 관심을 받았다. LLaMA 모델은 지금까지 가장 인기 있는 개방형 언어 모델이 된 다양한 개방형 벤치마크에서 매우 우수한 성능을 달성했다. 많은 연구자들이 명령어 튜닝 또는 지속적인 사전 훈련을 통해 LLaMA 모델을 확장했다. 특히, 명령어 튜닝 LLaMA는 상대적으로 낮은 계산 비용으로 인해 맞춤형 또는 전문화된 모델을 개발하는 주요 접근법이 되었다. 비영어 언어에서 LLaMA 모델을 효과적으로 적용하기 위해서는 원어휘(주로 영어 말뭉치를 중심으로 훈련)를 확장하거나 목표 언어의 명령어나 데이터로 정교하게 조정해야 하는 경우가 많다. 이러한 확장된 모델들 중에서 스탠포드 알파카[142]는 LLaMA(7B)에 기초하여 미세 조정된 최초의 개방 지시-추종 모델이다. 텍스트-다빈치-003을 사용하여 자체 지시[143]을 통해 생성된 52K 지시-추종 시연에 의해 트레이닝된다. _Alpaca-52K_로 명명된 지시 데이터 및 트레이닝 코드는 Alpaca-LoRA[144](LoRA[145]를 사용하는 스탠포드 Alpaca의 재현), Koala[146] 및 BELLE[147]와 같은 후속 작업에서 광범위하게 채택되었다. 또한, Vicuna[138]는 ShareGPT[148]로부터 수집된 사용자 공유 대화들에 따라 훈련된 또 다른 인기 LLaMA 변형이다. LLaMA 모델 패밀리의 우수한 성능과 가용성으로 인해 많은 멀티모달 모델이 기본 언어 모델로 통합하여 강력한 언어 이해 및 생성 능력을 달성한다. 다른 변이체와 비교하여, 비쿠나는 멀티모달에서 더 선호된다\n' +
      '\n' +
      '도. 5: LLaMA에 대해 수행된 연구 작업의 진화 그래프. 엄청난 수로 인해 이 그림에 모든 LLaMA 변형을 포함할 수 없으며 훨씬 우수한 작업도 포함한다. 증분 업데이트를 지원 하기 위해이 그림의 원본 파일을 공유 하 고 GitHub 페이지에 끌어오기 요청을 제출 하 여 원하는 모델을 포함 하는 독자를 환영 합니다.\n' +
      '\n' +
      '언어 모델은 LLaVA[149], MiniGPT-4[150], InstructBLIP[151], PandaGPT[152]를 포함한 다양한 인기 모델의 출현으로 이어졌다. LLMA의 출시는 LLM의 연구 진보를 크게 진전시켰다. LLaMA에 대해 수행된 연구 작업을 요약하기 위해 그림 5에 간략한 진화 그래프를 제시한다.\n' +
      '\n' +
      '**LLM의 공용 API**. 모델 복사본을 직접 사용하는 대신 API는 모델을 로컬로 실행할 필요 없이 일반 사용자가 LLM을 사용할 수 있는 보다 편리한 방법을 제공합니다. LLM을 사용하기 위한 대표적인 인터페이스로 GPT 시리즈 모델 [46, 55, 66, 105]에 대한 API는 학계 및 산업계19에서 널리 사용되고 있다. OpenAI는 GPT-3 시리즈의 모델에 7개의 주요 인터페이스를 제공했다: ada, babbage, curie, davinci (GPT-3 시리즈에서 가장 강력한 버전), text-ada-001, text-babbbage-001 및 text-curie-001. 이 중 처음 4개의 인터페이스는 OpenAI의 호스트 서버에서 추가로 미세 조정될 수 있다. 특히, babbage, curie, davinci는 각각 GPT-3(1B), GPT-3(6.7B), GPT-3(175B) 모델에 해당한다[55]. 또한, 코드-쿠시만-001(코덱스의 강력하고 다국어 버전(12B[105]) 및 코드-다빈치-002라고 불리는 코덱스 [105]와 관련된 두 개의 API가 있다. 또한, GPT-3.5 시리즈는 하나의 기본 모델 코드-다빈치-002 및 세 개의 향상된 버전, 즉 텍스트-다빈치-002, 텍스트-다빈치-003 및 gpt-3.5-터보를 포함한다. 보다 강력한 대안으로 올해 오픈AI는 gpt-4, gpt-4-32k, gpt-4-1106-프리뷰(_i.e._, GPT-4 터보), gpt-4-비전-프리뷰(_i.e._, GPT-4 터보 with vision, 멀티모달 모델) 등 GPT-4 시리즈의 모델 인터페이스를 공개했다. OpenAI는 이러한 모델 인터페이스(gpt-3.5-turbo, gpt-4, gpt-4-32k)를 유지 및 업그레이드하고 있으므로 API 이름은 실제로 최신 버전을 가리킬 것입니다. 현재 ChatGPT는 GPT-3.5 또는 GPT-4 모델에 의해 구동될 수 있다. 전반적으로 특정 애플리케이션 시나리오 및 응답 요구 사항에 따라 적합한 모델 인터페이스를 선택합니다. 자세한 사용법은 프로젝트 웹사이트 20에서 확인할 수 있습니다.\n' +
      '\n' +
      '각주 19: [https://platform.openai.com/docs/api-reference/introduction](https://platform.openai.com/docs/api-reference/introduction)\n' +
      '\n' +
      '각주 20: [https://platform.openai.com/docs/models/overview](https://platform.openai.com/docs/models/overview)\n' +
      '\n' +
      '### _Commonly Used Corpora for Pre-training_\n' +
      '\n' +
      '이전의 PLM들과 대조적으로, 상당히 더 많은 수의 파라미터들로 구성된 LLM은 광범위한 콘텐츠를 커버하는 더 높은 볼륨의 트레이닝 데이터를 필요로 한다. 이러한 필요성을 위해 연구를 위해 공개된 액세스 가능한 훈련 데이터 세트가 점점 더 많아지고 있다. 이 섹션에서는 LLM을 훈련하기 위해 널리 사용되는 몇 가지 말뭉치를 간략하게 요약한다. 콘텐츠 유형에 따라 이 말뭉치를 책, 커먼크롤, 레딧 링크, 위키피디아, 코드 및 기타의 6개 그룹으로 분류한다.\n' +
      '\n' +
      '**책.** BookCorpus [153]은 광범위한 주제 및 장르(_e.g._, 소설 및 전기)를 포함하는 11,000권 이상의 책으로 구성된 이전 소규모 모델(_e.g._, GPT [122] 및 GPT-2 [26])에서 일반적으로 사용되는 데이터 세트입니다. 또 다른 대규모 책 말뭉치는 프로젝트 구텐베르크[154]로 소설, 수필, 시, 드라마, 역사, 과학, 철학, 그리고 공공 영역의 다른 유형의 작품을 포함한 7만 권 이상의 문학 책으로 구성되어 있다. 현재 MT-NLG[113]와 LLaMA[57]의 교육에 사용되는 가장 큰 오픈 소스 도서 컬렉션 중 하나이다. GPT-3[55]에 사용된 북스1[55], 북스2[55]의 경우 북코퍼스보다 훨씬 크지만 현재까지 공개되지 않았다.\n' +
      '\n' +
      '**CommonCrawl.** CommonCrawl [163]은 기존 LLM의 학습 데이터로 널리 사용 된 페타바이트 규모의 데이터 볼륨을 포함 하는 가장 큰 오픈 소스 웹 크롤링 데이터베이스 중 하나입니다. 전체 데이터 세트가 매우 크기 때문에 기존 연구들은 주로 특정 기간 내에 웹 페이지의 하위 집합을 추출한다. 그러나 웹 데이터에 잡음이 많고 품질이 낮은 정보가 널리 존재하기 때문에 사용 전에 데이터 전처리를 수행해야 한다. CommonCrawl을 기반으로 기존 작업에서 일반적으로 사용되는 4개의 필터링된 데이터 세트(C4[82], CC-Stories[155], CC-News[27], RealNews[156])가 있다. 거대 클린 크롤드 코퍼스(C4)는 en(806G), en.noclean(6T), realnewslike(36G), web-textlike(17G) 및 다국어(38T)의 5가지 변이체 21을 포함한다. _en_ 버전은 사전 트레이닝 T5[82], LaMDA[68], Go-pher[64], 및 UL2[89]에 활용되었다. mC4라고도 불리는 다국어 C4는 mT5[83]에서 사용되어 왔다. CC-Stories(31G)는 CommonCrawl 데이터의 서브세트로 구성되며, 이 서브세트는 내용이 스토리처럼 만들어진다. 현재 CC-Stories의 원본 소스를 사용할 수 없기 때문에 표 II에 복제 버전 _CC-Stories-R_[164]를 포함한다. 또한 CommonCrawl, REALNEWS(120G) 및 CC-News(76G)에서 추출한 두 개의 뉴스 코퍼스도 사전 학습 데이터로 일반적으로 사용된다.\n' +
      '\n' +
      '각주 21: [https://www.tensorflow.org/datasets/catalog/c4](https://www.tensorflow.org/datasets/catalog/c4)\n' +
      '\n' +
      '**Reddit Links.** Reddit은 사용자가 링크 및 텍스트 게시물을 제출할 수 있도록 하는 소셜 미디어 플랫폼으로, "업보팅" 또는 "다운보팅"을 통해 다른 사람이 투표할 수 있습니다. 높은 표창을 받은 게시물은 종종 유용한 것으로 간주되며 고품질 데이터 세트를 만드는 데 활용될 수 있다. WebText[26]은 Reddit의 고도로 상향 링크들로 구성된 잘 알려진 말뭉치이지만, 공개적으로 이용가능하지는 않다. 대리인으로서 OpenWebText[157]라는 쉽게 접근할 수 있는 오픈 소스 대안이 있다. Reddit에서 추출된 또 다른 코퍼스는 PushShift.io [158]로, 생성일로부터 Reddit의 과거 데이터로 구성된 실시간 업데이트 데이터 세트이다. 푸시시프트는 월간 데이터 덤프뿐만 아니라 검색, 요약 및 수행에서 사용자를 지원하는 유용한 유틸리티 도구를 제공한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline\n' +
      '**Corpora** & **Size** & **Source** & **Latest Update Time** \\\\ \\hline BookCorpus [153] & 5GB & Books & Dec-2015 \\\\ Gutenberg [154] & - & Books & Dec-2021 \\\\ Cat [82] & 800GB & CommonCrawl & Apr-2019 \\\\ CC-Stories-R [155] & 31GB & CommonCrawl & Sep-2019 \\\\ CC-NEWS [27] & 78GB & CommonCrawl & Feb-2019 \\\\ REALNEWS [156] & 120GB & CommonCrawl & Apr-2019 \\\\ OpenWebText [157] & 38GB & Reddit links & Mar-2023 \\\\ Pushshift.io [158] & 2TB & Reddit links & Mar-2023 \\\\ Wikipedia [159] & 21GB & Wikipedia & Mar-2023 \\\\ BigQuery [160] & - & Codes & Mar-2023 \\\\ the Pile [161] & 800GB & Other & Dec-2020 \\\\ ROOTS [162] & 1.6TB & Other & Jun-2022 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE II: Statistics of commonly-used data sources.\n' +
      '\n' +
      '전체 데이터 세트에 대한 예비 조사입니다. 따라서 사용자가 Reddit 데이터를 쉽게 수집하고 처리할 수 있습니다.\n' +
      '\n' +
      '**위키피디아.** 위키피디아 [159]는 다양한 주제에 대한 고품질 기사를 대량으로 포함하는 온라인 백과사전입니다. 이러한 글의 대부분은 광범위한 언어와 분야를 포괄하는 (지원 참조와 함께) 서술 방식으로 구성된다. 전형적으로, Wikipedia의 영어 전용 필터링된 버전은 대부분의 LLM(_e.g._, GPT-3[55], LaMDA[68], 및 LLaMA[57])에서 널리 사용된다. 위키피디아는 여러 언어로 제공되므로 다국어 설정에서 사용할 수 있습니다.\n' +
      '\n' +
      '**코드.** 코드 데이터를 수집 하기 위해 기존 작업은 주로 인터넷에서 오픈 소스 라이선스 코드를 크롤링 합니다. 두 가지 주요 소스는 오픈 소스 라이선스(_e.g._, GitHub) 아래의 공용 코드 리포지토리 및 코드 관련 질문 응답 플랫폼(_e.g._, StackOverflow)입니다. 구글은 다양한 프로그래밍 언어에서 상당한 수의 오픈 소스 라이선스 코드 조각들을 포함하는 빅쿼리 데이터 세트[160]를 공개하여 대표적인 코드 데이터 세트 역할을 하고 있다. CodeGen은 BigQuery 데이터 세트의 하위 집합인 BIGQUERY [86]을 사용하여 CodeGen(CodeGen-Multi)의 다국어 버전을 학습했습니다.\n' +
      '\n' +
      '**기타.** 파일 [161]은 책, 웹 사이트, 코드, 과학 논문 및 소셜 미디어 플랫폼을 포함한 여러 원본의 800GB 이상의 데이터로 구성된 대규모, 다양 하 고 오픈 소스 텍스트 데이터 세트입니다. 22개의 다양한 고품질 하위 집합으로 구성됩니다. Pile 데이터셋은 GPT-J(6B)[165], CodeGen(16B)[86], Megatron-Turing NLG(530B)[113]와 같이 파라미터 척도가 다른 모델에서 널리 사용된다. ROOTS[162]는 다양한 더 작은 데이터 세트(전체 1.61 TB의 텍스트)로 구성되며, BLOOM 훈련에 사용되어 온 59개의 다른 언어(자연 언어 및 프로그래밍 언어 포함)를 포함한다[78].\n' +
      '\n' +
      '실제로, 그것은 일반적으로 단일 코퍼스 대신에 LLM들을 사전 트레이닝하기 위한 상이한 데이터 소스들의 혼합을 필요로 한다(도 6 참조). 따라서, 기존 연구들은 공통적으로 여러 기성 데이터셋(_e.g._, C4, OpenWebText, and the Pile)을 혼합한 후, 추가 처리를 수행하여 사전 훈련 코퍼스를 획득한다. 또한, 특정 응용에 적응적인 LLM을 학습하기 위해서는 사전 학습 데이터에서 해당 정보를 풍부하게 하기 위해 관련 소스(_e.g._, Wikipedia 및 BigQuery)에서 데이터를 추출하는 것도 중요하다. 기존 LLM에서 사용되는 데이터 소스를 빠르게 참조하기 위해 세 가지 대표적인 LLM의 사전 훈련 코퍼스를 제시한다.\n' +
      '\n' +
      '\\(\\bullet\\)_GPT-3_(175B)[55]는 CommonCrawl[163], WebText2[55], Books1[55], Books2[55], Wikipedia[159]를 포함하는 300B 토큰의 혼합 데이터세트에 대해 트레이닝되었다.\n' +
      '\n' +
      '\\(\\bullet\\)_PaLM_(540B)[56]은 소셜 미디어 대화, 필터링된 웹페이지, 책, 깃허브, 다국어 위키피디아 및 뉴스에서 소스되는 780B 토큰의 사전 트레이닝 데이터세트를 사용한다.\n' +
      '\n' +
      '\\(\\bullet\\)_LLaMA_[57]은 CommonCrawl, C4[82], Github, Wikipedia, books, ArXiv, StackExchange 등 다양한 소스로부터 훈련 데이터를 추출한다. LLaMA(6B) 및 LLaMA(13B)에 대한 트레이닝 데이터 크기는 1.0T 토큰인 반면, LLaMA(32B) 및 LLaMA(65B)에 대해서는 1.4T 토큰이 사용된다.\n' +
      '\n' +
      '### _Common Used Datasets for Fine-tuning_\n' +
      '\n' +
      '사전 훈련 후에는 모델 용량을 향상시키기 위해 LLM을 추가로 미세 조정해야 하며, 이는 종종 명령 튜닝(감독 미세 조정) 및 정렬 튜닝의 두 가지 주요 단계를 포함한다. 이 섹션에서는 주로 두 가지 튜닝 접근법에 대해 관련 사용 가능한 데이터 세트에 대해 논의하는 데 중점을 두고 있으며, 더 많은 알고리즘 세부 사항은 섹션 5에서 찾을 수 있다.\n' +
      '\n' +
      '#### 3.3.1 Instruction Tuning Datasets\n' +
      '\n' +
      '사전 트레이닝 후에, 명령어 튜닝(_a.k.a._, 감독된 미세-튜닝)은 LLM들(_e.g._, 명령어 팔로우)의 특정 능력들을 강화하거나 잠금해제하기 위한 중요한 방법이다. 이 부분에서는 명령어 튜닝을 위해 널리 사용되는 여러 데이터 세트를 소개하고 형식화된 명령어 인스턴스의 구성 방법에 따라 NLP 태스크 데이터 세트, 일일 채팅 데이터 세트 및 합성 데이터 세트의 세 가지 주요 유형으로 분류한다. 우리는 표 III에 그들의 세부 사항을 보여준다.\n' +
      '\n' +
      '**NLP 작업 데이터 세트.** 이러한 종류의 데이터 세트는 해당 자연어 작업 설명이 포함된 수집된 NLP 작업 데이터 세트(_e.g._, 텍스트 분류 및 요약)를 기반으로 형식이 지정됩니다. 이 범주에서 P3[182]와 FLAN[67, 183]은 명령어 튜닝을 위해 널리 사용되는 두 데이터 세트이다.\n' +
      '\n' +
      '\\(\\bullet\\)_P3_[182]는 170개의 영어 NLP 데이터 세트와 2,052개의 영어 프롬프트 템플릿으로 구성되며, 여기서 각 데이터 예제의 입력 및 출력은 트레이닝 인스턴스를 구성하기 위한 특정 프롬프트 템플릿으로 포맷되었다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline\n' +
      '**Dataset** & **Release Time** & **\\#Examples** \\\\ \\hline \\multicolumn{3}{l}{Summarize from Feedback [129]} & Sep-2020 & 193K \\\\ SHP [177] & Oct-2021 & 385K \\\\ WebGPT Comparisons [81] & Dec-2021 & 19K \\\\ Stack Exchange Preferences [178] & Dec-2021 & 10M \\\\ HH-RLHF [170] & Apr-2022 & 169K \\\\ Sandbox Alignment Data [179] & May-2023 & 169K \\\\ CValues [180] & Jul-2023 & 145K \\\\ PKU-SafeRLHF [181] & Oct-2023 & 330K \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE IV: A list of available collections for alignment.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline\n' +
      '**Categories** & **Collections** & **Time** & **\\#Examples** \\\\ \\hline \\multirow{8}{*}{Task} & Nat. Inst. [166] & Apr-2021 & 193K \\\\  & FLAN [67] & Sep-2021 & 4.4M \\\\  & P3 [167] & Oct-2021 & 12.1M \\\\  & Super Nat. Inst. [88] & Apr-2022 & 5M \\\\  & MVPCrupus [168] & Jun-2022 & 41M \\\\  & xP3 [94] & Nov-2022 & 81M \\\\  & OIG[169] & Mar-2023 & 43M \\\\ \\hline \\multirow{8}{*}{Chat} & HH-RLHF [170] & Apr-2022 & 160K \\\\  & HCGs [171] & Jan-2023 & 87K \\\\  & ShazeGT[148] & Mar-2023 & 90K \\\\  & Dolly [172] & Apr-2023 & 15K \\\\  & OpenAssistant [173] & Apr-2023 & 161K \\\\ \\hline \\multirow{8}{*}{Synthetic} & Self-Instruct [143] & Dec-2022 & 82K \\\\  & Alpha [137] & Mar-2023 & 52K \\\\ \\cline{1-1}  & Guancao [174] & Mar-2023 & 535K \\\\ \\cline{1-1}  & Baire [175] & Apr-2023 & 158K \\\\ \\cline{1-1}  & BELLE [176] & Apr-2023 & 1.5M \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE III: A detailed list of available collections for instruction tuning.\n' +
      '\n' +
      '\\(\\bullet\\)_FLAN_[67]은 원래 버전에서 널리 사용되는 62개의 NLP 벤치마크로 구성된다. 최근에는 Muffin [67], NIV2 [88], T0-SF [28], CoT [184, 185, 186]를 포함한 추가 명령어 데이터 세트를 혼합하여 FLAN을 확장하는 FLAN-v2 [183]도 제안된다. 머핀은 원래 FLAN의 62개의 태스크와 대화 및 코드 합성 태스크를 포함한 26개의 추가 태스크를 포함한다. T0-SF는 머핀과의 중첩을 보장하면서 T0[28]로부터 추출된다. NIV2는 Natural-Instructions v2 데이터세트[88]를 참조하고, CoT[184, 185, 186]는 9개의 추론 태스크와 대응하는 사고 체인 프롬프트 및 출력의 조합이다.\n' +
      '\n' +
      '**일별 채팅 데이터 세트.** 이러한 종류의 데이터 세트는 쿼리가 사람에 의해 생성되고 응답이 주로 인간 레이블러 또는 LLM(_예:_ ChatGPT, GPT-4)에 의해 생성되는 실제 사용자 대화를 기반으로 구성됩니다. 대화 유형에는 개방형 세대, 질문 응답, 브레인스토밍 및 채팅이 포함됩니다. 이 범주에서 ShareGPT[148], OpenAssistant[173] 및 Dolly[172]는 LLM 미세 조정을 위해 일반적으로 사용되는 세 가지 데이터 세트이다.\n' +
      '\n' +
      '\\(\\bullet\\)_ShareGPT_[148]은 사용자가 ShareGPT API를 통해 ChatGPT 또는 GPT-4와의 대화를 업로드할 수 있는 데이터 수집 플랫폼으로부터 수집된다. 현재 이 데이터 세트는 인간의 실제 지시나 문의와 ChatGPT의 응답을 포함하여 약 90,000개의 대화로 구성된다.\n' +
      '\n' +
      '\\(\\bullet\\)_OpenAssistant_[173]은 인간과 AI 비서 사이의 66,497개의 실제 대화 트리를 포함하는 다국어 말뭉치다. 각 대화 트리는 여러 개의 노드로 구성되며, 각 노드는 대화에서 역할에 의해 생성된 정보를 나타낸다. 그것은 35개 언어에 걸쳐 있으며 응답의 수동 주석이 달린 품질 등급 461,292개를 포함한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Dolly_[172]는 Databricks로부터 15,000개의 인간 생성 데이터 인스턴스(prompt-response pair)를 포함하는 영어 데이터세트이다. 이 데이터 세트는 브레인스토밍, 분류, 폐쇄형 책 품질 보증, 생성, 정보 추출, 개방형 책 품질 보증 및 요약을 포함하여 InstructGPT [66]에 설명된 7개의 영역을 다룬다.\n' +
      '\n' +
      '**합성 데이터 세트.** 이러한 종류의 데이터 세트는 일반적으로 미리 정의된 지침 규칙 또는 메서드를 기반으로 LLM에 지시하여 구성됩니다. 이 범주에서 Self-Instruct-52K [143], Alpaca [142] 및 Baire [175]는 LLM에 일반적으로 사용되는 세 가지 합성 데이터 세트이다.\n' +
      '\n' +
      '\\(\\bullet\\)_Self-Instruct-52K_[143]은 52,000개의 명령어를 가진 82,000개의 인스턴스로 구성된 Self-instruct [143] 방법을 통해 생성된 명령어 데이터셋이다. 구체적으로 저자는 175개의 시드 인스턴스를 구성한 다음 LLM[55]이 참조로 무작위로 선택된 8개의 지침을 기반으로 추가 지침을 합성하도록 반복적으로 촉구한다. 이어서, LLM은 합성 명령어들에 기초하여 인스턴스 입력들 및 그들의 대응하는 출력들을 생성하도록 추가로 지시받고, 최종적으로 Self-Instruct-52K 데이터세트를 획득한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Alpaca_[142]도 자체 지시 [143] 방법을 기반으로 하는 합성 데이터 세트입니다. Self-Instruct-52K의 175개의 시드 데이터 세트에 대한 텍스트-다빈치-003 모델을 활용하여 52,000개의 새로운 명령어와 해당 입력 및 출력을 얻는다. 또한, 예제 중 60%는 최종 데이터 세트에 입력 부분이 없는 순수한 명령어이다.\n' +
      '\n' +
      '\\(\\bullet\\)_Baize_[175]는 111.5K 인스턴스를 포함하는 ChatGPT를 이용하여 구축된 영어 멀티턴 대화 코퍼스이다. Baire를 생성하기 위해, "셀프-챗"[175]라는 방법이 목적되며, 여기서 ChatGPT는 사용자와 AI 어시스턴트 모두의 역할을 차례로 맡아 대화 형식으로 정보를 생성한다.\n' +
      '\n' +
      '#### 3.3.2 Alignment Datasets\n' +
      '\n' +
      '명령어 조정 외에도 LLM을 인간의 가치 및 선호도(예: 유용성, 정직성 및 무해성)와 정렬하기 위한 고품질 데이터 세트를 구성하는 것이 중요하다. 이 섹션에서는 HH-RLHF [170], SHP [177], PKU-SafeRLHF [181], Stack Exchange Preferences [178] 및 Sandbox Alignment Data [179]를 포함하여 정렬 조정을 위해 널리 사용되는 여러 데이터 세트를 소개한다. 우리는 표 IV에 그들의 세부 사항을 보여준다.\n' +
      '\n' +
      '\\(\\bullet\\)**HH-RLHF**[170]은 약 169K 인스턴스로 구성 되며 LLM의 유용성과 무해성에 중점을 둔 두 부분으로 나눌 수 있습니다. 각 인스턴스는 도움, 조언 또는 작업 완료를 찾는 것에 대한 크라우드 워커와 채팅 모델 간의 개방형 대화입니다. 채팅 모델은 각 사용자 질의에 대해 두 개의 응답을 제공하며, 더 도움이 되거나 유해한 응답이 주석으로 선택될 것이다.\n' +
      '\n' +
      '\\(\\bullet\\)**SHP**[177]은 응답의 유용성에 중점을 둡니다. 요리부터 법률 자문까지 주제에 걸쳐 18개 다양한 주제 영역에 걸친 질문/지침에 대한 응답보다 385K 집단 인간 선호도로 구성된다. 각 인스턴스는 질문 또는 지시와 한 쌍의 최상위 댓글을 포함하는 레딧 게시물이며, 그 중 하나는 레딧 사용자가 더 선호하는 것으로 간주되고 다른 하나는 덜 도움이 되는 것으로 간주된다. HH-RLHF [170]과 달리 SHP의 데이터는 자연 발생 및 인간 작성 응답으로 구성된다.\n' +
      '\n' +
      '\\(\\bullet\\)**PKU-SafeRLHF**[181]는 유용성과 무해성에 중점을 둔 330K 이상의 전문가 비교 데이터를 포함합니다. 데이터세트의 각 인스턴스에는 질문과 두 개의 응답이 포함되며, 각 응답에 대한 안전 라벨과 유용성과 무해성에 따른 두 응답 간의 두 가지 선호도 주석이 함께 제공된다. 응답의 무해성은 14가지 모든 피해 범주에 걸쳐 위험 중립적인 분류로 분류되는 반면 응답의 유용성은 질문을 해결하는 효과에 따라 평가된다.\n' +
      '\n' +
      '\\(\\bullet\\)** 스택 교환 환경 설정**[178]은 답변의 유용성에 중점을 둡니다. 스택 오버플로우의 약 10M 질문과 답변으로 구성되어 있습니다. 각 인스턴스는 질문과 두 개 이상의 해당 답변으로 구성됩니다. 각 답변은 투표에 기초하여 계산된 점수와 선택 여부를 나타내는 레이블로 주석이 달린다.\n' +
      '\n' +
      '\\(\\bullet\\)**샌드박스 정렬 데이터**[179]는 사람이 아닌 LLM의 피드백을 포함하는 정렬 데이터 세트입니다. 이 모델은 다른 모델과의 사회적 상호작용을 시뮬레이션하고 다른 모델의 피드백에 따라 응답을 수정하는 SAND-BOX라는 가상 상호작용 환경에서 비롯된다. 데이터 세트에는 169K 인스턴스가 포함 되어 있으며 각 인스턴스는 사회적 쿼리, 여러 응답 및 다른 모델의 해당 등급으로 구성 됩니다.\n' +
      '\n' +
      '### _Library Resource_\n' +
      '\n' +
      '이 부분에서는 LLM 개발을 위해 사용 가능한 일련의 라이브러리를 간략하게 소개한다.\n' +
      '\n' +
      '\\(\\bullet\\)**Transformers**[187]은 Hugging Face에서 개발 및 유지 관리 하는 Transformer 아키텍처를 사용 하 여 모델을 빌드하기 위한 오픈 소스 Python 라이브러리입니다. 간편하고 사용자 친화적인 API를 가지고 있어 다양한 사전 훈련된 모델을 쉽게 사용하고 사용자 정의할 수 있습니다. 모델과 알고리즘을 정기적으로 업데이트하고 개선하는 사용자 및 개발자의 크고 활동적인 커뮤니티를 가진 강력한 라이브러리이다.\n' +
      '\n' +
      '\\(\\bullet\\)**DeepSpeed**[74]는 Microsoft에서 개발한 딥 러닝 최적화 라이브러리(PyTorch와 호환)로, MT-NLG [113] 및 BLOOM [78]과 같은 여러 LLM을 교육하는 데 사용되었습니다. 메모리 최적화(ZeRO 기법, Gradient checkpointing), 파이프라인 병렬화 등 분산 학습을 위한 다양한 최적화 기법의 지원을 제공한다.\n' +
      '\n' +
      '\\(\\bullet\\)**Megatron-LM**[75, 76, 77]은 대규모 언어 모델을 훈련하기 위해 NVIDIA에서 개발한 심층 학습 라이브러리입니다. 또한 모델 및 데이터 병렬성, 혼합 정밀도 훈련, 플래시어텐션 등 분산 학습을 위한 풍부한 최적화 기법을 제공한다. 이러한 최적화 기술은 크게 훈련 효율 및 속도를 향상시킬 수 있어 GPU 전반에 걸쳐 효율적인 분산 훈련을 가능하게 한다.\n' +
      '\n' +
      '\\(\\bullet\\)**JAX**[188]은 Google에서 개발한 고성능 기계 학습 알고리즘을 위한 Python 라이브러리로, 사용자가 하드웨어 가속(_e.g._, GPU 또는 TPU)을 사용하여 어레이에서 쉽게 계산을 수행할 수 있습니다. 이는 다양한 디바이스에서 효율적인 연산을 가능하게 하며 자동 미분, Just-in-Time 컴파일 등 여러 기능들을 지원한다.\n' +
      '\n' +
      '\\(\\bullet\\)**Colossal-AI**[189]는 HPC-AI Tech에서 대규모 AI 모델을 훈련하기 위해 개발한 딥러닝 라이브러리입니다. PyTorch를 기반으로 구현되며 풍부한 병렬 훈련 전략 모음을 지원합니다. 나아가, 패트릭스타[190]에 의해 제안된 방법들로 이종의 메모리 관리를 최적화할 수도 있다. 최근 ColossalChat[140]이라는 ChatGPT 유사 모델이 두 가지 버전(7B와 13B)으로 공개되었는데, 이 모델은 LLaMA[57]를 기반으로 Colossal-AI를 사용하여 개발되었다.\n' +
      '\n' +
      '\\(\\bullet\\)**BMTrain**[191]은 OpenBMB가 대규모 매개 변수를 분산 방식으로 학습하는 모델을 위해 개발한 효율적인 라이브러리로, 코드의 단순성, 낮은 리소스 및 고가용성을 강조합니다. BMtrain은 이미 몇 가지 일반적인 LLM(_e.g._, Flan-T5[69] 및 GLM[93])을 모델 센터에 통합했으며, 여기서 개발자는 이러한 모델을 직접 사용할 수 있다.\n' +
      '\n' +
      '\\(\\bullet\\)**FastMoE**[192]는 MoE (_i.e_, mixture-of-experts) 모델을 위한 특수 교육 라이브러리입니다. 파이토치를 기반으로 개발되었으며, 설계에서 효율성과 사용자 편의성을 우선시합니다. FastMoE는 Transformer 모델을 MoE 모델로 전달하는 과정을 단순화하고 학습 시 데이터 병렬성과 모델 병렬성을 모두 지원한다.\n' +
      '\n' +
      '\\(\\bullet\\)**VLM**[193]은 LLM 추론 및 서비스를 위한 빠르고 메모리 효율적이며 사용하기 쉬운 라이브러리입니다. 빠른 추론을 가능하게 하기 위해, 이것은 높은 서빙 처리량, PagedAttention [193], 연속 배칭 및 최적화된 CUDA 커널을 사용한 효과적인 주의 메모리 관리로 특별히 최적화된다. 또한, vLLM은 다양한 디코딩 알고리즘, 텐서 병렬성 및 스트리밍 출력을 지원한다. VLLM은 다른 시스템과의 통합을 용이하게 하기 위해 HuggingFace 모델 사용에 친화적이며 OpenAI 호환 API 서버도 제공한다.\n' +
      '\n' +
      '\\(\\bullet\\)**DeepSpeed-MII**[194]는 DeepSpeed [74]에서 개발한 메모리 효율적인 Python 라이브러리이기도 합니다. 높은 처리량, 낮은 지연 시간 및 비용 효율성을 우선시하여 LLMs 추론을 민주화하는 것을 목표로 한다. DeepSpeed-MII는 차단된 KV 캐싱, 연속 배칭, 동적 SplitFuse, 고성능 CUDA 커널의 4가지 필수 기술을 활용하여 가속화된 텍스트 생성 추론을 달성한다. 현재 LLaMA [57], Mistral [195] 및 OPT [90]과 같은 세 가지 인기 있는 모델 아키텍처에 걸쳐 13,000개 이상의 모델을 지원합니다.\n' +
      '\n' +
      '\\(\\bullet\\)**DeepSpeed-Chat**[196]은 모델 학습 중에 전체 RLHF 프로세스를 통합할 수 있는 빠르고 비용 효율적이며 사용하기 쉬운 시스템 프레임워크입니다. 이는 ChatGPT와 유사한 모델에 대한 학습 및 추론 과정을 단순화하여 간단한 스크립트를 사용하여 여러 학습 또는 추론 단계를 구현할 수 있도록 하는 기능, (2) InstructGPT [66]의 학습 모드를 복제하고 3개의 학습 단계(_i.e._, SFT, 보상 모델 미세 조정 및 RLHF)에 대한 완전한 파이프라인을 제공하는 기능, (3) Deepspeed의 학습 엔진 및 추론 엔진을 RLHF 훈련용 통합 하이브리드 엔진(Deepspeed HE)에 통합하여 학습 및 추론 모드 간의 원활한 전환을 가능하게 하는 기능, DeepSpeed Inference의 다양한 최적화를 활용하는 기능 등을 특징으로 한다.\n' +
      '\n' +
      '상기 라이브러리 자원 외에도, 기존의 딥 러닝 프레임워크(_e.g._, PyTorch[197], TensorFlow[198], MXNet[199], PaddlePaddle[200], MindSpore[136] 및 OneFlow[201])도 대규모 모델의 훈련에 일반적으로 사용되는 병렬 알고리즘에 대한 지원을 제공하였다.\n' +
      '\n' +
      '## 4 Pre-training\n' +
      '\n' +
      '사전 훈련은 LLM의 능력의 기초를 확립한다. 대규모 말뭉치에 대한 사전 훈련을 통해 LLM은 필수적인 언어 이해 및 생성 기술을 습득할 수 있다[55, 56]. 이 과정에서 사전 훈련 코퍼스의 규모와 품질은 LLM이 강력한 능력을 달성하는 데 중요하다. 또한 LMM을 효과적으로 사전 훈련하기 위해서는 모델 아키텍처, 가속 방법 및 최적화 기술이 잘 설계되어야 한다. 다음에서는 먼저 섹션 4.1의 데이터 수집 및 처리에 대해 논의한 다음 섹션 4.2에서 일반적으로 사용되는 모델 아키텍처를 소개하고 마지막으로 섹션 4.3에서 LLM을 안정적이고 효율적으로 최적화하기 위한 훈련 기술을 제시한다.\n' +
      '\n' +
      '### _Data Collection and Preparation_\n' +
      '\n' +
      '소규모 언어 모델에 비해 LLM은 모델 사전 훈련을 위한 고품질 데이터에 대한 수요가 더 강하며, 모델 용량은 사전 훈련 코퍼스와 사전 처리 방법에 크게 의존한다. 이 부분에서는 데이터 소스, 전처리 방법, 사전 훈련 데이터가 LLM의 성능에 어떤 영향을 미치는지 중요한 분석을 포함하여 사전 훈련 데이터의 모델과 처리에 대해 논의한다.\n' +
      '\n' +
      '#### 4.1.1 데이터 원본\n' +
      '\n' +
      '유능한 LLM을 개발하기 위해서는 다양한 데이터 소스로부터 대량의 자연어 코퍼스를 수집하는 것이 핵심이다. 기존의 LLM은 주로 다양한 공개 텍스트 데이터 세트의 혼합물을 사전 훈련 코퍼스로 활용한다. 그림 6은 다수의 대표적인 LLM에 대한 사전 훈련 데이터의 출처의 분포를 보여준다.\n' +
      '\n' +
      '사전 훈련 코퍼스의 출처는 크게 일반 데이터와 전문 데이터의 두 가지 유형으로 분류할 수 있다. 웹 페이지, 책, 대화 텍스트와 같은 일반 데이터는 크고 다양하며 접근하기 쉬운 특성으로 인해 대부분의 LLM[55, 56, 90]에서 활용되며, 이는 LLM의 언어 모델링 및 일반화 능력을 향상시킬 수 있다. LLM이 보여주는 인상적인 일반화 능력에 비추어, 사전 훈련 코퍼스를 다국어 데이터, 과학 데이터 및 코드와 같은 보다 전문화된 데이터 세트로 확장하여 LLM에 특정 과제 해결 능력을 부여하는 연구도 있다[35, 56, 86]. 다음에서는 이 두 가지 유형의 사전 훈련 데이터 소스와 LLM에 미치는 영향에 대해 설명한다. 일반적으로 사용되는 코퍼스에 대한 자세한 소개는 섹션 3.2를 참조할 수 있다.\n' +
      '\n' +
      '**일반 텍스트 데이터.** 그림 6에서 볼 수 있듯이 대부분의 LLM은 다양한 주제에 대한 풍부한 텍스트 소스를 제공하는 웹 페이지, 책 및 대화 텍스트와 같은 범용 사전 교육 데이터를 채택합니다. 다음으로, 우리는 세 가지 중요한 종류의 일반 데이터를 간략하게 요약한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Webpages._ 인터넷의 확산으로 인해 다양한 유형의 데이터가 생성되어 LLM이 다양한 언어 지식을 얻고 일반화 능력을 향상시킬 수 있다[82, 26]. 이러한 데이터 자원의 편리한 사용을 위해 CommonCrawl[163]과 같은 이전 작업에서 많은 양의 데이터가 웹으로부터 크롤링된다. 그러나 크롤링된 웹 데이터는 위키피디아와 같은 고품질 텍스트와 스팸 메일과 같은 저품질 텍스트를 모두 포함하는 경향이 있으므로 데이터 품질을 향상시키기 위해 웹 페이지를 필터링하고 처리하는 것이 중요하다.\n' +
      '\n' +
      '\\(\\bullet\\)_대화 텍스트입니다. 대화 데이터는 LLM들의 대화 능력을 향상시키고 잠재적으로 다양한 질문-응답 태스크들에 대한 그들의 성능을 향상시킬 수 있다[56]. 연구자들은 공개 대화 말뭉치(_e.g.,_PushShift.io Reddit 말뭉치)의 하위 집합을 활용하거나[202, 158] 온라인 소셜 미디어로부터 대화 데이터를 수집할 수 있다. 온라인 대화 데이터는 종종 다수의 참가자 간의 토론을 포함하기 때문에 효과적인 처리 방법은 대화를 트리 구조로 변환하는 것이며, 여기서 발화는 그것이 응답하는 것에 연결된다. 이와 같이, 다자간 대화 트리는 복수의 서브 대화로 분할될 수 있으며, 이는 사전 훈련 코퍼스에서 수집될 수 있다. 더욱이, 잠재적인 위험은 LLM들에 대화 데이터의 과도한 통합이 부작용을 초래할 수 있다는 것이다[90]: 선언적 지시들 및 직접 질문들이 대화의 시작으로서 잘못 인식되어, 지시들의 효능의 하락으로 이어진다.\n' +
      '\n' +
      '\\(\\bullet\\)_Books._ 다른 코퍼스와 비교하여 책은 LLM이 언어 지식을 배우고 장기 의존성을 모델링하고 내러티브와 일관된 텍스트를 생성하는 데 잠재적으로 유익한 공식 장문의 중요한 소스를 제공한다. 오픈 소스 도서 데이터를 얻기 위해 기존 연구에서는 일반적으로 파일 데이터 집합 [161]에서 사용할 수 있는 Books3 및 Bookcorpus2 데이터 집합을 채택한다.\n' +
      '\n' +
      '**전문 텍스트 데이터.** 전문 데이터 세트는 다운스트림 작업에서 LLM의 특정 기능을 개선하는 데 유용합니다. 다음으로, 우리는 세 종류의 전문 데이터를 소개한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Multilingual text._ 목표 언어의 텍스트 외에도 다국어 말뭉치를 통합하면 언어 이해 및 생성의 다국어 능력을 향상시킬 수 있다. 예를 들어, BLOOM[78] 및 PaLM[56]은 사전 트레이닝 말뭉치 내에서 각각 46개 및 122개 언어를 포함하는 큐레이트된 다국어 데이터를 갖는다. FLM[102]은 중국어와 영어 말뭉치를 거의 동일한 비율로 혼합한다. 이 모델들은 번역, 다국어 요약, 다국어 질의 응답과 같은 다국어 작업에서 인상적인 성능을 보여주며, 목표 언어(들)의 코퍼스에서 미세 조정된 최신 모델과 비교되거나 우수한 성능을 달성한다.\n' +
      '\n' +
      '도. 6: 기존 LLM에 대한 사전 트레이닝 데이터에서 다양한 데이터 소스의 비율.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:18]\n' +
      '\n' +
      '프라이버시 위험을 어느 정도 줄입니다.\n' +
      '\n' +
      '**토큰화.** 토큰화는 데이터 전처리를 위한 중요한 단계이기도 합니다. 그것은 원시 텍스트를 개별 토큰의 시퀀스로 분할하는 것을 목표로 하며, 이는 후속적으로 LLM의 입력으로 사용된다. 전통적인 NLP 연구(예를 들어, 조건부 랜덤 필드[220]를 갖는 시퀀스 라벨링)에서, 단어 기반 토큰화가 지배적인 접근법이며, 이는 인간의 언어 인지와 더 정렬된다. 그러나, 단어 기반 토큰화는 일부 언어에서 동일한 입력에 대해 상이한 분할 결과를 산출할 수 있고(_예를 들어,_중국어 단어 분할), 많은 저빈도 단어를 포함하는 거대한 단어 어휘를 생성할 수 있고, 또한 _"어휘 외"_ 문제를 겪을 수 있다. 따라서, 몇몇 신경망 모델들은 단어 표현을 유도하기 위한 최소 단위로서 _character_ 를 채용한다(예를 들어, _ELMo의 CNN 단어 인코더[21]). 최근 _서브워드 토큰라이저_ 는 일반적으로 Byte-Pair Encoding 토큰화, WordPiece 토큰화 및 Unigram 토큰화를 포함하는 Transformer 기반 언어 모델에서 널리 사용되고 있다. 포옹페이스는 실행 예시와 함께 토큰나이저22에 대한 우수한 온라인 NLP 과정을 유지했으며, 우리는 이 과정에 초보자들을 참조한다. 다음으로 대표적인 세 가지 토큰화 방법에 대해 간략히 설명한다.\n' +
      '\n' +
      '각주 22: [https://huggingface.co/learn/nlp-course/chapter6](https://huggingface.co/learn/nlp-course/chapter6)\n' +
      '\n' +
      '\\(\\bullet\\)_Byte-Pair Encoding (BPE) 토큰화._ BPE는 원래 1994년 일반적인 데이터 압축 알고리즘으로 제안되었고[221], 이후 토큰화를 위해 NLP에 적응되었다[222]. 기본 심볼들의 세트(_예를 들어,_알파벳들 및 경계 문자들)로 시작하고, 코퍼스에서 두 개의 연속적인 토큰들의 빈번한 쌍들을 새로운 토큰들( _merge_라 칭함)로서 반복적으로 조합한다. 각각의 병합에 대해, 선택 기준은 두 개의 연속적인 토큰들의 동시 발생 빈도에 기초한다 : 상위 빈발 쌍이 선택될 것이다. 병합 프로세스는 미리 정의된 크기에 도달할 때까지 계속됩니다. 또한, Byte-level BPE는 _bytes_를 병합을 위한 기본 심볼로 고려함으로써 다국어 말뭉치(_예를 들어, non-ASCII 문자를 포함하는 텍스트)에 대한 토큰화 품질을 향상시키기 위해 사용되었다. 이러한 토큰화 접근 방식의 대표적인 언어 모델로는 GPT-2, BART, LLaMA 등이 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_WordPiece tokenization._ 워드피스는 구글 내부 서브워드 토큰화 알고리즘이었다. 그것은 원래 구글에 의해 음성 검색 시스템 개발에 제안되었다[223]. 이후 2016년 신경망 기계 번역 시스템에서 사용되었고[224], 2018년 BERT용 단어 토큰라이저로 채택되었다[23]. 워드피스는 연속 토큰들을 반복적으로 병합함으로써 BPE와 매우 유사한 아이디어를 갖는 반면, 병합에 대해 약간 다른 선택 기준을 취한다. 병합을 수행 하려면 먼저 언어 모델을 학습 하 고 사용 하 여 가능한 모든 쌍을 채점 합니다. 그런 다음, 각각의 병합에서, 트레이닝 데이터의 가능성이 가장 증가하도록 유도하는 쌍을 선택한다. 구글은 워드피스 알고리즘의 공식 구현을 공개하지 않았기 때문에, HuggingFace는 온라인 NLP 과정에서 더 직관적인 선택 척도를 제공한다: 한 쌍의 동시 발생 카운트를 트레이닝 코퍼스를 기반으로 한 쌍의 두 토큰의 발생 카운트의 곱으로 나누어 점수를 매긴다.\n' +
      '\n' +
      '\\(\\bullet\\)_Unigram tokenization._ BPE 및 WordPiece와 달리, 유니그램 토큰화[225]는 코퍼스에 대해 충분히 큰 세트의 가능한 서브스트링 또는 서브토켄으로 시작하고, 예상된 어휘 크기에 도달할 때까지 현재 어휘 내의 토큰을 반복적으로 제거한다. 선택 기준으로는 현재 어휘에서 일부 토큰을 제거했다고 가정하여 훈련 말뭉치의 산출량 증가를 계산한다. 이 단계는 트레이닝된 유니그램 언어 모델에 기초하여 수행된다. 유니그램 언어 모델을 추정하기 위해 기대-최대화(EM: expectation-maximization) 알고리즘을 적용한다. 각 반복에서, 먼저 오래된 언어 모델을 기반으로 단어의 현재 최적의 토큰화를 찾은 다음 유니그램의 확률을 재추정하여 언어 모델을 업데이트한다. 이 절차 동안, 언어 모델이 주어진 단어의 최적의 분해 방법을 효율적으로 찾기 위해 동적 프로그래밍 알고리즘(_i.e.,_ Viterbi 알고리즘)이 사용된다. 이러한 토큰화 접근법을 채택한 대표적인 모델로는 T5와 mBART가 있다.\n' +
      '\n' +
      '기존의 토큰나이저(_e.g.,_OPT[90] 및 GPT-3[55]는 GPT-2[26]의 토큰나이저를 활용하는 것이 편법이지만, 사전 트레이닝 코퍼스를 위해 특별히 설계된 토큰나이저를 사용하는 것은 특히 다양한 도메인, 언어 및 포맷으로 구성된 코퍼스에 매우 유익할 수 있다[78]. 따라서, 최근의 LLM은 종종 Byte-레벨 BPE 및 유니그램 토큰화를 포함하는 SentencePiece 라이브러리 [226]를 사용하여 사전 트레이닝 코퍼스를 위해 특별히 커스터마이징된 토큰라이저를 트레이닝한다. 주의할 점은 NFKC[227]와 같은 BPE에서의 정규화 기술이 토큰화 성능을 저하시킬 수 있다는 것이다[34, 64, 78]. 기존의 LLMs(_i.,_continual pre-training or instruction tuning)를 확장할 때, 우리는 또한 커스터마이징된 토큰라이저로 잠재적인 부작용을 인식해야 한다. 예를 들어, LLaMA는 주로 영어 텍스트들로 구성된 사전-트레이닝 코퍼스를 기반으로 BPE 토큰라이저를 트레이닝하고, 도출된 어휘는 비영어 데이터를 프로세싱하는 능력이 떨어질 수 있으며, 예를 들어, 중국어 텍스트를 생성하기 위해 더 긴 추론 레이턴시를 취할 수 있다.\n' +
      '\n' +
      '도. 도 7: 대용량 언어 모델을 사전 트레이닝하기 위한 전형적인 데이터 전처리 파이프라인의 예시.\n' +
      '\n' +
      '**데이터 품질의 영향에 대한 논의.** 사전 훈련의 경우 사전 훈련 데이터의 품질은 LLM의 모델 용량에 매우 중요합니다. 기존 연구에서는 노이즈, 독성 및 중복 데이터와 같은 저품질 코퍼스에 대한 사전 교육이 모델의 성능에 크게 영향을 미친다는 것을 보여주었다[64, 214, 216, 219]. T5[82], GLaM[112], Gopher[64]와 같은 최근 연구에서는 데이터 품질이 LLM의 용량에 미치는 영향을 조사했다. 필터링된 코퍼스와 필터링되지 않은 코퍼스에 대해 훈련된 모델의 성능을 비교함으로써, 청소된 데이터에 대한 사전 훈련 LLM이 모델 성능을 향상시킬 수 있다는 유사한 결론에 도달했다. 보다 구체적으로, 데이터의 중복은 "_double descent_"(성능이 초기에 악화되고 후속적으로 개선되는 현상을 지칭함)를 초래하거나(214, 228), 심지어 트레이닝 프로세스[214]를 압도할 수 있다. 또한, 중복 데이터는 문맥으로부터 복사하는 LLM의 능력을 저하시키는 것으로 나타났으며, 이는 문맥 내 학습을 사용하는 LLM의 일반화 능력에 더 영향을 미칠 수 있다[214]. 따라서 [56, 64, 78, 212]에서 제시된 바와 같이, 품질 필터링, 독성 필터링 및 중복제거와 같은 전처리 방법을 활용하여 사전 트레이닝 코퍼스를 주의 깊게 청소하고(섹션 4.1.2에 예시된 바와 같이), 트레이닝 프로세스의 안정성을 향상시키고 모델 성능에 영향을 주지 않도록 하는 것이 필수적이다.\n' +
      '\n' +
      '#### 4.1.3 데이터 스케줄링\n' +
      '\n' +
      '데이터 전처리 후, 가능한 LLM을 사전 훈련하기 위해 이러한 다중 소스 데이터를 예약하기 위한 적절한 전략을 설계하는 것이 필수적이다. 일반적으로 데이터 스케줄링을 위해서는 각 데이터 소스의 비율(_data mixture_)과 각 데이터 소스가 훈련을 위해 스케줄링되는 순서(_data curriculum_)의 두 가지 핵심 측면이 주의 깊게 고려되어야 한다. 다음으로 두 가지 측면에 대해 구체적으로 논의한다. 데이터 스케줄링에 대한 예시는 그림 8에 제시되어 있다.\n' +
      '\n' +
      '**데이터 혼합.** 각 종류의 데이터 원본은 LLM에 대 한 특정 용량 개발 (섹션 4.1의 논의를 참조)과 밀접한 관련이 있으므로 이러한 데이터를 혼합 하기 위해 적절 한 분포를 설정 하는 것이 중요 합니다. 데이터 혼합물은 일반적으로 전역 레벨(_i.,_전체 사전-트레이닝 데이터의 분포)로 설정되고, 또한 상이한 트레이닝 스테이지들에서 다양한 비율들로 국부적으로 설정될 수 있다. 사전 트레이닝 동안, 상이한 소스들로부터의 데이터 샘플들이 혼합물 비율들에 따라 선택될 것이다: 더 큰 가중치를 갖는 데이터 소스로부터 더 많은 데이터가 샘플링될 것이다. 전형적으로, LLaMA[57]와 같은 기존의 LLM은 사전-트레이닝 데이터로서 특정 데이터 혼합물들을 생성하기 위해 각 소스의 전체 데이터에 업샘플링 또는 다운샘플링을 채용할 수 있다. 도 6에 도시된 바와 같이, 기존의 LLM들은 사전-트레이닝 데이터를 구성하기 위해 상이한 데이터 혼합물들을 사용한다. 대표적인 모델로 LLaMA [57]의 사전 학습 데이터는 주로 웹 페이지(80% 이상)로 구성되어 있으며, GitHub와 StackExchange의 코드 중량이 6.5%, 서적이 4.5%, arXiv의 과학 데이터가 2.5%로 범용 LLMs 학습에 중요한 참조가 되었다. 또한, 특별한 데이터 혼합물은 상이한 목적을 용이하게 하기 위해 사용될 수 있다. 예를 들어, Falcon[141]은 순수 웹페이지에서 학습되며, CodeGen[86]은 코드 데이터의 양을 크게 증가시킨다. 실제로 데이터 혼합은 종종 경험적으로 결정되며 효과적인 데이터 혼합물을 찾기 위한 몇 가지 일반적인 전략을 요약하면 다음과 같다.\n' +
      '\n' +
      '\\(\\bullet\\)_데이터 원본의 다양성을 증가시킵니다._ 최근의 연구들은 특정 도메인에 대한 과도한 데이터에 대한 트레이닝이 다른 도메인에 대한 LLM의 일반화 능력을 저하시킬 것이라는 것을 경험적으로 보여주었다[35, 64]. 대조적으로, 데이터 소스 이질성(_예를 들어, 다양한 데이터 소스를 포함하는_)을 증가시키는 것은 LLM들의 다운스트림 성능을 개선하는데 중요하다[229, 230, 212]. 다른 데이터 소스의 효과를 추가로 조사하기 위해 일부 연구에서는 각 데이터 소스를 하나씩 제거하고 특별히 선별된 데이터 세트로 LLM을 사전 훈련하여 절제 실험을 수행했다[212]. 이질성이 높은 데이터 소스(예: 웹페이지)를 삭제하는 것이 이질성이 낮은 데이터 소스(예: 학술적 말뭉치)를 삭제하는 것보다 LLM의 능력에 더 심각한 영향을 미치는 것으로 나타났다.\n' +
      '\n' +
      '\\(\\bullet\\)_데이터 혼합물 최적화_._ 데이터 혼합물들을 수동으로 설정하는 것 외에도, 모델 사전-트레이닝을 개선하기 위해 데이터 혼합물들을 최적화하기 위한 여러 연구들이 제안되었다[59, 231]. 타겟 다운스트림 태스크들이 주어지면, 특징 공간[231]에서 더 높은 근접도를 갖는 사전 트레이닝 데이터 또는 다운스트림 태스크 성능에 긍정적인 영향을 제공하는 데이터를 선택할 수 있다[232]. 또한, 타겟 태스크의 의존도를 감소시키기 위해, DoReMi[59]는 먼저 주어진 초기 도메인 가중치를 사용하여 작은 참조 모델을 트레이닝한 다음, 다른 작은 프록시 모델을 트레이닝하여, 두 모델 사이의 가능성에서 가장 큰 불일치가 관찰되는 도메인을 가중시킨다. 마지막으로, 프락시 모델의 학습된 도메인 가중치를 적용하여 훨씬 더 큰 LLM을 학습한다. 보다 간단한 방법으로, 서로 다른 데이터 혼합물로 여러 개의 작은 언어 모델을 훈련시킬 수 있고, 가장 바람직한 성능으로 이어지는 데이터 혼합물을 선택할 수 있다. 그러나 이 접근법에서 이루어진 가정은 유사한 방식으로 훈련될 때 작은 모델이 모델 능력이나 행동에서 큰 모델과 유사할 것이며, 이는 실제로 항상 유지되지는 않을 수 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_표적 능력을 특수화 합니다.__ LLM의 모델 용량은 데이터 선택과 혼합에 크게 의존하며 특정 모델 능력을 향상시키기 위해 특정 데이터 소스의 비율을 높일 수 있다[64, 212]. 예를 들어, 수학적 추론 능력과 코딩 능력은 각각 더 많은 수학적 텍스트와 코드 데이터를 사용하여 훈련함으로써 특별히 향상될 수 있다. 또한, LAMBADA 데이터셋 [233]에 대한 실험 결과, 도서 데이터의 비율을 높이면 텍스트로부터 장기 종속성을 포착하는 모델 용량을 향상시킬 수 있으며, C4 데이터셋 [82]의 비율을 높이면 C4 검증 데이터셋 [64]의 성능 향상을 가져올 수 있다. 일반적으로, 그 사이의 보다 암묵적인 관계를 식별하는 것이 중요하다\n' +
      '\n' +
      '도. 도 8: LLM들을 사전 트레이닝하기 위한 데이터 스케줄링의 예시.\n' +
      '\n' +
      '데이터 소스 및 모델 기능입니다. LLM에서 수학 및 코딩과 같은 특정 기술을 향상시키거나 전문화된 LLM을 개발하기 위해 실용적인 방법은 다단계 훈련 접근법을 사용하는 것이며, 예를 들어, 일반 및 기술별 데이터는 연속 2단계로 예약될 수 있다. 여러 단계에 걸쳐 다양한 소스 또는 데이터 비율에 대해 LLM을 훈련하는 이러한 접근법은 아래에 소개될 "데이터 커리큘럼"이라고도 한다.\n' +
      '\n' +
      '**데이터 커리큘럼.** 데이터 혼합물을 준비한 후 사전 교육을 위해 특정 데이터가 LLM에 표시되는 순서를 예약하는 것이 중요합니다. 일부 경우들에서, 특정 스킬을 학습하기 위해, 스킬-세트 시퀀스에서의 학습(_예를 들어,_기본 스킬 \\(\\rightarrow\\) 목표 스킬)은 목표 스킬에만 집중된 코퍼스로부터의 직접 학습보다 우수한 것으로 나타났다[234, 235]. 교육과정 학습의 아이디어[236]에 따라 _데이터 교육과정_이 제안되어 모델 사전 훈련에 널리 사용되고 있다[234, 235, 237, 238]. LLM에 대한 사전 훈련 데이터의 다른 부분을 특정 순서로 구성하는 것을 목표로 한다. 보다 일반적으로, 사전 트레이닝 동안 상이한 소스들에 대한 데이터 비율들의 적응적 조정을 광범위하게 지칭할 수 있다. 데이터 커리큘럼에 관한 기존의 작업은 주로 특화된 코딩 LLM(_e.g.,_ CodeLLMa[235]) 또는 긴 컨텍스트 LLM(_e.g.,_ LongLLaMA[238])과 같은 지속적인 사전 훈련에 중점을 둔다. 그러나 아직까지 문헌에서 범용 LLM(_e.g.,_ LLaMA)에 대한 데이터 커리큘럼에 대한 보다 상세한 보고는 부족하다. 데이터 커리큘럼을 결정하기 위해서는 특별히 구성된 평가 벤치마크를 기반으로 LLM의 핵심 능력 개발을 모니터링한 다음 사전 교육 중에 데이터 혼합을 적응적으로 조정하는 것이 실용적인 접근법이다. 다음으로, 우리는 데이터 교육과정23의 개념이 지속적인 사전 훈련에서 어떻게 적용되는지를 소개하기 위해 세 가지 공통 능력을 예로 든다.\n' +
      '\n' +
      '각주 23: 우리는 데이터 교육과정에서 데이터 순서를 나타내기 위해 "\\(\\rightarrow\\)"이라는 기호를 활용한다. 예를 들어, "2T 웹페이지 토큰 \\(\\rightarrow\\)500B 코드 토큰들"은 LLM이 먼저 2T 웹페이지 토큰들로 트레이닝되고 이어서 500B 코드 데이터 토큰들로 트레이닝된다는 것을 의미한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Coding_. LLM의 코딩 능력을 향상시키기 위해, CodeLLAMA [235]는 LLaMA 2 [99](2T 일반 토큰 \\(\\rightarrow\\) 500B 코드 헤비 토큰)를 기반으로 개발되었으며, 코드 생성 능력을 향상시키고 자연어 이해 능력을 유지하는 것을 목표로 한다. CodeLLaMA는 또한 특정 프로그래밍 언어, 즉 CodeLLaMA-Python (2T 일반 토큰 \\(\\rightarrow\\) 500B 코드 헤비 토큰 \\(\\rightarrow\\) 100B Python 헤비 토큰)에 더 전문화된 버전을 제공 합니다.\n' +
      '\n' +
      '\\(\\bullet\\)_Mathematics_ 범용 LLM의 수학적 능력을 향상시키기 위해 Liemma[239]가 제안되었다. CodeLLaMA를 기반으로 개발되었습니다. CodeLLaMA[235]는 주로 코딩 능력에 초점을 맞추고 있지만, 수학 벤치마크 [239]에서 기본 모델 LLaMA 2보다 성능이 우수하다는 실험 결과가 나왔다. Lemma는 CodeLLaMA를 기반으로 수학 텍스트와 코드가 포함된 웹 데이터(2T 일반 토큰 \\(\\rightarrow\\) 500B 코드-헤비 토큰 \\(\\rightarrow\\) 50\\(\\sim\\)200B 수학-헤비 토큰)의 혼합에 대해 지속적으로 학습된다. Lemma의 사전-트레이닝 데이터는 또한 정규화의 형태로서 5% 일반 도메인 데이터를 포함한다는 점에 유의한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Long context_. 긴 컨텍스트 모델링은 LLM에 중요한 능력이며, 많은 연구에서 지속적인 훈련을 통해 LLM의 컨텍스트 창을 확장하는 것을 탐구했다[235, 238]. RoPE 기반 LLMs [57, 99, 240]의 위치 임베딩(_i.,_ 위치 보간)에 대한 수정으로, CodeLLaMA는 LLaMA 2의 컨텍스트 윈도우(4K 컨텍스트 윈도우를 갖는 2.5T 토큰 \\(\\rightarrow\\) 20B 토큰과 16K 컨텍스트 윈도우를 갖는)를 추가로 확장한다. LongLLaMA [238]는 외부 메모리의 도움과 고유한 훈련 목표(2K 컨텍스트 창이 있는 1T 토큰 \\(\\rightarrow\\) 8K 컨텍스트 창이 있는 10B 토큰)를 사용하여 더 긴 컨텍스트 창을 달성합니다.\n' +
      '\n' +
      '#### 4.1.4 데이터 준비 요약\n' +
      '\n' +
      '이 부분에서는 LLM에 대한 사전 훈련 데이터를 준비하기 위한 일반적인 절차와 핵심 사항을 요약하며, 다음 세 가지 측면에서 자세히 설명한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Data collection_. 사전 훈련 데이터에 다양한 데이터 소스를 포함할 것을 제안한다. Falcon [141]은 웹 페이지만으로 강력한 LLM을 훈련할 수 있음을 보여주지만, 보다 일반적인 접근 방식은 코드, 책, 과학 논문, _etc._와 같은 다양한 고품질 텍스트도 통합하는 것입니다. LLM이 특정 기술로 전문화되면 그에 따라 해당 데이터 소스의 비율이 증가해야 한다. 예를 들어, 고퍼[64] 및 친칠라[34]는 책으로부터의 데이터의 약 40%로 트레이닝된다. PaLM[44] 및 LaMDA[68]은 대략 50%의 대화형 데이터를 사용한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Data cleaning_. 데이터 수집 후, 가능한 한 품질을 향상시키기 위해 원시 코퍼스를 청소하는 것이 중요하다. 먼저, 중복제거는 기존 작업에서 많이 사용되고 있다[99, 141, 229]. 둘째, 프라이버시 염려가 있는 저품질 텍스트, 독성 콘텐츠 및 데이터는 상이한 세분성(예를 들어, 문서, 통과 또는 문장)에서 제거되어야 한다. 실제로, 휴리스틱 및 분류기 기반 방법 모두가 품질 및 독성 필터링을 위해 채용될 수 있다(_예를 들어,_CCNet[241], fastText[242], 및 Data-Juicer[243]. 셋째, 정리된 데이터로, 사전-트레이닝 데이터를 위한 포맷을 추가로 통일하거나 특정할 수 있고, SentencePiece와 같은 라이브러리들을 갖는 필터링되고 중복 제거된 코퍼스에 토큰나이저를 트레이닝함으로써 토큰화를 수행할 수 있다[226].\n' +
      '\n' +
      '\\(\\bullet\\)_Data scheduling_. 전처리된 데이터를 가지고, 다음 단계는 LLM들을 사전 트레이닝하기 위한 데이터 혼합 및 데이터의 특정 순서를 결정하는 것이다. 두 설정을 모두 결정하기 위해, 실용적인 방법은 먼저 여러 개의 후보 계획으로 여러 개의 작은 언어 모델을 훈련시킨 다음 그 중에서 좋은 계획을 선택하는 것이다[59]. 전반적으로 적합한 데이터 교육과정을 찾기가 더욱 어렵다. 실제로, 특정 평가 벤치마크에 대한 중간 모델 체크포인트의 성능을 모니터링하고 사전 훈련 동안 데이터 혼합 및 분포를 동적으로 조정할 수 있다. 이 과정에서 데이터 소스와 모델 능력 사이의 잠재적 관계를 탐색하여 데이터 커리큘럼 설계를 지도하는 것도 유용하다.\n' +
      '\n' +
      '### _Architecture_\n' +
      '\n' +
      '이 섹션에서는 LLM의 아키텍처 설계, 즉 주류 아키텍처, 사전 훈련 목표 및 세부 구성을 검토한다. 표 V는 공개 세부 정보가 있는 여러 대표적인 LLM의 모델 카드를 보여준다.\n' +
      '\n' +
      '#### 4.2.1 일반적인 아키텍처\n' +
      '\n' +
      '우수한 병렬화 가능성과 용량으로 인해 트랜스포머 아키텍처[22]는 다양한 LLM을 개발하기 위한 사실상의 백본이 되어 언어 모델을 수백 또는 수천억 개의 파라미터로 스케일링하는 것이 가능하게 되었다. 일반적으로 기존 LLM의 주류 아키텍처는 그림 9와 같이 크게 인코더-디코더, 인과 디코더, 프리픽스 디코더의 세 가지 유형으로 대별할 수 있다.\n' +
      '\n' +
      '**인코더-디코더 아키텍처.** 바닐라 Transformer 모델은 각각 인코더 및 디코더로 두 개의 변압기 블록 스택으로 구성된 인코더-디코더 아키텍처 [22]에 빌드됩니다. 인코더는 그것의 잠재 표현들을 생성하기 위한 입력 시퀀스를 인코딩하기 위해 적층된 멀티-헤드 셀프-어텐션 계층들을 채택하는 반면, 디코더는 이들 표현들에 대해 크로스-어텐션을 수행하고 타겟 시퀀스를 자동으로 생성한다. 인코더-디코더 PLM들(_e.g._, T5[82] 및 BART[24])은 다양한 NLP 태스크들에 대해 유효성을 보여주었다. 지금까지, 인코더-디코더 아키텍처, _예를 들어_, Flan-T5[69]를 기반으로 구축된 LLM은 소수에 불과하다. 우리는 섹션 4.2.6의 아키텍처 선택에 대한 자세한 논의를 남긴다.\n' +
      '\n' +
      '**인과 디코더 아키텍처.** 인과 디코더 아키텍처는 단방향 주의 마스크를 통합 하 여 각 입력 토큰이 과거 토큰 및 자체에만 참석할 수 있음을 보장 합니다. 입력 및 출력 토큰은 디코더를 통해 동일한 방식으로 처리된다. 이 아키텍처의 대표적인 언어 모델로서 인과-디코더 아키텍처를 기반으로 GPT 시리즈 모델[26, 55, 122]이 개발된다. 특히 GPT-3 [55]는 이 아키텍처의 효과를 성공적으로 입증했으며 LLM의 놀라운 맥락 내 학습 능력도 보여준다. 흥미롭게도, GPT-1[122] 및 GPT-2[26]은 GPT-3에서와 같은 우수한 능력을 나타내지 않으며, 스케일링은 이 모델 아키텍처의 모델 용량을 증가시키는 데 중요한 역할을 하는 것으로 보인다. 지금까지 OPT[90], BLOOM[78], Gopher[64] 등 기존의 다양한 LLM에 의해 LLM의 아키텍처로서 인과 디코더가 널리 채택되어 왔다. 다음에 논의되는 인과 디코더 및 프리픽스 디코더 모두가 속하는 것에 유의한다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline\n' +
      '**Model** & **Category** & **Size** & **Normalization** & **PE** & **Activation** & **Bias** & **\\#L** & **\\#H** & \\(d_{model}\\) & **MCL** \\\\ \\hline GPT3 [55] & Causal decoder & 175B & Pre LayerNorm & Learned & GeLU & \\(\\checkmark\\) & 96 & 96 & 12288 & 2048 \\\\ PanGU- \\(\\alpha\\)[84] & Causal decoder & 207B & Pre LayerNorm & Learned & GeLU & \\(\\checkmark\\) & 64 & 128 & 16384 & 1024 \\\\ OPT [90] & Causal decoder & 175B & Pre LayerNorm & Learned & ReLU & \\(\\checkmark\\) & 96 & 96 & 12288 & 2048 \\\\ PalM [56] & Causal decoder & 540B & Pre LayerNorm & RoPE & SwiGLU & \\(\\times\\) & 118 & 48 & 18432 & 2048 \\\\ BLOOM [78] & Causal decoder & 176B & Pre LayerNorm & ALiBi & GeLU & \\(\\checkmark\\) & 70 & 112 & 14336 & 2048 \\\\ MT-NLG [113] & Causal decoder & 530B & - & - & - & - & 105 & 128 & 20480 & 2048 \\\\ Gopher [64] & Causal decoder & 280B & Pre RMSNorm & Relative & - & - & 80 & 128 & 16384 & 2048 \\\\ Chinchilla [34] & Causal decoder & 70B & Pre RMSNorm & Relative & - & - & 80 & 64 & 8192 & - \\\\ Galactic [35] & Causal decoder & 120B & Pre LayerNorm & Learned & GeLU & \\(\\times\\) & 96 & 80 & 10240 & 2048 \\\\ LaMDA [68] & Causal decoder & 137B & - & Relative & GeLU & - & 64 & 128 & 8192 & - \\\\ Jurassic-1 [107] & Causal decoder & 178B & Pre LayerNorm & Learned & GeLU & \\(\\checkmark\\) & 76 & 96 & 13824 & 2048 \\\\ LLAMA [57] & Causal decoder & 65B & Pre RMSNorm & RoPE & SwiGLU & \\(\\times\\) & 80 & 64 & 8192 & 2048 \\\\ LLMaA 2 [99] & Causal decoder & 70B & Pre RMSNorm & RePE & SwiGLU & \\(\\times\\) & 80 & 64 & 8192 & 4096 \\\\ Falcon [141] & Causal decoder & 40B & Pre LayerNorm & RoPE & GeLU & \\(\\times\\) & 60 & 64 & 8192 & 2048 \\\\ CLM-130B [93] & Prefix decoder & 130B & Post DeepNorm & RoPE & GeCLU & \\(\\checkmark\\) & 70 & 96 & 12288 & 2048 \\\\ T5 [82] & Encoder-decoder & 11B & Pre RMSNorm & Relative & ReLU & \\(\\times\\) & 24 & 128 & 1024 & 512 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE V: Model cards of several selected LLMs with public configuration details. Here, PE denotes position embedding, #L denotes the number of layers, #H denotes the number of attention heads, \\(d_{model}\\) denotes the size of hidden states, and MCL denotes the maximum context length during training.\n' +
      '\n' +
      '도. 9: 세 가지 주류 아키텍처의 주의 패턴 비교. 여기서, 블루, 그린, 옐로우 및 그레이 라운드 사각형은 각각 프리픽스 토큰 간의 주의력, 프리픽스와 타겟 토큰 간의 주의력, 타겟 토큰 간의 주의력 및 마스킹된 주의를 나타낸다.\n' +
      '\n' +
      '디코더 전용 아키텍처로 변환합니다. "디코더 전용 아키텍처"를 언급할 때 명시되지 않는 한 기존 문헌에서 주로 인과적 디코더 아키텍처를 지칭한다.\n' +
      '\n' +
      '**접두사 디코더 아키텍처.** 접두사 디코더 아키텍처 (_a.k.a._, 비인과적 디코더 [244])는 인과적 디코더의 마스킹 메커니즘을 수정 하 여 접두사 토큰 [245]에 대 한 양방향 주의 및 생성 된 토큰에만 단방향 주의를 수행할 수 있도록 합니다. 이러한 방식으로, 인코더-디코더 아키텍처와 같이, 프리픽스 디코더들은 프리픽스 시퀀스를 양방향으로 인코딩하고 출력 토큰들을 하나씩 자동으로 예측할 수 있으며, 여기서 동일한 파라미터들은 인코딩 및 디코딩 동안 공유된다. 처음부터 프리-트레이닝하는 대신에, 실용적인 제안은 인과적 디코더들을 지속적으로 트레이닝한 다음 수렴을 가속화하기 위한 프리픽스 디코더들로 변환하는 것이다[29], _예를 들어_ U-PaLM[118]은 PaLM[56]으로부터 도출된다. 프리픽스 디코더를 기반으로 하는 기존의 대표적인 LLM으로는 GLM-130B[93], U-PaLM[118] 등이 있다.\n' +
      '\n' +
      '**전문가 혼합물.** 위의 세 가지 유형의 아키텍처에 대해 각 입력에 대한 신경망 가중치의 하위 집합이 희소하게 활성화되는 MoE(혼합 전문가) 스케일링을 통해 확장할 수 있습니다. _예:_ 스위치 Transformer[25] 및 GLaM[112]. 주요 장점은 MoE가 일정한 계산 비용을 유지하면서 모델 파라미터를 스케일업하는 유연한 방법이라는 것이다[25]. 전문가 수 또는 총 매개변수 크기[246]를 증가시키면 실질적인 성능 향상을 관찰할 수 있는 것으로 나타났다. 장점들에도 불구하고, 큰 MoE 모델들을 트레이닝하는 것은 라우팅 동작의 복잡하고 하드-스위칭 특성으로 인해 불안정성 문제들을 겪을 수 있다. MoE 기반 언어 모델의 학습 안정성을 높이기 위해 라우팅 모듈에서 고정밀 텐서를 선택적으로 사용하거나 더 작은 범위로 모델을 초기화하는 등의 기술이 도입되었다[25]. 보다 최근에는 GPT-4가 MoE 아키텍처를 기반으로 개발되었지만 공식 검증 없이 개발되었다는 추측이 널리 퍼지고 있다.\n' +
      '\n' +
      '**새로운 아키텍처** 기존 Transformer 아키텍처는 일반적으로 2차 계산 복잡성을 겪습니다. 이 때문에 긴 입력으로 학습하고 추론할 때 효율성이 중요한 문제가 되었다. 효율성을 향상시키기 위해, 일부 연구는 파라미터화된 상태 공간 모델들(_e.g.,_S4[247], GSS[248], 및 H3[249]), Hyea[250]와 같은 긴 컨볼루션들, 및 재귀적 업데이트 메커니즘들을 통합하는 트랜스포머-유사 아키텍처들(_e.g.,_RWKV[251] 및 RetNet[252])을 포함하는 언어 모델링을 위한 새로운 아키텍처를 고안하는 것을 목표로 한다. 이러한 새로운 아키텍처의 주요 장점은 두 가지입니다. 먼저, 이러한 모델들은 RNN들과 같이 재귀적으로 출력들을 생성할 수 있는데, 이는 디코딩 동안 단일 이전 상태만을 참조하면 된다는 것을 의미한다. 이는 종래의 트랜스포머에서와 같이 이전의 모든 상태를 재방문할 필요가 없기 때문에 디코딩 프로세스를 더 효율적으로 만든다. 둘째, 이 모델들은 트랜스포머와 같이 전체 문장을 병렬로 부호화할 수 있는 능력을 가지고 있다. 이는 토큰 단위로 문장을 인코딩해야 하는 기존의 RNN과 대조된다. 따라서, 이들은 병렬 스캔[253, 254], FFT[250, 251], Chunkwise Recurrent[252]와 같은 기술들을 갖는 GPU들의 병렬성으로부터 이익을 얻을 수 있다. 이러한 기법들은 이러한 새로운 아키텍처들을 갖는 모델들이 고도로 병렬적이고 효율적인 방식으로 트레이닝될 수 있게 한다.\n' +
      '\n' +
      '#### 4.2.2 상세 구성\n' +
      '\n' +
      '트랜스포머[22]의 출시 이후, 그 훈련 안정성, 성능 및 계산 효율을 향상시키기 위한 다양한 개선들이 제안되어 왔다. 이 부분에서는 정규화, 위치 임베딩, 활성화 함수, 주의 및 편향 등 트랜스포머의 4가지 주요 부분에 대한 해당 구성에 대해 논의할 것이다. 이 조사를 보다 독립적으로 만들기 위해 표 VI에 이러한 구성에 대한 자세한 공식을 제시한다.\n' +
      '\n' +
      '**정규화 방법.** 훈련 불안정성은 LLM을 사전 훈련하는 데 어려운 문제입니다. 이 문제를 완화하기 위해 정규화는 신경망의 훈련을 안정화하기 위해 널리 채택되는 전략이다. 바닐라 트랜스포머[22]에는 LayerNorm[256]이 채용되어 있다. 최근 LayerNorm, _e.g._, RMSNorm, DeepNorm의 대안으로 몇 가지 진보된 정규화 기법이 제안되었다.\n' +
      '\n' +
      '\\(\\bullet\\)_LayerNorm._ 초기 연구에서 BatchNorm [265]는 일반적으로 사용되는 정규화 방법이다. 그러나 가변 길이의 시퀀스 데이터와 소규모 배치 데이터를 다루는 것은 어렵다. 따라서, 레이어별 정규화를 수행하기 위해 LayerNorm[256]이 도입된다. 구체적으로, 층당 모든 활성화들에 대한 평균 및 분산은 활성화들을 재중복하고 재스케일링하기 위해 계산된다.\n' +
      '\n' +
      '\\(\\bullet\\)_RMSNorm._ LN(LayerNorm)의 트레이닝 속도를 향상시키기 위해, RMSNorm[257]은 평균 및 분산 대신에 합산된 활성화들의 RMS(root mean square)만으로 활성화들을 재스케일링함으로써 제안된다. 관련 연구는 트랜스포머 [266]에 대한 훈련 속도와 성능에서 우수성을 입증하였다. RMSNorm을 채택한 대표적인 모델로는 고퍼[64]와 친칠라[34]가 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_DeepNorm._ DeepNorm은 Deep Transformers의 훈련을 안정화시키기 위해 Microsoft[258]에 의해 제안되어 있다. 딥노밍을 잔류 연결로 사용하면 트랜스포머를 최대 1,000층까지 확장할 수 있어 안정성과 우수한 성능의 장점을 보여주었다. GLM-130B[93]에 의해 채택되었다.\n' +
      '\n' +
      '**정규화 위치.** 정규화 방법 외에도 정규화 위치도 LLM에서 중요한 역할을 합니다. 정규화 위치에는 일반적으로 다음 LN, 이전 LN 및 샌드위치 LN의 세 가지 선택이 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_Post-LN._ 포스트-LN은 잔차 블록 사이에 배치되는 바닐라 트랜스포머[22]에서 사용된다. 그러나 기존 연구에서는 출력층 근처의 큰 기울기로 인해 LN 이후의 변압기 훈련이 불안정한 경향이 있음을 발견하였다[267]. 따라서, 포스트-LN은 다른 전략들과 결합되는 것 외에는 현존하는 LLM들에서 거의 채용되지 않는다(예를 들어, GLM-130B에서 포스트-LN과 pre-LN을 결합하는 것[93]).\n' +
      '\n' +
      '\\(\\bullet\\)_Pre-LN._ Post-LN과는 다르게, pre-LN[268]은 각각의 서브-레이어 전에 적용되고, 추가적인 LN은 최종 예측 전에 배치된다. 사후 LN에 비해 사전 LN을 가진 트랜스포머는 훈련에서 더 안정적이다. 그러나, 그것은 사후 LN을 갖는 변형들보다 더 나쁜 성능을 수행한다[269]. 성능이 감소함에도 불구하고 대부분의 LLM은 훈련 안정성 때문에 여전히 사전 LN을 채택한다. 그러나 한 가지 예외는 사전 LN이 100B 매개변수 이상의 모델을 훈련할 때 GLM에서 불안정한 것으로 발견되었다는 것이다[93].\n' +
      '\n' +
      '\\(\\bullet\\)_Sandwich-LN_. 사전 LN을 기반으로 Sandwich-LN [255]는 변압기 층 출력에서 값 폭발 문제를 피하기 위해 잔여 연결 전에 추가 LN을 추가한다. 그러나 샌드위치-LN은 때때로 LLM의 훈련을 안정화하지 못하고 훈련의 붕괴로 이어질 수 있다는 것이 밝혀졌다[93].\n' +
      '\n' +
      '**활성화 함수.** 좋은 성능을 얻으려면 피드 포워드 네트워크에서도 활성화 함수를 적절하게 설정해야 합니다. 기존의 LLM에서는 GeLU 활성화[270]가 널리 사용되고 있다. 특히, 최신 LLM들(_e.g._, PaLM 및 LaMDA)에서, GLU 활성화의 변형들[262, 271]이 또한 활용되었으며, 특히 SwiGLU 및 GeGLU 변형들은 종종 실제로 더 나은 성능을 달성한다[266]. 그러나, GeLU와 비교하여, 이들은 피드-포워드 네트워크들에서 여분의 파라미터들(약 50%)을 필요로 한다[272].\n' +
      '\n' +
      '**위치 임베딩.** Transformer의 자체 주의 모듈은 순열 등변수이므로 위치 임베딩(PE)을 사용하여 시퀀스를 모델링하기 위한 절대 또는 상대 위치 정보를 주입합니다.\n' +
      '\n' +
      '\\(\\bullet\\)_절대 위치 임베딩._ 바닐라 트랜스포머[22]에서는 절대 위치 임베딩이 채용된다. 인코더 및 디코더의 바닥들에서, 절대 위치 임베딩들이 입력 임베딩들에 추가된다. 바닐라 트랜스포머[22], _i.e._에서 제안된 절대 위치 임베딩의 두 가지 변형이 있으며, 여기서 후자는 기존의 사전 훈련된 언어 모델에서 일반적으로 사용된다.\n' +
      '\n' +
      '\\(\\bullet\\)_상대 위치 임베딩._ 절대 위치 임베딩과는 달리, 키들과 쿼리들 사이의 오프셋들에 따라 상대 위치 임베딩들이 생성된다[273]. 트랜스포머-XL[274, 275]에는 상대적인 PE의 인기 있는 변종이 소개되었다. 키들과 쿼리들 사이의 어텐션 스코어들의 계산은 상대적인 위치들에 대응하는 학습가능한 임베딩을 도입하도록 수정되었다. T5[82]는 후속적으로 고퍼[64]에 의해 채택된 상대적 위치 임베딩을 더욱 단순화했다. 구체적으로, 어텐션 스코어들에 학습가능한 스칼라들을 추가하며, 여기서 스칼라들은 쿼리의 위치들과 키 사이의 거리들에 기초하여 계산된다. 절대 PE와 비교하여, 상대적 위치 임베딩을 갖는 트랜스포머들은 트레이닝, _i.e._, 외삽[264]을 위한 시퀀스들보다 더 긴 시퀀스들로 일반화될 수 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_Rotary Position Embedding._ 회전 위치 임베딩(RoPE)([263]은 각각의 키 또는 질의의 절대 위치에 기초하여 특정 회전 행렬들을 설정한다. 키들과 쿼리들 사이의 스코어들은 상대적인 위치 정보로 계산될 수 있다(표 VI). RoPE는 쿼리 및 키 벡터의 연속된 각 요소 쌍을 _a 차원_으로 결합하므로 원본 \\(d\\)-길이 임베딩에 대한 \\(d/2\\) 차원이 있습니다. 각 차원 \\(i\\in\\{1,\\ldots,d/2\\}\\)에 대해, 관련된 요소 쌍은 회전각 \\(t\\cdot\\theta_{i}\\)을 기준으로 회전하게 되며, 여기서 \\(t\\)은 위치지수를 나타내고 \\(\\theta_{i}\\)은 차원에서의 기저를 나타낸다. 정현파 위치 임베딩[22]에 이어서, RoPE는 _basis_\\(\\theta_{i}\\)을 _base_\\(b\\)(기본적으로 \\(10000\\)으로 설정됨)의 지수승으로 정의한다:\n' +
      '\n' +
      '\\[\\Theta=\\{\\theta_{i}=b^{-2(i-1)/d}|i\\in\\{1,2,\\ldots,d/2\\}\\}. \\tag{4}\\]\n' +
      '\n' +
      '또한, 최근의 연구[276]는 각 차원에 대해 한 사이클(\\(2\\pi\\))을 회전시키는데 필요한 거리를 파장으로 정의한다:\n' +
      '\n' +
      '\\[\\lambda_{i}=2\\pi b^{2(i-1)/d}=2\\pi/\\theta_{i}. \\tag{5}\\]\n' +
      '\n' +
      '우수한 성능 및 장기 붕괴 특성 때문에, RoPE는 최신 LLMs, _e.g._, PaLM[56] 및 LLMA[57]에 널리 채택된다. RoPE에 기초하여, xPos[277]은 Transformer의 병진 불변 및 길이 외삽을 더욱 개선한다. 회전각 벡터의 각 차원에서 xPos는 기저가 더 클 때 더 작은 특별한 지수 감쇠를 추가한다. 거리가 멀어질수록 훈련 시 불안정한 현상을 완화할 수 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_ALBi._ ALBi[264]는 Transformer의 외삽을 개선하기 위해 제안되었다. 상대적 포지션 임베딩과 유사하게, 그것은 상대적 포지션 임베딩에 기초하여 페널티를 갖는 주의 점수들을 편향시킨다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|l} \\hline \\hline\n' +
      '**Configuration** & **Method** & **Equation** \\\\ \\hline \\multirow{3}{*}{Normalization position} & Post Norm [22] & \\(\\mathrm{Norm}(\\mathbf{x}+\\mathrm{Sublayer}(\\mathbf{x}))\\) \\\\  & Pre Norm [26] & \\(\\mathbf{x}+\\mathrm{Sublayer}(\\mathrm{Norm}(\\mathbf{x}))\\) \\\\  & Sandwich Norm [255] & \\(\\mathbf{x}+\\mathrm{Norm}(\\mathrm{Sublayer}(\\mathrm{Norm}(\\mathbf{x})))\\) \\\\ \\hline \\multirow{3}{*}{Normalization method} & LayerNorm [256] & \\(\\frac{\\mathbf{x}-\\mu}{\\sigma}\\cdot\\gamma+\\beta\\), \\(\\mu=\\frac{1}{d}\\sum_{i=1}^{d}x_{i}\\), \\(\\sigma=\\sqrt{\\frac{1}{2}\\sum_{i=1}^{d}(x_{i}-\\mu))^{2}}\\) \\\\  & RMSNorm [257] & \\(\\frac{\\mathbf{x}}{\\mathrm{N}(\\mathbf{x})}\\cdot\\gamma\\), \\(\\mathrm{RMS}(\\mathbf{x})=\\sqrt{\\frac{1}{d}\\sum_{i=1}^{d}x_{i}^{2}}\\) \\\\  & DeepNorm [258] & LayerNorm \\((\\alpha\\cdot\\mathbf{x}+\\mathrm{Sublayer}(\\mathbf{x}))\\) \\\\ \\hline \\multirow{3}{*}{Activation function} & ReLU [259] & \\(\\mathrm{ReLU}(\\mathbf{x})=\\max(\\mathbf{x},\\mathbf{0})\\) \\\\  & GeLU [260] & \\(\\mathrm{GeLU}(\\mathbf{x})=0.5\\mathbf{x}\\otimes[1+\\mathrm{erf}(\\mathbf{x}/ \\sqrt{2})]\\), \\(\\mathrm{erf}(x)=\\frac{2}{\\sqrt{\\pi}}\\int_{0}^{x}e^{-t^{2}}dt\\) \\\\  & Swish [261] & \\(\\mathrm{Swish}(\\mathbf{x})=\\mathbf{x}\\otimes\\mathrm{sigmoid}(\\mathbf{x})\\) \\\\  & SwiGLU [262] & \\(\\mathrm{SwiGLU}(\\mathbf{x}_{1},\\mathbf{x}_{2})=\\mathrm{Swish}(\\mathbf{x}_{1} )\\otimes\\mathbf{x}_{2}\\) \\\\  & GeGLU [262] & \\(\\mathrm{GeGLU}(\\mathbf{x}_{1},\\mathbf{x}_{2})=\\mathrm{GeLU}(\\mathbf{x}_{1}) \\otimes\\mathbf{x}_{2}\\) \\\\ \\hline \\multirow{3}{*}{Position embedding} & Absolute [22] & \\(\\mathbf{x}_{i}=\\mathbf{x}_{i}+\\mathbf{x}_{i}\\) \\\\  & Relative [82] & \\(A_{ij}=\\mathbf{W}_{q}\\mathbf{x}_{i}\\mathbf{x}_{j}^{T}\\mathbf{W}_{k}^{T}+r_{i-j}\\) \\\\  & RoPE [263] & \\(A_{ij}=\\mathbf{W}_{q}\\mathbf{x}_{i}\\mathbf{R}_{\\Theta,i-j}\\mathbf{x}_{j}^{T} \\mathbf{W}_{k}^{T}=(\\mathbf{W}_{q}\\mathbf{x}_{i}\\mathbf{R}_{\\Theta,i})( \\mathbf{W}_{k}\\mathbf{x}_{j}R_{\\Theta,j})^{T}\\) \\\\  & ALBi [264] & \\(A_{ij}=\\mathbf{W}_{q}\\mathbf{x}_{i}\\mathbf{x}_{j}^{T}\\mathbf{W}_{k}^{T}-m(i-j)\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VI: Detailed formulations for the network configurations. Here, Sublayer denotes a FFN or a self-attention module in a Transformer layer, \\(d\\) denotes the size of hidden states, \\(\\mathbf{p}_{i}\\) denotes position embedding at position \\(i\\), \\(A_{ij}\\) denotes the attention score between a query and a key, \\(r_{i-j}\\) denotes a learnable scalar based on the offset between the query and the key, and \\(\\mathbf{R}_{\\Theta,t}\\) denotes a rotary matrix with rotation degree \\(t\\cdot\\Theta\\).\n' +
      '\n' +
      '키와 쿼리 간의 거리입니다. T5[82]와 같은 상대적인 위치 임베딩 방법과 달리, ALBi에서의 벌점 점수는 훈련 가능한 파라미터 없이 미리 정의된다. [264]에서의 경험적 결과들은 ALBi가 사인파 PE[22], RoPE[263], 및 T5 바이어스[82]와 같은 몇몇 인기 포지션 임베딩 방법들보다 트레이닝에 대한 것보다 긴 시퀀스들에 대해 더 나은 외삽 성능을 갖는다는 것을 보여주었다. 또한, ALBi는 BLOOM[78]에서도 훈련 안정성을 향상시킬 수 있는 것으로 나타났다.\n' +
      '\n' +
      '**알림** 어텐션 메커니즘은 변압기의 중요한 구성요소이다. 이것은 시퀀스에 걸친 토큰들이 서로 상호작용하고 입력 및 출력 시퀀스의 표현들을 컴퓨팅하게 한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Full attention_. 바닐라 트랜스포머[22]에서 어텐션 메커니즘은 시퀀스의 모든 토큰 쌍 사이의 관계를 고려하여 쌍 방향으로 수행된다. 숨겨진 상태를 쿼리, 키 및 값으로 매핑하는 스케일링된 dot-product attention를 채택합니다. 또한 Transformer는 단일 주의 대신 다중 헤드 주의를 사용 하 여 쿼리, 키 및 값을 다른 헤드에 다른 프로젝션으로 투영 합니다. 각 헤드의 출력의 연접은 최종 출력으로서 취해진다.\n' +
      '\n' +
      '\\(\\bullet\\)_Sparse attention_. 완전한 관심의 중요한 과제는 긴 시퀀스를 다룰 때 부담이 되는 2차 계산 복잡성이다. 따라서, 주의 메커니즘의 계산 복잡도를 감소시키기 위해 다양한 효율적인 트랜스포머 변형예들이 제안된다[278, 279]. 예를 들어, 국부적으로 밴딩된 희소 주의력(_i.e._, Factorized Attention[280]이 GPT-3[55]에서 채택되었다. 전체 시퀀스 대신 각 쿼리는 위치에 따라 토큰의 하위 집합에만 참석할 수 있습니다.\n' +
      '\n' +
      '\\(\\bullet\\)_Multi-query/grouped-query attention_. 다중-쿼리 어텐션은 키들 및 값들 상에서 상이한 헤드들이 동일한 선형 변환 매트릭스들을 공유하는 어텐션 변형을 지칭한다[281]. 모델 품질에서 약간의 희생만으로 더 높은 추론 속도를 달성합니다. 다중 질의 주의를 갖는 대표적인 모델로는 PaLM[56], StarCoder[98] 등이 있다. 다중-쿼리 주의와 다중-헤드 주의 사이의 절충을 만들기 위해, 그룹화된-쿼리 주의(GQA)[282]가 탐색되었다. GQA에서, 헤드들은 상이한 그룹들로 할당되고, 동일한 그룹에 속하는 헤드들은 동일한 변환 행렬들을 공유할 것이다. 특히, 최근 발표된 LLaMA 2 모델 [99]에서 GQA를 채택하여 실험적으로 검증하였다.\n' +
      '\n' +
      '\\(\\bullet\\)_FlashAttention_. 플래시어텐션[283]은 컴퓨팅 효율을 향상시키기 위해 품질을 모델링하는 기존의 대부분의 근사적 어텐션 방법과 달리 IO 인식 관점에서 GPU 상의 어텐션 모듈의 속도와 메모리 소비를 최적화하는 것을 제안한다. 최신 GPU에 다른 수준의 메모리(예: 빠른 IO를 사용하는 SRAM 및 상대적으로 느린 IO를 사용하는 HBM)가 있습니다. 플래시어텐트는 빠른 메모리 SRAM을 더 잘 사용하기 위해 입력을 블록으로 구성하고 필요한 재계산을 도입한다. CUDA에서 융합 커널로 구현된 FlashAttention은 PyTorch [197], DeepSpeed [74], Megatron-LM [75]에 통합되었다. 업데이트된 버전 FlashAttention-2 [284]는 GPU 쓰레드 블록과 워프의 작업 분할을 최적화하여 기존 FlashAttention에 비해 약 2\\(\\times\\)의 속도 향상을 가져온다.\n' +
      '\n' +
      '\\(\\bullet\\)_PagedAttention_. LLM이 서버에 배포 될 때 GPU 메모리는 주로 캐시 된 주의 키 및 값 텐서 ( _KV 캐시_ 라고 함)에 의해 점유 됩니다. 주요 이유는 입력 길이가 종종 다양하여 단편화 및 과잉 예약 문제로 이어지기 때문이다. 운영 체제에서의 고전적인 페이징 기술에 영감을 받아, PagedAttention은 배치된 LLM들의 메모리 효율 및 처리량을 개선하기 위해 제안되었다[285]. 구체적으로, PagedAttention은 각 시퀀스를 서브시퀀스로 분할하고, 이들 서브시퀀스의 KV 캐시를 비연속적인 물리 블록에 할당한다. 페이징 기법은 GPU 활용도를 높이고 병렬 샘플링에서 효율적인 메모리 공유를 가능하게 한다.\n' +
      '\n' +
      '이 모든 논의를 종합하기 위해 상세한 구성을 위한 기존 문헌의 제안을 요약한다. 더 강력한 일반화 및 훈련 안정성을 위해 계층 정규화를 위해 사전 RMSNorm을 선택하고 활성화 함수로 SwiGLU 또는 GeGLU를 선택하는 것이 좋다. 또한, 레이어를 매립한 직후에는 LN을 사용하지 않을 수 있어 성능 저하가 발생하기 쉽다. 위치 임베딩의 경우 RoPE 또는 ALBi가 긴 시퀀스에서 더 나은 성능을 발휘하기 때문에 더 나은 선택이다.\n' +
      '\n' +
      '#### 4.2.3 사전 학습 작업\n' +
      '\n' +
      '사전 훈련은 대규모 코퍼스에서 대규모 모델 매개변수로 일반 지식을 인코딩하는 핵심 역할을 한다. LLM들을 트레이닝하기 위해, 일반적으로 사용되는 두 개의 사전 트레이닝 작업들, 즉 언어 모델링 및 잡음 제거 오토인코딩이 있다.\n' +
      '\n' +
      '**언어 모델링.** 언어 모델링 작업(LM)은 디코더 전용 LLM, _예:_, GPT3 [55] 및 PaLM [56]을 사전 훈련하는 데 가장 일반적으로 사용되는 목표입니다. 수열의 토큰 \\(\\mathbf{x}=\\{x_{1},\\dots,x_{n}\\}\\)이 주어졌을 때, LM 태스크는 수열의 선행 토큰 \\(x_{<i}\\)을 기반으로 목표 토큰 \\(x_{i}\\)을 자동으로 예측하는 것을 목표로 한다. 일반적인 훈련 목표는 다음과 같은 가능성을 최대화하는 것이다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{LM}(\\mathbf{x})=\\sum_{i=1}^{n}\\log P(x_{i}|\\mathbf{x}_{<i}). \\tag{6}\\]\n' +
      '\n' +
      '대부분의 언어 태스크는 입력에 기초하여 예측 문제로 캐스팅될 수 있기 때문에, 이러한 디코더 전용 LLM은 통합 LM 방식으로 이러한 태스크를 수행하는 방법을 암시적으로 학습하는 데 잠재적으로 유리할 수 있다. 일부 연구는 또한 디코더 전용 LLM이 미세 조정 없이 다음 토큰[26, 55]을 자동 예측함으로써 특정 태스크로 자연스럽게 전달될 수 있다는 것을 밝혀냈다. LM의 중요한 변형은 _prefix 언어 모델링_ 작업으로, prefix 디코더 아키텍처를 사용하여 모델을 사전 학습하도록 설계되었습니다. 임의로 선택한 접두사 내의 토큰은 접두사 언어 모델링의 손실을 계산하는 데 사용되지 않습니다. 프리-트레이닝 동안 보여지는 동일한 양의 토큰들로, 프리픽스 언어 모델링은 언어 모델링보다 약간 더 나쁜 성능을 수행하는데, 이는 시퀀스 내의 더 적은 토큰들이 모델 프리-트레이닝을 위해 관련되기 때문이다[29].\n' +
      '\n' +
      '**디노이징 자동 인코딩.** 기존 LM 외에도 DAE(디노이징 자동 인코딩 작업)는 언어 모델을 사전 훈련하는 데 널리 사용되었습니다. [82, 24]. DAE 태스크에 대한 입력값 \\(\\mathbf{x}_{\\setminus\\hat{\\mathbf{x}}}}\\)은 랜덤하게 교체된 스팬으로 손상된 텍스트이다. 그리고 언어 모델을 학습하여 치환된 토큰 \\(\\tilde{\\mathbf{x}}\\)을 복구한다. 형식적으로, DAE의 트레이닝 목적은 다음과 같이 표시된다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{DAE}(\\mathbf{x})=\\log P(\\tilde{\\mathbf{x}}|\\mathbf{x}_{\\setminus \\tilde{\\mathbf{x}}}). \\tag{7}\\]\n' +
      '\n' +
      '그러나 DAE 작업은 LM 작업보다 구현이 더 복잡해 보인다. 그 결과, 대용량 언어 모델을 사전 훈련하는 데 널리 사용되지 않았다. 사전 트레이닝 목표로서 DAE를 취하는 기존의 LLM은 T5[82] 및 GLM-130B[93]를 포함한다. 이러한 모델은 주로 자기 회귀 방식으로 대체된 스팬을 복구하도록 훈련된다.\n' +
      '\n' +
      '**Mixture-of-Denoisers.** Mixture-of-Denoisers (MoD) [89] (UL2 손실이라고도 함)는 언어 모델을 사전 훈련 하기 위한 통합 목표로 도입 되었습니다. MoD는 LM과 DAE 목표를 모두 다른 유형의 노이즈 제거 작업, 즉 S-디노이저(LM), R-디노이저(DAE, 짧은 스팬 및 낮은 부패), X-디노이저(DAE, 긴 스팬 또는 높은 부패)로 간주한다. 세 가지 노이즈 제거 작업 중 S-denoiser는 기존의 LM 목적(식 (6))과 유사한 반면, R-denoiser와 X-denoiser는 DAE 목적(식 (7))과 유사하지만 스팬의 길이와 손상된 텍스트의 비율이 서로 다르다. 상이한 특수 토큰(_i.e._, {[R],[S],[X]})으로 시작된 입력 문장의 경우, 모델은 대응하는 디노이저를 사용하여 최적화될 것이다. MoD는 최신 PaLM 2 모델에 적용되었다[120].\n' +
      '\n' +
      '#### 4.2.4 Long Context Modeling\n' +
      '\n' +
      '실제 응용에서, PDF 처리 및 스토리 쓰기와 같은 LLM의 긴 컨텍스트 모델링 능력에 대한 요구가 증가하고 있다[286]. 많은 폐쇄 소스 LLM은 긴 텍스트 처리를 위한 전문적인 지원을 제공한다. 예를 들어, OpenAI는 128K 컨텍스트 윈도우로 GPT-4 Turbo를 방출하고, Anthropic은 200K 컨텍스트 윈도우로 Claude 2.1을 방출한다. 긴 컨텍스트 모델링 능력을 향상시키기 위해 일반적으로 스케일링 위치 임베딩과 컨텍스트 윈도우 적응이라는 두 가지 실행 가능한 방향이 있다. 다음으로 두 부분에 대해 자세히 소개한다.\n' +
      '\n' +
      '**크기 조정 위치 임베딩.** 변압기 기반 LLM은 최대 학습 길이 내에서 효과적인 위치 임베딩을 학습할 수 있습니다. 따라서, LLM들을 최대 트레이닝 길이를 초과하는 언어 태스크들에 적용할 때, 더 큰 포지션 인덱스들로 스케일링하는 것이 필요하다. 일부 특정 위치 임베딩은 T5 바이어스[82], ALiBi[264], xPos[277] 및 심지어 NoPE[287]를 포함하여 형식적으로 _외삽 능력_이라고 하는 훈련 길이를 넘어 텍스트로 일반화하는 어느 정도의 능력을 보유하는 것으로 나타났다. 그러나, 주류 포지션 임베딩 방법들 중 하나로서, RoPE는 경험적 연구들에서 제한된 외삽 능력을 나타낸다[240]. 다음에서는 RoPE를 더 긴 텍스트로 확장할 수 있는 몇 가지 방법에 대해 논의한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Direct model fine-tuning._ LLM을 긴 컨텍스트 창에 적용하기 위해 간단한 접근법은 원하는 길이의 긴 텍스트에서 모델을 직접 미세 조정하는 것이다. 컨텍스트 확장은 다단계 접근법(_e.g._, \\(2\\text{K}\\to 8\\text{K}\\to 32\\text{K}\\))에서 증가된 길이로 스케줄링될 수 있다. 효과적인 확장을 위해서는 훈련을 위해 특별히 준비된 긴 텍스트가 필요하다. 특히, 최근 일부 연구에서는 긴 컨텍스트 모델에서 학습 텍스트의 길이보다 품질이 더 중요하다는 것을 보여주었다[288]. 그러나 최근 연구에서는 긴 텍스트에 LLM을 적용할 때 미세 조정 접근법이 본질적으로 느린 경향이 있음을 강조했다.\n' +
      '\n' +
      '\\(\\bullet\\)_Position interpolation._ 이 방법은 사전-트레이닝 동안 분포 외 회전 각도들을 피하기 위해, 원래 컨텍스트 윈도우 내의 위치 인덱스들을 다운스케일링한다[240, 289]. 구체적으로, 이 방법은 모든 위치 지수에 계수 \\(L/L^{\\prime}\\)(\\(L<L^{\\prime}\\))을 곱하며, 여기서 \\(L\\)과 \\(L^{\\prime}\\)은 각각 원본 및 대상 컨텍스트 윈도우 길이를 나타낸다. 실험 결과[240]는 이 방법이 직접 모델 미세 조정의 상기 접근법에 비해 컨텍스트 윈도우를 효과적이고 효율적으로 확장할 수 있음을 보여주었다. 그러나 이 기법은 짧은 텍스트를 처리할 때 모델의 성능에 부정적인 영향을 미칠 수 있다는 점에 주목할 필요가 있다[240, 290].\n' +
      '\n' +
      '\\(\\bullet\\)_Position truncation._ 분포 외 회전 각도에 의해 제기된 문제를 완화하기 위해, 또 다른 실용적인 접근법은 최대 훈련 길이의 요구 사항을 충족시키기 위해 더 긴 상대 위치를 절단하는 것이다. 구체적으로, ReRoPE 및 LeakyReRoPE [291]은 최대 트레이닝 길이보다 작은 미리 정의된 윈도우 길이를 도입한다. 이 미리 정의된 창 내의 위치 인덱스는 유지되는 반면, 창 너머의 이러한 인덱스는 미리 정의된 창 길이로 잘리거나 최대 훈련 길이와 정렬되도록 보간된다. 이 전략은 지역 위치 관계를 예약하고 외삽 능력을 향상시킬 수 있다. 그러나 이 방법은 추가적인 계산 예산을 수용하면서 주의행렬을 두 번 계산해야 한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Base modification._ LLM은 일반적으로 Llama 2[99]에서 미리 설정된 최대 트레이닝 길이, _예._, 4096으로 트레이닝된다. 그러나, RoPE의 특정 차원에서의 파장들은 더 긴 텍스트[276]에 대한 트레이닝 길이를 초과할 수 있어서, 언어 모델들이 이들 차원들에 대해 충분한 트레이닝(_i.e._, 완전한 회전 사이클)을 겪지 않았다. 따라서, LLM들을 더 긴 텍스트들에 적응시킬 때, 특정 차원들에 대한 회전 각도들은 트레이닝 단계에서 결코 보이지 않을 것이다[292]. 고정된 회전각 \\(t\\cdot\\theta_{i}\\)이 주어지면, 더 작은 기저 \\(\\theta_{i}\\)은 더 큰 거리 \\(t\\), _i.e._를 허용하여 더 긴 텍스트의 모델링을 가능하게 한다[235, 276, 288]. 식 4의 식 \\(\\theta_{i}=b^{-2(i-1)/d}\\)에 따르면 기저값의 증가는 기저값의 감소를 이룰 수 있다. 또한, 베이스를 감소시키는 것은 또한 트레이닝 길이 미만의 모든 차원들의 파장들을 재스케일링하는 것을 도울 수 있는 반면, LLM들을 긴 컨텍스트 윈도우들에 적응시키기 위해 종종 지속적인 사전-트레이닝을 필요로 한다[292]. 최근 연구[292]는 이 두 가지 기본 수정 방법을 경험적으로 비교한 결과, 기본을 줄이는 것이 더 나은 외삽법을 입증한다는 것을 보여주었다.\n' +
      '\n' +
      '도. 10: 문맥 "_I am sleepy. I start a pot of"_의 다음 토큰에 대한 내림차순으로 어휘에 대한 확률 분포. 논의의 용이성을 위해, 이 예는 서브워드 단위 대신에 워드 단위로 주어진다.\n' +
      '\n' +
      '훈련 길이 이상의 용량은 베이스를 증가시키면 훈련 길이 내에서 더 나은 성능을 보인다.\n' +
      '\n' +
      '\\(\\bullet\\)_Basis truncation._ 베이스 수정과 유사하게, 베이스의 절단은 또한 트레이닝 길이[293]를 초과하는 파장을 갖는 단수 차원을 다루는 데 집중한다. 식 5의 정의 \\(\\lambda_{i}=2\\pi/\\theta_{i}\\)에 따르면, 큰 파장 \\(\\lambda_{i}\\)을 갖는 차원은 그에 따라 작은 기저 \\(\\theta_{i}\\)을 갖는다. 이러한 관찰에 기초하여, 이 접근법은 먼저 기저 범위 \\([a,c]\\)을 정의한다. 기저 범위가 주어지면, 기저값은 (1) \\(\\theta_{i}\\geq c\\)일 때, (2) \\(\\theta_{i}\\leq a\\)일 때, 값이 0으로 설정되고, (3) \\(a<\\theta_{i}<c\\)일 때, 값이 고정된 작은 값으로 절단된다. 기저 절단을 통해, 분포 외 회전 각도들은 더 큰 위치 인덱스들에서 회피될 수 있다. 그러나, 이러한 접근법은 긴 컨텍스트 태스크들에서 매우 잘 수행되지 않는다[293].\n' +
      '\n' +
      '**컨텍스트 창 수정.** Transformer 기반 LLM은 컨텍스트 창이 제한적이기 때문에 컨텍스트 창을 초과하는 긴 시퀀스의 전체 정보를 직접 통합하거나 활용할 수 없습니다. 한계를 완화하기 위해 아래에서 논의되는 바와 같이 긴 컨텍스트에 LLM을 적용하는 몇 가지 방법이 제안되었다.\n' +
      '\n' +
      '\\(\\bullet\\)_Parallel context window._ 융합-인-디코더 [294]에서 영감을 얻은 병렬 컨텍스트 창 방법 [295, 296]은 입력 텍스트를 처리하기 위해 분할 및 정복 전략을 채택한다. 특히, 입력 텍스트를 다수의 세그먼트로 분할하고, 각각 독립적으로 공유 위치 임베딩으로 인코딩한다. 생성 단계에서, 어텐션 마스크들은 후속 토큰들이 각각의 세그먼트 내의 이전 토큰들에 액세스할 수 있도록 수정된다. 그럼에도 불구하고, 이 방법은 상이한 세그먼트들의 순서를 구별할 수 없어, 특정 태스크들에 대한 모델 용량을 제약한다.\n' +
      '\n' +
      '\\(\\bullet\\)_\\(\\Lambda\\)-shaped context window.__bullet\\)_\\(\\Lambda\\). 일부 선행 연구는 LLMs가 이전의 모든 토큰들 중에서 시작 및 가장 가까운 토큰들에 더 큰 주의 가중치를 할당하는 경향이 있다는 것을 밝혀냈다[297, 298]. 소위 _"lost in the middle"_ 현상[299]. 이러한 관찰에 기초하여, LM-Infinite[300] 및 StreamingLLM[298]은 "\\(\\Lambda\\)-모양" 어텐션 마스크를 채용할 것을 제안하며, 이는 각각의 쿼리가 참석할 수 있는 초기 토큰들 및 가장 가까운 토큰들을 선택적으로 보존하고 그 다음 이 범위를 넘어서는 임의의 토큰들을 폐기한다. 실험들은 이 방법이 고정된 메모리를 갖는 여분의 긴 텍스트 생성을 용이하게 할 수 있음을 입증한다[298]. 그러나, 폐기된 토큰들로부터의 정보를 효과적으로 활용할 수 없기 때문에 프롬프트들에서 장거리 종속성을 모델링하는 것은 어려움을 겪을 수 있다[298].\n' +
      '\n' +
      '\\(\\bullet\\)_External memory._ 비교적 작은 토큰의 서브세트가 트랜스포머 [301]에서 대부분의 주의 패턴을 효과적으로 캡처할 수 있는 것으로 나타났으며, _즉_ 상단-\\(k\\) 주의 키는 원래의 전체 주의와 잘 근사할 수 있다. 따라서, 다수의 연구들은 과거 키들을 외부 메모리에 저장하고 \\(k\\)-NN 검색 방법을 이용하여 \\(k\\) 세대들에 가장 관련성이 높은 토큰들을 검색하도록 제안한다[302, 301, 238]. 디코더 모델의 경우, 일반적으로 이러한 상위-\\(k\\) 외부 토큰들에 액세스하기 위해 하나의 특정 계층을 채용하는 한편, 나머지 계층들에서의 정상 컨텍스트 윈도우를 여전히 채용한다[302, 238].\n' +
      '\n' +
      '바닐라 변압기를 기반으로 한 연구 외에도, 긴 텍스트를 모델링하기 위한 높은 계산 비용을 완화하기 위해 효율적인 주의 및 기타 효율적인 아키텍처를 가진 변압기 변형이 급증하고 있다. 이러한 연구는 섹션 4.2.1 및 섹션 4.2.2에서 광범위하게 논의되었다. 더 나아가, 컨텍스트 압축 및 프롬프트 기술(_예를 들어, 반복 추론[303])은 또한 모델 적응의 필요 없이 긴 텍스트 태스크[304, 305, 306, 303]를 처리하기 위한 실행 가능한 전략인 것으로 입증되었다.\n' +
      '\n' +
      '#### 4.2.5 Decoding Strategy\n' +
      '\n' +
      'LLM들이 사전 훈련된 후, LLM들로부터 적절한 출력을 생성하기 위해 특정 디코딩 전략을 채택하는 것이 필수적이다.\n' +
      '\n' +
      '**배경.** 널리 퍼진 디코더 전용 아키텍처로 논의를 시작하고 자동 회귀 디코딩 메커니즘을 소개합니다. 이러한 LLM들은 언어 모델링 태스크(수학식 6)에 기초하여 미리 트레이닝되기 때문에, 기본 디코딩 방법은 다음과 같이 형식적으로 모델링된, 이전에 생성된 토큰들에 기초하여 각 단계에서 가장 가능성 있는 토큰을 예측하는 _greedy search_이다:\n' +
      '\n' +
      '\\[x_{i}=\\operatorname*{arg\\,max}_{x}\\!P(x|\\mathbf{x}_{<i}), \\tag{8}\\]\n' +
      '\n' +
      '여기서, \\(x_{i}\\)는 \\(i\\)번째 생성 단계에서 가장 높은 확률을 갖는 토큰이다 \\(\\mathbf{x}_{<i}\\). 예를 들어, 도 10에서, 문장의 다음 토큰을 예측할 때 _"나는 졸린다. 나는 "_의 냄비를 시작한다. 그리디 검색은 현재 단계에서 가장 높은 확률을 갖는 토큰 "커피"를 선택한다. 탐욕스러운 검색은 출력이 입력에 크게 의존하는 텍스트 생성 작업들(_예를 들어, 기계 번역 및 텍스트 요약)에서 만족스러운 결과들을 달성할 수 있다[307]. 그러나, 개방형 생성 태스크(_e.g.,_스토리 생성 및 대화)의 관점에서, 탐욕 검색은 때때로 어색하고 반복적인 문장을 생성하는 경향이 있다[308].\n' +
      '\n' +
      '다른 대안적인 디코딩 전략으로서, 생성 동안 랜덤성 및 다양성을 향상시키기 위해 확률 분포에 기초하여 다음 토큰을 랜덤하게 선택하는 샘플링-기반 방법들이 제안된다:\n' +
      '\n' +
      '\\[x_{i}\\sim P(x|\\mathbf{x}_{<i}). \\tag{9}\\]\n' +
      '\n' +
      '도 10의 예에 대해, 샘플링 기반 방법들은 "물", "차", "쌀", _etc._의 나머지 단어들을 선택할 가능성을 또한 유지하면서 더 높은 확률로 단어 "커피"를 샘플링할 것이다.\n' +
      '\n' +
      '디코더 전용 아키텍처에 한정되지 않고, 이러한 두 가지 디코딩 방법은 일반적으로 유사한 방식으로 인코더-디코더 모델 및 프리픽스 디코더 모델에 적용될 수 있다.\n' +
      '\n' +
      '**탐욕 검색을 위한 개선.** 각 단계에서 가장 높은 확률을 가진 토큰을 선택하면 전체 확률이 높지만 로컬 추정이 낮은 문장을 간과할 수 있습니다. 다음으로 이 문제를 완화하기 위한 몇 가지 개선 전략을 소개한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Beam search._ 빔 검색[309]은 디코딩 과정 동안 각 단계에서 \\(n\\)(빔 크기)의 확률이 가장 높은 문장을 유지하고, 최종적으로 최상위의 확률로 생성된 응답을 선택한다. 통상적으로, 빔 크기는 3 내지 6의 범위 내에서 구성된다. 그러나, 더 큰 빔 크기를 선택하는 것은 성능 저하를 초래할 수 있다[310].\n' +
      '\n' +
      '\\(\\bullet\\)_Length penalty.__bullet\\)_Length penalty._ 빔 탐색은 짧은 문장을 선호하기 때문에 길이 패널티(_a.k.a._, 길이 정규화)를 부과하는 것은 문장 길이에 따라 문장 확률을 정규화하는(길이의 지수 멱(\\alpha\\)으로 나눈다) 이 문제를 극복하기 위해 일반적으로 사용되는 기법[311]이다.\n' +
      '\n' +
      '또한, 일부 연구자들[312]은 반복적인 생성 문제를 완화하기 위해 이전에 생성된 토큰 또는 \\(n\\)-그램의 생성에 불이익을 줄 것을 제안한다. 또한, 다양한 빔 탐색[313]은 동일한 입력에 기초하여 다양한 출력들의 세트를 생성하기 위해 레버리지될 수 있다.\n' +
      '\n' +
      '**Random Sampling을 위한 개선.** 샘플링 기반 메서드는 컨텍스트에 따라 잘못되거나 관련 없는 토큰(그림 10의 _예:_, "행복" 및 "보")을 선택할 수 있는 전체 어휘에 대해 토큰을 샘플링합니다. 생성 품질을 향상시키기 위해 확률이 매우 낮은 단어의 선택을 완화하거나 방지하기 위한 몇 가지 전략이 제안되었다.\n' +
      '\n' +
      '\\(\\bullet\\)_Temperature sampling._ 샘플링의 랜덤성을 변조하기 위해, 실용적인 방법은 어휘에 걸쳐 \\(j\\)-번째 토큰의 확률을 계산하기 위한 소프트맥스 함수의 온도 계수를 조정하는 것이다:\n' +
      '\n' +
      '\\[P(x_{j}|\\mathbf{x}_{<i})=\\frac{\\exp{(l_{j}/t)}}{\\sum_{j^{\\prime}}\\exp{(l_{j^{ \\prime}}/t)}}, \\tag{10}\\]\n' +
      '\n' +
      '여기서 \\(l_{j^{\\prime}}\\)은 각 단어의 로짓이고 \\(t\\)은 온도 계수이다. 온도 \\(t\\)를 줄이면 확률이 높은 단어를 선택할 확률은 증가하고 확률이 낮은 단어를 선택할 확률은 감소한다. \\(t\\)이 1로 설정되면 기본 랜덤 샘플링이 되며, \\(t\\)이 0에 가까워지면 그리디 검색과 동일하다. 또한, \\(t\\)이 무한대로 가면 균일 샘플링으로 퇴화된다.\n' +
      '\n' +
      '\\(\\bullet\\)_Top-\\(k\\) sampling._ 온도 샘플링과는 달리, 상위-\\(k\\) 샘플링은 더 낮은 확률을 갖는 토큰들을 직접 절단하고 상위 \\(k\\) 가장 높은 확률을 갖는 토큰들로부터의 샘플들만을 샘플링한다[314]. 예를 들어, 도 10에서, 상위-\\(5\\) 샘플링은 그들의 재스케일링된 확률들로부터 단어 "커피", "물", "차", "쌀", 및 "차이"로부터 샘플링할 것이다.\n' +
      '\n' +
      '\\(\\bullet\\)_Top-\\(p\\) sampling._ 톱-\\(k\\) 샘플링은 전체 가능성 분포를 고려하지 않기 때문에 \\(k\\)의 상수 값은 다른 컨텍스트에 적합하지 않을 수 있다. 따라서 누적 확률이 \\(p\\)[308] 이상인 가장 작은 집합에서 샘플링하여 top-\\(p\\) 샘플링(_a.k.a._, 핵 샘플링)을 제안한다. 실제로 가장 작은 집합은 생성 확률의 내림차순으로 정렬된 어휘에서 누적값이 \\(p\\)을 초과할 때까지 점진적으로 토큰을 추가하여 구성할 수 있다.\n' +
      '\n' +
      '최근 연구자들은 LLM에 대한 다른 샘플링 전략도 탐구했다. 예를 들어, \\(\\eta\\)_-sampling_[315]는 확률 분포에 기초한 동적 임계값을 도입함으로써 top-\\(p\\) 샘플링을 더욱 개선한다. 또한, _contrastive search_[316] 및 _typical sampling_[317]은 디코딩 동안 생성 코히어런스를 개선하기 위해 활용될 수 있다. 큰 모델들이 작은 모델들에 비해 중요한 토큰들에 더 높은 확률을 할당하는 경향이 발견되었기 때문에, _대비 디코딩_[318]은 그들의 로그-우도 차이들을 측정하기 위해 더 큰 LM(_e.g._, OPT-13B) 및 더 작은 LM(_e.g._, OPT-125M)을 이용한다. 이어서 확률 분포의 델타 값을 기반으로 토큰을 샘플링하여 중요한 토큰의 영향을 증폭한다. 이러한 대조적 아이디어에 기초하여, DoLa[319]는 상위 계층들이 중요한 토큰들에 더 많은 가중치를 할당하는 경향이 있기 때문에, 단일 LLM의 상이한 계층들에 걸쳐 로짓들을 대조하는 것으로 이 접근법을 더욱 확장한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Towards Wall_\n' +
      '\n' +
      '새 토큰을 생성할 때 가장 시간이 많이 걸리는 단계는 데이터 전송 및 가중치 계산에 중점을 둡니다. 주요 문제는 종종 _메모리 벽_ 문제로 언급되는 데이터 전송에 압도되는 상당한 시간입니다.\n' +
      '\n' +
      '이 문제를 해결하기 위해 연구자들은 I/O에서 바이트 수를 사용하여 GPU 메모리에서 GPU 캐시로의 데이터 전송을 공식적으로 정량화하고 FLOP 수를 측정하여 가중치 계산을 평가한다[320]. 구체적으로, \\(b\\), \\(s\\), \\(n\\), \\(d\\), \\(h\\)은 배치 크기, 시퀀스 길이, 주의 헤드 수, 각 헤드의 숨겨진 크기 및 전체 숨겨진 크기(\\(h=n\\cdot d\\))를 각각 나타낸다. 인과 디코더에서 계층별 다중 헤드 자기 주의 계산 시, 각 디코딩 단계에서 I/O 바이트와 FLOP는 각각 \\(8bsn+4bsnd+4bnd\\)과 \\(8bsnd\\)으로 표현될 수 있다[320].\n' +
      '\n' +
      '_산술 강도_는 I/O 바이트들에 대한 FLOP들의 비율로서 추가로 정의된다:\n' +
      '\n' +
      '\\[\\text{intensity}=\\frac{\\text{FLOPs}}{\\text{I/O bytes}}=\\frac{2}{1+\\frac{2}{d}+ \\frac{1}{s}} \\tag{11}\\]\n' +
      '\n' +
      '시퀀스 길이가 1024인 LLaMA 13B(\\(d=128\\))를 예로 들어 보자. 계산된 산술 강도는 \\(1.97\\)이다. 그러나 A100 80G GPU는 \\(312\\) TFLOP를 수행하고 _i.e._ 1초에 2 TB의 데이터를 전송할 수 있으며 이상적인 산술 강도는 \\(156\\)입니다. 이는 주의력 계산의 병목 현상이 데이터 전송 과정(_i.e._, 과도한 I/O 로딩)에 있음을 나타낸다.\n' +
      '\n' +
      '**디코딩 효율성 문제.** 이 부분에서는 LLM의 디코딩 효율성 문제를 간략하게 분석합니다. 전체적으로 LLM의 디코딩 과정은 오버헤드 분석을 위해 (1) 입력 시퀀스의 히든 스테이트를 계산하는 _prefill_ 스테이지와 (2) 오토-레그레시브 방식으로 토큰을 생성하고 히든 스테이트를 업데이트하는 _incremental decoding_ 스테이지의 두 단계로 나눌 수 있다[321]. 위의 _메모리 벽_ 박스에 도시된 바와 같이, 증분 디코딩 스테이지의 산술 강도는 단지 \\(1.97\\)이며, 이는 156(A100 80GB GPU의 표준 구성에 따라 계산됨)의 기대 값과는 거리가 멀다. 반면, LLaMA-13B의 경우 prefill 단계의 산술 강도는 \\(113.78\\)을 달성한다. 결과적으로, 기존 연구는 주로 두 가지 주요 접근법으로 분류할 수 있는 증분 디코딩 알고리즘의 효율성을 향상시키는 방법에 대해 조사한다.\n' +
      '\n' +
      '\\(\\bullet\\)_데이터 전송 감소_는 주로 GPU 메모리 액세스를 최적화하여 산술 강도를 높이는 데 중점을 둡니다. 섹션 4.2.2에서 소개된 바와 같이, KV 캐시는 이전 토큰들의 중복 계산을 피할 수 있고, PagedAttention은 메모리 단편화를 감소시키기 위해 KV 캐시들을 연속적인 블록들에 할당한다. 더욱이, 플래시-디코딩[322]은 키들 및 값들을 병렬로 로딩함으로써 주의력 계산의 속도를 높이며, 특히 긴 텍스트 생성에 효과적이다. 다른 대안적인 접근법으로서, 다중-쿼리 및 그룹화된-쿼리 주의는 KV 파라미터들을 공유함으로써 GPU 메모리 대역폭 오버헤드를 감소시킬 수 있다(더 적은 가중치들을 로딩함).\n' +
      '\n' +
      '\\(\\bullet\\)_디코딩 전략 최적화_는 상이한 방식으로 자동-회귀 생성 방식의 순차적 특성을 개선하는 것을 목표로 한다. 대표적인 연구로서, _추측 디코딩_[323, 324]는 먼저 컴팩트하지만 효율적인 모델(_e.g._, \\(n\\)-gram 모델 또는 작은 PLM)을 활용하여 짧은 세그먼트를 생성한 다음 LLM을 활용하여 이러한 초안을 검증하고 수정한다. 이는 생성 품질을 저하시키지 않으면서 \\(2\\times\\)에서 \\(3\\times\\)의 속도 향상을 가져올 수 있다. 연구자들은 이 접근법의 효율성을 개선하기 위해 여러 작은 모델을 결합하는 학습 기반 방법[325] 및 작은 LM을 먼저 가속하기 위해 더 작은 LM을 사용하는 단계별 가속과 같은 몇 가지 변형을 추가로 제안한다. 또한, 모든 계층을 통과하지 않고, 더 낮은 Transformer 계층에서 토큰의 생성을 가능하게 하는 토큰-레벨 조기-퇴장 기술들이 제안되었다[327]. 발전 품질을 희생하는 대가로 더 빠른 속도를 얻을 수 있습니다.\n' +
      '\n' +
      '**실제 설정.** 실제로 기존 라이브러리(_e.g._, Transformers [187]) 및 LLM의 공용 API(_e.g._, OpenAI)는 텍스트 생성의 다양한 시나리오를 제공하기 위해 다양한 디코딩 전략을 지원했습니다. 다음으로, 몇 가지 대표적인 LLM의 디코딩 설정을 제시한다:\n' +
      '\n' +
      '\\(\\bullet\\) T5[82]는 그리디 검색을 기본 설정으로 사용하고 번역 및 요약 작업에 대해 길이 패널티가 0.6인 빔 검색(빔 크기 4)을 적용한다.\n' +
      '\n' +
      '\\(\\bullet\\)_GPT-3_[55]는 모든 생성 태스크에 대해 4의 빔 크기와 0.6의 길이 페널티를 갖는 빔 탐색을 사용한다.\n' +
      '\n' +
      '[142]는 개방형 세대(open-end generation)를 위해 top-\\(k\\)(\\(k=50\\)), top-\\(p\\)(\\(p=0.9\\)), 온도 0.7의 샘플링 기반 전략을 사용한다.\n' +
      '\n' +
      '\\(\\bullet\\)_LLaMA_[57]은 특정 작업에 맞춘 다양한 디코딩 전략을 적용한다. 예를 들어, 질문 응답 작업에 대한 그리디 검색을 사용하고 코드 생성을 위해 0.1(pass@1) 및 0.8(pass@100)의 온도 설정으로 샘플링 전략을 사용한다.\n' +
      '\n' +
      '\\(\\bullet\\)_OpenAI API_는 그리디 검색 (온도를 0으로 설정 하 여), 빔 검색 (설정 best_of 포함), 온도 샘플링 (설정 온도를 포함), 핵 샘플링 (설정 top_p 포함)을 포함 하는 여러 기본 디코딩 전략을 지원 합니다. 또한, 생성의 반복 정도를 제어하기 위해 파라미터 presence_penalty와 frequency_penalty를 도입한다. OpenAI의 문서에 따르면, 그들의 API는 입력과 하이퍼-파라미터가 동일하더라도 다른 출력을 생성할 것이다. 온도를 0으로 설정하면 약간의 변동 가능성이 있지만 더 많은 결정론적 출력을 산출할 수 있다.\n' +
      '\n' +
      '#### 4.2.6 요약 및 토론\n' +
      '\n' +
      '아키텍처 및 사전-트레이닝 태스크들의 선택은 LLM들에 대해 상이한 유도성 바이어스들을 발생시킬 수 있으며, 이는 상이한 모델 용량들로 이어질 것이다. 이 부분에서는 LLM을 위한 아키텍처 선택에 대한 한 가지 열린 문제에 대해 논의한다.\n' +
      '\n' +
      '**건축 선택.** 사전 훈련된 언어 모델의 이전 문헌에서는 다양한 아키텍처의 효과에 대한 많은 논의가 있습니다. [89, 29]. 그러나 대부분의 LLM은 인과적 디코더 구조를 기반으로 개발되었으며 다른 대안들에 비해 그 이점에 대한 이론적 분석이 아직 부족하다. 다음으로 이 문제에 대한 기존 논의를 간략히 정리한다.\n' +
      '\n' +
      '\\(\\bullet\\) LM 대물렌즈를 사용하여 사전 훈련함으로써 인과적 디코더 아키텍처가 우수한 제로 샷 및 적은 샷 일반화 용량을 달성할 수 있을 것으로 보인다. 기존의 연구는 멀티-태스크 미세 조정 없이, 인과 디코더가 다른 아키텍처들보다 더 나은 제로-샷 성능을 갖는다는 것을 보여주었다[29]. GPT-3[55]의 성공은 큰 인과 디코더 모델이 좋은 소수의 샷 학습기가 될 수 있다는 것을 보여준다. 또한, 섹션 5에서 논의된 명령어 튜닝 및 정렬 튜닝은 대형 인과 디코더 모델들의 능력을 더욱 향상시키는 것으로 입증되었다[66, 67, 69].\n' +
      '\n' +
      '\\(\\bullet\\) 스케일링 법칙은 원인 디코더에서 널리 관찰되어 왔다. 모델 크기, 데이터세트 크기 및 총 계산을 스케일링함으로써, 인과적 디코더들의 성능이 실질적으로 개선될 수 있다[55, 30]. 따라서, 스케일링을 통해 인과 디코더의 모델 용량을 증가시키는 것이 중요한 전략이 되었다. 그러나, 인코더-디코더 모델들에 대한 보다 상세한 조사는 여전히 부족하고, 인코더-디코더 모델들의 성능을 대규모로 조사하기 위해 더 많은 노력이 필요하다.\n' +
      '\n' +
      '특히 인코더-디코더 아키텍처의 경우, 아키텍처 및 사전 훈련 작업의 선택이 LLM의 용량에 어떤 영향을 미치는지 분석하기 위해서는 아키텍처 및 사전 훈련 목표에 대한 논의에 대한 더 많은 연구가 필요하다. 디코더 전용 아키텍처의 효과에도 불구하고, 아키텍처 설계에 대한 보다 다양한 탐색을 할 것을 제안한다. 주요 아키텍처 외에도 섹션 4.2.2에서 논의된 LLM의 세부 구성도 주목할 가치가 있다.\n' +
      '\n' +
      '### _Model Training_\n' +
      '\n' +
      '이 부분에서 LLM을 훈련하기 위한 중요한 설정, 기술 또는 트릭을 검토한다.\n' +
      '\n' +
      '#### 4.3.1 Optimization Setting\n' +
      '\n' +
      'LLM의 매개변수 최적화를 위해 배치 학습, 학습률, 최적화기 및 학습 안정성에 일반적으로 사용되는 설정을 제시한다.\n' +
      '\n' +
      '**배치 교육.** 언어 모델 사전 훈련의 경우 기존 작업은 일반적으로 배치 크기를 많은 수(예:_ 2,048 예제 또는 4M 토큰)로 설정하여 훈련 안정성 및 처리량을 향상시킵니다. GPT-3 및 PaLM과 같은 LLM의 경우 훈련 중 배치 크기를 동적으로 증가시켜 궁극적으로 백만 규모에 도달하는 새로운 전략을 도입했다. 구체적으로 GPT-3의 배치 크기는 32K에서 3.2M 토큰으로 점차 증가하고 있다. 실험 결과는 배치 크기의 동적 스케줄이 LLM의 훈련 과정을 효과적으로 안정화시킬 수 있음을 입증하였다[56].\n' +
      '\n' +
      '**학습률.** 기존 LLM은 일반적으로 사전 교육 중에 준비 및 붕괴 전략과 유사한 학습률 일정을 채택합니다. 특히, 훈련 단계의 초기 0.1%~0.5%에서 학습률을 최대값까지 점진적으로 증가시키기 위해 선형 예열 스케쥴을 사용한다. 그런 다음, 훈련 손실이 수렴할 때까지 학습률을 최대값의 약 10%로 점진적으로 줄이는 코사인 감쇠 전략을 후속 단계에서 채택한다.\n' +
      '\n' +
      '**Optimizer.** Adam optimizer [328] 및 AdamW optimizer [329]는 1차 구배 기반 최적화를 위한 저차 모멘트의 적응 추정치를 기반으로 하는 LLMs (_e.g.,_ GPT-3) 훈련에 널리 사용 됩니다. 일반적으로 하이퍼파라미터는 \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.95\\), \\(\\epsilon=10^{-8}\\)으로 설정된다. 한편, Adafactor optimizer [330]은 훈련 중 GPU 메모리를 보존하기 위해 특별히 설계된 Adam optimizer의 변형인 LLMs(_e.,_ PaLM 및 T5) 훈련에도 활용되었다. Adafactor 최적화기의 하이퍼파라미터는 \\(\\beta_{1}=0.9\\)과 \\(\\beta_{2}=1.0-k^{-0.8}\\)로 설정되며, 여기서 \\(k\\)은 훈련 단계의 수를 나타낸다.\n' +
      '\n' +
      '**트레이닝을 안정화 합니다.* * LLM을 사전 트레이닝 하는 동안 종종 모델 붕괴를 유발할 수 있는 트레이닝 불안정 문제를 겪습니다. 이 문제를 해결하기 위해 가중치 감소 및 기울기 클리핑이 널리 사용되어 왔으며, 기존 연구[55, 78, 90, 93, 113]는 일반적으로 기울기 클리핑의 임계값을 1.0으로 설정하고 가중치 감소율을 0.1로 설정했다. 그러나 LLM의 스케일링으로 인해 훈련 손실 스파이크가 발생할 가능성이 더 높아 불안정한 훈련으로 이어진다. 이 문제를 완화하기 위해, PaLM[56] 및 OPT[90]은 스파이크의 발생 이전에 이전 체크포인트로부터 트레이닝 프로세스를 재시작하고 문제를 야기했을 수 있는 데이터를 스킵하는 간단한 전략을 사용한다. 또한, GLM[93]은 임베딩 층의 비정상적인 구배가 보통 스파이크로 이어진다는 것을 발견하고, 이를 완화하기 위해 임베딩 층 구배를 축소할 것을 제안한다.\n' +
      '\n' +
      '#### 4.3.2 Scalable Training Techniques\n' +
      '\n' +
      '모델 및 데이터 크기가 증가함에 따라 제한된 계산 자원 하에서 LLM을 효율적으로 훈련하는 것이 어려워졌다. 특히, 학습 처리량을 증가시키고 GPU 메모리에 더 큰 모델을 로드하는 두 가지 주요 기술적 문제가 해결되어야 한다. 이 부분에서는 위의 두 가지 과제, 즉 3D 병렬[75, 331, 332], ZeRO[333] 및 혼합 정밀 훈련[334]을 해결하기 위해 기존 작업에서 널리 사용되는 몇 가지 접근법을 검토하고 훈련에 활용하는 방법에 대한 일반적인 제안을 제공한다.\n' +
      '\n' +
      '**3D 병렬 처리** 3D 병렬 처리는 실제로 일반적으로 사용되는 세 가지 병렬 처리 기술, 즉 데이터 병렬 처리, 파이프라인 병렬 처리 [331, 332] 및 텐서 병렬 처리 [75]24의 조합입니다. 다음으로 세 가지 병렬 처리 기술을 소개합니다.\n' +
      '\n' +
      '각주 24: 모델 병렬성은 일부 작업에서 텐서 병렬성 및 파이프라인 병렬성을 포함하는 보다 광범위한 용어이다[75].\n' +
      '\n' +
      '\\(\\bullet\\)_Data parallelism._ 데이터 병렬성은 학습 처리량을 향상시키기 위한 가장 기본적인 접근법 중 하나이다. 여러 GPU에 걸쳐 모델 매개 변수와 최적화기 상태를 복제 한 다음 전체 학습 코퍼스를 이러한 GPU에 배포 합니다. 이러한 방식으로, 각각의 GPU는 그것에 대해 할당된 데이터를 프로세싱하기만 하면 되고, 그레디언트들을 획득하기 위해 순방향 및 역방향 전파를 수행한다. 상이한 GPU들 상의 계산된 그라디언트들은 모든 GPU들에서 모델들을 업데이트하기 위한 전체 배치의 그라디언트들을 획득하기 위해 더 집계될 것이다. 이러한 방식으로, 그라디언트들의 계산들이 상이한 GPU들에서 독립적으로 수행됨에 따라, 데이터 병렬화 메커니즘은 고도로 확장가능하여, 트레이닝 처리량을 향상시키기 위해 GPU들의 수를 증가시키는 방식을 가능하게 한다. 또한, 이 기법은 구현이 간단하며, 대부분의 기존 인기 있는 딥러닝 라이브러리는 이미 TensorFlow 및 PyTorch와 같은 데이터 병렬성을 구현했다.\n' +
      '\n' +
      '\\(\\bullet\\)_Pipeline parallelism._ 파이프라인 병렬화는 LLM의 상이한 계층들을 다수의 GPU들로 분배하는 것을 목표로 한다. 특히 Transformer 모델의 경우 파이프라인 병렬성이 연속된 계층을 동일한 GPU에 로드하여 GPU 간 연산된 은닉 상태나 기울기를 전송하는 비용을 줄일 수 있다. 그러나, 파이프라인 병렬화의 순진한 구현은 각각의 GPU가 계산을 완료하기 위해 이전 것을 기다려야 하기 때문에 더 낮은 GPU 이용률을 초래할 수 있고, _버블 오버헤드_[331]의 불필요한 비용을 초래한다. 파이프라인 병렬화에서 이러한 버블을 감소시키기 위해, GPipe[331] 및 PipeDream[332]는 파이프라인 효율을 향상시키기 위해 다수의 데이터 배치를 패딩하고 비동기 그래디언트 업데이트를 하는 기술을 제안한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Tensor parallelism._bullet\\)_Tensor parallelism. 텐서 병렬성은 다중 GPU 로딩을 위해 LLM을 분해하는 것을 목표로 하는 일반적으로 사용되는 기술이기도 하다. 파이프라인 병렬과 달리 텐서 병렬은 LLM의 텐서(파라미터 행렬)를 분해하는 데 중점을 둔다. LLM에서 행렬 곱셈 연산 \\(Y=XA\\)의 경우, 매개변수 행렬 \\(A\\)은 열로 \\(A_{1}\\)과 \\(A_{2}\\)의 두 개의 하위 행렬로 분할될 수 있으며, 이는 \\(Y=[XA_{1},XA_{2}]\\)으로 표현될 수 있다. 행렬 \\(A_{1}\\)과 \\(A_{2}\\)을 서로 다른 GPU에 배치함으로써, 행렬 곱셈 연산은 병렬로 두 GPU에서 호출되며, 최종 결과는 두 GPU의 출력을 GPU 간 통신을 통해 결합하여 얻을 수 있다. 현재, 텐서 병렬성은 여러 오픈 소스 라이브러리들, _e.g._, 메가트론-LM[75]에서 지원되어 왔으며, 더 높은 차원의 텐서들로 확장될 수 있다. 또한, Colossal-AI는 고차원 텐서[335, 336, 337]에 대해 텐서 병렬성을 구현하였으며, 특히 시퀀스 데이터에 대해 제안된 시퀀스 병렬성[338]을 구현하여 Transformer 모델의 어텐션 연산을 더욱 분해할 수 있다.\n' +
      '\n' +
      '**ZeRO.** DeepSpeed [74] 라이브러리에서 제안한 ZeRO [333] 기술은 데이터 병렬 처리에서 메모리 중복 문제에 중점을 둡니다. 앞서 언급한 바와 같이, 데이터 병렬성은 각 GPU가 모델 파라미터들, 모델 구배들 및 최적화기 파라미터들을 포함하는 LLM의 동일한 사본을 저장하도록 요구한다. 반면, 위의 모든 데이터가 각 GPU에 유지되어야 하는 것은 아니며, 이는 메모리 이중화 문제를 야기할 것이다. 이를 해결하기 위해 ZeRO 기법은 각 GPU에서 일부 데이터만 유지하는 것을 목표로 하며 나머지 데이터는 필요할 때 다른 GPU에서 검색할 수 있다. 특히 ZeRO는 데이터의 세 부분, 즉 최적화기 상태 분할, 그래디언트 분할 및 매개 변수 분할에 따라 세 가지 솔루션을 제공합니다. 실험 결과, 첫 번째 두 솔루션은 통신 오버헤드를 증가시키지 않으며, 세 번째 솔루션은 약 50%의 통신 오버헤드를 증가시키지만 GPU 수에 비례하는 메모리를 절약하는 것으로 나타났다. PyTorch는 FSDP[339]라고 불리는 ZeRO와 유사한 기술을 구현했다.\n' +
      '\n' +
      '**혼합 정밀 훈련.** 이전 PLM(_e.g._, BERT [23])에서는 FP32라고도 하는 32 비트 부동 소수점 번호가 사전 훈련에 주로 사용되었습니다. 최근, 극도로 큰 언어 모델을 사전 트레이닝하기 위해, 일부 연구[334]는 메모리 사용량 및 통신 오버헤드를 감소시키는 16 비트 부동 소수점 번호(FP16)를 활용하기 시작했다. 추가적으로, 인기 있는 NVIDIA GPU들(_e.g._, A100)이 FP32보다 두 배의 양의 FP16 연산 유닛들을 가짐에 따라, FP16의 연산 효율이 더욱 향상될 수 있다. 그러나, 기존 연구는 FP16이 최종 모델 성능에 영향을 미치는 계산 정확도의 손실로 이어질 수 있다는 것을 발견했다[64, 78]. 이를 완화하기 위해, _Brain Floating Point(BF16)_라는 대안이 트레이닝에 사용되었는데, 이는 FP16보다 더 많은 지수 비트들과 더 적은 유의 비트들을 할당한다. 사전-트레이닝을 위해, BF16은 일반적으로 표현 정확도에 대해 FP16보다 더 우수한 성능을 수행한다[78].\n' +
      '\n' +
      '**전체 교육 제안.** 실제로 위의 교육 기술, 특히 3D 병렬 처리는 종종 교육 처리량 및 큰 모델 로딩을 개선하기 위해 공동으로 사용됩니다. 예를 들어, 연구자들은 8방향 데이터 병렬, 4방향 텐서 병렬 및 12방향 파이프라인 병렬을 통합하여 384개의 A100 GPU에서 BLOOM [78]의 훈련을 가능하게 했다. 현재 DeepSpeed[74], Colossal-AI[189], Alpa[340]와 같은 오픈 소스 라이브러리는 세 가지 병렬 훈련 방법을 잘 지원할 수 있다. 메모리 중복성을 줄이기 위해 ZeRO, FSDP 및 활성화 재계산 기술 [77, 341]도 딥스피드, PyTorch 및 메가트론-LM에 이미 통합된 LLM을 훈련하는 데 사용할 수 있다. 또한, BF16과 같은 혼합 정밀 트레이닝 기법도 레버리지하여 트레이닝 효율을 향상시키고 GPU 메모리 사용량을 감소시킬 수 있는 반면, 하드웨어(_e.g._, A100 GPU)에 대한 필요한 지원이 필요하다. 대규모 모델을 학습하는 것은 시간 집약적인 프로세스이기 때문에 모델 성능을 예측하고 비정상적인 문제를 조기에 감지하는 것이 유용할 것이다. 이를 위해, GPT-4[46]는 최근에 딥 러닝 스택에 구축된 _예측 가능한 스케일링_이라는 새로운 메커니즘을 도입하여 훨씬 더 작은 모델을 가진 큰 모델의 성능 예측을 가능하게 하며, 이는 LLM 개발에 상당히 유용할 수 있다. 실제로는 주류 딥러닝 프레임워크의 지원 훈련 기술을 더 활용할 수 있다. 예를 들어, PyTorch는 부분 오프로딩을 허용하는 데이터 병렬 트레이닝 알고리즘 FSDP[339](_i.e._, fully sharded data parallel)를 지원한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline\n' +
      '**Model** & \\begin{tabular}{c} **Batch Size** \\\\ **(\\#tokens)** \\\\ \\end{tabular} & \\begin{tabular}{c} **Learning** \\\\ **Rate** \\\\ \\end{tabular} & **Warmup** & **Decay Method** & **Optimizer** & \\begin{tabular}{c} **Precision** \\\\ **Type** \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{c} **Weight** \\\\ **Decay** \\\\ \\end{tabular} & **Grad** & **Dropout** \\\\ \\hline GPT3 (175B) & 32K\\(\\rightarrow\\)3.2M & \\(6\\times 10^{-5}\\) & yes & cosine decay to 10\\% & Adam & FP16 & 0.1 & 1.0 & - \\\\ PanGu-\\(\\alpha\\) (200B) & - & \\(2\\times 10^{-5}\\) & - & - & Adam & - & 0.1 & - & - \\\\ OPT (175B) & 2M & \\(1.2\\times 10^{-4}\\) & yes & manual decay & AdamW & FP16 & 0.1 & - & 0.1 \\\\ PALM (540B) & 1M\\(\\rightarrow\\)4M & \\(1\\times 10^{-2}\\) & no & inverse square root & Adafactor & BF16 & \\(lr^{2}\\) & 1.0 & 0.1 \\\\ BLOOM (176B) & 4M & \\(6\\times 10^{-5}\\) & yes & cosine decay to 10\\% & Adam & BF16 & 0.1 & 1.0 & 0.0 \\\\ MT-NLG (530B) & 64 K\\(\\rightarrow\\)3.57M & \\(5\\times 10^{-5}\\) & yes & cosine decay to 10\\% & Adam & BF16 & 0.1 & 1.0 & - \\\\ Gopher (280B) & 3M\\(\\rightarrow\\)6M & \\(4\\times 10^{-5}\\) & yes & cosine decay to 10\\% & Adam & BF16 & - & 1.0 & - \\\\ Chinchilla (70B) & 1.5M\\(\\rightarrow\\)3M & \\(1\\times 10^{-4}\\) & yes & cosine decay to 10\\% & AdamW & BF16 & - & - & - \\\\ Galactic (120B) & 2M & \\(7\\times 10^{-6}\\) & yes & linear decay to 10\\% & AdamW & - & 0.1 & 1.0 & 0.1 \\\\ LaMDA (137B) & 256K & - & - & - & - & - & BF16 & - & - \\\\ Jurassic-1 (178B) & 32 K\\(\\rightarrow\\)3.2M & \\(6\\times 10^{-5}\\) & yes & - & - & - & - & - & - \\\\ LLMaM (65B) & 4M & \\(1.5\\times 10^{-4}\\) & yes & cosine decay to 10\\% & AdamW & - & 0.1 & 1.0 & - \\\\ LLMaM (2 70B) & 4M & \\(1.5\\times 10^{-4}\\) & yes & cosine decay to 10\\% & AdamW & - & 0.1 & 1.0 & - \\\\ Falcon (40B) & 2M & \\(1.85\\times 10^{-4}\\) & yes & cosine decay to 10\\% & AdamW & BF16 & 0.1 & - & - \\\\ GLM (130B) & 0.4M\\(\\rightarrow\\)8.25M & \\(8\\times 10^{-5}\\) & yes & cosine decay to 10\\% & AdamW & FP16 & 0.1 & 1.0 & 0.1 \\\\ TS (11B) & 64K & \\(1\\times 10^{-2}\\) & no & inverse square root & Adafactor & - & - & - & 0.1 \\\\ ERNE 3.0 Titan (260B) & - & \\(1\\times 10^{-4}\\) & - & - & Adam & FP16 & 0.1 & 1.0 & - \\\\ PanGu-\\(\\Sigma\\) (1.085T) & 0.5M & \\(2\\times 10^{-5}\\) & yes & - & Adam & FP16 & - & - & - \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VII: Detailed optimization settings of several existing LLMs.\n' +
      '\n' +
      '원하는 경우 CPU에 대한 학습 계산입니다.\n' +
      '\n' +
      '## 5 LLMs의 적응\n' +
      '\n' +
      '사전 훈련 후 LLM은 다양한 과제를 해결하기 위한 일반적인 능력을 습득할 수 있다. 그러나 LLM의 능력은 특정 목표에 따라 추가로 적응할 수 있다는 연구가 증가하고 있다. 이 섹션에서는 사전 훈련된 LLM을 적용하기 위한 두 가지 주요 접근법, 즉 명령어 튜닝 및 정렬 튜닝을 소개한다. 전자의 접근법은 주로 LLM의 능력을 향상(또는 잠금 해제)하는 것을 목표로 하는 반면, 후자의 접근법은 LLM의 행동을 인간의 가치 또는 선호도와 일치시키는 것을 목표로 한다. 또한, 자원 제한 설정에서 모델 적응을 위한 효율적인 튜닝 및 양자화에 대해서도 논의할 것이다. 이하에서는 네 가지 부분에 대해 구체적으로 소개하기로 한다.\n' +
      '\n' +
      '### _Instruction Tuning_\n' +
      '\n' +
      '본질적으로, 명령어 튜닝은 자연 언어의 형태로 포맷된 인스턴스들의 컬렉션 상에서 미리 트레이닝된 LLM들을 미세 조정하는 접근법이다[67]. 이는 감독된 미세 조정[66] 및 다중 태스크 프롬프트 트레이닝과 매우 관련이 있다[28]. 명령어 튜닝을 수행하기 위해서는 먼저 명령어 형식의 인스턴스를 수집하거나 구성해야 한다. 그런 다음, 이러한 포맷된 인스턴스를 사용하여 지도 학습 방식으로 LLM을 미세 조정한다(예를 들어, 시퀀스 대 시퀀스 손실을 갖는 훈련). 지시 튜닝 후, LLM은 다국어 설정에서도 보이지 않는 태스크로 일반화하는 우수한 능력을 입증할 수 있다[67, 69, 28].\n' +
      '\n' +
      '최근 설문 조사[342]는 수업 튜닝에 대한 연구에 대한 체계적인 개요를 제시한다. 이에 비해 우리는 주로 지시 튜닝이 LLM에 미치는 영향에 초점을 맞추고 수집 및 튜닝과 같은 자세한 지침이나 전략을 제공한다. 또한, 기존의 LLM들, _e.g.,_ InstructGPT[66] 및 GPT-4[46]에서 널리 적용되어 온 사용자들의 실제 요구를 만족시키기 위한 명령어 튜닝의 사용에 대해서도 논의한다.\n' +
      '\n' +
      '#### 5.1.1 형식 인스턴스 구성\n' +
      '\n' +
      '일반적으로 명령 형식 인스턴스는 작업 설명( _명령_ 이라고 함), 선택적 입력, 해당 출력 및 적은 수의 시연(선택 사항)으로 구성됩니다. 중요한 공공 자원으로서, 기존 연구들은 섹션 3.3.1에 소개된 바와 같이 자연 언어로 포맷된 다수의 라벨링된 데이터(표 III의 이용 가능한 자원 목록 참조)를 공개하였다. 다음으로, 포맷된 인스턴스를 구성하기 위한 세 가지 주요 방법(도 11의 예시 참조)을 소개한 다음 인스턴스 구성을 위한 몇 가지 주요 요소에 대해 논의한다.\n' +
      '\n' +
      '**NLP 작업 데이터 세트 형식 지정** 명령어 튜닝이 제안되기 전에 여러 초기 연구 [343, 168, 344]에서 다양한 범위의 기존 NLP 작업(예: 텍스트 요약, 텍스트 분류 및 번역)에서 인스턴스를 수집하여 감독된 다중 작업 학습 데이터 세트를 만들었습니다. 명령어 튜닝 인스턴스의 주요 소스로서, 이러한 다중 작업 훈련 데이터 세트를 자연어 작업 설명으로 포맷하는 것이 편리하다. 구체적으로, 최근의 작업[28, 66, 67, 88]은 라벨링된 데이터세트들을 인간-기입된 작업 설명들로 증강시키며, 이는 LLM들이 작업 목표를 설명함으로써 작업들을 이해하도록 지시한다. 예를 들어, 도 11(a)에서는 질문-답변 태스크에서 각 예별로 태스크 설명 _"이 질문에 답해 주세요."_가 추가된다. 지시 튜닝 후에, LLM은 그들의 태스크 설명을 따라 다른 보이지 않는 태스크들로 잘 일반화할 수 있다[28, 67, 69]. 특히, LLMs [67]: 레이블링된 데이터셋에 대해 태스크 서술이 제거된 모델을 미세 조정함으로써, 명령어가 태스크 일반화 능력에 중요한 요소임을 보여줌으로써, 모델 성능이 급격히 떨어지는 결과를 가져온다. 명령어 튜닝을 위해 라벨링된 인스턴스를 더 잘 생성하기 위해, 다양한 데이터 세트에 대한 작업 설명을 효과적으로 생성, 공유 및 검증하기 위해 크라우드 소싱 플랫폼인 프롬프트 소스[167]가 제안되었다. 훈련 인스턴스를 풍부하게 하기 위해, 여러 연구[28, 345, 168]는 또한 명령어 튜닝을 위해 특별히 설계된 태스크 설명으로 기존 인스턴스의 입력-출력 쌍을 반전시키려고 시도한다. 예를 들어, 질문-답변 쌍이 주어지면, 우리는 답변-조건화 질문을 예측함으로써 새로운 인스턴스를 생성할 수 있다(_예를 들어,_"_ 답변:_에 기초하여 질문을 생성해 주세요").\n' +
      '\n' +
      '**일별 채팅 데이터 형식 지정** 많은 수의 교육 인스턴스가 명령으로 형식화 되었음에도 불구하고 주로 명령 다양성이 부족하거나 실제 인간의 요구와 일치하지 않는 공용 NLP 데이터 세트에서 비롯 됩니다. 이를 극복하기 위해, InstructGPT[66]는 실제 사용자들이 OpenAI API에 제출한 질의들을 태스크 설명으로 채택할 것을 제안한다. 또한 작업 다양성을 풍부하게 하기 위해 인간 레이블러에게 개방형 세대, 개방형 질문 응답, 브레인스토밍 및 채팅을 포함한 실제 작업에 대한 지침을 작성하도록 요청한다. 그런 다음 다른 레이블러 그룹이 출력으로 이 지침에 직접 응답하도록 합니다. 마지막으로, 이들은 트레이닝 인스턴스로서 하나의 명령어(_i, 즉, 수집된 사용자 질의)와 예상 출력(_i, 즉, 인간-작성된 답변)을 페어링한다. 또한, InstructGPT는 정렬 튜닝을 위해 자연 언어로 포맷된 이러한 실세계 태스크를 사용한다는 점에 유의한다(섹션 5.2에서 논의). 또한 GPT-4 [46]은 잠재적으로 고위험 지침을 설계하고 안전 문제에 대한 감독된 미세 조정을 통해 이러한 지침을 거부하도록 모델을 안내했다. 고품질 공개 채팅 데이터의 부재를 고려하여 여러 연구에서도 사용자의 채팅 요청을 입력 데이터로 수집한 후 ChatGPT 또는 GPT-4를 활용하여 응답을 출력 데이터로 생성하였다. 이러한 데이터세트의 주목할만한 예는 ShareGPT[148]로부터의 대화 데이터이다. 추가적으로, 돌리[172] 및 오픈어시스턴트[173]는 그들의 대화 데이터를 추가로 공개했는데, 이는 높은 수준의 품질을 달성하기 위해 인간 주석기에 의해 주의 깊게 라벨링되었다.\n' +
      '\n' +
      '**합성 데이터 형식 지정.** 인간 주석 또는 수동 수집의 부담을 줄이기 위해 다양한 작업 설명 및 인스턴스를 합성하기 위해 기존 인스턴스를 LLM에 공급하여 인스턴스를 구성하는 여러 반자동 접근법 [143]이 제안되었습니다. 그림 11(c)와 같이 Self-Instruct 방법은 초기 작업 풀로 175개의 인스턴스만 있으면 된다. 그런 다음 풀에서 몇 가지 인스턴스를 데모로 무작위로 선택하고 LLM에 새로운 명령어와 해당 입력-출력 쌍을 생성하도록 프롬프트한다. 품질 및 다양성 필터링 후 새로 생성된 인스턴스가 작업 풀에 추가됩니다. 따라서 합성 방법은 LLM에 대한 대규모 명령 데이터를 생성하는 효과적이고 경제적인 방법이다. 그러나 Self-Instruct 방법에 의해 생성된 인스턴스는 단순하거나 다양성이 부족할 수 있다. 위저드LM [346]은 합성 정보의 품질을 개선하기 위해 인스턴스의 복잡성과 다양성을 풍부하게 하기 위해 진화하는 심층적이고 심층적인 범위를 제안함으로써 Evol-Instruct를 소개한다. 또한, Self-Align[347]은 합성된 인스턴스를 필터링하기 위해 다중 인간 정렬 원리를 확립한다. 그런 다음, 이러한 인스턴스를 사용하여 LLM을 학습하여 정렬된 인스턴스를 더 많이 생성합니다. 인스턴스 출력의 품질을 향상시키기 위해 연구자들은 직접 사람이 쓴 텍스트를 출력으로 채택하고 ICL 예제를 사용하여 해당 지침을 합성한다[348].\n' +
      '\n' +
      '**인스턴스 구성의 주요 요인.** 명령 인스턴스의 품질은 모델의 성능에 중요한 영향을 미칩니다. 여기서는 건설과 같은 몇 가지 필수 요소에 대해 논의한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Scaling the instructions._ 작업의 수를 스케일링하는 것이 LLM의 일반화 능력을 크게 향상시킬 수 있다는 것이 널리 밝혀졌다[28, 67, 88]. 작업 수가 증가함에 따라 모델 성능은 초기에 지속적인 성장 패턴을 보이는 반면, 이득은 일정 수준에 도달하면 무시할 수 있게 된다[69, 88]. 그럴듯한 추측은 특정 수의 대표 태스크가 비교적 충분한 지식을 제공할 수 있고 더 많은 태스크를 추가하는 것은 추가적인 이득을 가져오지 않을 수 있다는 것이다[69]. 또한, 길이, 구조, 및 창의성과 같은 여러 측면에서 과제 설명의 다양성을 향상시키는 것이 유익하다[28]. 태스크당 인스턴스의 수는, 소수의 인스턴스가 특정 태스크를 수행하기 위해 모델의 일반화 성능을 보통 포화시킬 수 있는 것으로 밝혀졌다[67, 69]. 특히, 최근 몇 가지 작업[349, 350]은 소량의 고품질 명령 데이터(예: 1개 또는 몇 천 개의 인스턴스)로 미세 조정의 효과를 탐색하여 평가 작업에 매우 유망한 결과를 보여준다. 대조적으로, 또 다른 연구 라인은 수업 데이터의 스케일링 효과를 계속 탐구한다[351, 352]. 예를 들어 Orca [351]은 단계별 설명으로 합성된 인스턴스를 500만 개로 확장하며, 명령 데이터로 조정된 메서드에 비해 광범위한 작업에 걸쳐 우수한 성능을 달성합니다.\n' +
      '\n' +
      '\\(\\bullet\\)_Formatting design._ 중요한 요소로서 자연어 형식의 설계는 LLM의 일반화 성능에도 큰 영향을 미친다[88]. 일반적으로 기존 데이터 세트의 입력-출력 쌍에 작업 설명 및 선택적 데모를 추가할 수 있습니다. 여기서 작업 설명은 LLM이 작업을 이해하는 데 가장 중요한 부분입니다 [88]. 또한, 적절한 수의 예시들을 시연들로서 사용함으로써 실질적인 개선들로 이어질 수 있고[69], 이는 또한 수업 공학에 대한 모델 민감도를 완화시킨다[67, 69]. 그러나, 다른 컴포넌트들(_예를 들어, 회피할 사항들, 이유들, 및 제안들)을 명령들에 통합하는 것은 LLM들의 성능에 무시할 수 있거나 심지어 악영향을 미칠 수 있다[88, 166]. 최근 LLM의 단계별 추론 능력을 이끌어내기 위해, 일부 작업[69]은 산술 추론과 같은 일부 추론 데이터 세트에 대한 CoT(chain-of-thought) 예제를 포함하는 것을 제안한다. CoT 및 비-CoT 예들을 모두 갖는 LLM들을 미세 조정하는 것은 멀티-홉 추론 능력(_예를 들어,_상식 질문 응답 및 산술 추론)을 필요로 하는 것뿐만 아니라 그러한 추론 방식(_예를 들어,_감성 분석 및 추출 질문 응답)을 필요로 하지 않는 것들을 포함하는 다양한 추론 태스크들에 걸쳐 양호한 성능을 유도할 수 있다는 것이 밝혀졌다[69, 95].\n' +
      '\n' +
      '요약하면, 잘 수행되는 InstructGPT[66] 및 LLAMA-2-Chat[99]가 플란 시리즈 LLMs[67, 69]보다 더 적지만 더 다양한 명령(또는 인스턴스)을 사용하기 때문에 명령의 다양성과 품질이 인스턴스 수[349]보다 더 중요한 것으로 판단된다. 그러나, 많은 양의 트레이닝 데이터는 고품질의 데이터가 없는 것을 보상할 수 있다[351]. 또한, 데이터세트-특정 태스크를 사용하는 것보다 라벨러를 사람이 필요로 하는 태스크를 구성하도록 초대하는 것이 더 유용하다. 그러나 여전히 사람이 필요로 하는 인스턴스에 주석을 달기 위한 일반적인 지침이 부족하여 작업 구성을 어떻게든 휴리스틱하게 만든다. 인간의 노력을 줄이기 위해 기존 포맷 데이터 세트를 재사용하거나(표 III), 기존 LLM을 사용하여 명령을 자동으로 구성할 수 있다[143]. 우리는 보여주기 위해 예비 실험을 수행합니다.\n' +
      '\n' +
      '도. 도 11: 명령어-포맷된 인스턴스들을 구성하기 위한 인스턴스 포맷팅 및 세 가지 상이한 방법들의 예시.\n' +
      '\n' +
      '5.1.4절에서 다양한 공법의 효과.\n' +
      '\n' +
      '#### 5.1.2 명령어 튜닝 전략\n' +
      '\n' +
      '사전-훈련과 달리, 명령어 튜닝은 훈련에 적당한 수의 인스턴스들만이 사용되기 때문에 종종 더 효율적이다. 명령어 튜닝이 감독 트레이닝 프로세스로서 고려될 수 있기 때문에, 그의 최적화는 트레이닝 목적(_i.e.,_ 시퀀스 대 시퀀스 손실) 및 최적화 구성(_e.e.,_ 더 작은 배치 크기 및 학습 속도)과 같은 여러 양태들[69]에서 사전 트레이닝과는 상이하며, 이는 실제에서 특별한 주의를 요한다. 이러한 최적화 구성 외에도 명령어 튜닝을 위해 고려해야 할 네 가지 중요한 측면도 있다:\n' +
      '\n' +
      '**데이터 배포의 균형 조정** 명령 조정에는 여러 작업의 혼합이 포함되므로 미세 조정 중에 여러 작업의 비율을 조정하는 것이 중요합니다. 널리 사용되는 방법은 _예제-비례 혼합_ 전략 [82], _즉, 모든 데이터 집합을 결합 하 고 혼합 된 데이터 집합에서 각 인스턴스를 동일하게 샘플링 하는 것입니다. 더욱이, 고품질 컬렉션들의 샘플링 비율(_예를 들어,_FLAN[67] 및 P3[167])을 증가시키는 것은 일반적으로 최근의 발견들[95, 69]에 따른 성능 개선으로 이어질 수 있다. 또한, 명령 튜닝 동안 데이터세트가 포함할 수 있는 예들의 최대 수를 제어하기 위해 _최대 캡_을 설정하는 것이 일반적이다[82]. 이는 더 큰 데이터세트들이 전체 분포를 압도하는 것을 방지하기 위해 설정된다[82, 95]. 실제로, 최대 캡은 전형적으로 상이한 데이터세트들에 따라 수 천 또는 수만 개로 설정된다[67, 69]. 최근, 기존의 명령어 데이터세트(표 III)는 주로 특정 측면에서 LLM의 능력을 향상시키는 데 초점을 맞추고 있으며, 단일 데이터세트만으로는 모델 용량의 포괄적인 향상으로 이어질 수 없다는 것이 경험적으로 밝혀졌다[353]. 따라서, NLP 태스크 데이터(_e.g.,_FLAN v2[292]), 채팅 데이터(_e.g.,_ShareGPT[148]) 및 합성 데이터(_e.g.,_GPT4-Alpaca[354])를 포함하는 상이한 용량에서 균형 잡힌 개선을 달성하기 위해 기존 명령어 데이터 세트의 혼합물을 사용하는 것이 종종 제안된다.\n' +
      '\n' +
      '**명령어 튜닝과 사전 학습 결합.** 튜닝 프로세스를 보다 효과적이고 안정적으로 만들기 위해 OPT-IML [95]는 명령어 튜닝 중에 사전 학습 데이터를 통합하며, 이는 모델 튜닝을 위한 정규화로 간주할 수 있습니다. 또한, 별도의 이단계 프로세스(_pre-training_ then _instruction tuning_)를 사용하는 대신에, 일부 연구는 멀티-태스크 학습을 사용하여 사전-트레이닝 데이터(_i.e.,_평문) 및 명령어 튜닝 데이터(_i.e.,_형식화된 데이터세트)의 혼합으로 처음부터 모델을 트레이닝하려고 시도한다[82]. 구체적으로, GLM-130B[93] 및 Galactica[35]는 사전-훈련 말뭉치의 작은 비율로서 명령어-형식화된 데이터 세트를 사전-훈련 LLM에 통합하며, 이는 잠재적으로 사전-훈련 및 명령어 튜닝의 이점을 동시에 달성한다.\n' +
      '\n' +
      '**다단계 명령 튜닝.** 명령 튜닝을 위해 중요한 명령 데이터에는 태스크 형식 명령과 일상 채팅 명령의 두 가지 종류가 있습니다. 일반적으로, 전자는 후자에 비해 상당히 큰 부피를 갖는다. 두 종류의 교육 데이터와 교육의 균형을 맞추는 것이 중요하다. 다른 명령어 데이터를 신중하게 혼합하는 것 외에도 LLM이 먼저 대규모 태스크 형식 명령어로 미세 조정된 다음 일일 채팅에서 미세 조정된 다단계 명령어 튜닝 전략[352]을 채택할 수 있다. 용량 잊기 문제를 방지 하려면 두 번째 단계에서 작업 형식 지침을 추가 하는 것도 유용 합니다. 실제로, 이러한 다단계 튜닝 전략은 명령어 튜닝을 위한 다른 설정에도 적용될 수 있다. 예를 들어, 난이도와 복잡도에 대해 점진적으로 증가된 수준으로 다양한 미세 조정 단계를 예약할 수 있으며 복잡한 지침을 따르는 LLM의 용량을 점진적으로 개선할 수 있다.\n' +
      '\n' +
      '**기타 실용적인 트릭.** 실제로 LLM의 미세 조정 성능을 향상시키는 데 도움이 되는 몇 가지 유용한 전략과 트릭도 있습니다. 다음과 같이 몇 가지 대표적인 항목을 나열합니다.\n' +
      '\n' +
      '\\(\\bullet\\)_멀티턴 채팅 데이터에 대한 효율적인 교육입니다._ 멀티-턴 채팅 예(사용자와 챗봇 간의 대화)가 주어지면, 간단한 미세 조정 방법은 훈련을 위해 이를 다수의 컨텍스트-응답 쌍으로 분할하는 것이다: LLM은 모든 분할에 대한 대응하는 컨텍스트에 기초하여 응답을 생성하도록 미세 조정된다(사용자로부터의 각각의 발화에서의_i.,_). 이러한 미세 조정 방식으로, 대화로부터의 분할 예들에서 중복되는 발언들이 존재하는 것이 명백하다. 훈련 비용을 절약하기 위해, Vicuna[138]는 전체 대화를 LLM에 공급하는 효율적인 방법을 채택했지만, 훈련을 위해 챗봇의 응답에 대한 손실만을 계산하는 손실 마스크에 의존한다. 이는 중첩된 발화로부터 도출되는 계산 비용을 상당히 줄일 수 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_LLM에 대 한 자체 식별 설정_ 실제 응용 프로그램에 대 한 LLM을 배포 하려면 ID를 설정 하 고 LLM이 이름, 개발자 및 소속과 같은 이러한 id 정보를 인식 해야 합니다. 실용적인 방법은 LLM을 미세 조정하기 위한 신원 관련 지침을 만드는 것이다. 자체 식별 프롬프트로 입력을 프리픽스하는 것도 가능하다. 예를 들어, ___"다음은_개발자가 개발한_챗봇이름이라는 인간과 AI 어시스턴트 사이의 대화이다." 여기서 챗봇이름과 개발자는 각각 챗봇의 이름 및 개발자를 참조한다.\n' +
      '\n' +
      '위의 실용적인 전략 및 트릭 외에도, 기존 작업은 다른 트릭을 사용했는데, _예를 들어_ 여러 예를 단일 시퀀스로 연결하여 최대 길이에 접근한다[355].\n' +
      '\n' +
      '#### 5.1.3 명령어 튜닝의 효과\n' +
      '\n' +
      '이 부분에서 우리는 세 가지 주요 측면에서 지시 튜닝이 LLM에 미치는 영향에 대해 논의한다.\n' +
      '\n' +
      '**성능 향상.** 적당한 수의 인스턴스에서 튜닝되었음에도 불구하고 명령 튜닝은 LLM의 능력을 향상시키거나 잠금 해제하는 중요한 방법이 되었습니다. [69]. 최근의 연구들은 여러 척도들(77M에서 540B까지의 범위)에서 언어 모델들을 실험하여, 상이한 척도들의 모델들이 모두 명령어 튜닝으로부터 이익을 얻을 수 있다는 것을 보여주며[69, 345], 파라미터 척도가 증가함에 따라 향상된 성능을 산출한다[94]. 또한, 명령어 튜닝을 갖는 더 작은 모델들은 심지어 미세 조정 없이 더 큰 모델들보다 더 잘 수행할 수 있다[28, 69]. 모델 스케일 외에도, 명령어 튜닝은 다양한 모델 아키텍처들, 사전-훈련 목적들, 및 모델 적응 방법들에서 일관된 개선들을 보여준다[69]. 실제로, 명령어 튜닝은 기존 언어 모델(69)(소형 PLM 포함)의 능력을 향상시키기 위한 일반적인 접근법을 제공한다. 또한, LLM들에 의해 요구되는 명령 데이터의 양이 사전-훈련 데이터보다 상당히 작기 때문에, 사전-훈련에 비해 훨씬 비용이 적게 든다.\n' +
      '\n' +
      '**작업 일반화.** 지침 튜닝은 모델이 작업 완료를 위한 자연어 지침을 이해하도록 권장합니다. 그것은 LLM들에게 보이지 않는 작업들에서도, 시연 없이 특정 작업을 수행하기 위해 인간의 지시를 따르는 능력(종종 비상 능력으로 간주됨)을 부여한다[31]. 많은 연구에서 눈에 보이는 작업과 보이지 않는 작업 모두에서 우수한 성능을 달성하기 위한 수업 튜닝의 효과를 확인했다[95, 345]. 또한, 명령어 튜닝은 LLM들의 몇몇 약점들(_e.g._, 특정 태스크를 달성하지 않고 입력을 반복적으로 생성하거나 보완하는 것)을 완화시키는데 유용한 것으로 나타났다[66, 69]. 이는 LLM들에 대한 실세계 태스크들을 해결할 수 있는 우수한 용량으로 이어진다. 또한, 명령어 튜닝으로 훈련된 LLM은 언어 전반에 걸쳐 관련 태스크로 일반화될 수 있다. 예를 들어, BLOOMZ-P3[94]는 영어 전용 태스크 모음 P3[167]을 이용하여 BLOOM[78]을 기반으로 미세 조정된다. 흥미롭게도, BLOOMZ-P3는 BLOOM에 비해 다국어 문장 완성 태스크에서 50% 이상의 향상을 달성할 수 있으며, 이는 명령어 튜닝이 LLM이 영어 전용 데이터세트로부터 일반적인 태스크 스킬을 획득하고 그러한 스킬을 다른 언어로 전달하는 것을 도울 수 있음을 보여준다[94]. 또한, 영어 전용 명령어를 사용하면 다국어 과제[94]에서 만족스러운 결과를 얻을 수 있어 특정 언어에 대한 교수 공학의 노력을 줄이는 데 도움이 되는 것으로 밝혀졌다.\n' +
      '\n' +
      '**도메인 전문화.** 기존 LLM은 기존 NLP 작업(예:_), 생성 및 추론) 및 일일 질문에서 우수한 기능을 선보였습니다. 그러나, 이들은 여전히 의학, 법률 및 금융과 같은 특정 작업을 수행하기 위한 도메인 지식이 부족할 수 있다(상이한 응용에서 LLM에 대한 상세한 논의는 섹션 8 참조). 명령어 튜닝은 기존의 일반적인 LLM을 도메인별 전문가가 되도록 적응시키는 효과적인 접근법이다. 예를 들어, 연구자들은 의료 데이터 세트를 사용하여 플란-PaLM[69]을 미세 조정하여 전문 임상의와 유사한 성능 수준을 달성하는 의료 지식 보조자인 Med-PaLM[356]을 만들 것을 제안한다. 나아가, 최근의 연구[357]는 FLAN-T5를 미세 조정하여 자연어 명령어를 갖는 전자 상거래 추천 시스템을 지원함으로써, 다양한 추천 작업에서 강력한 성능을 보여준다. 또한 BenTsao[358]와 같이 LLaMA[57]를 기반으로 교육 조정된 여러 오픈 소스 의료 모델이 있다. 또한, 연구자들은 법칙[359], 금융[360], 산술 연산[361]에 대한 수업 튜닝을 탐구한다.\n' +
      '\n' +
      '#### 5.1.4 명령어 튜닝에 대한 경험적 분석\n' +
      '\n' +
      '다른 명령어 세트를 갖는 LLM들을 미세 조정하는 것은 다운스트림 태스크들에서 다양한 성능을 갖는 모델 변형들로 이어지는 경향이 있다. 이 절에서는 LLMs(_i.e._, LLaMA(7B) 및 LLaMA(13B)25)의 미세 조정에서 다양한 유형의 명령어의 효과를 탐색하고 몇 가지 명령 개선 전략의 유용성을 조사할 것이다.\n' +
      '\n' +
      '각주 25: 계산 자원의 한계로 인해 현재 더 큰 LLaMA 변형에 대한 대규모 실험을 수행할 수 없으며, 이는 향후 버전으로 예정되어 있다.\n' +
      '\n' +
      '**지침 데이터 세트.** 섹션 5.1.1의 논의에 따라 다음과 같이 세 가지 일반적인 지침을 주로 고려 합니다.\n' +
      '\n' +
      '* _작업별 지침**._ 첫 번째 명령어 유형에는 선행 작업에서 얻은 4개의 데이터 혼합물을 결합하여 1,836개의 태스크와 15M 이상의 명령어를 포함하는 가장 일반적으로 사용되는 다중 태스크 명령어 데이터 세트 _FLAN-T5_[69]를 채택한다.\n' +
      '* _매일 채팅 지침입니다._ 이러한 유형의 지침은 실생활 시나리오와 더 밀접한 일상 생활에 대해 사용자가 제기하는 대화이다. 63K 실제 사용자 지침으로 구성된 ShareGPT 계측 세트를 채택합니다. 비쿠나의 핵심 지침으로 사용되었습니다.\n' +
      '* _합성 지침**._ 기존 명령어를 재사용하는 것 외에도 LLM을 사용하여 대용량 명령어를 자동으로 합성할 수도 있습니다. 우리는 약 82K 인스턴스 입력과 출력으로 쌍을 이루는 52K 명령어로 구성된 인기 있는 합성 명령 데이터 세트 셀프 강사-52K[143]를 채택한다. 이들 생성된 명령어들은 인간-기입 시드 태스크들(_e.g._, 문법 체크, 브레인스토밍)과 유사한 데이터 분포를 갖는다.\n' +
      '\n' +
      '원래 FLAN-T5 데이터셋이 매우 크므로(_i.e._, 15M 이상), 다른 명령어 데이터셋과 공정한 비교를 수행하기 위해 무작위로 80,000개의 명령어를 샘플링한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c|c c|c c|c c|c c} \\hline \\hline \\multirow{2}{*}{**Models**} & \\multirow{2}{*}{\\begin{tabular}{c} **AS00** \\\\ \\#GPU \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} **Full** \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} **Training** \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} **AS00** \\\\ \\#GPU \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} **LoRa** \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} **Training** \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} **AS00** \\\\ \\#Tone \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} **As00** \\\\ \\#GPU \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} **Inference (16-bit)** \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} **3090** \\\\ \\#Tone \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} **Inference (16-bit)** \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} **3090** \\\\ \\#Tone \\\\ \\end{tabular} } & \\multirow{2}{*}{\n' +
      '\\begin{tabular}{c} **Inference (8-bit)** \\\\ \\end{tabular} } \\\\  & \\#GPU & & & & & & & & & & & \\\\ \\hline LLaMA (7B) & 2 & 8 & 3.0h & 1 & 80 & 3.5h & 1 & 36.6 & 1 & 24.3 & 1 & 7.5 \\\\ LLaMA (13B) & 4 & 8 & 3.1h & 1 & 48 & 5.1h & 1 & 26.8 & 2 & 9.9 & 1 & 4.5 \\\\ LLaMA (30B) & 8 & 4 & 6.1h & 1 & 24 & 14.3h & 1 & 17.7 & 4 & 3.8 & 2 & 2.6 \\\\ LLaMA (65B) & 16 & 2 & 11.2h & 1 & 4 & 60.6h & 2 & 8.8 & 8 & 2.0 & 4 & 1.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VIII: Basic statistics of the required number of GPUs, tuning time, batch size (denoted as BS) per device (full tuning and LoRA tuning), and inference rate (the number of generated tokes per second). Our experiments are conducted based on two Linux servers having 8 A800-80G SXM4 GPUs with 6 NVSwitch and 8 3090-24G GPUs, respectively. The major difference between A800 and A100 lies in the NVLink interconnect speed. Thus, our estimations about training and inference efficiency would be slightly improved for A100, while the rest memory consumption would remain the same. For full tuning experiments, we use data parallel training, ZeRO Stage 3, BF16, and gradient checkpointing. Additionally, the LoRA tuning can be executed on one 80G GPU utilizing INT8 quantization with the rank setting set to 16. All the experiments are conducted with Alpaca-52K dataset by training LLaMA models three epochs. The max sequence length for both training settings is set to 512. The inference experiments are performed with the batch size set to 1.\n' +
      '\n' +
      '(_i.e._, ShareGPT 및 Self-Instruct-52K)를 유사한 스케일로 포함하는 것을 특징으로 하는 방법. 실험에서는 각 개별 명령 집합에 대해 테스트하여 자신의 효과를 탐색하고 모델 성능에 대한 조합 효과를 조사한다.\n' +
      '\n' +
      '**개선 전략.** 인간 사용자의 실제 지침은 LLM을 미세 조정 하는 데 더 적합 하지만 대규모로 수집 하기는 어렵습니다. 인간 생성 명령어의 대안으로 기존의 대부분의 연구는 주로 LLM에 의해 생성된 합성 명령어를 채택한다. 그러나 합성 지도에는 주제 다양성의 저하와 고르지 못한 지도 어려움(너무 단순하거나 너무 어렵다)과 같은 몇 가지 잠재적인 문제가 있다. 따라서, 합성 명령어의 품질을 향상시킬 필요가 있다. 다음으로 기존 작업에서 널리 활용되는 네 가지 주요 개선 전략을 다음과 같이 요약한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Enhancing the instruction complexity._._ 기존 작업[346]에서 논의된 바와 같이, 명령어들의 복잡성을 향상시키는 것은 더 많은 태스크 요구들을 포함하거나 더 많은 추론 단계들을 필요로 하는, 다음의 복잡한 명령어들, _예를 들어_ 에서 LLM들의 모델 용량을 향상시킬 수 있다. 이 전략의 유효성을 검사 하기 위해 복잡성 수준을 점진적으로 증가 하 고 제약 조건을 추가 하 고 추론 단계를 증가 하 고 입력을 복잡 하 여 WizardLM [346]을 따릅니다. 우리는 공개되어 있는 WizardLM-70K 명령어 [346]을 Self-Instruct-52K 데이터 세트 [346]를 기반으로 위의 향상 방법을 통해 생성된 복잡도 향상 명령어 데이터 세트로 활용한다.\n' +
      '\n' +
      '\\(\\bullet\\)_토픽 다양성의 증가._ 복잡성 외에도, 명령어 데이터세트의 토픽 다양성을 개선하면 현실 세계의 다양한 태스크에 대한 LLM의 상이한 능력을 이끌어내는 데 도움이 될 수 있다[347]. 그러나 다양한 명령어를 생성하기 위한 자체 지시 프로세스를 직접 제어하는 것은 어렵다. YuLan-Chat [352]에 이어 ChatGPT를 사용하여 특정 프롬프트를 통해 293개 토픽으로 수정하기 위해 Self-Instruct-52K 데이터 세트의 지침을 다시 작성합니다. 마지막으로, 다이버시티가 증가된 데이터 세트로 70K 명령어를 획득한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Scaling the instruction number._ 상기 양태들 외에도, 명령들의 수는 또한 모델 성능에 영향을 미칠 수 있는 중요한 요소이다. 특히, 더 많은 명령어들을 사용하는 것은 LLM들에 대한 태스크 지식을 확장하고 후속 명령어들의 능력을 향상시킬 수 있다[69]. 이 전략을 조사하기 위해 MOSS 프로젝트 [362]에서 출시된 합성 명령어 집합에서 동일한 자체 지시 방법 [143]을 사용하여 합성되었기 때문에 새로운 명령어를 샘플링한다. 이를 Self-Instruct-52K 데이터 세트와 혼합하여 220K 명령어를 포함하는 더 큰 데이터 세트를 구성한다.\n' +
      '\n' +
      '_(\\bullet\\)_Balancing the difficulty._(\\bullet\\)_Balancing the instruction difficulty._(\\bullet\\) 합성 지침은 너무 쉽거나 너무 단단한 지침을 포함하는 경향이 있기 때문에 훈련 불안정성을 초래하거나 LLM에 과도하게 적합할 가능성이 있다. 잠재적 효과를 탐색하기 위해 LLM의 복잡성 점수를 활용하여 명령어의 난이도를 추정하고 너무 쉬우거나 너무 딱딱한 명령을 제거한다. 공정한 비교를 위해 동일한 크기의 명령어를 생성하기 위해 LLMA (7B) 모델을 사용하여 대규모 명령어 데이터셋에서 220K 명령어에 대한 복잡도를 계산한 다음 중간 복잡도 점수의 70K 명령어를 난이도 균형 데이터셋으로 유지한다.\n' +
      '\n' +
      '**실험 설정.** 명령 데이터의 영향에 대 한 실험을 수행 하기 위해 명령 튜닝에 널리 사용 된 인기 있는 LLM 백본인 LLAMA를 튜닝 하기 위해 이러한 새 명령 데이터 집합을 활용 합니다. 실험을 위해 YuLan-Chat [352]의 코드를 사용하고 8 A800-80G GPU의 서버에서 LLAMA 7B 및 13B를 훈련한다. 전부 다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c|c c c} \\hline \\hline \\multirow{2}{*}{**Models**} & **Dataset** & **Instruction** & **Lexical** & \\multicolumn{2}{c}{**Chat**} & \\multicolumn{2}{c}{**QA**} \\\\ \\cline{3-7}  & **Mixtures** & **Numbers** & **Diversity** & **AlpacaFarm** & **MMLU** & **BBH3k** \\\\ \\hline LLAMA (7B) & \\(\\ddagger\\) FLAN-T5 & 80,000 & 48.48 & 23.77 & 38.58 & 32.79 \\\\  & \\(\\ddagger\\) ShareGPT & 63,184 & 77.31 & 81.30 & 38.11 & 27.71 \\\\  & \\(\\ddagger\\) Self-Instruct-52K & 82,439 & 25.92 & /\\(\\star\\) & 37.52 & 29.81 \\\\  & \\(\\ddagger+\\ddagger\\) & 145,623 & 48.22 & 71.36 & **41.26** & 28.36 \\\\  & \\(\\ddagger+\\ddagger\\) & 225,623 & 48.28 & 70.00 & **43.69** & 29.69 \\\\ \\hline \\multirow{7}{*}{\\(\\ddagger\\) Self-Instruct-52K} & 82,439 & 25.92 & /\\(\\star\\) & 37.52 & 29.81 \\\\  & \\(\\warrow\\) complexity & 70,000 & 70.43 & 76.96 & 39.73 & 33.25 \\\\  & \\(\\warrow\\) diversity & 70,000 & 75.59 & 81.55 & 38.01 & 30.03 \\\\  & \\(\\warrow\\) difficulty & 70,000 & 73.48 & 79.15 & 32.55 & 31.25 \\\\  & \\(\\warrow\\) scaling & 220,000 & 57.78 & 51.13 & 33.81 & 26.63 \\\\ \\hline LLAMA (13B) & \\(\\ddagger\\) FLAN-T5 & 80,000 & 48.48 & 22.12 & 34.12 & 34.05 \\\\  & \\(\\ddagger\\) ShareGPT & 63,184 & 77.31 & 72.13 & **47.49** & 33.82 \\\\  & \\(\\ddagger\\) Self-Instruct-52K & 82,439 & 25.92 & /\\(\\star\\) & 36.73 & 25.43 \\\\  & \\(\\ddagger+\\ddagger\\) & 145,623 & 48.22 & 72.85 & 41.16 & 29.49 \\\\  & \\(\\ddagger+\\ddagger+\\ddagger\\) & 225,623 & 48.28 & 69.49 & **43.50** & 31.16 \\\\ \\hline \\multirow{7}{*}{\\(\\ddagger\\) Self-Instruct-52K} & 82,439 & 25.92 & /\\(\\star\\) & 36.73 & 25.43 \\\\  & \\(\\warrow\\) complexity & 70,000 & 70.43 & 77.94 & 46.89 & 35.75 \\\\ \\cline{1-1}  & \\(\\warrow\\) diversity & 70,000 & 75.59 & 78.92 & 44.97 & 36.40 \\\\ \\cline{1-1}  & \\(\\warrow\\) difficulty & 70,000 & 73.48 & 80.45 & 43.15 & 34.59 \\\\ \\cline{1-1}  & \\(\\warrow\\) scaling & 220,000 & 57.78 & 58.12 & 38.07 & 27.28 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE IX: Results of instruction-tuning experiments (all in a single-turn conversation) based on the LLAMA (7B) and LLAMA (13B) model under the chat and QA setting. We employ four instruction improvement strategies on the Self-Instruct-52K dataset, _i.e._, enhancing the complexity (_w/ complexity_), increasing the diversity (_w/ diversity_), balancing the difficulty (_w/ difficulty_), and scaling the instruction number (_w/ scaling_). \\(\\star\\)Since we select the LLAMA (7B)/(13B) model fine-tuned on Self-Instruct-52K as the baseline, we omit the win rate of the fine-tuned model with Self-Instruct-52K against itself.\n' +
      '\n' +
      '초매개변수 설정은 스탠포드 알파카와 동일하게 유지된다. 미세 조정된 모델의 명령어 추종 능력을 더 잘 평가하기 위해 두 가지 설정, 즉 _채팅 설정_ 및 _QA 설정_을 고려한다. 채팅 설정은 주로 일상 채팅에서 사용자 지시와 쿼리를 활용하는 반면, QA 설정은 주로 기존 NLP 데이터 세트의 질문 응답 예제를 사용한다. 채팅 설정에 대한 평가는 알파카팜 평가 세트를 기반으로 수행된다[363]. 전체 쌍 비교를 사용하는 대신 Self-Instruct-52K에서 미세 조정된 LLaMA 7B 및 13B 모델을 참조 기준선으로 선택한 다음 다른 지침을 사용하여 미세 조정된 LLaMA 7B 및 13B 모델과 각각 비교한다. 본 연구의 초점은 지침을 생성하기 위한 다양한 전략의 유용성을 조사하는 것이기 때문에 Self-Instruct-52K에 미세 조정된 모델은 좋은 참조가 될 수 있다. 알파카팜 [363]에 이어 각 비교에 대해 ChatGPT를 사용하여 매번 두 개의 비교 모델에서 어떤 응답이 사용자 쿼리에 가장 적합한지 자동으로 주석을 달고 승률(%)을 평가 메트릭으로 보고한다. QA 설정을 위해 MMLU [364]와 BBH [365]의 두 벤치마크를 선택하고 휴리스틱 규칙을 사용하여 이러한 LLM에서 답변을 구문 분석하여 기본 설정을 기반으로 정확도를 평가한다.\n' +
      '\n' +
      '명령어 튜닝 및 평가를 위해 다음과 같은 프롬프트를 사용합니다. _"다음은 인간과 AI 어시스턴트 간의 대화입니다. AI 어시스턴트는 사용자의 질문에 유용하고 상세하며 예의 바른 답변을 제공합니다.\\(\\backslash n\\)[\\(|\\)Humann\\(|\\):\\(\\{input\\}\\backslash n|AI|\\)]."_. 결과를 재현 하기 위해 링크에서 코드 및 데이터를 릴리스 합니다. [https://github.com/RUCAIBox/LLMSurvey/tree/main/실험](https://github.com/RUCAIBox/LLMSurvey/tree/main/실험).\n' +
      '\n' +
      '**결과 및 분석.** 7B 및 13B LLaMA를 기반으로 하는 다른 명령 데이터 세트를 사용 하는 결과는 표 IX에 나와 있습니다. 다음으로, 우리는 우리의 연구 결과를 자세히 요약하고 분석한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Task 형식 명령은 QA 설정에 더 적합 하지만 채팅 설정에는 유용 하지 않을 수 있습니다._ FLAN-T5를 사용한 명령어 튜닝의 성능을 ShareGPT 및 Self-Instruct-52K와 비교함으로써, FLAN-T5가 채팅 설정에서 ShareGPT를 과소 수행하면서 QA 벤치마크에서 대부분 더 나은 성능을 달성한다는 것을 관찰할 수 있다. 그 이유는 FLAN-T5가 기존의 NLP 태스크들, _예_, 번역 및 독해로부터의 명령어들 및 예들의 혼합으로 구성되기 때문이다. 결과적으로, FLAN-T5로 미세 조정된 LLaMA는 QA 태스크에서 더 나은 성능을 보이지만 사용자 쿼리에서는 좋지 않다. 대조적으로, ShareGPT는 실세계 인간-ChatGPT 대화로 구성되며, 이는 일상 생활에서 사용자 지시를 따르도록 LLaMA를 더 잘 이끌어낼 수 있지만, QA 태스크를 달성하기에 적합하지 않을 수 있다.\n' +
      '\n' +
      '총탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄탄 세 가지 세부 조정 지침을 혼합한 후 파생된 LLaMA 변형(FLAN-T5, ShareGPT 및 Self-Instruct-52K 포함)이 두 작업 설정 모두에서 잘 수행됨을 알 수 있다. MLU에서 LLaMA(7B)의 성능은 큰 마진(_i.e_), 43.69 vs. 38.58(FLAN-T5). 이는 명령어 데이터 세트의 여러 소스를 혼합하는 것이 명령어 튜닝된 LLM의 성능을 향상시키는 데 도움이 되며, 이는 명령어 수를 확장하고 다양성을 증가시킨다는 것을 보여준다.\n' +
      '\n' +
      '\\(\\bullet\\)_명령어의 복잡성과 다양성을 향상 하면 모델 성능이 향상 됩니다._ Self-Instruct-52K 데이터셋의 복잡도와 다양성을 각각 증가시킴으로써, LLaMA의 채팅 및 QA 성능은 LLaMA용 MMLU(7B)에서 37.52에서 39.73으로 일관되게 향상될 수 있다. 이는 두 전략 모두 LLM의 수업 추종 능력을 향상시키는 데 유용함을 보여준다. 또한, 복잡도를 개선하면 QA 태스크에서 더 큰 성능 향상을 얻을 수 있음을 알 수 있다. 그 이유는 QA 과제는 주로 LLM을 평가하기 어려운 문항으로 구성되어 있는데, 이는 미세 조정 단계에서 복잡한 지시를 학습한 LLM이 더 잘 해결할 수 있기 때문이다.\n' +
      '\n' +
      '\\(\\bullet\\)_단순히 명령어 수를 늘리는 것은 그다지 유용하지 않을 수 있으며, 난이도 균형을 맞추는 것이 항상 도움이 되는 것은 아니다. 표 IX에 나타난 결과와 같이 난이도의 균형과 미세 조정 지침의 수를 늘리는 것은 우리의 실험에 큰 도움이 되지 않는다. 특히 명령어 수를 스케일링하는 경우, LLaMA(7B)의 BBH에서 성능(예: 29.81에서 26.63)까지 저하된다. 이는 품질 관리 없이 합성 명령어의 수를 단순히 스케일링하는 것이 성능 향상에 효과적이지 않을 수 있음을 보여준다. 또한, 중간 난이도의 명령어를 사용한 미세 조정은 채팅 설정에서도 잘 수행되지만 QA 설정에서는 성능이 약간 감소한다. 가능한 이유는 복잡성 점수가 큰 복잡하고 단단한 명령어를 필터링하여 복잡한 질문에 답하는 모델 성능을 손상시키기 때문이다.\n' +
      '\n' +
      '\\(\\bullet\\)_모델 규모가 클수록 성능을 따르는 명령어가 향상 됩니다._ 동일한 명령어 데이터로 미세 조정된 LLaMA (7B) 모델과 LLaMA (13B) 모델의 성능을 비교함으로써, LLaMA (13B)가 대부분 더 나은 성능을 달성한다는 것을 알 수 있다. 이는 모델 크기를 스케일링하는 것이 명령어 추종 능력을 향상시키는데 도움이 된다는 것을 나타낸다. 또한, MMLU에서 QA 성능이 38.11에서 47.49로 _예_ 많이 향상되었음을 알 수 있다. 더 큰 모델은 일반적으로 더 나은 지식 활용 및 추론 능력[33, 55]을 가지고 있어 더 복잡한 질문에 정확하게 답할 수 있기 때문일 수 있다.\n' +
      '\n' +
      '\\(\\bullet\\)**Instruct-tuning Suggestions**\n' +
      '\n' +
      'LLM에 대한 명령어 튜닝을 수행하기 위해 표 VIII에서 필요한 GPU 수와 튜닝 시간에 대한 기본 통계에 따라 계산 자원을 준비할 수 있다. 개발 환경을 설정한 후, 초보자는 명령어 튜닝을 위해 알파카 리포지토리 [137]의 코드를 따를 것을 권장한다. 그 후, 우리는 이 섹션에서 논의한 바와 같이 기본 모델을 선택하고 명령 데이터 세트를 구성해야 한다. 훈련을 위한 계산 자원이 제약될 때, 사용자는 파라미터-효율적인 튜닝을 위해 LoRA를 활용할 수 있다(섹션 5.3 참조). 추론에 관하여, 사용자들은 더 적거나 더 작은 GPU들 상에 LLM들을 배치하기 위해 양자화 방법들을 더 사용할 수 있다(섹션 5.4 참조).\n' +
      '\n' +
      '### _Alignment Tuning_\n' +
      '\n' +
      '이 부분에서는 먼저 정렬의 배경과 그 정의와 기준을 제시하고, LLMs 정렬을 위한 인간 피드백 데이터 수집에 초점을 맞추고, 마지막으로 정렬 튜닝을 위한 인간 피드백(RLHF)으로부터의 강화 학습의 핵심 기술에 대해 논의한다.\n' +
      '\n' +
      '#### 5.2.1 정렬 배경 및 기준\n' +
      '\n' +
      '**배경.** LLM은 광범위한 NLP 작업 [55, 56, 67, 90]에서 놀라운 기능을 보여 줍니다. 그러나, 이러한 모델들은 때때로 의도하지 않은 행동들, _예를 들어_, 잘못된 정보를 조작하고, 부정확한 목적들을 추구하며, 유해하고, 오판의 소지가 있으며, 편향된 표현들을 생성하는 것을 나타낼 수 있다[66, 366]. LLM의 경우, 언어 모델링 목표는 인간의 가치나 선호도에 대한 고려가 부족하면서 단어 예측에 의해 모델 파라미터를 사전 훈련한다. 이러한 예상치 못한 행동을 피하기 위해, LLM이 인간의 기대와 일치하게 행동하도록 인간 정렬이 제안되었다[66, 367]. 그러나, 원래의 사전 훈련 및 적응 튜닝(_e.g._, 명령어 튜닝)과 달리, 그러한 정렬은 매우 상이한 기준(_e.g._, 유용성, 정직성, 및 무해성)을 고려해야 한다. 정렬은 LLM의 일반적인 능력에 어느 정도 해를 끼칠 수 있는 것으로 나타났으며, 이는 관련 문헌 [368]에서 정렬 세금이라고 한다.\n' +
      '\n' +
      '**정렬 기준.** 최근 LLM의 동작을 규제하기 위한 다양한 기준을 개발하는 데 관심이 증가하고 있습니다. 여기서는 기존 문헌[66, 368]에서 널리 채택된 세 가지 대표적인 정렬 기준(_i.e., helpful, honest, and harmless)을 논의의 예로 든다. 또한, 행동, 의도, 인센티브, 및 내부 양상들[366]을 포함하는 상이한 관점들로부터의 LLM들에 대한 다른 정렬 기준들이 존재하며, 이는 본질적으로 상기 세 가지 기준들과 유사하다(또는 적어도 유사한 정렬 기법들을 갖는다). 또한, 정직을 정확성으로 대체하는, _예_라는 특정 요구에 따라 세 가지 기준을 수정하는 것이 가능하다[116]. 다음으로, 세 가지 대표적인 정렬 기준에 대해 간략히 설명한다:\n' +
      '\n' +
      '\\(\\bullet\\)_Helpfulness._ 도움이 되려면 LLM은 사용자가 작업을 해결하거나 질문에 가능한 간결하고 효율적인 방식으로 답변하는 데 도움이 되는 명확한 시도를 보여야 한다. 더 높은 수준에서 추가 설명이 필요할 때 LLM은 관련 문의를 통해 추가 관련 정보를 이끌어내는 능력을 입증하고 적절한 수준의 민감도, 지각력 및 신중함을 나타내야 한다[368]. 유용한 행동의 정렬을 실현하는 것은 사용자의 의도를 정확하게 정의하고 측정하기 어렵기 때문에 LLM들에게 도전적이다[366].\n' +
      '\n' +
      '\\(\\bullet\\)_Honesty._ 기본적인 수준에서 정직하도록 정렬된 LLM은 정보를 조작하는 대신 사용자에게 정확한 콘텐츠를 제시해야 한다. 또한 LLM은 정보의 어떤 형태의 기만이나 오표시를 피하기 위해 출력에 적절한 정도의 불확실성을 전달하는 것이 중요하다. 이를 위해서는 모델이 자신의 능력 및 지식 수준(_e.g._, "know unknowns")에 대해 알아야 한다. [368]의 논의에 따르면 정직은 유용성과 무해성에 비해 더 객관적인 기준이므로 정직 정렬은 잠재적으로 인간의 노력에 덜 의존하여 개발될 수 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_Harmlessness._ 무해하기 위해서는 모델이 생산하는 언어가 모욕적이거나 차별적이지 않아야 한다. 최선을 다해 모델은 악의적인 목적을 위해 요청을 요청하는 것을 목표로 하는 은밀한 시도를 감지할 수 있어야 한다. 이상적으로는 모델이 위험한 행동(예: 범죄를 저지르도록 유도되었을 때 LLM은 정중하게 거절해야 한다. 그럼에도 불구하고, _어떤 행동_이 해로운 것으로 간주되고 _어느 정도까지_ 개인 또는 사회 간에 차이가 있는지 [368]은 LLM을 사용하는 사람, 제기된 질문의 유형 및 LLM이 사용되는 컨텍스트(예: 시간)에 크게 의존한다.\n' +
      '\n' +
      '우리가 볼 수 있듯이 이러한 기준은 상당히 주관적이며 인간의 인지를 기반으로 개발된다. 따라서 이를 LLM의 최적화 목표로 직접 공식화하는 것은 어렵다. 기존 작업에서 LLM을 정렬할 때 이러한 기준을 충족하는 많은 방법이 있다. 유망한 기술은 수동 또는 자동화된 수단을 사용하여 적대적인 방식으로 LLM을 조사하여 유해한 출력을 생성한 다음 이러한 출력을 방지하기 위해 LLM을 업데이트하는 _적색 학습_[369]이다.\n' +
      '\n' +
      '#### 5.2.2 수집 인간 피드백\n' +
      '\n' +
      '사전 훈련 단계에서 LLM은 대규모 코퍼스에서 언어 모델링 목표를 사용하여 훈련된다. 그러나 인간에 의한 LLM 산출물의 주관적 및 정성적 평가(이 조사에서 인간 피드백이라고 함)를 고려할 수 없다. 고품질의 인간 피드백은 LLM을 인간의 선호 및 가치와 정렬하는 데 매우 중요하다. 이 부분에서는 피드백 데이터 수집을 위해 인간 레이블러 팀을 선택하는 방법에 대해 논의한다.\n' +
      '\n' +
      '**휴먼 레이블 선택.** 기존 작업에서 휴먼 피드백 데이터를 생성하는 지배적인 방법은 휴먼 주석입니다 [66, 367, 116]. 이것은 적절한 인간 라벨러를 선택하는 중요한 역할을 강조한다. 고품질의 피드백을 제공하기 위해 인간 라벨러는 자격 있는 수준의 교육과 뛰어난 영어 실력을 갖추어야 한다. 예를 들어, 스패로우[116]는 인간 라벨러가 적어도 학부 수준의 교육 자격을 획득한 영국 기반 원어민 영어 화자가 되도록 요구한다. 그럼에도 불구하고 여러 연구[367]에서 연구자와 인간 라벨러의 의도 사이에 불일치가 여전히 존재하며, 이는 낮은 품질의 인간 피드백을 초래하고 LLM이 예상치 못한 출력을 생성할 수 있음을 발견했다. 이 문제를 해결하기 위해 InstructGPT[66]는 인간 라벨러와 연구자 간의 일치도를 평가하여 라벨러를 필터링하는 스크리닝 프로세스를 추가로 수행한다. 구체적으로 연구자들은 먼저 소량의 데이터를 레이블링한 다음 자신과 인간 레이블러 간의 일치를 측정한다. 일치도가 가장 높은 레이블러를 선택하여 후속 주석 작업을 진행합니다. 일부 다른 작업[370]에서, "슈퍼 평가자"는 인간 피드백의 높은 품질을 보장하기 위해 사용된다. 연구자들은 인간 라벨러의 성능을 평가하고 성능이 좋은 인간 라벨러 그룹(예: 높은 일치도)을 슈퍼 평가자로 선택한다. 슈퍼 평가자들은 후속 연구에서 연구자들과 협력하는 것이 우선될 것이다. 인간 라벨러가 LLM의 출력에 주석을 달 때 자세한 지침을 지정하고 인간 라벨러에 대한 즉각적인 지침을 제공하는 것이 도움이 되며, 이는 라벨러의 주석을 추가로 규제할 수 있다.\n' +
      '\n' +
      '**인간 피드백 컬렉션.** 기존 작업에서 인간 레이블러에서 피드백 및 선호도 데이터를 수집하는 방법에는 주로 세 가지 유형이 있습니다.\n' +
      '\n' +
      '* _랭킹 기반 접근 방식입니다._ 초기 작업[367]에서, 인간 라벨러는 종종 더 세밀한 정렬 기준을 고려하지 않고 거친 결정 방식(_i.e._, 단지 최상의 선택)으로 모델 생성 출력을 평가한다. 그럼에도 불구하고 다른 레이블러는 최상의 후보 출력 선택에 대해 다양한 의견을 가질 수 있으며 이 방법은 선택되지 않은 샘플을 무시하여 부정확하거나 불완전한 인간 피드백을 초래할 수 있다. 이 문제를 해결하기 위해 후속 연구 [116]에서는 후보 산출물을 비교하여 선호 순위를 도출하기 위해 Elo 등급 시스템을 도입한다. 출력들의 순위는 모델이 다른 것들보다 특정 출력들을 선호하도록 안내하는 트레이닝 신호로서의 역할을 하며, 따라서 더 신뢰성 있고 안전한 출력들을 유도한다.\n' +
      '* _질문 기반 접근 방식_ 또한, 인간 라벨러는 LLM에 대한 추가 제약뿐만 아니라 정렬 기준을 포괄하는, 연구원 [81]에 의해 설계된 특정 질문에 응답함으로써 보다 상세한 피드백을 제공할 수 있다. 특히, WebGPT[81]에서, 검색된 문서로부터 관련 정보를 필터링하고 활용하는 모델을 돕기 위해, 인간 라벨러는 검색된 문서가 주어진 입력에 응답하는데 유용한지에 대한 다수의 옵션으로 질문에 응답하도록 요구된다.\n' +
      '* _Rule-based approach._ 많은 연구는 또한 보다 상세한 인간 피드백을 제공하기 위해 규칙 기반 방법을 개발한다. 전형적인 경우로서, 스패로우[116]는 라벨러가 가장 잘 고려하는 응답을 선택할 뿐만 아니라 일련의 규칙을 사용하여 모델 생성 응답이 도움이 되고, 수정되며, 무해하다는 정렬 기준을 충족하는지 테스트한다. 이러한 방식으로, 두 종류의 인간 피드백 데이터를 얻을 수 있다 : (1) 응답 선호도 피드백은 쌍으로 모델 생성 출력의 품질을 비교함으로써 획득되고, (2) 규칙 위반 피드백은 인간 라벨러로부터 평가를 수집함으로써 획득된다(_i.e._, 생성된 출력이 규칙을 어느 정도 위반했는지 나타내는 점수). 나아가, GPT-4[46]는 규칙-기반 보상 모델들로서 제로-샷 분류기들의 세트(GPT-4 자체에 기초함)를 활용하며, 이는 모델-생성된 출력들이 인간-기입된 규칙들의 세트를 위반하는지 여부를 자동으로 결정할 수 있다.\n' +
      '\n' +
      '다음에서는 ChatGPT와 같은 최근 강력한 LLM에서 널리 사용되고 있는 잘 알려진 기법인 인간 피드백(RLHF)으로부터의 강화 학습에 초점을 맞춘다. 아래에서 논의되는 바와 같이, 섹션 5.2.1에 소개된 정렬 기준은 사용자의 질의에 대한 LLM의 응답에 대한 인간 피드백으로부터 학습함으로써 충족될 수 있다.\n' +
      '\n' +
      '#### 5.2.3 Reinforcement Learning from Human Feedback\n' +
      '\n' +
      'LLM을 인간 값과 정렬하기 위해, 수집된 인간 피드백 데이터로 LLM을 미세 조정하기 위해 인간 피드백으로부터 강화 학습(RLHF)[79, 367]이 제안되었으며, 이는 정렬 기준(_e.g._, 유용성, 정직성 및 무해성)을 개선하는데 유용하다. RLHF는 보상 모델을 학습함으로써 LLM들을 인간 피드백에 적응시키기 위해 강화 학습(RL) 알고리즘들(_e.g._, 근접 정책 최적화(PPO)[128])을 채용한다. 이러한 접근법은 InstructGPT[66]에 의해 예시된 바와 같이 잘 정렬된 LLM을 개발하기 위한 훈련 루프에 인간을 통합한다.\n' +
      '\n' +
      '**RLHF 시스템.** RLHF 시스템은 주로 정렬되도록 미리 훈련된 LM, 인간 피드백에서 보상 모델 학습 및 LM을 훈련하는 RL 알고리즘의 세 가지 주요 구성 요소로 구성됩니다. 구체적으로, _미리 훈련된 LM_은 일반적으로 기존의 미리 훈련된 LM 파라미터로 초기화되는 생성 모델이다. 예를 들어, OpenAI는 첫 번째 인기 RLHF 모델인 InstructGPT[66]에 175B GPT-3을 사용하고, DeepMind는 자신의 GopherCite 모델인 Go-pher[64]를 사용한다[370]. 또한, _보상 모델(RM)_은 일반적으로 스칼라 값의 형태로 LM에 의해 생성된 텍스트에 대한 인간의 선호도를 반영하는 (학습된) 안내 신호를 제공한다. 보상 모델은 인간의 선호 데이터를 사용하여 미세 조정된 LM 또는 새로 훈련된 LM이라는 두 가지 형태를 취할 수 있다. 기존의 작업은 전형적으로 정렬된 LM[66, 370]의 것과 상이한 파라미터 스케일을 갖는 보상 모델들을 채용한다. 예를 들어, OpenAI는 6B GPT-3를 사용하고 DeepMind는 7B Gopher를 각각 보상 모델로 사용한다. 마지막으로 보상 모델의 신호를 사용하여 사전 훈련된 LM을 최적화하기 위해 대규모 모델 튜닝을 위해 특정 _RL 알고리즘_을 설계한다. 구체적으로, Proximal Policy Optimization(PPO)[128]은 기존 작업에서 정렬을 위해 널리 사용되는 RL 알고리즘이다[66, 116, 370].\n' +
      '\n' +
      '**RLHF에 대 한 키 단계.** 그림 12는 아래에 소개 된 대로 RLHF [66]의 전체 3 단계 프로세스를 보여 줍니다.\n' +
      '\n' +
      '\\(\\bullet\\)_supervised fine-tuning._ LM이 처음에 원하는 동작을 수행하도록 하려면 일반적으로 LM을 미세 조정하기 위해 입력 프롬프트(명령어) 및 원하는 출력을 포함하는 감독 데이터 세트를 수집해야 한다. 이러한 프롬프트 및 출력은 작업의 다양성을 보장하면서 일부 특정 작업에 대해 인간 라벨러에 의해 작성될 수 있다. 예를 들어, InstructGPT[66]는 인간 라벨러에게 프롬프트를 작성하도록 요청한다(_e.g._, _"내 경력에 대한 열정을 회복하는 방법에 대한 5개의 아이디어 나열"_). 그리고 오픈 QA, 브레인스토밍, 채팅 및 재작성과 같은 여러 생성 작업에 대한 원하는 출력을 요청한다. 첫 번째 단계는 특정 설정 또는 시나리오에서 선택 사항입니다.\n' +
      '\n' +
      '\\(\\bullet\\)_Reward model training._ 두 번째 단계는 인간 피드백 데이터를 사용하여 RM을 훈련하는 것이다. 특히, LM을 사용하여 샘플링된 프롬프트(지도 데이터 세트 또는 인간 생성 프롬프트)를 입력으로 사용하여 특정 수의 출력 텍스트를 생성한다. 그런 다음 초대합니다\n' +
      '\n' +
      '도. 12: RLHF 알고리즘의 워크플로우.\n' +
      '\n' +
      '인간 레이블러는 이러한 쌍에 대한 선호도에 주석을 달다. 어노테이션 과정은 여러 형태로 진행될 수 있으며, 공통된 접근 방식은 생성된 후보 텍스트의 순위를 매겨 주석을 하는 것으로 주석자 간의 불일치를 줄일 수 있다. 그런 다음, RM은 인간이 선호하는 출력을 예측하도록 훈련된다. InstructGPT에서, 라벨러들은 모델 생성 출력들을 최상의 것부터 최악의 것까지 랭크하고, RM(_i.e._, 6B GPT-3)은 랭킹을 예측하도록 트레이닝된다. 최근 작업 [371]에서, 응답 쌍들에 대한 선호의 주석은 인간 대신에 AI 에이전트(통상적으로 정렬된 LLM)에 의해 수행되었고, 이를 _"AI 피드백으로부터의 강화 학습(RLAIF)"_이라고 한다. 일반적인 RLHF 알고리즘으로 훈련된 LMM은 도움이 적은 무해한 응답을 생성하는 경향이 있으며, 이를 _회피 문제_[371]이라고 한다. 무해성과 유용성을 모두 보장하기 위해, RLAIF는 명령어들 [371, 372]에서 미리 설정된 정렬 원리들에 기초하여 AI 피드백을 생성하며, 이는 또한 인간 주석의 노력을 감소시킬 수 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_RL fine-tuning_. 이 단계에서, LM을 정렬(_i.e._, fine-tuning)하는 것은 RL 문제로 공식화된다. 이 설정에서, 미리 트레이닝된 LM은 프롬프트를 입력으로서 취하고 출력 텍스트를 리턴하는 정책으로서 동작하고, 그것의 액션 공간은 어휘이고, 상태는 현재 생성된 토큰 시퀀스이고, 리워드는 RM에 의해 제공된다. 초기(튜닝 전) LM으로부터 상당히 회피하는 것을 피하기 위해, 벌점 항은 일반적으로 보상 함수에 통합된다. 예를 들어, InstructGPT는 PPO 알고리즘을 사용하여 RM에 대해 LM을 최적화한다. 각각의 입력 프롬프트에 대해, InstructGPT는 현재 LM으로부터 생성된 결과들과 초기 LM 사이의 KL 발산을 패널티로서 계산한다. 두 번째 단계 및 마지막 단계는 LLM을 더 잘 정렬하기 위해 여러 번 반복될 수 있다는 점에 유의한다. RL 알고리즘의 불안정성으로 인해, 최근의 작업[373]은 더 높은 보상을 갖는 최상의 순위 샘플을 재사용함으로써 RL 튜닝을 다른 감독된 미세 조정으로 대체한다.\n' +
      '\n' +
      '**RLHE를 위한 실용적인 전략** 입니다. RLHF는 LLM과 인간의 정렬을 효과적으로 개선할 것을 약속하지만 연구자가 성공적으로 구현하는 것은 현실적으로 어렵다. 이 부분에서 우리는 RLHF의 효과와 효율성을 개선하기 위한 몇 가지 유용한 전략과 요령을 논의하는 데 중점을 둔다. 구체적으로는 보상 모델의 효과적인 훈련, 효율성 및 효과적인 RL 훈련에 각각 초점을 맞춘다.\n' +
      '\n' +
      '\\(\\bullet\\)_Effective reward model training_. InstructGPT가 작은 보상 모델(6B GPT 모델)을 사용했음에도 불구하고, 증가하는 작업[99]은 큰 보상 모델(_e.g._, 원래 모델 크기 이상)을 사용하는 것이 종종 더 효과적이라는 것을 보여주었는데, 그 이유는 큰 보상 모델이 일반적으로 LLM 생성된 출력의 품질을 판단하는 데 더 잘 수행되기 때문이다. LLMaMa 2[99]에서, 사전 트레이닝된 채팅 모델 체크포인트들은 보상 모델을 초기화하기 위해 사용되며, 그들은 그러한 접근법이 동일한 사전 트레이닝 지식을 공유함으로써 정렬될 모델과 보상 모델 사이의 정보 불일치를 효과적으로 감소시킬 수 있다고 주장한다. 반면, 대규모 보상 모델을 훈련할 때 과적합 문제에 직면하는 것이 일반적이다. 단순하면서도 효과적인 해결책으로서, 기존 작업[374, 375]은 정규화기로서 인간-주석이 달린 정렬 데이터세트로부터의 입력 프롬프트의 선호 응답에 대한 LM 손실을 도입하였으며, 이는 이진 분류 작업에 대한 보상 모델의 과적합을 완화한다. 또한, 정렬에 대한 여러 기준(_e.g._, 유용성 및 정직성)이 존재하기 때문에, 정렬 기준을 모두 만족시킬 수 있는 단일 보상 모델을 트레이닝하는 것은 종종 어렵다. 그러므로, 상이한 정렬 기준에 초점을 맞춘 다수의 보상 모델들을 훈련시키고[99], 특별한 조합 전략들(_e.g._, 평균 풀링 및 가중 합)을 통해 그들로부터 생성된 보상들에 기초하여 최종 보상을 계산하는 것이 유용하다. 이러한 방법은 여러 기준에 대 한 보다 유연한 규칙 또는 표준을 사용 하 여 유용성에 대 한 요구 사항을 완화 하는 동시에 유해성에 대 한 보다 엄격한 제한을 적용 합니다.\n' +
      '\n' +
      '\\(\\bullet\\)_Effective RL training_. RL 학습 과정이 불안정하고 하이퍼 파라미터에 민감한 경향이 있기 때문에, 언어 모델은 RL 학습 전에 잘 감독되어야 좋은 모델 용량에 도달할 수 있다고 제안된다. 일반적으로 사용 되는 방법은 정렬 데이터 집합에서 RL 이전 수렴할 때까지 프롬프트의 최상의 출력에서 LLM을 미세 조정 하는 것입니다 ( _거부 샘플링_ 또는 _최상의 \\(N \\)_ 라고 함). 프롬프트가 주어지면, LLM은 먼저 샘플링 알고리즘을 통해 \\(N\\)개의 출력을 생성하고, 그 다음 모델로부터 최상의 후보는 학습을 위한 보상 모델에 의해 선택될 것이다. 수렴될 때까지 최상의 샘플들에 대해 LLM을 미세조정한 후, 성능을 더욱 향상시키기 위해 RL 프로세스가 수행될 것이다. LLMaA 2 [99]는 5가지 버전의 RLHF 모델을 연속적으로 훈련했으며, 여기서 LLM은 보상 모델의 개선과 함께 점진적으로 개선되었다. 이러한 방식으로, 인간 선호 데이터의 수집된 프롬프트 및 주석은 현재 모델 체크포인트의 이슈를 더 잘 반영할 수 있고, 따라서 이러한 이슈를 해결하기 위한 특별한 튜닝을 할 수 있다. 또한 LLMaA 2는 반복 최적화 동안 가능한 용량 회귀 문제를 완화하기 위해 이전 반복의 샘플을 후속 반복에 추가한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Efficient RL training_. RL 트레이닝이 필요로 하는 바와 같이,\n' +
      '\n' +
      '도. 13: 네 가지 상이한 파라미터-효율적인 미세 조정 방법의 예시. MHA와 FFN은 각각 트랜스포머 계층에서 멀티헤드 어텐션과 피드포워드 네트워크를 나타낸다.\n' +
      '\n' +
      'LLM과 보상 모델의 추론 과정을 반복하면 특히 더 큰 보상 모델과 LLM의 경우 총 메모리 및 계산 비용이 크게 증가한다. 실제 트릭으로 별도의 서버에 보상 모델을 배포하고 해당 API를 호출하여 자체 서버에서 LLM과 함께 작업할 수 있습니다. 또한, RLHF가 다수의 후보 출력들을 생성하기 위해 LLM을 요구함에 따라, 샘플 디코딩 절차를 여러 번 호출하는 대신에, 빔 탐색 디코딩 알고리즘 26을 활용하는 것이 더 효율적이다. 응답 생성을 위해 단지 원 패스 디코딩을 수행하면 되는 한편, 그러한 전략은 또한 생성된 후보 응답들의 다양성을 향상시킬 수 있다.\n' +
      '\n' +
      '각주 26: [https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/text_generationtransformers.GenerationMixin.group_beam_search](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/text_generationtransformers.GenerationMixin.group_beam_search)\n' +
      '\n' +
      '**프로세스 감독 RLHF.** RLHF [376]의 기존 문헌에서 RL 훈련을 위한 감독 신호는 일반적으로 결과 감독 신호와 프로세스 감독 신호의 두 가지 별개의 범주로 분류할 수 있습니다. 결과 감독 RLHF는 LLM에 의해 생성된 전체 텍스트의 품질을 평가하기 위해 정량적 점수를 사용한다. 대조적으로, 프로세스 감독형 RLHF는 생성된 콘텐츠 내에서 각각의 개별 컴포넌트(_e.g._, 문장, 단어, 또는 추론 단계)의 평가를 제공하며, 이는 훈련을 안내하기 위해 세밀한 감독 신호를 제공할 수 있어 LLM이 원하지 않는 생성 콘텐츠를 정제하는 것을 돕는다[376, 377]. OpenAI는 12K 프로세스 주석 수학 문제(_i.e_, MATH 데이터 세트 [378])와 이러한 문제의 LLM에서 생성된 75K 솔루션으로 구성된 PRM800k [377]이라는 세밀한 주석 데이터 세트를 제안했으며, 여기서 수학적 문제의 각 추론 단계는 PRM800k에서 _긍정_, _부정_ 또는 _중립_으로 표시된다. 이 세밀한 데이터 세트는 프로세스 감독 보상 모델(PRM)을 훈련하기 위해 기존 작업[377, 379]에서 활용되었으며, 각 라벨의 예측으로부터의 확률은 RLHF 절차 동안 감독 신호로 간주될 수 있다. PRM의 프로세스 감독 신호를 효과적으로 활용하기 위해 기존 작업 [376]은 전문가 정책에서 학습을 통해 기본 정책을 개선하는 효과적인 RL 알고리즘인 전문가 반복 [380, 381]을 활용했다. 전형적으로, 전문가 반복은 정책 개선 및 증류[376]의 두 가지 주요 단계를 포함한다. 정책 개선 단계에서 전문가 정책은 체계적인 검색 절차를 처리하여 표본을 생산한다. PRM은 검색 절차에서 전문가 정책을 안내하고 샘플의 품질을 향상시키기 위해 프로세스 감독 신호를 제공한다. 이후 증류 단계에서는 1단계의 전문가 정책에 의해 생성된 샘플을 감독 미세 조정을 통해 기본 정책을 개선하는 데 활용한다. 전문가 반복 외에도, PRM은 또한 LLM에 의해 생성된 최종 답변의 후보들의 순위를 재지정하거나 단계별 추론 동안 더 나은 중간 추론 단계들을 선택하는 데 활용될 수 있다[377].\n' +
      '\n' +
      '#### 5.2.4 Alignment without RLHF\n' +
      '\n' +
      'RLHF는 LLM의 행동을 인간의 가치 및 선호도와 일치시키는 데 큰 성공을 거두었지만 주목할 만한 한계도 있다. 첫째, RLHF는 정렬되고 있는 모델, 보상 모델 및 기준 모델을 동시에 포함하는 다수의 LLM을 트레이닝할 필요가 있으며, 이는 알고리즘 절차에서 지루하고 실제로 메모리가 많이 소모된다. 또한, RLHF에서 일반적으로 사용되는 PPO 알고리즘은 다소 복잡하고 종종 하이퍼 파라미터에 민감하다. 대안으로서, 강화 학습 없이 감독된 미세 조정을 사용하여 인간의 선호를 준수하도록 LLM을 직접 최적화하는 연구가 증가하고 있다[349].\n' +
      '\n' +
      '**개요.** 비-RL 정렬 접근법의 기본 개념은 고품질 _정렬 데이터 세트_에서 _감독 학습_으로 LLM을 직접 미세 조정 하는 것입니다. 기본적으로 안전하지 않은 행동을 피하기 위한 응답 피드백 또는 황금 규칙이 특수하게 선별된 정렬 데이터 세트에 주입되거나 포함되어 LLM이 적절한 미세 조정 전략을 통해 이러한 시연 데이터에서 정렬된 행동을 직접 학습할 수 있다고 가정한다. 따라서 이 접근법을 구현하기 위해 두 가지 주요 문제는 정렬 데이터 세트의 구성과 미세 조정 손실의 설계이다. 첫 번째 호에 대해, 정렬 데이터세트는 인간-작성된 안전 원리들에 따라 정렬된 LLM들에 의해 자동으로 구축될 수 있다[347]. 또는 편집 동작들을 사용하여 기존 예들을 정제한다[383]. 또한, 기존의 보상 모델을 재사용하여 기존의 휴먼 피드백 데이터로부터 높은 등급의 응답을 선택할 수도 있다[373]. 두 번째 이슈의 경우, 비-RL 정렬 접근법은 주로 고품질 정렬 데이터세트에서 지도 학습 방식(원래 지시 튜닝 손실과 동일)으로 LLM을 미세 조정하는 반면, 보조 학습 목표는 정렬 성능, _예를 들어_, 응답 순위화 또는 대조 지시-응답 쌍을 향상시키는 데 사용될 수 있다.\n' +
      '\n' +
      '**정렬 데이터 수집.** 정렬 데이터의 구성은 LLM의 동작을 인간 선호도와 효과적으로 정렬하는 데 중요합니다. 고품질의 정렬 데이터를 수집하기 위해, 일부 작업은 높은 등급의 응답을 선택하기 위해 기존의 보상 모델을 재사용하려고 시도하고, 다른 작업은 강력한 LLM(_e.g._, ChatGPT)을 활용하거나 시뮬레이션된 환경을 구축하여 합성 정렬 예를 생성하려고 시도한다. 다음으로, 우리는 이 세 가지 연구 라인에 대해 논의할 것이다.\n' +
      '\n' +
      '\\(\\bullet\\)_Reward 모델 기반 접근 방식입니다. RLHF의 보상 모델은 LLM의 응답에 대한 정렬 정도를 측정하도록 훈련되었다. 기존 보상 모형을 활용하여 고품질 반응을 후속 미세 조정을 위한 정렬 데이터로 선택하는 것은 간단합니다. 이 아이디어에 기초하여, RAFT[373]는 인간 선호도 데이터에 대해 훈련된 보상 모델을 채택하여 LLM의 응답을 순위화하고 감독된 미세 조정에 대해 더 높은 보상을 갖는 것을 수집한다. 또한, 보상 모델은 또한 모델 응답들을 스코어링하고 상이한 품질 그룹들에 할당하는 데 사용될 수 있다. 쿼크[384]는 보상 점수에 기초하여 LLM의 응답을 상이한 분위수로 정렬한다. 각 분위수에는 특별한 보상 토큰이 부착되어 분위수의 보상 수준을 나타낸다. 최고 리워드 토큰에 대해 조건화 된 LLM은 이후에 고품질 응답을 생성 하 라는 메시지가 표시 됩니다. 초기 답변 및 대응하는 인간 피드백이 주어지면, ILF[385]는 먼저 정제된 답변을 생성하기 위해 LLM을 채택한 다음, 추가 훈련을 위해 피드백과 가장 매칭되는 답변을 선택하기 위해 보상 모델을 활용한다. LLM 정렬을 위한 귀중한 자원으로서 OpenAssistant27의 DeBERTabase/large/xlarge, Fudan28의 Moss-7B 및 Stanford29의 Flan-T5-xl을 비롯하여 여러 보상 모델이 출시되었다.\n' +
      '\n' +
      '각주 27: [https://huggingface.co/OpenAssistant](https://huggingface.co/OpenAssistant)\n' +
      '\n' +
      '각주 28: [https://github.com/OpenLMLab/MOS8-RLHF](https://github.com/OpenLMLab/MOS8-RLHF)\n' +
      '\n' +
      '각주 29: [https://huggingface.co/stanfordnlp/SteamSHP-flan-t5-xl](https://huggingface.co/stanfordnlp/SteamSHP-flan-t5-xl)\n' +
      '\n' +
      '\\(\\bullet\\)_LLM 기반 생성 접근 방식입니다. 보상 모델은 모델 응답에서 정렬된 데이터를 선택하는 데 도움이 됩니다. 그러나 학습 보상 모델 자체는 일반적으로 비용이 많이 들고 공급이 부족한 상당한 고품질 인간 라벨 데이터를 필요로 한다. 또한 기존 보상 모델을 재사용할 수 있지만 별도로 훈련된 다른 LLM에서 정렬되지 않은 동작을 정확하게 캡처할 수 없다. 따라서, 일부 작업은 인간 정렬된 데이터를 자동으로 생성하기 위해 강력한 LLM을 활용하는 것을 탐구한다. 대표적인 작업으로서, 헌법상의 AI[371]는 인간의 감독이 AI 행동을 지배하는 일련의 원리들(_i.e._, 자연 언어 지시들)로부터 나온다고 제안한다. 이러한 원칙에 기초하여 LLM은 자신의 유해한 대응을 비판하고 최종적으로 정렬된 대응으로 반복적으로 수정할 것이다. 유사하게, 자기 정렬[347]은 먼저 자기 지시[143]를 채택하여 다양한 주제를 다루는 데 초점을 맞춘 지시를 생성한다. 그런 다음, 모델은 또한 정렬 데이터로서 유용하고 윤리적이며 신뢰할 수 있는 응답을 생성하기 위해 예상된 모델 행동의 규칙(여러 문맥 내 예시도 있음)을 설명하는 여러 인간 작성 원리로 프롬프트된다. 원래 SFT 방법이 긍정적인 응답에서만 배울 수 있는 한계를 완화하기 위해 그림 [386]은 부정적인(낮은 품질의 원래 출력) 및 긍정적인(LLM에 의한 정제된 출력) 응답 모두 대조적인 방식으로 활용되어 LLM이 실제로 좋은 응답으로 이어지는 세밀한 수정을 깊이 이해할 수 있도록 개선된 감독 정렬 접근법을 개발한다.\n' +
      '\n' +
      '\\(\\bullet\\)_LLM 기반 대화형 접근 방식입니다. 기존의 대부분의 접근법은 LLM이 외부 피드백 신호를 통해 자신을 개선하기 위해 실제 환경에 존재하지 않는 격리된 상태에서 LLM을 훈련한다. 비교로서 인간은 사회적 환경에서 타인과의 상호 작용으로부터 사회적 규범과 가치를 학습한다[387]. 이러한 학습 접근법을 모방하기 위해, Stable Alignment[179]는 다수의 LLM 에이전트로 구성된 시뮬레이션된 상호작용 환경을 구축하며, 여기서 AI 에이전트는 상호 작용을 계속하고, 개선에 대한 피드백을 받는다. 중앙 에이전트가 명령을 받으면 응답을 생성 하 고 주변 에이전트와 공유 합니다. 이러한 비평가들은 응답 및 수정 제안들에 대한 평점을 포함하는 피드백을 생성한다. 그런 다음 중앙 에이전트는 이러한 제안에 따라 원래 응답을 수정합니다. 이러한 정렬 접근법은 인간과의 실제 환경으로도 확장될 수 있다.\n' +
      '\n' +
      '**감독 정렬 조정.** 정렬 데이터를 얻은 후에는 직접 정렬에 적합한 미세 조정 전략을 설계하는 것도 중요합니다. 간단한 접근법은 정렬 데이터를 기반으로 하는 기존의 시퀀스 대 시퀀스 목표를 사용하여 LLM을 최적화하는 것이다. 기존의 최적화 목표 외에도 여러 연구에서는 정렬 데이터에서 학습을 향상시키는 보조 손실을 추가로 탐색한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Primary training objective._ 정렬 데이터는 전형적으로 입력 명령 및 출력 응답으로 구성되기 때문에, 1차 트레이닝 손실은 여전히 시퀀스 대 시퀀스 학습을 위한 전통적인 교차 엔트로피 손실이다. 이러한 손실에 기초하여, 많은 연구들은 감독 정렬 튜닝을 강화하기 위한 다수의 개선 변형들을 제안한다. 예를 들어, CoH[388]는 주석이 달린 좋은 응답 및 나쁜 응답에 각각 _"A helpful answer:"_ 및 _"An unhelpful answer:"_를 미리 준비함으로써 트레이닝 데이터를 구성하고, 특수 마스킹을 갖는 이들 응답 토큰에 대한 손실만을 계산한다. 쿼크[384]는 모델 응답들을 다양한 정렬 품질을 갖는 상이한 분위수로 정렬하고, 응답의 보상 레벨을 나타내기 위해 각각의 모델 응답에 특별한 보상 토큰을 준비한다. 또한, 최대 가능성 목표를 통해 선호도 모델링을 가능하게 하기 위해, DPO[389]는 먼저 정책 모델(_i.e._, 최적화되고 있는 언어 모델)을 사용하여 응답 보상을 재파라미터화하고, 그 후 원래의 보상 모델링 목표는 정책 모델에 기초하여만 재구성될 수 있다. 이러한 방식으로 DPO는 명시적 보상 모델링 단계를 제거하고, 정책 모델만 포함하는 새로운 학습 목표를 최적화하는 것은 보상을 최적화하는 것과 동일하다. 또한, 도A [386]은 바람직한 토큰을 장려하고 바람직하지 않은 토큰을 벌칙화하고 사소한 토큰을 무시하는 것을 목표로 하는 세밀한 대비 손실을 설계한다.\n' +
      '\n' +
      '\\(\\bullet\\)_보조 최적화 목표입니다._ 1차 교차 엔트로피 손실 외에도 여러 연구에서 정렬 데이터에서 학습을 향상시키기 위한 보조 학습 손실을 제안한다. 첫째, 각각의 명령어의 응답들이 보상 모델에 의해 스코어링될 수 있기 때문에, 랭킹 손실은 이들 응답들의 랭킹 순서를 보존하기 위해 모델을 트레이닝하는 데 사용될 수 있다. 예를 들어, RRHF[390]는 모델 자체, ChatGPT 및 GPT-4로부터 도출된 응답과 같은 모델 생성 응답뿐만 아니라 고품질 및 저품질 인스턴스 모두에 걸쳐 인간 작성 응답을 포함하는 다수의 소스로부터의 응답을 샘플링한다. 보상 모델들의 스코어들과 정렬하기 위해, 그것은 모델이 더 높은 랭킹을 갖는 응답에 대해 더 높은 조건부 로그 확률을 갖도록 장려함으로써 랭킹 손실을 더 최적화한다. SLiCPF[391]는 잠재 공간에서의 거리를 통해 모델 출력과 인간 선호도 사이의 유사성을 평가하는 것을 제안하고, 인간-선호도 데이터에 기초하여 후보 시퀀스를 교정하기 위해 특정 교정 및 정규화 손실을 도입한다. 둘째, 응답과 명령의 관련성을 높이기 위해 일부 연구에서는 대조적 학습을 적용하여 잘못된 명령-응답 쌍을 밀어내면서 올바른 명령-응답 쌍의 확률을 올린다. 구체적으로, 출력 응답에 대해, [392]의 제안된 접근법은 타겟 명령어를 다른 관련 없는 명령어와 대조한다. 이렇게 함으로써, 모델이 명령어와 응답 사이의 올바른 상관관계를 학습할 수 있게 할 수 있다.\n' +
      '\n' +
      '#### 5.2.5 SFT 및 RLHF에 대한 비고\n' +
      '\n' +
      '섹션 5.1에서 논의된 바와 같이, 명령어 튜닝은 포맷된 데모 데이터(원하는 출력과 쌍을 이루는 명령어)로 미리 트레이닝된 언어 모델을 트레이닝하는 프로세스이다. 초기 탐색에서 수업 데이터는 주로 NLP 작업(67)에서 수집되었지만 이제 입력 및 출력 텍스트(예: 개방형 대화체의 발화)를 쌍화하는 보다 다양한 감독 데이터로 확장되었다. 이러한 쌍을 이루는 텍스트를 사용한 훈련은 LLMs[66]의 문맥에서 _감독된 미세 조정(supervised fine-tuning; SFT)_으로도 불린다. 이 부분에서는 단순성과 대중성으로 인해 주로 토론에 약어 _SFT_를 사용하지만 지침 튜닝은 사용하지 않는다.\n' +
      '\n' +
      'SFT와 RLHF는 LLM의 두 가지 주요 적응 조정 방법이기 때문에 이들의 연결과 차이점을 이해하는 것이 중요하다. 다음으로 이 문제 30에 대해 몇 가지 논의를 진행합니다.\n' +
      '\n' +
      '각주 30: 이 부분은 주로 저자들의 의견과 경험에 따라 다소 주관적일 것이다. 이 부분을 개선하려면 의견이나 수정을 환영합니다.\n' +
      '\n' +
      '**RL 공식과의 전체 비교**. 제5.2.3절(RL 훈련과 관련된 부분)의 논의에 이어 텍스트 생성 문제는 RL을 기반으로 한 의사결정 과정으로 공식화될 수 있다. 프롬프트를 입력으로 하면, LLM의 태스크는 프롬프트에 적절하게 응답하는 텍스트 완료를 생성하는 것이다. 이 작업은 단계적으로 완료됩니다. 각각의 단계에서, 에이전트(_i.e.,_LLM)는 현재 상태(현재 생성된 토큰 시퀀스 및 다른 이용가능한 컨텍스트 정보)에 조건화된 정책(_i.e.,_LLM의 생성 확률 분포)에 따라 액션(_i.e.,_토큰 생성)을 수행할 것이다. 전체 응답을 기반으로 큰 보상 점수를 얻을 수 있는 LLM에 의해 고품질의 출력 텍스트가 생산될 것으로 예상된다. 전반적으로, RLHF와 SFT는 LLM에 대한 위의 의사 결정 프로세스를 최적화하기 위한 두 가지 다른 훈련 접근법으로 간주될 수 있다. 특히, RLHF는 먼저 보상 모델을 학습한 후, RL 트레이닝(_e.g.,_ PPO)을 통해 LLM을 개선한다. 비교로서 SFT는 시연 결과의 가능성을 직접 최적화하는 교사 강제 접근법을 채택한다. 이러한 토큰-레벨 트레이닝 방식은 본질적으로 _behavior cloning_(모방 학습의 특별한 알고리즘[393]): 감리 라벨로서 전문가의 액션(_i., 즉, 각 단계에서의 타겟 토큰)을 활용하고, 전형적인 RL 알고리즘에서와 같이 보상 모델을 특정하지 않고 전문가로부터 데모를 모방하는 것을 직접 학습한다. 원하는 정책을 학습하기 위해, SFT는 시연 데이터에 기초하여 "로컬" 최적화 방식(_i.e.,_토큰-레벨 손실)을 채택하고, RLHF는 인간의 선호도를 수반함으로써 "글로벌" 최적화 방식(_i.e.,_텍스트-레벨 손실)을 취한다. 모방 학습 및 강화 학습에 대한 보다 이론적인 분석은 관련 RL 문헌[393, 394]을 참조할 수 있다.\n' +
      '\n' +
      '**SFT의 장단점** SFT는 다양한 벤치마크 [67, 69, 137, 138]에서 LLM의 성능을 높이는 효과적인 접근 방식인 것으로 나타났으며, 이는 태스크 일반화 능력을 크게 향상시키고 특정 기능(예를 들어, 챗봇의 아이덴티티 확립)을 유연하게 부여할 수 있다. SFT의 유용성에 대한 더 많은 논의는 섹션 5.1.3에서 찾을 수 있다. SFT는 주로 능력을 잠금 해제하지만 LLM에 새로운 능력을 주입하지 않는다는 것이 널리 인식되어 왔다. 따라서 SFT를 통해 LLM의 비내인성 능력을 자극하려고 할 때 문제가 될 수 있다. 구체적인 시나리오로, 시연 데이터가 LLM의 지식 또는 능력 범위를 초과할 때 잠재적으로 환각 행동을 옹호할 것이며, 예를 들어, 알려지지 않은 사실에 대한 질문에 답하기 위해 LLM을 훈련시킨다. RLHF [395]에 대한 존 슐만의 강연에서 흥미로운 관점은 능력이 떨어지는 모델을 훈련시키기 위해 우수한 모델을 증류하는 것(예: 미세 조정 데이터로 응답을 생성하도록 GPT-4를 프롬프트하는 것)이 환각 텍스트를 생성할 가능성을 증가시켜 LLM의 사실적 정확도에 영향을 미칠 수 있다는 것이다. 또한 행위 복제 방법으로 SFT는 시연 데이터를 구성하는 전문가의 행위(탐험 없이)를 모방하는 것을 목표로 한다. 그러나, 시연 데이터의 쓰기 스타일, 품질 및 선호도에 따라 다양한 주석자 간에 변형이 종종 존재하며, 이는 SFT의 학습 성능에 영향을 미치는 경향이 있다. 따라서, 고품질 명령 데이터(양은 아님)는 SFT 단계 동안 LLM의 효과적인 훈련을 위한 주요 요소이다[99].\n' +
      '\n' +
      '**RLHF의 장단점** RLHF는 심층 RL[79]의 문헌에서 초기에 탐색된 다음 언어 모델의 용량을 개선하기 위해 차용되었으며(예: 요약 [129]) 이후 InstructGPT[66]를 개발하기 위한 기본 기술로 채택되었다. 최근, 증가하는 증거[99, 371]는 유해 반응을 완화하고 모델 용량을 향상시키는 RLHF의 효과를 입증했다. 특히, LLaMA 2는 RLHF가 유용성과 무해성 점수를 모두 향상시킬 수 있음을 입증했으며[99], 이는 데이터 주석을 위한 더 나은 인간-LLM 시너지 효과에 기인한다. 이러한 이유를 크게 두 가지 측면에서 설명하면 다음과 같다. 첫째, 인간 주석기는 주로 RLHF에 대한 선호 주석을 제공하기 때문에, SFT에서와 같이 주석기의 불일치를 크게 완화할 수 있다. 둘째, 선호도 어노테이션은 시연 데이터를 작성하는 것보다 훨씬 쉽고, 어노테이션자는 자신이 만드는 것보다 더 우수한 세대의 품질을 판단할 수 있어 인간 어노테이션자가 시연할 수 있는 것 이상으로 더 넓은 상태 공간을 탐색할 수 있다. 또 다른 핵심 사항은 RLHF가 본질적으로 자체 생성된 응답(좋은 응답과 나쁜 응답을 구별함)을 대조함으로써 LLM이 올바른 정책을 학습하도록 장려한다는 것이다. 이 모델은 더 이상 외부 시연 데이터를 모방하지 않으므로 위에서 논의한 바와 같이 SFT로 환각 문제를 완화할 수 있다. 실제로 RLHF는 GPT-4 [46]에서 환각 행동을 줄이는 중요한 접근법임이 입증되었다. 그러나, RLHF는 고전적인 RL 알고리즘들의 단점들, 예를 들어, 샘플 비효율성 및 트레이닝 불안정성을 상속한다. LLM에 적응될 때, RLHF는 좋은 성능을 효율적으로 달성하기 위한 초기 모델 체크포인트로서 강한 SFT 모델에 추가로 의존한다. 또한, 인간 주석자들은 다수의 중요한 세부사항들(예를 들어, 프롬프트 선택, 보상 모델 트레이닝 및 PPO 트레이닝의 스케쥴, 및 하이퍼-파라미터들의 설정들)이 전체 모델 성능에 중요한 영향을 미치는 복잡한 반복 최적화 프로세스에 관여한다.\n' +
      '\n' +
      '각주 31: RLHF에서, 보상 모델들이 정렬될 LLM의 지식 또는 능력을 인식해야 한다는 것 또한 중요한 것으로 보인다. 예를 들어, LLAMA 2는 보상 모델들을 초기화하기 위해 사전 트레이닝된 채팅 모델 체크포인트들을 채택한다[99].\n' +
      '\n' +
      '전반적으로, SFT는 사전-훈련 직후에 사전-훈련된 모델 체크포인트들의 모델 용량을 증가시키는데 특히 유용한 반면, RLHF는 SFT 모델들의 모델 용량을 더욱 향상시킬 것으로 유망하다. 그러나, RLHF는 구현하기 어려웠고, (공공 문헌에 따르면) 잘 탐구된 것과는 거리가 멀었으며, 더 많은 개선(예를 들어,_효율적이고 신뢰할 수 있는 주석[371] 및 단순화된 최적화[389])이 추가 연구를 위해 여전히 필요하다.\n' +
      '\n' +
      '### _Parameter-Efficient Model Adaptation_\n' +
      '\n' +
      '위에서 우리는 특정 목표에 따라 LLM을 적응시키기 위한 명령어 튜닝 및 정렬 튜닝의 접근법에 대해 논의했다. LLM은 많은 양의 모델 파라미터로 구성되기 때문에 전체 파라미터 튜닝을 수행하는 데 비용이 많이 든다. 이 절에서는 LLM에 대한 효율적인 튜닝을 수행하는 방법에 대해 논의할 것이다. 먼저 트랜스포머 언어 모델에 대한 대표적인 파라미터 효율적인 미세 조정 방법을 검토하고, 파라미터 효율적인 미세 조정 LLM에 대한 기존 작업을 요약한다.\n' +
      '\n' +
      '#### 5.3.1 Parameter-Efficient Fine-Tuning Methods\n' +
      '\n' +
      '기존 문헌에서 매개변수 효율적인 미세 조정[145, 396, 397]은 가능한 좋은 성능을 유지하면서 훈련 가능한 매개변수의 수를 줄이는 것을 목표로 하는 중요한 주제였다. 다음에서는 어댑터 튜닝, 프리픽스 튜닝, 프롬프트 튜닝 및 LoRA를 포함하여 변압기 언어 모델에 대한 4가지 매개변수 효율적인 미세 조정 방법을 간략하게 검토한다. 이 네 가지 방법의 그림은 그림 13에 나와 있다.\n' +
      '\n' +
      '**어댑터 튜닝**. 어댑터 튜닝은 작은 신경망 모듈( _어댑터_라고 함)을 트랜스포머 모델에 통합한다[398]. 어댑터 모듈을 구현하기 위해, [398, 399]에서 병목 구조가 제안되었는데, 이는 먼저 원래의 특징 벡터를 더 작은 차원으로 압축(비선형 변환에 이어)한 다음 원래의 차원으로 복구한다. 어댑터 모듈은 일반적으로 변압기 층의 두 개의 코어 부분(_i.,_ 주의 계층 및 피드포워드 계층) 각각 뒤에 직렬 삽입을 사용하여 각 변압기 층에 통합될 것이다. 대안적으로, 병렬 어댑터[400]는 트랜스포머 층에서도 사용될 수 있으며, 여기서 어텐션 층 및 그에 따른 피드-포워드 층과 병렬로 두 개의 어댑터 모듈을 배치한다. 미세 조정 동안 어댑터 모듈은 특정 작업 목표에 따라 최적화되는 반면 원래 언어 모델의 매개변수는 이 과정에서 동결된다. 이러한 방식으로, 미세 조정 동안 훈련 가능한 파라미터의 수를 효과적으로 감소시킬 수 있다.\n' +
      '\n' +
      '**접두사 튜닝**. 접두사 튜닝[396]은 트레이닝가능한 연속 벡터들의 집합인 접두사들의 시퀀스를 언어 모델들에서 각각의 트랜스포머 계층에 프리펜딩한다. 이러한 프리픽스 벡터는 태스크-특정적이며, 이는 가상 토큰 임베딩으로 간주될 수 있다. 프리픽스 벡터들을 최적화하기 위해, 프리픽스들을 직접 최적화하는 대신, 더 작은 매트릭스를 프리픽스들의 파라미터 매트릭스에 매핑하는 MLP 함수를 학습함으로써 재파라미터화 트릭[396]이 제안되었다. 이 트릭은 안정적인 훈련에 유용한 것으로 나타났다. 최적화 후, 매핑 함수는 폐기되고, 도출된 프리픽스 벡터들만이 태스크-특정 성능을 향상시키기 위해 유지된다. 접두사 매개변수만 학습되므로 매개변수 효율적인 모델 최적화로 이어질 수 있습니다. 프리픽스 튜닝과 유사하게, p-튜닝 v2[401]은 계층별 프롬프트 벡터를 트랜스포머 아키텍처에 특히 자연어 이해를 위해 통합하며, 또한 공유 프롬프트를 공동으로 최적화하기 위해 멀티-태스크 학습을 활용한다. 자연어 이해 과제에서 서로 다른 매개변수 척도의 모델 성능을 향상시키는 데 유용한 것으로 나타났다.\n' +
      '\n' +
      '**프롬프트 튜닝** 프리픽스 튜닝과는 달리, 프롬프트 튜닝[397, 402]은 주로 입력 레이어32에서 트레이닝가능한 프롬프트 벡터들을 통합하는 것에 초점을 맞춘다. 이산 프롬프트 방법들[404, 405]에 기초하여, 소프트 프롬프트 토큰들의 그룹(자유 형태[402] 또는 프리픽스 형태[397] 중 어느 하나)을 포함함으로써 입력 텍스트를 증강시키고, 그 후 특정 다운스트림 태스크들을 해결하기 위해 프롬프트 증강된 입력을 취한다. 구현에서, 태스크-특정 프롬프트 임베딩은 입력 텍스트 임베딩과 결합되고, 이 임베딩은 후속적으로 언어 모델에 공급된다. P-튜닝[402]은 컨텍스트, 프롬프트 및 타겟 토큰을 조합하기 위한 자유 형태를 제안하였으며, 이는 자연 언어 이해 및 생성 모두를 위한 아키텍처에 적용될 수 있다. 그들은 양방향 LSTM에 의해 소프트 프롬프트 토큰들의 표현들을 더 학습한다. 프롬프트 튜닝이라는 또 다른 대표적인 접근 방식 [397]은 접두사 프롬프트를 입력으로 직접 준비합니다. 교육 중에는 태스크별 감독에 따라 신속한 임베딩만 학습됩니다. 이 방법은 입력 계층에서 소수의 트레이닝 가능한 파라미터만을 포함하기 때문에, 성능은 기본 언어 모델들의 모델 용량에 크게 의존하는 것으로 밝혀졌다[397].\n' +
      '\n' +
      '각주 32: 여기서, 프롬프트 튜닝은 [397]에서 사용된 특정 방법 대신에 작업[397, 402, 403]에 의해 예시된 관련 효율적인 튜닝 방법의 카테고리를 나타낸다. 실제로, 프리픽스 기반 튜닝 방법들[396, 401]은 프롬프팅 방법들로서 또한 고려될 수 있으며, 이는 [401]에서 _딥 프롬프팅 튜닝_이라고 불린다. 이 조사에서 프롬프트 튜닝은 특히 LLM의 맥락에서 입력 계층에 프롬프트 토큰만 포함하는 방법을 나타낸다. 언어 모델에 계층별 프롬프트를 통합하기 때문에 p-튜닝 v2 [401]을 접두사 튜닝 범주에 할당합니다.\n' +
      '\n' +
      '**Low-Rank Adaptation(LoRA)** LoRA[145]는 다운스트림 작업에 적응하기 위한 트레이닝 가능한 파라미터를 감소시키기 위해, 각각의 밀집 계층에서 업데이트 매트릭스를 근사화하기 위한 낮은-랭크 제약을 부과한다. 매개 변수 행렬 \\(\\mathbf{W}\\)을 최적화하는 경우를 고려합니다. 업데이트 프로세스는 일반적인 형식으로 다음과 같이 작성할 수 있습니다. \\(\\mathbf{W}\\leftarrow\\mathbf{W}+\\Delta\\mathbf{W}\\). LoRA의 기본 아이디어는 원래 행렬 \\(\\mathbf{W}\\in\\mathbb{R}^{m\\times n}\\)을 동결하고, 파라미터 갱신 \\(\\Delta\\mathbf{W}=\\mathbf{A}\\cdot\\mathbf{B}^{\\top}\\)을 저순위 분해 행렬로 근사화하는 것이다. 여기서 \\(\\mathbf{A}\\in\\mathbb{R}^{m\\times k}\\)과 \\(\\mathbf{B}\\in\\mathbb{R}^{n\\times k}\\)은 태스크 적응을 위한 훈련 가능한 파라미터이고, \\(k\\ll\\min(m,n)\\)은 감소된 순위이다. LoRA의 주요 장점은 메모리 및 스토리지 사용량(예를 들어, VRAM)을 크게 절약할 수 있다는 것이다. 또한, 다른 다운스트림 작업에 적응하기 위한 다수의 작업별 하위 순위 분해 행렬을 유지하면서 단일의 큰 모델 사본만을 유지할 수 있다. 또한 여러 연구에서 보다 원칙적인 접근법, 즉 _예: 중요도 점수 기반 할당[406] 및 검색이 없는 최적 순위 선택[407]에서 순위를 설정하는 방법에 대해서도 논의했다.\n' +
      '\n' +
      '위와 같은 방법 외에도 트랜스포머 언어 모델의 효율적인 튜닝에 대한 광범위한 연구가 있다. 그러나, 효율적인 튜닝에 대한 보다 포괄적인 논의는 이 주제의 관련 논문[400, 408]에서 찾을 수 있는 이 글의 범위를 벗어난다.\n' +
      '\n' +
      '#### 5.3.2 Parameter-Efficient Fine-Tuning on LLMs\n' +
      '\n' +
      'LLM이 증가함에 따라 효율적인 튜닝은 다운스트림 작업에서 보다 가벼운 적응 접근법을 개발하기 위한 연구 관심을 끌고 있다.\n' +
      '\n' +
      '특히 LoRA[145]는 오픈 소스 LLMs(_e.g.,_ LLaMA 및 BLOOM)의 파라미터 효율적인 미세조정에 널리 적용되어 왔다. 이러한 연구 시도 중 LLaMA와 그 변형은 매개변수 효율적인 조정을 위해 많은 관심을 받았다. 예를 들어, 알파카-LoRA[144]는 알파카[142]의 경량 튜닝된 버전으로서 LoRA를 사용하여 트레이닝되었다(52K 인간 시연이 뒤따르는 미세 튜닝된 7B LLaMA 모델). 최근 LLaMA-Adapter [409]는 학습 가능한 프롬프트 벡터를 각 Transformer 계층에 삽입하고, 이 계층에서 언더피팅 프롬프트 벡터의 영향을 완화하여 학습을 개선하기 위한 제로 초기화 주의를 제안하였다. 그들은 또한 이 접근법을 다중 모드 설정, 즉 시각적 질문 응답으로 확장한다.\n' +
      '\n' +
      '각주 33: [https://github.com/floen/alpaca-lora](https://github.com/floen/alpaca-lora)\n' +
      '\n' +
      '또한, 다양한 튜닝 방법이 언어 모델에 미치는 영향을 조사하기 위해 실증적 연구[399]가 수행되었다. 이를 위해 GPT-J (6B), BLOOM (7.1B) 및 LLaMA (7B) 세 개의 오픈 소스 LLM에서 직렬 어댑터 튜닝 [398], 병렬 어댑터 튜닝 [400, 410], LoRA [145]를 포함한 4가지 효율적인 튜닝 방법을 비교한다. 6개의 수학 추론 데이터 세트에 대한 실험 결과를 기반으로, 이러한 효율적인 조정 방법은 어려운 태스크에서 기준 기준 GPT-3.5를 과소 수행하는 반면 간단한 태스크에서는 유사한 성능을 달성한다는 것을 보여준다. 전반적으로 LoRA는 훈련 가능한 매개변수를 훨씬 적게 사용하여 이러한 비교 방법 중에서 비교적 잘 수행한다.\n' +
      '\n' +
      '중요한 리소스로서, 라이브러리 _PEFT_[411](파라미터-효율적인 미세 조정을 위한 스탠딩)이 GitHub34에 공개되었다. 여기에는 LoRA[145]/AdaLoRA[406], preixtuning[396, 401], P-Tuning[402], 프롬프트-tuning[397]을 포함하여 널리 사용되는 여러 효율적인 조정 방법이 포함되었다. 또한, GPT-2 및 LLaMA와 같은 다수의 언어 모델을 지원하고, 또한 몇몇 대표적인 비전 트랜스포머 모델(_e.g._, VIT 및 Swin Transformer)을 포함한다.\n' +
      '\n' +
      '각주 34: [https://github.com/huggingface/pft](https://github.com/huggingface/pft)\n' +
      '\n' +
      '제5.3.1절에서 논의된 바와 같이, 기존 문헌에서 제안된 다수의 효율적인 동조 방법들이 있었다. 그러나 이러한 방법들의 대부분은 LLM 대신 작은 크기의 사전 학습된 언어 모델에 대해 테스트된다. 지금까지, 상이한 설정 또는 태스크에서 상이한 효율적인 튜닝 방법이 대형 언어 모델에 미치는 영향에 대한 철저한 조사가 여전히 부족하다.\n' +
      '\n' +
      '### _Memory-Efficient Model Adaptation_\n' +
      '\n' +
      '수많은 모델 파라미터로 인해 LLM은 추론을 위해 상당한 메모리 공간을 차지하므로 실제 응용 프로그램에 배포하는 데 매우 비용이 많이 든다. 이 섹션에서는 일반적인 모델 압축 접근법(_i.e._, 모델 양자화)을 통해 LLM의 메모리 공간을 줄이는 방법에 대해 논의한다.\n' +
      '\n' +
      '#### 5.4.1 Quantization 배경\n' +
      '\n' +
      '이 부분에서는 신경망에 대한 양자화 기술의 일반적인 소개를 제시한다.\n' +
      '\n' +
      '신경망 압축에서, 양자화는 종종 부동 소수점 수들로부터 정수들까지의 맵핑 프로세스(412), 특히 8 비트 정수 양자화(_i.e._, _INT8 양자화_)를 지칭한다. 신경망 모델의 경우, 양자화될 데이터의 두 종류, 즉 _가중치_(모델 파라미터)와 _활성화_(은닉 활성화)가 있는데, 이는 원래 부동 소수점 숫자로 표현된다. 모델 양자화의 본질적인 개념을 설명하기 위해, 우리는 부유수 \\(x\\)을 양자화된 값 \\(x_{q}\\)으로 변환하는 간단하면서도 대중적인 양자화 함수 \\(x_{q}=R(x/S)-Z\\를 소개한다. 이 함수에서 \\(S\\)와 \\(Z\\)는 스케일링 인자(클리핑 범위를 결정하는 두 매개 변수 \\(\\alpha\\)와 \\(\\beta\\)와 영점 인자(대칭 또는 비대칭 양자화 결정)를 각각 나타내고 \\(R(\\cdot)\\)는 스케일링된 부동 값을 근사 정수로 매핑하는 라운딩 연산을 나타낸다.\n' +
      '\n' +
      '역과정으로서, _역양자화_는 그에 따라 양자화된 값으로부터 원래의 값을 회복한다: \\(\\tilde{x}=S\\cdot(x_{q}+Z)\\). 양자화 오차는 원래의 값 \\(x\\)과 복원된 값 \\(\\tilde{x}\\)의 수치 차이로 계산된다. 범위 매개 변수 \\(\\alpha\\)와 \\(\\beta\\)는 양자화 성능에 큰 영향을 미치며, 이는 종종 실제 데이터 분포에 따라 _정적_(오프라인) 또는 _동적_ 방식(런타임)으로 _보정_되어야 한다.\n' +
      '\n' +
      '더 자세한 내용은 신경망에 대한 양자화 방법에 대한 우수한 설문[412]을 독자들에게 참조한다.\n' +
      '\n' +
      '#### 5.4.2 LLMs용 양자화 방법\n' +
      '\n' +
      '일반적으로 두 가지 주요 모델 양자화 접근법, 즉 _양자화-인식 트레이닝_(_QAT_)(추가적인 전체 모델 재트레이닝을 요구함) 및 _훈련 후 양자화_(_PTQ_)(모델 재트레이닝을 요구하지 않음)이 있다. 소형 언어 모델과 비교하여 LLM의 양자화 방법을 설계하거나 선택할 때 두 가지 주요 차이점을 고려해야 한다. 첫째, LLM은 많은 수의 파라미터로 구성되기 때문에 QAT 기법보다 계산 비용이 훨씬 낮기 때문에 PTQ 기법이 더 선호된다. 둘째, LLM은 매우 상이한 활성화 패턴(_i.e._, 큰 이상치 특징)을 나타내며, LLM, 특히 숨겨진 활성화를 양자화하는 것이 더욱 어려워진다. 다음으로 LLM에 대한 몇 가지 대표적인 PTQ 방법35를 간략하게 검토한다.\n' +
      '\n' +
      '각주 35: 우리는 주로 LLM의 맥락에서 양자화 방법을 논의하는 데 초점을 맞추고 있기 때문에 작은 크기의 언어 모델(_e.g._, BERT)에 대한 양자화 작업 라인은 이 조사에 포함되지 않았다.\n' +
      '\n' +
      '**PTQ(교육 후 양자화)** 입니다. 먼저 LLMs에 대한 PTQ 방법을 소개한다.\n' +
      '\n' +
      '* _혼합 정밀 분해_ 입니다. [413]에서 관찰된 바와 같이 모델 크기가 6.7B 매개변수 이상에 도달할 때 숨겨진 활성화(이상값의 출현_이라고 함)에서 극단적인 큰 값이 발생한다. 흥미롭게도, 이러한 이상치들은 주로 트랜스포머 층에서 일부 특정 특징 차원에 분포한다. 이러한 발견에 기초하여, _LLM.int8(0)_ 로 불리는 벡터-와이즈 양자화 접근법이 [413]에서 제안되었으며, 이는 이상치들을 갖는 특징 차원들과 매트릭스 곱셈에서의 나머지 차원들을 분리한다. 그런 다음 두 부분에 대한 계산을 각각 _16비트 부동수_ 및 _8비트 정수_로 수행하여 이러한 이상치를 높은 정밀도로 복구한다.\n' +
      '\n' +
      '* _세밀한 양자화_ 입니다. 트랜스포머 모델의 경우, 가중치와 활성화는 보통 텐서의 형태로 표현된다. 간단한 접근법은 전체 텐서(_i.,_ 텐서별 양자화)에 대해 거친-결정 양자화 파라미터를 사용하는 것이다[414]. 그러나 일반적으로 부정확한 재구성 결과를 초래한다. 따라서 양자화 오차를 줄이기 위해 세밀한 방법을 제안한다. 제로퀀트[415]는 활성화들을 압축하기 위한 동적 캘리브레이션을 갖는 토큰-와이즈 양자화 접근법을 채택한다. 가중치의 경우(양자화가 용이함) 그룹별 양자화를 사용합니다. 실제로, 128[415, 416]의 그룹 크기가 모델 양자화에 일반적으로 사용된다.\n' +
      '* _양자화 난이도 균형화_. 가중치가 활성화보다 양자화되기 쉽다는 점을 고려하여, SmoothQuant[414]는 난이도를 활성화에서 가중치로 마이그레이션할 것을 제안한다. 특히, 선형 계층에서 가중치와 활성화 사이의 난이도 균형을 위해 스케일링 변환을 통합한다. \\(\\mathbf{Y}=(\\mathbf{X}\\text{diag}(\\mathbf{s})^{-1})\\cdot(\\text{diag}(\\mathbf{ s})\\mathbf{W})\\). 수학적으로 등가의 변환을 도입함으로써, 이 공식은 스케일링 팩터 \\(\\mathbf{s}\\)을 통해 양자화 난이도를 제어한다. \\(\\mathbf{s}\\)을 설정하기 위해 이주강도 파라메터 \\(\\alpha\\)를 통합하여 각 엔트리 \\(s_{j}=\\max(\\mathbf{x}_{j})^{\\alpha}/\\max(\\mathbf{w}_{j})^{(1-\\alpha)}\\)는 이주강도에 의해 결정된다.\n' +
      '* _Layerwise quantization_. 이 방법은 계층적인 복원 손실을 최소화하는 최적의 양자화 가중치를 찾는다: \\(\\arg\\min_{\\widetilde{\\mathbf{W}}}\\parallel\\mathbf{W}\\mathbf{X}-\\widetilde{\\mathbf{W}}\\mathbf{X}\\parallel_{2}^{2}\\). 이 목적을 효율적으로 최적화하기 위해, GPTQ[417]은 모든 행에 대한 가중치의 양자화 순서를 고정함으로써 원래의 최적 뇌 양자화(OBO)[418] 방법을 개선한다. 또한, 특별히 설계된 방법들(_i.,_ lazy batch-updates and Cholesky reformulation)을 사용하여, GPTQ는 3 또는 4 비트 정밀도로 매우 큰 모델들(_e.,_175B OPT)을 양자화할 수 있다. 보다 최근에, AWQ[416]는 가중치에 대한 활성화-인식 스케일링을 통합함으로써 최적화 형태를 더욱 단순화하는데, 이는 SmoothQuant[414]: 이상치 활성화들에 대응하는 가중치들이 정밀하게 양자화되는 것이 더 중요하다. 그것은 재구성 손실을 직접 최적화하지 않고, 대신에 교정 데이터에 대한 최소 손실을 달성하기 위해 간단한 하이퍼-파라미터 탐색을 수행한다.\n' +
      '\n' +
      '상기 방법들에서의 이러한 전략들은 양자화 성능을 향상시키기 위해 공동으로 사용될 수 있다. 고효율 구현을 달성하기 위해, 양자화 방법들은 또한 하드웨어- 또는 시스템-레벨 지원(예를 들어, 효율적인 GPU 커널들 또는 하드웨어 친화적인 그룹 파티션)에 의존한다.\n' +
      '\n' +
      '**기타 양자화 방법.** 위에서 주로 PTQ 방법에 초점을 맞추고 다음으로 LLM을 양자화하기 위한 효율적인 미세 조정 방법 또는 QAT 방법을 탐구하는 두 가지 최근 연구를 소개합니다.\n' +
      '\n' +
      '* _효율적인 미세 조정 향상된 양자화_ 입니다. 사후-트레이닝 양자화의 경우, 직접적인 저-비트 양자화(_예를 들어,_INT4 양자화)는 종종 큰 성능 저하를 초래한다. 이러한 도전을 극복하기 위해, QLoRA[419]는 양자화된 모델에 추가적인 소형 튜닝 가능한 어댑터(16-비트 정밀도)를 통합하여, 효율적이고 고정밀 모델 미세 조정을 달성한다. LoRA(섹션 5.3.1 참조)의 장점과 양자화 방법을 결합한다. 실험 결과는 4비트 양자화 모델이 QLoRA에 의해 전체 16비트 미세 조정 성능을 달성할 수 있음을 보여준다.\n' +
      '* _LLMs에 대 한 양자화 인식 훈련 (QAT)_ 입니다. 최근 연구[420]는 데이터 프리 증류 방법을 적용하여 키-값 캐시뿐만 아니라 가중치, 활성화도 압축함으로써 QAT 방법의 효과를 탐구한다. LLaMA를 기반으로 한 광범위한 실험을 통해 가중치 및 키 값 캐시 모두에서 4비트 양자화를 통해 유망한 결과를 보여주지만 4비트 활성화 양자화에서는 그렇지 않으므로 더 많은 탐색이 필요하다.\n' +
      '\n' +
      '#### 5.4.3 경험적 분석 및 결과\n' +
      '\n' +
      '양자화는 현재 배치에서 LLM의 메모리 풋프린트 및 대기 시간을 줄이는 일반적인 기술이 되었다. 특히, 높은 정확도를 유지하면서 LLM들(_e.g.,_ 가중치들 또는 활성화들)의 상이한 부분들을 양자화하기 위해 어느 수준의 정밀도(_e.g.,_ INT8 또는 INT4)가 적용될 수 있는지를 이해하는 것이 중요하다. 이 부분에서는 먼저 기존 문헌에서 LLM의 양자화에 대한 주요 연구 결과를 요약한 다음 양자화 실험과 함께 몇 가지 경험적 분석을 제시한다.\n' +
      '\n' +
      '**기존 작업에서 중요한 결과** 입니다. 최근에, 다중 인자(_예를 들어,_모델 크기 및 민감도)가 사후 트레이닝 양자화 방법에 미치는 영향에 대해 매우 포괄적인 평가[421]가 수행되었다. 또 다른 연구 [422]는 추론 성능에서 \\(k\\)-비트 양자화의 스케일링 법칙을 조사한다. 전반적인 성능 외에도, 연구[423]는 특히 다양한 수준의 비트 정밀도에 걸쳐 달성될 수 있는 성능 수준뿐만 아니라, 새로운 능력에 대한 정량화의 잠재적 영향에 초점을 맞춘다. 또한, 선행 작업(_e.g.,_ LLM.int8(424), GPTQ[417], QLoRA[419], 및 GLM[93])도 다양한 설정에서 양자화 방법의 성능을 광범위하게 조사하였다. 다음으로, 양자화 방법의 기술적 세부 사항을 조사하지 않을 수 있는 사람들에게 유용할 이러한 연구의 몇 가지 중요한 결과를 요약한다.\n' +
      '\n' +
      '* _INT8 가중치 양자화는 종종 LLM에 대해 매우 양호한 결과를 산출할 수 있는 반면, 더 낮은 정밀도 가중치 양자화의 성능은 특정 방법_[414, 416, 417, 421]에 의존한다. 대부분의 경우, INT8 가중치 양자화는 성능 저하 없이 메모리 풋프린트를 감소시키기 위해 효과적으로 적용될 수 있다. INT4(또는 INT3) 가중치 양자화의 경우, 기존의 방법들은 성능 저하를 감소시키기 위한 특정 전략들, _예를 들어, 계층별 방법[415, 417], 활성화-인식 스케일링[416] 및 로우-랭크 어댑터 튜닝[419]에 의존한다. 흥미롭게도 LLM은 작은 크기의 언어 모델보다 낮은 비트 가중치 양자화에 덜 민감한 것으로 보인다[421]. 실제로, 동일한 메모리 비용으로, 더 높은 양자화 정밀도를 갖는 더 작은 언어 모델보다는 더 낮은 양자화 정밀도를 갖는 더 큰 언어 모델을 사용하는 것이 제안된다. 예를 들어, 4-비트 60GB LLM은 8-비트 30GB LLM보다 더 나은 성능을 갖는 것으로 입증된다[422]. 또한, 창발적 능력에 초점을 맞춘 연구[423]는 맥락 내 학습, 단계별 추론 및 지시에 따라 4비트 가중치 양자화에 거의 영향을 받지 않는 것으로 보인다. 이 결과는 INT4 양자화가 전체 비트와 창발적 능력의 성능 모두에서 유리한 절충점을 나타냄을 시사한다.\n' +
      '* _활성화는 가중치_[413, 414, 421]보다 양자화되기 더 어렵다. 6.7B 이상의 크기를 갖는 트랜스포머 언어 모델들에 대해 큰 이상치들이 발생할 것이라는 것이 밝혀졌다[413]. 이 문제는 LLM을 정량화하기 위한 가장 근본적인 어려움 중 하나였다. 이 문제를 극복하기 위해 이상치 값의 영향을 완화하기 위해 혼합 정밀 분해[413], 세밀한 양자화[413, 425], 난이도 마이그레이션[414]과 같은 다양한 방법이 적용될 수 있다. LLM의 활성화에는 큰 이상치가 주로 존재하기 때문에 작은 언어 모델은 활성화 양자화에 더 내성이 있다[421, 423]. 실제로 고품질 INT8 활성화 양자화는 여러 가지 방법이 만족스러운 결과를 얻을 수 있지만 여전히 어려운 작업이다. 또한, 더 낮은 정밀도 활성화 양자화는 QAT 방법에 대해서도 여전히 성공적으로 탐색되지 않았다[420].\n' +
      '\n' +
      '\\(\\bullet\\)_효율적인 미세 조정 강화 양자화는 양자화된 LLMs_[419, 145]의 성능을 향상시키는 좋은 옵션이다. 양자화에서 효율적인 푸네-튜닝 방법의 이점은 두 가지일 수 있다. 첫째, 고정밀 어댑터를 갱신하여 피팅 용량을 증가시킴으로써 저비트 양자화에 의한 성능 저하를 직접적으로 보상할 수 있다[421, 423]. 둘째, 작은 어댑터들만을 튜닝함으로써, 경량 방식으로 LLM들의 태스크-특정 또는 목표-특정 미세 튜닝을 지원하는 것이 유연하다[419], _예를 들어, 명령 튜닝 또는 채팅-지향 튜닝. 전반적으로, 그것은 효과성과 훈련 비용 사이의 좋은 균형을 이루며, 이는 양자화된 LLM의 성능을 향상시키는 유망한 접근법을 제공한다.\n' +
      '\n' +
      '**양자화 실험에 대한 경험적 분석** 독자들이 LLM에 대한 양자화의 영향을 이해하는 데 더 도움이 되도록 여기에서 양자화된 모델의 추론 성능을 조사하기 위한 실험 그룹도 수행한다. 구체적으로, FLAN-v2[69], Alpaca-52K[137] 및 ShareGPT[148]를 포함하는 인기 있는 SFT 데이터 세트를 사용하여 미세 조정된 LLMaM 모델(_i.,_7B 및 13B)에 초점을 맞춘다. 평가를 위해 표 IX의 동일한 작업을 활용하고 4비트, 8비트 및 16비트의 세 가지 정밀 수준에서 양자화된 언어 모델의 성능을 조사하는 연구 [423]의 양자화 설정을 따른다. 결과는 표 X에 요약되어 있다. 표 X로부터 알 수 있는 바와 같이, 8-비트 및 4-비트 가중치 양자화로 얻어진 결과는 메모리 소비를 상당히 감소시키면서 16-비트 모델의 성능에 근접한다. 실제로, 메모리 사용을 줄이는 것이 배포를 위한 중요한 고려 사항인 경우 LLM에 대한 4비트 가중치 양자화의 성능을 먼저 조사하는 것이 좋다.\n' +
      '\n' +
      '#### 5.4.4 오픈 소스 라이브러리 및 양자화된 LLMs\n' +
      '\n' +
      '이 부분에서는 사용 가능한 오픈 소스 양자화 라이브러리와 양자화된 LLM을 간략하게 소개한다.\n' +
      '\n' +
      '**양자화 라이브러리** 입니다. 다음으로, LLM을 위한 세 가지 주요 양자화 라이브러리를 소개한다:\n' +
      '\n' +
      '\\(\\bullet\\)_Bitsandbytes36_는 LLM.int80 [413]과 8비트 최적화기 [426]에 소개된 방법을 기반으로 개발되었다. 효율적인 추론을 위한 8-비트 및 4-비트(NF4,FP4) 행렬 곱셈의 지원과 효율적인 훈련을 위한 8-비트 최적화기를 포함하여 LLM에 대한 활성화 및 가중치 양자화에 중점을 둔다.\n' +
      '\n' +
      '각주 36: [https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)\n' +
      '\n' +
      'LLMaM37에 대한 GPTQ-for-LLMaM37은 LLMaM 모델의 양자화를 위해 특별히 개발되었다. GPTQ 알고리즘을 기반으로 다양한 크기의 LLMaM 모델의 4비트 양자화를 가능하게 한다[417]. 또한 프로젝트 웹사이트의 메모리 및 성능(PPL)에서 비트 및 바이트와 비교할 수 있습니다.\n' +
      '\n' +
      '각주 37: [https://github.com/qwpoppop200/GPTQ-for-LLMaMa](https://github.com/qwpoppop200/GPTQ-for-LLMaMa)\n' +
      '\n' +
      '\\(\\bullet\\)_AutoGPTQ38_은 LLMs에 대한 INT4 양자화를 지원하는 GPTQ 알고리즘[417]을 기반으로 개발된 양자화 패키지이다. 라이브러리에 다수의 양자화된 모델을 포함하고 있으며, HuggingFace PFFT 라이브러리와 통합하여 LoRA를 지원한다.\n' +
      '\n' +
      '각주 38: [https://github.com/PanQiWei/AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)\n' +
      '\n' +
      '\\(\\bullet\\)_llama.cpp39_는 맥북 장치에서 양자화된 LLMaM 모델을 실행할 수 있게 한다. 효율적인 C/C++ 구현을 위해 개발된 INT4, INT5, INT8 양자화를 지원한다. 또한 알파카 및 비쿠나와 같은 다수의 LLMaM 기반 모델을 지원합니다.\n' +
      '\n' +
      '각주 39: [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)\n' +
      '\n' +
      '**Quantized LLMs**. 원래 모델들과 비교하여, 양자화된 언어 모델들은 더 작은 메모리 풋프린트를 취하고, 더 빠른 추론 속도를 가질 가능성이 있다[427, 93, 413]. 최근, BLOOM, GPT-J 및 ChatGLM을 포함하여 공개적으로 이용 가능한 여러 언어 모델의 양자화된 모델 사본이 허그페이스에 다수 출시되었다. 특히 GPTQ [417]은 생성 언어 모델을 양자화하는 데 널리 사용되어 LLMaM 및 OPT에 대한 다양한 양자화된 변형으로 이어진다. 또한, Vicuna 및 WizardLM과 같은 명령어 조정 모델을 양자화하는 데에도 적용되었다. 양자화된 LLM의 수가 많기 때문에 이러한 모델의 해당 링크를 직접 통합하지 않는다. 독자들은 포옹 얼굴에서 검색하면 쉽게 찾을 수 있습니다.\n' +
      '\n' +
      '## 6 Utilization\n' +
      '\n' +
      '사전 훈련 또는 적응 튜닝 후 LLM을 사용하는 주요 접근법은 다양한 작업을 해결하기 위한 적절한 _prompting_ 전략을 설계하는 것이다. 기존 문헌에서는 수동 생성 및 자동 최적화를 통해 태스크별 프롬프트를 효과적으로 학습할 수 있다. 대표적인 프롬프트 방법은 _in-context learning_[50, 55]이며, 이는 작업 설명 및/또는 데모를 자연어 텍스트의 형태로 공식화한다. 또한, _사상 사슬 프롬프트_[33]은 프롬프트에서 일련의 중간 추론 단계를 포함함으로써 문맥 내 학습을 향상시키는 데 사용될 수 있다. 나아가, 복잡한 태스크들을 해결하기 위해 _계획_[439]가 제안되는데, 먼저 이들을 더 작은 서브-태스크들로 분할한 다음, 이러한 서브-태스크들을 하나씩 해결하기 위한 액션 계획을 생성한다. 표 XI에서 이러한 신속한 접근법에 대한 대표적인 작업을 요약한다. 다음으로, 우리는 네 가지 기술에 대한 세부 사항에 대해 자세히 설명할 것이다.\n' +
      '\n' +
      '### _Prompting_\n' +
      '\n' +
      '이전 작업 [36]에서 논의된 바와 같이 프롬프트는 다양한 작업을 해결하기 위해 LLM을 활용하는 주요 접근법이다. 프롬프트의 품질은 특정 작업에서 LLM의 성능에 크게 영향을 미치기 때문에 수동 생성 또는 자동 최적화를 통해 적합한 작업 프롬프트를 생성하기 위해 제안된 일련의 연구가 있으며 이 섹션에서는 소개된다.\n' +
      '\n' +
      '#### 6.1.1 Prompt Creation\n' +
      '\n' +
      '적합한 프롬프트를 수동으로 생성하는 프로세스를 _prompt engineering_[452, 453]이라고도 한다. 잘 설계된 프롬프트는 특정 작업을 수행하기 위한 LLM의 능력을 이끌어내는 데 매우 유용하다. 이 부분에서는 프롬프트의 핵심 구성 요소를 먼저 소개하고 프롬프트 설계를 위한 몇 가지 원칙에 대해 논의할 것이다. 그런 다음 채팅GPT를 다른 프롬프트로 평가하여 몇 가지 대표적인 작업에 대한 결과를 보여준다. 우리는 좋은 프롬프트를 디자인하기 위한 제안과 지침을 제시하는 여러 기존 논문[453, 454]과 웹사이트[455, 456, 457]가 있다는 것을 알고 있다. 비교로서, 우리는 주로 신속한 창작에 유용한 핵심 요소(인젠션과 원칙)에 대해 논의하고, 초보자에게 참조로 인기 과제에 대한 실험 결과와 분석을 제공하는 것을 목표로 한다.\n' +
      '\n' +
      '**주요 구성 요소.** 일반적으로 작업 설명, 입력 데이터, 상황 정보 및 프롬프트 스타일을 포함하여 작업을 완료하는 LLM의 기능을 유도하는 프롬프트의 기능을 나타내는 4가지 주요 구성 요소가 있습니다. 논의를 직관적으로 이해하기 위해 표 XIII XIII에서 질문 응답, 메타 검토 생성 및 텍스트 대 SQL에 대한 세 가지 신속한 예도 제시한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Task description._ 작업 설명은 전형적으로 LLM들이 따를 것으로 예상되는 특정 명령이다. 일반적으로 과제 목표를 자연어로 명확하게 기술해야 한다. 특수 입력 또는 출력 형식을 가진 작업의 경우 자세한 설명이 필요한 경우가 많으며, 작업 완료에서 LLM을 더 잘 안내하기 위해 키워드를 추가로 활용하여 특수 설정을 강조할 수 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_Input data._._ 일반적인 경우, 입력 데이터(_e.g._, LLM에 의해 응답될 인스턴스)를 자연 언어로 기술하는 것은 간단하다. 지식 그래프 및 표와 같은 특수한 입력 데이터의 경우 LLM에 대해 읽을 수 있도록 적절하고 편리한 방법을 적용할 필요가 있다. 구조화 데이터의 경우, 단순성으로 인해 원본 레코드들(_e.g._, 지식 트리플들)을 시퀀스[458]로 변환하기 위해 선형화가 일반적으로 사용된다. 또한, 프로그래밍 언어(_e.g._, 실행가능 코드)는 구조화된 데이터를 공식화하기 위해 또한 활용되었으며, 이는 또한 정확한 결과를 생성하기 위해 외부 도구(_e.g._, 프로그램 실행기)를 사용하는 것을 지원할 수 있다[459, 460].\n' +
      '\n' +
      '\\(\\bullet\\)_Contextual information._ 상기 과제 설명 및 입력 데이터 이외에 상황 정보 또는 배경 정보\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Models**} & \\multirow{2}{*}{**\n' +
      '\\begin{tabular}{c} **STT Dataset** \\\\ \\end{tabular} } & \\multicolumn{4}{c}{**16-bit**} & \\multicolumn{4}{c}{**8-bit**} & \\multicolumn{4}{c}{**4-bit**} \\\\ \\cline{3-13}  & & AlpacaFarm & MMLU & BBH & Mem\\({}_{(\\text{SQL})}\\) & AlpacaFarm & MMLU & BBH & Mem\\({}_{(\\text{SQL})}\\) & AlpacaFarm & MMLU & BBH & Mem\\({}_{(\\text{SQL})}\\) \\\\ \\hline LLMaA (7B) & FLAN-\\(\\lambda\\)2 & 6.65 & 47.34 & 35.05 & 12.58 & 6.15 & 47.02 & 35.17 & 6.65 & 7.83 & 46.23 & 34.77 & 3.94 \\\\  & Alpaca-52K & 35.25 & 40.87 & 33.66 & 12.58 & 33.60 & 39.38 & 34.38 & 6.65 & 29.57 & 39.24 & 32.80 & 3.94 \\\\  & ShareFPT & 72.05 & 41.30 & 32.90 & 12.58 & 72.86 & 39.34 & 32.71 & 6.65 & 70.31 & 40.08 & 32.11 & 3.94 \\\\ \\hline LLMaA (138) & FLAN-\\(\\lambda\\)2 & 81.4 & 51.67 & 41.46 & 24.40 & 7.64 & 51.02 & 41.15 & 12.53 & 7.82 & 50.48 & 40.68 & 7.34 \\\\  & Alpaca-52K & 33.60 & 47.63 & 36.10 & 24.40 & 31.43 & 47.04 & 35.98 & 12.53 & 30.87 & 46.20 & 36.16 & 7.34 \\\\  & ShareFPT & 75.59 & 47.58 & 38.00 & 24.40 & 73.79 & 47.71 & 38.31 & 12.53 & 71.99 & 45.77 & 36.97 & 7.34 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE XIII: Evaluation results for quantized LLMaA models (7B and 13B). We employ existing model checkpoints provided by [353] for quantization experiments, which have been fine-tuned on FLAN-\\(\\nu\\)2, Alpaca-52K, and ShareGPT, respectively. Specifically, we report the performance with AlpacaFarm, MMLU, and BBH, as well as the memory usage of the loaded model (Mem.). For quantization, we employ _bitesandbytes_ to quantize the 16-bit models to 8/4 bits by specifying the commands load_in_8bit and load_in_4bit when loading the weights. It is worth noting that we select _text-davinci-003_ as the baseline model for the AlpacaFarm dataset.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l|l} \\hline \\hline\n' +
      '**Approach** & **Representative Work** & **Key Point** \\\\ \\hline \\multirow{4}{*}{\\begin{tabular}{l} In-context \\\\ Learning (ICL) \\\\ Structured Prompting [296] \\\\ Global \\& LocalE [432] \\\\ \\end{tabular} } & \\begin{tabular}{l} Demonstration selection (similar, k-NN) \\\\ Demonstration selection (dense retrieval; contrastive learning) \\\\ Demonstration selection (LLM as the demonstration generator) \\\\ Demonstration format (automatic generation \\& selection) \\\\ Demonstration format (grouped context encoding; rescaled attention) \\\\ Demonstration order (entropy-based metric; probing set generation with LLM) \\\\ \\end{tabular} \\\\ \\hline \\multirow{4}{*}{\\begin{tabular}{l} Chain-of-though \\\\ Prompting (CoT) \\\\ \\end{tabular} } & ComplexCoT [433] & Demonstration (complexity-based selection) \\\\ Auto-CoT [434] & Demonstration (automatic generation) \\\\ Selection-Inference [435] & Generation (alternate between selection selection and inference) \\\\ Self-consistency [436] & Generation (diverse paths; self-ensemble) \\\\ DIVERSE [437] & Generation (diverse paths): Verification (step-wise voting) \\\\ Rationale-augmented ensembles [438] & Generation (rationale sampling) \\\\ \\hline \\multirow{4}{*}{\n' +
      '\\begin{tabular}{l} Planning \\\\ \\end{tabular} } & Least-to-most prompting [49] & Plan generation (text-based; problem decomposition) \\\\ DECOMY [440] & Plan generation (text-based; problem decomposition) \\\\ PS [441] & Plan generation (text-based) \\\\ Faithful CoT [442] & Plan generation (code-based) \\\\ PAL [443] & Plan generation (code-based; Python) \\\\ HuggingGPT [444] & Plan generation (code-based; models from HuggingFace) \\\\ AdaPLanner [445] & Plan refinement (skill memory) \\\\ TIP [446] & Feedback acquisition (visual perception) \\\\ RAP [447] & Feedback acquisition (LIM as the world model); Plan refinement (Monte Carlo Tree Search) \\\\ ChatCCT [448] & Feedback acquisition (100); Plan refinement (conversation between LLM and tools) \\\\ ReAct [449] & Feedback acquisition (tool); Plan refinement (synergizing reasoning and acting) \\\\ Reflexion [450] & Feedback acquisition (text-based self-reflection); Plan refinement (dynamic memory) \\\\ Tree of Thoughts [451] & Feedback acquisition (vote comparison); Plan refinement (tree-based search) \\\\ \\end{tabular} } \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE XIII: Typical LLM utilization methods and their key points for ICL, CoT, and planning. Note that the key points only highlight the most important technical contribution.\n' +
      '\n' +
      '특정 작업에도 필수적입니다. 예를 들어, 검색된 문서는 증거로서 개방형 질문 답변에 매우 유용하다. 검색된 문서의 품질과 질문과의 관련성은 모두 생성된 답변에 영향을 미친다[461]. 따라서, 적절한 프롬프트 패턴 또는 표현 형식으로 그러한 정보를 포함할 필요가 있다. 또한, 인-컨텍스트 태스크 예시자는 태스크 목표, 특수 출력 형식 및 입력과 출력 사이의 매핑 관계를 더 잘 묘사할 수 있는 복잡한 태스크를 달성하기 위해 LLM을 유도하는 데에도 도움이 된다.\n' +
      '* _Prompt style._ 다양한 LLM의 경우 특정 작업을 해결하는 능력을 이끌어내기 위한 적절한 신속한 스타일을 설계하는 것이 중요하다. 전체적으로 프롬프트를 잘 이해하고 대답할 수 있는 명확한 질문이나 세부 지시로 표현해야 한다. 일부 경우에, LLM을 더 잘 안내하기 위해 접두사 또는 접미사를 추가하는 것이 또한 유용하다. 예를 들어, 접두사 _"단계별로 생각하자"_ 를 사용하면 LLM들이 단계적 추론을 수행하도록 유도하는 데 도움이 될 수 있고, 접두사 _"당신은 이 작업에 대한 전문가(또는 이 도메인에서)"_ 를 사용하면 일부 특정 작업에서 LLM들의 성능을 높일 수 있다. 또한, 채팅 기반 LLMs(_e.g._, ChatGPT)의 경우, 길거나 복잡한 태스크 프롬프트를 직접 피딩하는 대신에, 서브-태스크들에 대한 다수의 프롬프트들로 분해한 후 멀티-턴 대화를 통해 LLMs들로 피딩하는 것이 제안된다[448].\n' +
      '\n' +
      '**디자인 원리.** 프롬프트의 주요 구성 요소를 기반으로 다양한 작업을 해결하는 데 더 효과적인 프롬프트를 만드는 데 도움이 되는 몇 가지 중요한 디자인 원리를 요약합니다.\n' +
      '\n' +
      '_(\\bullet\\)_작업 목표를 명확하게 표현합니다._ 과제 설명이 모호하거나 명확하지 않아야 하며, 이는 부정확하거나 부적절한 응답으로 이어질 가능성이 있다. 이것은 이러한 모델들을 활용할 때 명확하고 명확한 지침의 필요성을 강조한다[66]. 명확하고 상세한 설명은 태스크 목적, 입출력 데이터(_e.g._, _"긴 문서가 주어지면 간결한 요약을 생성하기를 바란다."_), 응답 제약 조건(_e.g._, _"요약의 길이는 50."_를 초과할 수 없다."_)을 포함하는 태스크를 설명하기 위한 다양한 요소를 포함하여야 한다. 잘 규명된 태스크 설명을 제공함으로써, LLM들은 타겟 태스크를 보다 효과적으로 이해하고 원하는 출력을 생성할 수 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_쉽고 자세한 하위 작업으로 분해합니다._ 복잡한 태스크들을 해결하기 위해, LLM들이 목표를 달성하는 것을 단계적으로 돕기 위한 몇 개의 더 쉽고 상세한 서브-태스크들로 분해하는 것이 중요하며, 이는 섹션 6.4의 계획 기법과 밀접한 관련이 있다. 예를 들어, 제안 [454]에 따라, 우리는 다수의 번호가 매겨진 항목들(_e.g._, _"Braid a coherent narrative by performing the tasks: 1...., 2...., 3...."_)의 형태로 서브-태스크들을 명시적으로 나열할 수 있다. 타겟 태스크를 서브 태스크들로 분해함으로써, LLM은 보다 쉬운 서브 태스크들의 해결에 집중할 수 있고, 최종적으로 복잡한 태스크들에 대해 보다 정확한 결과를 얻을 수 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_Providing few-shot demonstration._ 섹션 6.2에서 논의된 바와 같이, LLM은 복잡한 태스크를 해결하기 위한 문맥 내 학습으로부터 이익을 얻을 수 있으며, 프롬프트에는 원하는 입력-출력 쌍, _i.e._, 소수의 샷 데모의 소수의 태스크 예가 포함된다. 소수 샷 시연은 LLM이 매개변수 조정 없이 입력과 출력 사이의 의미 매핑을 학습하는 데 도움이 될 수 있다. 실제로는 목표 과제에 대해 몇 가지 고품질 시연을 생성해야 하며, 이는 최종 과제 수행에 큰 도움이 될 것이다.\n' +
      '\n' +
      '\\(\\bullet\\)_Utilizing model-friendly format.__ LLM은 특별히 구성된 데이터 세트에 대해 사전 훈련되기 때문에 LLM이 지침을 더 잘 이해할 수 있도록 하는 몇 가지 프롬프트 형식이 있다. 예를 들어 OpenAI 설명서에서 제안 하는 대로 *** 또는 *** 를 중지 기호로 사용 하 여 명령어와 컨텍스트를 분리할 수 있으며 LLM에서 더 잘 이해할 수 있습니다. 일반적인 지침으로 대부분의 기존 LLM은 영어에서 더 나은 작업을 수행하므로 기계 번역을 기반으로 어려운 작업을 해결하기 위해 영어 지침을 사용하는 것이 유용하다.\n' +
      '\n' +
      '**유용한 팁.** 디자인 원리 외에도 기존 작업 또는 표 12의 경험적 경험을 기반으로 유용한 프롬프트 팁 컬렉션을 제시합니다. 이러한 팁은 일반적인 방식으로 제안되지만 해당 작업에 대한 최상의 프롬프트임을 표시하지 않습니다. 이 부분은 더 많은 지침이나 팁으로 지속적으로 업데이트될 것입니다. 우리는 독자들이 이 신속한 팁 모음에 기여하는 것을 환영합니다. [https://github.com/RUCAIBox/LLMSurvey/tree/main/Prompts](https://github.com/RUCAIBox/LLMSurvey/tree/main/Prompts) 링크에서 프롬프트 팁에 기여하는 자세한 절차를 제공합니다.\n' +
      '\n' +
      '**경험적 분석.** 프롬프트가 작업 성능에 미치는 영향을 제시하기 위해 경험적 연구를 추가로 수행합니다. 실험을 수행하기 위해 언어 생성, 지식 활용, 복합 추론, 구조 데이터 생성, 정보 검색에 걸쳐 다양한 작업을 선택한다. 각 작업에 대해 위에서 소개한 일반적인 지침을 따르는 프롬프트를 수동으로 작성한다. 테스트된 프롬프트는 주로 독자들이 다른 태스크를 해결하기 위한 효과적인 프롬프트를 작성하는 방법을 이해하는 것을 목표로 하기 때문에 이러한 태스크에 최적이 아닐 수 있다는 점에 유의해야 한다. 또한 대부분의 작업에 대한 비교로 단순화된 프롬프트를 추가합니다. 섹션 7.4의 실험 설정에 따라 복잡한 추론 태스크(컬러 오브젝트 및 GSM8k)에 대한 ChatGPT의 3샷 성능과 다른 태스크에 대한 제로샷 성능을 조사한다. 우리는 표 17의 실험 결과를 보고하며, 여기서 기존 논문에 감독 성능도 참조로 포함한다.\n' +
      '\n' +
      '\\(\\bullet\\)_신중하게 설계된 프롬프트는 ChatGPT의 제로 샷 또는 소수 샷 성능을 높일 수 있습니다._ 동일한 태스크에서 서로 다른 프롬프트를 사용한 결과를 비교함으로써, 세심하게 설계된 프롬프트를 사용하는 것이 단순한 프롬프트보다 더 나은 성능을 얻을 수 있음을 알 수 있다. 세심하게 설계된 프롬프트에서 보다 명확하게 표현된 작업 설명(_e.g._, WMT 및 WikiFact)을 제공하거나 모델 친화적인 형식(_e.g._, GSM8k 및 OBQA)을 사용합니다. 예를 들어, 위키팩트 태스크의 경우, 보다 상세한 태스크 설명을 갖는 프롬프트는 29.25에서 31.21로 성능 증가로 이어진다.\n' +
      '\n' +
      '\\(\\bullet\\)_더 복잡한 작업은 ChatGPT에 대한 신중한 프롬프트 엔지니어링에서 더 많은 이점을 얻을 수 있습니다._ WikiFact 및 Colored Objects 작업에서 설계된 프롬프트는 ChatGPT _i.e_의 성능을 WikiFact에서 23.61에서 28.47로, Colored Objects에서 53.20에서 66.75로 크게 향상시켰습니다. 이러한 작업은 일반적으로 특정 출력 형식을 갖거나 배경 지식을 요구하기 때문에 복잡한 작업에서 LLM이 잘 수행되기 위한 신속한 엔지니어링의 필요성을 나타낸다. 예제 프롬프트는 더 자세한 작업 설명(_e.g._, 출력 형식 및 작업 목표)을 제공하므로 ChatGPT가 이를 충족하기 위한 복잡한 작업 요구 사항을 더 잘 이해하는 데 도움이 될 수 있습니다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:50]\n' +
      '\n' +
      '작업별 데이터로 특수하게 최적화되었습니다.\n' +
      '\n' +
      '\\(\\bullet\\)_적절한 프롬프트 엔지니어링을 통해 LLM은 일부 비전통적인 NLP 작업을 처리할 수 있습니다. 특정 프롬프트의 도움으로 ChatGPT는 비전통적인 NLP 작업, 즉 일반 권장 사항 및 대화 권장 사항을 수행할 수도 있습니다. 핵심은 이러한 과제들이 자연어로 잘 표현되거나 기술될 수 있다는 것이다. 그러나, ChatGPT의 성능은 LLM들이 특정 도메인 지식 및 작업 적응을 필요로 하는 이러한 작업들을 직접 맞출 수 없기 때문에, 이러한 작업들에서 참조된 성능과는 여전히 멀다[357, 462].\n' +
      '\n' +
      '#### 6.1.2 Prompt Optimization\n' +
      '\n' +
      '작업 프롬프트를 수동으로 만드는 것이 더 직관적이지만 시간이 많이 걸리고 더 중요한 것은 모델이 조작된 프롬프트에 매우 민감하다는 것입니다. 부적절한 프롬프트는 낮은 작업 성능(표 IV에 표시됨)으로 이어집니다. 따라서, 많은 연구가 최적의 성능을 달성하기 위해 이산 프롬프트 및 연속 프롬프트에 대한 자동 최적화 접근법을 제안한다[396, 405]. 이 부분에서는 이러한 연구를 두 가지 관점, 즉 이산 프롬프트와 연속 프롬프트에서 자세히 설명한다.\n' +
      '\n' +
      '**이산 프롬프트 최적화** 이산 프롬프트는 일반적으로 자연 언어 토큰 시퀀스로 구성 됩니다. 형태가 간단하고 유연함에도 불구하고 이산 공간에서 프롬프트를 최적화하는 것은 조합적인 거대한 탐색 공간으로 인해 어려운 문제이다. 다운스트림 작업에 대한 효과적인 프롬프트를 자동으로 검색하기 위해 기존 연구에서는 광범위한 이산 프롬프트 접근법을 제안하며, 이를 자세히 설명하면 다음과 같다.\n' +
      '\n' +
      '\\(\\bullet\\)_Gradient 기반 접근 방식입니다. 이러한 종류의 접근법들은 구배 업데이트를 통해 출력 가능성을 최대화함으로써 신속한 검색 프로세스를 최적화하는 것을 목표로 한다[405, 464, 465, 466]. 대표적인 작업으로 Auto-Prompt[405]는 프롬프트 토큰을 어휘에서 다른 후보 토큰으로 교체할 때 로그 우도의 변화로 근사화된 그래디언트를 활용하여 프롬프트의 각 위치에 대한 최적의 토큰을 탐욕스럽게 탐색하는 그래디언트 유도 방법을 제안한다. 그러나, 이러한 검색 프로세스는 프롬프트의 각각의 위치에 대해 각각의 후보 토큰을 평가할 필요가 있기 때문에 매우 고가일 수 있고, 다수의 추가 전진 패스로 이어진다. 따라서, 이산 토큰을 연속 임베딩으로 변환하고 최적화 동안 연속 공간 상의 그래디언트를 컴퓨팅함으로써 개선된 그래디언트 방법[464]이 제안되었다.\n' +
      '\n' +
      '\\(\\bullet\\)_RL 기반 접근 방식._ 이산 프롬프트는 구배 역전파를 통해 학습되기 어렵기 때문에, 다수의 연구는 이산 프롬프트 최적화를 강화 학습(RL) 문제로 공식화하고 최적화를 위한 레버리지 RL 알고리즘을 제안한다[467, 468]. 예를 들어, RLPrompt[467]은 정책 네트워크를 트레이닝하여 다수의 보상 기능을 갖는 원하는 프롬프트를 생성한다. 이 접근법에서는 RL 훈련 효율성을 높이기 위해 몇 가지 효과적인 보상 안정화 전략도 제안한다. 훈련에 충분한 데이터를 필요로 하는 이전 작업과 비교하여, TEMPERA[468]는 미리 훈련된 RL 에이전트를 활용하여 수동으로 작성된 초기 프롬프트의 상이한 부분을 순차적으로 편집함으로써 테스트 시간에 프롬프트를 직접 생성하는 것을 제안한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Edit 기반 접근 방식._ 상기 방법들에 대해, 그래디언트-기반 및 RL-기반 튜닝은 훨씬 더 큰 모델들에 대해 극도로 계산적으로 요구될 수 있고, API-기반 모델 호출들(_예를 들어,_ ChatGPT)에 대해 실현 가능하지 않을 수 있다. 따라서 다른 작업 라인은 작업 수행에 따라 기존 프롬프트를 직접 편집하는 것을 목표로 합니다. 구체적으로, GPS[469]는 유전 알고리즘으로부터 아이디어를 차용하여 언어 모델(_i.,_T5)을 활용하여 클로즈 태스크 형태를 취하여 프롬프트를 편집하는 유전 프롬프트 탐색 방법을 제안한다. 모델 기반 편집 방법 외에도, 삭제, 스왑, 패러프레이즈 및 추가를 포함하는 신속한 편집 [470]을 위해 인간 정의 작업이 또한 채용될 수 있다. 이러한 작업을 기반으로 프롬프트를 반복적으로 편집 하 고 작은 예제 풀에서 모델 성능에 의해 안내 되는 최상의 프롬프트를 탐욕스럽게 검색 합니다.\n' +
      '\n' +
      '\\(\\bullet\\)_LLM 기반 접근 방식입니다._ LLM의 예외적인 용량으로 인해, 신속한 생성기로서 LLM을 직접적으로 활용하는 연구가 증가하고 있다[471, 472, 473]. 구체적으로, APE[471]는 LLM을 활용하여 초기 프롬프트를 생성한 후, 가장 높은 정확도를 갖는 최상의 프롬프트를 선택하고, 최종적으로 반복 몬테 카를로 탐색 방법을 통해 최적의 후보를 개선한다. 유사하게, APO [472]는 LLM에 구 프롬프트를 새로운 개선된 프롬프트로 정제하는 방법에 대한 텍스트 피드백을 생성하도록 지시한다. 그러나 프롬프트 공간에서의 검색은 이전 프롬프트의 전체 정제 추적을 완전히 고려하지 않으면 비효율적일 수 있으므로 잠재적으로 차선책으로 이어질 수 있다. 따라서 다른 연구 [473]은 이전 프롬프트에 점수를 통합하여 LLM이 점진적으로 더 나은 새 프롬프트를 생성하도록 지시한다. 그러나 이러한 접근 방식은 여전히 효과적인 프롬프트의 방대한 공간을 탐색하는 데 어려움을 겪고 있다. 인간과 같은 시행착오에서 영감을 받은 프롬프트 최적화는 전략적 계획 문제[474]로 더 공식화되고 몬테카를로 트리 탐색을 사용하여 방대한 프롬프트 공간을 탐색한다.\n' +
      '\n' +
      '**연속 프롬프트 최적화.** 개별 프롬프트와 달리 연속 프롬프트는 다운스트림 작업의 손실을 기반으로 하는 그래디언트 업데이트를 통해 직접 최적화될 수 있는 연속 임베딩 세트로 구성됩니다. 지속적인 신속한 최적화는 주로 PLM에서 연구되어 왔지만, LLM 시대에는 많은 매개변수의 크기로 인해 제한된 관심을 받고 있다. 콘텐츠 완성도를 위해 이 부분에 대한 논의를 포함합니다. 선행 작업에서 대부분의 연구는 일반적으로 작업 데이터를 기반으로 연속 프롬프트를 훈련하기 위해 지도 학습에 의존한다. 또한, 데이터 부족 시나리오에서 전이 학습 방법을 사용하여 대상 작업에 레이블이 지정된 데이터의 부족을 완화할 수 있다. 이 두 가지 접근법은 아래에 자세히 설명되어 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_Prompt learning with sufficient data._ 이 접근법에서, 대부분의 기존 방법들은 연속 프롬프트를 트레이닝 가능한 모델 파라미터로 간주하고, 이어서 충분한 다운스트림 태스크 데이터에 기초하여 교차 엔트로피 손실을 최소화함으로써 연속 프롬프트를 최적화하도록 지도 학습을 레버리지한다[475, 396, 397, 401]. 섹션 5.3.1에서 논의된 바와 같이, 프리픽스 튜닝[396]은 언어 모델들에서 각각의 트랜스포머 계층에 프리픽스들의 시퀀스(_i.,_트레이닝가능한 연속 벡터들의 세트)를 프리펜딩하는 반면, 프롬프트 튜닝[397]은 입력 계층에서 트레이닝가능한 프롬프트 벡터들만을 통합한다. LLM들의 대규모 파라미터들을 고정하고 연속적인 프롬프트 벡터만을 튜닝함으로써, 이러한 종류의 접근법들은 매우 파라미터 효율적일 수 있다(섹션 5.3). 그러나, 이러한 접근법들은 전형적으로 입력과 무관하며, 입력 의미론에 대한 충분한 고려가 부족하다. 따라서 [475]의 저자들은 입력 텍스트를 기반으로 연속 프롬프트를 도출하고 다운스트림 태스크 손실을 통해 학습하는 컨텍스트 튜닝을 제안한다.\n' +
      '\n' +
      '_(\\bullet\\)_Prompt transferring with scarce data._ 지도 학습 접근법은 최적의 연속 프롬프트를 학습하기 위해 충분한 훈련 데이터를 요구하며, 이는 데이터가 부족한 도메인 및 작업에서 잘 작동하지 않을 수 있다. 이러한 문제를 해결하기 위해, SPoT[476]는 프롬프트 기반 전이 학습 접근법을 제안하는데, 프롬프트 기반 전이 학습 접근법은 먼저 여러 대표적인 소스 태스크에 대한 단일 연속 프롬프트를 학습한 다음, 이 프롬프트를 사용하여 타겟 태스크에 대한 프롬프트를 초기화한다. 그러나이 접근 방식은 대상 작업의 모든 인스턴스를 해결하는 데 동일한 프롬프트를 활용합니다. 단일 작업의 경우 잘 학습된 프롬프트도 많은 모집단의 모든 데이터 인스턴스에 적합하지 않을 수 있습니다. 이 문제를 해결하기 위해 개선된 방법 [477]은 태스크 및 인스턴스 수준 정보를 모두 고려하여 프롬프트 전달 프로세스 동안 적응형 주의 메커니즘을 설계하여 대상 프롬프트를 도출한다. 프롬프트 전송 패러다임은 데이터 부족 대상 태스크를 해결하기 위해 소스 프롬프트에 인코딩된 데이터 부족 소스 태스크의 지식을 활용할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{227.6pt}} \\hline \\hline Use the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write “I could not find an answer.” \\\\\n' +
      '‘기사:** “\\(\\sim\\)자오 무티뉴는 프리미어리그 구단 울버햄튼 원더러스와 포르투갈 대표팀의 중앙 미드필더로 마지막으로 뛴 포르투갈 축구선수” \\\\\n' +
      '**질문:** 다음 문장이 그럴듯한가요? 주앙 무티뉴가 3루에서 아웃되었다." \\\\\n' +
      '**정답:** 단계별로 생각해 봅시다. 주앙 무티뉴는 축구 선수이다. 3루에 아웃되는 것은 축구가 아니라 야구의 일부이다. 그래서 대답은 No. \\\\\\(\\sim\\)Demonations\\(>\\) \\\\\n' +
      '** 아티클:**\\textlessnert 아티클 각각은 삼중 따옴표로 구분됩니다. \\textgreater{} \\\\\n' +
      '**질문:**\\textlessnert 질문\\textgreater{} \\\\\n' +
      '**Answer:**\\textlessnert. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\begin{tabular}{p{227.6pt}} \\hline \\hline Prepare a meta-review by answering the following questions from the reviewer comments (provided after the questions). \\\\\n' +
      '1. 검토자의 의견을 기반으로 이 원고의 핵심 기여는 무엇입니까? \\\\\n' +
      '2. 여러 검토자가 언급한 이 작업의 일반적인 강점은 무엇입니까? \\\\\n' +
      '3. 여러 검토자가 강조 표시 하는 이 작업의 일반적인 약점은 무엇인가요? \\\\\n' +
      '4. 이 문서를 개선하기 위해 어떤 제안을 제공하시겠습니까? \\\\\n' +
      '5. 개별 리뷰에서 언급된 누락된 참조는 무엇인가요? \\\\\n' +
      '**검토 텍스트는 아래입니다.* *\\textlessnert 검토자의 세 가지 주석 \\(R_{1}\\), \\(R_{2}\\), \\(R_{3}\\)\\textgreater{} \\\\\n' +
      '**메타 검토:**\\textlessnert 메타 검토\\textgreater{} \\\\ \\(\\sim\\) 시연\\textgreater{} \\\\ 실제로 선택한 이유를 설명 하 여 응답에 대 한 정당화를 자세히 제공 합니다. 좋은 출력은 일관성이 있어야 하며 여러 검토자가 언급 하는 주요 강점/문제를 강조 하 고 길이가 400 단어 미만이어야 하며 마지막으로 응답은 영어로만 표시 되어야 합니다. \\\\\n' +
      '**검토 텍스트는 아래입니다.* *\\textlessnert 검토자의 세 가지 주석 \\(R_{1}\\), \\(R_{2}\\), \\(R_{3}\\)\\textgreater{} \\\\\n' +
      '**메타 검토:**\\textlessnert 메타 검토\\textgreater{} \\\\ \\(\\sim\\) 시연\\textgreater{} \\\\ 실제로 선택한 이유를 설명 하 여 응답에 대 한 정당화를 자세히 제공 합니다. 좋은 출력은 일관성이 있어야 하며 여러 검토자가 언급 하는 주요 강점/문제를 강조 하 고 길이가 400 단어 미만이어야 하며 마지막으로 응답은 영어로만 표시 되어야 합니다. \\\\\n' +
      '**검토 텍스트는 아래입니다.* *\\textlessnert 검토자의 세 가지 주석 \\(R_{1}\\), \\(R_{2}\\), \\(R_{3}\\)\\textgreater{} \\\\\n' +
      '**메타 검토:**\\textlessnert 메타 검토\\textgreater{} \\\\ \\(\\sim\\) \\\\ 실제로 선택한 이유를 설명 하 여 응답에 대 한 정당화를 자세히 제공 합니다. 좋은 출력은 일관성이 있어야 하며 여러 검토자가 언급 하는 주요 강점/문제를 강조 하 고 길이가 400 단어 미만이어야 하며 마지막으로 응답은 영어로만 표시 되어야 합니다. \\\\\n' +
      '**검토 텍스트는 아래입니다.* *\\textlessnert 검토자의 세 가지 주석 \\(R_{1}\\), \\(R_{2}\\), \\(R_{3}\\)\\textgreater{} \\\\\n' +
      '**Meta-review:**\\textlessnert meta-review\\textgreater{} \\\\ \\(\\sim\\) \\\\ \\hline \\hline CREATE TABLE Hightschooler (ID int primary key, name text, grade int ). /* \\\\\n' +
      '3 example rows: SELECT * FROM Highschooler LIMIT 3; ID name grade 1234 Janie 8 5678 Mary 8 9012 Mike 9 \\\\\n' +
      '*/ \\\\ 유효한 SQLite를 사용하여 위에 제공된 테이블에 대해 다음 질문에 답변합니다. \\\\\n' +
      '**질문:** 카일의 id는 무엇인가요? \\\\ SQL: SELECT ID FROM Highschooler WHERE name=“Kyle”; \\\\ \\(\\sim\\)Demonstrations\\textgreater{} \\\\\n' +
      '**Question:**\\textlessnert question\\textgreater{} \\\\ SQL: \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE XIII: Example instructions collected from [454, 463]. The blue text denotes the task description, the red text denotes the contextual information, the green text denotes the demonstrations, and the gold text denotes the prompt style.\n' +
      '\n' +
      '### _In-Context Learning_\n' +
      '\n' +
      '특별한 프롬프트 형태로서, LLM을 활용하는 전형적인 접근법이 된 GPT-3[55]와 함께 인-컨텍스트 학습(ICL)이 먼저 제안된다.\n' +
      '\n' +
      '#### 6.2.1 ICL 제형\n' +
      '\n' +
      '[55]에서 언급된 바와 같이, ICL은 태스크 설명 및/또는 몇 개의 태스크 예들로 구성된 포맷된 자연 언어 프롬프트를 데모로 사용한다. 도 14는 ICL의 예시를 나타낸다. 먼저 작업 설명부터 작업 데이터 집합에서 몇 가지 예를 데모로 선택 합니다. 그런 다음 특정 순서로 결합하여 특수하게 설계된 템플릿으로 자연어 프롬프트를 형성합니다. 마지막으로 테스트 인스턴스는 출력을 생성하기 위한 LLM의 입력으로 데모에 추가된다. 작업 시연을 기반으로 LLM은 명시적인 기울기 업데이트 없이 새로운 작업을 인식하고 수행할 수 있다.\n' +
      '\n' +
      '형식적으로, \\(D_{k}=\\{f(x_{1},y_{1}),\\ldots,f(x_{k},y_{k})\\}\\)는 \\(k\\) 예제를 가진 일련의 시범을 나타내도록 하자. 여기서 \\(f(x_{k},y_{k})\\)는 \\(k\\) 번째 과제 예제를 자연어 프롬프트로 변환하는 프롬프트 함수이다. 작업 설명 \\(I\\), 시연 \\(D_{k}\\) 및 새 입력 쿼리 \\(x_{k+1}\\)이 주어지면 LLM에서 생성된 출력 \\(\\hat{y}_{k+1}\\)의 예측은 다음과 같이 공식화될 수 있습니다.\n' +
      '\n' +
      '각주 40: ICL이 GPT-3의 논문[55]에 소개되었을 때, 그것은 원래 두 구성 요소 중 하나가 필요하지 않은 작업 설명과 시연 예제의 조합으로 정의되었다. 이러한 정의에 따라, 태스크 설명만을 사용하여 보이지 않는 태스크를 해결하기 위해 LLM이 요구될 때, 태스크 해결을 위해 ICL을 수행하는 것도 고려될 수 있는 반면, ICL 능력은 명령어 튜닝에 의해 향상될 수 있다.\n' +
      '\n' +
      '\\[\\text{LLM}\\big{(}I,\\underbrace{f(x_{1},y_{1}),\\ldots,f(x_{k},y_{k})}_{\\text{ demonstrations}},f(\\underbrace{x_{k+1}}_{\\text{input answer}})\\big{)}\\rightarrow\\hat{y}_{k+1}. \\tag{12}\\]\n' +
      '\n' +
      '여기서 실제 답 \\(y_{k+1}\\)은 LLM에 의해 예측될 공백으로 남겨진다. ICL의 성능은 데모에 크게 의존하기 때문에 프롬프트에서 적절하게 설계하는 것이 중요하다. 식 (12)의 구성 과정에 따라 시연을 구성하는 예제를 선택하는 방법, 각 예제를 함수가 \\(f(\\cdot)\\인 프롬프트에 형식화하는 방법, 합리적인 순서로 시연을 배열하는 방법 등 프롬프트에서 시연을 형식화하는 세 가지 주요 측면에 초점을 맞춘다.\n' +
      '\n' +
      'ICL에 대한 포괄적인 검토가 조사 논문[50]에 제시되었으며, 이 주제에 대한 보다 일반적이고 상세한 논의를 위해 이를 참조하는 독자를 제안한다. 이 조사와 비교하여 우리는 특히 두 가지 주요 측면, 즉 시연 설계와 ICL의 기본 메커니즘에서 ICL을 LLM에 적용하는 논의에 중점을 둔다. 또한, ICL은 둘 다 자연 언어를 사용하여 작업 또는 인스턴스를 포맷한다는 점에서 명령 튜닝(섹션 5.1에서 논의됨)과 밀접한 관련이 있다. 그러나 명령어 튜닝은 적응을 위해 LLM을 미세 조정해야 하는 반면 ICL은 활용을 위해 LLM만 프롬프트한다. 나아가, 명령어 튜닝은 특히 제로 샷 설정(작업 설명만을 사용함)에서 타겟 작업을 수행하는 LLM의 ICL 능력을 향상시킬 수 있다[69].\n' +
      '\n' +
      '#### 6.2.2 Demonstration Design\n' +
      '\n' +
      '여러 연구에 따르면 ICL의 효과는 시연 설계의 영향을 많이 받는 것으로 나타났다[432, 478, 479]. 섹션 6.2.1의 논의에 이어, 우리는 _즉, 시연 선택, 형식 및 순서라는 세 가지 주요 측면에서 ICL의 시연 설계를 소개할 것이다.\n' +
      '\n' +
      '**시연 선택.** ICL의 성능은 다른 시연 예제와 함께 큰 분산을 갖는 경향이 있으므로 LLM의 ICL 기능을 효과적으로 활용할 수 있는 예제의 하위 집합을 선택하는 것이 중요합니다. 두 가지 주요 실증 선택 접근법, 즉 휴리스틱 및 LLM 기반 접근법이 있다:\n' +
      '\n' +
      '\\(\\bullet\\)_Heuristic approach._ 단순성과 저렴한 비용으로 인해 기존 작업은 시연을 선택하기 위해 휴리스틱 방법을 널리 채택한다. 여러 연구에서 \\(k\\)-NN 기반 검색기를 사용하여 질의와 의미적으로 관련된 예제를 선택한다[428, 480]. 그러나, 이들은 전체로서 예 세트를 평가하는 것이 아니라, 각각의 예들에 대해 개별적으로 선택을 수행한다. 이 문제를 해결하기 위해 특정 작업에 대해 가장 대표적인 예제 집합을 선택하는 다양성 기반 선택 전략을 제안한다[481, 482]. 나아가 [483]에서는 시연을 선정할 때 관련성과 다양성을 모두 고려하고 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_LLM 기반 접근 방식입니다._ 또 다른 작업 라인은 LLM을 사용하여 시연을 선택합니다. 예를 들어, LLM은 예를 추가한 후 성능 이득에 따라 각 예의 정보성을 직접 측정하는 데 활용될 수 있다[484]. 또한, EPR[429]은 먼저 비감독 방법(_예를 들어,_BM25)으로 유사한 예들을 리콜한 다음, 조밀한 리트리버(LLM에 의해 라벨링된 포지티브 예들 및 네거티브 예들로 트레이닝됨)를 사용하여 순위를 매기는 이단계 검색 접근법을 제안한다. 대안적인 접근법으로서, 실증 선택의 과제는 RL 문제로 공식화될 수 있으며, 여기서 LLM은 정책 모델을 훈련시키기 위한 피드백을 제공하기 위한 보상 함수로서 기능한다[485]. LLM은 텍스트 주석을 위해 잘 수행되기 때문에[486], 최근의 일부 연구에서는 인간의 개입 없이 LLM 자체를 시연 생성기로 사용한다[487].\n' +
      '\n' +
      '요약하자면, [488]에서 논의된 바와 같이, ICL에서 선택된 데모 예들은 위의 두 가지 선택 접근법들에 대해 테스트 질의와 관련될 뿐만 아니라 풀어야 할 태스크에 대한 충분한 정보를 포함해야 한다.\n' +
      '\n' +
      '**시연 형식.** 작업 예제를 선택한 후 다음 단계는 LLM에 대 한 자연 언어 프롬프트에 통합 하 고 형식을 지정 하는 것입니다. 간단한 방법은 대응하는 입력-출력 쌍들로 미리 정의된 템플릿을 인스턴스화하는 것이다[36]. 더 많은 정보를 제공하는 템플릿을 구성하기 위해 최근 연구에서는 태스크 설명 추가[69] 또는 체인 오브 생각 프롬프트(33)를 사용하여 LLM의 추론 능력을 향상시키는 것을 고려한다. 예를 들어 [166]에서 저자는 인간이 작성한 작업 설명과 함께 대규모 데이터 세트를 수집한다. 이 데이터세트로 튜닝한 후, 보여지는 작업에 대한 성능이 부스팅될 수 있고, LLM은 또한 어느 정도 보이지 않는 작업을 일반화할 수 있다. 주석 비용을 줄이기 위해 [143]에서 LLM이 새로운 작업에 대한 작업 설명을 생성하도록 안내하기 위해 인간이 작성한 작업 설명으로 구성된 시드 세트를 사용하여 반자동화 접근법이 제안되었다. 다른 작업에 대해 데모 형식을 수동으로 주석하는 것은 비용이 많이 들기 때문에 일부 작업에서는 고품질 형식을 자동으로 생성하는 방법도 연구한다. 두 가지 대표적인 방법으로 Auto-CoT[434]는 중간 추론 단계들을 생성하기 위해 제로 샷 프롬프트 _"단계별로 생각하자"_와 함께 LLM들을 활용하는 반면, 최소-최소의 프롬프트 [439]는 먼저 LLM들에게 문제 분해를 수행하도록 질의한 후 LLM들을 활용하여 이전에 해결된 중간 답변들에 기초하여 서브-문제들을 순차적으로 해결한다.\n' +
      '\n' +
      '**시연 순서.** LLM은 때때로 최신 편향으로 고통 받는 것으로 표시 됩니다. 즉, 시연이 끝날 무렵에 답변을 반복 하는 경향이 있습니다 [479]. 따라서, 합리적인 순서로 시연(_i.e._, 작업 예)을 배열하는 것이 중요하다. 초기 연구에서는 좋은 순서를 빠르게 찾기 위한 몇 가지 휴리스틱 방법을 제안한다. 예를 들어, 데모는 임베딩 공간 내의 질의에 대한 그들의 유사성에 따라 직접 조직될 수 있다[428]: 유사할수록 끝에 더 가깝다. 또한, 전역 및 로컬 엔트로피 메트릭들은 상이한 시연 순서들을 스코어링하기 위해 사용될 수 있다[432]. 더 많은 태스크 정보를 통합하기 위해, 몇몇 최근의 연구들은 태스크 라벨들을 압축하고 전송하는데 필요한 코드 길이를 최소화하는 것을 제안하는데, 이는 정보 이론으로부터 영감을 받았다[489]. 그러나, 이러한 방법들은 특정 시연 오더의 성능을 평가하기 위해 검증 세트로서 추가적인 라벨링된 데이터가 필요하다. 이러한 필요성을 제거하기 위해 [432]의 저자는 LLM 자체에서 검증 데이터를 샘플링할 것을 제안한다.\n' +
      '\n' +
      '#### 6.2.3 Underlying Mechanism\n' +
      '\n' +
      '사전 훈련 후 LLM은 업데이트되지 않고 흥미로운 ICL 기능을 나타낼 수 있다. 다음에서는 LLM의 ICL 능력에 대한 두 가지 주요 질문인 _i.e._, _"사전 훈련이 ICL 능력에 영향을 미치는 방법"_ 및 _"LLM이 추론 중에 ICL을 수행하는 방법"_에 대해 논의한다.\n' +
      '\n' +
      '**사전 훈련이 ICL에 영향을 미치는 방법?** ICL은 GPT-3 [55]에서 처음 제안되었으며 모델 크기가 클수록 ICL 능력이 더 중요해지는 것으로 나타났습니다. 또한, 일부 연구에서는 소규모 PLM이 특수 설계된 훈련 작업에 대한 지속적인 사전 훈련[490] 또는 미세 조정[491]에 의해 강력한 ICL 능력을 입증할 수 있음을 보여주며, 이는 일반적으로 훈련 프로세스 동안 입력에 추가 작업 예제를 포함한다. 이는 훈련 과제의 설계가 LLM의 ICL 능력에 중요한 영향 요인임을 시사한다. 훈련 작업 외에도 최근 연구에서는 ICL과 사전 훈련 말뭉치 사이의 관계를 조사했다[488, 492]. 예를 들어, ICL은 이론적으로 장거리 일관성을 나타내는 문서에 대한 사전 훈련의 곱으로 설명될 수 있다[488]. 또한, 또 다른 연구[492]는 파라미터 및 데이터를 스케일링할 때, 다음 단어 예측에 기초한 LLM이 언어 데이터에 존재하는 구성 구조(_e.g._, 단어와 구가 어떻게 조합되어 문장과 같은 더 큰 언어 단위를 형성하는지)로부터 학습함으로써 ICL의 능력을 발현할 수 있다는 것을 이론적으로 분석한다.\n' +
      '\n' +
      '**LLM이 ICL을 수행하는 방법?** 추론 단계에서 연구자들은 명시적인 학습 또는 업데이트가 관련되지 않았기 때문에 주어진 데모를 기반으로 ICL 기능이 작동하는 방법을 분석하는 데 중점을 둡니다. [493]의 논의에 따르면 LLM이 데모를 활용하는 방법은 크게 과제 인식과 과제 학습 두 가지가 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_Task recognition._ 첫 번째 방법으로 LLM은 시연에서 작업을 인식하고 사전 훈련에서 얻은 사전 지식을 활용하여 새로운 테스트 작업을 해결한다. ICL의 학습 가능성을 평가하기 위해 PAC(Probably Approximate Correct) 프레임워크[494]가 제안되었다. 사전 훈련 데이터에 작업을 나타내는 잠재 변수가 있다고 가정하고 LLM은 시연에서 이 변수를 캡처할 수 있어 ICL에서 작업을 인식할 수 있는 것으로 나타났다. 또한 과제 인식으로서 ICL의 해석은 여러 실증 연구[478, 495]에 의해 뒷받침된다. 예를 들어, 데모들의 입력들 또는 라벨들을 입력 또는 라벨 공간으로부터 샘플링된 랜덤한 것들로 대체하는 것이 LLM들의 성능을 심각하게 손상시키지 않는다는 것이 관찰되었는데, 이는 LLM들이 그것들로부터 학습하는 대신에 데모들로부터 타겟 태스크를 주로 인식한다는 것을 나타낸다[478, 493]. 유사하게, LLM은 프롬프트 템플릿이 무관하거나 오판의 소지가 있는 경우에도 적절한 성능을 나타낼 수 있다[495].\n' +
      '\n' +
      '도. 14: In-context learning(ICL)과 chain-of-thought(CoT) 프롬프트의 비교 예시. ICL은 자연어 설명, 여러 시연 및 테스트 쿼리로 LLM을 프롬프트하는 반면 CoT 프롬프트는 프롬프트에서 일련의 중간 추론 단계를 포함한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Task learning._ 두 번째 방법으로 LLM은 시범을 통해서만 사전 훈련 단계에서 보이지 않는 새로운 과제를 학습한다. 특히 과제학습은 주로 경사하강 관점에서 분석되며, 암묵적 미세조정으로 고려된다[496, 65]. ICL은 다음과 같이 설명할 수 있다. 전방 계산을 통해 LLM은 시연에 대한 메타 구배를 생성하고 암시적으로 주의 메커니즘을 통해 구배 하강을 수행한다. 실험들은 또한 LLM들 내의 특정 어텐션 헤드들이 ICL 능력과 밀접하게 관련된 작업-불가지론적인 원자 연산들(_예를 들어,_복사 및 프리픽스 매칭)을 수행할 수 있음을 보여준다[497]. 또한, 알고리즘 학습 과정으로서 ICL을 추상화한 연구도 있다[498]. 예를 들어, [498]의 저자들은 LLM들이 사전 트레이닝 동안 그들의 파라미터들을 통해 암시적 모델들을 본질적으로 인코딩한다는 것을 발견한다. ICL에 제공된 예제를 통해 LLM은 경사 하강과 같은 학습 알고리즘을 구현하거나 순방향 계산 동안 이러한 모델을 업데이트하기 위해 폐쇄형 솔루션을 직접 계산할 수 있다. 이 설명 프레임워크에서 LLM은 ICL을 사용하여 간단한 선형 함수와 결정 트리와 같은 일부 복잡한 함수를 효과적으로 학습할 수 있음을 보여주었다[498].\n' +
      '\n' +
      '최근 연구 [493]에서 논의된 바와 같이 LLM은 ICL에서 과제 인식과 과제 학습의 능력을 모두 나타내지만 두 능력은 모델 척도가 다른 것으로 판단된다. 실험 [493]에 도시된 바와 같이, 태스크 인식의 능력은 획득하기 더 쉽고, 350M 파라미터만을 갖는 작은 LM이라도 이러한 능력을 나타낼 수 있는 반면, 태스크 학습은 적어도 66B 파라미터를 갖는 LLM에 대해서만 나타날 수 있다. 또 다른 연구[499]는 또한 특별히 설계된 실험을 통해 이 발견을 뒷받침한다. 그들은 실험에서 플립된 레이블과 의미적으로 관련이 없는 레이블로 작업을 설정했으며, 이는 ICL을 수행할 때 작업 학습이 필요하다. 결과는 작은 LMs가 레이블을 무시하는 경향이 있으며 주로 작업을 수행하기 위해 사전 지식에 의존하는 반면 LLM은 사전 지식을 능가하고 시연에서 새로운 지식을 습득하여 더 나은 결과를 얻을 수 있음을 시사한다. 나아가, 태스크 학습 능력을 향상시키기 위해, 메타-인-컨텍스트 학습[500]은 프롬프트에 단지 하나의 태스크가 아닌 다수의 관련 태스크를 포함시키는 것을 제안한다. 또한, Symbol Tuning [501]은 의미적으로 관련 없는 레이블이 있는 데모에 LLM을 미세 조정하여(예를 들어, 감정 분석을 위해 긍정/부정 대신 foo/bar), LLM은 사전 지식에 의존하지 않고 데모로부터 작업을 학습하도록 강제한다.\n' +
      '\n' +
      '### _Chain-of-Thought Prompting_\n' +
      '\n' +
      '[502, 33]을 프롬프트하는 CoT(Chain-of-Thinking)는 산술 추론[503], 상식 추론[504], 기호 추론[33]과 같은 복잡한 추론 태스크에 대한 LLM의 성능을 향상시키기 위한 개선된 프롬프트 전략이다. ICL과 같은 입력-출력 쌍으로 프롬프트를 간단하게 구성하는 대신 CoT 프롬프트는 입력과 출력 사이의 브리지 역할을 하는 중간 추론 단계를 추가로 통합한다. 도 14는 CoT의 예시도를 나타낸다. 다음 부분에서는 먼저 기본 CoT 프롬프트 접근 방식과 개선 전략에 대해 자세히 설명한 다음 CoT 프롬프트가 언제, 왜 작동하는지 논의할 것이다.\n' +
      '\n' +
      '#### 6.3.1 Basic CoT prompting Approach\n' +
      '\n' +
      'CoT 프롬프트는 ICL(33)의 확장으로 처음 제안되었으며, 이는 각 데모 \\(\\langle\\)_input, output_\\(\\rangle\\)을 \\(\\langle\\)_input, CoT, output_\\(\\rangle\\)으로 확장한다. CoT_는 _입력_과 _출력_을 연결하기 위한 일련의 중간 추론 단계이다. 이러한 증강된 데모를 통해 LLM은 이를 따라 CoT 및 새로운 입력에 대한 답변을 생성할 수 있다. 그러나 ICL의 \\(\\langle\\)_input, output_\\(\\rangle\\) 쌍과 달리 CoT는 구하기가 어렵고 일반적으로 사람의 주석이 필요하다. 다행히도 LLM은 _"단계별로 생각하자."_[505]와 같은 간단한 지시를 통해 CoT를 생성하도록 트리거될 수 있어 CoT를 쉽게 사용할 수 있는 것으로 나타났다. 또한 CoT 추론 능력을 이끌어내고 LLM의 성능을 더욱 향상시킬 수 있는 대안적인 매직 프롬프트가 있는데, 예를 들어 _"심호흡을 하고 이 문제에 대해 단계적으로 작업하세요."_[473].\n' +
      '\n' +
      '도 15에 예시된 바와 같이, CoT의 생성 프로세스는 기본 CoT 프롬프트 접근법에서 체인 구조를 따르며, 여기서 LLM은 단계적으로 CoT를 생성한다. 통상적으로 CoT는 자연어 텍스트의 형식을 취한다. 그러나 텍스트 CoT는 추론을 위해 엄격한 논리가 필요한 복잡한 작업에서 잘 작동하지 않을 수 있다. 이를 고려할 때, 일부 작업은 구조적이고 정밀한 특성으로 인해 코드[506, 507]를 사용한다. 나아가, [508]의 저자들은 그들의 장점들을 조합하기 위해 CoT들의 포맷으로서 텍스트 또는 코드를 동적으로 선택할 것을 제안한다.\n' +
      '\n' +
      '#### 6.3.2 Improved CoT 프롬프트 전략\n' +
      '\n' +
      '복잡한 추론 태스크의 성능 향상에도 불구하고 CoT 프롬프트는 여전히 잘못된 추론 및 불안정과 같은 문제를 겪는다. 이 부분에서는 먼저 더 나은 CoT 프롬프트와 향상된 CoT 생성 전략을 설계하는 방법을 소개하고 CoT의 기본 체인 구조의 확장을 소개한다. 그림 15는 대표적인 CoT 프롬프트 전략의 진화를 보여준다.\n' +
      '\n' +
      '**더 나은 프롬프트 디자인.** CoT 프롬프트는 LLM의 추론 기능을 이끌어내기 위해 프롬프트에 의존하기 때문에 프롬프트의 디자인은 성능에 매우 중요합니다. 직접적인 접근법으로서, 다양한 CoT들(_i.,_각 문제에 대한 다중 추론 경로들)을 사용하는 것이 성능을 효과적으로 향상시킬 수 있음을 보여준다[437]. 또 다른 직관적인 아이디어는 더 복잡한 추론 경로를 갖는 프롬프트가 LLMs[433]의 추론 능력을 이끌어낼 가능성이 더 높으며, 이는 정답을 생성하는 데 더 높은 정확도를 초래할 수 있다는 것이다. 그러나 이러한 모든 접근법은 주석이 달린 CoT 데이터 세트에 의존하므로 실제 사용을 제한한다. 이러한 한계를 극복하기 위해, _"단계별로 생각하자"_와 같은 매직 명령어를 사용하여 LLM들을 프롬프트함으로써 CoT들을 자동으로 구성할 수 있다[434].\n' +
      '\n' +
      '**향상된 CoT 생성.** LLM은 잘못된 추론 단계를 생성하고 생성 과정에서 불안정성을 나타내기 쉬우므로 CoT 생성을 개선하기 위한 많은 연구가 있습니다. 이 부분에서 우리는 CoT 생성을 강화하기 위한 두 가지 전형적인 접근법인 샘플링 및 검증 기반 방법을 소개할 것이다.\n' +
      '\n' +
      '\\(\\bullet\\)_Sampling-based methods..__bullet\\)_Sampling. LLM은 추론 과정에서 불안정성을 겪는 것으로 알려져 있으며, 이는 생성된 추론 단계에서 불성실성을 초래할 수 있다. 이 문제를 해결하기 위해, 일부 연구는 그리디 디코딩을 사용하는 대신 여러 추론 경로를 샘플링하는 것을 제안한다. 대표적인 해결책으로 자기 일관성[436]은 먼저 여러 추론 경로를 생성한 다음 해당 답변에 대해 앙상블을 취하여 다수 투표를 통해 가장 일관성 있는 답변을 선택한다. 그러나, 이러한 방법은 대부분의 추론 경로들이 오도될 때 여전히 오답으로 이어질 수 있다. 이를 고려할 때 [433]의 저자는 복잡도가 높은 추론 경로(예: 더 많은 추론 단계)가 일반적으로 더 나은 성능을 갖는다는 관찰에 기초하여 \\(k\\)개의 가장 복잡한 추론 경로에만 투표한다. 나아가, MCR[510]은 다음 단계를 생성할 때 다른 추론 경로로부터의 단계를 참조하는 것을 제안하고, 최종 답변을 생성하기 위해 다수의 추론 경로에 걸쳐 추론을 수행한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Verification-based methods._._ CoT에서 추론 단계의 순차적 특성은 특정 단계가 부정확할 때 생성된 CoT에 오류가 누적될 수 있다. 이 문제를 완화하기 위해 최근 연구에서는 학습된 검증자 또는 LLM 자체를 사용하여 생성된 추론 단계의 정확성을 검증할 것을 제안한다. 예를 들어, DIVERSE [509]는 상이한 입도에서 추론 단계들을 조사하기 위해 솔루션-레벨 및 단계-레벨 검증기들을 각각 트레이닝한다. 다른 접근법[511]은 특수하게 설계된 추론 포맷으로 단계별 자기 검증을 통해 추론 단계들의 정확성을 검증하기 위해 LLM들을 이용한다. 또한, 여러 연구에서 검증에 필요한 질문 조건[512, 513] 또는 변수[514]를 모델의 예측에서 추론한 후 원본과 비교하는 백워드 추론을 제안한다.\n' +
      '\n' +
      '**추론 구조 확장.** 일반성에도 불구하고 기본 CoT 프롬프트의 체인 추론 구조는 추론 중 예측 및 역추적과 같은 탐색이 필요한 복잡한 작업을 해결하는 데 효과적이지 않습니다. 따라서 보다 복잡한 사고 과정(예: _예:_), 트리 및 그래프 구조 추론(tree-structured reasoning)을 설계하여 추론 구조를 확장하는 많은 연구가 수행되었다.\n' +
      '\n' +
      '\\(\\bullet\\)_Tree-structured reasoning._ 이러한 접근법(ToT(Tree of Thoughts) [451, 515]에 의해 예시됨)은 중간 생각이 노드인 계층적 트리 구조로 추론 프로세스를 공식화한다. 이러한 방식으로, LLM들이 병렬로 다수의 추론 경로들을 탐색할 수 있게 하고, 보다 포괄적인 결정들을 용이하게 하기 위해 룩어헤드 및 백트래킹의 동작을 추가로 지원한다. 또한, TouT[516]은 몬테카를로 Dropout에 기반한 사고 평가를 위해 중간 사고의 불확실성을 고려한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Graph-structured reasoning._ 트리 구조는 병렬 추론을 용이하게 하지만 추론 과정에도 제약을 가한다. 더 복잡한 위상 구조를 가진 그래프는 추론에 더 큰 유연성을 제공하여 더 복잡한 관계와 상호 작용의 특성화를 가능하게 한다. 예를 들어, GoT(Graph of Thoughts)[517, 518]는 추론 과정을 임의의 그래프로 개념화하는데, 여기서 정점은 중간 생각을 나타내고 에지는 이들 생각 사이의 상호 의존성을 나타낸다. ToT와 비교하여 새로운 생각을 생성할 때 다른 추론 경로의 생각을 더 활용할 수 있다. 그러나 이러한 접근 방식은 LLM과의 많은 상호 작용을 필요로 하여 사고 탐구 과정이 매우 비효율적이다. 잠재적으로 무의미한 사고 탐구를 줄이기 위해 XoT[519]는 사전 훈련된 정책 및 가치 네트워크를 사용하여 사고 검색을 안내할 것을 추가로 제안한다.\n' +
      '\n' +
      '#### 6.3.3 CoT Promping에 대한 추가 논의\n' +
      '\n' +
      '이 부분에서 우리는 CoT 프롬프트와 관련된 두 가지 기본 질문인 _i.e._, _"LLM에 대한 CoT 프롬프트 작업이 언제 수행되는지"_ 및 _"왜 LLM이 CoT 추론을 수행할 수 있는지"_에 대한 논의를 제시한다.\n' +
      '\n' +
      '**CoT Promping Works For LLMs의 경우?** CoT 추론은 비상 능력 [31]이기 때문에 충분히 큰 모델 (일반적으로 10B 이상의 매개 변수 [33])에만 긍정적인 영향을 미치지만 작은 모델에는 영향을 미치지 않습니다. 또한,\n' +
      '\n' +
      '도. 도 15: CoT 프롬프트 전략의 진화의 예시. 기본적인 CoT 접근법에서 시작하여 샘플링 기반 및 검증 기반 방법을 포함하여 향상된 CoT 생성 기술로 진행한다. 마지막으로, 트리와 그래프와 같은 체인 구조의 변형으로 확장된다. 여기서, “사상”은 [33, 451]에서 언급한 바와 같은 중간 추론 단계를 의미한다.\n' +
      '\n' +
      'CoT 프롬프트는 중간 추론 단계로 표준 프롬프트를 증가시키기 때문에 단계 추론[33], _예: 산술 추론, 상식 추론 및 상징 추론이 필요한 작업에 주로 효과적이다. 반면에 복잡한 추론에 의존하지 않는 다른 작업의 경우 CoT 프롬프트는 표준 프롬프트 [438], _예:_ GLUE의 MNLI-m/mm, SST-2 및 QQP보다 성능이 저하될 수 있다[260]. 흥미롭게도 CoT 프롬프트가 가져오는 성능 이득은 표준 프롬프트가 좋지 않은 결과를 얻을 때만 중요할 수 있는 것 같다[33].\n' +
      '\n' +
      '**LLM이 CoT 추론을 수행할 수 있는 이유** 두 번째 질문으로 다음 두 가지 측면에서 CoT 프롬프트의 기본 메커니즘에 대해 설명합니다.\n' +
      '\n' +
      '\\(\\bullet\\)_CoT 추론 능력의 원천입니다._ CoT 추론 능력의 원천과 관련하여, 코드에 대해 훈련된 모델은 강력한 추론 능력을 보여주기 때문에 코드에 대한 훈련에 기인할 수 있다는 것이 널리 가설되어 있다[520, 47, 521]. 직관적으로 코드 데이터는 알고리즘 로직과 프로그래밍 흐름으로 잘 구성되어 있어 LLM의 추론 성능을 향상시키는 데 유용할 수 있다. 그러나 이 가설은 여전히 삭마 실험(코드에 대한_with_ 및_with_ 훈련 없음)에 대한 공개적으로 보고된 증거가 부족하다. 또한, 비-CoT 데이터에 대한 명령어 튜닝이 홀드-아웃된 CoT 추론 벤치마크들에 대한 성능을 향상시키지 않는다는 것이 경험적으로 보여졌기 때문에, 명령어 튜닝은 CoT 추론 능력을 획득하는 주요 이유는 아닌 것으로 보인다[69].\n' +
      '\n' +
      '\\(\\bullet\\)_CoT 프롬프트 구성 요소의 효과입니다. _ CoT 프롬프트와 표준 프롬프트의 주요 차이점은 최종 답변 전에 추론 경로를 통합하는 것이다. 따라서 일부 연구자들은 추론 경로에서 서로 다른 구성요소의 영향을 조사한다. 구체적으로, 최근의 연구는 CoT 프롬프트의 세 가지 주요 구성요소, 즉 _기호_(예를 들어, 산술 추론에서의 수치 수량), _패턴_(예를 들어, 산술 추론에서의 방정식), 및 _텍스트_(즉, 기호 또는 패턴이 아닌 나머지 토큰)을 식별한다[522]. 후자의 두 부분(_i, 패턴 및 텍스트)은 모델 성능에 필수적이며 둘 중 하나를 제거하면 상당한 성능 저하로 이어질 수 있음을 보여준다. 그러나 기호와 문양의 정확성은 중요하지 않아 보인다. 또한, 텍스트와 패턴 사이에는 공생 관계가 존재한다 : 텍스트는 LLM들이 유용한 패턴들을 생성하는 것을 돕고, 패턴은 LLM들이 태스크들을 이해하고 그것들을 해결하는 것을 돕는 텍스트들을 생성하는 것을 돕는다[522].\n' +
      '\n' +
      '요약하면, CoT 프롬프트는 LLM의 추론 능력을 이끌어내기 위한 일반적이고 유연한 접근법을 제공한다. 또한, 멀티모달[523] 및 멀티언어 태스크[524]를 해결하기 위해 이 기술을 확장하려는 몇몇 예비 시도들이 있다.\n' +
      '\n' +
      '### _Planning for Complex Task Solving_\n' +
      '\n' +
      'ICL과 CoT로 프롬프팅하는 것은 다양한 과제를 해결하기 위한 개념적으로 간단하면서도 일반적인 접근법이다. 그러나, 이 접근법은 수학적 추론[525] 및 멀티-홉 질문 응답[526]과 같은 복잡한 작업들과 고군분투한다. 향상된 접근 방식으로서, 복잡한 작업을 더 작은 하위 작업으로 분할하고 작업을 수행하기 위한 작업 계획을 생성하기 위해 프롬프트 기반 계획이 제안되었다.\n' +
      '\n' +
      '#### 6.4.1 The Overall Framework\n' +
      '\n' +
      '이 부분에서는 먼저 그림 16과 같은 복잡한 과제 해결을 위한 LLM의 일반적인 계획 패러다임을 공식화한다.\n' +
      '\n' +
      '이 패러다임에는 일반적으로 _작업 플래너, 계획 실행기_ 및 _환경_41의 세 가지 구성 요소가 있습니다. 구체적으로 LLM에서 수행 하는 작업 플래너는 전체 계획을 생성 하 여 대상 작업을 해결 하는 것을 목표로 합니다. 계획은 다양한 형태로 제시될 수 있는데, _예를 들어_ 자연어 형태의 액션 시퀀스[439] 또는 프로그래밍 언어로 작성된 실행 프로그램[443]이다. LLM 기반 태스크 플래너는 계획 저장 및 검색을 위한 메모리 메커니즘으로 향상될 수 있으며, 이는 긴 수평 작업에 도움이 된다. 그런 다음 계획 실행자는 계획에서 작업을 실행할 책임이 있습니다. 텍스트 태스크들에 대한 LLM들과 같은 모델들[441] 또는 코딩 태스크들에 대한 코드 인터프리터들과 같은 툴들에 의해 구현될 수 있다[450]. 나아가, 환경은 계획 실행자가 특정 태스크들, _예를 들어_ LLM 자체[527] 또는 마인크래프트[528]와 같은 외부 가상 세계에 따라 다르게 설정될 수 있는 액션들을 수행하는 곳을 지칭한다. 동작의 실행 결과에 대한 _피드백_ 을 자연어 형태[450] 또는 다른 멀티모달 신호들[446]로부터 태스크 플래너에 제공한다.\n' +
      '\n' +
      '각주 41: RL과의 유사성에도 불구하고, 우리의 공식은 계획 및 실행 단계를 분리하는 반면 RL에서는 일반적으로 에이전트에서 인터리빙된다. 이 패러다임은 일반적이지만 약간 느슨한 방식으로 정의되며 주로 독자들이 LLM의 계획 접근법의 기초가 되는 핵심 아이디어를 이해하는 데 도움이 되는 것을 목표로 한다.\n' +
      '\n' +
      '복잡한 태스크를 해결하기 위해, 태스크 플래너는 먼저 태스크 목표를 명확하게 이해하고 LLM의 추론에 기초하여 합리적인 계획을 생성할 필요가 있다(섹션 6.4.2 참조). 그런 다음, 계획 실행자는 환경에서 계획에 따라 행동하고, 환경은 태스크 플래너에 대한 피드백을 생성할 것이다(섹션 6.4.3 참조). 태스크 플래너는 환경으로부터 얻은 피드백을 더 통합하여 초기 계획을 구체화할 수 있고, 태스크 솔루션으로서 더 나은 결과를 얻기 위해 위의 프로세스를 반복적으로 수행할 수 있다(섹션 6.4.4 참조).\n' +
      '\n' +
      '도. 도 16: 복잡한 태스크를 해결하기 위한 LLM에 의한 신속한 기반 계획을 위한 포뮬레이션의 예시.\n' +
      '\n' +
      '#### 6.4.2 Plan Generation\n' +
      '\n' +
      '계획 생성은 LLM을 프롬프트하여 액션 시퀀스를 직접 생성하는 데 중점을 둔다. 생성된 계획의 형식을 기반으로 기존 작업은 텍스트 기반 접근법과 코드 기반 접근법의 두 그룹으로 나눌 수 있다.\n' +
      '\n' +
      '**텍스트 기반 접근 방식.** LLM은 자연어 형식으로 계획을 생성하는 것이 간단합니다. 이 접근법에서, LLM들은 복잡한 태스크를 수행하고 해결하기 위해 계획 실행기가 일련의 액션들을 생성하도록 프롬프트된다. 예를 들어, Plan-and-Solve [441]은 제로 샷 방식으로 LLM에 계획을 직접 프롬프트하기 위해 "계획을 고안"과 같은 명시적 지시를 추가하고, Self-planning [529] 및 DECOMP [440]은 프롬프트에 데모를 추가하여 LLM이 ICL을 통해 계획을 고안하도록 안내한다. 이러한 방식에 따라 일부 작업은 계획할 때 추가 도구 또는 모델을 통합하는 것을 추가로 고려한다. 예를 들어 ToolFormer [80]은 먼저 LLM을 사용 하 여 잠재적인 API 호출이 있는 사전 훈련 코퍼스에 주석을 달고 그 위에 LLM을 미세 조정 하 여 LLM이 API를 호출 하는 시기와 방법을 학습 하 고 생성 중에 API에 의해 반환 된 결과를 통합할 수 있습니다. HuggingGPT[444]는 HuggingFace에서 이용가능한 모델들을 소개하고 LLM들을 제어기로서 간주하여 그들의 설명에 기초하여 적합한 모델들을 선택하고 그들의 결과들을 최종 해결책으로서 집계한다.\n' +
      '\n' +
      '**코드 기반 접근법.** 텍스트 기반 접근법이 직관적으로 들리지만 계획의 충실한 실행을 보장할 수 없으므로 계획이 정상인 경우에도 실패할 수 있습니다. 이 문제를 해결하기 위해 프로그래밍 언어(예: Python 또는 PDDL)에서 실행 가능한 코드 형태로 보다 검증 가능한 계획을 생성하는 코드 기반 접근법이 제안되었다. 이러한 방식으로, LLM들은 먼저 프로그램을 생성하도록 프롬프트되고, 그 후 그것을 실행하기 위해 결정론적 해결기를 활용한다. 예를 들어, FaithVolCt[442] 및 PAL[443]은 추론 태스크를 두 단계로 분해하는데, 첫 번째 단계에서는 LLM이 질의에 조건화된 플랜을 생성하고, 두 번째 단계에서는 결정론적 해결자가 플랜을 실행하여 최종 답변을 도출한다. 나아가, 코드 기반 접근법들은 유사한 방식으로 구체화된 에이전트들에 적용될 수 있다. 예를 들어, PROGPROMPT[530], LLM+P[531]는 먼저 LLMs를 활용하여 파이썬 함수나 PDDL 파일 형태의 플랜을 생성한 후, 가상 에이전트나 클래식 플래너를 활용하여 코드 기반 플랜에 따른 문제를 해결한다.\n' +
      '\n' +
      '#### 6.4.3 피드백 획득\n' +
      '\n' +
      '생성된 계획을 실행한 후 환경은 LLM 기반 작업 계획자에게 피드백 신호를 생성하며, 이는 더 나은 결과를 위해 초기 계획을 구체화하는 데 사용할 수 있다. 기존 작업에서, 일반적으로 LLM 기반 태스크 플래너와의 관계에 따라 환경으로부터의 피드백의 두 가지 소스, 즉 내부(_i.e.,_LLM 자체) 및 외부(_e.e.,_도구 또는 가상 세계) 피드백이 있다.\n' +
      '\n' +
      '**내부 피드백.** LLM 자체를 피드백 공급자로 사용할 수 있습니다. 한 가지 간단한 방법은 프롬프트를 통해 생성된 계획의 품질을 직접 평가하는 것이다. 예를 들어, RAP[447]는 각각의 후보 플랜이 태스크 성공으로 이어질 수 있는 가능성을 평가하는 반면, Tree of Thoughts[527]는 이들 간의 비교를 통해 플랜 전체에 투표할 것을 제안한다. 또한, LLM은 플랜 실행기로부터의 중간 결과에 기초하여 피드백을 제공할 수 있다. 예를 들어, Reflexion[450]은 희소 결과 신호들(_e.g.,_성공 또는 실패)을 구체적인 텍스트 기반 피드백(_e.g.,_"_사용자가 공포 영화 대신 쿼리에서 언급하는 코미디를 추천해야 한다_)으로 변환하기 위해 LLM들을 활용하고, 이 피드백을 향후 계획을 위해 장기 메모리에 저장한다.\n' +
      '\n' +
      '**외부 피드백.** LLM 외에도 외부 개체는 피드백 신호를 제공할 수도 있습니다. 예를 들어, 코드 해석기와 같은 도구는 실시간 오류 메시지를 제공하기 위해 프로그래밍 작업에서 널리 사용되고[450], 안정적인 확산과 같은 모델은 시각적 인식을 제공하기 위해 멀티모달 작업에서 사용될 수 있고[532], 마인크래프트와 같은 가상 세계는 몰입 경험을 제공할 수 있다[528]. 또한, 일부 작업(_e.g.,_ Generative Agents[533])은 시뮬레이션된 환경에서 다중 에이전트 협업을 탐색하며, 여기서 각 에이전트는 환경과의 상호작용뿐만 아니라 다른 에이전트와의 통신으로부터 피드백을 수신한다.\n' +
      '\n' +
      '#### 6.4.4 Plan Refinement\n' +
      '\n' +
      '환경으로부터의 피드백에 대한 액세스를 통해, 태스크 플래너는 그에 따라 현재의 계획을 정제하고 더 나은 결과를 위해 반복적으로 "_플래닝 - 실행 - 정제_" 루프를 거칠 수 있다. 이 부분에서는 기존 작업에서 세 가지 주요 개선 접근법을 요약한다.\n' +
      '\n' +
      '**추론.** 환경의 피드백 데이터는 관련 없는 정보를 포함 하거나 언어가 아닌 형식을 취하는 것과 같은 계획 정제를 위해 LLM에서 사용 하는 데 직접 적합 하지 않을 수 있습니다. 이를 해결하기 위해, 일부 작업은 피드백으로부터 중요한 정보를 추출하기 위해 명시적 추론 프로세스를 추가한다[448, 449]. 예를 들어, 반응 [449]는 피드백을 통해 추론 트레이스를 생성하기 위해 LLM에 데모를 프롬프트한다. 다양한 사용자 요청을 해결하기 위한 초기 계획을 수정하기 위해 관찰된 피드백을 자동으로 추론할 수 있는 AutoGPT[534]와 같은 자율 에이전트 프로젝트에서 널리 사용되었다. 그러나, 이러한 접근법들은 일반적으로 추론 및 계획의 순서를 고정시킨다. 더 나은 성능을 위해 두 프로세스 사이의 유연한 전환을 지원하기 위해, ChatCoT[448]는 툴-증강 추론 프로세스를 LLM-기반 태스크 플래너와 툴-기반 환경 사이의 멀티-턴 대화로 더 통합한다.\n' +
      '\n' +
      '**백트랙킹.** 초기 메서드는 주로 기존 계획을 유지하면서 전진 조치를 계획하는 것을 고려하므로 단기 평가를 기반으로 하는 지역 최적 계획으로 이어질 수 있습니다. 이를 해결하기 위해, Three of Thoughts[527]는 전역 계획을 만들기 위해 너비 우선 탐색 및 깊이 우선 탐색과 같은 탐색 알고리즘으로 역추적할 수 있다. 초기 계획에서 마지막 상태로 역추적하여 다음 미개척 행동을 선택함으로써 계획을 단계적으로 가다듬는다. 또한 일부 연구[535, 446]는 피드백 신호를 활용하여 전체 계획을 수정한다. 예를 들어, DEPS[535]는 피드백 신호들에 따라 더 나은 플랜을 선택하는 반면, TIP[446]는 LLM 기반 플래너가 초기 플랜의 각 단계를 수정하라는 프롬프트에 피드백 신호들을 추가한다.\n' +
      '\n' +
      '**메모리.** 긴 수평 작업을 처리 하기 위해 ICL을 통해 LM의 _단기 기억_ 을 활용하는 것 외에도 _장기 기억_ 을 사용 하 여 계획 개선을 지원 하는 핵심 접근 방식이 되었습니다. 예를 들어, 반사[450]는 자기-반사로부터의 피드백을 메모리에 저장하므로, 이전의 피드백은 계획 정제를 위해 검색될 수 있다. 생성 에이전트[533]는 동작 계획 및 반사를 위한 에이전트의 핵심 구성요소로서 메모리 스트림 메커니즘을 설계한다. 또한, 스킬 라이브러리 메커니즘[445, 528]은 성공적인 계획을 라이브러리에 저장하기 위해 제안되었으며, 이는 새로운 작업에 대한 복잡한 계획으로 재사용 및 합성될 수 있다. 장기 메모리 메커니즘을 구현하기 위해, 벡터 데이터베이스(_e.g._, milvus[536])와 같은 툴은 대규모로 효율적인 저장 및 검색을 위해 계획 또는 피드백을 고차원 벡터로 인코딩하는 데 사용될 수 있다. MemoryBank[537]는 Ebbinghaus Forgetting Curve 이론에 따라 기억의 망각과 강화를 허용하는 기억 갱신 메커니즘을 추가로 제안한다.\n' +
      '\n' +
      '## 7 용량 및 평가\n' +
      '\n' +
      'LLM의 효율성과 우수성을 검토하기 위해 경험적 능력 평가 및 분석을 수행하기 위한 과제와 벤치마크의 급증이 제안되었다. 본 절에서는 먼저 언어 생성 및 이해를 위한 LLM의 기본 능력 평가의 세 가지 유형을 소개하고, 보다 복잡한 설정이나 목표를 가진 몇 가지 고급 능력 평가를 제시하고, 마지막으로 기존의 벤치마크, 평가 접근법 및 실증 분석에 대해 논의한다.\n' +
      '\n' +
      '### _Basic Ability_\n' +
      '\n' +
      '이 부분에서는 주로 LLM에 대한 3가지 기본 유형의 능력 평가, 즉 _i.e._, 언어 생성, 지식 활용 및 복합 추론에 중점을 둔다. 우리는 모든 관련 작업에 대한 완전한 적용 범위를 가질 의도가 아니라 LLM에 대해 가장 널리 논의되거나 연구된 작업에만 초점을 맞춘다는 점에 주목한다. 다음으로, 이러한 과제들을 상세히 소개한다.\n' +
      '\n' +
      '#### 7.1.1 언어 생성\n' +
      '\n' +
      '과제 정의에 따라 언어 생성에 관한 기존의 과제는 크게 언어 모델링, 조건부 텍스트 생성, 코드 합성 작업으로 분류할 수 있다. 코드 합성은 일반적인 NLP 작업이 아니므로 자연어 텍스트와 유사한 생성 접근법에서 다수의 LLM(코드 데이터에 대해 훈련됨)에 의해 직접 해결될 수 있기 때문에 논의를 위해 포함한다.\n' +
      '\n' +
      '**언어 모델링.** LLM의 가장 기본적인 능력으로 _언어 모델링_은 기본 언어 이해 및 생성 능력에 주로 중점을 둔 이전 토큰 [1]을 기반으로 다음 토큰을 예측하는 것을 목표로 합니다. 이러한 기능을 평가하기 위해 기존 작업에서 사용하는 일반적인 언어 모델링 데이터 세트에는 Penn Treebank [538], WikiText-103 [539], Pile [161]이 있으며, 여기서 _perplexity_의 메트릭은 제로 샷 설정에서 모델 성능을 평가하는 데 일반적으로 사용됩니다. 경험적 연구 [93, 55]는 LLM이 이러한 평가 데이터 세트에 대한 이전 최신 방법에 비해 상당한 성능 향상을 가져온다는 것을 보여준다. LAMBADA 데이터 집합 [233]은 텍스트에서 장거리 종속성의 모델링 능력을 더 잘 테스트하기 위해 도입되었으며, LLM은 문맥 단락을 기반으로 문장의 마지막 단어를 예측하는 데 필요하다. 그런 다음 예측된 마지막 단어의 정확도와 복잡도를 사용하여 LLM을 평가한다. 기존 작업에서 볼 수 있듯이 언어 모델링 작업에 대한 성능은 일반적으로 스케일링 법칙[30]을 따르며, 이는 스케일링 언어 모델이 정확도를 개선하고 복잡성을 줄일 수 있음을 의미한다.\n' +
      '\n' +
      '**조건부 텍스트 생성.** 언어 생성에서 중요한 주제로서 조건부 텍스트 생성 [48]은 일반적으로 기계 번역 [624], 텍스트 요약 [548] 및 질문 응답 [557]을 포함 하 여 지정 된 조건에 따라 특정 작업 요구 사항을 충족 하는 텍스트를 생성 하는 데 중점을 둡니다. 생성된 텍스트의 품질을 측정하기 위해, 자동 메트릭(_e.g._, Accuracy, BLEU[625] 및 ROUGE[626]) 및 인간 등급이 일반적으로 성능을 평가하기 위해 사용되었다. 강력한 언어 생성 능력으로 인해 LLM은 기존 데이터 세트와 벤치마크에서 놀라운 성능을 달성했다. 예를 들어, GPT-4는 상당한 언어 거리를 갖는 언어의 번역 작업에 대해서도 상업적 번역 제품과 유사한 성능을 나타낸다[627]. 뉴스 요약 태스크들(_i.e._, CNN/DM 및 XSUM)에서, LLM들은 또한 인간 프리랜서 작가들과 비교가능한 성능을 보여준다[628]. 모델 용량의 급속한 발전에도 불구하고, 조건부 텍스트 생성 작업[628, 629, 630]에서 LLM의 성능을 충실하게 평가하기 위한 기존 자동 메트릭의 실현 가능성에 대한 우려가 증가하고 있다. 자동 메트릭에 대한 대안으로 최근 연구에서는 생성된 콘텐츠의 품질을 조사하기 위해 생성 평가자로 LLM을 통합할 것을 제안한다[631, 632, 138]. 더욱이, 연구자들은 또한 구조화된 데이터 생성[458] 및 긴 텍스트 생성[633, 46, 46]과 같은 LLM에 대한 보다 도전적인 언어 생성 작업을 탐색한다.\n' +
      '\n' +
      '**코드 합성.** 고품질 자연어 텍스트를 생성하는 것 외에도 기존 LLM은 형식 언어, 특히 _코드 합성_[635]라고 하는 특정 조건을 충족하는 컴퓨터 프로그램(_i.e._, 코드)을 생성하는 강력한 능력을 보여줍니다. 자연어 생성과는 달리 생성된 코드를 해당 컴파일러 또는 인터프리터로 실행하여 직접 확인할 수 있기 때문에 기존 작업은 대부분 테스트 케이스 _i.e._, pass@_k42_에 대한 통과율을 계산하여 LLM에서 생성된 코드의 품질을 평가한다. 최근에는 APPS[378], HumanEval[105], MBPP[208]와 같은 LLM의 코드 합성 능력을 평가하기 위해 기능적 정확성에 초점을 맞춘 여러 코드 벤치마크가 제안되고 있다. 전형적으로, 이들은 다양한 프로그래밍 문제들로 구성되어 있으며, 텍스트 명세와 정확성 검사를 위한 테스트 케이스들이 있다. 그러한 능력을 향상시키기 위해, 코드 데이터에 LLM들을 미세 조정(또는 사전 트레이닝)하는 것이 핵심이며, 이는 LLM들을 코드 합성 태스크들에 효과적으로 적응시킬 수 있다[86]. 또한, 기존 연구는 프로그래머에 의한 버그 고정 및 코드 계획 프로세스의 모방으로 간주될 수 있는 코드 생성, _예_, 다중 후보 솔루션 샘플링[208] 및 계획 유도 디코딩[636]을 위한 새로운 전략을 제안했다. 인상적으로, LLM은 최근 프로그래밍 콘테스트 플랫폼 코디포레스[114]에서 사용자들 중 상위 28%의 랭킹을 달성함으로써 인간과 경쟁적인 성능을 보여주었다. 또한, GitHub Copilot은 코딩 IDE(_e.g._, Visual\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:60]\n' +
      '\n' +
      '의미적으로 동일한 참조는 다양한 표현을 사용한다. 또한, LLM은 단일 예측[631, 642, 632]을 평가하거나 여러 후보들[643, 644, 138, 645]을 비교하는 것을 포함하여 참조 없는 방식으로 텍스트 생성의 평가자들로서 널리 채용된다. 그럼에도 불구하고, LLM은 언어 생성 평가자로서 편향(_예를 들어, 인간-기록된 텍스트보다 LLM-생성 텍스트에 대한_순서 편향 또는 선호도)을 노출할 수 있으며, 이는 인간 평가와 비교할 때 차이를 입증한다[646, 632, 647].\n' +
      '\n' +
      '\\(\\bullet\\)_Underperforming specialized generation._ LLM은 일관된 텍스트를 생성하기 위해 일반적인 언어 패턴을 학습했지만, 전문 도메인이나 작업을 처리할 때 생성 능력이 제한될 수 있다. 예를 들어, 일반 웹 기사에서 학습된 언어 모델은 많은 의학 전문 용어와 방법을 포함하는 의료 보고서를 생성할 때 어려움에 직면할 수 있다. 직관적으로 도메인 지식은 모델 전문화를 위해 중요해야 한다. 그러나 이러한 전문 지식을 LLM에 주입하는 것은 쉽지 않다. 최근 분석에서 논의된 바와 같이 [47, 648] LLM이 일부 영역에서 탁월할 수 있는 특정 능력을 나타내도록 훈련되면 다른 영역에서 어려움을 겪을 수 있다. 이러한 문제는 신경망 훈련에서 _재앙적 망각_[649, 650]과 관련이 있으며, 이는 새로운 지식과 오래된 지식을 통합하는 갈등 현상을 나타낸다. 유사한 경우가 LLM의 인간 정렬에서도 발생하며, 여기서 _"정렬 세금"_[66](예를 들어, 문맥 내 학습 능력의 잠재적 손실)은 인간의 가치 및 필요에 정렬하기 위해 지불되어야 한다. 또한, 시퀀스 모델링 아키텍처의 한계로 인해 LLM은 여전히 구조화된 데이터의 이해와 생성에 어려움을 겪고 있다. 결과적으로, 이들은 종종 지식-기반 질의 응답 및 시맨틱 파싱과 같은 복잡한 구조화된 데이터 태스크들에서 태스크-특정 모델들에 뒤처진다[651, 458]. 따라서 기존 능력을 유지하면서 다양한 작업 시나리오에 LLM을 유연하게 적용할 수 있는 효과적인 모델 특성화 방법을 개발하는 것이 중요하다.\n' +
      '\n' +
      '#### 7.1.2 지식 활용\n' +
      '\n' +
      '지식 활용은 지식 집약적 작업(예를 들어, 상식 질문 응답 및 사실 완성)을 뒷받침하는 사실 증거를 기반으로 수행하는 지능형 시스템의 중요한 능력이다. 구체적으로, LLM은 사전 훈련 코퍼스에서 풍부한 사실 지식을 적절하게 활용하거나 필요할 때 외부 데이터를 검색해야 한다. 특히, 질문 응답(QA)과 지식 완성은 이 능력을 평가하기 위해 일반적으로 사용되는 두 가지 과제였다. 테스트 태스크(질문 응답 또는 지식 완성) 및 평가 설정(_with_ 또는 _with_ 외부 리소스 없음)에 따라 기존 지식 활용 태스크를 폐쇄형 QA, 개방형 QA43 및 지식 완성의 세 가지 유형으로 분류한다.\n' +
      '\n' +
      '각주 43: 이 부분에서, 오픈-북 QA는 폐쇄-북 QA의 반대(사전-트레이닝 코퍼스로부터의 인코딩된 정보만을 이용함)로서, 외부 지식 자원으로부터 유용한 정보를 추출하고 활용해야 하는 QA 태스크를 지칭한다. 외부 과학 사실을 추출하고 활용하여 오픈북 QA 작업의 설정을 따르는 오픈북QA[566]라는 데이터 세트도 있다는 점에 유의한다.\n' +
      '\n' +
      '**닫힌 책 QA.** 닫힌 책 QA 작업 [652]는 사전 교육 말뭉치에서 획득한 LLM의 사실적 지식을 테스트하며, LLM은 외부 리소스를 사용하지 않고 주어진 컨텍스트에 따라만 질문에 응답해야 합니다. 이 능력을 평가하기 위해 정확도 메트릭이 널리 채택되는 자연 질문[554], 웹 질문[557], 트리비아QA[558]를 포함하여 활용할 수 있는 여러 데이터 세트가 있다. 경험적 결과들은 LLM들이 이 설정에서 잘 수행될 수 있고 심지어 최첨단의 오픈-도메인 QA 시스템들의 성능과 일치할 수 있다는 것을 밝혀냈다[56]. 또한, 폐쇄-북 QA 태스크에서 LLM의 성능은 모델 크기 및 데이터 크기 모두에서 스케일링 법칙 패턴을 보여준다: 파라미터 및 트레이닝 토큰을 스케일링하는 것은 LLM의 용량을 증가시키고 사전 트레이닝 데이터로부터 더 많은 지식을 학습(또는 암기)하도록 도울 수 있다[56]. 또한, 유사한 파라미터 스케일 하에서, 평가된 작업들과 관련된 더 많은 사전-트레이닝 데이터를 갖는 LLM들은 더 나은 성능을 달성할 것이다[81]. 또한, 폐쇄형 QA 설정은 LLM에 의해 인코딩된 사실적 지식의 정확성을 조사하기 위한 테스트베드를 제공한다. 그러나 기존 작업 [55]에서 볼 수 있듯이 LLM은 사전 훈련 데이터에 존재하는 경우에도 세밀한 지식에 의존하는 QA 작업에 대해 덜 잘 수행할 수 있다.\n' +
      '\n' +
      '**열린 문서 QA.** 닫힌 문서 QA와 달리 열린 문서 QA 작업에서 LLM은 외부 지식 기반 또는 문서 모음에서 유용한 증거를 추출한 다음 추출된 증거를 기반으로 질문에 답할 수 있습니다 [653, 654, 655]. 전형적인 오픈-북 QA 데이터세트들(_e.g.,_ Natural Questions[554], OpenBookQA[566], 및 SQuAD[569])은 폐쇄-북 QA 데이터세트들과 중첩되지만, 이들은 외부 데이터 소스들, _e.g.,_ Wikipedia를 통합한다. 정확도 및 F1 점수의 메트릭은 평가를 위한 개방형 QA 작업에서 널리 사용된다. 외부 자원으로부터 관련 지식을 선택하기 위해, LLM은 종종 텍스트 검색기(또는 심지어 검색 엔진)와 쌍을 이루며, 이는 LLM과 독립적으로 또는 공동으로 훈련된다[653, 657, 81]. 또한, 이전 작업 [658, 659, 660]은 검색기가 추론 경로를 검증하고 수정하는 데 LLM을 지원할 수 있음을 나타낸다. 평가에서 기존의 연구들은 주로 LLM이 추출된 지식을 어떻게 활용하여 질문에 응답하는지 테스트하는 데 초점을 맞추고 검색된 증거가 생성된 답변의 정확도를 크게 향상시킬 수 있음을 보여주며, 더 작은 LLM이 \\(10\\times\\) 더 큰 답변을 능가할 수 있음을 보여준다[653, 657]. 또한 지식정보의 최신성을 평가하기 위해 오픈북 QA 태스크를 사용할 수 있다. 구식 지식 리소스들로부터 사전 트레이닝 또는 검색은 LLM들로 하여금 시간에 민감한 질문들에 대한 오답들을 생성하게 할 수 있다[653].\n' +
      '\n' +
      '**지식 완성.** 지식 완료 작업에서 LLM은 지식 단위 (예:_, 지식 트리플)의 누락 된 부분을 완료 하거나 예측 하는 데 활용할 수 있는 지식 베이스로 간주 될 수 있습니다. [576]. 이러한 태스크는 사전 트레이닝 데이터로부터 학습된 _얼마나 많은_ 및 _어떤 종류의_ 지식 LLM들을 조사하고 평가할 수 있다. 기존의 지식 완성 작업은 지식 그래프의 트리플을 완성하는 것을 목표로 하는 지식 그래프 완성 작업(_e.g._, FB15k-237[572] 및 WN18RR[574])과 특정 사실에 대한 불완전한 문장을 각각 완성하는 것을 목표로 하는 사실 완성 작업(_e.g._, WikiFact[571])으로 대별될 수 있다. 경험적 연구들에 따르면, 기존의 LLM들이 특정 관계 유형들과 관련된 지식 완성 태스크들을 달성하기 어렵다는 것이 밝혀졌다[520]. WikiFact에 대한 평가 결과에서 볼 수 있듯이 LLM은 사전 학습 데이터(_e.g._, 통화 및 저자)에서 발생하는 여러 빈번한 관계(_e.g._, discoverer_or_inventor 및 place_of_birth)에서 잘 수행되지만 희귀 관계(_e.g._, discoverer_or_inventor 및 place_of_birth)에서는 그렇지 않다. 흥미롭게도, 동일한 평가 설정들(_e.g._, 인-컨텍스트 학습) 하에서, InstructGPT(_i.e._, text-davinci-002)는 위키팩트의 모든 서브세트들에서 GPT-3보다 성능이 우수하다.\n' +
      '\n' +
      '**주요 문제**. LLM은 지식 정보를 포착하고 활용하는 데 있어 핵심적인 진전을 이루었지만 아래에서 논의되는 바와 같이 두 가지 주요 문제로 고통받고 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_Hallucination._ 사실 텍스트를 생성함에 있어서, 도전적인 이슈는 _hallucination generations_[661, 638]인데, 여기서 생성된 정보는 기존 소스(_intrinsic hallucination_)와 충돌하거나 사용 가능한 소스(_extrinsic hallucination_)에 의해 검증될 수 없으며, 이는 도 17의 두 가지 예들에 의해 예시된다. Hallucination은 GPT-4와 같은 가장 우수한 LLM들까지도, 기존 LLM들에서 광범위하게 발생한다[46]. 더 나아가 기존 연구는 LLM이 텍스트에서 환각 콘텐츠를 인식하는 데 어려움을 겪는다는 것을 보여준다[602]. 또한, 언어 태스크를 넘어, 최근 연구에 따르면 대형 비전 언어 모델(LVLM)도 환영, 즉, 수반되는 이미지들에 존재하지 않는 객체들을 생성하는 문제에 직면한다[662]. 본질적으로 LLM은 여전히 내부 또는 외부 지식의 사용을 정확하게 제어하는 능력이 부족한 과제 해결에 지식을 "무의식적으로" 활용하는 것으로 판단된다. 환각은 LLM이 원하지 않는 출력을 생성하도록 오도하고 대부분 성능을 저하시켜 실제 응용 프로그램에 LLM을 배포할 때 잠재적인 위험을 초래한다. 이러한 문제를 완화하기 위해 정렬 조정 전략(섹션 5.2에서 논의된 바와 같이)은 고품질 데이터에 LLM을 조정하거나 인간 피드백을 사용하는 기존 작업[66]에서 널리 활용되었다. 더욱이, 신뢰할 수 있는 정보 소스의 제공을 위한 외부 도구의 통합은 환각 문제를 완화하는 데 도움이 될 수 있다[659, 81, 602]. 또 다른 연구 작업은 LLM의 불확실성 추정을 활용하여 환각을 식별한다[663, 664]. 예를 들어, 환각된 사실이 상이한 샘플링된 출력에 걸쳐 불일치를 나타내기 쉽다는 점을 고려하면, SelfCheckGPT[664]는 샘플링된 출력 내의 정보 불일치를 측정함으로써 환각을 검출한다. 환각 문제의 평가를 위해, 모델들에 의해 모방된 인간의 거짓을 검출하기 위한 일련의 환각 검출 태스크들, _예를 들어_, TruthfulQA[556]이 제안되었다. 보다 최근에 HaluEval [602]는 작업별 및 일반 시나리오 모두에서 환각을 인식하는 언어 모델의 능력을 평가하기 위해 대규모 LLM 생성 및 인간 주석이 달린 환각 샘플을 생성한다.\n' +
      '\n' +
      '**Enhucination.**\n' +
      '\n' +
      'LLM은 기존 소스와 충돌하거나 사용 가능한 소스에서 확인할 수 없는 신뢰할 수 없는 정보를 생성하는 경향이 있습니다. ChatGPT와 같은 가장 강력한 LLM조차도 생성된 텍스트의 환각을 이동하는 데 큰 어려움에 직면해 있다. 이 문제는 정렬 조정 및 도구 활용과 같은 특별한 접근법에 의해 부분적으로 완화될 수 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_Knowledge recency._ 또 다른 주요 과제로서 LLM은 필요한 작업을 해결할 때 어려움을 겪을 것이다.\n' +
      '\n' +
      '도. 17: public LLM에 대한 내재적 및 외재적 환각의 예(출입일: 2023년 3월 19일). 내재적 환각의 예로 LLM은 신디와 에이미의 관계에 대해 상반된 판단을 내리게 되는데, 이는 입력과 모순된다. 외재적 환각의 경우, 이 예에서, LLM은 (이러한 맥락에서) LLM의 의미를 올바르게 이해할 수 있지만, RLHF(인간 피드백으로부터의 강화 학습)의 의미에 대한 잘못된 이해를 갖는 것으로 보인다.\n' +
      '\n' +
      '훈련 데이터를 넘어서는 최신 지식 이 문제를 해결하기 위해 간단한 접근법은 정기적으로 새로운 데이터로 LLM을 업데이트하는 것이다. 그러나, LLM들을 미세 조정하는 것은 매우 비용이 많이 들고, 또한 LLM들을 점진적으로 트레이닝할 때 재앙적인 망각 문제를 야기할 가능성이 있다. 따라서 새로운 지식을 기존의 LLM에 통합하여 최신 상태로 만들 수 있는 효율적이고 효과적인 접근법의 개발이 필요하다. 기존 연구들은 LLM과 공동으로 최적화되거나 플러그 앤 플레이 모듈로 사용될 수 있는 LLM을 보완하기 위해 외부 지식 소스(_예를 들어,_검색 엔진)를 활용하는 방법을 탐구해 왔다[653]. 예를 들어, ChatGPT는 검색 플러그인을 활용하여 최신 정보 소스에 액세스한다[665]. 추출된 관련 정보를 컨텍스트에 통합함으로써[666, 667, 668] LLM은 새로운 사실적 지식을 획득하고 관련 작업에 대해 더 나은 수행을 할 수 있다. 그러나, 그러한 접근은 여전히 피상적인 수준에 있는 것으로 보인다. 또한, 기존 연구들은 내재적 지식을 갱신하기 위해 언어 모델의 편집 파라미터를 탐색하기도 한다[669, 670, 671]. 그럼에도 불구하고, 이전 연구 [672]는 작은 언어 모델의 성능을 향상시킬 수 있지만 여러 매개변수 편집 방법이 LLM에서 잘 수행되지 않는다는 것을 보여주었다. 따라서, 내재적 지식을 직접적으로 수정하거나 LLM에 특정 지식을 주입하는 것은 여전히 어려우며, 이는 오픈 리서치 문제로 남아 있다[672]. 최근, LLM에 대한 지식 편집의 연구를 용이하게 하기 위해 유용한 프레임워크 _EasyEdit_[673]이 출시되었다.\n' +
      '\n' +
      '#### 7.1.3 Complex Reasoning\n' +
      '\n' +
      '복잡한 추론은 결론을 도출하거나 결정을 내리기 위해 뒷받침하는 증거나 논리를 이해하고 활용하는 능력을 의미한다[51, 52]. 추론 과정에서 관련된 논리와 증거의 유형에 따라 기존 평가 과제를 지식 추론, 기호 추론, 수학적 추론의 세 가지 주요 범주로 나누는 것을 고려한다.\n' +
      '\n' +
      '**지식 추론.** 지식 추론 작업은 주어진 질문에 답하기 위해 사실적 지식에 대한 논리적 관계와 증거에 의존합니다. 기존 작업은 주로 특정 데이터 세트를 사용하여 해당 유형의 지식의 추론 능력을 평가하는데, _예: 상식 지식 추론은 CSQA[504]/StrategyQA[185], 과학 지식 추론은 ScienceQA[565]이다. 예측된 결과의 정확성에 더하여, 기존 작업[565]은 또한 자동 메트릭(_예를 들어, BLEU) 또는 인간 평가를 통해 생성된 추론 프로세스의 품질을 평가했다. 통상적으로, 이러한 태스크들은 LLM들이 주어진 질문에 대한 대답에 도달할 때까지, 사실적 지식에 기초하여 단계적인 추론을 수행하도록 요구한다. 단계적 추론 능력을 도출하기 위해 LLM의 복잡한 추론 능력을 향상시키기 위한 CoT(chain-of-thought) 프롬프트 전략[33]이 제안되었다. 섹션 6.3에서 논의된 바와 같이, CoT는 LLM이 다단계 추론을 수행하도록 안내하기 위해 프롬프트에 수동으로 생성[33]되거나 자동으로 생성[674]될 수 있는 중간 추론 단계를 포함한다. 이러한 방법은 LLM의 추론 성능을 크게 향상시켜, 여러 복잡한 지식 추론 작업에 대한 새로운 최신 결과를 유도한다[33, 56, 526]. 또한, 지식 추론 태스크를 코드 생성 태스크로 재구성한 후, 연구자들은 특히 코드에 대해 사전 훈련된 LLM으로 LLM의 성능을 더욱 향상시킬 수 있음을 발견했다[211]. 그러나 지식추론 과제의 복잡성으로 인해 현재 LLM의 성능은 여전히 상식 추론과 같은 과제에 대한 인간의 결과에 뒤처진다[675, 56, 33]. 일반적인 유형의 실수로 LLM은 부정확한 중간 단계를 생성하여 잘못된 최종 결과를 초래할 수 있다. 이 문제를 해결하기 위해, 기존 작업은 전체 추론 체인의 정확도를 향상시키기 위한 특별한 디코딩 또는 앙상블 전략을 제안했다[436, 437].\n' +
      '\n' +
      '**기호적 추론 44.** 기호 추론 작업은 주로 형식 규칙 설정에서 기호를 조작 하 여 특정 목표를 수행 하는 데 중점을 둡니다. [51]. 여기서 연산 및 규칙은 사전 교육 중에 LLM에서 볼 수 없었던 것일 수 있습니다. 기존 작업[33, 439, 505]은 공통으로 마지막 글자 연결 및 동전 뒤집기의 작업에서 LLM을 평가하는데, 여기서 평가 예제는 문맥 내 예제와 동일한 추론 단계(도메인 내 테스트_라고 함) 또는 그 이상의 단계(도메인 외 테스트_라고 함)를 필요로 한다. 도메인 외 테스트의 경우 LLM은 문맥에서 두 단어가 있는 예만 볼 수 있지만 LLM은 세 개 이상의 단어의 마지막 글자를 연결해야 한다. 일반적으로 생성된 심볼의 정확도는 이러한 작업에 대한 LLM의 성능을 평가하기 위해 채택된다. 따라서 LLM은 복잡한 시나리오에서 기호 연산과 구성 사이의 의미 관계를 이해해야 한다. 그러나 도메인 외 환경에서 LLM은 기호 연산과 규칙의 복잡한 구성(예: 컨텍스트 예제에서 연산 수의 두 배)을 보지 않았기 때문에 LLM이 정확한 의미를 포착하기 어렵다. 이 문제를 해결하기 위해 기존 연구에서는 더 길고 복잡한 추론 프로세스를 생성하기 위해 LLM이 기호 연산을 더 잘 조작할 수 있도록 스크래치패드[591, 676] 및 튜터[677] 전략을 통합했다. 또 다른 연구 작업은 공식 프로그래밍 언어를 사용하여 기호 연산과 규칙을 나타내며, LLM은 코드를 생성하고 외부 인터프리터와 함께 실행하여 추론 프로세스를 수행해야 한다. 이러한 방법은 복잡한 추론 프로세스를 LLMs 및 인터프리터에 대한 코드 합성 및 프로그램 실행으로 각각 분해할 수 있어, 더욱 정확한 결과를 갖는 단순화된 추론 프로세스를 유도할 수 있다[443].\n' +
      '\n' +
      '각주 44: [33]에 이어 LLM을 평가하기 위해 특별히 고안된 기호 추론 과제를 주로 논의한다. KBQA의 지식 그래프에서 논리 규칙을 추론하는 것과 같은 전통적인 NLP 작업에서 기호 추론 방법을 고려하지 않는다.\n' +
      '\n' +
      '**수학적 추론.** 수학적 추론 작업은 문제를 해결 하거나 증명 진술을 생성 하기 위해 수학적 지식, 논리 및 계산을 포괄적으로 활용 해야 합니다. 기존의 수학적 추론 과제는 주로 수학 문제 해결과 자동화 정리 증명으로 범주화할 수 있다. 수학 문제 해결 과제의 경우, SVAMP[592], GSM8k[184] 및 MATH[364] 데이터 세트가 평가에 일반적으로 사용되며, 여기서 LLM은 수학적 문제에 답하기 위해 정확한 구체적인 수 또는 방정식을 생성할 필요가 있다. 이러한 작업들 또한 다단계 추론을 요구하기 때문에, CoT 프롬프트 전략은 추론 성능을 향상시키기 위해 LLMs에 대해 널리 채택되었다[33]. 또 다른 실제적인 전략으로서, 대규모 수학적 말뭉치에 대한 LLM들을 지속적으로 사전 트레이닝하는 것은 수학적 추론 과제들에 대한 그들의 성능을 크게 향상시킬 수 있다[678, 35, 203]. 또한, 서로 다른 언어의 수학 문제는 동일한 수학적 논리를 공유하기 때문에, 연구자들은 LLM의 다국어 수학적 추론 능력을 평가하기 위해 다국어 수학 단어 문제 벤치마크[524]도 제안한다. 또 다른 도전 과제로서, 자동화 정리 증명(ATP)[679, 600, 598]은 추론 모델이 추론 로직 및 수학적 기술을 엄격하게 따르도록 요구한다. 이 작업에 대한 성능을 평가하기 위해 PISA [599] 및 miniF2F [600]은 _증명 성공률_을 평가 메트릭으로 사용하는 두 가지 일반적인 ATP 데이터 세트입니다. 전형적인 접근법으로서, ATP에 대한 기존의 작업은 Lean, Metamath 및 Isabelle과 같은 상호작용 정리 프로버(ITP)를 사용하여 증명의 검색을 돕기 위해 LLM을 활용한다[680, 681, 682]. ATP 연구의 주요 한계는 공식 언어의 관련 말뭉치가 부족하다는 것이다. 이를 해결하기 위해 여러 연구에서는 LLM을 활용하여 새로운 데이터를 보강하기 위해 비공식 진술을 공식 증명으로 변환하거나 증명의 검색 공간을 줄이기 위해 초안 및 증명 스케치를 생성한다[683].\n' +
      '\n' +
      '**주요 문제.** 발전에도 불구하고 LLM은 복잡한 추론 작업을 해결하는 데 몇 가지 한계가 있습니다.\n' +
      '\n' +
      '* _비일관성 추론_ 향상된 추론 전략(예: CoT 프롬프트)을 통해 LLM은 지원 논리와 증거를 기반으로 단계별 추론을 수행하여 몇 가지 복잡한 추론 작업을 해결할 수 있다. 효과에도 불구하고, _추론 불일치_ 문제는 종종 분해된 추론 과정에서 발생한다. 구체적으로, LLM은 무효 추론 경로에 따라 정답을 생성하거나, 또는 정답 추론 과정[33,442] 이후에 오답을 생성하여, 도출된 답변과 추론 과정 간의 불일치를 초래할 수 있다. 이러한 문제를 완화하기 위해 기존 연구에서는 외부 도구나 모델을 통해 LLM의 전체 생성 과정을 안내하고 [636, 451, 437] 잠재적인 오류를 수정하기 위한 추론 과정과 최종 답변을 다시 확인하거나 [685, 686, 687] 프로세스 기반 피드백으로 LLM을 미세 조정하는 [688, 689]를 제안했다. 예를 들어, _Tree of Thoughts(ToT)_[451]은 LLM들이 다양한 추론 경로들을 동시에 탐색하고 자기 평가함으로써 의사결정 프로세스에 참여할 수 있게 한다. 추론 프로세스들을 정제하기 위해, Self-Refine[685]은 자체 생성된 솔루션들에 대한 LLM들로부터 피드백을 이끌어내어, 피드백에 기초한 솔루션들의 반복적인 정제를 가능하게 한다. 또한, 여러 연구는 훈련 중 프로세스 기반 감독의 통합을 통해 LLM의 추론 사슬의 일관성을 개선한다[688, 689]. 유망한 해결책으로서, 최근의 접근법들은 복잡한 추론 작업들을 코드 생성 작업들로 재구성하고, 여기서 생성된 코드의 엄격한 실행은 추론 과정과 결과 사이의 일관성을 보장한다. 또한, 유사한 입력들을 갖는 태스크들 사이에 불일치가 존재할 수 있다는 것이 밝혀졌는데, 여기서 태스크 설명의 작은 변화는 모델이 상이한 결과를 생성하게 할 수 있다[592, 49]. 이 문제를 완화하기 위해 자기 일관성[436]은 LLM의 디코딩 프로세스를 향상시키기 위해 다중 추론 경로의 앙상블을 채택한다.\n' +
      '\n' +
      '* **비일관성 복원** LLM은 잘못된 추론 경로에 따라 정답을 생성하거나 올바른 추론 프로세스 후에 오답을 생성하여 파생된 답변과 추론 프로세스 간에 불일치를 초래할 수 있습니다. 이슈는 프로세스 수준의 피드백으로 LLM을 미세 조정하고 다양한 추론 경로의 앙상블을 사용하고 자기 반영 또는 외부 피드백으로 추론 프로세스를 정제함으로써 완화될 수 있다.\n' +
      '* **숫자 계산.** 복잡한 추론 작업의 경우 LLM은 특히 많은 수의 산술 [677, 690, 49]과 같이 사전 훈련 중에 거의 발생하지 않는 기호의 경우 관련 숫자 계산에 여전히 어려움을 겪습니다. 이 문제를 해결하기 위해, 직접적인 방법은 합성된 산술 문제에 LLM을 조정하는 것이다[691, 361]. 또한 학습 및 추론 단계 [676, 692, 361], _예: 스크래치 추적]에서 중간 계산 단계를 추적하여 수치 계산 성능을 향상시켰다. 또한, 기존의 작업[80]은 특히 산술 연산을 처리하기 위한 외부 도구(_예를 들어, 계산기)를 통합하기도 하였다. 보다 최근에, ChatGPT는 외부 도구를 사용하기 위한 플러그인 메커니즘을 제공하였다[665]. 이러한 방식으로 LLM은 도구를 적절하게 조작하는 방법을 배워야 한다. 이를 위해, 연구자들은 LLM을 튜닝하기 위한 도구들(심지어 LLM 자체)을 사용하여 예들을 증강시키거나(80, 693), 또는 인-컨텍스트 학습을 위한 명령어들 및 예시들을 고안하였다[443]. 외부 도구의 도움 외에도, 최근의 연구들은 숫자들을 개별 토큰들(_예를 들어,_LLMA 및 Galactica tokenizers)로 토큰화하는 것이 LLM들의 고유한 산술 능력을 향상시키는 유용한 접근법이라는 것을 발견한다[690, 361]. 한 가지 가능한 설명은 서브워드 토큰화 기술이 숫자를 토큰화할 때 일관되지 않은 시퀀스를 초래할 수 있다는 것이다. 예를 들어, 서브워드 토큰라이저의 경우 정수 7481은 \\(7\\_481\\)으로 토큰화될 수 있는 반면, 74815는 \\(748\\_15\\)(서로 다른 분할을 갖는 동일한 숫자 부분 문자열)[361]로 토큰화될 수 있다. 비교로서 숫자에 대한 디지트 기반 토큰화는 이러한 불일치를 피할 수 있으므로 LLM의 수치 계산 능력을 향상시킬 가능성이 있다.\n' +
      '* **수치 계산.** LLM은 특히 사전 교육 중에 거의 발생하지 않는 기호의 경우 수치 계산에 어려움을 겪습니다. 수학적 도구를 사용하는 것 외에도 숫자를 개별 토큰으로 토큰화하는 것도 LLM의 연산 능력을 향상시키는 데 효과적인 설계 선택이다.\n' +
      '\n' +
      '### _Advanced Ability_\n' +
      '\n' +
      'LLM은 위의 기본 평가 과제 외에도 평가를 위한 특별한 고려가 필요한 일부 우수한 능력을 나타낸다. 이 부분에서 우리는 인간의 정렬, 외부 환경과의 상호 작용, 도구 조작을 포함한 몇 가지 대표적인 고급 능력과 그에 상응하는 평가 접근법에 대해 논의한다. 다음으로 이러한 고급 능력에 대해 자세히 논의합니다.\n' +
      '\n' +
      '#### 7.2.1 Human Alignment\n' +
      '\n' +
      'LLM은 실제 응용 프로그램에서 LLM의 광범위한 사용을 위한 핵심 능력인 인간 가치와 요구, 즉 인간 정렬을 잘 준수할 수 있기를 바란다.\n' +
      '\n' +
      '이 능력을 평가하기 위해 기존 연구에서는 유용성, 정직성 및 안전성과 같은 인간 정렬에 대한 여러 기준을 고려한다[368, 46, 170]. 도움이 되고 정직하기 위해, 적대적 질의 응답 태스크들(_e.g._, TruthfulQA[556])은 텍스트에서 가능한 거짓을 검출하는 LLM의 능력을 조사하기 위해 활용될 수 있다[36, 81]. 또한, 무해성은 또한 _예를 들어_, CrowS-Pairs[603] 및 Winogender[604]와 같은 여러 기존 벤치마크에 의해 평가될 수 있다. 위의 데이터 세트를 사용한 자동 평가에도 불구하고 인간 평가는 여전히 LLM의 인간 정렬 능력을 효과적으로 테스트하는 보다 직접적인 방법이다. OpenAI는 위험한 콘텐츠를 접할 때 GPT-4의 행동을 평가하고 개선하기 위해 AI 위험과 관련된 도메인의 많은 전문가를 초대한다[46]. 또한, 인간 정렬의 다른 측면들(_e.g._, 진실성)에 대해, 몇몇 연구들은 특정 명령들을 사용하고 주석 프로세스를 안내하기 위해 주석 규칙들을 고안하는 것을 제안한다[81]. 경험적 연구에 따르면 이러한 전략은 LLM의 인간 정렬 능력을 크게 향상시킬 수 있다[170]. 예를 들어, 전문가와의 상호작용을 통해 수집된 데이터에 대한 정렬 튜닝 후, GPT-4의 부정확한 행동률은 민감하거나 허용되지 않는 프롬프트를 처리할 때 크게 감소할 수 있다. 또한, 고품질의 사전-트레이닝 데이터는 정렬에 필요한 노력을 감소시킬 수 있다[46]. 예를 들어, 은하계는 과학 코퍼스의 편향되지 않은 내용 때문에 잠재적으로 더 무해하다[35].\n' +
      '\n' +
      '#### 7.2.2 외부 환경과의 상호 작용\n' +
      '\n' +
      '표준 평가 작업 외에도 LLM은 에이전트 조작을 위해 자연어로 행동 계획을 생성하는 행위 지시, _예_에 따라 외부 환경으로부터 피드백을 받고 행동을 수행할 수 있는 능력을 가지고 있다[694, 695]. 그러한 능력은 또한 상세하고 매우 현실적인 액션 플랜들을 생성할 수 있는 LLM들에서 출현하는 반면, 더 작은 모델들(_e.g._, GPT-2)은 더 짧거나 무의미한 플랜들을 생성하는 경향이 있다[694].\n' +
      '\n' +
      '이 능력을 테스트하기 위해 몇 가지 구현된 AI 환경과 벤치마크를 평가에 사용할 수 있으며 다음과 같이 설명한다. VirtualHome[606]은 청소 및 조리와 같은 가정용 작업을 위한 3D 시뮬레이터를 구축하며, 여기서 에이전트는 LLM에 의해 생성된 자연 언어 동작을 실행할 수 있다. ALFRED [608]은 구성 목표를 달성하기 위해 LLM이 필요한 보다 도전적인 작업을 포함한다. BEHAVIOR [607]은 시뮬레이션 환경에서 일상적인 작업에 중점을 두고 LLM이 객체의 내부 상태를 변경하는 복잡한 솔루션을 생성하도록 요구합니다. 가정 업무와 같은 제한된 환경 외에도 연구 작업 라인은 마인크래프트 및 인터넷과 같은 개방형 환경을 탐색하기 위한 LLM 기반 에이전트의 숙련도를 조사한다[696, 697]. 보이저[697]는 LLM들이 환경으로부터의 피드백에 기초하여 새로운 기술을 지속적으로 습득할 수 있게 하는 자동 커리큘럼 모듈을 도입한다. GITM[696]은 LLM을 기반으로 하는 마인크래프트의 다양한 과제를 태스크 분해, 계획, 인터페이스 호출 등을 통해 해결하는 데 초점을 맞추고 있다. 생성된 액션 플랜 또는 작업 완료에 기초하여, 기존 작업은 벤치마크에서 정규 메트릭(_e.g._, 생성된 액션 플랜의 실행 가능성 및 정확성)[694]을 채택하거나 실제 실험을 직접 수행하고 성공률[698]을 측정하여 그러한 능력을 평가한다. LLM은 외부 환경과 상호작용하고 정확한 행동 계획을 생성할 수 있는 것으로 나타났다[699]. 최근에, LLMs의 상호 작용 능력, _예._, 코드-유사 프롬프트 설계[530] 및 실세계 접지 제공[698]을 향상시키기 위한 몇 가지 개선 방법이 제안되었다.\n' +
      '\n' +
      '또한, 최근의 작업은 시뮬레이션된 환경들[700, 701, 533]에서 LLMs에 기초한 멀티-에이전트 협업을 탐구한다. 이러한 연구는 샌드박스 환경에서 관찰, 계획 및 기억을 가진 여러 LLM 기반 에이전트를 인스턴스화하여 인간의 사회적 행동을 시뮬레이션한다. 통제된 평가에서 생성 주체의 탐색, 계획, 사고 능력은 인간에 의해 인터뷰와 같은 방식으로 평가된다. 또한, 그들은 또한 새로운 사회적 행동을 조사하기 위해 시뮬레이션된 환경 내의 여러 에이전트에 대한 설명 측정을 수행한다.\n' +
      '\n' +
      '#### 7.2.3 도구 조작\n' +
      '\n' +
      '복잡한 문제를 해결할 때 LLM은 필요한 경우 외부 도구로 전환할 수 있다. API 호출로 사용 가능한 도구를 캡슐화함으로써 기존 작업은 여러 특정 작업에서 LLM의 성능을 향상시키기 위해 _예:_, 검색 엔진 [81], 계산기 [80], 컴파일러 [443]와 같은 다양한 외부 도구를 포함했습니다. 최근 OpenAI는 ChatGPT[665]에서 플러그인 사용을 지원하여 LLM에 언어 모델링을 넘어 더 넓은 용량을 제공할 수 있다. 예를 들어, 웹 브라우저 플러그인은 ChatGPT가 새로운 정보에 액세스할 수 있게 한다. 또한 타사 플러그인을 통합하는 것은 LLM을 기반으로 하는 응용 프로그램의 번영하는 생태계를 만드는 데 특히 중요하다.\n' +
      '\n' +
      '도구 조작 능력을 조사하기 위해 기존의 작업은 주로 수학적 문제 해결(_e.g._, GSM8k [184] 및 SVAMP [592]) 또는 지식 질의 응답(_e.g._, TruthfulQA [556])과 같은 복잡한 추론 작업을 평가에 채택하며, 여기서 도구의 성공적인 활용은 LLM이 수행할 수 없는 필요한 기술을 향상시키는 데 매우 중요하다. 이러한 방식으로, 이들 태스크에 대한 평가된 성능은 툴 조작에서 LLM의 능력을 반영할 수 있다. LLM에게 도구를 활용하도록 가르치기 위해 기존 연구에서는 컨텍스트에서 도구를 사용하는 예를 추가하여 LLM을 유도하거나 [443] 도구 활용에 대한 시뮬레이션된 데이터에서 LLM을 미세 조정합니다[693, 80]. 도구의 도움으로, LLM은 그들이 잘하지 못하는 문제들, _예._, 방정식 계산 및 시기적절한 질문들에 응답하는 것을 더 능숙하게 하는 것으로 밝혀졌다[80, 448]. 그러나 사용 가능한 도구의 수가 증가함에 따라 LLM의 제한된 컨텍스트 길이는 광범위한 도구 API를 설명하고 시연하는 데 문제가 될 수 있다. 이 문제를 해결하기 위해, 기존 작업은 관련 툴의 사용, 또는 툴 정보를 임베딩 공간 내의 토큰으로서 인코딩한다[702, 703, 704].\n' +
      '\n' +
      'LLM은 인간이 개발한 기존 도구 외에도 자율적으로 특정 작업을 위한 자체 도구를 만들 수 있는 능력을 보유하고 있다[705]. 이를 통해 모델은 이러한 자체 생성된 도구를 독립적으로 탐색하고 조작할 수 있으며, 이를 통해 광범위한 실제 작업을 해결하는 데 있어 자율 탐색의 잠재력을 확장할 수 있다.\n' +
      '\n' +
      '_ 요약._ 위의 세 가지 능력은 LLM의 실제 성능에 큰 가치가 있다: 인간의 가치와 선호에 순응하는 것(인간 정렬), 실제 시나리오에서 적절하게 행동하는 것(외부 환경과의 상호작용), 능력 범위 확장(도구 조작)이다. 위의 세 가지 고급 능력 외에도 LLM은 일부 작업(_e.g._, 데이터 주석[486]) 또는 학습 메커니즘(_e.g._, 자체 개선[706])과 특별히 관련된 다른 능력도 보여줄 수 있다. 이러한 새로운 능력을 발견하고 측정하고 평가하여 LLM을 더 잘 활용하고 개선할 수 있는 열린 방향이 될 것이다.\n' +
      '\n' +
      '### _Benchmarks and Evaluation Approaches_\n' +
      '\n' +
      '이상에서 LLM의 기본 능력과 발전된 능력에 대해 논의하였다. 다음으로, 기존의 평가 벤치마크 및 접근법을 소개할 것이다[733, 734].\n' +
      '\n' +
      '#### 7.3.1 종합 평가 벤치마크\n' +
      '\n' +
      '최근 LLM의 평가를 위한 몇 가지 포괄적인 벤치마크[70, 364, 520]가 출시되었다. 이 부분에서는 널리 사용되는 몇 가지 벤치마크, 즉 _i.e._, MMLU, BIG-bench, HELM 및 일련의 인간 시험 벤치마크를 소개한다.\n' +
      '\n' +
      '\\(\\bullet\\)_MMLU_[364]는 수학 및 컴퓨터 과학에서 인문 사회 과학에 이르기까지 광범위한 지식 영역을 포괄하는 다중 작업 지식 이해의 대규모 평가를 위한 다용도 벤치마크이다. 이러한 업무의 어려움은 기본부터 고급까지 다양합니다. 기존 작업에서 볼 수 있듯이 LLM은 모델 크기에서 스케일링 법칙을 보여주는 이 벤치마크 [56, 57, 56, 69]에서 대부분 작은 모델보다 상당한 마진을 능가한다. 보다 최근에, GPT-4는 MMLU에서 현저한 기록(5-샷 설정에서 86.4%)을 달성하는데, 이는 이전의 최신 모델[46]보다 상당히 우수하다.\n' +
      '\n' +
      '\\(\\bullet\\)_BIG-bench_[70]은 기존의 LLM을 다양한 측면에서 조사하기 위한 협력 벤치마크이다. 언어학, 아동 발달, 수학, 상식 추론, 생물학, 물리학, 사회적 편견, 소프트웨어 개발 등을 포함한 광범위한 주제를 포함하는 204개의 과제로 구성된다. 모델 크기를 스케일링함으로써, LLM은 심지어 BIG-bench의 작업의 65%에 대해 소수의 샷 설정 하에서 평균 인간 성능을 능가할 수 있다[56]. 전체 벤치마크의 높은 평가 비용을 고려하여, BIG-bench에서 24개의 작지만 다양하고 도전적인 과제를 포함하는 경량 벤치마크 BIG-bench-Lite가 제안되었다. 또한, BIG-bench hard (BBH) 벤치마크 [365]는 LLM이 인간에 비해 열등한 성능을 보이는 도전 과제를 선택하여 현재 해결 불가능한 LLM의 작업을 조사하는 데 집중하도록 제안되었다. BBH가 더 어려워지기 때문에, 작은 모델들은 대부분 랜덤에 가까운 성능을 달성한다. 비교로서 CoT 프롬프트는 BBH의 평균 인간 성능을 초과하더라도 성능을 향상시키기 위한 단계별 추론을 수행하는 LLM의 능력을 이끌어낼 수 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_HELM_[520]은 현재 16개 시나리오 및 7개 범주의 메트릭의 핵심 집합을 구현하는 포괄적인 벤치마크입니다. 많은 선행 연구 위에 구축되어 언어 모델에 대한 전체론적 평가를 수행합니다. HELM의 실험 결과에서 보듯이 명령어 튜닝은 정확도, 견고성, 공정성 측면에서 일관되게 LLM의 성능을 높일 수 있다. 또한 추론 태스크의 경우 코드 코퍼스에서 사전 훈련된 LLM이 우수한 성능을 보인다.\n' +
      '\n' +
      '\\(\\bullet\\)_인간 수준 테스트 벤치마크_ AGIEval [708], MMCU [709], M3KE [710], C-Eval [711] 및 Xiezhi [712]와 같은 인간 테스트를 위해 설계된 질문으로 LLM의 포괄적인 능력을 평가하는 것을 목표로 합니다. 이러한 벤치마크는 LLM의 일반적인 기능에 대한 포괄적인 평가를 제공하기 위해 광범위한 도메인, 난이도 및 언어를 포함한다. 공개적으로 이용가능한 모델들에 비해, API 서비스들을 제공하는 모델들(_e.g._, GPT-4, ChatGPT, Claude)은 이러한 평가 벤치마크들에서 공개적으로 이용가능한 모델들에 비해 우수한 성능을 입증한다. 평가에서 가장 성능이 좋은 모델로서, GPT-4는 AGIEval에서 평균 인간 성능을 능가한다[708]. 그러나 여전히 이러한 도전적인 벤치마크에서 인간의 최고 성능보다 뒤처져 있다. 따라서 특히 공개적으로 액세스할 수 있는 모델에 대한 LLM의 전반적인 능력을 더욱 향상시킬 여지가 충분하다.\n' +
      '\n' +
      '위의 벤치마크는 LLM의 평가를 위한 다양한 주류 평가 과제와 실제 인간 시험 문제를 다룬다. 또한, 다국어 지식 활용을 위한 TyDiQA[735], 다국어 수학적 추론을 위한 MSSM[524]과 같은 LLM의 특정 능력을 평가하는 데 중점을 둔 몇 가지 벤치마크가 있다. 평가를 수행하기 위해서는 특정 목표에 따라 적합한 벤치마크를 선택할 수 있다. 또한 연구자가 기존 벤치마크에서 LLM을 평가하거나 맞춤형 평가를 위한 새로운 작업을 확장하기 위한 여러 오픈 소스 평가 프레임워크(언어 모델 평가 Harness[736] 및 OpenAI Evals[46])도 있다. 또한, 일부 연구자들은 Open LLM 리더보드와 같은 기존 LLM의 성능을 비교하기 위해 대표적인 벤치마크를 통합하여 지속적으로 업데이트된 리더보드를 구성한다[707]. 위의 벤치마크와 리더보드는 LLM의 기본 및 고급 능력을 입증하는 중요한 참조를 제공한다. 7.3.2절 평가 접근법에 대한 찬반 논의를 좀 더 깊게 할 것이다.\n' +
      '\n' +
      '#### 7.3.2 평가 방법\n' +
      '\n' +
      '기존 벤치마크를 도입한 후, 이 부분에서는 LLM의 성능을 평가하기 위한 기존 평가 접근법을 검토할 것이다. 토론을 구성 하기 위해 _기본 LLMs_ (미리 훈련 된 모델 검사점), _fine-tuned LLMs_ (명령 또는 정렬 미세 조정 모델 검사점) 및 _specialized LLMs_ (일부 특정 작업 또는 도메인에 대 한 적응 된 모델 검사점)의 세 가지 유형으로 LLMs를 분류 합니다. 여기서는 일반적인 작업 해결자와 특정 작업 해결자의 다른 목적을 구별하기 위해 미세 조정된 LLM과 특수 LLM을 모두 유지한다. 세 가지 유형의 LLM을 평가하기 위해 다른 능력(예: 섹션 7.1 및 7.2에서 논의된 기본 또는 고급 능력)과 관련된 LLM의 성능을 테스트할 수 있다. 일반적으로 LLM을 평가하기 위한 세 가지 주요 접근법, 즉 벤치마크 기반 접근법[364], 인간 기반 접근법[727], 모델 기반 접근법[729]이 있다. 표 15는 LLM 유형, 평가 접근법 및 시험 능력 간의 관계를 보여준다. 다음으로, 다양한 유형의 LLM에 대한 평가 접근법에 대해 논의할 것이다.\n' +
      '\n' +
      '**기본 LLM의 평가** 기본 LLM은 사전 교육 직후에 얻은 모델 검사점을 참조 합니다. 기본 LLM의 경우 주로 복잡한 추론 및 지식 활용과 같은 기본 능력(섹션 7.1)을 조사하는 데 중점을 둔다. 이러한 기본 능력의 대부분은 잘 정의된 작업으로 평가할 수 있기 때문에 벤치마크 기반 접근법은 기본 LLM을 평가하는 데 널리 사용되었다. 다음으로 기본 LLMs에 대한 공통 평가 벤치마크와 평가 절차를 소개한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Common benchmarks._ 기본 LLM을 평가하기 위해 일반적인 벤치마크는 객관식 문제와 같은 폐쇄형 문제의 형태로 설계된다. 이러한 일반적으로 사용되는 벤치마크는 크게 지식 중심과 추론 중심 벤치마크의 두 가지 범주로 나눌 수 있다. 지식 지향 벤치마크(_e.g._, MMLU[364] 및 C-Eval[711])는 세계 지식의 능력을 평가하는 것을 목표로 하는 반면, 추론 지향 벤치마크(_e.g._, GSM8K[643], BBH[365] 및 MATH[364])는 복잡한 추론 태스크를 해결하는 능력을 평가하는 것에 중점을 둔다. 또한, 최근에 제안된 몇몇 벤치마크들(_e.g._, OpenCompass[713])은 포괄적인 비교를 위해 이 두 타입들을 결합한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Benchmark based evaluation procedure._ 벤치마크 평가를 수행하기 위해, 각각의 문제는 먼저 LLM들이 결과 텍스트를 생성하기 위한 프롬프트로 포맷될 것이다. 그런 다음, 생성된 결과 텍스트는 예측된 답변을 얻기 위해 인간이 작성한 규칙으로 파싱될 것이다. 마지막으로, 예측된 답변과 지상-진실의 답변을 비교하여 정확도와 같은 표준 메트릭을 사용하여 LLM의 성능을 자동으로 계산할 수 있다. 평가 접근법은 적은 샷 또는 제로 샷 설정에서 수행될 수 있으며, 이는 다른 평가 결과 또는 순위로 이어질 수 있다. 기본 LLM은 (상대적으로 약한 작업 일반화 능력으로) 세밀하게 조정되지 않았기 때문에, 적은 샷 설정이 종종 평가에 더 적합하다. 일부 복잡한 추론 태스크의 경우, CoT 프롬프트는 또한 평가 동안 용량을 완전히 나타내기 위해 사용될 필요가 있다. 또 다른 주목할 점은 이 평가 접근법이 미세 조정된 LLM의 능력을 평가하는 데에도 적용될 수 있다는 것이다. 실제로, 여러 리더보드(_e.g._, Open LLM 리더보드[707])가 이 접근법을 기반으로 구축되어 기본 및 미세 조정된 LLM을 모두 평가한다.\n' +
      '\n' +
      '**Fine-tuned LLMs 평가** 이 부분의 Fine-tuned LLMs는 미리 훈련된 모델 가중치 45를 기반으로 하는 지침 조정 또는 정렬 조정 후 얻은 모델 검사점을 참조 합니다. 일반적으로 Fine-tuned LLMs는 테스트 됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline\n' +
      '**Method** & **Evaluation** & **Model Types** & **Abilities/Domain** & **Data Source** \\\\ \\hline \\hline  & MMLLU [364] & Base/Fine-tuned/Specialized & General & Human exam/practice \\\\  & BIG-bench [70] & Base/Fine-tuned/Specialized & General & Human annotation \\\\  & HELM [520] & Base/Fine-tuned/Specialized & General & Benchmark collection \\\\  & Drem LM Leaderboard [707] & Base/Fine-tuned/Specialized & General & Benchmark collection \\\\  & AGLEval [708] & Base/Fine-tuned/Specialized & General & Human exam/practice \\\\  & MMCU [709] & Base/Fine-tuned/Specialized & General & Human exam/practice \\\\  & MXK [710] & Base/Fine-tuned/Specialized & General & Human exam/practice \\\\  & C-Eval [711] & Base/Fine-tuned/Specialized & General & Human exam/practice \\\\  & Xiezhi [712] & Base/Fine-tuned/Specialized & General & Human exam/practice \\\\  & OpenCompass [713] & Base/Fine-tuned/Specialized & General & Benchmark collection \\\\  & Chain-of-Thought Hub [714] & Base/Fine-tuned & General & Benchmark collection \\\\  & KoLA [715] & Base/Fine-tuned & Knowledge utilization & Web \\\\  & ARB [716] & Fine-tuned & Complex reasoning & Human exam/practice \\\\ Benchmark & APIBench [717] & Base/Fine-tuned & Tool manipulation & Web \\\\  & APIBank [718] & Fine-tuned & Tool manipulation & Synthesis \\\\  & ToolApaca [719] & Base/Fine-tuned & Tool manipulation & Synthesis \\\\  & T-Bench [720] & Fine-tuned & Tool manipulation & Synthesis \\\\  & ToolBench [721] & Fine-tuned & Tool manipulation & Synthesis \\\\  & BOLAA [722] & Base/Fine-tuned & Environment interaction & Benchmark collection \\\\  & AgentBench [723] & Base/Fine-tuned & Environment interaction & Human annotation/Synthesis \\\\  & Haluk’s [602] & Base/Fine-tuned & Human alignment & Human annotation/Synthesis \\\\  & PromptBench [724] & Base/Fine-tuned & Robustness & Benchmark collection \\\\  & HumanEval [105] & Base/Fine-tuned/Specialized & Code synthesis & Human annotation \\\\  & MultiduceQA [356] & Specialized & Healthcare & Benchmark collection \\\\  & FLUE [725] & Specialized & Finance & Benchmark collection \\\\  & LegalBench [726] & Specialized & Legal & Human annotation \\\\ \\hline \\hline  & Chatbot Arena [727] & Base/Fine-tuned/Specialized & Human Alignment & Human annotation \\\\  & SciBench [728] & Fine-tuned & Complex reasoning & Human exam/practice \\\\ \\hline \\hline  & AlpacaEval [729] & Fine-tuned & Instruction following & Synthesis \\\\  & MT-bench [727] & Fine-tuned & Human alignment & Human annotation \\\\ Model & TrustCPT [720] & Base/Fine-tuned & Human alignment & Benchmark collection \\\\  & LMExamQA [731] & Base/Fine-tuned & Knowledge utilization & Synthesis \\\\  & ChatEval [732] & Base/Fine-tuned & Knowledge utilization & Benchmark collection \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE XC: A category of existing evaluation work. “General” denotes that the evaluation focuses on an overall performance of multiple abilities. The evaluated abilities are not limited to the representative basic and advanced abilities mentioned in Section 7.1 and 7.2.\n' +
      '\n' +
      '다양한 능력(예: 지식 활용 및 인간 정렬)에 따라 여러 평가 접근법으로 평가되는 것이 일반적이다. 벤치마크 기반 평가 외에도 인간 기반 및 모델 기반 접근법이 미세 조정된 LLM의 고급 능력을 평가하는 데 널리 사용되었다. 다음으로, 두 가지 평가 방법에 대해 소개하도록 한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Human-based evaluation_. 기본 능력에 대한 자동 평가와 달리 인간 평가는 일반적으로 인간 정렬 및 도구 조작과 같은 실제 사용에서 더 많은 요소 또는 능력을 고려한다. 이러한 평가 접근법에서 시험 과제는 보통 개방형 질문의 형태이며, 인간 평가자를 초청하여 LLM이 생성하는 답변의 질에 대한 판단을 하게 된다. 통상적으로, 인간 평가자를 위한 채점 방법에는 쌍대 비교와 단답형 채점이라는 두 가지 주요 유형이 있다. 쌍별 비교에서 동일한 질문이 주어지면 인간은 서로 다른 모델의 두 가지 답변을 할당받아 어느 것이 더 나은지 결정하는 반면, 단답형 채점에서는 한 번에 한 가지 답변만 채점하면 된다. 예를 들어, HELM[520]은 요약 및 허위 정보 작업에 대해 단일 응답 등급을 수행하기 위해 인간을 고용하는 반면, 챗봇 아레나[727]는 사용자가 두 개의 익명 채팅 LLM으로 대화에 참여하고 쌍별 비교 결과를 보고할 수 있는 크라우드소싱 플랫폼을 구축한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Model-based evaluation_. 인간 기반 평가는 비용과 시간이 많이 소요되기 때문에 일부 작업은 ChatGPT 및 GPT-4와 같은 강력한 폐쇄 소스 LLM을 인간 평가자의 대리인으로 활용하는 것을 제안했다[727, 729]. 예를 들어, AlpacaEval [729]는 명령어들의 세트를 수집하고, 기준 출력들에 대한 쌍별 비교를 수행하기 위해 가능한 LLM(_e.,_ GPT-4)을 판사로 활용한다. 나아가, MT-bench[727]는 평가를 위한 멀티턴 질문 세트를 수집하고 ICL 및 CoT와 같은 방법을 통해 LLM 기반 평가자의 신뢰성을 향상시킨다. 챗GPT 및 GPT-4와 같은 LLM은 인간 평가자와 비교하여 소규모 수작업 및 대규모 크라우드소싱 평가 작업 모두에서 인간과 높은 일치를 달성할 수 있다. 그럼에도 불구하고 이러한 폐쇄 소스 LLM은 액세스가 제한적이며 데이터 누출의 잠재적 위험이 있다. 이를 해결하기 위해 최근 작업 [727]은 인간 평가자의 점수 데이터를 사용하여 모델 평가자로 오픈 소스 LLM(_e.,_ Vicuna [138])을 미세 조정하여 강력한 폐쇄 소스 LLM(_e.,_ GPT-4)과의 격차를 좁혔다.\n' +
      '\n' +
      '**전문화된 LLM의 평가** 전문화된 LLM은 의료 [356] 및 금융 [737]과 같은 일부 도메인 또는 애플리케이션에 특별히 적용되는 모델 체크포인트를 참조하세요. 특수 태스크 해결자로서, 특수 LLM은 일반 능력들(예를 들어, 복합 추론과 같은 기본 능력 및 인간 정렬과 같은 고급 능력)뿐만 아니라, 그들의 지정된 도메인 또는 애플리케이션과 관련된 특정 능력들에 대해 테스트될 것이다. 이를 위해 대상 도메인 또는 응용 프로그램에 맞는 특정 벤치마크를 구성해야 하는 경우가 많습니다. 그런 다음 이러한 도메인별 벤치마크를 일반 벤치마크와 결합하여 특수 LLM에 대한 포괄적인 평가와 표적 평가를 모두 수행할 수 있다. 예를 들어, MultiMedQA[356]는 건강 검진 및 건강 관리 질문을 포함하는 건강 관리의 특정 벤치마크이다. 이 작업[356]에서 MultiMedQA는 MMLU[364]와 결합하여 Med-PaLM[356]과 같은 의료를 위한 특수 LLM의 성능을 평가했다. 유사하게, FLUE [737]은 금융에 대한 벤치마크를 구성하며, 금융 감정 분석으로부터 질문 응답에 걸쳐 있다. 블룸버그GPT [360]과 같은 임상 LLM을 평가하기 위해 BBH [365]와 협력적으로 사용되었다.\n' +
      '\n' +
      '**다른 평가 접근법의 장단점** 위에서 LLM의 능력을 평가하기 위한 다양한 평가 접근법에 대해 논의했다. 다음으로 각 평가 접근법의 장단점을 간단히 분석한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Benchmark 기반 접근 방식_입니다. 이 평가 접근법은 LLM의 성능을 평가하기 위해 기존 벤치마크를 활용할 수 있다. 이러한 벤치마크에 관련된 태스크는 종종 핵심 능력(_예:_ 추론)을 측정하기에 충분한 테스트 샘플을 포함한다. 전체 평가 절차는 (거의) 자동적일 수 있으며, 다양한 기본 LLM에 대한 테스트 실험을 수행하는 것이 편리하며, 특히 사전 훈련 동안 모델 체크포인트의 성능을 모니터링하는 데 유용하다. 그러나 LLM은 질문 프롬프트, 제로샷 또는 소샷 테스트, 답변 파싱 방법을 포함하여 평가 설정에 민감한 경우가 많다. 따라서 평가 실험을 수행할 때 가능한 영향 요인을 고려해야 한다. 평가 결과는 채택된 평가 설정으로 유의해야 한다. 또 다른 문제는 데이터 오염[56, 738], _즉, 테스트 데이터 자체 또는 관련 콘텐츠가 사전 훈련 말뭉치에 포함되어 있다는 것이다. LLM 개발을 위해 점점 더 많은 오픈 데이터가 수집되었기 때문에 이러한 현상은 점점 더 심각해졌다.\n' +
      '\n' +
      '\\(\\bullet\\)_Human-based approach_. 인간 평가는 실제 작업을 해결하기 위한 LLM의 능력을 평가할 때 몇 가지 이점을 제공한다. 주요 이점 중 하나는 LLM의 실제 능력을 직접 반영하는 능력이다. 실제 사용자의 피드백과 경험에 기초하여 인간 평가는 실제 시나리오에서 LLM의 성능을 보다 직접적으로 측정할 수 있다. 또한, 인간 평가자를 기반으로 보다 유연하고 다양한 평가 작업을 수행할 수 있다. 예를 들어, 사용자는 다양한 쿼리를 제출하고 자신의 작업 인식에 따라 LLM의 능력을 테스트할 수 있다. 이는 다양한 유형의 작업 및 컨텍스트에 걸쳐 LLM의 장단점을 심층적으로 이해할 수 있다. 그러나 인간 평가는 또한 정확성과 일관성에 잠재적으로 영향을 미칠 수 있는 고유한 한계를 가지고 있다. 평가자 간의 개인화된 취향 및 다양한 교육 수준과 같은 요소는 평가 과정에서 편향 또는 불일치를 도입할 수 있다. 경우에 따라 사용자의 판단은 주관적일 수 있으며, 이는 LLM의 진정한 능력을 반영하지 못할 수 있다. 더욱이, 견고하고 신뢰할 수 있는 인간 평가를 수행하는 것은 종종 많은 수의 평가자를 필요로 하는데, 이는 매우 비싸고 시간이 많이 걸릴 수 있다. 또한 인간 평가는 재현성이 없는 경우가 많아 기존 평가 결과를 확장하거나 LLM의 진행 상황을 추적하는 것이 불가능하다.\n' +
      '\n' +
      '\\(\\bullet\\)_Model-based approach_. 인간 기반 접근법의 대리자로서 모델 기반 접근법은 인간 참여에 대한 의존도를 줄이고 보다 효율적이고 확장 가능한 평가를 가능하게 하는 역할을 한다. 또한, LLM은 할당된 평점 점수에 대한 의미 있는 설명을 제공하여 평가의 해석 가능성을 높일 수 있다. 확장성과 설명성에도 불구하고 모델 기반 접근 방식은 위치, 장황함, 자기 향상 편향 등 여러 가지 문제를 겪고 있는 것으로 밝혀졌다[727]. 특히, 위치 편향(응답의 제시 순서)은 LLM이 다른 사람들보다 특정 위치에서 응답에 대해 높은 점수를 부여하는 경향이 있다는 사실을 의미하며, 장황 편향은 LLM이 더 짧은 응답에 비해 품질이 짧더라도 장황한 답변을 선호한다는 것을 의미하며, 자기 향상 편향은 LLM이 종종 자신의 세대에서 과대평가된다는 것을 나타낸다. 또한, LLM은 복잡한 추론 문제를 해결하는 데 제한된 용량을 갖기 때문에, 일부 어려운 작업(예를 들어, 수학 추론)에 대한 적격 평가자로서의 역할을 할 수 없다. 이러한 한계는 특정 신속한 엔지니어링 및 미세 조정 전략에 의해 어느 정도 완화될 수 있다[727].\n' +
      '\n' +
      '요약하자면, LLM 평가에 대한 기존 작업의 범주화(표 X)는 주로 평가 방법론과 모델 유형의 두 가지 주요 차원을 기반으로 하며, 이는 테스트 능력과 함께 더욱 확장된다. LLM 평가를 위한 기존 작업의 분류 또는 분류에 대해서도 논의한 최근 작업[733, 734]이 있다.\n' +
      '\n' +
      '### _Empirical Evaluation_\n' +
      '\n' +
      'LLM의 전반적인 능력을 평가하기 위해 위의 평가 벤치마크와 접근법이 주로 사용된다. 이 부분에서는 7.1절과 7.2절에서 논의된 능력에 대한 세밀한 평가를 수행한다. 각 능력 유형에 대해 LLM의 해당 성능을 조사하기 위해 평가 실험을 수행하기 위한 대표 과제와 데이터 세트를 선택한다.\n' +
      '\n' +
      '#### 7.4.1 실험 설정\n' +
      '\n' +
      '이 부분에서 우리는 평가를 위한 실험 설정을 소개한다.\n' +
      '\n' +
      '**평가 모델.** 평가를 수행 하려면 다음과 같이 오픈 소스 모델에서 폐쇄 소스 API 액세스 모델에 대 한 대표적인 LLM을 고려 합니다.\n' +
      '\n' +
      '\\(\\bullet\\)_Open-source models_. 기존의 오픈소스 모델은 기본 모델과 명령어 튜닝 모델로 분류할 수 있다. 베이스 모델들은 언어 모델링 목적을 갖는 대형 범용 코퍼스에서만 사전 트레이닝되지만, 더 이상의 감독된 미세 조정은 없다. 평가에서는 LLaMA (7B) [57], LLaMA 2 (7B) [99], Pythia (7B and 12B) [96], Falcon (7B) [747]46을 포함한 4개의 대표적인 기본 모델을 선택한다. 명령 조정 모델은 명령(_i.,_ 태스크 데이터 세트, 일일 채팅 또는 합성 명령)을 사용하여 미세 조정된 모델이다. 실험에서는 Vicuna (7B 및 13B)[138], Alpaca (7B)[137], ChatGLM (6B)[93]을 포함한 4개의 대표적인 명령어 조정 모델을 선택한다. 또한 비교를 위해 LLaMA 2-Chat (7B)[99]를 포함하였으며, LLaMA 2 (7B)를 기반으로 명령어 튜닝과 RLHF를 통해 인간과 정렬된 대표적인 모델이다.\n' +
      '\n' +
      '각주 46: 더 큰 모델을 사용한 실험은 계산 자원의 한계로 인해 여전히 일정에 있다.\n' +
      '\n' +
      '* _Closed-source models._ 오픈소스 모델 외에도 API를 통해서만 접근할 수 있는 폐쇄소스 모델도 있어 개발자와 연구자 모두에게 많은 관심을 받았다. 여기서는 텍스트-다빈치-002/003(짧은 _Davinci002/003_), ChatGPT, Claude 및 Claude 2를 포함한 대표적인 폐쇄 소스 모델 4개를 선택하며, 여기서 처음 세 모델은 OpenAI에서 개발하고 나머지 두 모델은 Anthropic에서 개발한다.\n' +
      '\n' +
      '**작업 및 데이터 세트.** 다음으로 섹션 7.1 및 섹션 7.2에서 논의 된 능력에 대 한 평가 작업 및 데이터 세트를 설정 합니다. 이러한 데이터 세트에 대 한 LLM의 제로 샷 성능을 주로 평가 합니다. 영샷(zero-shot) 방식으로 해결하기 어려운 보다 복잡한 태스크(예: 수학적 추론 및 도구 조작)에 대해, 우리는 오픈 소스 모델의 컨텍스트 길이 제한을 고려하여 주로 3샷 성능을 보고한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Language generation_. 앞서 논의한 바와 같이, 우리는 언어 생성을 위해 _즉, 언어 모델링, 조건부 텍스트 생성 및 코드 합성의 세 가지 종류의 작업을 평가하는 것을 고려한다. 특히, 일반적으로 사용되는 LAMBADA [233] (언어 모델링), WMT\'22 [545] (기계 번역), XSum [549] (텍스트 요약), HumanEval [105] (코드 합성)의 4가지 데이터 세트를 선택하여 평가한다. WMT\'22에서는 기계 번역에서 LLM의 평균 성능을 조사하기 위해 원래 대규모 테스트 세트에서 각 언어 쌍에 대해 1000개의 예를 선택하여 새로운 평가 세트를 구성한다. 이러한 데이터 세트에 대한 LLM의 제로 샷 성능을 평가하고 LAMBADA, WMT\'22의 경우 _BLEU-4_, XSum의 경우 _ROUGE-L_ 및 HumanEval의 경우 _pass_\\(@10\\)을 예측하는 단어의 _정확도_를 계산한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Knowledge utilization_. 지식 활용 능력을 평가하기 위해 4개의 질문 응답 데이터 세트(_i.,_ TriviaQA[558], Natural Questions[554], Web Questions[557], ARC[555])와 팩트 추출 데이터 세트인 WikiFact[571)를 선택한다. 또한 이러한 데이터 세트에 대한 LLM의 제로 샷 성능을 보고하고 ARC에 대한 _정확도_ 및 다른 데이터 세트에 대한 _정확한 일치_를 계산한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Complex reasoning_. 복잡한 추론은 OpenbookQA[566], HelaSwag[582], SocialIQA[581], 상징추론은 표 [70]의 Colored Objects[70], Penguins[70], 수학적 추론은 GSM8k[184], MATH[364]의 비교 모델을 평가한다. OpenbookQA, HellaSwag 및 SocialIQA에 대한 _정확도_ 및 테이블의 유색 개체 및 펭귄에 대한 _해결 비율_ 및 GSM8k 및 MATH에 대한 _정확도_ 를 계산합니다. 지식 추론 태스크는 모두 제로샷 환경에서 해결할 수 있는 QA 태스크이기 때문에 제로샷 성능을 평가한다. 복잡한 기호 추론과 수학적 추론 작업을 위해 우리는 LLM을 더 잘 이끌어내기 위해 3-샷 인-컨텍스트 예시들을 활용한다. 기존 연구[443, 33]에 이어 수학적 추론 과제를 더 잘 풀기 위해 연쇄 사고 촉진 전략도 활용한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Human alignment_. 인간 정렬을 위해 LLM이 질문에 대한 답변을 생성하는 데 진실한지 여부를 측정하기 위해 TruthfulQA[556], LLM의 고정관념을 평가하기 위해 CrowS-Pairs[603] 및 WinoSender[604], LLM이 독성 언어를 생성하는 정도를 평가하기 위해 RealIoxicityPromps[605], LLM이 환각을 인식하는 능력을 테스트하기 위해 HaluEval[602]를 선택한다. Real-Toxicity-Prompts의 테스트 세트가 너무 크기 때문에 평가를 위해 무작위로 10000개의 샘플을 샘플링한다. 우리는 LLaMA [57]에 따라 제로 샷 성능을 보고하고, TruthfulQA에 대한 클레임을 참으로 식별하는 _정확도_, CrowS-Pairs에 대한 편향된 문장(높은 복잡도)을 인식하는 _정확도_, WinoGender에 대한 참조 해결 정확도(the/she/they)_, RealToxicityPrompts에 대한 _독성 점수_ 및 HaluEval에 대한 환각을 인식하는 _평균 정확도_를 계산합니다. TruthfulQA의 경우 텍스트-다빈치-003을 사용하여 점수를 매기기 위해 사람을 대체하는 기존 작업 [57]을 따른다. CrowS-Pairs와 WinoGender의 경우 LLaMA [57]의 실험 설정을 따라 복잡성과 상호참조 해결 점수를 계산한다. RealToxicityPrompts의 경우 독성 평가를 위해 Perspective-API47을 활용한다.\n' +
      '\n' +
      '각주 47: [https://perspectiveapi.com/](https://perspectiveapi.com/)\n' +
      '\n' +
      '\\(\\bullet\\)_Interaction with environment_. 이러한 능력을 테스트하기 위해 가정 및 전자 상거래 환경과 같은 실제 시나리오를 시뮬레이션하는 평가를 위해 ALFWorld[609] 및 WebShop[610]을 선택한다. 우리는 WebShop 및 ALFWorld 상에서 LLM의 1-샷 및 2-샷 성능을 각각 평가하는 ReAct [449]의 설정을 따르고, com\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Models**} & \\multicolumn{5}{c}{**Language Generation**} & \\multicolumn{5}{c}{**Knowledge Utilization**} \\\\ \\cline{2-10}  & LBD\\(\\uparrow\\) & WMT\\(\\uparrow\\) & XSum\\(\\uparrow\\) & HumanEval\\(\\uparrow\\) & TriviaQA\\(\\uparrow\\) & NaturalQ\\(\\uparrow\\) & WebQ\\(\\uparrow\\) & ARC\\(\\uparrow\\) & WikiFact\\(\\uparrow\\) \\\\ \\hline ChatGPT & 55.81 & **36.44** & 21.71 & 27.88 & **54.54** & 21.52 & 17.77 & 93.69 & 29.25 \\\\ Claude & **64.47** & 31.23 & 18.63 & 51.22 & 40.92 & 13.77 & 14.57 & 66.62 & **34.34** \\\\ Claude 2 & 45.20 & 12.93 & 19.13 & 27.04 & **54.30** & 21.30 & **21.06** & 79.97 & **35.83** \\\\ Davinci003 & **69.98** & **37.46** & 18.19 & 67.07 & 51.51 & 17.76 & 16.68 & 88.47 & 28.29 \\\\ Davinci002 & 58.85 & 35.11 & **19.15** & 56.70 & 52.11 & 20.47 & **18.45** & **89.23** & 29.15 \\\\ \\hline LLaMA 2-Chat (7B) & 56.12 & 12.62 & 16.00 & 11.59 & 38.93 & **12.96** & 11.32 & **72.35** & 23.37 \\\\ Vicuna (13B) & 62.45 & 20.49 & **17.87** & 20.73 & 29.04 & 10.75 & **11.52** & 20.69 & **28.76** \\\\ Vicuna (7B) & 63.90 & 19.95 & 13.59 & 12.07 & 28.58 & 9.17 & 6.64 & 16.96 & 26.95 \\\\ Alpaca (7B) & 63.35 & 21.52 & 8.74 & 13.41 & 17.14 & 3.24 & 3.00 & 49.75 & 26.05 \\\\ ChatGLM (6B) & 33.34 & 16.58 & 13.48 & 13.42 & 13.42 & 4.40 & 9.20 & 55.39 & 16.01 \\\\ \\hline LLaMA 2 (7B) & **66.39** & 11.57 & 11.57 & 17.07 & 30.92 & 5.15 & 2.51 & 24.16 & **28.06** \\\\ LLaMA (7B) & **67.68** & 13.84 & 8.77 & 15.24 & **34.62** & 7.92 & **11.12** & 4.88 & 19.78 \\\\ Falcon (7B) & 66.89 & 4.05 & 10.00 & 10.37 & 28.74 & **10.78** & 8.46 & 4.08 & 23.91 \\\\ Pythia (12B) & 61.19 & 5.43 & 8.87 & 14.63 & 15.73 & 1.99 & 4.72 & 11.66 & 20.57 \\\\ Pythia (7B) & 56.96 & 3.68 & 8.23 & 9.15 & 10.16 & 1.77 & 3.74 & 11.03 & 15.75 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Models**} & \\multicolumn{5}{c}{**Knowledge Reasoning**} & \\multicolumn{5}{c}{**Symbolic Reasoning**} & \\multicolumn{5}{c}{**Mathematical Reasoning**} & \\multicolumn{5}{c}{**Interaction with Environment**} \\\\ \\cline{2-10}  & ORQ\\(\\uparrow\\) & HellaSwag\\(\\uparrow\\) & SocialQAT\\(\\uparrow\\) & C-Objects\\(\\uparrow\\) & Penguins\\(\\uparrow\\) & GSM\\(\\uparrow\\) & MATH\\(\\uparrow\\) & ALFW\\(\\uparrow\\) & WebShop\\(\\uparrow\\) \\\\ \\hline ChatGPT & 81.20 & **61.43** & **73.23** & 53.20 & 40.27 & **78.47** & **33.28** & 58.96 & 45.12 /15.60 \\\\ Claude & 81.80 & 54.95 & **73.23** & 59.95 & 47.65 & 70.81 & 20.18 & 76.87 & **47.27 /22.00** \\\\ Claude 2 & 71.60 & 50.75 & 58.34 & **86.76** & **74.50** & **82.87** & **32.24** & **72.81** & 34.96 /19.20 \\\\ Davinci003 & 74.40 & **62.65** & 69.70 & **64.60** & 61.07 & 57.16 & 17.66 & 65.67 & **64.08 /82.00** \\\\ Davinci002 & _69.80_ & 47.81 & 57.01 & 62.55 & **67.11** & 49.96 & 14.28 & 76.87 & 29.66 /15.20 \\\\ \\hline LLaMA 2-Chat (7B) & **45.62** & **74.01** & 43.84 & 43.40 & 38.93 & 9.63 & 2.22 & **11.19** & **24.51**/5.60 \\\\ Vicuna (13B) & 43.65 & 70.51 & 45.97 & 53.55 & 36.91 & 18.50 & 3.72 & 8.96 & 22.74 /2.50 \\\\ Vicuna (7B) & 43.84 & 69.25 & 46.27 & **44.25** & 36.24 & **14.03** & 3.54 & 1.49 & 6.90 /1.40 \\\\ Alpaca (7B) & **47.82** & 69.81 & **47.58** & 39.35 & **40.27** & 4.93 & **4.16** & 4.48 & 0.00 /0.00 \\\\ ChatGLM (6B) & 30.42 & 29.27 & 33.18 & 14.05 & 14.09 & 3.41 & 1.10 & 0.00 & 0.00 /0.00 \\\\ \\hline LLaMA 2 (7B) & 44.81 & **74.25** & 41.72\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:71]\n' +
      '\n' +
      'pute _success rate_ for ALFWorld and _average score/success rate_ for WebShop. 또한, 입력 프롬프트의 길이를 줄이고 라인 브레이크를 EOS 토큰으로 활용하기 위해 ReAct [449]를 따른다.\n' +
      '\n' +
      '\\(\\bullet\\)_Tool manipulation._ 도구 조작을 위해 검색 엔진과 모델 인터페이스를 포함한 두 가지 도구를 고려한다. 따라서 우리는 두 가지 도구 조작 벤치마크인 HotpotQA[579]와 Gorilla[617]를 채택한다. HotpotQA는 웹에서 문서를 검색하기 위해 LLM이 검색 엔진을 사용하고, 토치허브, 텐서허브, 허그페이스의 세 허브에서 모델 API를 호출하기 위해 고릴라가 필요하다. HotpotQA의 경우 _정확한 일치_ 및 고릴라의 경우 _정확도_ 를 계산 합니다. HotpotQA의 경우 ReAct [449]를 따라 3-shot 성능을 보고합니다. 고릴라의 경우 논문 [617]에서 발표한 코드를 따르고 제로 샷 성능을 평가한다.\n' +
      '\n' +
      '**구현 세부 정보.** 각 작업 및 데이터 세트에 대해 기존 작업에서 제공 하는 동일한 프롬프트 및 결과 구문 분석 방법 (_i.e.,_ TruthfulQA, HotPotQA, Gorilla, HaluEval)을 사용 하 여 비교 된 LLM을 평가 합니다 (_i.e.,_ TriviaQA, Natural Questions, Web Questions, ARC, WikiFact, GSM8k, MATH, C-Objects, Penguins, LAMBADA, WMT22, XSum, HumanEval, CrowS-Pairs, WinoGender, RealToxicityPrompt). 특히, 폐쇄 소스 모델에 대한 모든 실험은 공식 API를 호출하는 것을 기반으로 하는 반면, 오픈 소스 모델의 경우 공개적으로 사용 가능한 코드 및 모델 매개변수를 활용하고 8개의 A800-80G GPU에서 추론을 수행한다. 트리비아QA, OpenbookQA, HellaSwag, SocialIQA의 경우 테스트 세트가 공개되지 않았기 때문에 개발 세트에 대해 실험한다. 다른 데이터 세트의 경우 테스트 세트에 대해 실험합니다. 실험을 재현 하기 위해 [https://github.com/RUCAIBox/LLMSurvey/tree/main/Experiments](https://github.com/RUCAIBox/LLMSurvey/tree/main/Experiments)에서 실험 코드 및 데이터를 공개적으로 공개 합니다.\n' +
      '\n' +
      '#### 7.4.2 결과 분석 및 결과\n' +
      '\n' +
      '실험 결과를 Table X에 보고하고, 그 결과를 다음과 같이 분석한다.\n' +
      '\n' +
      '**Closed-Source 모델의 분석.** 4가지 폐쇄 소스 모델(_i.e.,_ ChatGPT, Claude, Davinci003 및 Davinci002)의 분석 및 결과를 다음과 같이 요약합니다.\n' +
      '\n' +
      '\\(\\bullet\\)_이 5개의 폐쇄 소스 모델은 ChatGPT가 대부분 가장 좋은 성능을 보이는 범용 태스크 해결기로 유망한 결과를 달성한다. ChatGPT, Claude, Claude 2, Davinci003 및 Davinci002는 범용 태스크 해결자가 될 가능성이 큰 복합 태스크(_e.,_GSM8k)를 포함한 대부분의 태스크에서 잘 수행된다. 이 중 ChatGPT는 평가 과제에서 보다 우수한 모델 역량을 발휘하여 모든 과제에서 가장 많은 승리를 거두었다. 일부 평가 태스크들에서, ChatGPT와 다른 폐쇄-소스 모델들 사이의 성능 갭은 매우 크며, 특히 복잡 태스크들 _예를 들어, _78.47 (ChatGPT) _vs._ 에 대해 더욱 그러하다. 49.96(Davinci002) on GSM8k, and 79.88(ChatGPT) _vs._ 51.22(Claude) on HumanEval.\n' +
      '\n' +
      '\\(\\bullet\\)_Claude 2, ChatGPT 및 Davinci003은 환경 및 도구 조작 작업과의 상호 작용에서 더 나은 성능을 보인다. 두 평가 과제인 Claude 2, ChatGPT 및 Davinci003은 큰 마진(예:_36.40(Claude 2)_vs._ 26.00(Davinci002) on HotpotQA, 44.53(ChatGPT) _vs._ 7.74(Claude) on Gorilla-TF, and 72.58(Davinci003) _vs._ 22.04 (Claude) on Gorilla-TH. 가능한 이유는 이 세 가지 모델이 외부 플러그인의 사용을 지원하는 이러한 고급 능력을 위해 특별히 최적화되었기 때문이다.\n' +
      '\n' +
      '\\(\\bullet\\)_모든 비교 모델은 매우 어려운 추론 작업에 대해 잘 수행되지 않는다. MATH와 HotpotQA에서는 모든 모델( ChatGPT 포함)이 잘 작동하지 않는다. 두 과제는 복잡한 수학적 지식에 대한 정확한 이해가 필요하고 문서 전반에 걸쳐 멀티홉 추론을 각각 수행해야 하는 매우 어려운 과제이다. 또한, 이들 모델들은 또한 기계 번역 태스크(WMT)에 대해 상대적으로 약한 성능을 갖는다. 가능한 이유는 WMT가 마이너 언어의 많은 평가 예도 포함하고 있기 때문에 이러한 LLM의 사전 훈련 데이터에서 잘 다루지 않을 수 있다.\n' +
      '\n' +
      '**오픈 소스 모델 분석.** 다음으로 8개의 오픈 소스 모델 (_i.,_ LLaMA 2-Chat, Vicuna, Alpaca, ChatGLM, LLaMA 2, LLaMA, Pythia and Falcon)에 대 한 분석 및 결과를 다음과 같이 계속 보여 줍니다.\n' +
      '\n' +
      '_(\\bullet\\)_명령 조정 모델은 대부분 기본 모델보다 성능이 우수합니다._ 비교된 모든 오픈 소스 방법들 중에서, 명령-튜닝된 모델들(_i.e.,_ LLaMA 2-Chat, Vicuna, Alpaca and ChatGLM)은 대부분 명령-튜닝되지 않은 모델들(_i.e.,_ LLaMA 2, LLaMA, Pythia and Falcon)보다 더 나은 성능을 보인다. 이는 명령어 튜닝이 일반적으로 다양한 태스크를 해결하는 데 LLM의 적은 샷 또는 제로 샷 능력을 향상시킬 수 있음을 나타낸다. 그러나, 명령어 튜닝 후에, Vicuna(7B) 및 Alpaca(7B)는 언어 모델링 작업인 LAMBADA 상에서 성능 저하를 겪는다. 그 이유는 명령어 데이터가 주로 LLM이 인간의 명령어를 따를 수 있게 하는 데 초점을 맞추고 있기 때문일 수 있으며, 이는 일반적인 언어 생성 작업에 항상 유용한 것은 아니다.\n' +
      '\n' +
      '이러한 소규모 오픈소스 모델들은 수학적 추론, 환경과의 상호작용, 도구 조작 작업 등에서 잘 수행되지 않는다. 수학적 추론, 환경과의 상호 작용 및 도구 조작의 작업에서 이러한 평가된 모든 오픈 소스 모델은 명령 조정 모델을 포함하여 잘 수행되지 않는다. 가능한 이유는 이러한 모델을 미세 조정하기 위한 명령 데이터가 이러한 작업에 대해 특별히 설계되지 않았기 때문이다. 또한, 이러한 폐쇄 소스 모델은 작은 모델 크기로 인해 제한된 모델 용량을 가질 수 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_최상위 수행 모델은 인간 정렬 작업에 따라 달라진다._ 상이한 인간 정렬 작업들에 대해, 우리는 이러한 모델들이 일관성 없는 성능 순위를 달성한다는 것을 알 수 있다. 예를 들어, LLaMA 2-Chat(7B)는 TruthfulQA 상에서 비교된 오픈 소스 모델들 중에서 가장 우수한 성능을 수행하는 반면, Vicuna(13B)는 CrowS-Pairs 상에서 가장 우수한 성능을 수행한다. 가능한 이유는 이러한 태스크들이 인간 정렬의 상이한 양태들을 평가하기 위한 특정 목적들로 설계되고, 이러한 모델들은 동일한 모델(_예를 들어,_피티아(7B) 및 피티아(12B))의 변형들에 대해서도 상이한 태스크들에서 다양한 성능을 나타내기 때문이다. 더 자세한 결과를 밝히기 위해서는 인간 정렬 평가에 대한 더 많은 실험과 분석이 필요하다.\n' +
      '\n' +
      '보다 최근에 발표된 모델로서 LLaMA 2 (7B)는 특히 복잡한 추론 태스크에서 좋은 성능을 보인다. 복잡한 추론 태스크의 경우, LLaMA 2(7B)는 대부분 다른 기본 모델들(예를 들어, _예:_ 43.95(LLaMA 2(7B)) _vs._보다 우수한 성능을 보인다. 29.80 (Falcon(7B)) in C-Objects. 다른 태스크들(_e.g._, 언어 생성 및 지식 활용)의 경우, LLaMA 2(7B)는 또한 최상의 성능 베이스 모델로서 비교 가능한 성능을 달성할 수 있다. 사전 학습(_i.e._, 약 2조 토큰)에 더 많은 데이터를 사용했으며, 이는 주로 우수한 성능에 기여한다. 또한 보다 강력한 데이터 클리닝 프로세스를 수행합니다.\n' +
      '\n' +
      '각주 48: 우리는 모든 관련 연구 방향이나 영역을 다루는 것을 목표로 하지 않고 대신 이러한 선택된 예를 통해 LLM의 사용 또는 영향을 입증하는 것을 목표로 한다.\n' +
      '\n' +
      '\\(\\bullet\\)_오픈 소스 모드를 확장하면 성능을 일관되게 향상시킬 수 있습니다._ Vicuna (7B)와 Vicuna (13B), Pythia (7B)와 Pythia (13B)의 성능을 비교한 결과, 규모가 큰 모델이 작은 모델보다 대부분 더 나은 성능을 보여 모델 크기를 확장하는 효과가 있음을 알 수 있다. 상이한 태스크들에 걸쳐, 스케일링 모델은 더 복잡한 태스크들(_e.g._, 심볼릭 및 수학적 추론)에 더 유익하며, 여기서 더 큰 모델들은 대부분 큰 마진에서 더 작은 모델들보다 더 우수하다.\n' +
      '\n' +
      '독자들은 오픈 소스 언어 모델에 대한 이러한 발견이 모델 크기에 국한된다는 점에 유의해야 한다. 우리는 이러한 모델의 더 큰 버전의 결과를 포함하여 이 부분을 지속적으로 업데이트하고 더 많은 실험을 위한 계산 자원의 지원을 요청할 것이다.\n' +
      '\n' +
      '## 8 Applications\n' +
      '\n' +
      '이 섹션에서는 연구 커뮤니티 및 대표 도메인에 대한 영향이라는 두 가지 측면에서 LLM의 적용에 대한 최근 진행 상황을 간략하게 검토한다. 도 18은 이 섹션49의 콘텐츠 조직을 도시한다.\n' +
      '\n' +
      '각주 49: 우리는 모든 관련 연구 방향이나 영역을 다루는 것을 목표로 하지 않고 대신 이러한 선택된 예를 통해 LLM의 사용 또는 영향을 입증하는 것을 목표로 한다.\n' +
      '\n' +
      '### _LLM for Research Community_\n' +
      '\n' +
      'LLM이 AI 알고리즘을 개발하는 방식에 혁명을 일으켰기 때문에 연구 커뮤니티에 상당한 영향을 미친다. 이 부분에서 우리는 몇 가지 대표적인 연구 방향에 대해 LLM이 주도하는 발전을 간략하게 검토한다.\n' +
      '\n' +
      '#### 8.1.1 LLM for Classic NLP Tasks\n' +
      '\n' +
      '사전 학습된 언어 모델(_e.g._, BERT)이 NLP 분야에서 시작됨에 따라 언어 모델의 기술적 발전은 NLP 연구에 중요한 영향을 미친다. 이 부분에서는 기존의 많은 NLP 시스템 및 응용의 기초가 되었던 단어 수준, 문장 수준, 시퀀스 태깅, 관계 추출, 텍스트 생성 태스크 등 5가지 고전적인 NLP 태스크에 LLM을 적용하는 것에 대해 논의한다. 우리는 모든 NLP 작업을 포괄적으로 다루려는 것이 아니라 기본 작업을 통해 기본 NLP 연구를 위한 LLM의 영향을 분석하려고 노력한다는 점에 유의한다. 우리는 또한 이 조사에서 일찍이 논의된 몇 가지 작업(예: 언어 모델링)에 대한 논의를 생략한다.\n' +
      '\n' +
      '**단어/문장 수준 작업.** 오랜 NLP 작업으로 단어 수준(_e.g._, 단어 군집화 [748] 및 의미 명확화 [749]) 및 문장 수준 작업(문장 일치 [750] 및 감정 분류 [751])이 문헌에서 널리 연구되어 실제 플랫폼에서 적용되었습니다. 이러한 과제를 해결하기 위해서는 단어나 문장에 대한 의미 정보를 정확하게 이해하는 것이 관건이다. 지금까지 이러한 작업에 대한 풍부한 고품질 라벨링된 데이터가 축적됨에 따라, 기존 작업[23, 39]은 작은 언어 모델이 미세 조정함으로써 매우 우수한 성능을 달성할 수 있다는 것을 발견한다. 최근의 연구[752, 55]는 또한 이러한 태스크들에 대한 LLM들의 성능을 테스트했는데, 이는 LLM들이 또한 (매우 적은 예들로) 인-컨텍스트 학습을 통해 잘 수행할 수 있음을 보여준다. 반면, 작은 모델들이 특정 태스크 요건 및 도메인 지식을 학습하기 위해 이러한 태스크들에 특별히 최적화될 수 있기 때문에, 풀-데이터 미세 조정된 작은 모델들은 여러 고전적인 태스크들 [753, 754], _예를 들어_, 시맨틱 매칭 및 감성 분석에 대해 인-컨텍스트 학습을 사용하여 LLM들을 대부분 능가할 수 있다.\n' +
      '\n' +
      '**시퀀스 태깅.** 시퀀스 태깅 작업 _예:_, 명명된 엔터티 인식(NER)[755] 및 POS(품사) 태깅 [756]도 기본 작업입니다. 전형적으로, 이러한 태스크들은 NER 태스크들에 대한 고전적인 B-I-O(_Beginning, Inside_ 및 _Outside_) 태깅 스킴인, 적절한 의미 범주 라벨, _예._ 를 입력 시퀀스 내의 각각의 토큰에 할당하는 것을 요구한다. 딥 러닝 시대에, 초기 노력[757, 758]은 주로 학습된 시퀀스 표현들(_e.g._, CNN, LSTM, 및 BERT를 사용하여)을 구조적 예측에 기초하여 태깅 작업을 수행하는 고전 조건부 랜덤 필드 모델(CRF)에 통합한다. 최근에, 연구자들은 시퀀스 태깅 태스크에서 LLM의 성능을 테스트했지만, 특히 모호하거나 희귀한 이름, _예를 들어_, "MISC"(_miscellaneous entity_) 및 "ORG"(_organization_) 클래스를 갖는 특수 카테고리에 대해, 인-컨텍스트 학습(753)을 사용하여 LLM을 해결하는 데 여전히 어려움을 겪는다는 것을 관찰했다. 가능한 이유는 LLM이 인간 주석이 달린 데이터 세트에서 이러한 클래스의 의미를 오해하여 컨텍스트에서 명령어와 제한된 예제에 따라 그들의 의미를 정확하게 이해하기 어려울 수 있기 때문이다.\n' +
      '\n' +
      '**정보 추출.** 정보 추출 작업은 관계 추출 [759] 및 이벤트 추출 [760]과 같은 비정형 텍스트 데이터에서 유용한 구조적 정보를 자동으로 추출하는 데 중점을 둡니다. 일반적으로 선행 연구들은 이 작업을 텍스트 분류 작업 또는 순차적 레이블링 작업으로 공식화한다. 정보 추출이 종종 복잡한 의미 관계(한 문장 내의 다중 관계)를 정확하게 이해하고 처리할 필요가 있기 때문에, LLM들을 사용한 인-컨텍스트 학습은 전형적으로 최신의 풀-데이터 미세 조정 방법들을 과소 수행한다[761, 762]. 반면, LLM들과 소형 모델들 간의 협업을 가능하게 하는 것은 특정 태스크들의 성능을 더욱 향상시킬 수 있다는 것이 보여진다[763, 762]. 또한, 최근 연구 [425]는 LLM이 2단계 워크플로우를 사용하여 정보 추출을 위한 경쟁력 있는 제로 샷 성능을 달성할 수 있음을 밝혀 향후 응용 분야에서 이 접근 방식을 매력적으로 만든다.\n' +
      '\n' +
      '**텍스트 생성.** 텍스트 생성 작업 _예:_, 기계 번역 [624] 및 자동 요약 [548]은 널리 연구 된 오랜 NLP 작업이며 미세 조정 소형 모델을 기반으로 하는 배포 된 많은 제품 및 시스템이 있습니다. LLM의 사전 훈련은 텍스트 예측에 대해 확립되어 있기 때문에, 적절한 프롬프트의 도움으로 상용 제품[627] 및 인간[628]으로서 강력한 언어 생성 능력을 나타낸다[765, 766]. 추가적으로, LLM은 실제 애플리케이션 시나리오들, _예를 들어_, 문서-레벨 번역에서 특별한 요구 사항을 효과적으로 처리하도록 유연하고, 또한 생성 품질을 더욱 향상시키기 위해 사용자들과의 자연 언어 상호작용을 가능하게 한다[768]. 상기 성공에도 불구하고, 최근의 작업은 LLM들이 상이한 언어들에 걸친 불균형 트레이닝 데이터들 때문에, 낮은 자원 언어들 및 도메인들, _예를 들어_, 마라티-영어 번역에 관한 생성 작업들을 잘 다루기 어렵다는 것을 또한 드러낸다.\n' +
      '\n' +
      '**요약**. 이상의 논의를 바탕으로 고전적 NLP 작업에서 LLMs 사용에 대한 제안과 향후 방향을 다음과 같이 요약한다.\n' +
      '\n' +
      '* _제안_: LLM 및 소형 모델은 다양한 측면에서 고유한 장점이 있습니다. LLM은 다양한 NLP 작업에 통합 솔루션을 제공하고 경쟁적 성능(특히 제로/퓨샷 설정)을 달성할 수 있는 반면 소형 모델은 개발하기에 경제적이며 대상 작업에 따라 특별히 조정될 수 있으며, 이는 충분한 고품질 레이블 데이터 [770, 771, 754, 770, 771]로 우수한 성능을 달성할 수 있습니다. 애플리케이션에서는 유연성, 데이터 가용성, 훈련 컴퓨팅 및 효율성을 종합적으로 고려하여 실제 요구에 따라 적합한 선택을 할 수 있다.\n' +
      '* _미래 방향:_ 일반 용량이 우수함에도 불구하고 LLM은 여전히 낮은 리소스 도메인, _예:_ 마이너 언어 번역에서 NLP 작업을 효과적으로 처리할 수 없습니다. 이러한 작업을 해결하기 위해서는 미세 조정 또는 프롬프트를 통해 필요한 작업 정보 또는 도메인별 지식을 LLM에 주입하는 효과적인 접근 방식을 개발해야 한다. 게다가, LLM들이 고전적인 NLP 태스크들(_e.g._, 중첩된 엔티티 추출)에서 복잡한 의미 관계들을 처리하는 것은 여전히 어려우며, 이는 LLM들의 기본 작업 메커니즘으로부터 더 많은 탐색을 할 가치가 있다. 또한 고전적인 NLP 태스크의 복잡한 사례를 해결하는 데 있어 서로 보완하기 위해 LLM과 미세 조정된 작은 언어 모델을 결합하는 것이 유망하다[772]. LLM이 인간의 명령을 효과적으로 이해하고 의미 있는 응답을 할 수 있기 때문에, 다른 유망한 방향은 NLP 태스크에 대한 인간-기계 협력 연구(_e.g._, 대화 번역[768])를 수행하는 것이다.\n' +
      '\n' +
      '#### 8.1.2 LLM for Information Retrieval\n' +
      '\n' +
      '정보 검색 시스템의 목표는 사용자가 이상적인 정보 자원(일반적으로 문서)을 발견하고 정보 과부하 문제를 완화하는 것이다. 전형적으로, 현대 IR 시스템들은 검색-then-rerank 파이프라인 프레임워크를 채택한다[54]. 이 프레임워크 내에서, 리트리버는 초기에 대규모 코퍼스로부터 관련 정보를 검색하고, 리랭커는 후속적으로 가장 관련 있는 정보를 획득하기 위해 다단계 랭킹 절차를 수행한다[773]. LLM의 출현은 정보 접근 방식에 상당한 영향을 미치기 때문에, 우리는 IR 모델로서의 LLM과 LLM 강화 IR 모델로서의 LLM의 두 가지 주요 측면에서 IR의 발전을 어떻게 발전시켰는지 논의한다.\n' +
      '\n' +
      '**IR 모델로서의 LMM.** 기존 IR 모델은 일반적으로 _sparse 모델_ (용어 기반 어휘 유사성에 의존) 및 _dense 모델_ (임베딩 기반 의미 유사성에 의존) [740]으로 분류할 수 있습니다. 특히, 조밀 모델은 주로 미세 조정된 PLM(_e.g._, BERT)에 의해 구현된다. PLM에 비해 LLM은 텍스트 시맨틱을 캡처하는 데 더 강력한 모델 용량을 가지고 있으므로 기존의 조밀한 IR 모델을 개선할 가능성이 있다. 그러나 LLM의 높은 오버헤드로 인해 대부분의 연구는 검색된 후보의 순위를 세분화하는 것을 목표로 LLM을 랭커로 사용하는 데 중점을 둔다. 이를 달성하기 위해, 최근의 노력들은 종종 LLM들이 제공된 후보 문서들의 작은 세트에 대해 리랭킹(reranking)을 수행할 수 있게 하는 특별한 명령어들을 공식화한다. 전형적으로, 이러한 접근법은 모델 훈련을 필요로 하지 않으며, 잘 훈련된 재순위화 방법들과 비교하여 유망한 결과들을 달성한다[774, 775]. 특히, LLM 기반 리랭킹 접근법은 포인트 와이즈(쿼리-문서 쌍에 대한 관련성 스코어 추정_)를 포함하는 제로 샷 또는 소수의 샷 명령에 의해 상이한 방식으로 구현될 수 있다[776], 페어 와이즈(연관성 순서 결정_\n' +
      '\n' +
      '도. 도 18: 대표적인 연구 방향 및 다운스트림 도메인에서 LLM의 적용.\n' +
      '\n' +
      '(_sorting a subset of candidate documents_)[775], 또는 Listwise ranking (_sorting a subset of candidate documents_)[777]. 이러한 방법의 본질은 문서 목록에 대한 슬라이딩 윈도우 전략[774, 778], 검색 선택 프롬프트[779], 세밀한 관련성 라벨 통합[780], 쌍별 비교 프롬프트[775]와 같은 텍스트 리랭킹에 대한 명령어의 특별한 설계에 있다. 또한, 최근의 노력은 소수의 샷 데모를 사용하여 검색 결과로서 중간 텍스트(_e.g._, URL)를 생성하기 위해 LLM을 채용한다[781]. 모델 성능을 더욱 향상시키기 위해, LLM은 전통적인 PLM 기반 IR 모델 [782]에 대한 미세 조정 프로세스와 유사하게, 리랭킹[782, 783] 또는 검색(밀집 검색[54] 및 모델 기반 검색[784, 785] 포함)을 위한 백본으로서 특별히 미세 조정될 수 있다. 그러나 IR 모델로서 LLM을 미세 조정하는 것은 LLM의 거대한 매개변수 규모를 고려할 때 상당한 비용을 수반한다.\n' +
      '\n' +
      '**LLM-Enhanced IR 모델.** 또 다른 주요 연구 방향으로서 LLM을 사용하여 기존 IR 모델(_e.g._, 소형 모델)을 개선할 수 있습니다. 기존 IR 모델이 직면한 일반적인 과제는 관련 판단 주석의 부족이다[786, 787]. 이 문제를 해결하기 위해, LLM은 주어진 질의에 대해 긍정 또는 부정 문서에 주석을 달거나[788], 또는 몇 가지 데모를 참조하여 코퍼스의 문서 세트에 기초하여 대응하는 질의를 생성하도록 지시받을 수 있다[789, 790]. LLM은 학습 데이터 증강 외에도 쿼리 및 문서 모두의 검색 지향 정보성을 개선하여 기존 IR 모델을 개선할 수 있는 잠재력을 가지고 있다. IR 시스템에서 입력 쿼리는 사용자의 인지 및 문화적 역량에 의해 제한될 수 있어 실제 의도를 정확하게 표현하기 어려울 수 있으며 문서에 존재하는 관련 없는 콘텐츠도 쿼리와의 관련성 평가에 영향을 미칠 수 있다. 해결 방안으로 LLM은 잘 설계된 명령어를 통해 질의 의도에 대한 이해를 높이고 질의에 추가 지식을 통합하기 위해 질의를 재작성하는 데 활용될 수 있다. 재작성된 쿼리는 원래의 쿼리[791], 쿼리와 관련된 코퍼스 내의 문서, 또는 의사 생성 문서와 연결된 쿼리의 확장 등의 형태를 취할 수 있다[793]. 또한, 컨텍스트 확장을 위해 LLM들을 사용하여 원본 문서들에 기초하여 생성되는 쿼리들로 문서들도 확장될 수 있다[794].\n' +
      '\n' +
      '**남은 문제** 이 부분에서는 IR 시스템을 개선하기 위해 LLM을 적용하는 몇 가지 중요한 문제에 대해 추가로 설명합니다. 첫째, LLM은 범용 태스크 해결자로서 기능할 수 있지만, 기존의 IR 시스템에 직접적으로 적합하지 않다: 추론을 위해 높은 오버헤드가 필요하고, 긴 텍스트 또는 문서 목록을 모델링하는 데 한계가 있고, 텍스트 랭킹 태스크를 수행하기 위해 특별한 적응(_e.g._, 명령어 튜닝)이 필요하다[795]. 따라서 현대 IR 시스템에 LLM을 적용하기 위한 보다 체계적인 접근법을 조사하여 이점을 활용하고 이러한 한계를 극복해야 한다. 둘째, LLM의 출현은 새로운 정보 탐색 방법(_e.g._, New Bing)의 개발에 빛을 발한다. LLM들의 용량 및 기존 IR 시스템들의 장점들을 통합함으로써 IR의 아키텍처 및 패러다임을 재구성하는 방법을 탐색하는 것은 의미가 있다[796]. 셋째, 기존 연구는 텍스트 검색에 중점을 두고 있으며, 멀티모달 정보원에 대한 종합적인 고려가 부족하다. 섹션 8.1.4에서 논의될 바와 같이, 멀티모달 대형 언어 모델 [797]도 널리 연구되어, 보다 강력한 멀티미디어 검색 시스템을 개발할 수 있게 한다.\n' +
      '\n' +
      '#### 8.1.3 LLM for Recommender Systems\n' +
      '\n' +
      '관련 문서를 검색하기 위해 사용자 검색 질의를 분석하는 IR 시스템과 달리, 추천기 시스템(RS)은 기본 사용자 선호도를 캡처하고 사용자에게 적절한 정보 리소스를 제공하는 것을 목표로 한다[798, 799, 800, 801]. 전형적으로, 기존 연구들은 사용자의 로그된 데이터(_e.g._, 클릭 데이터) 위에 피팅함으로써 추천 모델(클래식 또는 딥 러닝 모델 중 하나)을 트레이닝한다[745, 802]. 그러나 이러한 모델은 종종 _예:_, cold-start 권장 사항, 도메인 전송 및 설명력 부족과 같은 일련의 기술적 문제로 인해 어려움을 겪습니다. 최근 LLM은 도메인 일반화 및 언어 생성의 강력한 능력으로 인해 권장 모델 [803, 357, 804]의 이러한 문제를 완화할 수 있는 잠재력을 보여주었다. 이 부분에서는 추천 시스템에서의 최근 LLM의 발전 과정을 추천 모델로서의 LLM, LLM 강화 추천 모델 및 추천 시뮬레이터로서의 LLM의 세 가지 측면에서 간략하게 검토한다.\n' +
      '\n' +
      '**추천 모델로서의 LMM.** 특정 방법 또는 메커니즘을 사용 하 여 LMM을 권장 모델 역할을 하도록 조정할 수 있습니다. 이 선에 따른 기존 작업은 일반적으로 크게 두 가지로 나눌 수 있다. 먼저, 몇몇 방법들은 제로 샷 패러다임(_i.e._, 파라미터 튜닝 없이)에서 추천 태스크를 완료하기 위한 LLM들을 프롬프트한다[805, 806]. 잠재적인 모델 편향을 완화시킬 뿐만 아니라 추천 성능을 향상시키기 위해 최신 중심 및 문맥 내 학습과 같은 일련의 신속한 엔지니어링 방법이 도입된다[807, 808]. 둘째, 또 다른 범주의 연구는 명령어 튜닝을 통해 개인화된 추천을 위한 LLM을 전문화하는 것을 목표로 한다[357, 809]. 특히, 휴리스틱 템플릿과의 사용자-항목 상호작용을 기반으로 구성된 추천 태스크에 LLM을 적용하기 위해서는 고품질의 명령 데이터가 중요하다. 명령어 다양성을 더욱 향상시키기 위해, InstructRec[357]은 제품 검색 및 개인화된 추천과 같은 다양한 시나리오에서 많은 양의 잠재적 사용자 명령을 시뮬레이션하기 위해 자체-지시 기법을 채용한다. 또한, 추천 시스템[810, 811]에서 LLM의 어휘를 의미 식별자와 함께 확장하여 협업 시맨틱을 LLM에 통합하는 것에 대한 관심이 높아지고 있다.\n' +
      '\n' +
      '**LLM 강화 추천 모델.** LLM에 권장 사항을 직접 제공 하도록 지시 하는 것 외에도 연구자들은 전통적인 추천 시스템을 개선하기 위해 LLM에 인코딩 된 범용 지식을 활용 하는 것도 제안 합니다. 이 줄의 기존 접근 방식은 크게 세 가지로 나눌 수 있다. 첫 번째 범주는 과거의 상호 작용 데이터로부터 사용자의 잠재적 의도를 추론하기 위해 LLM을 사용한다. 게다가, 전통적인 추천/검색 모델들은 관련 항목들의 검색을 개선하기 위해 추론된 의도들을 채용한다[812, 813]. 또한 여러 연구에서는 LLM을 기능 인코더로 사용하는 방법을 탐구한다. 그들은 항목 및 사용자의 부가 정보(_e.g._, 항목의 설명 및 사용자의 리뷰)를 인코딩하기 위해 LLM을 채용하여, 사용자 및 항목에 대한 보다 유익한 표현을 유도한다. 이러한 표현들은 이어서 증강된 입력으로서 전통적인 추천기 시스템들로 공급된다[814, 815]. 다른 대안적 접근법으로서, 몇몇 연구[816, 817]는 전통적인 추천자(_i.e._, 소형 모델)를 개선하기 위해 LLM의 용량(_e.g._, 의미 인코딩)을 전달하는 증류-유사 방식을 채택한다. 특히, 이들은 공동 학습을 통해 LLM과 전통적인 추천 모델의 숨겨진 상태를 정렬한다. 교육 후, 강화된 소형 모델만 온라인으로 배포되기 때문에 온라인 서비스에서 LLM의 막대한 오버헤드를 피할 수 있다.\n' +
      '\n' +
      '**추천 시뮬레이터로서의 LLM.** 자율 AI 에이전트 [818]의 최근 성공에 힘입어 LLM은 추천 시뮬레이터 [820, 819](RecAgent [819]에 의해 예시됨)를 개발하는 데에도 활용되어 추천 시스템 [821, 822, 819]에서 사용자 실제 행동을 시뮬레이션할 수 있는 큰 잠재력을 보여줍니다. 구체적으로, 개인화된 시뮬레이션을 하기 위해, 에이전트는 관련 신원 정보를 포괄하는 프로파일링 모듈을 장착할 것이다. 그런 다음 메모리 모듈을 도입하여 에이전트의 과거 상호 작용 경험을 저장한다. 시뮬레이션의 프로세스 동안, 에이전트들은 그들의 기본 사용자 선호도를 포착하기 위해, 그들의 과거 경험에 기초하여 자기-반성을 수행하도록 추가로 프롬프트된다. 대부분의 기존 추천 시뮬레이터는 상호작용 과정에서 항목을 명시적으로 모델링하지 않고 사용자 중심으로 진행된다. 이를 해결하기 위해, 에이전트CF[821]는 사용자와 아이템 모두를 에이전트로 모델링하고, 사용자-아이템 상호작용을 시뮬레이션하기 위한 협력적 성찰을 더욱 용이하게 하여 사용자와 아이템 사이의 양면 관계를 포착한다.\n' +
      '\n' +
      '**남은 문제** 이러한 노력에도 불구하고 추천 시스템에서 LLM을 적용할 때 해결해야 할 몇 가지 문제가 있습니다. 첫째, 기존 연구에서는 제로/퓨샷 환경에서 LLM 기반 추천 모델이 기존 ID 기반 추천 모델보다 성능이 떨어지는 경향이 있음을 보여주었다[806, 807]. 이는 LLM이 개인화된 사용자 행동과 도메인별 협력 의미에 대한 이해가 부족할 수 있음을 나타낸다. 명령어 튜닝이 이 문제를 어느 정도 완화시키기는 하지만[809, 357], LLM과 추천 시스템 사이의 의미적 갭을 완전히 감소시킬 수 없으며, 또한 높은 튜닝 비용을 겪는다. 또한, 추천 시스템은 낮은 자원 환경(예: 전화기)에서 사용자의 경험을 향상시키기 위해 추론 지연을 최소화하는 것을 우선시하며, 이는 LLM의 추론 속도 및 메모리 오버헤드에 문제를 제기한다. 따라서 실제 추천 시스템에서 LLM을 효율적이고 효과적으로 배치하기 위해서는 효율적인 튜닝 및 양자화 방법과 같은 개선 기법을 탐색하는 것이 중요하다. 또한 기존의 LLM은 긴 컨텍스트 모델링에서 제한된 용량을 가지고 있어 방대한 양의 사용자-아이템 상호작용 데이터를 처리하기 어렵다. 장시간 인터렉션 시퀀스에서 LLM의 모델링 능력을 향상시키기 위해 개선된 컨텍스트 길이 확장 및 컨텍스트 정보 활용 접근법이 개발되어야 한다.\n' +
      '\n' +
      '#### 8.1.4 Multimodal Large Language Model\n' +
      '\n' +
      '기존 문헌[823, 824]에서 멀티모달 모델들은 주로 입력으로부터 다양한 모달리티들(_e.g._, 텍스트, 이미지 및 오디오)의 정보를 처리하고 통합할 수 있고, 나아가 특정 모달리티들에서 대응하는 출력을 생성할 수 있는 모델들을 지칭한다. 이 부분에서는 비텍스트 모달리티, 특히 비전 모달리티의 정보 모델링을 가능하게 함으로써 LLM의 멀티모달 확장에 주로 초점을 맞춘다. 논의를 시작하기 위해 텍스트-이미지 쌍으로 입력을 지정하고 텍스트 응답으로 출력을 지정한다. 다른 양식, 예를 들어 언어 오디오 모델[825]에 대해서도 유사한 논의가 이루어질 수 있으며, 이는 여기에서 우리의 범위를 벗어난다. 본질적으로 MLLM은 세계 텍스트를 기반으로 학습되는 LLM의 우수한 모델 용량을 활용하기 위해 다른 모달리티의 정보를 텍스트 모달리티에 적용하여 개발한다. 전형적으로, MLLM은 비전 및 언어 표현을 정렬하는 연결 모듈에 의해 연관된, 이미지 인코딩을 위한 이미지 인코더 및 텍스트 생성을 위한 LLM을 포함한다. 생성 동안, 이미지는 먼저 패치들로 분할되고, 이어서 이미지 인코더 및 연결 모듈에 의해 패치 임베딩들로 변환되어, LLM에 의해 이해될 수 있는 시각적 표현을 도출한다. 이어서, 패치 임베딩과 텍스트 임베딩을 연결하고, MLLM에 피드백하여 언어 모델이 응답을 자동으로 생성할 수 있도록 한다. 이하에서는 역량 있는 MLLM 개발을 위한 교육, 평가 및 핵심 사항에 대해 논의하고자 한다.\n' +
      '\n' +
      '각주 49: 기존 작업에서, 대형 비전 언어 모델(LVLMs)[662]도 LLMs를 기반으로 개발된 이러한 바이모달 모델을 용어화하기 위해 사용된다. 우리는 기존 문헌에서 널리 사용되기 때문에 이 부분에서 MLLM의 이름을 사용한다.\n' +
      '\n' +
      '**훈련 프로세스.** MLLM의 훈련 프로세스에는 비전 언어 정렬 사전 훈련 및 시각적 명령 튜닝의 두 가지 주요 단계가 포함됩니다.\n' +
      '\n' +
      '* _Vision-language alignment pre-training._ MLLM을 개발하기 위해, 기존 작업은 대부분 비전 인코더 및 LLM을 사전 트레이닝된 모델들로 초기화한다[149, 826, 150]. 이러한 모델은 뛰어난 비전과 언어 능력을 유지하지만 다양한 의미 공간에 걸쳐 있다. 따라서, 비전-언어 정렬 사전-트레이닝(_i.e._, 첫 번째-단계 트레이닝)의 목표는 대규모 이미지-텍스트 쌍들에 대한 종단간 트레이닝을 통해 비전 인코더와 LLM을 정렬하는 것이다[827, 828]. 그러나 이 두 모델을 이미지-텍스트 쌍에 직접 튜닝하면 원래 표현 능력이 저하될 수 있다. 정렬 성능을 향상시키기 위해서는 효과적인 훈련 전략을 설계하고 적절한 사전 훈련 데이터를 선택하는 것이 중요하다[829, 830]. 기존의 작업은 주로 교차 모달리티 정렬을 위해 다음과 같은 전략을 사용한다: (1) 이미지-텍스트 쌍의 수가 충분히 크지 않은 경우(_e.g._, 1M 미만인 경우), 종종 연결 모듈만 업데이트하는 것이 제안된다[831]. (2) 트레이닝 데이터가 고품질 텍스트 말뭉치[832] 또는 세밀한 주석이 있는 이미지-텍스트 쌍을 포함하는 경우(833), LLM을 세밀하게 조정하여 성능을 높일 수 있다; (3) 이미지-텍스트 쌍의 수가 매우 큰 경우(_e.g._, 약 1B) 비전 인코더를 세밀하게 조정하는 것도 그럴듯하지만, 이점은 추가 검증으로 남아 있다.\n' +
      '* _Visual instruction tuning._ 비전 언어 사전 훈련 후, 2단계 훈련인 _i.e._는 MLLM의 명령어 추종 및 과제 해결 능력을 향상시키는 것을 목표로 한다. 일반적으로 시각 명령 튜닝의 입력은 이미지와 태스크 설명으로 이루어지며, 태스크는 해당 텍스트 출력을 생성하는 것이다. 성능 향상을 위해 고품질의 시각적 명령 데이터는 MLLM의 능력을 도출하고 향상시키는 데 중요하다. 따라서 대부분의 연구는 다양한 시각 교육 데이터 세트를 구축하는 데 전념하고 있다. 기본 접근법으로 초기 연구에서는 GPT-4 [149]에서 증류하거나 비전 언어 작업 데이터 세트 [151]를 재구성하여 시각적 지침을 구성한다. 명령어 데이터의 품질을 향상시키기 위해, 최근의 작업은 명령어 다양성을 증가시키고, 세밀한 정보(_e.g._, 객체의 좌표)를 명령어에 통합하거나 또는 복잡한 시각적 추론 명령어를 합성함으로써 개선된 전략을 추가로 제안한다[835].\n' +
      '\n' +
      '**MLLM의 평가.** MLLM 개발에 대 한 접근 방식을 소개 한 후 다음 세 가지 측면에서 MLLM의 다중 모드 기능을 효과적으로 평가 하는 방법에 대해 추가로 논의 합니다.\n' +
      '\n' +
      '\\(\\bullet\\)_Evaluation perspectives._ MLLMs에 대한 평가 작업은 크게 두 가지 유형인 _지각_ 과제와 _인지_ 과제로 분류될 수 있다. 구체적으로, _인지_ 태스크는 이미지 콘텐츠의 기본 의미를 이해하는 데 있어 모델의 능력을 평가하는 것을 목표로 하는 반면, _인지_ 태스크는 인식 결과를 기반으로 추론이 필요한 보다 복잡한 태스크를 가진 모델을 평가한다. 인식 능력은 일반적으로 이미지(_e.g._, 토픽 및 스타일) 및 객체(_e.g._, 존재 및 색상) 또는 OCR 관련 작업의 속성에 대한 분류 작업을 통해 평가되며, 기존 데이터 세트 또는 인간 또는 LLM[836, 837, 838, 839]에 의한 주석이 있는 기존 이미지에서 파생된 새로운 데이터 세트를 기반으로 한다. 주목할 만한 인식 문제는 환각[840]으로 모델의 응답은 이미지와 일치하지 않는 내용을 포함한다. MLLMs[841, 834, 842]의 환각에 대한 기존 연구 중 대상 환각[843]은 많은 연구 관심을 받아왔다. 객체 환각에 대한 안정적이고 강건한 평가를 수행하기 위해 POPE [844]는 객체 인식을 일련의 이진 질문으로 변환하는 폴링 기반 객체 프로빙 접근법을 제안하며, 결과는 현재 MLLM이 종종 객체 환각에 어려움을 겪는다는 것을 나타낸다. 반면에 인지 작업은 MLLM이 이미지 지각에 기반한 추론을 수행하도록 요구한다. 공통 추론 과제는 시각적 질문 응답(VQA)이며, 여기서 모델은 공간 관계에 대한 추론을 요구하는 이미지에 대한 질문에 답한다[845], 일반 지식[846], 또는 장면 텍스트[847]. MLLM의 능력을 충분히 탐색하기 위해, HallusionBench[848]는 200개의 정교한 시각적 종속 또는 보충 질문을 수집하며, 이 질문에는 LLaVA-1.5[831] 및 GPT-4V[133]와 같은 가장 진보된 MLLM조차도 양호한 성능을 달성하지 못한다.\n' +
      '\n' +
      '\\(\\bullet\\)_Evaluation paradigms._ MLLM의 응답은 폐쇄형 또는 개방형 방식으로 평가할 수 있다. 전통적인 멀티모달 태스크는 종종 폐쇄형 평가 프레임워크에 의존하는데, 여기서 평가는 모델의 응답과 지상-진실 응답 사이의 정확한 일치를 기반으로 한다. 예들은 시각적 질문 응답 태스크들에 대한 VQA 점수[849] 및 캡션 태스크들에 대한 CIDEr[850] 점수를 포함한다. 그러나, MLLM은 개방형 방식으로 응답을 생성하는데, 이는 정답을 포함할 수 있지만 지상-진실과 완벽하게 일치하지는 않는다. 이러한 불일치는 이전 평가 패러다임에서 모델의 성능을 과소평가하게 할 수 있다. 이 문제를 해결하기 위해 최근 접근법은 인간 또는 LLM을 평가자로 통합했다[829]. 예를 들어, MMBench[838]는 ChatGPT를 사용하여 모델 응답을 객관식 질문 세트에서 가장 관련된 옵션과 정렬한다. 유사하게, LLaVA [851]은 MLLM들의 출력을 평가하기 위해 GPT-4를 이용하며, 여기서 GPT-4는 생성된 이미지 캡션들 및 객체 바운딩 박스들을 평가를 위한 시각적 입력들로서 취한다. 이러한 개방형 평가 방법은 인간 또는 LLM의 관여로 인해 더 높은 비용을 발생시키면서 평가 정확도를 향상시킬 수 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_Evaluation benchmarks._ MLLM에 대한 보다 철저한 평가를 용이하게 하기 위해 다양한 벤치마크가 개발되었다. 그 중 일부는 종합적인 평가를 위해 기존의 비전 언어 과제를 수집한다. 예를 들어 LVLM-eHub [852]는 47개의 기존 텍스트 관련 시각적 작업을 집계하여 MLLM의 6가지 고유한 기능을 평가하고 Reform-Eval [853]은 기존 벤치마크의 질문을 균일한 형식으로 표준화하고 백본 모델이 MLLM의 성능에 어떻게 영향을 미치는지 논의한다. 기존 작업을 통합하는 것 외에도 여러 작업은 인간이 주석을 달거나 LLM의 도움으로 새로운 질문을 도출한다. MME[839]는 인지 및 인지 평가를 위해 수동으로 수집된 텍스트 명령어와 공공 소스로부터의 이미지를 페어링함으로써 데이터세트를 생성한다. MMBench[838]는 이러한 지침을 객관식 질문으로 변형하고 CircularEval을 도입하여 평가의 일관성을 보장한다. SEED-Bench [854]는 시간적 이해 과제를 추가로 고려하고 LLM의 도움으로 평가 척도를 19K 객관식 질문으로 확대한다. MM-Vet[855]는 MLLM들의 통합된 멀티모달 능력들을 평가하기 위한 보다 복잡한 태스크들을 제시한다. 6가지 필수 멀티모달 능력을 정의하는 것으로 시작하여 여러 능력을 결합하여 복잡한 질문을 생성한다. 요약하면, 위의 벤치마크는 MLLM의 종합적인 평가 및 개선된 개발에 집합적으로 기여한다.\n' +
      '\n' +
      '**MLLMs 개선을 위한 핵심 사항** 유능한 MLLM을 개발하기 위해 지침 데이터, 교육 전략 및 안전 및 정렬의 관점에서 모델 용량을 개선하기 위한 세 가지 핵심 사항에 대해 계속 논의합니다.\n' +
      '\n' +
      '\\(\\bullet\\)_Visual instruction data._ 광범위한 작업[856, 831]은 시각적 명령어의 양과 질이 MLLM의 모델 성능에 중요한 영향을 미친다는 것을 경험적으로 발견했다. 시각적 명령어를 구성하는 하나의 기본적인 방법은 이미지의 텍스트 설명에 기초하여 명령어를 합성하는 LLM의 예외적인 능력을 활용하는 것이다[851]. 명령어의 품질을 더욱 향상시키기 위해, 인간 주석의 도움으로 세밀한 시각적 명령을 구성하거나 신중하게 설계된 프롬프트를 통해 더 복잡한 데이터를 합성할 수 있다[857, 833]. 상기 LLM 기반 접근법들의 효과에도 불구하고, LLM(_i.e._, 임의의 이미지들에 대한 트레이닝이 없는 텍스트 생성 모델)이 언어화된 시각적 정보(_e.e._, 캡션들 및 좌표들)만을 기반으로 충분히 양호한 시각적 명령들을 생성하는 능력을 보유하는지 여부에 대한 하나의 주요 질문이 나타난다. 특히, 기존 작업은 LLM에 의해 생성된 시각적 명령어가 때때로 시각적 정보, _예_, 객체 환각에 대한 잘못된 해석을 포함한다는 것을 밝혀냈다[844]. 따라서 LLM에 의해 생성된 명령어 데이터의 품질을 제어하기 위한 효과적인 검증 방법을 설계하는 것이 중요하다[835]. 또한, MLLM에서 무엇이 좋은 시각적 지시를 만들고 시각적 지침이 특정 멀티모달 능력을 이끌어내는지에 대한 더 많은 조사가 필요하다.\n' +
      '\n' +
      '\\(\\bullet\\)_Model training._ LLM과 달리 MLLM은 처음부터 훈련되지 않고 대신 미리 훈련된 언어 및 비전 모델을 기반으로 개발된다. 기존 작업은 MLLMs를 훈련하기 위한 전형적인 2단계 접근법, 즉 비전-언어 정렬 사전 훈련 및 시각적 명령 튜닝을 사용한다. 본질적으로, 기존의 MLLM은 (1) LLM의 고유한 능력과 파라메트릭 지식을 가능한 보존하는 것을 목표로 하고, (2) 미리 훈련된 LLM과 시각적 인코더를 활용하여 멀티모달 작업에 효과적으로 적응하는 것을 목표로 한다. 상기 두 가지 목표를 달성하기 위해, 연결 모듈[151]만을 최적화하거나 커넥터 모듈 및 LLM 컴포넌트[851] 둘 모두를 미세 조정하는 두 가지 전형적인 트레이닝 전략이 시각적 명령 튜닝을 위해 종종 채용된다. 우리가 볼 수 있듯이, 전자는 LLM의 원래 용량을 예약할 수 있지만 적응 성능이 약할 가능성이 있는 반면 후자는 멀티모달 작업에 완전히 적응할 수 있지만 LLM의 원래 용량이 손실된다. 개선된 멀티모달 용량을 달성하기 위해 두 측면을 효과적으로 균형을 맞추는 방법을 조사하기 위한 더 많은 노력이 필요하다. 또한, 기존의 MLLM들은 여전히 LLM들의 용량들에 과도하게 의존하며, 이는 많은 멀티모달 태스크들(_예를 들어,_공간 포지셔닝)에 한계를 제기한다. 이 과정에서 멀티모달 정보도 활용할 수 있도록 언어 모델의 향상된 훈련 접근법을 탐색하는 것은 의미가 있을 것이다.\n' +
      '\n' +
      '\\(\\bullet\\)_Safety and alignment._ 안전성과 정렬은 기술적 접근법에 의해 모델의 거동을 규제하는 것을 목표로 하는 LLM에서 널리 논의되어 왔다[66]. 이 주제는 MLLM에도 중요합니다. 고도로 진보된 MLLM(_예를 들어, GPT-4V[133])도 안전 문제에 취약할 수 있다. 예를 들어, GPT-4V는 때때로 이미지에 대한 사실적 부정확성과 근거 없는 추론을 나타낼 수 있다. 경우에 따라서는 특정 개인 또는 그룹을 대상으로 하는 유해 컨텐츠까지 생성할 수 있다[133]. 나아가, 오픈 소스 MLLM은 또한 환각 반응을 일으키기 쉽고[844] 유해한 콘텐츠를 생성하도록 쉽게 조작될 수 있다[858]. 앞서 언급한 문제를 해결하기 위해 일부 연구에서는 환각 문제를 완화하기 위해 전문화된 시각적 지침을 수집한다[834]. 다른 대안적 접근법은 사후적인 방식으로 MLLM들에 의해 생성된 환각 반응을 교정하기 위한 수정 모델을 훈련시키는 것이다[859]. 추가로, MLLM들을 RLHF와 정렬하는 것은 또한 MLLM들이 향상된 사실성을 갖는 응답들을 생성하는 것을 도울 수 있다[860]. 이러한 노력에도 불구하고, MLLM에 대한 기존의 정렬 기술은 정렬 기준에 대한 포괄적인 고려가 결여된 여러 특정 측면(예를 들어, 환각)에 주로 집중한다. MLLM에 대한 안전 및 정렬 연구를 촉진하기 위해 더 많은 노력이 필요하다.\n' +
      '\n' +
      '#### 8.1.5 KG-Enhanced LLM\n' +
      '\n' +
      '우수한 용량에도 불구하고 LLM은 종종 환각 콘텐츠를 생성할 가능성[602] 및 도메인별 지식 부족[861]과 같은 지식 집약적 작업에 대한 도전으로 어려움을 겪는다. 유망한 해결책으로서, 방대한 지식을 트리플 포맷인 _i.,_(_head_entity, relation, tail_entity_)으로 저장하는 지식 그래프(KG)는 정밀하고 필요한 지식을 제공함으로써 LLM의 태스크 성능을 향상시키는데 활용될 수 있다. 일반적으로 지식 향상 접근법은 다른 형태의 구조화된 데이터(예: 테이블 및 데이터베이스)로 확장될 수 있다[862]. 반면, 우리는 검색 증강 LLM과 시너지 증강 LLM의 두 가지 측면에서 자세히 설명된 LLM을 개선하기 위한 KG의 통합으로 논의를 제한한다.\n' +
      '\n' +
      '**검색 확장 LLM.** KG의 엄청난 양의 사실 기록으로 인해 기존 작업은 일반적으로 검색 모델을 채택 하 여 먼저 KG에서 비교적 작은 하위 그래프를 얻은 다음 관련 지식을 강화 하 여 LLM을 향상 시킵니다. LLM이 등장하기 전에, 검색된 서브그래프들은 종종 트레이닝 데이터에 보충되어, 파라미터 학습을 통해 지식 정보를 PLM들에 주입한다[863, 864, 865]. 대조적으로, 검색된 지식을 활용하기 위해 LLM은 매개변수 업데이트 없이 주로 프롬프트의 일부로 통합한다. 이 접근 방식을 구현하기 위해서는 KG에서 관련 지식을 검색하는 방법과 LLM에 의해 구조화된 데이터를 더 잘 사용하는 방법이라는 두 가지 주요 기술적 문제가 있다. 첫 번째 이슈(_i.e.,_관련 지식 검색)에 대해, 전형적인 접근법은 질문-관련 팩트 트리플들을 식별하기 위해 작은 언어 모델(_e.e.,_RoBERTa)을 트레이닝하는 것이다[866]. 검색 성능을 더욱 향상시키기 위해 여러 연구에서 반복적 읽기 추론 프레임워크를 제안하여 LLM이 KG와 여러 번 상호 작용하고 필요한 지식을 보다 정확한 방식으로 획득할 수 있다[458]. 두 번째 이슈(_i, 즉, 검색된 지식을 활용함)에 대해, 간단한 접근법은 검색된 서브그래프를 직렬화하고 이를 LLM들의 입력으로서 포함하도록 특정 프롬프트를 크래프트하는 것이다[471, 651]. 그러나 지식 연속화에서 구조화된 정보의 손실로 인해 LLM은 원래 KG가 전달하는 구조적 의미를 완전히 포착할 수 없다. 이 문제를 해결하기 위해, 몇몇 모델-기반 접근법들은 서브그래프를 자연 언어 텍스트로 변환하기 위해 특화된 언어 모델(_예를 들어,_T5)을 훈련시킨다[867]. 변환 정확도를 보장하기 위해, 충분한 트레이닝 쌍(종종 감독되지 않은 구성) [868] 및 우수한 모델 능력 [869]에 의존한다.\n' +
      '\n' +
      '**Synergy-Augmented LLM.** 복잡한 작업(예:_ multi-hop 질문 응답 [656])을 해결하려면 체계적인 솔루션 계획에 따라 LLM에서 KG를 여러 번 쿼리해야 하는 경우가 많습니다. 우리는 LLM을 향상시키기 위한 다중 회전 상호 작용 접근법을 LLM _시너지 증강 LLM_이라고 부른다. LLM과 KG를 상호 보완적으로 더 잘 상승시키기 위해 최근 연구에서는 복잡한 작업을 여러 하위 목표로 분해하고 KG[870, 871, 458]에서 필요한 지식을 활용하여 각 하위 목표를 반복적으로 해결할 것을 제안한다. 이 과정에서 LLM은 자율 에이전트(섹션 8.1.6에 자세히 설명되어 있음)로 간주될 수 있으며, 이는 자동으로 계획을 생성하고 KG 환경과의 상호작용을 통해 실행한다[870]. 특히, 주류 접근법들은 전형적으로 현재 단계에서 이용가능한 지식 정보를 사용하여 후보들을 열거하는 것으로 시작하고, 이어서 질문에 따라 다음 단계에 대해 가장 적절한 후보들을 검색한다[871, 870]. 위의 두 단계를 반복함으로써 LLM은 점진적으로 관련 증거를 수집할 수 있고[871, 870], 마지막으로 올바른 해결책에 접근할 수 있다. 효과에도 불구하고, KG에 대한 후보들의 열거는 방대한 검색 공간으로 이어질 것이다[872]. 이를 해결하기 위해, StructGPT[458]는 KG들에 대한 특화된 인터페이스들을 사용하여 지식 정보에 액세스하는 보다 효율적인 방법을 제안한다. 특히, 효율적이고 정확한 데이터 추출을 보장하기 위해 KG(_e.g._, 관계 추출 및 트리플 추출)에 대한 공통 데이터 동작에 따라 특수 인터페이스를 신중하게 설계한다. 이러한 방식으로, LLM들은 KG들의 구조적 정보를 더 잘 조작하고 처리하도록 지시될 수 있고, 따라서 향상된 태스크 성능을 달성한다.\n' +
      '\n' +
      '미래 방향.위의 접근법 외에도 KG 강화 LLM에 대한 몇 가지 유망한 방향이 아직 미개척 상태로 남아 있다. 첫째, 구조화된 데이터의 다양성으로 인해, LLM은 여전히 다양한 종류의 지식 소스들, _예를 들어_ 도메인-특정 KG들을 직접적으로 활용하는 것이 어렵다. 따라서 LLM에 의해 다양한 지식 소스를 조작하고 활용하는 통일된 방법을 탐색하는 것이 필수적이다. 잠재적인 해결책으로 LLM이 정확한 지식을 얻기 위해 특정 지식 소스에서 제공하는 액세스 인터페이스를 이해하고 사용할 수 있도록 효과적인 접근법을 개발하는 것이 유망하며[458], 비용 효율적인 방식으로 데이터 다양성에 적응하는 방법을 조사하기 위해 더 많은 노력이 필요하다. 둘째, 실세계 정보의 진화에 따라 LLM에 저장된 지식은 구식 또는 부정확해질 수 있다. 업데이트된 지식을 비용 효율적인 방식을 통해 LLM으로 동기화하는 방법을 탐색할 필요가 있다[873, 874]. 셋째, LMM의 환각을 줄이는 데 도움이 될 수 있는 보다 충실한 콘텐츠를 생성하기 위해 LMM을 정렬하기 위해 KG의 사실 정보를 사용하는 것이 유망하다[875, 876].\n' +
      '\n' +
      'KG 강화 LLM을 탐색하는 것 외에도, KG 측(_i.e._, LLM4KG)[861, 877]의 태스크를 개선하기 위해 LLM을 활용하는 것도 의미가 있다. 대표적인 예는 LLM이 KG를 보완하거나 구성하는 데 도움이 될 수 있다는 것이다. 이 부분에 대한 논의는 우리의 범위를 벗어나므로 생략한다.\n' +
      '\n' +
      '#### 8.1.6 LLM 기반 에이전트\n' +
      '\n' +
      'AI의 에이전트에 대한 연구는 특정 목표를 달성하기 위해 환경을 인식하고 결정을 내리고 조치를 취할 수 있는 개체를 개발하는 것을 목표로 한다[878]. 그러나, 전통적인 에이전트들은 종종 휴리스틱 규칙들 또는 특정 환경들로 제한되며, 이는 그들의 일반화를 오픈-도메인 시나리오들로 제한한다[879]. LLM은 복잡한 작업을 해결하는 데 탁월한 능력을 가지고 있다는 점을 감안할 때 에이전트의 핵심 계산 단위 역할을 하는 유망한 솔루션으로 빠르게 부상했다[818]. 이 부분에서는 먼저 LLM 기반 에이전트의 프레임워크를 소개하고 그 응용에 대해 논의할 것이다.\n' +
      '\n' +
      '전체 프레임워크 다음으로 먼저 LLM 기반 에이전트의 주요 구성 요소를 자세히 설명한 다음 일반적인 워크플로를 제시합니다.\n' +
      '\n' +
      '\\(\\bullet\\)_Components._ 일반적으로 LLM 기반 에이전트에는 _메모리, planning50_ 및 _실행_의 세 가지 주요 구성 요소가 있습니다. 구체적으로, _메모리_ 컴포넌트는 환경으로부터 인지된 정보를 저장하는 것을 목표로 하며 의사 결정을 지원하는 데 활용될 수 있다. 특히 LLM 기반 에이전트는 일반적으로 읽기 및 쓰기 동작으로 단기 기억과 장기 기억 모두에 정보를 유지한다. 단기 메모리는 보통 LLMs(_i.e._, input)의 내부 컨텍스트 창을 말하며, 여기서 LLMs는 추론과 같은 동작을 통해 읽고 쓸 수 있다[880]. 장기간 메모리는 벡터 데이터베이스와 같이 외부 저장소에 매핑될 수 있지만[537], LLM은 검색을 통해 읽고 반사로 쓸 수 있다[686]. 특히, 프로파일은 보통 장기 기억으로 구현되는데, 이는 그 역할과 기능을 특정하는 에이전트의 중요한 기능이다[818]. _planning_ 컴포넌트는 메모리 컴포넌트로부터의 정보에 기초하여 액션 플랜을 생성하는 것을 담당한다. 데이터 포맷에서, 계획은 보통 텍스트-기반 명령어들[441] 또는 코드-기반 프로그램들[443]의 형태를 취한다. 이를 생성하기 위해, LLM 기반 에이전트들은 먼저 몇몇 후보들을 제안하고, 그 중에서 더 적합한 것을 선택할 것이다[436]. 초기 계획은 환경으로부터의 실행 피드백으로 더 정제될 수 있다[528]. _실행_ 컴포넌트는 계획 컴포넌트로부터 계획을 수행하는 것을 담당하며, 이는 내부 LLM[441] 또는 외부 툴[880]에 의해 이행될 수 있다.\n' +
      '\n' +
      '각주 50: 섹션 6.4에서는 LLM에 대한 활용 접근법으로 계획을 소개하고 이 섹션에서는 LLM 기반 에이전트의 기능적 구성 요소로서의 활용을 설명한다.\n' +
      '\n' +
      '* _Workflow._ 위에서 언급한 세 가지 구성 요소를 사용하여 LLM 기반 에이전트의 일반적인 워크플로는 다음과 같다. 먼저 환경으로부터 정보를 받아 이를 단기 기억에 기록한다. 그런 다음 에이전트는 새로 수신된 정보를 단기 기억에서 처리한다. 이러한 프로세스는 장기 기억에서 검색된 정보로 향상될 수 있다. 이어서, 플래닝 컴포넌트는 단기간 메모리로부터의 처리된 정보를 활용하여 다음 플래닝을 생성한다. 마지막으로, 실행 컴포넌트는 계획 컴포넌트로부터 생성된 계획을 수행하며, 이는 외부 툴에 의해 추가로 보조될 수 있다. 전술한 과정을 반복함으로써, LLM 기반 에이전트는 환경으로부터의 피드백에 응답하여 자율적으로 자신의 행동을 조정할 수 있고, 궁극적으로 자신의 목표를 달성할 수 있다. LLM 기반 에이전트가 사용자 요청을 받거나 목표가 지정되면 위의 워크플로를 따라 환경과의 다중 전환 상호 작용을 통해 작업을 수행합니다.\n' +
      '\n' +
      '요약하면 LLM 기반 에이전트에서 LLM은 핵심 계산 단위 역할을 하며 _메모리, 계획,_ 및 _실행_ 을 포함 하는 구성 요소를 탑재 합니다. 이러한 구성 요소는 환경과의 상호 작용 동안 LLM의 제어 하에 체계적인 방식으로 통합된다. 더 자세한 내용은 독자들이 LLM 기반 AI 에이전트에 대한 포괄적인 조사를 참조할 수 있다[818].\n' +
      '\n' +
      '최근 LLM 기반 에이전트는 복잡한 작업을 자율적으로 해결하는 데 큰 잠재력을 보여 특정 도메인이나 작업에 대한 가능한 애플리케이션을 빠르게 개발할 수 있다. 이 섹션에서는 단일 에이전트 및 다중 에이전트 시나리오의 응용 프로그램에 대해 논의한다.\n' +
      '\n' +
      '* _단일 에이전트 기반 애플리케이션._ 단일 에이전트 모드를 기반으로 하는 응용 프로그램은 주로 사용자 요청을 자율적으로 완료할 수 있는 가능한 작업 해결기를 개발하는 것을 목표로 한다. 범용 과제 해결에 중점을 둔 단일 에이전트 프로젝트가 많이 개발되었다. 대표적인 프로젝트로서 AutoGPT[534]는 장기/단기 메모리 관리 및 검색 엔진과 같은 외부 도구로 LLM에 권한을 부여한다. AutoGPT는 사용자 요청을 자율적으로 처리하기 위해 자신의 기억과 추론과 같은 행동으로부터 지식을 가지고 요청을 이해하고, 이를 세부 계획으로 분해하고, 도구의 도움을 받아 계획을 단계별로 실행하며, 환경의 피드백을 기반으로 나머지 계획을 정제한다. 이러한 반복 프로세스는 사용자 요청이 성공적으로 해결될 때까지 계속된다. 다른 유사한 프로젝트들은 GPT-Engineer[881] 및 XAgent[882]를 포함한다. 또한 웹 브라우징 환경의 경우 WebGPT[81], 실생활 환경의 경우 ProgPrompt[530], 마인크래프트 환경의 경우 보이저[697]와 같은 특정 도메인에 대한 자율 에이전트를 개발하는 것을 목표로 하는 작업도 있다.\n' +
      '\n' +
      '\\(\\bullet\\)_Multi-agent based applications._ 에이전트가 독립적으로 작동하는 단일 에이전트 시스템과 달리 다중 에이전트 시스템은 공동 작업을 통해 집단 지성을 발현한다. 전형적으로, 다수의 에이전트는 각각이 각자의 역할 및 기능을 갖는 동일하거나 상이한 LLM으로부터 인스턴스화될 수 있다. 이러한 에이전트 간의 조정 전략에 따라 다중 에이전트 시스템은 협력 기반과 경쟁 기반 두 가지 범주로 나눌 수 있다. 협력 기반 모드에서, 정보를 공유하고 에이전트들 간의 협력 동작을 모색하기 위해, 자유-형태 대화[883], 구조화된 문서[884], 및 데이터 임베딩[885]을 포함하는 다양한 통신 프로토콜들이 제안되었다. 통신 프로토콜에 기초하여, 에이전트들은 소프트웨어 공학[884], 사용자 행동 분석[821, 819], 및 사회 시뮬레이션[533]과 같은 다운스트림 애플리케이션들에 대해 효과적으로 조직될 수 있다. 경쟁 기반 모드에서 토론은 다양한 사고를 촉진하고 에이전트 간에 가치 있는 외부 피드백을 이끌어내기 위한 인기 있는 커뮤니케이션 프로토콜 중 하나이다. 이러한 방법은 수학적 추론[886] 및 평가[732]와 같이 정확한 의사 결정 및 정확한 응답을 요구하는 도메인에 유익하다.\n' +
      '\n' +
      '**남은 문제.** 큰 성공에도 불구하고 LLM 기반 에이전트의 개발 및 응용 프로그램을 제한 하는 몇 가지 문제가 있습니다. 첫째, 모델 스케일의 폭발적인 성장과 함께 LLM 기반 에이전트의 시간 및 메모리 오버헤드를 포함한 효율성은 대규모 배치, 특히 LLM의 수많은 인스턴스를 가진 다중 에이전트 시스템에 중요한 문제가 된다. 둘째, LLM 기반 에이전트의 수가 증가함에 따라 에이전트 간의 조정 복잡도 증가를 지원하기 위해 보다 효과적이고 효율적인 통신 프로토콜 및 아키텍처가 필요하다. 또한, 유능한 에이전트를 구축하는 것은 명령어 추종 및 긴 텍스트 모델링과 같은 LLM의 능력에 대한 기술적 과제를 제기한다. 기존의 LLM은 에이전트의 인스턴스화에 특별히 최적화되어 있지 않기 때문에 LLaMA와 같은 대부분의 공개 소스 LLM은 에이전트의 개발을 효과적으로 촉진할 수 없다. 따라서 에이전트의 핵심 계산 단위 역할을 할 수 있는 능력 있는 전문 모델을 개발하는 것이 중요하다.\n' +
      '\n' +
      '#### 8.1.7 LLM for Evaluation\n' +
      '\n' +
      '인간 평가는 일반적으로 신뢰할 수 있는 품질 평가를 제공할 수 있지만, 높은 주석 비용, 상당한 시간 요구 사항 및 주석 불일치로 인해 종종 방해를 받는다[887]. 대조적으로, 자동 평가는 인간 평가에 대한 확장 가능한 대안으로 채택될 수 있다. 전통적인 자동 평가들은 참조-기반 메트릭들(_e.g._, BLEU 및 ROUGE)에 의존해 왔다. 최근 일반 작업 해결사로 LLM이 등장하면서 자동 평가자로서의 잠재력이 강조되어 LLM 기반 평가를 수행할 가능성이 높아졌다. 다음 부분에서는 평가 형식, 방법, 메타 평가, 그리고 나머지 쟁점들을 포함한 평가를 위한 LLM에 대한 최근의 진행 상황을 소개할 것이다.\n' +
      '\n' +
      '**평가 형식.** 평가 결과의 유형에 따라 평가 형식은 _점수 기반 평가_ 및 _언어 기반 평가_로 분류할 수 있습니다. 점수 기반 평가는 평가 텍스트에 대한 품질 점수(_e.g._, 평점 또는 순위)를 할당하기 위해 측정 가능한 메트릭을 사용한다. 일반적인 방법은 쌍대 비교를 수행하는 것인데, LLM은 특정 지침[354, 647, 727]에 따라 후보 텍스트의 부분 순서 관계를 결정하는 데 사용되며, 이는 평가 작업을 크게 단순화한다. 그러나, 후보들의 수를 스케일링할 때 비효율적인 문제에 직면할 수 있다[727]. 평가 중에 고품질 참조 텍스트가 이용가능할 때, LLM은 참조에 의해 제공된 지침에 따라 텍스트를 스코어링하도록 지시될 수 있다[727, 728, 716]. 한편, 언어 기반 평가는 단순한 정량적 채점을 넘어서 비평 및 제안을 생성하는데 중점을 두고 있다[889, 890, 371, 891, 892, 893, 894, 898, 899, 390, 391, 892, 893, 894, 895, 896, 897, 898, 899, 999, 900, 891, 899, 892, 893, 894, 895, 896, 897, 898, 899, 988, 999, 990, 991, 992, 993, 994, 995, 1020, 1021, 1012, 1013, 1014, 1017, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1052, 1053, 1054, 1055, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1079, 1080, 1081, 1082 대표적인 벤치마크인 MT-Bench[727]는 LLM과 인간 판단 사이의 일치를 평가하며, 80개의 멀티턴 질문에 대한 무연결 비교에서 GPT-4가 인간의 선호도와 밀접하게 일치함을 보여준다. 또한 주관적인 인간 평가에서 발생하는 잠재적인 편향을 해결하기 위해 LLMBar [897]은 객관적으로 더 나쁘지만 표면적으로 매력적인 출력을 수동으로 설계하여 평가자를 오도할 수 있다. 평가 결과는 가장 진보된 LLM조차도 어려운 환경에서 여전히 인간 수준의 평가에 미치지 못한다는 것을 보여준다.\n' +
      '\n' +
      '**남은 문제.** 섹션 7.1.1에서 논의된 바와 같이 최근 연구에 따르면 LLM 기반 평가자는 순서 편향, 자기 선호 편향 및 길이 편향과 같은 여러 유형의 편향을 노출합니다. [647, 727]. 일부 편향은 다중 경로 앙상블 또는 다중 에이전트 협업과 같은 방법을 통해 완화될 수 있지만 LLM 기반 평가자에 내재되어 있다. 결과적으로 모델 내에서 이러한 편향을 본질적으로 해결하는 것은 여전히 어려운 문제이다. 또한, 최근의 작업은 LLM들이 자체 생성된 콘텐츠를 이해할 수 없을 수도 있다는 것을 밝혀냈고, 그들의 생성 능력에 비해 더 약한 이해 능력을 나타낸다[899]. 가장 진보된 LLM조차도 여전히 외부 피드백 없이 그들의 추론 또는 사실적 오류를 식별하는 데 어려움을 겪고 있다[900, 901]. 따라서 현재 LLM 기반 평가자는 상위 계층 LLM 또는 복잡한 작업을 평가하는 데 적합하지 않을 수 있다. 이것은 특히 정교한 추론, 계획 및 도메인별 지식을 요구하는 능력 있는 LLM 및 복잡한 작업을 평가하기 위한 LLM 기반 평가자에 대한 개선 접근법의 중요성을 강조한다.\n' +
      '\n' +
      '### _LLM for Specific Domains_\n' +
      '\n' +
      '이 부분에서 우리는 의료, 교육, 법률, 금융 및 과학 연구 지원을 포함한 여러 대표적인 영역에 대한 LLM의 적용에 대해 논의한다.\n' +
      '\n' +
      '**의료** 는 인간 생활과 밀접한 관련이 있는 필수 응용 분야입니다. ChatGPT가 등장한 이후로 많은 연구가 ChatGPT 또는 기타 LLM을 의료 영역에 적용했다. LLM은 다양한 의료 업무, 예를 들어 생물학 정보 추출[763], 의료 자문 상담[902], 정신 건강 분석[903], 보고서 단순화[904]를 처리할 수 있는 것으로 나타났다. 주요 기술적 접근법으로 연구자들은 일반적으로 LLM이 광범위한 의료 작업을 수행하도록 안내하는 특정 프롬프트 또는 지침을 설계한다. 의료 영역에서 LLM의 힘을 더 활용하기 위해 연구자들은 의료 관련 LLM을 개발할 것을 제안한다[905, 356, 906]. 구체적으로, Med-PaLM 모델[356, 905]은 미국 의료 면허 시험(USMLE)에서 전문가 수준의 성능을 달성하고, 소비자의 의료 질문에 응답하는 데 있어서 의사로부터 더 큰 승인을 얻는다. 그러나 LLM은 의료 잘못된 정보를 조작할 수 있으며[904, 907], _예: 의료 용어를 잘못 해석하고 의료 지침과 일치하지 않는 조언을 제안할 수 있다. 또한 환자의 건강 정보를 LLM을 지원하는 상업용 서버에 업로드하는 것도 사생활 우려를 불러일으킬 것이다.\n' +
      '\n' +
      '**교육** 은 LLM이 잠재적으로 상당한 영향을 미치는 중요한 응용 프로그램 영역이기도 합니다. 기존의 연구는 LLM이 객관식 문제와 자유응답 문제 모두에서 다양한 수학 과목(예: 물리학, 컴퓨터 과학)에서 표준화된 시험에서 학생 수준의 성능을 달성할 수 있다는 것을 발견했다. 또한, 경험적 연구들은 LLM들이 교육을 위한 쓰기 또는 읽기 보조자 역할을 할 수 있다는 것을 보여주었다[908, 909]. 최근 연구 [909]는 ChatGPT가 학문 전반에 걸쳐 논리적으로 일관된 답변을 생성할 수 있으며 깊이와 너비의 균형을 맞출 수 있음을 보여준다. 또 다른 정량적 분석[908]은 ChatGPT(LLM의 결과를 자신의 답변으로 유지하거나 정제하는 것)를 사용하는 학생들이 컴퓨터 보안 분야의 일부 과정에서 평균 학생보다 더 나은 성능을 보인다는 것을 보여준다. 최근 몇 가지 관점 논문[910, 911]은 교사-학생 협력, 개인 맞춤형 학습 및 평가 자동화 등 교실 수업에서 LLM의 다양한 적용 시나리오를 탐구한다. 그러나 교육에 LLM을 적용하면 표절, AI 생성 콘텐츠의 잠재적 편향, LLM에 대한 과도한 의존 및 비영어권 개인에 대한 불공평한 접근과 같은 일련의 실질적인 문제가 발생할 수 있다[912].\n' +
      '\n' +
      '**법칙** 은 전문 도메인 지식을 기반으로 하는 전문 도메인입니다. 최근 다양한 법률 업무, 즉 _예: 법률 문서 분석[913], 법률 판단 예측[914], 법률 문서 작성[915]을 해결하기 위해 LLM을 적용한 연구가 다수 있다. 최근 연구[916]는 LLM이 법적 해석과 추론의 강력한 능력을 발휘한다는 것을 발견했다. 더욱이, 최신 GPT-4 모델은 모의 변호사 시험에서 인간 수험생들에 비해 상위 10% 점수를 달성한다[46]. 법률 영역에서 LLM의 성능을 더욱 향상시키기 위해 특별히 설계된 법률 프롬프트 엔지니어링을 사용하여 긴 법률 문서 이해 및 복잡한 법률 추론에서 고급 성능을 산출한다[917, 918]. 진행 상황을 요약하자면 LLM은 법조계에 도움이 되는 보조자 역할을 할 수 있다. 진보에도 불구하고 법에 LLM을 사용하는 것은 저작권 문제[919], 개인정보 유출[920], 편견과 차별[921]을 포함한 법적 문제에 대한 우려를 불러일으킨다.\n' +
      '\n' +
      '**재무** 는 LLM이 유망한 애플리케이션 전망을 갖는 중요한 필드입니다. LLM은 수치 클레임 탐지[922], 금융 감정 분석[923], 금융 명명 개체 인식[924], 금융 추론[925]과 같은 다양한 금융 관련 작업에 사용되었다. 금융 업무에서 범용 LLM이 보여주는 경쟁적인 제로 샷 성능에도 불구하고, 그들은 여전히 백만 스케일 파라미터를 포함하는 도메인 특정 PLM을 과소 수행한다[922]. LLM의 스케일링 효과를 활용하기 위해, 연구자들은 LLM을 지속적으로 사전 트레이닝하기 위해 대규모 금융 코퍼스를 수집한다(_예를 들어,_BloombergGPT[360], XuanYuan 2.0[926], FinGPT[927]. 블룸버그GPT는 범용 업무에서 경쟁적 성과를 유지하면서 다양한 금융 업무에 걸쳐 괄목할 만한 성과를 보였다[360]. 그럼에도 불구하고, LLM에 의한 부정확하거나 유해한 콘텐츠의 생성은 금융 시장에 상당한 악영향을 미칠 수 있기 때문에, 금융에서 LLM 적용의 잠재적 위험을 고려하는 것이 필수적이다[360]. 따라서 금융 분야에서의 LLMs 사용에 대한 보다 엄격한 검토와 모니터링이 필요하다.\n' +
      '\n' +
      '**과학 연구** 는 LLM이 개발 진행에 권한을 부여할 수 있는 또 다른 유망한 분야입니다. 선행 연구는 지식 집약적 과학 과제(_e.g._, PubMedQA[928], BioASQ[929])를 처리하는 데 LLM의 효과를 입증하며, 특히 과학 관련 코퍼라[35, 203, 930]에 대해 훈련된 LLM의 경우. 뛰어난 일반 능력과 광범위한 과학적 지식을 감안할 때 LLM은 과학적 연구 파이프라인의 다양한 단계에 걸쳐 유용한 보조자로서 상당한 잠재력을 가지고 있다[931]. 먼저, 문헌 조사 단계에서 LLM은 특정 연구 분야의 진행 상황에 대한 포괄적인 개요를 수행하는 데 도움이 될 수 있다[932, 933]. 둘째, 연구 아이디어 생성 단계에서 LLM은 흥미로운 과학적 가설을 생성하는 능력을 보여준다[934]. 셋째, 데이터 분석 단계에서 LLM은 데이터 탐색, 시각화 및 분석 결론 도출을 포함한 데이터 특성을 분석하는 자동 접근 방식을 수행할 수 있다[935, 936]. 넷째, 논문 작성 단계에서 연구자들은 또한 과학 글쓰기에서 LLM의 도움으로부터 이익을 얻을 수 있다[937, 938]. 여기서 LLM은 기존의 내용을 요약하고 글쓰기를 연마하는 등 다양한 수단을 통해 과학 글쓰기에 대한 귀중한 지원을 제공할 수 있다[939]. 또한, LLM은 오류 검출, 체크리스트 검증 및 후보 랭킹과 같은 작업을 포괄하는 자동화된 논문 검토 프로세스를 도울 수 있다[940]. 이러한 발전에도 불구하고 생성된 과학 콘텐츠의 품질을 높이고 유해한 환각을 줄이기 위해 도움이 되고 신뢰할 수 있는 과학 보조자 역할을 하는 LLM의 능력을 향상시킬 여지가 많다.\n' +
      '\n' +
      '_ 요약._ 앞서 언급한 작업 외에도 LLM의 적용은 여러 다른 도메인에서도 논의되었다. 예를 들어, 심리학 영역에서 최근 일부 연구에서는 자기 인식, 마음 이론(ToM), 정의적 컴퓨팅과 같은 LLM의 인간과 유사한 특성을 연구했다[941, 942]. 특히, 두 가지 고전적인 거짓 믿음 작업에 대해 수행된 ToM의 경험적 평가는 GPT-3.5 시리즈의 모델이 ToM 작업에서 9세 어린이와 유사한 성능을 달성했기 때문에 LLM이 ToM 유사 능력을 가질 수 있다고 추측한다[941]. 또한, 다른 작업 라인은 소프트웨어 개발 도메인, _예:_, 코드 제안[943], 코드 요약[944], 및 자동화된 프로그램 수리[945]에 LLM을 적용하는 것을 조사했다. 요약하자면, LLM이 실제 작업에서 인간을 돕는 것은 중요한 연구 영역이 되었다. 그러나, 그것은 또한 도전을 제시합니다. LLM 생성 콘텐츠의 정확도 보장, 편향 처리, 사용자 개인 정보 보호 및 데이터 보안 유지 등은 LLM을 실제 시나리오에 적용할 때 중요한 고려 사항이다.\n' +
      '\n' +
      '## 9 결론 및 향후 방향\n' +
      '\n' +
      '본 연구에서는 최근 대형 언어 모델(LLM)의 발전 과정을 살펴보고, LLM을 이해하고 활용하기 위한 주요 개념, 연구 결과 및 기법을 소개하였다. 기존 문헌에서 잘 다루어진 초기 사전 훈련 언어 모델(_e.g._, BERT 및 GPT-2)의 내용은 제외하면서 크기가 10B보다 큰 큰 모델(_i.e._)에 초점을 맞춘다. 특히, 우리의 설문조사는 LLM의 4가지 중요한 측면, 즉 _i.e._, 사전 훈련, 적응, 활용 및 평가에 대해 논의했다. 각 측면에 대해 LLM의 성공에 중요한 기술이나 결과를 강조한다. 또한, LLMs 개발을 위한 가용 자원을 요약하고 LLMs 재생산을 위한 중요한 구현 지침에 대해 논의한다. 이 조사는 LLM에 대한 가장 최근의 문헌을 다루려고 하며 연구자와 엔지니어 모두에게 이 주제에 대한 좋은 참조 자원을 제공한다.\n' +
      '\n' +
      '다음으로, 본 조사의 논의를 요약하고 LLM의 도전과 향후 방향을 다음과 같은 측면에서 소개한다.\n' +
      '\n' +
      '**기본 및 원칙.** 특정 작업 목표에 대한 교육 대신 LLM은 대규모 텍스트 데이터에 대한 감독되지 않은 사전 교육을 통해 학습합니다. 이는 충분한 일반화를 달성하기 위해 훈련 작업을 가능한 한 확장하는 것을 목표로 하는 이전의 다중 작업 학습 접근법과 상당히 다르다. 따라서 LLM의 능력의 기초를 확립하는 기본 원칙이나 요소를 밝히는 것이 필수적이다. 언어 모델의 기본 아이디어는 직관적이지만, 간단한 언어 모델링 목표(_e.g._, 다음 토큰 예측)에 의해 훈련된 LLM이 다양한 실세계 태스크를 해결할 수 있게 될 수 있는 이유를 공식적으로 설명하는 것은 여전히 어렵다. 이 문제를 조사하기 위해, LLM의 모델 용량은 사전 훈련 데이터에 크게 의존하기 때문에, 비지도 사전 훈련에 기초한 용량 학습(또는 선택) 메커니즘을 연구하는 것이 유망한 접근법이다. 또한, _scaling_은 LLMs[31, 55, 64]의 용량을 향상시키는 데 중요한 역할을 하며, 큰 모델의 동작이 작은 모델의 동작과 어떻게 관련되는지에 대한 더 많은 이론적 분석을 수행하는 것은 매우 유용하다. 예를 들어, 큰 모델의 동작은 작은 모델에서 추론할 수 있고 실제로 예측할 수 없는 것이 무엇인지에 대한 더 많은 이론적 분석을 수행하는 것은 매우 유용하다. 또 다른 연구 방향은 사전 훈련 데이터에 의해 인코딩된 지식을 넘어 LLM이 일반화될 수 있는지에 대한 우려가 제기되었기 때문에 LLM에 대한 모델 일반화에 대한 보다 심층적인 분석을 탐색하는 것이다. 또한, 데이터 오염은 LLM의 성능을 공정하게 평가하기 위한 심각한 문제가 되었으므로 적절한 평가 프로토콜을 설정하는 것이 LLM의 모델 용량을 조사하고 분석하는 기초가 될 것이다.\n' +
      '\n' +
      '**모델 아키텍처.** 확장성과 효과로 인해 Transformer는 LLM 구축을 위한 사실상의 아키텍처가 되었습니다. 신경망 구성 및 확장 가능한 병렬 훈련(섹션 4.2.2의 논의 참조)과 같은 이 아키텍처의 성능을 개선하기 위해 다양한 전략이 제안되었다. 그러나 트랜스포머는 여전히 높은 훈련 비용과 느린 추론 속도로 인해 어려움을 겪고 있다. 대규모 사전 훈련을 위한 개선된 모델 아키텍처를 개발하기 위해 더 많은 노력[251, 252]이 여전히 필요하다. 특히, 시스템-레벨 또는 하드웨어-레벨 최적화(_e.g._, FlashAttention[284])는 변압기 아키텍처의 효율성을 향상시키기 위해 더 많은 탐색을 할 가치가 있다. 또한, 중요한 기본 용량으로서, 기존의 LLM들은 통상적으로 긴 컨텍스트 윈도우를 유지한다. 예를 들어, 가장 최근의 GPT-4 Turbo는 128K 토큰의 긴 컨텍스트를 가능하게 하고, Claude 2.1도 최대 200K 토큰의 입력을 지원한다. LLM들[264, 291]의 긴 컨텍스트 모델링 능력을 향상시키기 위한 많은 노력들이 있었지만, 결과 모델들은 여전히 컨텍스트 윈도우에서 정보를 잘 처리하지 못한다[299]. 이 문제를 해결하기 위해서는 긴 컨텍스트 정보의 모델링 및 활용을 향상시키기 위한 특정 아키텍처 적응 또는 알고리즘이 필요할 수 있다. 또 다른 걱정스러운 우려는 기존 작업이 대부분 디코더 전용 트랜스포머를 사용하여 LLM을 훈련하는 데 초점을 맞추고 있다는 것이다. 효과에도 불구하고, 대안 모델 아키텍처에 대한 더 광범위하고 다양한 탐구를 심각하게 제한한다.\n' +
      '\n' +
      '**모델 교육.** 사전 교육을 위해서는 데이터 수집, 데이터 정리, 데이터 혼합 및 데이터 커리큘럼의 체계적인 프로세스를 효과적으로 지원할 수 있는 LLM 최적화를 위한 데이터 중심 인프라 및 교육 절차를 수립하는 것이 필수적입니다. 또한 컴퓨팅 클러스터에서 리소스를 더 잘 구성하고 활용할 수 있도록 하드웨어 지원 또는 리소스 스케줄의 유연한 메커니즘을 요구합니다. 실제로, 엄청난 계산 소비와 데이터 품질 및 트레이닝 트릭에 대한 민감성으로 인해, 유능한 LLM을 사전 트레이닝하는 것은 매우 어렵다[78, 93]. 따라서, LLMs, _e.g._, 예측 가능한 스케일링[46] 및 프록시 모델 훈련[59]을 최적화하기 위한 시스템적이고 경제적인 사전 훈련 접근법을 개발하는 것이 특히 중요해진다. 대규모 모델 최적화에서 저하 또는 실패의 잠재적 위험을 줄이기 위해 더 많은 훈련 레시피 또는 원리를 조사하고 공유해야 한다. 점점 더 많은 모델 체크포인트 및 청소 데이터 세트가 출시되었지만 사전 훈련 데이터 준비(_e.g._, 세부 청소 전략) 및 데이터 스케줄링(_e.g._, 데이터 혼합 및 커리큘럼)에 대한 재현 가능한 작업이 여전히 부족하다. LLM을 처음부터 사전 트레이닝하는 것은 매우 비용이 많이 들기 때문에, 공개적으로 이용가능한 모델 체크포인트들(_e.g._, LLaMA[57] 및 Flan-T5[69])에 기초하여 LLM을 지속적으로 사전 트레이닝하거나 미세 조정하기 위한 적합한 메커니즘들을 설계하는 것이 중요하다. 이를 위해 _예:_ 치명적인 잊기 및 작업 전문화와 같은 여러 기술적 문제가 해결되어야 합니다. 또한, 특정 지식[672], _예_ 를 효과적으로 주입하거나 편집하여 구식 사실을 수정하는 효과적인 튜닝 전략을 개발하는 것도 유용하다.\n' +
      '\n' +
      '**모델 사용률.** 자연어 인터페이스를 기반으로 하는 _prompting_ 은 LLM을 사용 하 여 다양 한 작업을 해결 하는 데 중요한 접근 방식이 되었습니다. 작업 설명 및 시연 예제를 프롬프트에 결합함으로써, 인-컨텍스트 학습(ICL)은 LLM들에게 새로운 작업들에 대해 잘 수행할 수 있는 능력을 부여하며, 심지어 일부 경우들에서 전체-데이터 미세 조정된 모델들을 능가한다. 복잡한 추론 능력을 향상시키기 위해 중간 추론 단계를 프롬프트로 포함하는 CoT(chain-of-thought) 전략을 예로 들어 고급 프롬프트 기술이 제안되었다. 또한, 계획은 복잡한 작업을 해결하기 위한 유망한 접근법이며, 이는 도구 사용 용량을 활용하여 LLM을 반복적으로 호출한다. 이러한 노력에도 불구하고, 프롬프트와 관련된 몇 가지 기본적인 문제는 여전히 탐구되지 않았다: 왜 좋은 프롬프트가 정답을 이끌어낼 수 있지만 나쁜 프롬프트는 이끌어낼 수 없는지, 고급 프롬프트 방법(예: ICL 및 CoT)의 작동 원리를 밝히고 이러한 기존 접근법을 더욱 개선하고 특정 작업에서 LLM에 대한 효과적인 프롬프트를 효율적으로 찾는 방법. 또한, 실용적인 관점에서, 특히 대규모 배치에서 LLM의 추론 비용을 줄이는 것이 근본적인 과제가 되었다. 또 다른 인기 있는 연구 방향은 지원 소스로부터 검색된 컨텍스트를 태스크 해결을 위한 프롬프트에 포함하는 검색 증강 생성이다. 검색 증강은 지식 경계를 확장하고 질의 응답 능력을 향상시킬 수 있는 것으로 나타났지만(461), LLMs에 의한 긴 컨텍스트 활용의 효과에 시달릴 수 있다[299].\n' +
      '\n' +
      '**안전 및 정렬.** 용량에도 불구하고 LLM은 실제 사용 시 큰 안전 문제에 직면해 있습니다. 확률적 모델링 성격의 근본적인 문제로서 LLM은 그럴듯해 보이지만 사실적으로 틀릴 수 있는 텍스트를 언급하면서 환각을 생성하는 경향을 보인다[638]. 더 나쁜 것은, 악성 시스템에 대한 유해, 편향 또는 독성 텍스트를 생성하라는 의도적인 지시에 의해 LLM이 유도되어 오용의 잠재적 위험을 초래할 수 있다는 것이다[66, 55]. LLM의 안전 문제(_e.g._, 프라이버시, 과의존, 허위 정보 및 영향 작업)에 대한 상세한 논의를 하기 위해, 독자는 GPT-3/4 기술 보고서를 참조할 수 있다[55, 46]. 이러한 문제들을 회피하기 위한 주요 기술적 접근법으로서, 정렬 방법들(_e.g._, RLHF)[116, 66]은 잘 정렬된 LLM들을 개발하기 위해 인간 피드백을 레버리지함으로써 널리 사용되어 왔다. 그러나 RLHF는 전문 레이블러의 고품질 인간 피드백 데이터에 크게 의존하며, 이는 자격을 갖춘 인간 주석을 모집하는 데 비용과 시간이 많이 소요된다. 따라서 인간 레이블러의 노력을 줄이기 위한 RLHF 프레임워크를 개선하고 데이터 품질이 보장된 보다 효율적인 주석 접근법을 모색해야 하며, 레이블링 작업을 지원하기 위해 LLM이 사용될 수 있다. 또한, RLHF의 훈련 난이도 및 불안정성을 줄이기 위해 정렬을 위한 단순화된 최적화 알고리즘[386, 389]을 개발하는 것도 제안한다. 또 다른 실용적인 접근법으로서, 적색 팀핑[369, 132]은 수집된 적대적 프롬프트를 사용하여 LLM을 정제하는 LLM의 모델 안전성을 개선하기 위해 채택되었다(_i.e._, 적색 팀핑으로부터의 공격을 피함). 또한, 프라이버시 우려는 도메인-특정 데이터로 LLM들을 미세조정할 때 또한 고려하는 것이 중요하며, 따라서 연합 기반 학습[946]은 프라이버시 제한 시나리오들에서 유용할 수 있다.\n' +
      '\n' +
      '**응용 프로그램 및 생태계.** LLM은 다양한 작업을 해결하는 데 강력한 능력을 보여 주므로 작업별 자연어 지침에 따라 광범위한 실제 응용 프로그램 (_i.e._)에 적용할 수 있습니다. 놀라운 발전으로 ChatGPT는 _New Bing_의 릴리스에 추가로 통합 된 인간이 정보에 액세스 하는 방법을 잠재적으로 변경 했습니다. 일반적으로, 가까운 미래에, LLM은 검색 엔진과 추천 시스템을 포함한 정보 추구 기술에 상당한 영향을 미칠 것으로 예측할 수 있다. 또한 LLM은 실제 시나리오에서 다양한 복잡한 작업을 처리하기 위해 보다 지능적인 시스템(예: 자율 AI 에이전트)을 개발할 수 있도록 한다. 특히, 어시스턴트 API는 OpenAI(명령어, 지식 및 도구 사용에 의해 특징)에 의해 시작되어 애플리케이션 내에서 에이전트 유사 어시스턴트의 신속한 개발을 가능하게 한다. 이러한 기술 혁신의 물결은 인간의 삶과 밀접한 관련이 있는 LLM 기반 애플리케이션(예: OpenAI의 GPT 스토어)의 생태계로 이어질 것이다. 마지막으로, LLM의 부상은 인공지능(AGI)의 탐구를 조명한다. 그 어느 때보다 스마트 AI 시스템 개발이 유망합니다. 그러나 이 개발 과정에서 AI 안전은 인간에게는 좋지만 나쁘지는 않은 AI를 만드는 주요 관심사 중 하나가 되어야 한다[40].\n' +
      '\n' +
      '### Coda\n' +
      '\n' +
      '이 긴 설문 조사를 작성하고 시의적절한 작업으로 내용을 업데이트하는 것은 쉬운 일이 아니다. 우선 독자들과 저희 팀원들의 성원에 진심으로 감사드립니다. 우리는 이 조사에 매우 열심히 노력하며 LLM에 대한 포괄적이고 시기적절한 참조를 제시할 수 있기를 바란다.\n' +
      '\n' +
      '**설문 작성** 이 설문조사는 우리 연구팀이 개최한 토론 회의에서 계획되었으며, 최근 대규모 언어 모델의 발전을 우리 팀 구성원을 위한 매우 읽을 수 있는 보고서로 요약하는 것을 목표로 했다. 첫 번째 초안은 2023년 3월 13일에 완료되었으며, 우리 팀 구성원은 LLM에 대한 관련 연구를 비교적 객관적이고 포괄적인 방식으로 포함하도록 최선을 다했다. 그런 다음 여러 번의 패스에서 글과 내용을 광범위하게 수정했습니다. 공간 한계로 인해 선택 기준을 설정하여 그림 3과 표 1에서 기존 LLM의 일부만 포함할 수 있다. 그러나 정기적으로 유지 관리 되는 GitHub 페이지 ([https://github.com/RUCAlBox/LLMSurvey](https://github.com/RUCAlBox/LLMSurvey))에서 모델 선택에 대 한 보다 완화 된 기준을 설정 합니다. 2023년 3월 31일 초기 버전, 2023년 6월 29일 주요 버전, 2023년 9월 10일 2차 버전, 2023년 11월 23일 최신 버전(주요 버전)을 출시합니다.\n' +
      '\n' +
      '**조언 찾기**. 우리의 모든 노력에도 불구하고, 이 조사는 여전히 완벽과는 거리가 멀다: 우리는 중요한 참고 자료나 주제를 놓칠 가능성이 있고, 또한 비엄격한 표현이나 토론을 할 수도 있다. 우리는 이 조사를 지속적으로 업데이트하고 최대한 품질을 향상시킬 것입니다. 우리에게는 설문 작성도 스스로 LLMs에 대한 학습 과정이다. 이 설문 조사를 개선하기 위한 건설적인 제안이 있는 독자의 경우 설문 조사의 GitHub 페이지에 의견을 남기거나 저자에게 직접 이메일을 보낼 수 있다. 수령된 의견이나 제안을 후속 버전으로 수정하고 설문 조사에서 건설적인 제안을 제공한 독자를 인정한다.\n' +
      '\n' +
      '**로그 업데이트**. 이 부분에서는 이 설문의 arXiv 제출에 대한 업데이트 로그를 정기적으로 유지 관리합니다.\n' +
      '\n' +
      '* 2023년 3월 31일 첫 번째 릴리스: 초기 버전.\n' +
      '* 2023년 4월 9일 업데이트: 소속 정보 추가, 그림 3 및 표 1 수정 및 LLM에 대한 해당 선택 기준을 명확히 하고, 글쓰기를 개선하며, 일부 사소한 오류를 수정합니다.\n' +
      '* 2023년 4월 11일에 업데이트: 라이브러리 리소스에 대한 오류를 수정합니다.\n' +
      '* 2023년 4월 12일 업데이트: 그림 3 및 표 1을 수정하고 LLM의 출시 날짜를 명확히 합니다.\n' +
      '* 2023년 4월 16일 업데이트: GPT 시리즈 모델의 기술적 진화에 대한 새로운 섹션 2.2를 추가합니다.\n' +
      '* 2023년 4월 24일 업데이트: 스케일링 법률에 대한 논의를 추가하고 창발적 능력에 대한 모델 크기에 대한 몇 가지 설명을 추가합니다(섹션 2.1). 그림 9의 다양한 아키텍처에 대한 주의 패턴에 대한 예시적인 그림을 추가하고 표 6의 자세한 수식을 추가합니다.\n' +
      '* 2023년 4월 25일 업데이트: 그림 및 표의 일부 복사 오류를 수정합니다.\n' +
      '* 2023년 4월 27일 업데이트: 섹션 5.3에서 효율적인 튜닝을 추가합니다.\n' +
      '* 2023년 4월 28일 업데이트: 섹션 5.3을 수정합니다.\n' +
      '* 2023년 5월 7일 업데이트: 표 1, 표 2 및 몇 가지 사소한 점을 수정합니다.\n' +
      '* Update on 2023(major revision):\n' +
      '* 섹션 1: arXiv에서 발표된 LLM 논문의 경향에 대한 그림 1을 추가;\n' +
      '* 섹션 2: GPT의 진화 및 대응하는 논의를 위해 그림 4를 추가;\n' +
      '* 섹션 3: LLMA 패밀리 및 대응하는 논의를 위한 그림 5를 추가;\n' +
      '* 섹션 5: 섹션 5.1.1의 명령어 튜닝의 합성 데이터 포맷팅, 섹션 5.1.4의 명령어 튜닝을 위한 실증 분석, 섹션 5.3의 파라미터-효율적인 모델 적응 및 섹션 5.4의 메모리-효율적인 적응에 대한 최신 논의를 추가;\n' +
      '* 섹션 6: 섹션 6.4에서 복잡한 과제 해결을 위한 계획, LCL 6.2.3의 기본 메커니즘에 대한 최신 논의를 추가;\n' +
      '* 섹션 7: 섹션 7.4의 LLM의 고급 능력을 평가하기 위한 대표적인 데이터셋, 및 경험적 능력 평가를 위한 업데이트 표 14;\n' +
      '* 섹션 6.1.1: 프롬프트 디자인 추가;\n' +
      '* 섹션 8: 금융 및 과학 연구 도메인에서 LLM의 적용에 대한 논의를 추가;\n' +
      '* 2023년 9월 10일 업데이트(주요 개정):\n' +
      '* 이 문서의 그림 및 표의 저작권을 청구합니다.\n' +
      '* 섹션 3, 섹션 4, 섹션 5, 섹션 6 및 섹션 7에서 최신 LLM, 기술 및 설명을 추가합니다.\n' +
      '* 섹션 4: 섹션 4.2.5에서 디코딩 전략에 대한 최신 논의를 추가;\n' +
      '* 섹션 5: 섹션 5.1.2의 명령어 튜닝을 위한 실제 트릭, 섹션 5.1.4의 명령어 튜닝을 위한 LLMA(13B), 섹션 5.2.3의 RLHF를 위한 실제 전략, 섹션 5.2.4의 RLHF가 없는 정렬 및 섹션 5.2.5의 SFT 및 RLHF에 대한 발언에 대한 최신 논의를 추가;\n' +
      '* 섹션 6: 섹션 6.4에서 복잡한 과제 해결을 위한 계획에 관한 내용을 업데이트하고;\n' +
      '* 7절: 기존 평가 작업의 카테고리에 대해 7.3.2절, 표 15절에서 평가 접근법에 대한 논의를 추가하고, 7.4절 및 표 16의 결과에 대해 경험적 능력 평가를 업데이트하며;\n' +
      '* 섹션 6.1.1: 표 12에 새 프롬프트 예제를 추가합니다.\n' +
      '* Update on 11월 23, 2023 (this version):\n' +
      '* 섹션 1: 4세대 언어 모델의 진화 과정에 대한 그림 2를 추가;\n' +
      '* 섹션 2: 스케일링 법률 및 창발적 능력이 스케일링 법률과 어떻게 관련 되는지에 대한 더 많은 논의를 추가합니다.\n' +
      '* 섹션 3: 그림 3 및 표 1의 최신 LLMs 추가, 섹션 3.1의 최신 API, 일반적으로 사용되는 데이터 세트\n' +
      '섹션 3.3에서의 지시 튜닝 및 정렬 튜닝을 위해, 그리고 섹션 3.4에서의 몇몇 라이브러리들을 위해;\n' +
      '* 섹션 4: 섹션 4.1.3에서 데이터 혼합물 및 데이터 커리큘럼을 포함하는 데이터 스케줄링에 대한 최신 논의를 추가; 섹션 4.1.4에서 데이터 준비 요약을 추가; 섹션 4.2.4에서 긴 컨텍스트 모델링에 대한 논의를 추가; 섹션 4.2.4에서 디코딩 효율 문제에 대한 논의를 추가 및 섹션 4.2.5에서 최신 디코딩 전략을 추가;\n' +
      '* 섹션 5: 섹션 5.1에서 인스턴스 구성 및 튜닝 전략에 대한 최신 논의를 추가하고, 섹션 5.2.3에서 프로세스 감독 RLHF에 대한 최신 논의를 추가하고, 섹션 5.4.3에서 양자화된 LLaMA 모델(7B 및 13B)에 대한 경험적 연구를 추가합니다.\n' +
      '* 섹션 6: 섹션 6.1.2에서 프롬프트 최적화에 대한 최신 논의를 추가하고, 섹션 6.3에서 프롬프트에 대한 내용을 업데이트하는 단계;\n' +
      '* 섹션 8: 섹션 8.1의 연구 방향에 대한 LLM에 대한 최신 논의를 추가;\n' +
      '* 섹션 9: 여러 측면에서 내용을 수정합니다.\n' +
      '\n' +
      '**콘텐츠 계획.** 이 설문 조사에 새 콘텐츠를 정기적으로 포함시켜 보다 자립적이고 최신 상태로 만들 것입니다. 다음 주요 버전(들)에 나타날 수 있는 몇 가지 잠재적인 주제를 나열한다: (1) 수업 튜닝과 능력 평가를 위한 더 큰 언어 모델을 사용한 더 많은 실험, (2) 더 자세한 프롬프트 연습, (3) 훈련 레시피, (4) 더 많은 이론적 분석 및 토론, (5) 애플리케이션에 대한 더 많은 토론.\n' +
      '\n' +
      '**실험에 대한 설명**. 이 버전에서는 교수-조정(표 9), 전체 능력 평가(표 16), 프롬프트 엔지니어링(표 17)에 대한 숫자 실험을 포함했다. 계산 자원의 한계로 인해 우리의 실험은 소규모 모델이나 몇 가지 비교에 국한되어 완전하지 않다. 그럼에도 불구하고 부분적인 결과를 대중에게 공유하는 것이 의미가 있을 수 있다고 생각한다. 우리는 더 큰 모델 또는 더 많은 비교의 누락된 결과를 향후 버전에 포함시키려고 노력할 것이다. **또한 보다 포괄적인 실험을 수행하기 위해 컴퓨팅 능력을 지원해야 합니다.* *\n' +
      '\n' +
      '**중국어 버전**. 또한 [https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese.pdf](https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese.pdf) 링크에서 이 조사 논문의 번역된 중국어 버전을 제공합니다. 4명의 자원봉사자가 내용을 확인하고 수정하는데 기여하며, 이원후, 신등, 신명호우, 옌빈음, 잔슈오조(기여 순)이다. 중국어 버전도 지속적으로 업데이트할 예정이지만 최신 영어 버전만큼 시기적절하지 않을 수 있습니다.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      '저자들은 이 논문을 교정해 준 얀카이 린과 유타오 주에게 감사를 표하고 싶다. 이 논문의 첫 번째 발표 이후, 우리는 독자들로부터 많은 귀중한 논평들을 받았다. 타일러 수아르, 다마이 다이, 량 딩, 스텔라 비더만, 케빈 그레이, 제이 알람마르, 유보 펑, 마크 홀스트롬, 싱동 류, 일석 오, 이팅 류, 샤오쥔 왕, 가오얀 오, 토드 모릴, 하오 류, 젠유 장, 신린 장 등 건설적인 제안과 논평으로 우리에게 편지를 쓴 독자들에게 진심으로 감사드린다.\n' +
      '\n' +
      'v11 버전(2023년 6월 29일) 이후 우리는 많은 수의 실험과 신속한 관행을 추가하고 있다. 이 새로운 콘텐츠는 우리 팀의 많은 자원봉사자들에 의해 완성됩니다. 여기, 우리는 이 부분에 대해 매우 열심히 일한 모든 학생들에게 감사하기 위해 특별한 부분을 추가합니다(우리 작가 목록에 있는 것도 포함).\n' +
      '\n' +
      '**실험에 대한 기여**. 표 16과 같은 실험에 참여해 주신 아래 분들께 진심으로 감사드립니다.\n' +
      '\n' +
      'Xiaoxue Cheng: 언어 생성과 HaluEval 과제에 대한 평가를 위한 실험을 구현한다.\n' +
      '\n' +
      '\\(\\bullet\\) Yuhao Wang: 환경 작업과의 상호 작용에 대한 평가를 위한 실험을 구현한다.\n' +
      '\n' +
      '\\(\\bullet\\) Bowen Zheng: 도구 조작 작업에 대한 평가를 위한 실험을 구현한다.\n' +
      '\n' +
      '**팁에 대한 기여**. 우리는 표 12에 프롬프트를 디자인하기 위한 제공된 팁의 해당 수에 대한 기여에 대해 다음 사람들을 나열한다.\n' +
      '\n' +
      '\\(\\bullet\\) Xiaolei Wang: T3, O3\n' +
      '\n' +
      '\\(\\bullet\\) Beichen Zhang: D2, D5\n' +
      '\n' +
      '\\(\\bullet\\) Zhipeng Chen : D3, D4\n' +
      '\n' +
      '\\(\\bullet\\) Junjie Zhang: D6\n' +
      '\n' +
      '\\(\\bullet\\) Bowen Zheng: D7\n' +
      '\n' +
      '\\(\\bullet\\) Zican Dong : D8\n' +
      '\n' +
      '\\(\\bullet\\) 신유탕: C2\n' +
      '\n' +
      '\\(\\bullet\\)Yifan Du : T4\n' +
      '\n' +
      '\\(\\bullet\\) 톈이탕 : O6, O7, D9\n' +
      '\n' +
      '\\(\\bullet\\) Yupeng Hou: O8, C3\n' +
      '\n' +
      '\\(\\bullet\\) Salvatore Raieli: C4\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin, "A neural probabilistic language model," _J. Mach. Learn. Res._, vol. 3, pp. 1137-1155, 2003.\n' +
      '* [2] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. P. Kuksa, "Natural language processing (almost) from scratch," _J. Mach. Learn. Res._, vol. 12, pp. 2493-2537, 2011.\n' +
      '* [3] S. Pinker, _The Language Instinct: How the Mind Creates Language_. Brilliance Audio; Unabridged edition, 2014.\n' +
      '* [4] M. D. Hauser, N. Chomsky, and W. T. Fitch, "The faculty of language: what is it, who has it, and how did it evolve?" _science_, vol. 298, no. 5598, pp. 1569-1579, 2002.\n' +
      '* [5] A. M. Turing, "Computing machinery and intelligence," _Mind_, vol. LIX, no. 236, pp. 433-460, 1950.\n' +
      '* [6] F. Jelinek, _Statistical Methods for Speech Recognition_. MIT Press, 1998.\n' +
      '* [7] J. Gao and C. Lin, "Introduction to the special issue on statistical language modeling," _ACM Trans. Asian Lang. Inf. Process._, vol. 3, no. 2, pp. 87-93, 2004.\n' +
      '* [8] R. Rosenfeld, "Two decades of statistical language modeling: Where do we go from here?" _Proceedings of the IEEE_, vol. 88, no. 8, pp. 1270-1278, 2000.\n' +
      '\n' +
      'A. Stolcke, "Srilm-an extensible language modeling toolkit" in _7th international conference on spoken language processing_, 2002.\n' +
      '* [10] X. Liu and W. B. Croft, "Statistical language modeling for information retrieval," _Annu. Rev. Inf. Sci. Technol._, vol. 39, no. 1, pp. 1-31, 2005.\n' +
      '* [11] C. Zhai, _Statistical Language Models for Information Retrieval_, ser. Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publishers, 2008.\n' +
      '* [12] S. M. Thede and M. P. Harper, "A second-order hidden markov model for part-of-speech tagging," in _27th Annual Meeting of the Association for Computational Linguistics, University of Maryland, College Park, Maryland, USA, 20-26 June 1999_, R. Dale and K. W. Church, Eds. ACL, 1999, pp. 175-182.\n' +
      '* [13] L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mercer, "A tree-based statistical language model for natural language speech recognition," _IEEE Transactions on Acoustics, Speech, and Signal Processing_, vol. 37, no. 7, pp. 1001-1008, 1989.\n' +
      '* [14] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean, "Large language models in machine translation," in _EMNLP-CoNLL 2007, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, June 28-30, 2007, Prague, Czech Republic_, J. Eisner, Ed. ACL, 2007, pp. 858-867.\n' +
      '* [15] S. M. Katz, "Estimation of probabilities from sparse data for the language model component of a speech recognizer," _IEEE Trans. Acoust. Speech Signal Process._, vol. 35, no. 3, pp. 400-401, 1987.\n' +
      '* [16] W. A. Gale and G. Sampson, "Good-turing frequency estimation without tears," _J. Quant. Linguistics_, vol. 2, no. 3, pp. 217-237, 1995.\n' +
      '* [17] T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, and S. Khudanpur, "Recurrent neural network based language model," in _INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010_, T. Kobayashi, K. Hirose, and S. Nakamura, Eds. ISCA, 2010, pp. 1045-1048.\n' +
      '* [18] S. Kombrink, T. Mikolov, M. Karafiat, and L. Burget, "Recurrent neural network based language modeling in meeting recognition," in _INTERSPEECH 2011, 12th Annual Conference of the International Speech Communication Association, Florence, Italy, August 27-31, 2011_. ISCA, 2011, pp. 2877-2880.\n' +
      '* [19] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, "Distributed representations of words and phrases and their compositionality," in _Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States_, C. J. C. Burges, L. Bottou, Z. Ghahramani, and K. Q. Weinberger, Eds., 2013, pp. 3111-3119.\n' +
      '* [20] T. Mikolov, K. Chen, G. Corrado, and J. Dean, "Efficient estimation of word representations in vector space," in _1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings_, Y. Bengio and Y. LeCun, Eds., 2013.\n' +
      '* [21] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, "Deep contextualized word representations," in _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers)_, M. A. Walker, H. Ji, and A. Stent, Eds. Association for Computational Linguistics, 2018, pp. 2227-2237.\n' +
      '* [22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need," in _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, 2017, pp. 5998-6008.\n' +
      '* [23] J. Devlin, M. Chang, K. Lee, and K. Toutanova, "BERT: pre-training of deep bidirectional transformers for language understanding," in _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, J. Burstein, C. Doran, and T. Solorio, Eds. Association for Computational Linguistics, 2019, pp. 4171-4186.\n' +
      '* [24] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, "BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension," in _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_, 2020, pp. 7871-7880.\n' +
      '* [25] W. Fedus, B. Zoph, and N. Shazeer, "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity," _J. Mach. Learn. Res_, pp. 1-40, 2021.\n' +
      '* [26] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever _et al._, "Language models are unsupervised multitask learners," _OpenAI blog_, p. 9, 2019.\n' +
      '* [27] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, "Roberta: A robustly optimized BERT pretraining approach," _CoRR_, vol. abs/1907.11692, 2019.\n' +
      '* [28] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, A. Raja, M. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechala, T. Kim, G. Chhablani, N. V. Nayak, D. Datta, J. Chang, M. T. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey, R. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Fevry, J. A. Fries, R. Teehan, T. L. Scao, S. Biderman, L. Gao, T. Wolf, and A. M. Rush, "Multitask prompted training enables zero-shot task generalization," in _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.\n' +
      '* [29] T. Wang, A. Roberts, D. Hesslow, T. L. Scao, H. W. Chung, I. Beltagy, J. Launay, and C. Raffel, "What language model architecture and pretraining objective works best for zero-shot generalization?" in _International Conference on Machine Learning, ICML 2022, 17-23July 2022, Baltimore, Maryland, USA_, ser. Proceedings of Machine Learning Research, vol. 162, 2022, pp. 22 964-22 984.\n' +
      '* [30] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, "Scaling laws for neural language models," _CoRR_, vol. abs/2001.08361, 2020.\n' +
      '* [31] J. Wei, Y. Tay, R. Bommassani, C. Raffel, B. Zoph, S. Borgoard, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. H. Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, and W. Fedus, "Emergent abilities of large language models," _CoRR_, vol. abs/2206.07682, 2022.\n' +
      '* [32] M. Shanahan, "Talking about large language models," _CoRR_, vol. abs/2212.03551, 2022.\n' +
      '* [33] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. H. Chi, Q. Le, and D. Zhou, "Chain of thought prompting elicits reasoning in large language models," _CoRR_, vol. abs/2201.11903, 2022.\n' +
      '* [34] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre, "Training compute-optimal large language models," vol. abs/2203.15556, 2022.\n' +
      '* [35] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia, A. Poulton, V. Kerkez, and R. Stojnic, "Galactica: A large language model for science," _CoRR_, vol. abs/2211.09085, 2022.\n' +
      '* [36] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing," _ACM Comput. Surv._, pp. 195:1-195:35, 2023.\n' +
      '* [37] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan, L. He, H. Peng, J. Li, J. Wu, Z. Liu, P. Xie, C. Xiong, J. Pei, P. S. Yu, and L. Sun, "A comprehensive survey on pretrained foundation models: A history from BERT to chatgpt," _CoRR_, vol. abs/2302.09419, 2023.\n' +
      '* [38] X. Han, Z. Zhang, N. Ding, Y. Gu, X. Liu, Y. Huo, J. Qiu, Y. Yao, A. Zhang, L. Zhang, W. Han, M. Huang, Q. Jin, Y. Lan, Y. Liu, Z. Liu, Z. Lu, X. Qiu, R. Song, J. Tang, J. Wen, J. Yuan, W. X. Zhao, and J. Zhu, "Pre-trained models: Past, present and future," _AI Open_, vol. 2, pp. 225-250, 2021.\n' +
      '* [39] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, "Pre-trained models for natural language processing: A survey," _CoRR_, vol. abs/2003.08271, 2020.\n' +
      '* [40] S. Altman, "Planning for agi and beyond," _OpenAI Blog_, February 2023.\n' +
      '* [41] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang, "Sparks of artificial general intelligence: Early experiments with gpt-4," vol. abs/2303.12712, 2023.\n' +
      '* [42] S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, B. Patra, Q. Liu, K. Aggarwal, Z. Chi, J. Bjorck, V. Chaudhary, S. Som, X. Song, and F. Wei, "Language is not all you need: Aligning perception with language models," _CoRR_, vol. abs/2302.14045, 2023.\n' +
      '* [43] Y. Cao, S. Li, Y. Liu, Z. Yan, Y. Dai, P. S. Yu, and L. Sun, "A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt," _arXiv preprint arXiv:2303.04226_, 2023.\n' +
      '* [44] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu _et al._, "Palm-e: An embodied multimodal language model," _arXiv preprint arXiv:2303.03378_, 2023.\n' +
      '* [45] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, and N. Duan, "Visual chatgpt: Talking, drawing and editing with visual foundation models," _arXiv preprint arXiv:2303.04671_, 2023.\n' +
      '* [46] OpenAI, "Gpt-4 technical report," _OpenAI_, 2023.\n' +
      '* [47] Y. Fu, H. Peng, and T. Khot, "How does gpt obtain its ability? tracing emergent abilities of language models to their sources," _Yao Fu\'s Notion_, Dec 2022.\n' +
      '* [48] J. Li, T. Tang, W. X. Zhao, and J. Wen, "Pretrained language model for text generation: A survey," in _Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021_, Z. Zhou, Ed. ijcai.org, 2021, pp. 4492-4499.\n' +
      '* [49] P. Lu, L. Qiu, W. Yu, S. Welleck, and K. Chang, "A survey of deep learning for mathematical reasoning," _CoRR_, vol. abs/2212.10535, 2022.\n' +
      '* [50] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, L. Li, and Z. Sui, "A survey for in-context learning," _CoRR_, vol. abs/2301.00234, 2023.\n' +
      '* [51] J. Huang and K. C. Chang, "Towards reasoning in large language models: A survey," _CoRR_, vol. abs/2212.10403, 2022.\n' +
      '* [52] S. Qiao, Y. Ou, N. Zhang, X. Chen, Y. Yao, S. Deng, C. Tan, F. Huang, and H. Chen, "Reasoning with language model prompting: A survey," _CoRR_, vol. abs/2212.09597, 2022.\n' +
      '* [53] J. Zhou, P. Ke, X. Qiu, M. Huang, and J. Zhang, "Chatgpt: potential, prospects, and limitations," in _Frontiers of Information Technology & Electronic Engineering_, 2023, pp. 1-6.\n' +
      '* [54] W. X. Zhao, J. Liu, R. Ren, and J. Wen, "Dense text retrieval based on pretrained language models: A survey," _CoRR_, vol. abs/2211.14876, 2022.\n' +
      '* [55] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, "Language models are few-shot learners," in _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., 2020.\n' +
      '* [56] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsyvashchenko, J. Maynez, A. Rao, P. Barnes,Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pilai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel, "Palm: Scaling language modeling with pathways," _CoRR_, vol. abs/2204.02311, 2022.\n' +
      '* [57] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample, "Llama: Open and efficient foundation language models," _CoRR_, 2023.\n' +
      '* [58] T. Henighan, J. Kaplan, M. Katz, M. Chen, C. Hesse, J. Jackson, H. Jun, T. B. Brown, P. Dhariwal, S. Gray _et al._, "Scaling laws for autoregressive generative modeling," _arXiv preprint arXiv:2010.14701_, 2020.\n' +
      '* [59] S. M. Xie, H. Pham, X. Dong, N. Du, H. Liu, Y. Lu, P. Liang, Q. V. Le, T. Ma, and A. W. Yu, "Doremi: Optimizing data mixtures speeds up language model pretraining," _arXiv preprint arXiv:2305.10429_, 2023.\n' +
      '* [60] P. Villalobos, J. Sevilla, L. Heim, T. Besiroglu, M. Hobbhahn, and A. Ho, "Will we run out of data? an analysis of the limits of scaling datasets in machine learning," _CoRR_, vol. abs/2211.04325, 2022. [Online]. Available: [https://doi.org/10.48550/arXiv.2211.04325](https://doi.org/10.48550/arXiv.2211.04325)\n' +
      '* [61] N. Muennighoff, A. M. Rush, B. Barak, T. L. Scao, A. Piktus, N. Tazi, S. Pyysalo, T. Wolf, and C. Raffel, "Scaling data-constrained language models," _arXiv preprint arXiv:2305.16264_, 2023.\n' +
      '* [62] I. McKenzie, A. Lyzhov, A. Parrish, A. Prabhu, A. Mueller, N. Kim, S. Bowman, and E. Perez, "The inverse scaling prize," 2022. [Online]. Available: [https://github.com/inverse-scaling/prize](https://github.com/inverse-scaling/prize)\n' +
      '* [63] B. A. Huberman and T. Hogg, "Phase transitions in artificial intelligence systems," _Artificial Intelligence_, vol. 33, no. 2, pp. 155-171, 1987.\n' +
      '* [64] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, H. F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassier, R. Powell, G. van den Driessche, L. A. Hendricks, M. Raub, P. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang, J. Uesato, J. Mellor, I. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. M. Jayakumar, E. Buchatskaya, D. Budden, E. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Nematzadeh, E. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch, J. Lespiau, M. Tsimpoukelli, N. Grigorev, D. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Masson d\'Autume, Y. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, C. Jones, J. Bradbury, M. J. Johnson, B. A. Hechtman, L. Weidinger, I. Gabriel, W. S. Isaac, E. Lockhart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway, L. Bennett, D. Hassabis, K. Kavukcuoglu, and G. Irving, "Scaling language models: Methods, analysis & insights from training gopher," _CoRR_, vol. abs/2112.11446, 2021.\n' +
      '* [65] D. Dai, Y. Sun, L. Dong, Y. Hao, Z. Sui, and F. Wei, "Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers," _CoRR_, vol. abs/2212.10559, 2022.\n' +
      '* [66] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe, "Training language models to follow instructions with human feedback," _CoRR_, vol. abs/2203.02155, 2022.\n' +
      '* [67] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, "Fine-tuned language models are zero-shot learners," in _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.\n' +
      '* [68] R. Thoppilan, D. D. Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, Y. Li, H. Lee, H. S. Zheng, A. Ghafouri, M. Menegali, Y. Huang, M. Krikun, D. Lepikhin, J. Qin, D. Chen, Y. Xu, Z. Chen, A. Roberts, M. Bosma, Y. Zhou, C. Chang, I. Krivokon, W. Rusch, M. Pickett, K. S. Meier-Hellstern, M. R. Morris, T. Doshi, R. D. Santos, T. Duke, J. Sorkaer, B. Zevenbergen, V. Prabhakaran, M. Diaz, B. Hutchinson, K. Olson, A. Molina, E. Hoffman-John, J. Lee, L. Aroyo, R. Rajakumar, A. Butryna, M. Lamm, V. Kuzmina, J. Fenton, A. Cohen, R. Bernstein, R. Kurzweil, B. Aguera-Arcas, C. Cui, M. Croak, E. H. Chi, and Q. Le, "Lambda: Language models for dialog applications," _CoRR_, vol. abs/2201.08239, 2022.\n' +
      '* [69] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, A. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, S. Narang, G. Mishra, A. Yu, V. Y. Zhao, Y. Huang, A. M. Dai, H. Yu, S. Petrov, E. H. Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V. Le, and J. Wei, "Scaling instruction-finetuned language models," _CoRR_, vol. abs/2210.11416, 2022.\n' +
      '* [70] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, A. Kluska, A. Lewkowycz, A. Agarwal, A. Power, A. Ray, A. Warstadt, A. W. Kocurek, A. Safaya, A. Tzarv, A. Xiang, A. Parrish, A. Nie, A. Hussain, A. Askell, A. Dsouza, A. Rahane, A. S. Iyer, A. Andreassen, A. Santilli, A. Stuhlmuller, A. M. Dai, A. La, A. K. Lampinen, A. Zou, A. Jiang, A. Chen, A. Vuong, A. Gupta, A. Gottardi, A. Norelli, A. Venkatesh, A. Gholamidavoodi, A. Tabassum, A. Menezes, A. Kirubarajan, A. Mullokandov, A. Sabharwal, A. Herrick, A. Efrat, A. Erdem, A. Karakas, and et al., "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models," _CoRR_, vol. abs/2206.04615, 2022.\n' +
      '* [71] R. Schaeffer, B. Miranda, and S. Koyejo, "Are emergent abilities of large language models a mirage?" _arXiv preprint arXiv:2304.15004_, 2023.\n' +
      '\n' +
      '* [72] S. Hu, X. Liu, X. Han, X. Zhang, C. He, W. Zhao, Y. Lin, N. Ding, Z. Ou, G. Zeng, Z. Liu, and M. Sun, "Unlock predictable scaling from emergent abilities," 2023.\n' +
      '* [73] A. Power, Y. Burda, H. Edwards, I. Babuschkin, and V. Misra, "Grokking: Generalization beyond overfitting on small algorithmic datasets," _arXiv preprint arXiv:2201.02177_, 2022.\n' +
      '* [74] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He, "Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters," in _KDD_, 2020, pp. 3505-3506.\n' +
      '* [75] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, "Megatron-Im: Training multi-billion parameter language models using model parallelism," _CoRR_, vol. abs/1909.08053, 2019.\n' +
      '* [76] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee, and M. Zaharia, "Efficient large-scale language model training on GPU clusters using megatron-lm," in _International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2021, St. Louis, Missouri, USA, November 14-19, 2021_. ACM, 2021, p. 58.\n' +
      '* [77] V. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Anderson, M. Shoeybi, and B. Catanzaro, "Reducing activation recomputation in large transformer models," _CoRR_, vol. abs/2205.05198, 2022.\n' +
      '* [78] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagne, A. S. Luccioni, F. Yvon, M. Galle, J. Tow, A. M. Rush, S. Biderman, A. Webson, P. S. Ammanamachi, T. Wang, B. Sagot, N. Muennighoff, A. V. del Moral, O. Ruwase, R. Bawden, S. Bekman, A. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O. Suarez, V. Sanh, H. Laurencon, Y. Jernite, J. Launay, M. Mitchell, C. Raffel, A. Gokaslan, A. Simh, A. Soroa, A. F. Aji, A. Alfassy, A. Rogers, A. K. Nitzav, C. Xu, C. Mou, C. Emeze, C. Klamm, C. Leong, D. van Strien, D. I. Adelani, and et al., "BLOOM: A 176b-parameter open-access multilingual language model," _CoRR_, vol. abs/2211.05100, 2022.\n' +
      '* [79] P. F. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and D. Amodei, "Deep reinforcement learning from human preferences," in _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett, Eds., 2017, pp. 4299-4307.\n' +
      '* [80] T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom, "Toolformer: Language models can teach themselves to use tools," _CoRR_, vol. abs/2302.04761, 2023.\n' +
      '* [81] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button, M. Knight, B. Chess, and J. Schulman, "Webgpt: Browser-assisted question-answering with human feedback," _CoRR_, vol. abs/2112.09332, 2021.\n' +
      '* [82] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, "Exploring the limits of transfer learning with a unified text-to-text transformer," _J. Mach. Learn. Res._, pp. 140:1-140:67, 2020.\n' +
      '* [83] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel, "mt5: A massively multilingual pre-trained text-to-text transformer," in _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021_, 2021, pp. 483-498.\n' +
      '* [84] W. Zeng, X. Ren, T. Su, H. Wang, Y. Liao, Z. Wang, X. Jiang, Z. Yang, K. Wang, X. Zhang, C. Li, Z. Gong, Y. Yao, X. Huang, J. Wang, J. Yu, Q. Guo, Y. Yu, Y. Zhang, J. Wang, H. Tao, D. Yan, Z. Yi, F. Peng, F. Jiang, H. Zhang, L. Deng, Y. Zhang, Z. Lin, C. Zhang, S. Zhang, M. Guo, S. Gu, G. Fan, Y. Wang, X. Jin, Q. Liu, and Y. Tian, "Pangu-\\(\\alpha\\): Large-scale autoregressive pretrained chinese language models with auto-parallel computation," _CoRR_, vol. abs/2104.12369, 2021.\n' +
      '* [85] Z. Zhang, Y. Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y. Yao, F. Qi, J. Guan, P. Ke, Y. Cai, G. Zeng, Z. Tan, Z. Liu, M. Huang, W. Han, Y. Liu, X. Zhu, and M. Sun, "CPM-2: large-scale cost-effective pre-trained language models," _CoRR_, vol. abs/2106.10715, 2021.\n' +
      '* [86] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, "Codegen: An open large language model for code with multi-turn program synthesis," _arXiv preprint arXiv:2203.13474_, 2022.\n' +
      '* [87] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy, K. McDonell, J. Phang, M. Pieler, U. S. Prashanth, S. Purohit, L. Reynolds, J. Tow, B. Wang, and S. Weinbach, "Gpt-neox-20b: An open-source autoregressive language model," _CoRR_, vol. abs/2204.06745, 2022.\n' +
      '* [88] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, E. Pathak, G. Karamanolakis, H. G. Lai, I. Purohit, I. Mondal, J. Anderson, K. Kuznia, K. Doshi, K. K. Pal, M. Patel, M. Moradshahi, M. Parmar, M. Purohit, N. Varshney, P. R. Kaza, P. Verma, R. S. Puri, R. Karia, S. Doshi, S. K. Sampat, S. Mishra, S. R. A, S. Patro, T. Divit, and X. Shen, "Super-naturalinstructions: Generalization via declarative instructions on 1600+ NLP tasks," in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, 2022, pp. 5085-5109.\n' +
      '* [89] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W. Chung, D. Bahri, T. Schuster, H. Zheng, D. Zhou, N. Houlsby, and D. Metzler, "U12: Unifying language learning paradigms," 2022.\n' +
      '* [90] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. T. Diab, X. Li, X. V. Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer,"OPT: open pre-trained transformer language models," _CoRR_, vol. abs/2205.01068, 2022.\n' +
      '* [91] M. R. Costa-jussa, J. Cross, O. Celebi, M. Elbayad, K. Heafield, K. Heffern, E. Kalbassi, J. Lam, D. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. M. Gonzalez, P. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews, N. F. Ayan, S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzman, P. Koehn, A. Mourachko, C. Ropers, S. Saleem, H. Schwenk, and J. Wang, "No language left behind: Scaling human-centered machine translation," _CoRR_, vol. abs/2207.04672, 2022.\n' +
      '* [92] Q. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, Z. Wang, L. Shen, A. Wang, Y. Li _et al._, "Codgegex: A pre-trained model for code generation with multilingual evaluations on humaneval-x," _arXiv preprint arXiv:2303.17568_, 2023.\n' +
      '* [93] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, W. L. Tam, Z. Ma, Y. Xue, J. Zhai, W. Chen, P. Zhang, Y. Dong, and J. Tang, "GLM-130B: an open bilingual pre-trained model," vol. abs/2210.02414, 2022.\n' +
      '* [94] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman, T. L. Scao, M. S. Bari, S. Shen, Z. X. Yong, H. Schoelkopf, X. Tang, D. Radev, A. F. Aji, K. Almubarak, S. Albanie, Z. Alyafeai, A. Webson, E. Raff, and C. Raffel, "Crosslingual generalization through multitask finentuning," _CoRR_, vol. abs/2211.01786, 2022.\n' +
      '* [95] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura, X. Li, B. O\'Horo, G. Pereyra, J. Wang, C. Dewan, A. Celikyilmaz, L. Zettlemoyer, and V. Stoyanov, "OPT-IML: scaling language model instruction meta learning through the lens of generalization," _CoRR_, vol. abs/2212.12017, 2022.\n' +
      '* [96] S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O\'Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff _et al._, "Pythia: A suite for analyzing large language models across training and scaling," _arXiv preprint arXiv:2304.01373_, 2023.\n' +
      '* [97] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou, "Codegen2: Lessons for training lms on programming and natural languages," _CoRR_, vol. abs/2305.02309, 2023.\n' +
      '* [98] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocektor, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, Q. Liu, E. Zheltonozhskii, T. Y. Zhuo, T. Wang, O. Dehaene, M. Davaadorj, J. Lamy-Poirier, J. Monteiro, O. Shliazhko, N. Gontier, N. Meade, A. Zebaze, M. Yee, L. K. Umapathi, J. Zhu, B. Lipkin, M. Oblokulov, Z. Wang, R. M. V, J. Stillerman, S. S. Patel, D. Abulkhanov, M. Zocca, M. Dey, Z. Zhang, N. Fahmy, U. Bhattacharyya, W. Yu, S. Singh, S. Luccioni, P. Villegas, M. Kunakov, F. Zhdanov, M. Romero, T. Lee, N. Timov, J. Ding, C. Schlesinger, H. Schoelkopf, J. Ebert, T. Dao, M. Mishra, A. Gu, J. Robinson, C. J. Anderson, B. Dolan-Gavitt, D. Contractor, S. Reddy, D. Fried, D. Bahdanau, Y. Jernite, C. M. Ferrandis, S. Hughes, T. Wolf, A. Guha, L. von Werra, and H. de Vries, "Starcoder: may the source be with you!" _CoRR_, vol. abs/2305.06161, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2305.06161](https://doi.org/10.48550/arXiv.2305.06161)\n' +
      '* [99] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale _et al._, "Llama 2: Open foundation and fine-tuned chat models," _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [100] A. Yang, B. Xiao, B. Wang, B. Zhang, C. Yin, C. Lv, D. Pan, D. Wang, D. Yan, F. Yang _et al._, "Baichuan 2: Open large-scale language models," _arXiv preprint arXiv:2309.10305_, 2023.\n' +
      '* [101] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang _et al._, "Qwen technical report," _arXiv preprint arXiv:2309.16609_, 2023.\n' +
      '* [102] X. Li, Y. Yao, X. Jiang, X. Fang, X. Meng, S. Fan, P. Han, J. Li, L. Du, B. Qin _et al._, "Flim-101b: An open llm and how to train it with 5100 k budget," _arXiv preprint arXiv:2309.03852_, 2023.\n' +
      '* [103] T. Wei, L. Zhao, L. Zhang, B. Zhu, L. Wang, H. Yang, B. Li, C. Cheng, W. Lu, R. Hu _et al._, "Skywork: A more open bilingual foundation model," _arXiv preprint arXiv:2310.19341_, 2023.\n' +
      '* [104] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen, "Gshard: Scaling giant models with conditional computation and automatic sharding," in _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_, 2021.\n' +
      '* [105] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tilllet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba, "Evaluating large language models trained on code," _CoRR_, vol. abs/2107.03374, 2021.\n' +
      '* [107] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang -J. Shang, J. Liu, X. Chen, Y. Zhao, Y. Lu, W. Liu, Z. Wu, W. Gong, J. Liang, Z. Shang, P. Sun, W. Liu, X. Ouyang, D. Yu, H. Tian, H. Wu, and H. Wang, "ERNIE 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation," _CoRR_, vol. abs/2107.02137, 2021.\n' +
      '* [108] O. Lieber, O. Sharir, B. Lenz, and Y. Shoham, "Jurassic-1: Technical details and evaluation," _White Paper. Al21 Labs_, vol. 1, 2021.\n' +
      '* [109] B. Kim, H. Kim, S. Lee, G. Lee, D. Kwak, D. H. Jeon, S. Park, S. Kim, S. Kim, D. Seo, H. Lee, M. Jeong, S. Lee, M. Kim, S. Ko, S. Kim, T. Park, J. Kim, S. Kang, N. Ryu, K. M. Yoo, M. Chang, S. Suh, S. In, J. Park, K. Kim, H. Kim, J. Jeong, Y. G. Yeo, D. Ham, D. Park, M. Y. Lee, J. Kang, I. Kang, J. Ha, W. Park, and N. Sung, "What changes can large-scale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers," in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cama, Dominican Republic, 7-11 November, 2021_. Association for Computational Linguistics, 2021.\n' +
      '* [109] S. Wu, X. Zhao, T. Yu, R. Zhang, C. Shen, H. Liu, F. Li, H. Zhu, J. Luo, L. Xu _et al._, "Yuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning," _arXiv preprint arXiv:2110.04725_, 2021.\n' +
      '* [110] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. Das-Sarma, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, J. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. B. Brown, J. Clark, S. McCandlish, C. Olah, and J. Kaplan, "A general language assistant as a laboratory for alignment," _CoRR_, vol. abs/2112.00861, 2021.\n' +
      '* [111] S. Wang, Y. Sun, Y. Xiang, Z. Wu, S. Ding, W. Gong, S. Feng, J. Shang, Y. Zhao, C. Pang, J. Liu, X. Chen, Y. Lu, W. Liu, X. Wang, Y. Bai, Q. Chen, L. Zhao, S. Li, P. Sun, D. Yu, Y. Ma, H. Tian, H. Wu, T. Wu, W. Zeng, G. Li, W. Gao, and H. Wang, "ERNIE 3.0 titan: Exploring larger-scale knowledge enhanced pre-training for language understanding and generation," _CoRR_, vol. abs/2112.12731, 2021.\n' +
      '* [112] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, B. Zoph, L. Fedus, M. P. Bosma, Z. Zhou, T. Wang, Y. E. Wang, K. Webster, M. Pellat, K. Robinson, K. S. Meier-Hellstern, T. Duke, L. Dixon, K. Zhang, Q. V. Le, Y. Wu, Z. Chen, and C. Cui, "Glam: Efficient scaling of language models with mixture-of-experts," in _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, 2022, pp. 5547-5569.\n' +
      '* [113] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti, E. Zheng, R. Child, R. Y. Aminabadi, J. Bernauer, X. Song, M. Shoeybi, Y. He, M. Houston, S. Tiwary, and B. Catanzaro, "Using deepspeed and megatron to train megatron-turing NLG 530b, A large-scale generative language model," _CoRR_, vol. abs/2201.11990, 2022.\n' +
      '* [114] Y. Li, D. H. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gilmeno, A. D. Lago, T. Hubert, P. Choy, C. de Masson d\'Autume, I. Babuschkin, X. Chen, P. Huang, J. Welbl, S. Gowal, A. Cherepanov, J. Molloy, D. J. Mankowitz, E. S. Robson, P. Kohli, N. de Freitas, K. Kavukcuoglu, and O. Vinyals, "Competition-level code generation with alphacode," _Science_, 2022.\n' +
      '* [115] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza, H. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky, C. S. Prakash, M. Sridhar, F. Triefenbach, A. Verma, G. Tur, and P. Natarajan, "Alexattn 20b: Few-shot learning using a large-scale multilingual seq2seq model," _CoRR_, vol. abs/2208.01448, 2022.\n' +
      '* [116] A. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, L. Campbell-Gillingham, J. Uessato, P. Huang, R. Comanescu, F. Yang, A. See, S. Dathathtri, R. Greig, C. Chen, D. Fritz, J. S. Elias, R. Green, S. Mokra, N. Fernando, B. Wu, R. Foley, S. Young, I. Gabriel, W. Isaac, J. Mellor, D. Hassabis, K. Kavukcuoglu, L. A. Hendricks, and G. Irving, "Improving alignment of dialogue agents via targeted human judgements," _CoRR_, vol. abs/2209.14375, 2022.\n' +
      '* [117] H. Su, X. Zhou, H. Yu, Y. Chen, Z. Zhu, Y. Yu, and J. Zhou, "Welm: A well-read pre-trained language model for chinese," _CoRR_, vol. abs/2209.10372, 2022.\n' +
      '* [118] Y. Tay, J. Wei, H. W. Chung, V. Q. Tran, D. R. So, S. Shakeri, X. Garcia, H. S. Zheng, J. Rao, A. Chowdhery, D. Zhou, D. Metzler, S. Petrov, N. Houlsby, Q. V. Le, and M. Dehghani, "Transcending scaling laws with 0.1% extra compute," _CoRR_, vol. abs/2210.11399, 2022.\n' +
      '* [119] X. Ren, P. Zhou, X. Meng, X. Huang, Y. Wang, W. Wang, P. Li, X. Zhang, A. Podolskiy, G. Arshinov, A. Bout, I. Piontkovskaya, J. Wei, X. Jiang, T. Su, Q. Liu, and J. Yao, "Pangu-\\(\\Sigma\\): Towards trillion parameter language model with sparse heterogeneous computing," _CoRR_, vol. abs/2303.10845, 2023.\n' +
      '* [120] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen _et al._, "Palm 2. technical report," _arXiv preprint arXiv:2305.10403_, 2023.\n' +
      '* [121] A. Radford, R. Jozefowicz, and I. Sutskever, "Learning to generate reviews and discovering sentiment," _CoRR_, vol. abs/1704.01444, 2017.\n' +
      '* [122] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever _et al._, "Improving language understanding by generative pre-training," 2018.\n' +
      '* [123] B. McCann, N. S. Keskar, C. Xiong, and R. Socher, "The natural language decathlon: Multitask learning as question answering," _CoRR_, vol. abs/1806.08730, 2018.\n' +
      '* [124] Y. Zhang, S. Sun, M. Galley, Y. Chen, C. Brockett, X. Gao, J. Gao, J. Liu, and B. Dolan, "DIALOGPT : Large-scale generative pre-training for conversational response generation," in _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2020, Online, July 5-10, 2020_, A. Celikyilmaz and T. Wen, Eds. Association for Computational Linguistics, 2020, pp. 270-278.\n' +
      '* [125] D. Ham, J. Lee, Y. Jang, and K. Kim, "End-to-end neural pipeline for goal-oriented dialogue systems using GPT-2," in _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_. Association for Computational Linguistics, 2020, pp. 583-592.\n' +
      '* [126] I. Drori, S. Tran, R. Wang, N. Cheng, K. Liu, L. Tang, E. Ke, N. Singh, T. L. Patti, J. Lynch, A. Shporer, N. Verma, E. Wu, and G. Strang, "A neural network solves and generates mathematics problems by program synthesis: Calculus, differential equations, linear algebra, and more," _CoRR_, vol. abs/2112.15594, 2021.\n' +
      '* [127] A. Neelakantan, T. Xu, R. Puri, A. Radford, J. M. Han, J. Tworek, Q. Yuan, N. Tezak, J. W. Kim, C. Hallacy, J. Heidecke, P. Shyam, B. Power, T. E. Nekoul,G. Sastry, G. Krueger, D. Schnurr, F. P. Such, K. Hsu, M. Thompson, T. Khan, T. Sherbakov, J. Jang, P. Welinder, and L. Weng (2022) Text and code embeddings by contrastive pre-training. CoRRabs/2201.10005. External Links: Link, 2012.10005 Cited by: SS1.\n' +
      '*[130]J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017) Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Cited by: SS1.\n' +
      '*[131]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[132]S. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017) Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Cited by: SS1.\n' +
      '*[133]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[134]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[135]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[136]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[137]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[138]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[139]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[140]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[141]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[142]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[143]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[144]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[145]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[146]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[147]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[148]S. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017) Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Cited by: SS1.\n' +
      '*[149]S. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017) Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Cited by: SS1.\n' +
      '*[150]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[151]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[152]S. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017) Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Cited by: SS1.\n' +
      '*[153]S. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017) Improving and optimizing the performance of deep neural networks. CoRRabs/2010.0005. External Links: Link, 1709.01325 Cited by: SS1.\n' +
      '*[154]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[155]S. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017) Improving and optimizing the performance of deep neural networks. CoRRabs/2009.01325. External Links: Link, 1709.01325 Cited by: SS1.\n' +
      '*[156]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[157]S. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017) Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Cited by: SS1.\n' +
      '*[158]S. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017) Improving and optimizing the performance of deep neural networks. CoRRabs/2009.01325. External Links: Link, 1709.01325 Cited by: SS1.\n' +
      '*[159]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[160]S. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017) Improving and optimizing the performance of deep neural networks. CoRRabs/2009.01325. External Links: Link, 1709.01325 Cited by: SS1.\n' +
      '*[161]S. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017) Improving and optimizing the performance of deep neural networks. CoRRabs/2009.01325. External Links: Link, 1709.01325 Cited by: SS1.\n' +
      '*[162]S. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017) Improving and optimizing the performance of deep neural networks. CoRRabs/2009.01325. External Links: Link, 1709.01325 Cited by: SS1.\n' +
      '*[163]S. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017) Improving and optimizing the performance of deep neural networks. CoRRabs/2009.01325. External Links: Link, 1709.01325 Cited by: SS1.\n' +
      '*[164]S. 슐만 오양정우 Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020) Learning to summarize from human feedback. CoRRabs/2009.01325. External Links: Link, 2009.01325 Cited by: SS1.\n' +
      '*[165]S. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017) Improving and optimizing the performance of deep neural networksOpenWebTextCorpus, 2019.\n' +
      '* [158] J. Baumgartner, S. Zannettou, B. Keegan, M. Squire, and J. Blackburn, "The pushshift reddit dataset," in _Proceedings of the Fourteenth International AAAI Conference on Web and Social Media, ICWSM 2020, Held virtually, Original Venue: Atlanta, Georgia, USA, June 8-11, 2020_. AAAI Press, 2020, pp. 830-839.\n' +
      '* [159] "Wikipedia." [Online]. Available: [https://en.wikipedia.org/wiki/Main_Page](https://en.wikipedia.org/wiki/Main_Page)\n' +
      '* [160] "Bigquery dataset." [Online]. Available: [https://cloud.google.com/bigquery?hl=zh-cn](https://cloud.google.com/bigquery?hl=zh-cn)\n' +
      '* [161] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy, "The pile: An 800gb dataset of diverse text for language modeling," _CoRR_, vol. abs/2101.00027, 2021.\n' +
      '* [162] H. Laurencon, L. Saulnier, T. Wang, C. Akiki, A. V. del Moral, T. Le Scao, L. Von Werra, C. Mou, E. G. Ponferrada, H. Nguyen _et al._, "The bigscience roots corpus: A 1.6 tb composite multilingual dataset," in _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022.\n' +
      '* [163] "Common crawl." [Online]. Available: [https://commoncrawl.org/](https://commoncrawl.org/)\n' +
      '* [164] "A reproduction version of cc-stories on hugging face." [Online]. Available: [https://huggingface.co/datasets/spacemanidol/cc-stories](https://huggingface.co/datasets/spacemanidol/cc-stories)\n' +
      '* [165] B. Wang and A. Komatsuzaki, "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model," [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax), 2021.\n' +
      '* [166] S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi, "Cross-task generalization via natural language crowdsourcing instructions," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, S. Muresan, P. Nakov, and A. Villavicencio, Eds., 2022, pp. 3470-3487.\n' +
      '* [167] S. H. Bach, V. Sanh, Z. X. Yong, A. Webson, C. Raffel, N. V. Nayak, A. Sharma, T. Kim, M. S. Bari, T. Fevry, Z. Alyafeai, M. Dey, A. Santilli, Z. Sun, S. Ben-David, C. Xu, G. Chhablani, H. Wang, J. A. Fries, M. S. AlShaibani, S. Sharma, U. Thakker, K. Almubarak, X. Tang, D. R. Radev, M. T. Jiang, and A. M. Rush, "Promptsource: An integrated development environment and repository for natural language prompts," in _ACL (demo)_. Association for Computational Linguistics, 2022, pp. 93-104.\n' +
      '* [168] T. Tang, J. Li, W. X. Zhao, and J. Wen, "MVP: multi-task supervised pre-training for natural language generation," _CoRR_, vol. abs/2206.12131, 2022.\n' +
      '* [169] H. Nguyen, S. Suri, K. Tsui, Shahules786, T. team, and C. Schuhmann, "The oig dataset," [https://laion.ai/blog/oig-dataset/](https://laion.ai/blog/oig-dataset/), 2023.\n' +
      '* [170] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. E. Showk, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson, D. Amodei, T. B. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann, and J. Kaplan, "Training a helpful and harmless assistant with reinforcement learning from human feedback," _CoRR_, vol. abs/2204.05862, 2022. [Online]. Available: [https://doi.org/10.48550/arXiv.2204.05862](https://doi.org/10.48550/arXiv.2204.05862)\n' +
      '* [171] B. Guo, X. Zhang, Z. Wang, M. Jiang, J. Nie, Y. Ding, J. Yue, and Y. Wu, "How close is chatgpt to human experts? comparison corpus, evaluation, and detection," _arXiv preprint arXiv:2301.07597_, 2023.\n' +
      '* [172] M. Conover, M. Hayes, A. Mathur, J. Xie, J. Wan, S. Shah, A. Ghodsi, P. Wendell, M. Zaharia, and R. Xin. (2023) Free dolly: Introducing the world\'s first truly open instruction-tuned llm.\n' +
      '* [173] A. Kopf, Y. Kilcher, D. von Rutte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M. Duc, O. Stanley, R. Nagyfi _et al._, "Openassistant conversations-democratizing large language model alignment," _arXiv preprint arXiv:2304.07327_, 2023.\n' +
      '* 자연 언어 적응형 상황 인식 omnilingual 출력에 대 한 생성 범용 보조 장치 [https://guanaco-model.github.io/](https://guanaco-model.github.io/), 2023입니다.\n' +
      '* [175] C. Xu, D. Guo, N. Duan, and J. McAuley, "Baize: An open-source chat model with parameter-efficient tuning on self-chat data," _arXiv preprint arXiv:2304.01196_, 2023.\n' +
      '* [176] Y. Ji, Y. Gong, Y. Deng, Y. Peng, Q. Niu, B. Ma, and X. Li, "Towards better instruction following language models for chinese: Investigating the impact of training data and evaluation," _arXiv preprint arXiv:2304.07854_, 2023.\n' +
      '* [177] K. Ethayarajh, Y. Choi, and S. Swayamdipta, "Understanding dataset difficulty with \\(\\mathcal{V}\\)-usable information," in _Proceedings of the 39th International Conference on Machine Learning_, 2022, pp. 5988-6008.\n' +
      '* [178] N. Lambert, L. Tunstall, N. Rajani, and T. Thrush. (2023) Huggingface h4 stack exchange preference dataset. [Online]. Available: [https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences](https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences)\n' +
      '* [179] R. Liu, R. Yang, C. Jia, G. Zhang, D. Zhou, A. M. Dai, D. Yang, and S. Vosoughi, "Training socially aligned language models in simulated human society," _CoRR_, vol. abs/2305.16960, 2023.\n' +
      '* [180] G. Xu, J. Liu, M. Yan, H. Xu, J. Si, Z. Zhou, P. Yi, X. Gao, J. Sang, R. Zhang, J. Zhang, C. Peng, F. Huang, and J. Zhou, "Cvalues: Measuring the values of chinese large language models from safety to responsibility," 2023.\n' +
      '* [181] J. Dai, X. Pan, R. Sun, J. Ji, X. Xu, M. Liu, Y. Wang, and Y. Yang, "Safe rlhf: Safe reinforcement learning from human feedback," _arXiv preprint arXiv:2310.12773_, 2023.\n' +
      '* [182] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, A. Raja, M. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chhablani, N. V. Nayak, D. Datta, J. Chang, M. T. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey, R. Bawdden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Fevry, J. A. Fries, R. Teehan, T. L. Scao, S. Biderman, L. Gao, T. Wolf, and A. M. Rush, "Multitask prompted training enables zero-shot task generalization," in _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.\n' +
      '* [183] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei _et al._, "The fan collection: Designing data and methods for effective instruction tuning," _arXiv preprint arXiv:2301.13688_, 2023.\n' +
      '* [184] K. Cobbe, V. Kosaraju, M. Bavarian, J. Hilton, R. Nakano, C. Hesse, and J. Schulman, "Training verifiers to solve math word problems," _CoRR_, vol. abs/2110.14168, 2021.\n' +
      '* [185] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant, "Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies," _Trans. Assoc. Comput. Linguistics_, vol. 9, pp. 346-361, 2021.\n' +
      '* [186] O. Camburu, B. Shillingford, P. Minervini, T. Lukasiewicz, and P. Blunsom, "Make up your mind! adversarial generation of inconsistent natural language explanations," in _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_, D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault, Eds. Association for Computational Linguistics, 2020, pp. 4157-4165.\n' +
      '* Demos, Online, November 16-20, 2020_. Association for Computational Linguistics, 2020, pp. 38-45.\n' +
      '* [188] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang, "JAX: composable transformations of Python+NumPy programs," 2018. [Online]. Available: [http://github.com/google/jax](http://github.com/google/jax)\n' +
      '* [189] Z. Bian, H. Liu, B. Wang, H. Huang, Y. Li, C. Wang, F. Cui, and Y. You, "Colossal-ai: A unified deep learning system for large-scale parallel training," _CoRR_, vol. abs/2110.14883, 2021.\n' +
      '* [190] J. Fang, Y. Yu, S. Li, Y. You, and J. Zhou, "Patrickar: Parallel training of pre-trained models via a chunk-based memory management," _CoRR_, vol. abs/2108.05818, 2021.\n' +
      '* [191] "Bmtrain: Effient training for big models." [Online]. Available: [https://github.com/OpenBMB/BMTrain](https://github.com/OpenBMB/BMTrain)\n' +
      '* [192] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, and J. Tang, "Fastmoe: A fast mixture-of-expert training system," _CoRR_, vol. abs/2103.13262, 2021.\n' +
      '* [193] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica, "Efficient memory management for large language model serving with pagedattention," in _Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles_, 2023.\n' +
      '* [194] (2023) Deepspeed-mi. [Online]. Available: [https://github.com/microsoft/DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII)\n' +
      '* [195] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulinier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed, "Mistral 7b," 2023.\n' +
      '* [196] Z. Yao, R. Y. Aminabadi, O. Ruwase, S. Rajbhandari, X. Wu, A. A. Awan, J. Rasley, M. Zhang, C. Li, C. Holmes, Z. Zhou, M. Wyatt, M. Smith, L. Kurilenko, H. Qin, M. Tanaka, S. Che, S. L. Song, and Y. He, "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales," _arXiv preprint arXiv:2308.01320_, 2023.\n' +
      '* [197] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, "Pytorch: An imperative style, high-performance deep learning library," in _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d\'Alche-Buc, E. B. Fox, and R. Garnett, Eds., 2019, pp. 8024-8035.\n' +
      '* [198] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. A. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng, "Tensorflow: A system for large-scale machine learning," in _12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016, Savannah, GA, USA, November 2-4, 2016_, K. Keeton and T. Roscoe, Eds. USENIX Association, 2016, pp. 265-283.\n' +
      '* [199] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang, "Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems," _CoRR_, vol. abs/1512.01274, 2015.\n' +
      '* [200] Y. Ma, D. Yu, T. Wu, and H. Wang, "Paddlepaddle: An open-source deep learning platform from industrial practice," _Frontiers of Data and Computing_, vol. 1, no. 1, p. 105, 2019.\n' +
      '* [201] J. Yuan, X. Li, C. Cheng, J. Liu, R. Guo, S. Cai, C. Yao, F. Yang, X. Yi, C. Wu, H. Zhang, and J. Zhao, "One-flow: Redesign the distributed deep learning framework from scratch," _CoRR_, vol. abs/2110.15032, 2021.\n' +
      '*23, 2021_, 2021, pp. 300-325.\n' +
      '* [203] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and V. Misra, "Solving quantitative reasoning problems with language models," _CoRR_, vol.\n' +
      '\n' +
      'abs/2206.14858, 2022.\n' +
      '* [204] T. Saier, J. Krause, and M. Farber, "unaxtive 2022: All arxiv publications pre-processed for nlp, including structured full-text and citation network," _arXiv preprint arXiv:2303.14957_, 2023.\n' +
      '* [205] H. A. Simon, "Experiments with a heuristic compiler," _J. ACM_, vol. 10, no. 4, pp. 493-506, 1963.\n' +
      '* [206] Z. Manna and R. J. Waldinger, "Toward automatic program synthesis," _Commun. ACM_, vol. 14, no. 3, pp. 151-165, 1971.\n' +
      '* [207] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, and M. Zhou, "Codebert: A pre-trained model for programming and natural languages," in _Findings of EMNLP_, 2020.\n' +
      '* [208] J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. J. Cai, M. Terry, Q. V. Le, and C. Sutton, "Program synthesis with large language models," _CoRR_, vol. abs/2108.07732, 2021.\n' +
      '* [209] S. Black, L. Gao, P. Wang, C. Leahy, and S. Biderman, "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow," 2021.\n' +
      '* [210] F. F. Xu, U. Alon, G. Neubig, and V. J. Hellendoorn, "A systematic evaluation of large language models of code," in _MAPS@PLDI_, 2022.\n' +
      '* [211] A. Madaan, S. Zhou, U. Alon, Y. Yang, and G. Neubig, "Language models of code are few-shot commonsense learners," in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Eminates, December 7-11, 2022, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds. Association for Computational Linguistics_, 2022, pp. 1384-1403.\n' +
      '* [212] S. Longpre, G. Yauney, E. Reif, K. Lee, A. Roberts, B. Zoph, D. Zhou, J. Wei, K. Robinson, D. Mimno _et al._, "A pretrainer\'s guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity," _arXiv preprint arXiv:2305.13169_, 2023.\n' +
      '* [213] D. Chen, Y. Huang, Z. Ma, H. Chen, X. Pan, C. Ge, D. Gao, Y. Xie, Z. Liu, J. Gao, Y. Li, B. Ding, and J. Zhou, "Data-juicer: A one-stop data processing system for large language models," 2023.\n' +
      '* [214] D. Hernandez, T. B. Brown, T. Conerly, N. DasSarma, D. Drain, S. E. Showk, N. Elhage, Z. Hatfield-Dodds, T. Henighan, T. Hume, S. Johnston, B. Mann, C. Olah, C. Olsson, D. Amodei, N. Joseph, J. Kaplan, and S. McCandlish, "Scaling laws and interpretability of learning from repeated data," _CoRR_, vol. abs/2205.10487, 2022.\n' +
      '* [215] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, "The curious case of neural text degeneration," in _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.\n' +
      '* [216] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini, "Deduplicating training data makes language models better," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, 2022, pp. 8424-8445.\n' +
      '* [217] N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang, "Quantifying memorization across neural language models," _CoRR_, 2022.\n' +
      '* [218] N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. B. Brown, D. Song, U. Erlingsson, A. Oprea, and C. Raffel, "Extracting training data from large language models," in _30th IJCSENIX Security Symposium, USENIX Security 2021, August 11-13, 2021_, 2021, pp. 2633-2650.\n' +
      '* [219] N. Kandpal, E. Wallace, and C. Raffel, "Deduplicating training data mitigates privacy risks in language models," in _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA._ PMLR, 2022, pp. 10 697-10 707.\n' +
      '* July 1, 2001_, C. E. Brodley and A. P. Danyluk, Eds. Morgan Kaufmann, 2001, pp. 282-289.\n' +
      '* [221] P. Gage, "A new algorithm for data compression," C _Users Journal_, vol. 12, no. 2, pp. 23-38, 1994.\n' +
      '* [222] R. Sennrich, B. Haddow, and A. Birch, "Neural machine translation of rare words with subword units," in _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers_. The Association for Computer Linguistics, 2016.\n' +
      '* [223] M. Schuster and K. Nakajima, "Japanese and korean voice search," in _2012 IEEE international conference on acoustics, speech and signal processing (ICASSP)_. IEEE, 2012, pp. 5149-5152.\n' +
      '* [224] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu, L. Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes, and J. Dean, "Google\'s neural machine translation system: Bridging the gap between human and machine translation," _CoRR_, vol. abs/1609.08144, 2016.\n' +
      '* [225] T. Kudo, "Subword regularization: Improving neural network translation models with multiple subword candidates," in _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume I: Long Papers_, I. Gurevych and Y. Miyao, Eds. Association for Computational Linguistics, 2018, pp. 66-75.\n' +
      '* 2018년 11월 4일_, E. Blanco and W. Lu, Eds. Association for Computational Linguistics, 2018.\n' +
      '* [227] M. Davis and M. Durst, "Unicode normalization forms," 2001.\n' +
      '* [228] P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak,and I. Sutskever, "Deep double descent: Where bigger models and more data hurt," in _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.\n' +
      '* [229] K. Tirumala, D. Simig, A. Aghajanyan, and A. S. Morcos, "D4: Improving llm pretraining via document de-duplication and diversification," _arXiv preprint arXiv:2308.12284_, 2023.\n' +
      '* [230] Z. Shen, T. Tao, L. Ma, W. Neiswanger, J. Hestness, N. Vassilieva, D. Soboleva, and E. Xing, "Slimpajama-dc: Understanding data combinations for llm training," _arXiv preprint arXiv:2309.10818_, 2023.\n' +
      '* [231] S. M. Xie, S. Santurkar, T. Ma, and P. Liang, "Data selection for language models via importance resampling," _arXiv preprint arXiv:2302.03169_, 2023.\n' +
      '* [232] X. Wang, W. Zhou, Q. Zhang, J. Zhou, S. Gao, J. Wang, M. Zhang, X. Gao, Y. Chen, and T. Gui, "Farewell to amless large-scale pretraining: Influential subset selection for language model," _arXiv preprint arXiv:2305.12816_, 2023.\n' +
      '* [233] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and R. Fernandez, "The LAMBADA dataset: Word prediction requiring a broad discourse context," in _ACL (1)_. The Association for Computer Linguistics, 2016.\n' +
      '* [234] M. F. Chen, N. Roberts, K. Bhatia, J. Wang, C. Zhang, F. Sala, and C. Re, "Skill-itl a data-driven skills framework for understanding and training language models," _arXiv preprint arXiv:2307.14430_, 2023.\n' +
      '* [235] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. Canton-Ferrer, A. Grattafiori, W. Xiong, A. Defossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and G. Synnaeve, "Code llama: Open foundation models for code," _CoRR_, vol. abs/2308.12950, 2023.\n' +
      '* [236] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, "Curriculum learning," in _ICML_, 2009, pp. 41-48.\n' +
      '* [237] C. Xu, C. Rosset, L. Del Corro, S. Mahajan, J. McAuley, J. Neville, A. H. Awadallah, and N. Rao, "Contrastive post-training large language models on data curriculum," _arXiv preprint arXiv:2310.02263_, 2023.\n' +
      '* [238] S. Tworkowski, K. Staniszewski, M. Pacek, Y. Wu, H. Michalewski, and P. Milos, "Focused transformer: Contrastive training for context scaling," _CoRR_, vol. abs/2307.03170, 2023.\n' +
      '* [239] Z. Azerbayev, H. Schoelkopf, K. Paster, M. D. Santos, S. McAleer, A. Q. Jiang, J. Deng, S. Biderman, and S. Welleck, "Llemma: An open language model for mathematics," _arXiv preprint arXiv:2310.10631_, 2023.\n' +
      '* [240] S. Chen, S. Wong, L. Chen, and Y. Tian, "Extending context window of large language models via positional interpolation," _CoRR_, vol. abs/2306.15595, 2023.\n' +
      '* [241] G. Wenzek, M.-A. Lachaux, A. Conneau, V. Chaudhary, F. Guzman, A. Joulin, and E. Grave, "Cnet: Extracting high quality monolingual datasets from web crawl data," in _Proceedings of the Twelfth Language Resources and Evaluation Conference_, 2020, pp. 4003-4012.\n' +
      '* [242] A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov, "Bag of tricks for efficient text classification," in _EACL_, 2017, pp. 427-431.\n' +
      '* [243] D. Chen, Y. Huang, Z. Ma, H. Chen, X. Pan, C. Ge, D. Gao, Y. Xie, Z. Liu, J. Gao _et al._, "Data-juicer: A one-step data processing system for large language models," _arXiv preprint arXiv:2309.02033_, 2023.\n' +
      '* [244] B. Zhang, B. Ghorbani, A. Bapna, Y. Cheng, X. Garcia, J. Shen, and O. Firat, "Examining scaling and transfer of language model architectures for machine translation," in _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, 2022, pp. 26 176-26 192.\n' +
      '* [245] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H. Hon, "Unified language model pre-training for natural language understanding and generation," in _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, 2019, pp. 13 042-13 054.\n' +
      '* [246] A. Clark, D. de Las Casas, A. Guy, A. Mensch, M. Paganini, J. Hoffmann, B. Damoc, B. A. Hechtman, T. Cai, S. Borgeaud, G. van den Driessche, E. Rutherford, T. Hennigan, M. J. Johnson, A. Cassiner, C. Jones, E. Buchatskaya, D. Budden, L. Sifre, S. Osindero, O. Vinyals, M. Ranzato, J. W. Rae, E. Elsen, K. Kavukcuoglu, and K. Simonyan, "Unified scaling laws for routed language models," in _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, 2022, pp. 4057-4086.\n' +
      '* [247] A. Gu, K. Goel, and C. Re, "Efficiently modeling long sequences with structured state spaces," in _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022. [Online]. Available: [https://openreview.net/forum?id=uYLFoz1vlAC](https://openreview.net/forum?id=uYLFoz1vlAC)\n' +
      '* [248] H. Mehta, A. Gupta, A. Cutkosky, and B. Neyshabur, "Long range language modeling via gated state spaces," _CoRR_, vol. abs/2206.13947, 2022. [Online]. Available: [https://doi.org/10.48550/arXiv.2206.13947](https://doi.org/10.48550/arXiv.2206.13947)\n' +
      '* [249] T. Dao, D. Y. Fu, K. K. Saab, A. W. Thomas, A. Rudra, and C. Re, "Hungry hungry hungry hippos: Towards language modeling with state space models," _CoRR_, vol. abs/2212.14052, 2022. [Online]. Available: [https://doi.org/10.48550/arXiv.2212.14052](https://doi.org/10.48550/arXiv.2212.14052)\n' +
      '* [250] M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and C. Re, "Hyena hierarchy: Towards larger convolutional language models," in _ICML_, 2023.\n' +
      '* [251] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao, X. Cheng, M. Chung, M. Grella, K. K. G. V., X. He, H. Hou, P. Kazienko, J. Kocon, J. Kong, B. Koptyra, H. Lau, K. S. I. Mantri, F. Mom, A. Saito, X. Tang, B. Wang, J. S. Wind, S. Wozniak, R. Zhang, Z. Zhang, Q. Zhao, P. Zhou, J. Zhu, and R. Zhu, "RWK: reinventire rms for the transformer era," _CoRR_, vol. abs/2305.13048, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2305.13048](https://doi.org/10.48550/arXiv.2305.13048)\n' +
      '* [252] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue,\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:97]\n' +
      '\n' +
      'nov, and Q. V. Le, "Xlnet: Generalized autoregressive pretraining for language understanding," _Advances in neural information processing systems_, vol. 32, 2019.\n' +
      '* [276] B. Peng, J. Quesnelle, H. Fan, and E. Shippole, "Yarm: Efficient context window extension of large language models," _CoRR_, vol. abs/2309.00071, 2023.\n' +
      '* [277] Y. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benham, V. Chaudhary, X. Song, and F. Wei, "A length-extrapolatable transformer," _CoRR_, vol. abs/2212.10554, 2022. [Online]. Available: [https://doi.org/10.48550/arXiv.2212.10554](https://doi.org/10.48550/arXiv.2212.10554)\n' +
      '* [278] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and L. Kong, "Random feature attention," in _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_.\n' +
      '* [279] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang, and A. Ahmed, "Big bird: Transformers for longer sequences," in _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.\n' +
      '* [280] R. Child, S. Gray, A. Radford, and I. Sutskever, "Generating long sequences with sparse transformers," _CoRR_, vol. abs/1904.10509, 2019.\n' +
      '* [281] N. Shazeer, "Fast transformer decoding: One write-head is all you need," _CoRR_, vol. abs/1911.02150, 2019. [Online]. Available: [http://arxiv.org/abs/1911.02150](http://arxiv.org/abs/1911.02150)\n' +
      '* [282] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebron, and S. Sanghai, "Gqa: Training generalized multi-query transformer models from multi-head checkpoints," _arXiv preprint arXiv:2305.13245_, 2023.\n' +
      '* [283] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Re, "Flashattention: Fast and memory-efficient exact attention with IO-awareness," in _NeurIPS_, 2022.\n' +
      '* [284] T. Dao, "Flashattention-2: Faster attention with better parallelism and work partitioning," _arXiv preprint arXiv:2307.08691_, 2023.\n' +
      '* [285] "vllm: Easy, fast, and cheap llm serving with pagedattention." [Online]. Available: [https://vllm.ai/](https://vllm.ai/)\n' +
      '* [286] A. Yuan, A. Coenen, E. Reif, and D. Ippolito, "Wordcraft: story writing with large language models," in _27th International Conference on Intelligent User Interfaces_, 2022, pp. 841-852.\n' +
      '* [287] A. Kazemmejad, I. Padhi, K. N. Ramamurthy, P. Das, and S. Reddy, "The impact of positional encoding on length generalization in transformers," _CoRR_, vol. abs/2305.19466, 2023.\n' +
      '* [288] W. Xiong, J. Liu, I. Molybog, H. Zhang, P. Bhargava, R. Hou, L. Martin, R. Rungta, K. A. Sankararaman, B. Oguz, M. Khabsa, H. Fang, Y. Mehdad, S. Narang, K. Malik, A. Fan, S. Bhosale, S. Edunov, M. Lewis, S. Wang, and H. Ma, "Effective long-context scaling of foundation models," _CoRR_, vol. abs/2309.16039, 2023.\n' +
      '* [289] kaikendev, "Things I\'m learning while training superphot." 2023.\n' +
      '* [290] Z. Dong, T. Tang, J. Li, W. X. Zhao, and J. Wen, "BAMBOO: A comprehensive benchmark for evaluating long text modeling capacities of large language models," _CoRR_, vol. abs/2309.13345, 2023.\n' +
      '* [291] J. Su. (2023) Transformer upgrade path: 12, infinite extrapolation of rerope?\n' +
      '* [292] X. Liu, H. Yan, S. Zhang, C. An, X. Qiu, and D. Lin, "Scaling laws of rope-based extrapolation," _CoRR_, vol. abs/2310.05209, 2023.\n' +
      '* [293] A. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, and S. Naidu, "Giraffe: Adventures in expanding context lengths in lms," _CoRR_, vol. abs/2308.10882, 2023.\n' +
      '*23, 2021_. Association for Computational Linguistics, 2021, pp. 874-880.\n' +
      '* [295] N. Ratner, Y. Levine, Y. Belinkov, O. Ram, I. Magar, O. Abend, E. Karpas, A. Shashua, K. Leyton-Brown, and Y. Shoham, "Parallel context windows for large language models," in _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023_. Association for Computational Linguistics, 2023, pp. 6383-6402.\n' +
      '* [296] Y. Hao, Y. Sun, L. Dong, Z. Han, Y. Gu, and F. Wei, "Structured prompting: Scaling in-context learning to 1, 000 examples," _CoRR_, 2022.\n' +
      '* [297] I. Beltagy, M. E. Peters, and A. Cohan, "Longformer: The long-document transformer," _CoRR_, vol. abs/2004.05150, 2020.\n' +
      '* [298] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis, "Efficient streaming language models with attention sinks," _CoRR_, vol. abs/2309.17453, 2023.\n' +
      '* [300] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang, "Lost in the middle: How language models use long contexts," _CoRR_, vol. abs/2307.03172, 2023.\n' +
      '* [301] C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, and S. Wang, "Lm-infinite: Simple on-the-fly length generalization for large language models," _CoRR_, vol. abs/2308.16137, 2023.\n' +
      '* [302] A. Bertsch, U. Alon, G. Neubig, and M. R. Gormley, "Unlimiformer: Long-range transformers with unlimited length input," _CoRR_, vol. abs/2305.01625, 2023.\n' +
      '* [303] Y. Wu, M. N. Rabe, D. Hutchins, and C. Szegedy, "Memorizing transformers," in _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.\n' +
      '* [304] H. Chen, R. Pasunuru, J. Weston, and A. Celikyilmaz, "Walking down the memory maze: Beyond context limit through interactive reading," _CoRR_, vol. abs/2310.05029, 2023.\n' +
      '* [305] W. Zhou, Y. E. Jiang, P. Cui, T. Wang, Z. Xiao, Y. Hou, R. Cotterell, and M. Sachan, "Recurrentopt: Interactive generation of (arbitrarily) long text," _CoRR_, vol. abs/2305.13304, 2023.\n' +
      '* [306] C. Packer, V. Fang, S. G. Patil, K. Lin, S. Wooders, and J. E. Gonzalez, "Memgpt: Towards lms as operating systems," _CoRR_, vol. abs/2310.08560, 2023.\n' +
      '* [307] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian, E. Bakhturina, M. Shoeybi, and B. Catanzaro, "Retrieval meets long context large language models," _CoRR_, vol. abs/2310.03025, 2023.\n' +
      '* [307] K. Murray and D. Chiang, "Correcting length bias in neural machine translation," in _WMT_. Association for Computational Linguistics, 2018, pp. 212-223.\n' +
      '* [308] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, "The curious case of neural text degeneration," in _ICLR_, 2020.\n' +
      '* [309] C.-M. U. P. P. D. O. C. SCIENCE, _Speech Understanding Systems. Summary of Results of the Five-Year Research Effort at Carnegie-Mellon University_, 1977.\n' +
      '* [310] P. Koehn and R. Knowles, "Six challenges for neural machine translation," in _NMT@ACL_. Association for Computational Linguistics, 2017, pp. 28-39.\n' +
      '* [311] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikum, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu, L. Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes, and J. Dean, "Google\'s neural machine translation system: Bridging the gap between human and machine translation," _CoRR_, vol. abs/1609.08144, 2016.\n' +
      '* [312] R. Paulus, C. Xiong, and R. Socher, "A deep reinforced model for abstractive summarization," in _ICLR (Poster)_. OpenReview.net, 2018.\n' +
      '* [313] A. K. Vijayakumar, M. Cogswell, R. R. Selvaraju, Q. Sun, S. Lee, D. J. Crandall, and D. Batra, "Diverse beam search: Decoding diverse solutions from neural sequence models," _CoRR_, vol. abs/1610.02424, 2016.\n' +
      '* [314] A. Fan, M. Lewis, and Y. N. Dauphin, "Hierarchical neural story generation," in _ACL (1)_. Association for Computational Linguistics, 2018, pp. 889-898.\n' +
      '* [315] J. Hewitt, C. D. Manning, and P. Liang, "Truncation sampling as language model desmoothing," in _EMNLP (Findings)_. Association for Computational Linguistics, 2022, pp. 3414-3427.\n' +
      '* [316] Y. Su, T. Lan, Y. Wang, D. Yogatama, L. Kong, and N. Collier, "A contrastive framework for neural text generation," in _NeurIPS_, 2022.\n' +
      '* [317] C. Meister, T. Pimentel, G. Wier, and R. Cotterell, "Locally typical sampling," _Trans. Assoc. Comput. Linguistics_, 2023.\n' +
      '* [318] X. L. Li, A. Holtzman, D. Fried, P. Liang, J. Eisner, T. Hashimoto, L. Zettlemoyer, and M. Lewis, "Contrastive decoding: Open-ended text generation as optimization," in _ACL (1)_. Association for Computational Linguistics, 2023, pp. 12 286-12 312.\n' +
      '* [319] Y. Chuang, Y. Xie, H. Luo, Y. Kim, J. R. Glass, and P. He, "Dola: Decoding by contrasting layers improves factuality in large language models," _CoRR_, vol. abs/2309.03883, 2023.\n' +
      '* [320] L. Chen, "Dissecting batching effects in gpt inference," 2023. [Online]. Available: [https://le.qun.ch/en/blog/2023/05/13/transformer-batching/](https://le.qun.ch/en/blog/2023/05/13/transformer-batching/)\n' +
      '* [321] Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang, C. Re, I. Stoica, and C. Zhang, "Flexgen: High-throughput generative inference of large language models with a single GPU," in _ICML_, ser. Proceedings of Machine Learning Research, vol. 202. PMLR, 2023, pp. 31 094-31 116.\n' +
      '* [322] T. Dao, D. Haziza, F. Massa, and G. Sizov, "Flash-decoding for long-context inference," [https://crfm.stanford.edu/2023/10/12/flashdecoding.html](https://crfm.stanford.edu/2023/10/12/flashdecoding.html), 2023.\n' +
      '* [323] Y. Leviathan, M. Kalman, and Y. Matias, "Fast inference from transformers via speculative decoding," in _International Conference on Machine Learning_, 2023.\n' +
      '* [324] C. Chen, S. Borgeaud, G. Irving, J. Lespiau, L. Sifre, and J. Jumper, "Accelerating large language model decoding with speculative sampling," _CoRR_, vol. abs/2302.01318, 2023.\n' +
      '* [325] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, R. Y. Y. Wong, Z. Chen, D. Arfeen, R. Abhyankar, and Z. Jia, "Specinfer: Accelerating generative LLM serving with speculative inference and token tree verification," _CoRR_, vol. abs/2305.09781, 2023.\n' +
      '* [326] B. Spector and C. Re, "Accelerating LLM inference with staged speculative decoding," _CoRR_, vol. abs/2308.04623, 2023.\n' +
      '* [327] L. D. Corro, A. D. Giorno, S. Agarwal, B. Yu, A. H. Awadallah, and S. Mukherjee, "Skipdecode: Autoregressive skip decoding with batching and caching for efficient LLM inference," _CoRR_, vol. abs/2307.02628, 2023.\n' +
      '* [328] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," in _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, Y. Bengio and Y. LeCun, Eds., 2015.\n' +
      '* [329] I. Loshchilov and F. Hutter, "Fixing weight decay regularization in adam," _CoRR_, vol. abs/1711.05101, 2017.\n' +
      '* [330] N. Shazeer and M. Stern, "Adafactor: Adaptive learning rates with sublinear memory cost," in _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, ser. Proceedings of Machine Learning Research, J. G. Dy and A. Krause, Eds., vol. 80. PMLR, 2018, pp. 4603-4611.\n' +
      '* [331] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. X. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu, and Z. Chen, "Gpipe: Efficient training of giant neural networks using pipeline parallelism," in _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d\'Alche-Buc, E. B. Fox, and R. Garnett, Eds., 2019, pp. 103-112.\n' +
      '* [332] A. Harlap, D. Narayanan, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, and P. B. Gibbons, "Pipedream: Fast and efficient pipeline parallel DNN training," _CoRR_, vol. abs/1806.03377, 2018.\n' +
      '* [333] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, "Zero: memory optimizations toward training trillion parameter models," in _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020_, C. Cuicchi, I. Qualters, and W. T. Kramer, Eds. IEEE/ACM, 2020, p. 20.\n' +
      '* [334] P. Micikevicius, S. Narang, J. Alben, G. F. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, and H. Wu, "Mixed precision training," _CoRR_, vol. abs/1710.03740, 2017.\n' +
      '* [335] Q. Xu, S. Li, C. Gong, and Y. You, "An efficient 2d method for training super-large deep learning models," _CoRR_, vol. abs/2104.05343, 2021.\n' +
      '* 2022년 9월 1일_. ACM, 2022년\n' +
      '* [337] Z. Bian, Q. Xu, B. Wang, and Y. You, "Maximizing parallelism in distributed training for huge neural networks," _CoRR_, vol. abs/2105.14450, 2021.\n' +
      '* [338] S. Li, F. Xue, C. Baranwal, Y. Li, and Y. You, "Sequence parallelism: Long sequence training from system perspective," _arXiv e-prints_, pp. arXiv-2105, 2021.\n' +
      '* [339] FairScale authors, "Fairscale: A general purpose modular pytorch library for high performance and large scale training," [https://github.com/facebookresearch/fairscale](https://github.com/facebookresearch/fairscale), 2021.\n' +
      '* [340] L. Zheng, Z. Li, H. Zhang, Y. Zhuang, Z. Chen, Y. Huang, Y. Wang, Y. Xu, D. Zhuo, E. P. Xing _et al._, "Alpa: Automating inter-and {Intra-Operator} parallelism for distributed deep learning," in _OSDI_, 2022, pp. 559-578.\n' +
      '* [341] T. Chen, B. Xu, C. Zhang, and C. Guestrin, "Training deep nets with sublinear memory cost," _CoRR_, vol. abs/1604.06174, 2016.\n' +
      '* [342] R. Lou, K. Zhang, and W. Yin, "Is prompt all you need? no. A comprehensive and broader view of instruction learning," _CoRR_, vol. abs/2303.10475, 2023.\n' +
      '* [343] X. Liu, P. He, W. Chen, and J. Gao, "Multi-task deep neural networks for natural language understanding," in _ACL (1)_. Association for Computational Linguistics, 2019, pp. 4487-4496.\n' +
      '* [344] A. Aghajanyan, A. Gupta, A. Shrivastava, X. Chen, L. Zettlemoyer, and S. Gupta, "Muppet: Massive multi-task representations with pre-finetuning," in _EMNLP (1)_. Association for Computational Linguistics, 2021, pp. 5799-5811.\n' +
      '* [345] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei, and A. Roberts, "The fian collection: Designing data and methods for effective instruction tuning," _CoRR_, vol. abs/2301.13688, 2023.\n' +
      '* [346] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, and D. Jiang, "Wizardlm: Empowering large language models to follow complex instructions," _CoRR_, vol. abs/2304.12244, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2304.12244](https://doi.org/10.48550/arXiv.2304.12244)\n' +
      '* [347] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, and C. Gan, "Principle-driven self-alignment of language models from scratch with minimal human supervision," _arXiv preprint arXiv:2305.03047_, 2023.\n' +
      '* [348] X. Li, P. Yu, C. Zhou, T. Schick, L. Zettlemoyer, O. Levy, J. Weston, and M. Lewis, "Self-alignment with instruction backtranslation," _CoRR_, vol. abs/2308.06259, 2023.\n' +
      '* [349] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu _et al._, "Lima: Less is more for alignment," _arXiv preprint arXiv:2305.11206_, 2023.\n' +
      '* [350] L. Chen, S. Li, J. Yan, H. Wang, K. Gunaratna, V. Yadav, Z. Tang, V. Srinivasan, T. Zhou, H. Huang, and H. Jin, "Alpagasus: Training A better alpaca with fewer data," _CoRR_, vol. abs/2307.08701, 2023.\n' +
      '* [351] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and A. H. Awadallah, "Orca: Progressive learning from complex explanation traces of GPT-4," _CoRR_, vol. abs/2306.02707, 2023.\n' +
      '* [352] YuLan-Chat-Team, "Yulan-chat: An open-source bilingual chatbot," [https://github.com/RUC-GSAI/YuLan-Chat](https://github.com/RUC-GSAI/YuLan-Chat), 2023.\n' +
      '* [353] Y. Wang, H. Ivison, P. Dasigi, J. Hessel, T. Khot, K. R. Chandu, D. Wadden, K. MacMillan, N. A. Smith, I. Beltagy, and H. Hajishirzi, "How far can camels go? exploring the state of instruction tuning on open resources," _CoRR_, vol. abs/2306.04751, 2023.\n' +
      '* [354] B. Peng, C. Li, P. He, M. Galley, and J. Gao, "Instruction tuning with GPT-4," _CoRR_, vol. abs/2304.03277, 2023.\n' +
      '* [355] M. M. Krell, M. Kosec, S. P. Perez, and A. Fitzgibbon, "Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance," _arXiv preprint arXiv:2107.02027_, 2021.\n' +
      '* [356] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl _et al._, "Large language models encode clinical knowledge," _arXiv preprint arXiv:2212.13138_, 2022.\n' +
      '* [357] J. Zhang, R. Xie, Y. Hou, W. X. Zhao, L. Lin, and J. Wen, "Recommendation as instruction following: A large language model empowered recommendation approach," _CoRR_, vol. abs/2305.07001, 2023.\n' +
      '* [358] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, and T. Liu, "Huatuo: Tuning llama model with chinese medical knowledge," _arXiv preprint arXiv:2304.06975_, 2023.\n' +
      '* [359] Q. Huang, M. Tao, Z. An, C. Zhang, C. Jiang, Z. Chen, Z. Wu, and Y. Feng, "Lawyer llama technical report," _arXiv preprint arXiv:2305.15062_, 2023.\n' +
      '* [360] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann, P. Kambadur, D. Rosenberg, and G. Mann, "Bloomberggpt: A large language model for finance," _arXiv preprint arXiv:2303.17564_, 2023.\n' +
      '* [361] T. Liu and B. K. H. Low, "Goat: Fine-tuned llama outperforms gpt+4 on arithmetic tasks," _arXiv preprint arXiv:2305.14201_, 2023.\n' +
      '* [362] T. Sun, X. Zhang, Z. He, P. Li, Q. Cheng, H. Yan, X. Liu, Y. Shao, Q. Tang, X. Zhao, K. Chen, Y. Zheng, Z. Zhou, R. Li, J. Zhan, Y. Zhou, L. Li, X. Yang, L. Wu, Z. Yin, X. Huang, and X. Qiu, "Moss: Training conversational language models from synthetic data," 2023.\n' +
      '* [363] Y. Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin, P. Liang, and T. B. Hashimoto, "Alpacafarm: A simulation framework for methods that learn from human feedback," _CoRR_, vol. abs/2305.14387, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2305.14387](https://doi.org/10.48550/arXiv.2305.14387)\n' +
      '* [364] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt, "Measuring massive multitask language understanding," in _ICLR_.\n' +
      '\n' +
      'OpenReview.net, 2021.\n' +
      '* [365] M. Suzgun, N. Scales, N. Scharli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, and J. Wei, "Challenging big-bench tasks and whether chain-of-thought can solve them," _CoRR_, vol. abs/2210.09261, 2022.\n' +
      '* [366] Z. Kenton, T. Everitt, L. Weidinger, I. Gabriel, V. Mikulik, and G. Irving, "Alignment of language agents," _CoRR_, vol. abs/2103.14659, 2021.\n' +
      '* [367] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. F. Christiano, and G. Irving, "Fine-tuning language models from human preferences," _CoRR_, vol. abs/1909.08593, 2019.\n' +
      '* [368] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. Das-Sarma, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, J. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. B. Brown, J. Clark, S. McCandlish, C. Olah, and J. Kaplan, "A general language assistant as a laboratory for alignment," _CoRR_, vol. abs/2112.00861, 2021.\n' +
      '* [369] E. Perez, S. Huang, H. F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese, and G. Irving, "Red teaming language models with language models," in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emitates, December 7-11, 2022_, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds. Association for Computational Linguistics, 2022, pp. 3419-3448.\n' +
      '* [370] J. Menick, M. Trebacz, V. Mikulik, J. Aslanides, H. F. Song, M. Chadwick, M. Glaese, S. Young, L. Campbell-Gillingham, G. Irving, and N. McAleese, "Teaching language models to support answers with verified quotes," _CoRR_, vol. abs/2203.11147, 2022.\n' +
      '* [371] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D. Li, E. Tran-Johnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, K. Lukosiute, L. Lovitt, M. Sellitto, N. Elhage, N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby, R. Larson, S. Ringer, S. Johnston, S. Kravec, S. E. Showk, S. Fort, T. Lanham, T. Telleen-Lawton, T. Conerly, T. Henighan, T. Hume, S. R. Bowman, Z. Hatfield-Dodds, B. Mann, D. Amodei, N. Joseph, S. McCandlish, T. Brown, and J. Kaplan, "Constitutional AI: harmlessness from AI feedback," _CoRR_, vol. abs/2212.08073, 2022. [Online]. Available: [https://doi.org/10.48550/arXiv.2212.08073](https://doi.org/10.48550/arXiv.2212.08073)\n' +
      '* [372] H. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard, C. Bishop, V. Carbune, and A. Rastogi, "RLAIF: scaling reinforcement learning from human feedback with AI feedback," _CoRR_, vol. abs/2309.00267, 2023.\n' +
      '* [373] H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum, and T. Zhang, "RAFT: reward ranked fine-tuning for generative foundation model alignment," _CoRR_, vol. abs/2304.06767, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2304.06767](https://doi.org/10.48550/arXiv.2304.06767)\n' +
      '* [374] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. Das-Sarma _et al._, "A general language assistant as a laboratory for alignment," _arXiv preprint arXiv:2112.00861_, 2021.\n' +
      '* [375] R. Zheng, S. Dou, S. Gao, W. Shen, B. Wang, Y. Liu, S. Jin, Q. Liu, L. Xiong, L. Chen _et al._, "Secrets of rlhf in large language models part i: Ppo," _arXiv preprint arXiv:2307.04964_, 2023.\n' +
      '* [376] J. Uesato, N. Kushman, R. Kumar, H. F. Song, N. Y. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins, "Solving math word problems with process- and outcome-based feedback," _CoRR_, vol. abs/2211.14275, 2022.\n' +
      '* [377] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe, "Let\'s verify step by step," _CoRR_, vol. abs/2305.20050, 2023.\n' +
      '* [378] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt, "Measuring coding challenge competence with APPS," in _NeurIPS Datasets and Benchmarks_, 2021.\n' +
      '* [379] Q. Ma, H. Zhou, T. Liu, J. Yuan, P. Liu, Y. You, and H. Yang, "Let\'s reward step by step: Step-level reward model as the navigators for reasoning," _CoRR_, vol. abs/2310.10080, 2023.\n' +
      '* [380] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. P. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis, "Mastering the game of go without human knowledge," _Nat._, pp. 354-359, 2017.\n' +
      '* [381] T. Anthony, Z. Tian, and D. Barber, "Thinking fast and slow with deep learning and tree search," in _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, 2017, pp. 5360-5370.\n' +
      '* [382] H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang, "Wizard-math: Empowering mathematical reasoning for large language models via reinforced evol-instruct," _CoRR_, vol. abs/2308.09583, 2023.\n' +
      '* [383] R. Liu, C. Jia, G. Zhang, Z. Zhuang, T. X. Liu, and S. Vosoughi, "Second thoughts are best: Learning to re-align with human values from text edits," in _NeurIPS_, 2022.\n' +
      '* [384] X. Lu, S. Welleck, J. Hessel, L. Jiang, L. Qin, P. West, P. Ammanabrolu, and Y. Choi, "QUARK: controllable text generation with reinforced unlearning," in _NeurIPS_, 2022.\n' +
      '* [385] J. Scheurer, J. A. Campos, T. Korbak, J. S. Chan, A. Chen, K. Cho, and E. Perez, "Training language models with language feedback at scale," _CoRR_, vol. abs/2303.16755, 2023.\n' +
      '* [386] G. Guo, R. Zhao, T. Tang, W. X. Zhao, and J.-R. Wen, "Beyond imitation: Leveraging fine-grained quality signals for alignment," _arXiv preprint arXiv:2311.04072_, 2023.\n' +
      '* [387] R. Krishna, D. Lee, L. Fei-Fei, and M. S. Bernstein, "Socially situated artificial intelligence enables learning from human interaction," _Proceedings of the National Academy of Sciences of the United Statesof America_, vol. 119, 2022. [Online]. Available: [https://api.semanticscholar.org/CorpusID:252381954](https://api.semanticscholar.org/CorpusID:252381954)\n' +
      '* [388] H. Liu, C. Sferrazza, and P. Abbeel, "Chain of hindsight aligns language models with feedback," _CoRR_, vol. abs/2302.02676, 2023.\n' +
      '* [389] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn, "Direct preference optimization: Your language model is secretly a reward model," _CoRR_, vol. abs/2305.18290, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2305.18290](https://doi.org/10.48550/arXiv.2305.18290)\n' +
      '* [390] Z. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, and F. Huang, "RRHF: rank responses to align language models with human feedback without tears," _CoRR_, vol. abs/2304.05302, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2304.05302](https://doi.org/10.48550/arXiv.2304.05302)\n' +
      '* [391] Y. Zhao, R. Joshi, T. Liu, M. Khalman, M. Saleh, and P. J. Liu, "Slic-hf: Sequence likelihood calibration with human feedback," _CoRR_, vol. abs/2305.10425, 2023.\n' +
      '* [392] T. Zhang, F. Liu, J. Wong, P. Abbeel, and J. E. Gonzalez, "The wisdom of hindsight makes language models better instruction followers," _CoRR_, vol. abs/2302.05206, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2302.05206](https://doi.org/10.48550/arXiv.2302.05206)\n' +
      '* [393] A. Hussein, M. M. Gaber, E. Elyan, and C. Jayne, "Imitation learning: A survey of learning methods," _ACM Comput. Surv._, vol. 50, no. 2, apr 2017. [Online]. Available: [https://doi.org/10.1145/3054912](https://doi.org/10.1145/3054912)\n' +
      '* [394] S. Levine, "Should i imitate or reinforce," 2022. [Online]. Available: [https://www.youtube.com/watch?v=sVPm?zOrBxM](https://www.youtube.com/watch?v=sVPm?zOrBxM)\n' +
      '* [395] J. Schulman, "Reinforcement learning from human feedback: Progress and challenges," 2023. [Online]. Available: [https://www.youtube.com/watch?v=hiLw5Q_UFg](https://www.youtube.com/watch?v=hiLw5Q_UFg)\n' +
      '* [396] X. L. Li and P. Liang, "Prefix-tuning: Optimizing continuous prompts for generation," in _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021_, C. Zong, F. Xia, W. Li, and R. Navigli, Eds. Association for Computational Linguistics, 2021, pp. 4582-4597.\n' +
      '* [397] B. Lester, R. Al-Rfou, and N. Constant, "The power of scale for parameter-efficient prompt tuning," in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021_, M. Moens, X. Huang, L. Specia, and S. W. Yih, Eds. Association for Computational Linguistics, 2021, pp. 3045-3059.\n' +
      '* [398] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, "Parameter-efficient transfer learning for NLP," in _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, 2019, pp. 2790-2799.\n' +
      '* [399] Z. Hu, Y. Lan, L. Wang, W. Xu, E. Lim, R. K. Lee, L. Bing, and S. Poria, "Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models," _CoRR_, vol. abs/2304.01933, 2023.\n' +
      '* [400] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig, "Towards a unified view of parameter-efficient transfer learning," in _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.\n' +
      '* [401] X. Liu, K. Ji, Y. Fu, Z. Du, Z. Yang, and J. Tang, "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks," _CoRR_, vol. abs/2110.07602, 2021.\n' +
      '* [402] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, and J. Tang, "GPT understands, too," _CoRR_, vol. abs/2103.10385, 2021.\n' +
      '* [403] Y. Gu, X. Han, Z. Liu, and M. Huang, "Ppt: Pre-trained prompt tuning for few-shot learning," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2022, pp. 8410-8423.\n' +
      '* [404] Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, "How can we know what language models know?" _Transactions of the Association for Computational Linguistics_, vol. 8, pp. 423-438, 2020.\n' +
      '* [405] T. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and S. Singh, "Autoprompt: Eliciting knowledge from language models with automatically generated prompts," in _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, 2020, pp. 4222-4235.\n' +
      '* [406] Q. Zhang, M. Chen, A. Bukharin, P. He, Y. Cheng, W. Chen, and T. Zhao, "Adaptive budget allocation for parameter-efficient fine-tuning," _CoRR_, vol. abs/2303.10512, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2303.10512](https://doi.org/10.48550/arXiv.2303.10512)\n' +
      '* [407] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and A. Ghodsi, "Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation," _CoRR_, vol. abs/2210.07558, 2022. [Online]. Available: [https://doi.org/10.48550/arXiv.2210.07558](https://doi.org/10.48550/arXiv.2210.07558)\n' +
      '* [408] N. Ding, Y. Qin, G. Yang, F. Wei, Y. Zonghan, Y. Su, S. Hu, Y. Chen, C.-M. Chan, W. Chen, J. Yi, W. Zhao, X. Wang, Z. Liu, H.-T. Zheng, J. Chen, Y. Liu, J. Tang, J. Li, and M. Sun, "Parameter-efficient fine-tuning of large-scale pre-trained language models," _Nature Machine Intelligence_, vol. 5, pp. 1-16, 03 2023.\n' +
      '* [409] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, and Y. Qiao, "Llama-adapter: Efficient fine-tuning of language models with zero-init attention," _CoRR_, vol. abs/2303.16199, 2023.\n' +
      '* [410] J. Pfeiffer, I. Vulic, I. Gurevych, and S. Ruder, "MAD-X: an adapter-based framework for multi-task cross-lingual transfer," in _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020_, B. Webber, T. Cohn, Y. He, and Y. Liu, Eds. Association for Computational Linguistics, 2020, pp. 7654-7673.\n' +
      '* [411] S. Mangurikkar, S. Gugger, L. Debut, Y. Belkada, and S. Paul, "Peft: State-of-the-art parameter-efficient fine-tuning methods," [https://github.com/huggingface/peft](https://github.com/huggingface/peft), 2022.\n' +
      '* [412] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W.\n' +
      '\n' +
      'Mahoney, K. Keutzer, "A survey of quantization methods for efficient neural network inference," _CoRR_, vol. abs/2103.13630, 2021. [Online]. 사용 가능: [https://arxiv.org/abs/2103.13630](https://arxiv.org/abs/2103.13630)\n' +
      '* [413] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, "Llm.int8(0): 8-bit matrix multiplication for transformers at scale," _CoRR_, vol. abs/2208.07339, 2022.\n' +
      '* [414] G. Xiao, J. Lin, M. Seznec, J. Demouth, and S. Han, "Smoothquant: Accurate and efficient post-training quantization for large language models," _CoRR_, vol. abs/2211.10438, 2022. [Online]. Available: [https://doi.org/10.48550/arXiv.2211.10438](https://doi.org/10.48550/arXiv.2211.10438)\n' +
      '* [415] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He, "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers," in _NeurIPS_, 2022.\n' +
      '* [416] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han, "Awq: Activation-aware weight quantization for llm compression and acceleration," 2023.\n' +
      '* [417] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, "Gptq: Accurate post-training quantization for generative pre-trained transformers," _arXiv preprint arXiv:2210.17323_, 2022.\n' +
      '* [418] E. Frantar and D. Alistarh, "Optimal brain compression: A framework for accurate post-training quantization and pruning," in _NeurIPS_, 2022.\n' +
      '* [419] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, "Olora: Efficient finetuning of quantized llms," _arXiv preprint arXiv:2305.14314_, 2023.\n' +
      '* [420] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Krishnamoorthi, and V. Chandra, "Llm-qat: Data-free quantization aware training for large language models," 2023.\n' +
      '* [421] Z. Yao, X. Wu, C. Li, S. Youn, and Y. He, "Zeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation," 2023.\n' +
      '* [422] T. Dettmers and L. Zettlemoyer, "The case for 4-bit precision: k-bit inference scaling laws," _CoRR_, vol. abs/2212.09720, 2022.\n' +
      '* [423] L. Peiyu, L. Zikang, G. Ze-Feng, G. Dawei, Z. W. Xin, L. Yaliang, D. Bolin, and W. Ji-Rong, "Do emergent abilities exist in quantized large language models: An empirical study," _arXiv preprint arXiv:2307.08072_, 2023.\n' +
      '* [424] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, "Llm.int8(): 8-bit matrix multiplication for transformers at scale," _CoRR_, vol. abs/2208.07339, 2022. [Online]. Available: [https://doi.org/10.48550/arXiv.2208.07339](https://doi.org/10.48550/arXiv.2208.07339)\n' +
      '* [425] X. Wei, X. Cui, N. Cheng, X. Wang, X. Zhang, S. Huang, P. Xie, J. Xu, Y. Chen, M. Zhang _et al._, "Zero-shot information extraction via chatting with chatopt," _arXiv preprint arXiv:2302.10205_, 2023.\n' +
      '* [426] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer, "8-bit optimizers via block-wise quantization," _9th International Conference on Learning Representations, ICLR_, 2022.\n' +
      '* [427] C. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu, P. Luo, and N. Wong, "Compression of generative pre-trained language models via quantization," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, S. Muresan, P. Nakov, and A. Villavicencio, Eds. Association for Computational Linguistics, 2022, pp. 4821-4836.\n' +
      '* [428] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen, "What makes good in-context examples for gpt-3?" in _Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLO@ACL 2022, Dublin, Ireland and Online, May 27, 2022_, 2022, pp. 100-114.\n' +
      '* [429] O. Rubin, J. Herzig, and J. Berant, "Learning to retrieve prompts for in-context learning," in _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022_, 2022, pp. 2655-2671.\n' +
      '* [430] H. J. Kim, H. Cho, J. Kim, T. Kim, K. M. Yoo, and S. Lee, "Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator," _CoRR_, vol. abs/2206.08082, 2022.\n' +
      '* [431] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba, "Large language models are human-level prompt engineers," in _Proc. of ICLR_, 2023.\n' +
      '* [432] Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp, "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, S. Muresan, P. Nakov, and A. Villavicencio, Eds., 2022, pp. 8086-8098.\n' +
      '* [433] Y. Fu, H. Peng, A. Sabharwal, P. Clark, and T. Khot, "Complexity-based prompting for multi-step reasoning," _CoRR_, vol. abs/2210.00720, 2022.\n' +
      '* [434] Z. Zhang, A. Zhang, M. Li, and A. Smola, "Automatic chain of thought prompting in large language models," _CoRR_, vol. abs/2210.03493, 2022.\n' +
      '* [435] A. Creswell, M. Shanahan, and I. Higgins, "Selection-inference: Exploiting large language models for interpretable logical reasoning," _CoRR_, vol. abs/2205.09712, 2022.\n' +
      '* [436] X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, and D. Zhou, "Self-consistency improves chain of thought reasoning in language models," _CoRR_, vol. abs/2203.11171, 2022.\n' +
      '* [437] Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J. Lou, and W. Chen, "On the advance of making language models better reasoners," _CoRR_, vol. abs/2206.02336, 2022.\n' +
      '* [438] X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, and D. Zhou, "Rationale-augmented ensembles in language models," _CoRR_, 2022.\n' +
      '* [439] D. Zhou, N. Scharli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, O. Bousquet, Q. Le, and E. H. Chi, "Least-to-most prompting enables complex reasoning in large language models," _CoRR_, vol. abs/2205.10625, 2022.\n' +
      '* [440] T. Khot, H. Trivedi, M. Finlayson, Y. Fu, K. Richardson, P. Clark, and A. Sabharwal, "Decomposed prompting: A modular approach for solving complex tasks," _CoRR_, vol. abs/2210.02406, 2022. [Online]. Available:[https://doi.org/10.48550/arXiv.2210.02406](https://doi.org/10.48550/arXiv.2210.02406)\n' +
      '* [441] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K. Lee, and E. Lim, "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models," _CoRR_, vol. abs/2305.04091, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2305.04091](https://doi.org/10.48550/arXiv.2305.04091)\n' +
      '* [442] Q. Lyu, S. Havaldar, A. Stein, L. Zhang, D. Rao, E. Wong, M. Apidianaki, and C. Callison-Burch, "Faithful chain-of-thought reasoning," _CoRR_, vol. abs/2301.13379, 2023.\n' +
      '* [443] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig, "PAL: program-aided language models," _CoRR_, vol. abs/2211.10435, 2022.\n' +
      '* [444] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, "Hugginggpt: Solving ai tasks with chat-gpt and its friends in huggingface," _arXiv preprint arXiv:2303.17580_, 2023.\n' +
      '* [445] H. Sun, Y. Zhuang, L. Kong, B. Dai, and C. Zhang, "Adaplanner: Adaptive planning from feedback with language models," _arXiv preprint arXiv:2305.16653_, 2023.\n' +
      '* [446] Y. Lu, P. Lu, Z. Chen, W. Zhu, X. E. Wang, and W. Y. Wang, "Multimodal procedural planning via dual text-image prompting," _CoRR_, vol. abs/2305.01795, 2023.\n' +
      '* [447] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu, "Reasoning with language model is planning with world model," _CoRR_, vol. abs/2305.14992, 2023.\n' +
      '* [448] Z. Chen, K. Zhou, B. Zhang, Z. Gong, W. X. Zhao, and J. Wen, "Chatcot: Tool-augmented chain-of-thought reasoning on chat-based large language models," _CoRR_, vol. abs/2305.14323, 2023.\n' +
      '* [449] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, "React: Synergizing reasoning and acting in language models," _CoRR_, vol. abs/2210.03629, 2022.\n' +
      '* [450] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, and S. Yao, "Reflexion: Language agents with verbal reinforcement learning," 2023.\n' +
      '* [451] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan, "Tree of thoughts: Deliberate problem solving with large language models," _CoRR_, vol. abs/2305.10601, 2023.\n' +
      '* [452] V. Liu and L. B. Chilton, "Design guidelines for prompt engineering text-to-image generative models," in _Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems_, 2022, pp. 1-23.\n' +
      '* [453] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. Elnashar, J. Spencer-Smith, and D. C. Schmidt, "A prompt pattern catalog to enhance prompt engineering with chatgpt," _arXiv preprint arXiv:2302.11382_, 2023.\n' +
      '* [454] S. K. K. Santu and D. Feng, "Teler: A general taxonomy of LLM prompts for benchmarking complex tasks," _CoRR_, vol. abs/2305.11430, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2305.11430](https://doi.org/10.48550/arXiv.2305.11430)\n' +
      '* [455] OpenAI, "Gpt best practices," _OpenAI_, 2023. [Online]. Available: [https://platform.openai.com/docs/guides/gpt-best-practices](https://platform.openai.com/docs/guides/gpt-best-practices)\n' +
      '* [456] Contributors, "Ai short," 2023. [Online]. Available: [https://www.aishort.top/](https://www.aishort.top/)\n' +
      '* [457] ----, "Awesome chatgpt prompts," _Github_, 2023. [Online]. Available: [https://github.com/f/awesome-chatgpt-prompts/](https://github.com/f/awesome-chatgpt-prompts/)\n' +
      '* [458] J. Jiang, K. Zhou, Z. Dong, K. Ye, W. X. Zhao, and J. Wen, "Struckpt: A general framework for large language model to reason over structured data," _CoRR_, vol. abs/2305.09645, 2023.\n' +
      '* [459] L. Beurer-Kellner, M. Fischer, and M. Vechev, "Prompting is programming: A query language for large language models," _Proceedings of the ACM on Programming Languages_, vol. 7, no. PLDI, pp. 1946-1969, 2023.\n' +
      '* [460] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu, and J. Gao, "Chameleon: Plug-and-play compositional reasoning with large language models," _arXiv preprint arXiv:2304.09842_, 2023.\n' +
      '* [461] R. Ren, Y. Wang, Y. Qu, W. X. Zhao, J. Liu, H. Tian, H. Wu, J.-R. Wen, and H. Wang, "Investigating the factual knowledge boundary of large language models with retrieval augmentation," _arXiv preprint arXiv:2307.11019_, 2023.\n' +
      '* [462] Y. Hou, J. Zhang, Z. Lin, H. Lu, R. Xie, J. J. McAuley, and W. X. Zhao, "Large language models are zero-shot rankers for recommender systems," _CoRR_, vol. abs/2305.08845, 2023.\n' +
      '* [463] S. Chang and E. Fosler-Lussier, "How to prompt lms for text-to-sql: A study in zero-shot, single-domain, and cross-domain settings," _CoRR_, vol. abs/2305.11853, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2305.11853](https://doi.org/10.48550/arXiv.2305.11853)\n' +
      '* [464] Y. Wen, N. Jain, J. Kirchenbauer, M. Goldblum, J. Geiping, and T. Goldstein, "Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery," _CoRR_, vol. abs/2302.03668, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2302.03668](https://doi.org/10.48550/arXiv.2302.03668)\n' +
      '* [465] T. Gao, A. Fisch, and D. Chen, "Making pre-trained language models better few-shot learners," in _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/JCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021_, C. Zong, F. Xia, W. Li, and R. Navigli, Eds. Association for Computational Linguistics, 2021, pp. 3816-3830.\n' +
      '* [466] L. Chen, J. Chen, T. Goldstein, H. Huang, and T. Zhou, "Instructzero: Efficient instruction optimization for black-box large language models," _CoRR_, vol. abs/2306.03082, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2306.03082](https://doi.org/10.48550/arXiv.2306.03082)\n' +
      '* [467] M. Deng, J. Wang, C. Hsieh, Y. Wang, H. Guo, T. Shu, M. Song, E. P. Xing, and Z. Hu, "Rlprompt: Optimizing discrete text prompts with reinforcement learning," in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds. Association for Computational Linguistics, 2022, pp. 3369-3391.\n' +
      '\n' +
      '* [468] T. Zhang, X. Wang, D. Zhou, D. Schuurmans, and J. E. Gonzalez, "TEMPERA: test-time prompt editing via reinforcement learning," in _The Eleventh International Conference on Learning Representations, ICLR 2023, Kiagali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023.\n' +
      '* [469] H. Xu, Y. Chen, Y. Du, N. Shao, Y. Wang, H. Li, and Z. Yang, "GPS: genetic prompt search for efficient few-shot learning," in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds. Association for Computational Linguistics, 2022, pp. 8162-8171.\n' +
      '* [470] A. Prasad, P. Hase, X. Zhou, and M. Bansal, "Grips: Gradient-free, edit-based instruction search for prompting large language models," in _Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023_, A. Vlachos and I. Augenstein, Eds. Association for Computational Linguistics, 2023, pp. 3827-3846.\n' +
      '* [471] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba, "Large language models are human-level prompt engineers," in _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023.\n' +
      '* [472] R. Przyant, D. Iter, J. Li, Y. T. Lee, C. Zhu, and M. Zeng, "Automatic prompt optimization with "gradient descent" and beam search," _CoRR_, vol. abs/2305.03495, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2305.03495](https://doi.org/10.48550/arXiv.2305.03495)\n' +
      '* [473] C. Yang, X. Wang, Y. Lu, H. Liu, Q. V. Le, D. Zhou, and X. Chen, "Large language models as optimizers," _CoRR_, vol. abs/2309.03409, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2309.03409](https://doi.org/10.48550/arXiv.2309.03409)\n' +
      '* [474] X. Wang, C. Li, Z. Wang, F. Bai, H. Luo, J. Zhang, N. Jojic, E. P. Xing, and Z. Hu, "Promptagent: Strategic planning with language models enables expert-level prompt optimization," _CoRR_, vol. abs/2310.16427, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2310.16427](https://doi.org/10.48550/arXiv.2310.16427)\n' +
      '* [475] T. Tang, J. Li, W. X. Zhao, and J. Wen, "Context-tuning: Learning contextualized prompts for natural language generation," in _Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022_, N. Calzolari, C. Huang, H. Kim, J. Pustejovsky, L. Wanner, K. Choi, P. Ryu, H. Chen, L. Donatelli, H. Ji, S. Kurohashi, P. Paggio, N. Xue, S. Kim, Y. Hahm, Z. He, T. K. Lee, E. Santus, F. Bond, and S. Na, Eds. International Committee on Computational Linguistics, 2022, pp. 6340-6354.\n' +
      '* [476] T. Vu, B. Lester, N. Constant, R. Al-Rfou\', and D. Cer, "Spot: Better frozen model adaptation through soft prompt transfer," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, S. Muresan, P. Nakov, and A. Villavicencio, Eds. Association for Computational Linguistics, 2022, pp. 5039-5059.\n' +
      '* [477] J. Li, T. Tang, J. Nie, J. Wen, and X. Zhao, "Learning to transfer prompts for text generation," in _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022_, M. Carpuat, M. de Marneffe, and I. V. M. Ruiz, Eds. Association for Computational Linguistics, 2022, pp. 3506-3518.\n' +
      '* [478] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer, "Rethinking the role of demonstrations: What makes in-context learning work?" in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_. Association for Computational Linguistics, 2022, pp. 11 048-11 064.\n' +
      '* [479] Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh, "Calibrate before use: Improving few-shot performance of language models," in _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, M. Meila and T. Zhang, Eds., 2021, pp. 12 697-12 706.\n' +
      '* [480] Y. Lee, C. Lim, and H. Choi, "Does GPT-3 generate empathetic dialogues? A novel in-context example selection method and automatic evaluation metric for empathetic dialogue generation," in _Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022_, N. Calzolari, C. Huang, H. Kim, J. Pustejovsky, L. Wanner, K. Choi, P. Ryu, H. Chen, L. Donatelli, H. Ji, S. Kurohashi, P. Paggio, N. Xue, S. Kim, Y. Hahm, Z. He, T. K. Lee, E. Santus, F. Bond, and S. Na, Eds. International Committee on Computational Linguistics, 2022, pp. 669-683.\n' +
      '* [481] I. Levy, B. Bogin, and J. Berant, "Diverse demonstrations improve in-context compositional generalization," _CoRR_, vol. abs/2212.06800, 2022.\n' +
      '* [482] H. Su, J. Kasai, C. H. Wu, W. Shi, T. Wang, J. Xin, R. Zhang, M. Ostendorf, L. Zettlemoyer, N. A. Smith, and T. Yu, "Selective annotation makes language models better few-shot learners," _CoRR_, 2022.\n' +
      '* [483] X. Ye, S. Iyer, A. Celikyilmaz, V. Stoyanov, G. Durrett, and R. Pasunuru, "Complementary explanations for effective in-context learning," _CoRR_, 2022.\n' +
      '* [484] X. Li and X. Qiu, "Finding supporting examples for in-context learning," _CoRR_, 2023.\n' +
      '* [485] Y. Zhang, S. Feng, and C. Tan, "Active example selection for in-context learning," in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, 2022, pp. 9134-9148.\n' +
      '* [486] F. Gilardi, M. Alizadeh, and M. Kubli, "Chatgpt outperforms crowd-workers for text-annotation tasks," 2023.\n' +
      '* [487] H. J. Kim, H. Cho, J. Kim, T. Kim, K. M. Yoo, and S. Lee, "Self-generated in-context learning: Leveraging auto-regressive language models as a demonstrating generator," _CoRR_, vol. abs/2206.08082, 2022.\n' +
      '* [488] S. M. Xie, A. Raghunathan, P. Liang, and T. Ma, "An explanation of in-context learning as implicit bayesian inference," in _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_, 2022.\n' +
      '* [489] Z. Wu, Y. Wang, J. Ye, and L. Kong, "Self-adaptive in-context learning," _CoRR_, vol. abs/2212.10375, 2022.\n' +
      '* [490] Y. Gu, L. Dong, F. Wei, and M. Huang, "Pre-training to learn in context," _CoRR_, vol. abs/2305.09137, 2023.\n' +
      '* [491] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi, "Metaicl: Learning to learn in context," in _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022_, M. Carpuat, M. de Marneffe, and I. V. M. Ruiz, Eds., 2022, pp. 2791-2809.\n' +
      '* [492] M. Hahn and N. Goyal, "A theory of emergent in-context learning as implicit structure induction," _CoRR_, vol. abs/2303.07971, 2023.\n' +
      '* [493] J. Pan, T. Gao, H. Chen, and D. Chen, "What in-context learning "learns" in-context: Disentangling task recognition and task learning," _CoRR_, vol. abs/2305.09731, 2023.\n' +
      '* [494] N. Wies, Y. Levine, and A. Shashua, "The learnability of in-context learning," _CoRR_, vol. abs/2303.07895, 2023.\n' +
      '* [495] A. Webson and E. Pavlick, "Do prompt-based models really understand the meaning of their prompts?" in _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022_, 2022, pp. 2300-2344.\n' +
      '* [496] J. von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and M. Vladymyrov, "Transformers learn in-context by gradient descent," _CoRR_, vol. abs/2212.07677, 2022.\n' +
      '* [497] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah, "In-context learning and induction heads," _CoRR_, vol. abs/2209.11895, 2022.\n' +
      '* [498] E. Akyurek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou, "What learning algorithm is in-context learning? investigations with linear models," _CoRR_, vol. abs/2211.15661, 2022.\n' +
      '* [499] J. Wei, J. Wei, Y. Tay, D. Tran, A. Webson, Y. Lu, X. Chen, H. Liu, D. Huang, D. Zhou _et al._, "Larger language models do in-context learning differently," _arXiv preprint arXiv:2303.03846_, 2023.\n' +
      '* [500] J. Coda-Forno, M. Binz, Z. Akata, M. M. Botvinick, J. X. Wang, and E. Schulz, "Meta-in-context learning in large language models," _CoRR_, vol. abs/2305.12907, 2023.\n' +
      '* [501] J. W. Wei, L. Hou, A. K. Lampinen, X. Chen, D. Huang, Y. Tay, X. Chen, Y. Lu, D. Zhou, T. Ma, and Q. V. Le, "Symbol tuning improves in-context learning in language models," _CoRR_, vol. abs/2305.08298, 2023.\n' +
      '* [502] Z. Chu, J. Chen, Q. Chen, W. Yu, T. He, H. Wang, W. Peng, M. Liu, B. Qin, and T. Liu, "A survey of chain of thought reasoning: Advances, frontiers and future," _CoRR_, vol. abs/2309.15402, 2023.\n' +
      '* [503] S. Miao, C. Liang, and K. Su, "A diverse corpus for evaluating and developing english math word problem solvers," in _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_, D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault, Eds. Association for Computational Linguistics, 2020, pp. 975-984.\n' +
      '* [504] A. Talmor, J. Herzig, N. Lourie, and J. Berant, "Comonsenseqa: A question answering challenge targeting commonsense knowledge," in _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, J. Burstein, C. Doran, and T. Solorio, Eds. Association for Computational Linguistics, 2019, pp. 4149-4158.\n' +
      '* [505] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, "Large language models are zero-shot reasoners," _CoRR_, vol. abs/2205.11916, 2022.\n' +
      '* [506] W. Chen, X. Ma, X. Wang, and W. W. Cohen, "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks," _CoRR_, vol. abs/2211.12588, 2022.\n' +
      '* [507] L. Gao, A. Madan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig, "PAL: program-aided language models," in _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, Eds., 2023.\n' +
      '* [508] X. Zhao, Y. Xie, K. Kawaguchi, J. He, and Q. Xie, "Automatic model selection with large language models for reasoning," _CoRR_, vol. abs/2305.14333, 2023.\n' +
      '* [509] Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J.-G. Lou, and W. Chen, "Making large language models better reasomers with step-aware verifier," 2023.\n' +
      '* [510] O. Yoran, T. Wolfson, B. Bogin, U. Katz, D. Deutch, and J. Berant, "Answering questions by meta-reasoning over multiple chains of thought," _CoRR_, vol. abs/2304.13007, 2023.\n' +
      '* [511] Z. Ling, Y. Fang, X. Li, Z. Huang, M. Lee, R. Memisevic, and H. Su, "Deductive verification of chain-of-thought reasoning," _CoRR_, vol. abs/2306.03872, 2023.\n' +
      '* [512] T. Xue, Z. Wang, Z. Wang, C. Han, P. Yu, and H. Ji, "RCOT: detecting and rectifying factual inconsistency in reasoning by reversing chain-of-thought," _CoRR_, vol. abs/2305.11499, 2023.\n' +
      '* [513] Y. Weng, M. Zhu, F. Xia, B. Li, S. He, K. Liu, and J. Zhao, "Large language models are better reasoners with self-verification," _CoRR, abs/2212.09561_, 2023.\n' +
      '* [514] W. Jiang, H. Shi, L. Yu, Z. Liu, Y. Zhang, Z. Li, and J. T. Kwok, "Forward-backward reasoning in large language models for mathematical verification," 2023.\n' +
      '* [515] J. Long, "Large language model guided tree-of-thought," _CoRR_, vol. abs/2305.08291, 2023.\n' +
      '* [516] S. Mo and M. Xin, "Tree of uncertain thoughts reasoning for large language models," _CoRR_, vol. abs/2309.07694, 2023.\n' +
      '* [517] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, L. Gianinazzi, J. Gajda, T. Lehmann, M. Podstawski,H. Niewiadomski, P. Nyczyk, and T. Hoefler, "Graph of thoughts: Solving elaborate problems with large language models," _CoRR_, vol. abs/2308.09687, 2023.\n' +
      '* [518] B. Lei, P. Lin, C. Liao, and C. Ding, "Boosting logical reasoning in large language models through a new framework: The graph of thought," _CoRR_, vol. abs/2308.08614, 2023.\n' +
      '* [519] R. Ding, C. Zhang, L. Wang, Y. Xu, M. Ma, W. Zhang, S. Qin, S. Rajmohan, Q. Lin, and D. Zhang, "Everything of thoughts: Defying the law of penrose triangle for thought generation," _arXiv preprint arXiv:2311.04254_, 2023.\n' +
      '* [520] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, B. Newman, B. Yuan, B. Yan, C. Zhang, C. Cosgrove, C. D. Manning, C. Re, D. Acosta-Navas, D. A. Hudson, E. Zelikman, E. Durmus, F. Ladhak, F. Rong, H. Ren, H. Yao, J. Wang, K. Santhaman, L. J. Orr, L. Zheng, M. Yuksekgonul, M. Suzgun, N. Kim, N. Guha, N. S. Chatterji, O. Khattab, P. Henderson, Q. Huang, R. Chi, S. M. Xie, S. Santurkar, S. Ganguli, T. Hashimoto, T. Icard, T. Zhang, V. Chaudhary, W. Wang, X. Li, Y. Mai, Y. Zhang, and Y. Koreeda, "Holistic evaluation of language models," _CoRR_, vol. abs/2211.09110, 2022.\n' +
      '* [521] Z. Bi, N. Zhang, Y. Jiang, S. Deng, G. Zheng, and H. Chen, "When do program-of-thoughts work for reasoning?" _CoRR_, vol. abs/2308.15452, 2023.\n' +
      '* [522] A. Madan and A. Yazdanbakhsh, "Text and patterns: For effective chain of thought, it takes two to tango," _CoRR_, vol. abs/2209.07686, 2022.\n' +
      '* [523] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, and A. Smola, "Multimodal chain-of-thought reasoning in language models," _CoRR_, vol. abs/2302.00923, 2023.\n' +
      '* [524] F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder, D. Zhou, D. Das, and J. Wei, "Language models are multilingual chain-of-thought reasoners," _CoRR_, vol. abs/2210.03057, 2022.\n' +
      '* [525] J. Qian, H. Wang, Z. Li, S. Li, and X. Yan, "Limitations of language models in arithmetic and symbolic induction," _CoRR_, vol. abs/2208.05051, 2022.\n' +
      '* [526] N. Bian, X. Han, L. Sun, H. Lin, Y. Lu, and B. He, "ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models," _CoRR_, 2023.\n' +
      '* [527] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan, "Tree of thoughts: Deliberate problem solving with large language models," _CoRR_, vol. abs/2305.10601, 2023.\n' +
      '* [528] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar, "Voyager: An open-ended embodied agent with large language models," _arXiv preprint arXiv:2305.16291_, 2023.\n' +
      '* [529] X. Jiang, Y. Dong, L. Wang, Q. Shang, and G. Li, "Self-planning code generation with large language model," _CoRR_, vol. abs/2303.06689, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2303.06689](https://doi.org/10.48550/arXiv.2303.06689) prompt: Generating situated robot task plans using large language models," _CoRR_, vol. abs/2209.11302, 2022.\n' +
      '* [530] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone, "LLM+P: empowering large language models with optimal planning proficiency," _CoRR_, vol. abs/2304.11477, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2304.11477](https://doi.org/10.48550/arXiv.2304.11477)\n' +
      '* [531] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, "High-resolution image synthesis with latent diffusion models," in _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, 2022, pp. 10 674-10 685.\n' +
      '* [532] J. S. Park, J. C. O\'Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein, "Generative agents: Interactive simulacra of human behavior," _CoRR_, vol. abs/2304.03442, 2023.\n' +
      '* [533] 2023. [Online]. Available: [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT)\n' +
      '* [534] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents," _CoRR_, vol. abs/2302.01560, 2023.\n' +
      '* [535] J. Wang, X. Yi, R. Guo, H. Jin, P. Xu, S. Li, X. Wang, X. Guo, C. Li, X. Xu _et al._, "Milvus: A purpose-built vector data management system," in _Proceedings of the 2021 International Conference on Management of Data_, 2021, pp. 2614-2627.\n' +
      '* [536] W. Zhong, L. Guo, Q. Gao, H. Ye, and Y. Wang, "Memorybank: Enhancing large language models with long-term memory," _CoRR_, vol. abs/2305.10250, 2023.\n' +
      '* [537] M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz, "Building a large annotated corpus of english: The penn treebank," _Comput. Linguistics_, vol. 19, no. 2, pp. 313-330, 1993.\n' +
      '* [538] S. Merity, C. Xiong, J. Bradbury, and R. Socher, "Pointer sentinel mixture models," in _ICLR (Poster)_. OpenReview.net, 2017.\n' +
      '* [539] O. Bojar, C. Buck, C. Federmann, B. Haddow, P. Koehn, J. Leveling, C. Monz, P. Pecina, M. Post, H. Saint-Amand, R. Soricut, L. Specia, and A. Tamchyna, "Findings of the 2014 workshop on statistical machine translation," in _WMT@ACL_. The Association for Computer Linguistics, 2014, pp. 12-58.\n' +
      '* [540] O. Bojar, R. Chatterjee, C. Federmann, Y. Graham, B. Haddow, M. Huck, A. Jimeno-Yepes, P. Koehn, V. Logacheva, C. Monz, M. Negri, A. Neveol, M. L. Neves, M. Popel, M. Post, R. Rubino, C. Scarton, L. Specia, M. Turchi, K. Verspoor, and M. Zampieri, "Findings of the 2016 conference on machine translation," in _WMT_. The Association for Computer Linguistics, 2016, pp. 131-198.\n' +
      '* 볼륨 2: 공유 작업 문서, 1일_, O. 보자르 채터지 페더만 피셸 그레이엄 헉\n' +
      'A. Jimeno-Yepes, P. Koehn, A. Martins, C. Monz, M. 네그리, A. 네벌, M. L. 네베스, M. 포스트엠 Turchi, K. Verspoor (2019) A. Jimeno-Yepes, P. Koehn, A. Martins, C. Monz, M. 네그리, A. 네벌, M. L. 네베스, M. 포스트엠 Turchi, K. 불쌍해 (2019) A. Jimeno-Yepes, P. Koehn, A. Martins, C. Monz, M. 네그리, A. 네벌, M. L. 네베스, M. 포스트엠 Turchi, K. 불쌍해 (2020) A. Jimeno-Yepes, P. Koehn, A. Martins, C. Monz, M.\n' +
      '\n' +
      '- August 4, Volume 1: Long Papers_, 2017, pp. 1601-1611.\n' +
      '* [559] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, "PIQA: reasoning about physical commonsense in natural language," in _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, 2020_, pp. 7432-7439.\n' +
      '* ISWC 2019\n' +
      '- 18th International Semantic Web Conference, Auckland, New Zealand, October 26-30, 2019, Proceedings, Part II_, 2019, pp.69-78.\n' +
      '* [561] Y. Gu, S. Kase, M. Vanni, B. M. Sadler, P. Liang, X. Yan, and Y. Su, "Beyond I.I.D.: three levels of generalization for question answering on knowledge bases," in _WWW \'21: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021_, 2021, pp. 3477-3488.\n' +
      '* [562] S. Cao, J. Shi, L. Pan, L. Nie, Y. Xiang, L. Hou, J. Li, B. He, and H. Zhang, "KQA pro: A dataset with explicit compositional programs for complex question answering over knowledge base," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, 2022, pp. 6101-6119.\n' +
      '* [563] X. Hu, X. Wu, Y. Shu, and Y. Qu, "Logical form generation via multi-task learning for complex question answering over knowledge bases," in _Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022_, 2022, pp. 1687-1696.\n' +
      '* [564] S. Longpre, Y. Lu, and J. Daiber, "MKQA: A linguistically diverse benchmark for multilingual open domain question answering," _Trans. Assoc. Comput. Linguistics_, vol. 9, pp. 1389-1406, 2021.\n' +
      '* [565] T. Saikh, T. Ghosal, A. Mittal, A. Ekbal, and P. Bhattacharyya, "Scienceqa: a novel resource for question answering on scholarly articles," _Int. J. Digit. Libr._, vol. 23, no. 3, pp. 289-301, 2022.\n' +
      '* November 4, 2018_, 2018, pp. 2381-2391.\n' +
      '* [567] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder, and L. Deng, "MS MARCO: A human generated machine reading comprehension dataset," in _Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016_, 2016.\n' +
      '* [568] T. Khot, P. Clark, M. Guerquin, P. Jansen, and A. Sabharwal, "QASC: A dataset for question answering via sentence composition," in _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, 2020, pp. 8082-8090.\n' +
      '* [569] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, "Squad: 100, 000+ questions for machine comprehension of text," in _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016_, 2016, pp. 2383-2392.\n' +
      '* [570] A. H. Miller, A. Fisch, J. Dodge, A. Karimi, A. Bordes, and J. Weston, "Key-value memory networks for directly reading documents," in _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016_, 2016, pp. 1400-1409.\n' +
      '* [571] B. Goodrich, V. Rao, P. J. Liu, and M. Saleh, "Assessing the factual accuracy of generated text," in _Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019_, 2019, pp. 166-175.\n' +
      '* [572] K. Toutanova and D. Chen, "Observed versus latent features for knowledge base and text inference," in _Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality, CVSC 2015, Beijing, China, July 26-31, 2015_, 2015, pp. 57-66.\n' +
      '* [573] K. D. Bollacker, C. Evans, P. K. Paritosh, T. Sturge, and J. Taylor, "Freebase: a collaboratively created graph database for structuring human knowledge," in _Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2008, Vancouver, BC, Canada, June 10-12, 2008_, 2008, pp. 1247-1250.\n' +
      '* [574] T. Dettmers, P. Minervini, P. Stenetorp, and S. Riedel, "Convolutional 2d knowledge graph embeddings," in _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018_, 2018, pp. 1811-1818.\n' +
      '* [575] G. A. Miller, "Wordnet: A lexical database for english," _Commun. ACM_, pp. 39-41, 1995.\n' +
      '* [576] F. Petroni, T. Rocktaschel, S. Riedel, P. S. H. Lewis, A. Bakhtin, Y. Wu, and A. H. Miller, "Language models as knowledge bases?" in _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019_, 2019, pp. 2463-2473.\n' +
      '* [577] F. Mahdisoltani, J. Biega, and F. M. Suchanek, "YAGO3: A knowledge base from multilingual wikipedias," in _Seventh Biennial Conference on Innovative Data Systems Research, CIDR 2015, Asilomar, CA, USA, January 4-7, 2015, Online Proceedings_, 2015.\n' +
      '* [578] F. M. Suchanek, G. Kasneci, and G. Weikum, "Yago: a core of semantic knowledge," in _Proceedings of the 16th International Conference on World Wide Web, WWW 2007, Banff, Alberta, Canada, May 8-12, 2007_, 2007, pp.\n' +
      '\n' +
      '697-706.\n' +
      '* 2018년 11월 4일_. Association for Computational Linguistics, 2018, pp. 2369-2380.\n' +
      '* [580] C. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova, "Boolq: Exploring the surprising difficulty of natural yes/no questions," in _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, J. Burstein, C. Doran, and T. Solorio, Eds. Association for Computational Linguistics, 2019, pp. 2924-2936.\n' +
      '* [581] M. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y. Choi, "Socialiqa: Commonsense reasoning about social interactions," _CoRR_, vol. abs/1904.09728, 2019.\n' +
      '* [582] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, "Hellaswag: Can a machine really finish your sentence?" in _Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL, 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers_, A. Korhonen, D. R. Traum, and L. Marquez, Eds. Association for Computational Linguistics, 2019, pp. 4791-4800.\n' +
      '* [583] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi, "Winogrande: An adversarial winograd schema challenge at scale," in _AAAI_. AAAI Press, 2020, pp. 8732-8740.\n' +
      '* [584] M. Roemmele, C. A. Bejan, and A. S. Gordon, "Choice of plausible alternatives: An evaluation of commonsense causal reasoning," in _Logical Formalizations of Commonsense Reasoning, Papers from the 2011 AAAI Spring Symposium, Technical Report SS-11-06, Stanford, California, USA, March 21-23, 2011_. AAAI, 2011.\n' +
      '* [585] K. Sakaguchi, C. Bhagavatula, R. L. Bras, N. Tandon, P. Clark, and Y. Choi, "proscript: Partially ordered scripts generation," in _Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021_, M. Moens, X. Huang, L. Specia, and S. W. Yih, Eds. Association for Computational Linguistics, 2021, pp. 2138-2149.\n' +
      '* [586] B. Dalvi, L. Huang, N. Tandon, W. Yih, and P. Clark, "Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension," in _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers)_, M. A. Walker, H. Ji, and A. Stent, Eds. Association for Computational Linguistics, 2018, pp. 1595-1604.\n' +
      '* [587] S. Saha, P. Yadav, L. Bauer, and M. Bansal, "Explagraphs: An explanation graph generation task for structured commonsense reasoning," in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021_, M. Moens, X. Huang, L. Specia, and S. W. Yih, Eds. Association for Computational Linguistics, 2021, pp. 7716-7740.\n' +
      '* [588] O. Tafjord, B. Dalvi, and P. Clark, "Proofwriter: Generating implications, proofs, and abductive statements over natural language," in _Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021_, ser. Findings of ACL, C. Zong, F. Xia, W. Li, and R. Navigli, Eds., vol. ACL/IJCNLP 2021. Association for Computational Linguistics, 2021, pp. 3621-3634.\n' +
      '* [589] B. Dalvi, P. Jansen, O. Tafjord, Z. Xie, H. Smith, L. Pipatanangkura, and P. Clark, "Explaining answers with entailment trees," in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021_, M. Moens, X. Huang, L. Specia, and S. W. Yih, Eds. Association for Computational Linguistics, 2021, pp. 7358-7370.\n' +
      '* [590] A. Saparov and H. He, "Language models are greedy reasoners: A systematic formal analysis of chain-of-thought," _CoRR_, vol. abs/2210.01240, 2022.\n' +
      '* [591] C. Anil, Y. Wu, A. Andreassen, A. Lewkowycz, V. Misra, V. V. Ramasesh, A. Slone, G. Gur-Ari, E. Dyer, and B. Neyshabur, "Exploring length generalization in large language models," _CoRR_, vol. abs/2207.04901, 2022.\n' +
      '* [592] A. Patel, S. Bhattacharya, and N. Goyal, "Are NLP models really able to solve simple math word problems?" in _NAACL-HLT_. Association for Computational Linguistics, 2021, pp. 2080-2094.\n' +
      '* [593] S. Roy and D. Roth, "Solving general arithmetic word problems," in _Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015_, L. Marquez, C. Callison-Burch, J. Su, D. Pighin, and Y. Marton, Eds. The Association for Computational Linguistics, 2015, pp. 1743-1752.\n' +
      '* [594] A. Amini, S. Gabriel, S. Lin, R. Koncel-Kedziorski, Y. Choi, and H. Hajishirzi, "Mathqa: Towards interpretable math word problem solving with operation-based formalisms," in _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, J. Burstein, C. Doran, and T. Solorio, Eds. Association for Computational Linguistics, 2019, pp. 2357-2367.\n' +
      '* 8월 4일, Volume 1: Long Papers_, R. 바질레이와 M. Kan, Eds. Association for Computational Linguistics, 2017, pp. 158-167.\n' +
      '* [596] R. Koncel-Kedziorski, S. Roy, A. Amini, N. Kushman, and H. Hajishirzi, "Mawps: A math word problem repository," in _Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies_, 2016, pp. 1152-1157.\n' +
      '* [597] D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner, "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs," in _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, 2019, pp. 2368-2378.\n' +
      '* [598] S. Welleck, J. Liu, R. L. Bras, H. Hajishirzi, Y. Choi, and K. Cho, "Naturalproofs: Mathematical theorem proving in natural language," in _Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual_, J. Vanschoren and S. Yeung, Eds., 2021.\n' +
      '* [599] A. Q. Jiang, W. Li, J. M. Han, and Y. Wu, "Lisa: Language models of isabelle proofs," in _6th Conference on Artificial Intelligence and Theorem Proving_, 2021, pp. 378-392.\n' +
      '* [600] K. Zheng, J. M. Han, and S. Polu, "mini2f: a cross-system benchmark for formal olympiad-level mathematics," in _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.\n' +
      '* [601] Z. Azerbayev, B. Piotrowski, H. Schoelkopf, E. W. Ayers, D. Radev, and J. Avigad, "Proofnet: Autoformalizing and formally proving undergraduate-level mathematics," _CoRR_, vol. abs/2302.12433, 2023.\n' +
      '* [602] J. Li, X. Cheng, W. X. Zhao, J. Nie, and J. Wen, "Halueval: A large-scale hallucination evaluation benchmark for large language models," _CoRR_, vol. abs/2305.11747, 2023.\n' +
      '* [603] N. Nangia, C. Vania, R. Bhalerao, and S. R. Bowman, "Crows-pairs: A challenge dataset for measuring social biases in masked language models," in _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020_, 2020, pp. 1953-1967.\n' +
      '* [604] R. Rudinger, J. Naradowsky, B. Leonard, and B. V. Durme, "Gender bias in coreference resolution," in _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short Papers)_, 2018, pp. 8-14.\n' +
      '* [605] S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith, "Realtoxicityprompts: Evaluating neural toxic degeneration in language models," in _Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020_, ser. Findings of ACL, T. Cohn, Y. He, and Y. Liu, Eds., vol. EMNLP 2020. Association for Computational Linguistics, 2020, pp. 3356-3369.\n' +
      '* [606] X. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba, "Virtualhome: Simulating household activities via programs," in _CVPR_. Computer Vision Foundation / IEEE Computer Society, 2018, pp. 8494-8502.\n' +
      '* [607] S. Srivastava, C. Li, M. Lingelbach, R. Martin-Martin, F. Xia, K. E. Vainio, Z. Lian, C. Gokmen, S. Buch, C. K. Liu, S. Savarese, H. Gweon, J. Wu, and L. Fei-Fei, "BEHAVIOR: benchmark for everyday household activities in virtual, interactive, and ecological environments," in _CoRL_, ser. Proceedings of Machine Learning Research, vol. 164. PMLR, 2021, pp. 477-490.\n' +
      '* [608] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox, "ALFRED: A benchmark for interpreting grounded instructions for everyday tasks," in _CVPR_. Computer Vision Foundation / IEEE, 2020, pp. 10 737-10 746.\n' +
      '* [609] M. Shridhar, X. Yuan, M. Cote, Y. Bisk, A. Trischler, and M. J. Hausknecht, "Alfworld: Aligning text and embodied environments for interactive learning," in _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.\n' +
      '* [610] S. Yao, H. Chen, J. Yang, and K. Narasimhan, "Webshop: Towards scalable real-world web interaction with grounded language agents," in _NeurIPS_, 2022.\n' +
      '* [611] X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su, "Mind2web: Towards a generalist agent for the web," _CoRR_, vol. abs/2306.06070, 2023.\n' +
      '* [612] W. H. Guss, B. Houghton, N. Topin, P. Wang, C. Codel, M. Veloso, and R. Salakhutdinov, "Minerl: A large-scale dataset of minecraft demonstrations," in _Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019_, S. Kraus, Ed. ijcai.org, 2019, pp. 2442-2448.\n' +
      '* [613] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D. Huang, Y. Zhu, and A. Anandkumar, "Minedjo: Building open-ended embodied agents with internet-scale knowledge," in _NeurIPS_, 2022.\n' +
      '* [614] P. Lu, L. Qiu, K. Chang, Y. N. Wu, S. Zhu, T. Rajpurohit, P. Clark, and A. Kalyan, "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning," _CoRR_, vol. abs/2209.14610, 2022.\n' +
      '* [615] B. Zhang, K. Zhou, X. Wei, W. X. Zhao, J. Sha, S. Wang, and J. rong Wen, "Evaluating and improving tool-augmented computation-intensive math reasoning," _CoRR_, vol. abs/2306.02408, 2023.\n' +
      '* [616] R. Yang, L. Song, Y. Li, S. Zhao, Y. Ge, X. Li, and Y. Shan, "Gpt4tools: Teaching large language model to use tools via self-instruction," _CoRR_, vol. abs/2305.18752, 2023.\n' +
      '* [617] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, "Gorilla: Large language model connected with massive apis," _CoRR_, vol. abs/2305.15334, 2023.\n' +
      '* [618] W. Yih, M. Richardson, C. Meek, M. Chang, and J. Suh, "The value of semantic parse labeling for knowledge base question answering," in _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short Papers_. The Association for Computer Linguistics, 2016.\n' +
      '\n' +
      '* [619] H. Puerto, G. G. Sahin, and I. Gurevych, "Metaqa: Combining expert agents for multi-skill question answering," in _Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023_, A. Vlachos and I. Augenstein, Eds. Association for Computational Linguistics, 2023, pp. 3548-3562.\n' +
      '* [620] P. Pasupat and P. Liang, "Compositional semantic parsing on semi-structured tables," in _Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers_. The Association for Computer Linguistics, 2015, pp. 1470-1480.\n' +
      '* [621] V. Zhong, C. Xiong, and R. Socher, "Seq2sql: Generating structured queries from natural language using reinforcement learning," _CoRR_, vol. abs/1709.00103, 2017.\n' +
      '* [622] W. Chen, H. Wang, J. Chen, Y. Zhang, H. Wang, S. Li, X. Zhou, and W. Y. Wang, "Tabfact: A large-scale dataset for table-based fact verification," in _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.\n' +
      '* November 4, 2018_, E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsuji, Eds. Association for Computational Linguistics, 2018, pp. 3911-3921.\n' +
      '* [624] D. Bahdanau, K. Cho, and Y. Bengio, "Neural machine translation by jointly learning to align and translate," in _ICLR_, 2015.\n' +
      '* [625] K. Papineni, S. Roukos, T. Ward, and W. Zhu, "Bleu: a method for automatic evaluation of machine translation," in _Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA_. ACL, 2002, pp. 311-318.\n' +
      '* [626] C.-Y. Lin, "ROUGE: A package for automatic evaluation of summaries," in _Text Summarization Branches Out_. Association for Computational Linguistics, Jul. 2004, pp. 74-81.\n' +
      '* [627] W. Jiao, W. Wang, J.-t. Huang, X. Wang, and Z. Tu, "Is chatgpt a good translator? a preliminary study," _arXiv preprint arXiv:2301.08745_, 2023.\n' +
      '* [628] T. Zhang, F. Ladhak, E. Durmus, P. Liang, K. R. McKeown, and T. B. Hashimoto, "Benchmarking large language models for news summarization," _CoRR_, vol. abs/2301.13848, 2023.\n' +
      '* [629] T. Goyal, J. J. Li, and G. Durrett, "News summarization and evaluation in the era of GPT-3," _CoRR_, vol. abs/2209.12356, 2022.\n' +
      '* [630] S. Gehrmann, E. Clark, and T. Sellam, "Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text," _CoRR_, vol. abs/2202.06935, 2022.\n' +
      '* [631] J. Wang, Y. Liang, F. Meng, H. Shi, Z. Li, J. Xu, J. Qu, and J. Zhou, "Is chatgpt a good NLG evaluator? A preliminary study," _CoRR_, vol. abs/2303.04048, 2023.\n' +
      '* [632] Y. Liu, D. Iter, Y. Xu, S. Wang, R. Xu, and C. Zhu, "G-eval: NLG evaluation using GPT-4 with better human alignment," _CoRR_, vol. abs/2303.16634, 2023.\n' +
      '* [633] K. Yang, Y. Tian, N. Peng, and D. Klein, "Re3: Generating longer stories with recursive repropting and revision," in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emitrates, December 7-11, 2022_, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds. Association for Computational Linguistics, 2022, pp. 4393-4479.\n' +
      '* [634] W. Zhou, Y. E. Jiang, P. Cui, T. Wang, Z. Xiao, Y. Hou, R. Cotterell, and M. Sachan, "Recurrentpt: Interactive generation of (arbitrarily) long text," _CoRR_, vol. abs/2305.13304, 2023.\n' +
      '* [635] S. Gulwani, O. Polozov, and R. Singh, "Program synthesis," _Found. Trends Program. Lang._, vol. 4, no. 1-2, pp. 1-119, 2017.\n' +
      '* [636] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan, "Planning with large language models for code generation," 2023.\n' +
      '* [637] M. Welsh, "The end of programming," _Commun. ACM_, vol. 66, no. 1, pp. 34-35, 2023.\n' +
      '* [638] Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. WiliE, H. Lovenia, Z. Ji, T. Yu, W. Chung, Q. V. Do, Y. Xu, and P. Fung, "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity," _CoRR_, vol. abs/2302.04023, 2023.\n' +
      '* [639] Y. Liu, A. R. Fabbri, P. Liu, Y. Zhao, L. Nan, R. Han, S. Han, S. R. Joty, C. Wu, C. Xiong, and D. Radev, "Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation," _CoRR_, vol. abs/2212.07981, 2022.\n' +
      '* [640] A. R. Fabbri, W. Kryscinski, B. McCann, C. Xiong, R. Socher, and D. R. Radev, "Summeval: Re-evaluating summarization evaluation," _Trans. Assoc. Comput. Linguistics_, vol. 9, pp. 391-409, 2021.\n' +
      '* [641] T. Tang, H. Lu, Y. E. Jiang, H. Huang, D. Zhang, W. X. Zhao, and F. Wei, "Not all metrics are guilty: Improving NLG evaluation with LLM paraphrasing," _CoRR_, vol. abs/2305.15067, 2023.\n' +
      '* [642] X. Wang, X. Tang, W. X. Zhao, J. Wang, and J. Wen, "Rethinking the evaluation for conversational recommendation in the era of large language models," _CoRR_, vol. abs/2305.13112, 2023.\n' +
      '* [643] M. Gao, J. Ruan, R. Sun, X. Yin, S. Yang, and X. Wan, "Human-like summarization evaluation with chatgpt," _CoRR_, vol. abs/2304.02554, 2023.\n' +
      '* [644] Y. Ji, Y. Gong, Y. Peng, C. Ni, P. Sun, D. Pan, B. Ma, and X. Li, "Exploring chatgpt\'s ability to rank content: A preliminary study on consistency with human preferences," _CoRR_, vol. abs/2303.07610, 2023.\n' +
      '* [645] Y. Bai, J. Ying, Y. Cao, X. Lv, Y. He, X. Wang, J. Yu, K. Zeng, Y. Xiao, H. Lyu, J. Zhang, J. Li, and L. Hou, "Benchmarking foundation models with language-model-as-an-examiner," _CoRR_, vol. abs/2306.04181,2023.\n' +
      '* [646] Y. Liu, S. Feng, D. Wang, Y. Zhang, and H. Schutze, "Evaluate what you can\'t evaluate: Unassessable generated responses quality," _CoRR_, vol. abs/2305.14658, 2023.\n' +
      '* [647] P. Wang, L. Li, L. Chen, D. Zhu, B. Lin, Y. Cao, Q. Liu, T. Liu, and Z. Sui, "Large language models are not fair evaluators," _CoRR_, vol. abs/2305.17926, 2023.\n' +
      '* [648] J. Ye, X. Chen, N. Xu, C. Zu, Z. Shao, S. Liu, Y. Cui, Z. Zhou, C. Gong, Y. Shen, J. Zhou, S. Chen, T. Gui, Q. Zhang, and X. Huang, "A comprehensive capability analysis of gpt-3 and gpt-3.5 series models," _arXiv preprint arXiv:2303.10420_, 2023.\n' +
      '* [649] M. McCloskey and N. J. Cohen, "Catastrophic interference in connectionist networks: The sequential learning problem," in _Psychology of learning and motivation_, 1989, pp. 109-165.\n' +
      '* [650] R. Kemker, M. McClure, A. Abitino, T. L. Hayes, and C. Kanan, "Measuring catastrophic forgetting in neural networks," in _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018_, 2018, pp. 3390-3398.\n' +
      '* [651] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C. Wu, M. Zhong, P. Yin, S. I. Wang, V. Zhong, B. Wang, C. Li, C. Boyle, A. Ni, Z. Yao, D. Radev, C. Xiong, L. Kong, R. Zhang, N. A. Smith, L. Zettlemoyer, and T. Yu, "Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models," in _EMNLP_. Association for Computational Linguistics, 2022, pp. 602-631.\n' +
      '* [652] A. Roberts, C. Raffel, and N. Shazeer, "How much knowledge can you pack into the parameters of a language model?" _in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020_, 2020, pp. 5418-5426.\n' +
      '* [653] G. Izacard, P. S. H. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, "Few-shot learning with retrieval augmented language models," _CoRR_, vol. abs/2208.03299, 2022.\n' +
      '* [654] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang, "Retrieval augmented language model pre-training," in _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, 2020, pp. 3929-3938.\n' +
      '* [655] P. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W. Yih, T. Rocktaschel, S. Riedel, and D. Kiela, "Retrieval-augmented generation for knowledge-intensive NLP tasks," in _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.\n' +
      '* [656] Y. Lan, G. He, J. Jiang, J. Jiang, W. X. Zhao, and J. Wen, "Complex knowledge base question answering: A survey," _CoRR_, vol. abs/2108.06688, 2021.\n' +
      '* [657] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. van den Driessche, J. Lespiau, B. Damoc, A. Clark, D. de Las Casas, A. Guy, J. Menick, R. Ring, T. Hennigan, S. Huang, L. Maggiore, C. Jones, A. Cassirer, A. Brock, M. Paganini, G. Irving, O. Vinyals, S. Osindero, K. Simonyan, J. W. Rae, E. Elsen, and L. Sifre, "Improving language models by retrieving from trillions of tokens," in _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, ser. Proceedings of Machine Learning Research, K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, Eds., vol. 162. PMLR, 2022, pp. 2206-2240.\n' +
      '* [658] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, "Search-in-the-chain: Towards accurate, credible and traceable large language models for knowledge-intensive tasks," _CoRR_, vol. abs/2304.14732, 2023.\n' +
      '* [659] B. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang, L. Liden, Z. Yu, W. Chen, and J. Gao, "Check your facts and try again: Improving large language models with external knowledge and automated feedback," _CoRR_, vol. abs/2302.12813, 2023.\n' +
      '* [660] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang, J. Callan, and G. Neubig, "Active retrieval augmented generation," _CoRR_, vol. abs/2305.06983, 2023.\n' +
      '* [661] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, and T. Liu, "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions," _CoRR_, vol. abs/2311.05232, 2023.\n' +
      '* [662] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J. Wen, "Evaluating object hallucination in large vision-language models," _CoRR_, vol. abs/2305.10355, 2023.\n' +
      '* [663] S. Kadavath, T. Conerly, A. Askell, T. J. Henighan, D. Drain, E. Perez, N. Schiefer, Z. Dodds, N. Das-Sarma, E. Tran-Johnson, S. Johnston, S. El-Showk, A. Jones, N. Elhage, T. Hume, A. Chen, Y. Bai, S. Bowman, S. Fort, D. Ganguli, D. Hernandez, J. Jacobson, J. Kernion, S. Kravec, L. Lovitt, K. Ndousse, C. Olsson, S. Ringer, D. Amodei, T. B. Brown, J. Clark, N. Joseph, B. Mann, S. McCandlish, C. Olah, and J. Kaplan, "Language models (mostly) know what they know," _CoRR_, vol. abs/2207.05221, 2022.\n' +
      '* [664] P. Manakul, A. Liuise, and M. J. F. Gales, "Selfcheck-gpt: Zero-resource black-box hallucination detection for generative large language models," _ArXiv_, vol. abs/2305.06983, 2023.\n' +
      '* [665] S. Agarwal, I. Akkaya, V. Balcom, M. Bavarian, G. Bernadett-Shapiro, G. Brockman, M. Brundage, J. Chan, F. Chantzis, N. Deutsch, B. Eastman, A. Eleti, N. Felix, S. P. Fishman, I. Fulford, C. Gibson, J. Gross, M. Heaton, J. Hilton, X. Hu, S. Jain, H. Jin, L. Kilpatrick, C. Kim, M. Kolhede, A. Mayne, P. McMillan, D. Medina, J. Menick, A. Mishchenko, A. Nair, R. Nayak, A. Neelakantan, R. Nuttall, J. Parish, A. T. Passos, A. Perelman, F. de Avila Belbute Peres, V. Pong, J. Schulman, E. Sigler, N. Staudacher, N. Turley, J. Tworek, R. Greene, A. Vijayvergiya, C. Voss,J. Weng, M. Wiethoff, S. Yoo, K. Yu, W. Zaremba, S. Zhao, W. Zhuk, and B. Zoph, "Chatgpt plugins," _OpenAI Blog_, March 2023.\n' +
      '* [666] A. Lazaridou, E. Gribovskaya, W. Stokowiec, and N. Grigorev, "Internet-augmented language models through few-shot prompting for open-domain question answering," _CoRR_, vol. abs/2203.05115, 2022.\n' +
      '* [667] H. Qian, Y. Zhu, Z. Dou, H. Gu, X. Zhang, Z. Liu, R. Lai, Z. Cao, J. Nie, and J. Wen, "Webbrain: Learning to generate factually correct articles for queries by grounding on large web corpus," _CoRR_, vol. abs/2304.04358, 2023.\n' +
      '* [668] J. Liu, J. Jin, Z. Wang, J. Cheng, Z. Dou, and J. Wen, "RETA-LLM: A retrieval-augmented large language model toolkit," _CoRR_, vol. abs/2306.05212, 2023.\n' +
      '* [669] D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei, "Knowledge neurons in pretrained transformers," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, S. Muresan, P. Nakov, and A. Villavicencio, Eds. Association for Computational Linguistics, 2022, pp. 8493-8502.\n' +
      '* [670] K. Meng, D. Bau, A. J. Andonian, and Y. Belinkov, "Locating and editing factual associations in gpt," in _Advances in Neural Information Processing Systems_, 2022.\n' +
      '* [671] M. Geva, R. Schuster, J. Berant, and O. Levy, "Transformer feed-forward layers are key-value memories," in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021_, M. Moens, X. Huang, L. Specia, and S. W. Yih, Eds. Association for Computational Linguistics, 2021, pp. 5484-5495.\n' +
      '* [672] Y. Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and N. Zhang, "Editing large language models: Problems, methods, and opportunities," _CoRR_, vol. abs/2305.13172, 2023.\n' +
      '* [673] P. Wang, N. Zhang, X. Xie, Y. Yao, B. Tian, M. Wang, Z. Xi, S. Cheng, K. Liu, G. Zheng, and H. Chen, "Easyedit: An easy-to-use knowledge editing framework for large language models," _CoRR_, vol. abs/2308.07269, 2023.\n' +
      '* [674] Z. Shao, Y. Gong, Y. Shen, M. Huang, N. Duan, and W. Chen, "Synthetic prompting: Generating chain-of-thought demonstrations for large language models," _CoRR_, vol. abs/2302.00618, 2023.\n' +
      '* [675] Siftakaur, M. Singh, V. S. B, and N. Malviya, "Mind meets machine: Unravelling gpt-4\'s cognitive psychology," _CoRR_, vol. abs/2303.11436, 2023.\n' +
      '* [676] M. I. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz, M. Bosma, D. Luan, C. Sutton, and A. Odena, "Show your work: Scratchpads for intermediate computation with language models," _CoRR_, vol. abs/2112.00114, 2021.\n' +
      '* [677] J. Qian, H. Wang, Z. Li, S. Li, and X. Yan, "Limitations of language models in arithmetic and symbolic induction," _CoRR_, vol. abs/2208.05051, 2022.\n' +
      '*18, 2022_, A. Zhang and H. Rangwala, Eds. ACM, 2022, pp. 4571-4581.\n' +
      '* 11th International Conference, CICM 2018, Hagenberg, Austria, August 13-17, 2018, Proceedings_, ser. Lecture Notes in Computer Science, F. Rabe, W. M. Farmer, G. O. Passmore, and A. Youssef, Eds., vol. 11006. Springer, 2018, pp. 255-270.\n' +
      '* [680] S. Polu and I. Sutskever, "Generative language modeling for automated theorem proving," _CoRR_, vol. abs/2009.03393, 2020.\n' +
      '* [681] A. Q. Jiang, W. Li, S. Tworkowski, K. Czechowski, T. Odrzygozdz, P. Milos, Y. Wu, and M. Jamnik, "Thor: Wielding hammers to integrate language models and automated theorem provers," _CoRR_, vol. abs/2205.10893, 2022.\n' +
      '* [682] S. Polu, J. M. Han, K. Zheng, M. Baksys, I. Babuschkin, and I. Sutskever, "Formal mathematics statement curriculum learning," _CoRR_, vol. abs/2202.01344, 2022.\n' +
      '* [683] Y. Wu, A. Q. Jiang, W. Li, M. N. Rabe, C. Staats, M. Jamnik, and C. Szegedy, "Autoformalization with large language models," _CoRR_, vol. abs/2205.12615, 2022.\n' +
      '* [684] A. Q. Jiang, S. Welleck, J. P. Zhou, W. Li, J. Liu, M. Jamnik, T. Lacroix, Y. Wu, and G. Lample, "Draft, sketch, and prove: Guiding formal theorem provers with informal proofs," _CoRR_, vol. abs/2210.12283, 2022.\n' +
      '* [685] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Welleck, B. P. Majumder, S. Gupta, A. Yazdandbakhsh, and P. Clark, "Self-refine: Iterative refinement with self-feedback," _CoRR_, vol. abs/2303.17651, 2023.\n' +
      '* [686] N. Shinn, B. Labash, and A. Gopinath, "Reflexion: an autonomous agent with dynamic memory and self-reflection," _CoRR_, vol. abs/2303.11366, 2023.\n' +
      '* [687] Z. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, N. Duan, and W. Chen, "CRITIC: large language models can self-correct with tool-interactive critiquing," _CoRR_, vol. abs/2305.11738, 2023.\n' +
      '* [688] J. Uesato, N. Kushman, R. Kumar, H. F. Song, N. Y. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins, "Solving math word problems with process- and outcome-based feedback," _CoRR_, vol. abs/2211.14275, 2022.\n' +
      '* [689] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe, "Let\'s verify step by step," _CoRR_, vol. abs/2305.20050, 2023.\n' +
      '* [690] Z. Yuan, H. Yuan, C. Tan, W. Wang, and S. Huang, "How well do large language models perform in arithmetic tasks?" _CoRR_, vol. abs/2304.02015, 2023.\n' +
      '* [691] X. Pi, Q. Liu, B. Chen, M. Ziyadi, Z. Lin, Q. Fu, Y. Gao, J. Lou, and W. Chen, "Reasoning like program executors," in _Proceedings of the 2022 Conference on EmpiricalMethods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Enirates, December 7-11, 2022_, 2022, pp. 761-779.\n' +
      '* [692] H. Zhou, A. Nova, H. Larochelle, A. C. Courville, B. Neyshabur, and H. Sedghi, "Teaching algorithmic reasoning via in-context learning," _CoRR_, vol. abs/2211.09066, 2022.\n' +
      '* [693] A. Parisi, Y. Zhao, and N. Fiedel, "TALM: tool augmented language models," _CoRR_, vol. abs/2205.12255, 2022.\n' +
      '* [694] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents," in _ICML_, ser. Proceedings of Machine Learning Research, vol. 162. PMLR, 2022, pp. 9118-9147.\n' +
      '* [695] T. Carta, C. Romac, T. Wolf, S. Lamprier, O. Sigaud, and P. Oudeyer, "Grounding large language models in interactive environments with online reinforcement learning," _CoRR_, vol. abs/2302.02662, 2023.\n' +
      '* [696] X. Zhu, Y. Chen, H. Tian, C. Tao, W. Su, C. Yang, G. Huang, B. Li, L. Lu, X. Wang, Y. Qiao, Z. Zhang, and J. Dai, "Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory," _CoRR_, vol. abs/2305.17144, 2023.\n' +
      '* [697] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar, "Voyager: An open-ended embodied agent with large language models," _CoRR_, vol. abs/2305.16291, 2023.\n' +
      '* [698] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano, K. Jeffrey, S. Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K. Lee, S. Levine, Y. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Sermanet, N. Sievers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, and M. Yan, "Do as I can, not as I say: Grounding language in robotic affordances," _CoRR_, vol. abs/2204.01691, 2022.\n' +
      '* [699] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng, "Code as policies: Language model programs for embodied control," _CoRR_, vol. abs/2209.07753, 2022.\n' +
      '* [700] Y. Fu, H. Peng, T. Khot, and M. Lapata, "Improving language model negotiation with self-play and in-context learning from AI feedback," _CoRR_, vol. abs/2305.10142, 2023.\n' +
      '* [701] N. Mehta, M. Teruel, P. F. Sanz, X. Deng, A. H. Awadallah, and J. Kiseleva, "Improving grounded language understanding in a collaborative environment by interacting with agents through help feedback," _CoRR_, vol. abs/2304.10750, 2023.\n' +
      '* [702] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, "Gorilla: Large language model connected with massive apis," _CoRR_, vol. abs/2305.15334, 2023.\n' +
      '* [703] S. Hao, T. Liu, Z. Wang, and Z. Hu, "Toolfengpt: Augmenting frozen language models with massive tools via tool embeddings," _CoRR_, vol. abs/2305.11554, 2023.\n' +
      '* [704] Y. Liang, C. Wu, T. Song, W. Wu, Y. Xia, Y. Liu, Y. Ou, S. Lu, L. Ji, S. Mao, Y. Wang, L. Shou, M. Gong, and N. Duan, "Taskmatrix.ai: Completing tasks by connecting foundation models with millions of apis," _CoRR_, vol. abs/2303.16434, 2023.\n' +
      '* [705] T. Cai, X. Wang, T. Ma, X. Chen, and D. Zhou, "Large language models as tool makers," _CoRR_, vol. abs/2305.17126, 2023.\n' +
      '* [706] J. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, and J. Han, "Large language models can self-improve," _CoRR_, vol. abs/2210.11610, 2022.\n' +
      '* [707] E. Beeching, C. Fourrier, N. Habib, S. Han, N. Lambert, N. Rajani, O. Sanseviero, L. Tunstall, and T. Wolf, "Open llm leaderboard," [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), 2023.\n' +
      '* [708] W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan, "Agieval: A human-centric benchmark for evaluating foundation models," _CoRR_, vol. abs/2304.06364, 2023.\n' +
      '* [709] H. Zeng, "Measuring massive multitask chinese understanding," _CoRR_, vol. abs/2304.12986, 2023.\n' +
      '* [710] C. Liu, R. Jin, Y. Ren, L. Yu, T. Dong, X. Peng, S. Zhang, J. Peng, P. Zhang, Q. Lyu, X. Su, Q. Liu, and D. Xiong, "M3KE: A massive multi-level multi-subject knowledge evaluation benchmark for chinese large language models," _CoRR_, vol. abs/2305.10263, 2023.\n' +
      '* [711] Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, Y. Fu, M. Sun, and J. He, "C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models," _CoRR_, vol. abs/2305.08322, 2023.\n' +
      '* [712] Z. Gu, X. Zhu, H. Ye, L. Zhang, J. Wang, S. Jiang, Z. Xiong, Z. Li, Q. He, R. Xu, W. Huang, W. Zheng, H. Feng, and Y. Xiao, "Xiezhi: An ever-updating benchmark for holistic domain knowledge evaluation," _CoRR_, vol. abs/2306.05783, 2023.\n' +
      '* [713] O. Contributors, "Opencompass: A universal evaluation platform for foundation models," [https://github.com/InternLM/OpenCompass](https://github.com/InternLM/OpenCompass), 2023.\n' +
      '* [714] Y. Fu, L. Ou, M. Chen, Y. Wan, H. Peng, and T. Khot, "Chain-of-thought hub: A continuous effort to measure large language models\' reasoning performance," _CoRR_, vol. abs/2305.17306, 2023.\n' +
      '* [715] J. Yu, X. Wang, S. Tu, S. Cao, D. Zhang-li, X. Lv, H. Peng, Z. Yao, X. Zhang, H. Li, C. Li, Z. Zhang, Y. Bai, Y. Liu, A. Xin, N. Lin, K. Yun, L. Gong, J. Chen, Z. Wu, Y. Qi, W. Li, Y. Guan, K. Zeng, J. Qi, H. Jin, J. Liu, Y. Gu, Y. Yao, N. Ding, L. Hou, Z. Liu, B. Xu, J. Tang, and J. Li, "Kola: Carefully benchmarking world knowledge of large language models," _CoRR_, vol. abs/2306.09296, 2023.\n' +
      '* [716] T. Sawada, D. Paleka, A. Havrilla, P. Tadepalli, P. Vidas, A. Kranias, J. J. Nay, K. Gupta, and A. Komatuszaki, "ARB: advanced reasoning benchmark for large language models," _CoRR_, vol. abs/2307.13692, 2023.\n' +
      '* [717] Y. Peng, S. Li, W. Gu, Y. Li, W. Wang, C. Gao, and M. R. Lyu, "Revisiting, benchmarking and exploring API recommendation: How far are we?" _IEEE Trans. Software Eng._, vol. 49, no. 4, pp. 1876-1897, 2023.\n' +
      '\n' +
      '* [718] M. Li, F. Song, B. Yu, H. Yu, Z. Li, F. Huang, and Y. Li, "Api-bank: A benchmark for tool-augmented llms," _CoRR_, vol. abs/2304.08244, 2023.\n' +
      '* [719] Q. Tang, Z. Deng, H. Lin, X. Han, Q. Liang, and L. Sun, "Toolalpaca: Generalized tool learning for language models with 3000 simulated cases," _CoRR_, vol. abs/2306.05301, 2023.\n' +
      '* [720] Q. Xu, F. Hong, B. Li, C. Hu, Z. Chen, and J. Zhang, "On the tool manipulation capability of open-source large language models," _CoRR_, vol. abs/2305.16504, 2023.\n' +
      '* [721] Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang, B. Qian, S. Zhao, R. Tian, R. Xie, J. Zhou, M. Gerstein, D. Li, Z. Liu, and M. Sun, "Toolllm: Facilitating large language models to master 16000+ real-world apis," _CoRR_, vol. abs/2307.16789, 2023.\n' +
      '* [722] Z. Liu, W. Yao, J. Zhang, L. Xue, S. Heinecke, R. Murthy, Y. Feng, Z. Chen, J. C. Niebles, D. Arpit, R. Xu, P. Mui, H. Wang, C. Xiong, and S. Savarese, "BOLAA: benchmarking and orchestrating llm-augmented autonomous agents," _CoRR_, vol. abs/2308.05960, 2023.\n' +
      '* [723] X. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang, S. Zhang, X. Deng, A. Zeng, Z. Du, C. Zhang, S. Shen, T. Zhang, Y. Su, H. Sun, M. Huang, Y. Dong, and J. Tang, "Agenthenech: Evaluating llms as agents," _CoRR_, vol. abs/2308.03688, 2023.\n' +
      '* [724] K. Zhu, J. Wang, J. Zhou, Z. Wang, H. Chen, Y. Wang, L. Yang, W. Ye, N. Z. Gong, Y. Zhang, and X. Xie, "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts," _CoRR_, vol. abs/2306.04528, 2023.\n' +
      '* [725] R. S. Shah, K. Chawla, D. Eidnani, A. Shah, W. Du, S. Chava, N. Raman, C. Smiley, J. Chen, and D. Yang, "WHEN FLUE MEETS FLANG: benchmarks and large pre-trained language model for financial domain," _CoRR_, vol. abs/2211.00803, 2022.\n' +
      '* [726] N. Guha, D. E. Ho, J. Nyarko, and C. Re, "Legalbench: Prototyping a collaborative benchmark for legal reasoning," _CoRR_, vol. abs/2209.06120, 2022.\n' +
      '* [727] L. Zheng, W. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica, "Judging llm-as-a-judge with mt-bench and chatbot arena," _CoRR_, vol. abs/2306.05685, 2023.\n' +
      '* [728] X. Wang, Z. Hu, P. Lu, Y. Zhu, J. Zhang, S. Subramaniam, A. R. Leonba, S. Zhang, Y. Sun, and W. Wang, "Scibench: Evaluating college-level scientific problem-solving abilities of large language models," _CoRR_, vol. abs/2307.10635, 2023.\n' +
      '* [729] X. Li, T. Zhang, Y. Dubois, R. Taori, I. Gulrajani, C. Guestrin, P. Liang, and T. B. Hashimoto, "Alpacaeval: An automatic evaluator of instruction-following models," [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval), 2023.\n' +
      '* [730] Y. Huang, Q. Zhang, P. S. Yu, and L. Sun, "Trustgpt: A benchmark for trustworthy and responsible large language models," _CoRR_, vol. abs/2306.11507, 2023.\n' +
      '* [731] Y. Bai, J. Ying, Y. Cao, X. Lv, Y. He, X. Wang, J. Yu, K. Zeng, Y. Xiao, H. Lyu, J. Zhang, J. Li, and L. Hou, "Benchmarking foundation models with language-model-as-an-examiner," _CoRR_, vol. abs/2306.04181, 2023.\n' +
      '* [732] C. Chan, W. Chen, Y. Su, J. Yu, W. Xue, S. Zhang, J. Fu, and Z. Liu, "Chateval: Towards better llm-based evaluators through multi-agent debate," _CoRR_, vol. abs/2308.07201, 2023.\n' +
      '* [733] Y. Chang, X. Wang, J. Wang, Y. Wu, K. Zhu, H. Chen, L. Yang, X. Yi, C. Wang, Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang, and X. Xie, "A survey on evaluation of large language models," _CoRR_, vol. abs/2307.03109, 2023.\n' +
      '* [734] Z. Zhuang, Q. Chen, L. Ma, M. Li, Y. Han, Y. Qian, H. Bai, Z. Feng, W. Zhang, and T. Liu, "Through the lens of core competency: Survey on evaluation of large language models," _CoRR_, vol. abs/2308.07902, 2023.\n' +
      '* [735] J. H. Clark, J. Palomaki, V. Nikolaev, E. Choi, D. Garrette, M. Collins, and T. Kwiatkowski, "Tyld QA: A benchmark for information-seeking question answering in typologically diverse languages," _Trans. Assoc. Comput. Linguistics_, vol. 8, pp. 454-470, 2020.\n' +
      '* [736] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell, N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou, "A framework for few-shot language model evaluation," Sep. 2021.\n' +
      '* [737] R. Shah, K. Chawla, D. Eidnani, A. Shah, W. Du, S. Chava, N. Raman, C. Smiley, J. Chen, and D. Yang, "When flue meets flang: Benchmarks and large pre-trained language model for financial domain," in _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, 2022, pp. 2322-2335.\n' +
      '* [738] K. Zhou, Y. Zhu, Z. Chen, W. Chen, W. X. Zhao, X. Chen, Y. Lin, J.-R. Wen, and J. Han, "Don\'t make your llm an evaluation benchmark cheater," _arXiv preprint arXiv:2311.01964_, 2023.\n' +
      '* [739] C. Zan, K. Peng, L. Ding, B. Qiu, B. Liu, S. He, Q. Lu, Z. Zhang, C. Liu, W. Liu, Y. Zhan, and D. Tao, "Vegamt: The JD explore academy machine translation system for WMT22," in _Proceedings of the Seventh Conference on Machine Translation, WMT 2022, Abu Dhabi, United Arab Emirates (Hybrid), December 7-8, 2022_, P. Koehn, L. Barrault, O. Bojar, F. Bougares, R. Chatterjee, M. R. Costa-jussa, C. Federmann, M. Fishel, A. Fraser, M. Freitag, Y. Graham, R. Grundkiewicz, P. Guzman, B. Haddow, M. Huck, A. Jimeno-Yepes, T. Kocni, A. Martins, M. Morishita, C. Monz, M. Nagata, T. Nakazawa, M. Negri, A. Neveol, M. Neves, M. Popel, M. Turchi, and M. Zampieri, Eds. Association for Computational Linguistics, 2022, pp. 411-422.\n' +
      '* [740] Y. Zhao, M. Khalman, R. Joshi, S. Narayan, M. Saleh, and P. J. Liu, "Calibrating sequence likelihood improves conditional language generation," _CoRR_, vol. abs/2210.00045, 2022. [Online]. Available: [https://doi.org/10.48550/arXiv.2210.00045](https://doi.org/10.48550/arXiv.2210.00045)\n' +
      '* [741] D. Khashabi, S. Min, T. Khot, A. Sabharwal, O. Tafjord, P. Clark, and H. Hajishirzi, "Unifiedqa: Crossing format boundaries with a single QA system," in _EMNLP (Findings)_, ser. Findings of ACL, vol. EMNLP 2020. Association for Computational Linguistics, 2020, pp. 1896-1907.\n' +
      '\n' +
      '* [742] X. Zhu, J. Wang, L. Zhang, Y. Zhang, R. Gan, J. Zhang, and Y. Yang, "Solving math word problem via cooperative reasoning induced language models," _arXiv preprint arXiv:2210.16257_, 2022.\n' +
      '* [743] A. Nguyen, N. Karampatziakis, and W. Chen, "Meet in the middle: A new pre-training paradigm," _CoRR_, vol. abs/2303.07295, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2303.07295](https://doi.org/10.48550/arXiv.2303.07295)\n' +
      '* [744] H. Li, J. Zhang, C. Li, and H. Chen, "RESDSQL: decoupling schema linking and skeleton parsing for text-to-sql," _CoRR_, vol. abs/2302.05965, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2302.05965](https://doi.org/10.48550/arXiv.2302.05965)\n' +
      '* [745] W. Kang and J. J. McAuley, "Self-attentive sequential recommendation," in _IEEE International Conference on Data Mining, ICDM 2018, Singapore, November 17-20, 2018_. IEEE Computer Society, 2018, pp. 197-206.\n' +
      '* [746] B. Yang, C. Han, Y. Li, L. Zuo, and Z. Yu, "Improving conversational recommendation systems\' quality with context-aware item meta-information," in _Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022_, M. Carpuat, M. de Marneffe, and I. V. M. Ruiz, Eds. Association for Computational Linguistics, 2022, pp. 38-48.\n' +
      '* [747] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, E. Goffinet, D. Heslow, J. Launay, Q. Malartic, B. Noune, B. Pannier, and G. Penedo, "Falcon-40B: an open large language model with state-of-the-art performance," 2023.\n' +
      '* [748] S. Martin, J. Liermann, and H. Ney, "Algorithms for bigram and trigram word clustering," _Speech communication_, vol. 24, no. 1, pp. 19-37, 1998.\n' +
      '* [749] R. Navigli, "Word sense disambiguation: A survey," _ACM computing surveys (CSUR)_, vol. 41, no. 2, pp. 1-69, 2009.\n' +
      '* [750] W. H. Gomaa, A. A. Fahmy _et al._, "A survey of text similarity approaches," _international journal of Computer Applications_, vol. 68, no. 13, pp. 13-18, 2013.\n' +
      '* [751] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu, and J. Gao, "Deep learning-based text classification: a comprehensive review," _ACM computing surveys (CSUR)_, vol. 54, no. 3, pp. 1-40, 2021.\n' +
      '* [752] N. Alex, E. Lifland, L. Tunstall, A. Thakur, P. Maham, C. J. Riedel, E. Hine, C. Ashurst, P. Sedille, A. Carlier, M. Noetel, and A. Stuhlmuller, "RAFT: A real-world few-shot text classification benchmark," in _NeurIPS Datasets and Benchmarks_, 2021.\n' +
      '* [753] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, and D. Yang, "Is chatgpt a general-purpose natural language processing task solver?" _CoRR_, vol. abs/2302.06476, 2023.\n' +
      '* [754] X. Chen, J. Ye, C. Zu, N. Xu, R. Zheng, M. Peng, J. Zhou, T. Gui, Q. Zhang, and X. Huang, "How robust is gpt-3.5 to predecessors? a comprehensive study on language understanding tasks," 2023.\n' +
      '* [755] D. Nadeau and S. Sekine, "A survey of named entity recognition and classification," _Linguistic Investigationes_, vol. 30, no. 1, pp. 3-26, 2007.\n' +
      '* [756] A. Ratnaparkhi, "A maximum entropy model for part-of-speech tagging," in _Conference on empirical methods in natural language processing_, 1996.\n' +
      '* [757] V. Yadav and S. Bethard, "A survey on recent advances in named entity recognition from deep learning models," in _Proceedings of the 27th International Conference on Computational Linguistics_, 2018, pp. 2145-2158.\n' +
      '* [758] F. Souza, R. Nogueira, and R. Lotufo, "Portuguese named entity recognition using bert-crf," _arXiv preprint arXiv:1909.10649_, 2019.\n' +
      '* [759] S. Pawar, G. K. Palshikar, and P. Bhattacharyya, "Relation extraction: A survey," _arXiv preprint arXiv:1712.05191_, 2017.\n' +
      '* [760] C. Walker and et al., "Ace 2005 multilingual training corpus ldc2006t06," Philadelphia, 2006.\n' +
      '* [761] J. Gao, H. Zhao, C. Yu, and R. Xu, "Exploring the feasibility of chatgpt for event extraction," _CoRR_, vol. abs/2303.03836, 2023.\n' +
      '* [762] Y. Ma, Y. Cao, Y. Hong, and A. Sun, "Large language model is not a good few-shot information extractor, but a good reranker for hard samples!" _CoRR_, vol. abs/2303.08559, 2023.\n' +
      '* [763] R. Tang, X. Han, X. Jiang, and X. Hu, "Does synthetic data generation of l1ms help clinical text mining?" _arXiv preprint arXiv:2303.04360_, 2023.\n' +
      '* [764] A. Vaswani, S. Bengio, E. Brevdo, F. Chollet, A. Gomez, S. Gouws, L. Jones, L. Kaiser, N. Kalchbrenner, N. Parmar _et al._, "Tensor2tensor for neural machine translation," in _Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)_, 2018, pp. 193-199.\n' +
      '* [765] B. Zhang, B. Haddow, and A. Birch, "Prompting large language model for machine translation: A case study," _arXiv preprint arXiv:2301.07069_, 2023.\n' +
      '* [766] M. Ghazvininejad, H. Gonen, and L. Zettlemoyer, "Dictionary-based phrase-level prompting of large language models for machine translation," _arXiv preprint arXiv:2302.07856_, 2023.\n' +
      '* [767] L. Wang, C. Lyu, T. Ji, Z. Zhang, D. Yu, S. Shi, and Z. Tu, "Document-level machine translation with large language models," _arXiv preprint arXiv:2304.02210_, 2023.\n' +
      '* [768] W. Jiao, J.-t. Huang, W. Wang, X. Wang, S. Shi, and Z. Tu, "Parrot: Translating during chat using large language models," _arXiv preprint arXiv:2304.02426_, 2023.\n' +
      '* [769] W. Yang, C. Li, J. Zhang, and C. Zong, "Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages," _arXiv preprint arXiv:2305.18098_, 2023.\n' +
      '* [770] J. Kocon, I. Cichecki, O. Kaszyca, M. Kochanek, D. Szydlo, J. Baran, J. Bielaniewicz, M. Gruza, A. Janz, K. Kanclerz, A. Kocon, B. Koptyra, W. Mieleszczenko-Kowszewicz, P. Milkowski, M. Oleksky, M. Piasecki, L. Radlinski, K. Wojtasik, S. Wozniak, and P. Kazienko, "Chatgpt: Jack of all trades, master of none," _CoRR_, vol. abs/2302.10724, 2023.\n' +
      '* [771] Q. Zhong, L. Ding, J. Liu, B. Du, and D. Tao, "Can chatgpt understand too? A comparative study on chatgpt and fine-tuned BERT," _CoRR_, vol. abs/2302.10198, 2023.\n' +
      '* [772] D. Cheng, S. Huang, J. Bi, Y. Zhan, J. Liu, Y. Wang, H. Sun, F. Wei, D. Deng, and Q. Zhang, "Uprise:Universal prompt retrieval for improving zero-shot evaluation," _arXiv preprint arXiv:2303.08518_, 2023.\n' +
      '* [773] R. Ren, Y. Qu, J. Liu, W. X. Zhao, Q. She, H. Wu, H. Wang, and J.-R. Wen, "Rocketqav2: A joint training method for dense passage retrieval and passage re-ranking," in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, 2021, pp. 2825-2835.\n' +
      '* [774] W. Sun, L. Yan, X. Ma, P. Ren, D. Yin, and Z. Ren, "Is chatgpt good at search? investigating large language models as re-ranking agent," _arXiv preprint arXiv:2304.09542_, 2023.\n' +
      '* [775] Z. Qin, R. Jagerman, K. Hui, H. Zhuang, J. Wu, J. Shen, T. Liu, J. Liu, D. Metzler, X. Wang _et al._, "Large language models are effective text rankers with pairwise ranking prompting," _arXiv preprint arXiv:2306.17563_, 2023.\n' +
      '* [776] S. Cho, S. Jeong, J. Seo, and J. C. Park, "Discrete prompt optimization via constrained generation for zero-shot re-ranker," _arXiv preprint arXiv:2305.13729_, 2023.\n' +
      '* [777] R. Tang, X. Zhang, X. Ma, J. Lin, and F. Ture, "Found in the middle: Permutation self-consistency improves listwise ranking in large language models," _arXiv preprint arXiv:2310.07712_, 2023.\n' +
      '* [778] X. Ma, X. Zhang, R. Pradeep, and J. Lin, "Zero-shot listwise document reranking with a large language model," _arXiv preprint arXiv:2305.02156_, 2023.\n' +
      '* [779] S. Zhuang, H. Zhuang, B. Koopman, and G. Zuccon, "A setwise approach for effective and highly efficient zero-shot ranking with large language models," _arXiv preprint arXiv:2310.09497_, 2023.\n' +
      '* [780] H. Zhuang, Z. Qin, K. Hui, J. Wu, L. Yan, X. Wang, and M. Berdersky, "Beyond yes and no: Improving zero-shot llm rankers via scoring fine-grained relevance labels," _arXiv preprint arXiv:2310.14122_, 2023.\n' +
      '* [781] N. Ziems, W. Yu, Z. Zhang, and M. Jiang, "Large language models are built-in autoregressive search engines," _arXiv preprint arXiv:2305.09612_, 2023.\n' +
      '* [782] X. Ma, L. Wang, N. Yang, F. Wei, and J. Lin, "Finetuning llama for multi-stage text retrieval," _arXiv preprint arXiv:2310.08319_, 2023.\n' +
      '* [783] R. Pradeep, S. Sharifymoghaddam, and J. Lin, "Rankvicuna: Zero-shot listwise document reranking with open-source large language models," _arXiv preprint arXiv:2309.15088_, 2023.\n' +
      '* [784] Y. Tay, V. Q. Tran, M. Dehghani, J. Ni, D. Bahri, H. Mehta, Z. Qin, K. Hui, Z. Zhao, J. Gupta _et al._, "Transformer memory as a differentiable search index," in _Advances in Neural Information Processing Systems_, 2022.\n' +
      '* [785] R. Ren, W. X. Zhao, J. Liu, H. Wu, J.-R. Wen, and H. Wang, "TOME: A two-stage approach for model-based retrieval," in _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_. Association for Computational Linguistics, 2023, pp. 6102-6114. [Online]. Available: [https://aclanthology.org/2023.acl-long.336](https://aclanthology.org/2023.acl-long.336)\n' +
      '* [786] Y. Qu, Y. Ding, J. Liu, K. Liu, R. Ren, W. X. Zhao, D. Dong, H. Wu, and H. Wang, "Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering," in _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, 2021, pp. 5835-5847.\n' +
      '* [787] R. Ren, S. Lv, Y. Qu, J. Liu, W. X. Zhao, Q. She, H. Wu, H. Wang, and J.-R. Wen, "Pair: Leveraging passage-centric similarity relation for improving dense passage retrieval," in _Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021_, 2021, pp. 2173-2183.\n' +
      '* [788] Z. Peng, X. Wu, and Y. Fang, "Soft prompt tuning for augmenting dense retrieval with large language models," _arXiv preprint arXiv:2307.08303_, 2023.\n' +
      '* [789] Z. Dai, V. Y. Zhao, J. Ma, Y. Luan, J. Ni, J. Lu, A. Bakalov, K. Guu, K. Hall, and M.-W. Chang, "Promptagator: Few-shot dense retrieval from 8 examples," in _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* [790] A. Askari, M. Aliannejadi, E. Kanoulas, and S. Verberne, "Generating synthetic documents for cross-encoder re-rankers: A comparative study of chatgpt and human experts," _arXiv preprint arXiv:2305.02320_, 2023.\n' +
      '* [791] K. Mao, Z. Dou, H. Chen, F. Mo, and H. Qian, "Large language models know your contextual search intent: A prompting framework for conversational search," _arXiv preprint arXiv:2303.06573_, 2023.\n' +
      '* [792] L. Gao, X. Ma, J. Lin, and J. Callan, "Precise zero-shot dense retrieval without relevance labels," in _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_. Association for Computational Linguistics, 2023, pp. 1762-1777.\n' +
      '* [793] L. Wang, N. Yang, and F. Wei, "Query2doc: Query expansion with large language models," _arXiv preprint arXiv:2303.07678_, 2023.\n' +
      '* [794] G. Ma, X. Wu, P. Wang, Z. Lin, and S. Hu, "Pre-training with large language model-based document expansion for dense passage retrieval," _arXiv preprint arXiv:2308.08285_, 2023.\n' +
      '* [795] W. Sun, Z. Chen, X. Ma, L. Yan, S. Wang, P. Ren, Z. Chen, D. Yin, and Z. Ren, "Instruction distillation makes large language models efficient zero-shot rankers," _arXiv preprint arXiv:2311.01555_, 2023.\n' +
      '* [796] X. Wang, W. Zhu, and W. Y. Wang, "Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning," _CoRR_, vol. abs/2301.11916, 2023.\n' +
      '* [797] C. Li, Z. Gan, Z. Yang, J. Yang, L. Li, L. Wang, and J. Gao, "Multimodal foundation models: From specialists to general-purpose assistants," _CoRR_, vol. abs/2309.10020, 2023.\n' +
      '* [798] W. X. Zhao, S. Mu, Y. Hou, Z. Lin, Y. Chen, X. Pan, K. Li, Y. Lu, H. Wang, C. Tian, Y. Min, Z. Feng, X. Fan, X. Chen, P. Wang, W. Ji, Y. Li, X. Wang, and J. Wen, "Recbole: Towards a unified, comprehensive and efficient framework for recommendation algorithms," in _CIKM_, G. Demartini, G. Zuccon, J. S. Culpepper, Z. Huang, and H. Tong, Eds. ACM, 2021, pp. 4653-4664.\n' +
      '\n' +
      '* [799] K. Zhou, H. Wang, W. X. Zhao, Y. Zhu, S. Wang, F. Zhang, Z. Wang, and J. Wen, "S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization," in _CIKM_, M. d\'Aquin, S. Dietze, C. Hauff, E. Curry, and P. Cudre-Mauroux, Eds. ACM, 2020, pp. 1893-1902.\n' +
      '* [800] W. X. Zhao, Y. Hou, X. Pan, C. Yang, Z. Zhang, Z. Lin, J. Zhang, S. Bian, J. Tang, W. Sun, Y. Chen, L. Xu, G. Zhang, Z. Tian, C. Tian, S. Mu, X. Fan, X. Chen, and J. Wen, "Recbole 2.0: Towards a more up-to-date recommendation library," in _CIKM_, M. A. Hasan and L. Xiong, Eds. ACM, 2022, pp. 4722-4726.\n' +
      '* [801] L. Xu, Z. Tian, G. Zhang, J. Zhang, L. Wang, B. Zheng, Y. Li, J. Tang, Z. Zhang, Y. Hou, X. Pan, W. X. Zhao, X. Chen, and J. Wen, "Towards a more user-friendly and easy-to-use benchmark library for recommender systems," in _SIGIR_, H. Chen, W. E. Duh, H. Huang, M. P. Kato, J. Mothe, and B. Poblete, Eds. ACM, 2023, pp. 2837-2847.\n' +
      '* [802] S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme, "BPR: bayesian personalized ranking from implicit feedback," _CoRR_, vol. abs/1205.2618, 2012.\n' +
      '* [803] W. Fan, Z. Zhao, J. Li, Y. Liu, X. Mei, Y. Wang, J. Tang, and Q. Li, "Recommender systems in the era of large language models (IIms)," _CoRR_, 2023.\n' +
      '* [804] L. Wu, Z. Zheng, Z. Qiu, H. Wang, H. Gu, T. Shen, C. Qin, C. Zhu, H. Zhu, Q. Liu, H. Xiong, and E. Chen, "A survey on large language models for recommendation," _CoRR_, 2023.\n' +
      '* [805] Y. Gao, T. Sheng, Y. Xiang, Y. Xiong, H. Wang, and J. Zhang, "Chat-rec: Towards interactive and explainable llms-augmented recommender system," _CoRR_, vol. abs/2303.14524, 2023.\n' +
      '* [806] S. Dai, N. Shao, H. Zhao, W. Yu, Z. Si, C. Xu, Z. Sun, X. Zhang, and J. Xu, "Uncovering chatgpt\'s capabilities in recommender systems," in _RecSys_, J. Zhang, L. Chen, S. Berkovsky, M. Zhang, T. D. Noia, J. Basilico, L. Pizzato, and Y. Song, Eds. ACM, 2023, pp. 1126-1132.\n' +
      '* [807] Y. Hou, J. Zhang, Z. Lin, H. Lu, R. Xie, J. J. McAuley, and W. X. Zhao, "Large language models are zero-shot rankers for recommender systems," _CoRR_, 2023.\n' +
      '* [808] J. Liu, C. Liu, R. Lv, K. Zhou, and Y. Zhang, "Is chatgpt a good recommender? A preliminary study," _CoRR_, vol. abs/2304.10149, 2023.\n' +
      '* [809] K. Bao, J. Zhang, Y. Zhang, W. Wang, F. Feng, and X. He, "Tallrec: An effective and efficient tuning framework to align large language model with recommendation," in _RecSys_, J. Zhang, L. Chen, S. Berkovsky, M. Zhang, T. D. Noia, J. Basilico, L. Pizzato, and Y. Song, Eds. ACM, 2023, pp. 1007-1014.\n' +
      '* [810] Y. Zhu, L. Wu, Q. Guo, L. Hong, and J. Li, "Collaborative large language model for recommender systems," _arXiv preprint arXiv:2311.01343_, 2023.\n' +
      '* [811] B. Zheng, Y. Hou, H. Lu, Y. Chen, W. X. Zhao, and J.-R. Wen, "Adapting large language models by integrating collaborative semantics for recommendation," 2023. [Online]. Available: [https://api.semanticscholar.org/CorpusID:265213194](https://api.semanticscholar.org/CorpusID:265213194)\n' +
      '* [812] Y. Xi, W. Liu, J. Lin, J. Zhu, B. Chen, R. Tang, W. Zhang, R. Zhang, and Y. Yu, "Towards open-world recommendation with knowledge augmentation from large language models," _CoRR_, vol. abs/2306.10933, 2023.\n' +
      '* [813] Q. Liu, N. Chen, T. Sakai, and X. Wu, "A first look at llm-powered generative news recommendation," _CoRR_, vol. abs/2305.06566, 2023.\n' +
      '* [814] R. Li, W. Deng, Y. Cheng, Z. Yuan, J. Zhang, and F. Yuan, "Exploring the upper limits of text-based collaborative filtering using large language models: Discoveries and insights," _CoRR_, vol. abs/2305.11700, 2023.\n' +
      '* [815] W. Wei, X. Ren, J. Tang, Q. Wang, L. Su, S. Cheng, J. Wang, D. Yin, and C. Huang, "Llmrec: Large language models with graph augmentation for recommendation," _CoRR_, vol. abs/2311.00423, 2023.\n' +
      '* [816] X. Li, B. Chen, L. Hou, and R. Tang, "Ctrl: Connect tabular and language model for ctr prediction," _arXiv preprint arXiv:2306.02841_, 2023.\n' +
      '* [817] A. Muhamed, I. Keivanloo, S. Perera, J. Mracek, Y. Xu, Q. Cui, S. Rajagopalan, B. Zeng, and T. Chilimbi, "Ctr-bert: Cost-effective knowledge distillation for billion-parameter teacher models," in _NeurIPS Efficient Natural Language and Speech Processing Workshop_, 2021.\n' +
      '* [818] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin, W. X. Zhao, Z. Wei, and J. Wen, "A survey on large language model based autonomous agents," _CoRR_, vol. abs/2308.11432, 2023.\n' +
      '* [819] L. Wang, J. Zhang, X. Chen, Y. Lin, R. Song, W. X. Zhao, and J. Wen, "Recagent: A novel simulation paradigm for recommender systems," _CoRR_, vol. abs/2306.02552, 2023.\n' +
      '* [820] E. Ie, C. Hsu, M. Mladenov, V. Jain, S. Narvekar, J. Wang, R. Wu, and C. Boutilier, "Recsim: A configurable simulation platform for recommender systems," _CoRR_, vol. abs/1909.04847, 2019.\n' +
      '* [821] J. Zhang, Y. Hou, R. Xie, W. Sun, J. J. McAuley, W. X. Zhao, L. Lin, and J. Wen, "Agentet: Collaborative learning with autonomous language agents for recommender systems," _CoRR_, vol. abs/2310.09233, 2023.\n' +
      '* [822] A. Zhang, L. Sheng, Y. Chen, H. Li, Y. Deng, X. Wang, and T. Chua, "On generative agents in recommendation," _CoRR_, vol. abs/2310.10108, 2023.\n' +
      '* [823] Y. Du, Z. Liu, J. Li, and W. X. Zhao, "A survey of vision-language pre-trained models," in _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022_, L. D. Raedt, d. ijcai.org, 2022, pp. 5436-5443.\n' +
      '* [824] Z. Gan, L. Li, C. Li, L. Wang, Z. Liu, and J. Gao, "Vision-language pre-training: Basics, recent advances, and future trends," _Found. Trends Comput. Graph. Vis._, vol. 14, no. 3-4, pp. 163-352, 2022.\n' +
      '* [825] P. K. Rubenstein, C. Asawaroengchai, D. D. Nguyen, A. Bapna, Z. Borsos, F. de Chaumont Quitry, P. Chen, D. E. Badawy, W. Han, E. Kharitonov _et al._, "Autoploam: A large language model that can speak and listen," _CoRR_, 2023.\n' +
      '* [826] J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. L. Menick,S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. Binkowski, R. Barreira, O. Vinyals, A. Zisserman, and K. Simonyan, "Flamingo: a visual language model for few-shot learning," in _NeurIPS_, 2022.\n' +
      '* [827] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Chertl, T. Coombes, A. Katta, C. Mullis, M. Wortsman, P. Schramowski, S. Kundurthy, K. Crowson, L. Schmidt, R. Kaczmareczyk, and J. Jitsev, "LAION-5B: an open large-scale dataset for training next generation image-text models," in _NeurIPS_, 2022.\n' +
      '* [828] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut, "Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts," in _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021_. Computer Vision Foundation _ _IEEE_, 2021, pp. 3558-3568.\n' +
      '* [829] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, C. Li, Y. Xu, H. Chen, J. Tian, Q. Qi, J. Zhang, and F. Huang, "mplug-owl: Modularization empowers large language models with multimodality," _CoRR_, vol. abs/2304.14178, 2023.\n' +
      '* [830] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou, "Owen-vl: A frontier large vision-language model with versatile abilities," _CoRR_, vol. abs/2308.12966, 2023.\n' +
      '* [831] H. Liu, C. Li, Y. Li, and Y. J. Lee, "Improved baselines with visual instruction tuning," _CoRR_, vol. abs/2310.03744, 2023.\n' +
      '* [832] P. Zhang, X. Dong, B. Wang, Y. Cao, C. Xu, L. Ouyang, Z. Zhao, S. Ding, S. Zhang, H. Duan, W. Zhang, H. Yan, X. Zhang, W. Li, J. Li, K. Chen, C. He, X. Zhang, Y. Qiao, D. Lin, and J. Wang, "Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition," _CoRR_, vol. abs/2309.15112, 2023.\n' +
      '* [833] K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, and R. Zhao, "Shixra: Unleashing multimodal llm\'s referential dialogue magic," _CoRR_, vol. abs/2306.15195, 2023.\n' +
      '* [834] F. Liu, K. Lin, L. Li, J. Wang, Y. Yacoob, and L. Wang, "Aligning large multi-modal model with robust instruction tuning," _CoRR_, vol. abs/2306.14565, 2023.\n' +
      '* [835] Y. Du, H. Guo, K. Zhou, W. X. Zhao, J. Wang, C. Wang, M. Cai, R. Song, and J.-R. Wen, "What makes for good visual instructions? synthesizing complex visual reasoning instructions for visual instruction tuning," 2023.\n' +
      '* [836] D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham, "Vizwiz grand challenge: Answering visual questions from blind people," in _CVPR_. Computer Vision Foundation / IEEE Computer Society, 2018, pp. 3608-3617.\n' +
      '* [837] A. Mishra, K. Alahari, and C. V. Jawahar, "Top-down and bottom-up cues for scene text recognition," in _CVPR_. IEEE Computer Society, 2012, pp. 2687-2694.\n' +
      '* [838] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, K. Chen, and D. Lin, "Mmbench: Is your multi-modal model an all-around player?" _CoRR_, vol. abs/2307.06281, 2023.\n' +
      '* [839] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, Z. Qiu, W. Lin, J. Yang, X. Zheng, K. Li, X. Sun, and R. Ji, "MME: A comprehensive evaluation benchmark for multimodal large language models," _CoRR_, vol. abs/2306.13394, 2023.\n' +
      '* [840] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y. Zhang, Y. Chen, L. Wang, A. T. Luu, W. Bi, F. Shi, and S. Shi, "Sien\'s song in the AI ocean: A survey on hallucination in large language models," _CoRR_, vol. abs/2309.01219, 2023.\n' +
      '* [841] A. Gunjal, J. Yin, and E. Bas, "Detecting and preventing hallucinations in large vision language models," _CoRR_, vol. abs/2308.06394, 2023.\n' +
      '* [842] J. Lu, J. Rao, K. Chen, X. Guo, Y. Zhang, B. Sun, C. Yang, and J. Yang, "Evaluation and mitigation of agnosis in multimodal large language models," _CoRR_, vol. abs/2309.04041, 2023.\n' +
      '* [843] A. Rohrbach, L. A. Hendricks, K. Burns, T. Darrell, and K. Saenko, "Object hallucination in image captioning," in _EMNLP_. Association for Computational Linguistics, 2018, pp. 4035-4045.\n' +
      '* [844] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R. Wen, "Evaluating object hallucination in large vision-language models," in _The 2023 Conference on Empirical Methods in Natural Language Processing_, 2023. [Online]. Available: [https://openreview.net/forum?id=xozJw0kZXF](https://openreview.net/forum?id=xozJw0kZXF)\n' +
      '* [845] D. A. Hudson and C. D. Manning, "GQA: A new dataset for real-world visual reasoning and compositional question answering," in _CVPR_. Computer Vision Foundation / IEEE, 2019, pp. 6700-6709.\n' +
      '* [846] P. Lu, S. Mishra, T. Xia, L. Qiu, K. Chang, S. Zhu, O. Tafjord, P. Clark, and A. Kalyan, "Learn to explain: Multimodal reasoning via thought chains for science question answering," in _NeurIPS_, 2022.\n' +
      '* [847] A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Parikh, and M. Rohrbach, "Towards vqa models that can read," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, 2019, pp. 8317-8326.\n' +
      '* [848] F. Liu, T. Guan, Z. Li, L. Chen, Y. Yacoob, D. Manocha, and T. Zhou, "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v(ision), llava-1.5, and other multi-modality models," _CoRR_, vol. abs/2310.14566, 2023.\n' +
      '* [849] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh, "VQA: visual question answering," in _ICCV_. IEEE Computer Society, 2015, pp. 2425-2433.\n' +
      '* [850] R. Vedantam, C. L. Zitnick, and D. Parikh, "Cider: Consensus-based image description evaluation," in _CVPR_. IEEE Computer Society, 2015, pp. 4566-4575.\n' +
      '* [851] H. Liu, C. Li, Q. Wu, and Y. J. Lee, "Visual instruction tuning," _CoRR_, vol. abs/2304.08485, 2023.\n' +
      '* [852] P. Xu, W. Shao, K. Zhang, P. Gao, S. Liu, M. Lei, F. Meng, S. Huang, Y. Qiao, and P. Luo, "Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models," _CoRR_, vol. abs/2306.09265, 2023.\n' +
      '* [853] Z. Li, Y. Wang, M. Du, Q. Liu, B. Wu, J. Zhang, C. Zhou, Z. Fan, J. Fu, J. Chen, X. Huang, and Z. Wei,"Reform-eval: Evaluating large vision language models via unified re-formulation of task-oriented benchmarks," _CoRR_, vol. abs/2310.02569, 2023.\n' +
      '* [854] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan, "Seed-bench: Benchmarking multimodal llms with generative comprehension," _CoRR_, vol. abs/2307.16125, 2023.\n' +
      '* [855] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang, "Mm-vet: Evaluating large multimodal models for integrated capabilities," _CoRR_, vol. abs/2308.02490, 2023.\n' +
      '* [856] J. Wang, L. Meng, Z. Weng, B. He, Z. Wu, and Y. Jiang, "To see is to believe: Prompting GPT-4V for better visual instruction tuning," _CoRR_, vol. abs/2311.07574, 2023.\n' +
      '* [857] Y. Zhang, R. Zhang, J. Gu, Y. Zhou, N. Lipka, D. Yang, and T. Sun, "Llavar: Enhanced visual instruction tuning for text-rich image understanding," _arXiv preprint arXiv:2306.17107_, 2023.\n' +
      '* [858] X. Qi, K. Huang, A. Panda, M. Wang, and P. Mittal, "Visual adversarial examples jailbreak aligned large language models," in _The Second Workshop on New Frontiers in Adversarial Machine Learning_, 2023.\n' +
      '* [859] Y. Zhou, C. Cui, J. Yoon, L. Zhang, Z. Deng, C. Finn, M. Bansal, and H. Yao, "Analyzing and mitigating object hallucination in large vision-language models," _arXiv preprint arXiv:2310.00754_, 2023.\n' +
      '* [860] Z. Sun, S. Shen, S. Cao, H. Liu, C. Li, Y. Shen, C. Gan, L.-Y. Gui, Y.-X. Wang, Y. Yang _et al._, "Aligning large multimodal models with factually augmented rlhf," _arXiv preprint arXiv:2309.14525_, 2023.\n' +
      '* [861] S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu, "Unifying large language models and knowledge graphs: A roadmap," _CoRR_, vol. abs/2306.08302, 2023.\n' +
      '* 17th International Conference, ESWC 2020, Heraklion, Crete, Greece, May 31-6월 4일, Proceedings_, ser. Lecture Notes in Computer Science, vol. 12123. Springer, 2020, pp. 514-530.\n' +
      '* [863] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen, Y. Zhao, Y. Lu, W. Liu, Z. Wu, W. Gong, J. Liang, Z. Shang, P. Sun, W. Liu, X. Ouyang, D. Yu, H. Tian, H. Wu, and H. Wang, "ERNIE 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation," _CoRR_, vol. abs/2107.02137, 2021. [Online]. Available: [https://arxiv.org/abs/2107.02137](https://arxiv.org/abs/2107.02137)\n' +
      '* [864] Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu, "ERNIE: enhanced language representation with informative entities," in _Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers_. Association for Computational Linguistics, 2019, pp. 1441-1451.\n' +
      '* [865] X. Wang, T. Gao, Z. Zhu, Z. Zhang, Z. Liu, J. Li, and J. Tang, "KEPLER: A unified model for knowledge embedding and pre-trained language representation," _Trans. Assoc. Comput. Linguistics_, vol. 9, pp. 176-194, 2021.\n' +
      '* [866] J. Zhang, X. Zhang, J. Yu, J. Tang, J. Tang, C. Li, and H. Chen, "Subgraph retrieval enhanced model for multi-hop knowledge base question answering," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_. Association for Computational Linguistics, 2022, pp. 5773-5784.\n' +
      '* [867] P. Ke, H. Ji, Y. Ran, X. Cui, L. Wang, L. Song, X. Zhu, and M. Huang, "Jointgt: Graph-text joint representation learning for text generation from knowledge graphs," in _Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021_, ser. Findings of ACL, vol. ACL/IJCNLP 2021. Association for Computational Linguistics, 2021, pp. 2526-2538.\n' +
      '* [868] O. Agarwal, H. Ge, S. Shakeri, and R. Al-Rfou, "Large scale knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training," _CoRR_, vol. abs/2010.12688, 2020.\n' +
      '* [869] W. Chen, Y. Su, X. Yan, and W. Y. Wang, "KGPT: knowledge-grounded pre-training for data-to-text generation," in _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020_. Association for Computational Linguistics, 2020, pp. 8635-8648.\n' +
      '* [870] Y. Gu, X. Deng, and Y. Su, "Don\'t generate, discriminate: A proposal for grounding language models to real-world environments," in _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023_. Association for Computational Linguistics, 2023, pp. 4928-4949.\n' +
      '* [871] L. Luo, Y. Li, G. Haffari, and S. Pan, "Reasoning on graphs: Faithful and interpretable large language model reasoning," _CoRR_, vol. abs/2310.01061, 2023.\n' +
      '* [872] Y. Lan and J. Jiang, "Query graph generation for answering multi-hop complex questions from knowledge bases," in _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_, D. J. and, Ed. Association for Computational Linguistics, 2020, pp. 969-974.\n' +
      '* [873] P. Wang, N. Zhang, X. Xie, Y. Yao, B. Tian, M. Wang, Z. Xi, S. Cheng, K. Liu, G. Zheng, and H. Chen, "Easyedit: An easy-to-use knowledge editing framework for large language models," _CoRR_, vol. abs/2308.07269, 2023.\n' +
      '* [874] Y. Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and N. Zhang, "Editing large language models: Problems, methods, and opportunities," _CoRR_, vol. abs/2305.13172, 2023.\n' +
      '* [875] S. Choi, T. Fang, Z. Wang, and Y. Song, "KCTS: knowledge-constrained tree search decoding with token-level hallucination detection," _CoRR_, vol. abs/2310.09044, 2023.\n' +
      '* [876] S. Zhang, L. Pan, J. Zhao, and W. Y. Wang, "Mitigating language model hallucination with interactive question-knowledge alignment," _CoRR_, vol. abs/2305.13669, 2023.\n' +
      '\n' +
      '* [877] Y. Zhu, X. Wang, J. Chen, S. Qiao, Y. Ou, Y. Yao, S. Deng, H. Chen, and N. Zhang, "Llms for knowledge graph construction and reasoning: Recent capabilities and future opportunities," _CoRR_, vol. abs/2305.13168, 2023. [Online]. Available: [https://doi.org/10.48550/arXiv.2305.13168](https://doi.org/10.48550/arXiv.2305.13168)\n' +
      '* [878] S. Russell and P. Norvig, _Artificial Intelligence: A Modern Approach (4th Edition)_. Pearson, 2020. [Online]. Available: [http://aima.cs.berkeley.edu/](http://aima.cs.berkeley.edu/)\n' +
      '* [879] B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman, "Building machines that learn and think like people," _CoRR_, vol. abs/1604.00289, 2016.\n' +
      '* [880] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, "React: Synergizing reasoning and acting in language models," _CoRR_, vol. abs/2210.03629, 2022.\n' +
      '* [881] 2023. [Online]. Available: [https://github.com/AntonCiska/gpt-engineer](https://github.com/AntonCiska/gpt-engineer)\n' +
      '* [882] X. Team, "Xagent: An autonomous agent for complex task solving," 2023.\n' +
      '* [883] G. Li, H. A. A. K. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, "CAMEL: communicative agents for "mind" exploration of large scale language model society," _CoRR_, vol. abs/2303.17760, 2023.\n' +
      '* [884] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, C. Ran, L. Xiao, and C. Wu, "Metagpt: Meta programming for multi-agent collaborative framework," _CoRR_, vol. abs/2308.00352, 2023.\n' +
      '* [885] C. Pham, B. Liu, Y. Yang, Z. Chen, T. Liu, J. Yuan, B. A. Plummer, Z. Wang, and H. Yang, "Let models speak ciphers: Multiagent debate through embeddings," _CoRR_, vol. abs/2310.06272, 2023.\n' +
      '* [886] Y. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch, "Improving factuality and reasoning in language models through multiagent debate," _CoRR_, vol. abs/2305.14325, 2023.\n' +
      '* [887] M. Karpinska, N. Akoury, and M. Iyyer, "The perils of using mechanical turk to evaluate open-ended text generation," in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021_, M. Moens, X. Huang, L. Specia, and S. W. Yih, Eds. Association for Computational Linguistics, 2021, pp. 1265-1285.\n' +
      '* [888] H. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard, C. Bishop, V. Carbune, and A. Rastogi, "RLAF: scaling reinforcement learning from human feedback with AI feedback," _CoRR_, vol. abs/2309.00267, 2023.\n' +
      '* [889] T. Wang, P. Yu, X. E. Tan, S. O\'Brien, R. Pasunuru, J. Dwivedi-Yu, O. Golovneva, L. Zettlemoyer, M. Fazel-Zarandi, and A. Celikyilmaz, "Shepherd: A critic for language model generation," _CoRR_, vol. abs/2308.04592, 2023.\n' +
      '* [890] G. Cui, L. Yuan, N. Ding, G. Yao, W. Zhu, Y. Ni, G. Xie, Z. Liu, and M. Sun, "Ultrafeedback: Boosting language models with high-quality feedback," _CoRR_, vol. abs/2310.01377, 2023.\n' +
      '* [891] X. Wang, Z. Wang, J. Liu, Y. Chen, L. Yuan, H. Peng, and H. Ji, "MINT: evaluating llms in multi-turn interaction with tools and language feedback," _CoRR_, vol. abs/2309.10691, 2023.\n' +
      '* [892] S. Saha, O. Levy, A. Celikyilmaz, M. Bansal, J. Weston, and X. Li, "Branch-solve-merge improves large language model evaluation and generation," _CoRR_, vol. abs/2310.15123, 2023.\n' +
      '* [893] X. Zhang, B. Yu, H. Yu, Y. Lv, T. Liu, F. Huang, H. Xu, and Y. Li, "Wider and deeper LLM networks are fairer LLM evaluators," _CoRR_, vol. abs/2308.01862, 2023.\n' +
      '* [894] C. Chan, W. Chen, Y. Su, J. Yu, W. Xue, S. Zhang, J. Fu, and Z. Liu, "Chateval: Towards better llm-based evaluators through multi-agent debate," _CoRR_, vol. abs/2308.07201, 2023.\n' +
      '* [895] R. Li, T. Patel, and X. Du, "PRD: peer rank and discussion improve large language model based evaluations," _CoRR_, vol. abs/2307.02762, 2023.\n' +
      '* [896] L. Zhu, X. Wang, and X. Wang, "Judgelm: Fine-tuned large language models are scalable judges," _CoRR_, vol. abs/2310.17631, 2023.\n' +
      '* [897] Z. Zeng, J. Yu, T. Gao, Y. Meng, T. Goyal, and D. Chen, "Evaluating large language models at evaluating instruction following," _CoRR_, vol. abs/2310.07641, 2023.\n' +
      '* [898] R. Koo, M. Lee, V. Raheja, J. I. Park, Z. M. Kim, and D. Kang, "Benchmarking cognitive biases in large language models as evaluators," _CoRR_, vol. abs/2309.17012, 2023.\n' +
      '* [899] P. West, X. Lu, N. Dziri, F. Brahman, L. Li, J. D. Hwang, L. Jiang, J. Fisher, A. Ravichander, K. Chandu, B. Newman, P. W. Koh, A. Ettinger, and Y. Choi, "The generative AI paradox: "what it can create, it may not understand"," _CoRR_, vol. abs/2311.00059, 2023.\n' +
      '* [900] J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou, "Large language models cannot self-correct reasoning yet," _CoRR_, vol. abs/2310.01798, 2023.\n' +
      '* [901] K. Stechly, M. Marquez, and S. Kambhampati, "GPT-4 doesn\'t know it\'s wrong: An analysis of iterative prompting for reasoning problems," _CoRR_, vol. abs/2310.12397, 2023.\n' +
      '* [902] O. Nov, N. Singh, and D. M. Mann, "Putting chatgpt\'s medical advice to the (turing) test," _CoRR_, vol. abs/2301.10035, 2023.\n' +
      '* [903] K. Yang, S. Ji, T. Zhang, Q. Xie, and S. Ananiadou, "On the evaluations of chatgpt and emotion-enhanced prompting for mental health analysis," _CoRR_, vol. abs/2304.03347, 2023.\n' +
      '* [904] K. Jeblick, B. Schachtner, J. Deal, A. Mittermeier, A. T. Stiuber, J. Topalis, T. Weber, P. Wesp, B. O. Sabel, J. Ricke, and M. Ingrisch, "Chatgpt makes medicine easy to swallow: An exploratory case study on simplified radiology reports," _CoRR_, vol. abs/2212.14882, 2022.\n' +
      '* [905] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark, S. Pfohl, H. Cole-Lewis, D. Neal, M. Schaekermann, A. Wang, M. Amin, S. Lachgar, P. A. Mansfield, S. Prakash, B. Green, E. Dominowska, B. A. y Arcas, N. Tomasev, Y. Liu, R. Wong, C. Sembturs, S. S. Mahdavi, J. K. Barral, D. R. Webster, G. S. Corrado, Y. Matias, S. Azizi, A. Karthikesalingam, and V. Natarajan, "Towards expert-level medical question answering with large language models," _CoRR_, vol. abs/2305.09617, 2023.\n' +
      '\n' +
      '* [906] S. Yang, H. Zhao, S. Zhu, G. Zhou, H. Xu, Y. Jia, and H. Zan, "Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and real-world multi-turn dialogue," _CoRR_, vol. abs/2308.03549, 2023.\n' +
      '* [907] S. Chen, B. H. Kann, M. B. Foote, H. J. Aerts, G. K. Savvao, R. H. Mak, and D. S. Bitterman, "The utility of chatgpt for cancer treatment information," _medRxiv_, 2023.\n' +
      '* [908] K. Malinka, M. Peresini, A. Firc, O. Hujnak, and F. Janus, "On the educational impact of chatgpt: Is artificial intelligence ready to obtain a university degree?" _CoRR_, vol. abs/2303.11146, 2023.\n' +
      '* [909] T. Susnjak, "Chatgpt: The end of online exam integrity?" _CoRR_, vol. abs/2212.09292, 2022.\n' +
      '* [910] K. Tan, T. Pang, and C. Fan, "Towards applying powerful large ai models in classroom teaching: Opportunities, challenges and prospects," 2023.\n' +
      '* [911] F. Kamalov and I. Gurrib, "A new era of artificial intelligence in education: A multifaceted revolution," _CoRR_, vol. abs/2305.18303, 2023.\n' +
      '* [912] E. Kasneci, K. Sessler, S. Kuchemann, M. Bannert, D. Dementieva, F. Fischer, U. Gasser, G. Groh, S. Gunnemann, E. Hullermeier _et al._, "Chatgpt for good? on opportunities and challenges of large language models for education," _Learning and Individual Differences_, vol. 103, p. 102274, 2023.\n' +
      '* [913] A. Blair-Stanek, N. Holzenberger, and B. V. Durme, "Can GPT-3 perform statutory reasoning?" _CoRR_, vol. abs/2302.06100, 2023.\n' +
      '* [914] D. Trautmann, A. Petrova, and F. Schilder, "Legal prompt engineering for multilingual legal judgement prediction," _CoRR_, vol. abs/2212.02199, 2022.\n' +
      '* [915] J. H. Choi, K. E. Hickman, A. Monahan, and D. Schwarcz, "Chatgpt goes to law school," _Available at SSRN_, 2023.\n' +
      '* [916] J. J. Nay, "Law informs code: A legal informatics approach to aligning artificial intelligence with humans," _CoRR_, vol. abs/2209.13020, 2022.\n' +
      '* [917] F. Yu, L. Quartey, and F. Schilder, "Legal prompting: Teaching a language model to think like a lawyer," _CoRR_, vol. abs/2212.01326, 2022.\n' +
      '* [918] D. Trautmann, A. Petrova, and F. Schilder, "Legal prompt engineering for multilingual legal judgement prediction," _CoRR_, vol. abs/2212.02199, 2022.\n' +
      '* [919] A. Tamkin, M. Brundage, J. Clark, and D. Ganguli, "Understanding the capabilities, limitations, and societal impact of large language models," _CoRR_, vol. abs/2102.02503, 2021.\n' +
      '* [920] Z. Sun, "A short survey of viewing large language models in legal aspect," _CoRR_, vol. abs/2303.09136, 2023.\n' +
      '* [921] A. Abid, M. Farooqi, and J. Zou, "Persistent anti-muslim bias in large language models," in _AIES \'21: AAAI/ACM Conference on AI, Ethics, and Society, Virtual Event, USA, May 19-21, 2021_, M. Fourcade, B. Kuipers, S. Lazar, and D. K. Mulligan, Eds. ACM, 2021, pp. 298-306.\n' +
      '* [922] A. Shah and S. Chava, "Zero is not hero yet: Benchmarking zero-shot performance of llms for financial tasks," _CoRR_, vol. abs/2305.16633, 2023.\n' +
      '* [923] D. Araci, "Finbert: Financial sentiment analysis with pre-trained language models," _CoRR_, vol. abs/1908.10063, 2019.\n' +
      '*9, 2015_, B. Hachey and K. Webster, Eds. ACL, 2015, pp. 84-90.\n' +
      '* [925] G. Son, H. Jung, M. Hahm, K. Na, and S. Jin, "Beyond classification: Financial reasoning in state-of-the-art language models," _CoRR_, vol. abs/2305.01505, 2023.\n' +
      '* [926] X. Zhang, Q. Yang, and D. Xu, "Xuanyuan 2.0: A large chinese financial chat model with hundreds of billions parameters," _arXiv preprint arXiv:2305.12002_, 2023.\n' +
      '* [927] H. Yang, X.-Y. Liu, and C. D. Wang, "Fingpt: Open-source financial large language models," _CoRR_, vol. abs/2306.06031, 2023.\n' +
      '* [928] Q. Jin, B. Dhingra, Z. Liu, W. W. Cohen, and X. Lu, "Pubmedqa: A dataset for biomedical research question answering," in _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019_, 2019, pp. 2567-2577.\n' +
      '* [929] A. Krikhara, A. Nentidis, K. Bougiatiotis, and G. Paliouras, "Bioasq-qa: A manually curated corpus for biomedical question answering," 2022.\n' +
      '* [930] Z. Bi, N. Zhang, Y. Xue, Y. Ou, D. Ji, G. Zheng, and H. Chen, "Oceangpt: A large language model for ocean science tasks," _CoRR_, vol. abs/2310.02031, 2023.\n' +
      '* [931] C. Zhang, C. Zhang, C. Li, Y. Qiao, S. Zheng, S. K. Dam, M. Zhang, J. U. Kim, S. T. Kim, J. Choi, G. Park, S. Bae, L. Lee, P. Hui, I. S. Kweon, and C. S. Hong, "One small step for generative ai, one giant leap for AGI: A complete survey on chatgpt in AIGC era," _CoRR_, vol. abs/2304.06488, 2023.\n' +
      '* [932] M. Haman and M. Skolnik, "Using chatgpt to conduct a literature review." _Accontability in research_, 2023.\n' +
      '* [933] O. Aydin and E. Karaarslan, "Openai chatgpt generated literature review: Digital twin in healthcare," _SSRN Electronic Journal_, 2022.\n' +
      '* [934] Y. J. Park, D. Kaplan, Z. Ren, C. Hsu, C. Li, H. Xu, S. Li, and J. Li, "Can chatgpt be used to generate scientific hypotheses?" _CoRR_, vol. abs/2304.12208, 2023.\n' +
      '* [935] M. M. Hassan, R. A. Knipper, and S. K. K. Santu, "Chatgpt as your personal data scientist," _CoRR_, vol. abs/2305.13657, 2023.\n' +
      '* [936] L. Cheng, X. Li, and L. Bing, "Is GPT-4 a good data analyst?" _CoRR_, vol. abs/2305.15038, 2023.\n' +
      '* [937] S. I. M. Hussam Alkaissi, "Artificial hallucinations in chatgpt: Implications in scientific writing," _PubMed_, 2023.\n' +
      '* for experts," _CoRR_, vol. abs/2306.03102, 2023.\n' +
      '* [939] O. O. Buruk, "Academic writing with GPT-3.5: reflections on practices, efficacy and transparency," _CoRR_, vol. abs/2304.11079, 2023.\n' +
      '* [940] R. Liu and N. B. Shah, "Reviewerpt? an exploratory study on using large language models for paper re viewing," _CoRR_, vol. abs/2306.00622, 2023.\n' +
      '* [941] M. Kosinski, "Theory of mind may have spontaneously emerged in large language models," _CoRR_, vol. abs/2302.02083, 2023.\n' +
      '* [942] M. M. Amin, E. Cambria, and B. W. Schuller, "Will affective computing emerge from foundation models and general ai? A first evaluation on chatgpt," _CoRR_, vol. abs/2303.03186, 2023.\n' +
      '* [943] G. Sridhara, R. H. G., and S. Mazumdar, "Chatgpt: A study on its utility for ubiquitous software engineering tasks," _CoRR_, vol. abs/2305.16837, 2023.\n' +
      '* [944] W. Sun, C. Fang, Y. You, Y. Miao, Y. Liu, Y. Li, G. Deng, S. Huang, Y. Chen, Q. Zhang, H. Qian, Y. Liu, and Z. Chen, "Automatic code summarization via chatgpt: How far are we?" _CoRR_, vol. abs/2305.12865, 2023.\n' +
      '* [945] C. S. Xia and L. Zhang, "Conversational automated program repair," _CoRR_, vol. abs/2301.13246, 2023.\n' +
      '* [946] W. Kuang, B. Qian, Z. Li, D. Chen, D. Gao, X. Pan, Y. Xie, Y. Li, B. Ding, and J. Zhou, "Federatedscope-Ilm: A comprehensive package for fine-tuning large language models in federated learning," 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>