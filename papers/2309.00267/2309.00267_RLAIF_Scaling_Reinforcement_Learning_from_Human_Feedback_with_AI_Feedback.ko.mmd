# RLAIF: Scaling Reinforcement Learning from Human Feedback

인공지능 피드백

 해리슨 리, 샘랫 파탈, 하산 만수르, 토마스 메스나드, 요한 페렛, 켈리 루

콜튼 비숍, 이든 홀, 빅터 카르부네, 아비나브 라스토기, 수샨트 프라카시

Google Research

{harrisonlee,samratph,hassan}@google.com

###### Abstract

인간 피드백(RLHF)으로부터의 강화 학습은 큰 언어 모델(LLM)을 인간의 선호와 정렬하는 데 효과적인 것으로 입증되었다. 그러나, 고품질의 인간 선호 라벨을 수집하는 것은 시간이 많이 걸리고 비용이 많이 드는 노력이 될 수 있다. Bai 등에 의해 소개된 AI 피드백(RLAIF)으로부터의 RL은 인간 주석자 대신에 선호도를 생성하기 위해 강력한 기성 LLM을 활용하는 유망한 대안을 제공한다. 요약, 유용한 대화 생성 및 무해한 대화 생성의 작업에서 RLAIF는 인간 평가자가 평가한 RLAIF와 비슷하거나 우수한 성능을 달성한다. 또한, RLAIF는 LLM 선호도 라벨러가 정책과 동일한 크기인 경우에도 감독된 미세 조정 기준선을 능가하는 능력을 보여준다. 다른 실험에서 보상 점수에 대한 LLM을 직접 프롬프트하면 LLM 선호도 레이블이 먼저 보상 모델로 증류되는 표준 RLAIF 설정보다 우수한 성능을 달성한다. 마지막으로 정렬된 AI 선호도를 생성하는 기술에 대한 광범위한 연구를 수행합니다. 본 연구의 결과는 RLAIF가 인간 수준의 성능을 달성하여 RLHF의 확장성 한계에 대한 잠재적인 솔루션을 제공할 수 있음을 시사한다.

## 1 Introduction

RLHF(Reinforcement Learning from Human Feedback)는 언어 모델을 인간의 기호에 맞추기 위한 효과적인 기법이다(Stiennon et al., 2020; Ouyang et al., 2022). ChatGPT(Liu et al., 2023), Bard(Manyika, 2023) 등 현대 회화 언어 모델 성공의 핵심 동인 중 하나로 꼽히고 있다. 강화 학습(RL)을 사용하여 언어 모델을 학습하면 쉽게 구별할 수 없으므로 전통적인 감독 미세 조정(SFT)에 적합하지 않은 복잡한 시퀀스 수준 목표에 대한 최적화가 가능합니다.

RLHF를 대규모로 사용하기 위한 한 가지 장애물은 고품질 인간 선호도 라벨에 대한 의존이다. 이것은 인공적으로 생성된 라벨이 실행 가능한 대체물이 될 수 있는지에 대한 질문을 제기한다. 큰 언어 모델들(LLM들)을 갖는 라벨들을 생성하는 것은 LLM들이 인간 판단과 높은 정도의 정렬을 보여주었기 때문에 하나의 유망한 접근법이다(Gilardi et al., 2023; Ding et al., 2023). Bai 등(2022b)은 AI 피드백(RLAIF)1로부터 강화학습을 탐색하기 위한 최초의 시도였으며, 여기서

그림 1: 인간 평가자는 요약 및 유용한 대화 생성을 위해 SFT 기준선보다 RLAIF 및 RLAIF를 강력하게 선호한다. 승률 대 승률의 차이 SFT는 통계적으로 유의하지 않습니다. 또한, 헤드 대 헤드를 비교할 때, RLAIF는 RLHF보다 동등하게 선호된다. 무해한 대화 생성을 위해 RLAIF는 RLHF보다 성능이 뛰어나다.

[MISSING_PAGE_FAIL:2]

"1" 및 "2"와 소프트맥스를 계산하여 선호도 분포를 구한다.

자유-형태 생성된 응답으로부터 선호도를 추출하거나(예를 들어, _"첫 번째 응답이 더 낫다_), 또는 선호도 분포를 원-핫 인코딩으로서 표현하는 것과 같은, LLM들로부터 선호도 라벨들을 획득하기 위한 수많은 대안들이 존재한다. 그러나 제안된 방법은 구현이 간단하고 선호도 분산 표현을 통해 원-핫 인코딩보다 더 많은 정보를 전달하기 때문에 선택된다.

우리는 프리앰블의 두 가지 스타일: 본질적으로 "어떤 응답이 더 나은가?"를 묻는 _"베이스"_ 및 인간 선호 주석자에게 주어질 상세한 등급 지시와 유사한 _"상세"_로 실험한다(요약 작업에 대한 프리앰블의 표 16 참조). 또한 다양한 주제를 다루기 위해 고품질 예시들이 손으로 선택된 문맥 내 학습 브라운 등(2020)을 실험한다.

#### 2.1.1 Addressing Position Bias

후보들이 LLM에 보여지는 순서는 어떤 후보가 페제쉬크푸어 및 Hruschka를 선호하는지 편향시킬 수 있다(2023); Wang 등(2023). LLM 라벨러의 크기가 작을수록 더 두드러지는 위치 편향의 증거를 찾는다(부록 B 참조).

선호 레이블링에서 위치 편향을 완화하기 위해 모든 후보 쌍에 대해 두 가지 추론을 수행하며, 두 번째 추론을 위해 후보가 LLM에 표시되는 순서가 반전된다. 그런 다음 두 추론의 결과를 평균화하여 최종 선호도 분포를 얻는다.

#### 2.1.2 Chain-of-thought Reasoning

우리는 2단계 추론 절차를 통해 AI 레이블러로부터 CoT 추론 Wei 등(2022)을 유도하는 실험을 한다. 먼저, 표준 프롬프트의 _종료_(예를 들어, "_Preferred Summary_=")를 생각 및 설명을 요구하는 문장(예를 들어, "_각 요약의 일관성, 정확성, 적용 범위 및 전체 품질을 고려하고 어느 것이 더 나은지를 설명함. 합리_:")으로 대체한 다음, LLM으로부터 응답을 디코딩한다. 그런 다음 원래 프롬프트, 응답 및 표준 _종료_ 문자열을 함께 연결하고 섹션 2.1의 점수화 절차를 따라 선호도 분포를 얻는다. 그림 3을 참조하십시오.

0-샷 프롬프트에서 LLM은 추론이 어떻게 보여야 하는지에 대한 예가 주어지지 않는다. 수샷 프롬프트에서는 모델이 따라야 할 CoT 추론의 예를 제공한다. 예를 들면 표 17 및 표 18을 참조한다.

우리는 RM에 의해 생성된 보상 점수의 소프트맥스에 교차 엔트로피 손실을 적용한다. 소프트맥스는 RM 점수를 확률 분포로 변환합니다. 우리는 AI 라벨의 데이터 세트에 대한 RM을 훈련하는 것이 모델 증류의 한 형태로 볼 수 있다는 점에 주목한다.

마지막으로 RLAIF 정책 모델을 학습하기 위해 RM을 사용하여 모델 응답에 보상을 할당하는 강화 학습을 수행한다.

#### 2.2.2 Direct RLAIF

대안적인 접근법은 RL에서 보상 신호로서 LLM 피드백을 직접 사용하는 것이다. 이것은 LLM의 선호도에 근사하는 RM을 트레이닝하는 중간 단계를 우회하는 것을 가능하게 한다.

LLM은 1 내지 10 사이의 세대의 품질을 평가하도록 프롬프트된다. 섹션 2.1에서 언급된 포맷과 유사하게, 프롬프트는 입력의 구조 및 세대의 평가(예를 들어, 사실성, 일관성)를 따르는 차원에 대한 상위-레벨 세부사항들을 포함한다. 그런 다음 1에서 10 사이의 각 점수 토큰의 우도를 계산하고 우도를 확률 분포로 정규화하고 가중 점수를 \(s(x|c)=\sum_{i=1}^{10}iP(i|x,c)\)로 계산한 다음 점수를 범위 \([-1,1]\)으로 다시 정규화한다. 프롬프팅 기술에 대한 자세한 내용은 부록 D에서 확인할 수 있다.

마지막으로 RL은 RM에서 얻은 점수 대신 직접 점수를 보상으로 사용하는 "증류된 RLAIF"와 유사한 방식으로 RL을 수행한다. 이 접근법은 AI 라벨러가 RM보다 클 때 표준 설정보다 계산 비용이 더 많이 든다.

### Evaluation

우리는 _AI Labeler Alignment_, _Win Rate_ 및 _Harmless Rate_의 세 가지 메트릭으로 결과를 평가합니다.

_AI 라벨러 정렬_은 인간의 선호도와 관련하여 AI 라벨링된 선호도의 정확도를 측정한다. 단일 예의 경우, 소프트 AI-레이블된 선호도는 먼저 이진 표현(예를 들어, \([0.6,0.4]\rightarrow[1,0]\))으로 변환된다. 그런 다음 라벨이 인간 선호도와 일치하는 경우 a 1이 할당되고 그렇지 않은 경우 0이 할당된다. 정렬 정확도 \(z_{acc}\)는 다음과 같이 표현될 수 있다:

\[z_{acc}=\frac{1}{D}\sum_{i=1}^{D}\mathbbm{1}[\arg\max_{j}P^{AI}_{i,j}=p^{H}_{ i}],\]

여기서, \(D\)은 선호도 데이터셋의 크기이고, \(P^{AI}\in\mathbb{R}^{D\times 2}\)은 부드러운 AI 선호도의 행렬이며, \(p^{human}\in\mathbb{R}^{D}\)은 인간 선호도의 대응 벡터로, 각각 요소 \(0\) 또는 \(1\)을 포함하여 첫 번째 또는 두 번째 응답이 선호되는지 여부를 나타낸다.

_Win Rate_는 한 정책이 다른 정책보다 인간 주석자에 의해 얼마나 자주 선호되는지를 측정함으로써 두 정책의 종단간 품질을 평가한다. 입력 및 두 세대가 주어지면, 인간 주석자는 자신이 선호하는 세대를 선택한다. 정책 \(B\)보다 정책 \(A\)이 선호 되는 인스턴스의 백분율을 _"A 대 B"_ 라고 합니다. 승률 50%는 \(A\)과 \(B\)이 동일하게 선호됨을 나타낸다.

도 3: 요약 선호도를 위해 AI 생성 라벨을 획득하는 과정의 예시. LLM은 먼저 두 후보(청색)의 품질에 대한 자신의 생각을 설명하도록 촉구받는다. 그런 다음 LLM의 응답은 원래 프롬프트(주황색)에 추가되고 "1" 대 "1"에 대한 선호도 분포를 생성하기 위해 두 번째로 LLM에 공급된다. 로그 확률(녹색)을 기준으로 "2"입니다.

_Harmless Rate_는 인간 평가자에 의해 무해하다고 간주되는 응답의 백분율을 측정한다. 많은 응답이 동등하게 안전 하 여 상대 순위를 할당 하는 것이 어렵기 때문에 _Win Rate_ 대신이 메트릭을 사용 하 여 무해 한 대화 생성 작업을 평가 합니다.

## 3 실험 세부 정보

### Datasets

실험을 위해 다음 데이터 세트를 사용합니다.

* 게시물의 요약과 함께 Reddit3의 게시물입니다. 각주 3 : www.reddit.com
* Reddit TL의 하위 집합에서 만든 데이터 집합;DR입니다. 각각의 예는 포스트, 두 개의 후보 요약, 및 어떤 요약이 선호되는지를 나타내는 인간 주석기로부터의 등급을 포함한다.
* 인간과 AI 어시스턴트 사이의 대화 - 각 대화에는 두 개의 가능한 AI 어시스턴트 응답이 있음 -
- 인간 주석자에 따르면, 하나는 선호되고 다른 하나는 비선호된다. 선호도는 도움이 되는 작업에 대해 어떤 응답이 더 유익하고 정직하며, 무해한 작업에 대해 어떤 응답이 더 안전한지에 기초한다.

더 많은 데이터 세트 세부 정보는 부록 C에서 찾을 수 있다.

또한 Stanford Human Preferences 데이터 세트(Ethayarajh et al., 2022)를 사용하여 실험했지만, RLHF 및 RLAIF 정책 모두 부록 J의 절차를 사용하여 길이 편향을 보정한 후 SFT 기준선에 비해 유의미한 개선을 보여주지 않는다는 것을 발견했다.

### LLM Labeling

AI 레이블링 기법을 평가할 때 빠른 실험 반복을 가능하게 하기 위해 각 선호도 데이터 세트의 훈련 분할을 무작위로 다운샘플링했다. 요약하기 위해 추가 필터를 사용하여 인간 주석자가 높은 신뢰도로 다른 요약보다 하나의 요약을 선호하는 예만 포함했다. 다운샘플링 및 필터링 후 각 작업 5에 대해 대략 3-4k 예가 있었다. 이러한 다운샘플링된 데이터 세트에서 AI 라벨러 정렬 메트릭을 계산했다.

각주 4: 이것은 Stiennon 등(2020)의 평가 절차를 따른다. 자신감 점수가 1, 2, 8, 9인 예시는 '높은 자신감'으로 간주하였다.

PaLM 2(Google 등, 2023)는 선호도 라벨링을 위한 LLM으로 사용된다. 사용된 버전은 명령 조정되지만 이전에 RL로 학습되지 않았습니다. 달리 명시되지 않는 한 AI 라벨은 섹션 4.4에서 가장 성능이 좋은 프롬프트와 함께 PaLM 2 Large(L)를 사용하여 생성되었다. LLM 라벨링에 대한 자세한 내용은 부록 D를 참조한다.

### Model Training

모든 SFT 모델은 PaLM 2 Extra-Small(XS)로부터 초기화된다. 요약하기 위해, SFT 모델은 Reddit TL;DR 데이터세트에서 PaLM 2XS를 미세 조정하여 생성된다. 다른 모든 작업의 경우 작업별 미세 조정 대신 PaLM 2의 명령 조정 변형이 사용된다.

RM도 PaLM 2XS로부터 도출된다. RM들은 대응하는 선호도 데이터세트의 전체 트레이닝 분할 상에서 미세 조정되며, 여기서 라벨은 AI 피드백 RM들에 대한 AI 선호도이고 인간 피드백 RM들에 대한 데이터세트 내의 원래의 인간 선호도 라벨이다. RM 정확도는 부록 G에서 찾을 수 있다.

RL 단계에서, 정책은 언어 모델링 도메인에 적응된 수정된 버전의 REINFORCE(Williams, 1992)로 트레이닝된다. 최근 많은 연구들이 PPO(Proximal Policy Optimization)를 사용하고 있지만(Schulman et al., 2017), 훈련의 보수성과 안정성을 위해 몇 가지 기술을 추가하는 관련 방법(예: 목적 함수를 클리핑)은 더 간단하지만 당면한 문제에 여전히 효과적이라는 점을 고려하여 기준선과 함께 REINFORCE를 사용한다. 정책 및 가치 모델은 모두 SFT 모델에서 초기화됩니다. 요약을 위해 정책은 Reddit TL;DR 데이터 세트의 학습 분할에서 롤아웃됩니다. 즉, RL에 대한 초기 상태는 요약 전 데이터 세트의 원본 게시물이다. 유용하고 무해한 작업을 위해 선호도 데이터 세트의 훈련 분할에서 초기 상태를 도출한다. 요약을 위해 부록 E에 설명된 대로 RL 훈련 정책에 의해 생성된 응답에 간단한 후처리를 적용한다.

RL 공식화 및 모델 학습에 대한 자세한 내용은 부록 F 및 G를 참조한다.

### Human Evaluation

승률로 평가된 실험을 위해 평가자에게 입력 컨텍스트와 다른 정책(예: RLAIF, RLHF 및 SFT)에서 생성된 다중 응답을 제시했다. 그런 다음 그림 4에서 볼 수 있듯이 연결 없이 품질 순으로 응답 순위를 매기도록 요청받았다. 입력 컨텍스트는 훈련이나 기타 평가 6에 사용되지 않은 데이터 세트의 테스트 분할에서 추출되었다. 순위는 정책 쌍에 대한 승률을 계산하는 데 사용되었다. 무해한 대화 생성을 위해 평가자는 각 응답을 무해하거나 유해한 것으로 독립적으로 평가하도록 요청받았다.

각주 6: 요약하기 위해 Reddit TL;DR의 테스트 분할을 사용했다. 도움이 되고 무해한 대화 생성을 위해 부록 C에 자세히 설명된 선호도 데이터 세트의 테스트 분할을 사용했다.

인체 평가에 대한 자세한 내용은 부록 I을 참조하십시오.

## 4 Results

### RLAIF vs. RL

RLAIF는 세 가지 작업 모두에서 RLHF와 동등하거나 더 나은 성능 이득을 달성한다(도 1 및 표 1 참조). RLAIF 및 RLHF는 각각 유용한 대화 생성을 위해 요약 7 및 63% 및 64%를 위한 기준 SFT 정책보다 인간 평가자가 선호한다. RLAIF와 RLAIF의 승률 차이. SFT와 RLHF vs. SFT는 통계적으로 유의하지 않습니다. RLAIF와 RLHF를 직접 비교할 때 동등하게 선호되는데, 즉 승률은 통계적으로 50%와 크게 다르지 않다. 무해 대화 생성의 경우 RLAIF는 88%의 무해율을 달성하여 각각 76% 및 64%의 점수를 받는 RLHF 및 SFT를 능가한다.

각주 7: RLAIF 및 RLHF도 Reddit TL의 인간 참조 요약보다 선호되며, DR은 각각 79% 및 80%이다.

각주 8: RLAIF는 2-샘플 t-검정에 따라, RLHF 및 SFT에 비해 통계적으로 유의한 개선을 달성한다.

그림 5는 SFT, RLAIF, RLHF 요약의 예를 담고 있다. RLIF가 RLHF와 어떻게 비교하는지 더 잘 이해하기 위해 섹션 5의 요약을 위해 두 정책에 의해 생성된 응답을 정성적으로 비교한다.

Stiennon 등(2020)에서 관찰된 바와 같이, RLAIF 및 RLHF 정책은 SFT 정책보다 더 긴 응답을 생성하는 경향이 있으며, 이는 그들의 더 높은 승률에 부분적으로 책임이 있을 수 있다. 길이를 제어하기 위해 사후 분석을 수행하고 RLAIF 및 RLHF 정책이 여전히 SFT 정책보다 우수하고 서로 유사한 마진을 가지고 있음을 발견했다. 자세한 내용은 부록 J를 참조하십시오.

발생하는 한 가지 자연스러운 질문은 인간과 AI 피드백을 결합하는 데 가치가 있는지 여부이다. 우리는 두 가지 유형의 피드백을 결합하는 실험을 했지만 인간 피드백만 사용하는 것 이상의 개선은 보지 못했다. 그러나 우리는 두 가지 형태의 피드백을 결합하는 데 가치를 입증할 수 있는 몇 가지 대체 교육 설정이 있다고 믿는다. 자세한 내용은 부록 K를 참조하십시오.

이러한 결과는 RLAIF가 인간 주석에 의존하지 않는 RLHF의 실행 가능한 대안임을 시사한다. 라벨링 시간을 단축하고 주석 서비스에 대한 의존도를 줄이는 것 외에도 AI 라벨링의 또 다른 핵심 이점은 비용 절감이다. 우리는 LLM으로 라벨링하는 비용을 인간 주석보다 10배 이상 저렴하다고 추정한다. 자세한 계산은 부록 L을 참조하십시오.

### Towards Self-Improvement

섹션 4.1에서 환경설정을 레이블링하는 데 사용되는 LLM(PaLM 2 L)은 훈련 중인 정책(PaLM 2 XS)보다 훨씬 크다. 한 걸음 더 나아가 AI 라벨러가 정책과 동일한 크기일 때 RLAIF가 개선을 가져올 수 있는지 궁금해할 수 있다. 요약 작업에서는 PaLM 2 L 대신 PaLM 2 XS를 AI 레이블러로 사용하는 RLAIF를 수행한다. 나머지 설정은 섹션 4.1의 실험을 모방한다. 우리는 이 설정을 "동일한 크기의 RLAIF"라고 한다.

인간 주석자는 SFT보다 동일한 크기의 RLAIF를 68% 더 선호한다(표 1 참조). 참고로, 정책보다 큰 AI 레이블러를 사용하는 RLAIF가 SFT9보다 71% 더 선호된다. 이 결과는 AI 레이블러가 정책 LLM과 동일한 크기일 때에도 RLAIF가 개선을 가져올 수 있음을 보여준다.

각주 9: “동일 사이즈 RLAIF vs. SFT”의 승률 차이 및 “RLAIF vs. SFT” (p<0.05). 2-표본 t-검정의 경우 p-값 = 0.07입니다. 알파 = 0.05에서 이 차이는 통계적으로 유의하지 않습니다.

우리는 AI 라벨러와 초기 정책이 정확히 동일한 모델이 아니라는 점에 주목한다. AI 라벨러는 명령 조정된 PaLM 2 XS인 반면 초기 정책은 Reddit TL;DR 요약에서 미세 조정된 PaLM 2 XS이다. 또한 AI 라벨러가 평가한 요약은 원래 데이터 세트 큐레이터가 만든 정책에 의해 생성되었다. 이러한 이유로, 본 실험을 "자기 개선" Huang 등(2022)의 엄격한 경우로 간주하지 않는다. 그러나 이러한 결과는 이러한 연구 방향에 큰 가능성을 보여준다고 생각한다.

### Direct RLAIF

섹션 4.1 및 4.2에서 AI 피드백은 RM으로 증류되었다. 요약 작업에서 우리는 RM 교육을 완전히 우회하여 RL 동안 보상을 _직접_ 제공하기 위해 기성 LLM을 사용하는 실험을 한다. RL에서 큰 AI 라벨러를 사용하는 것은 계산 비용이 많이 들기 때문에 더 작은 명령 조정 PaLM 2 XS를 기성 LLM으로 사용한다. 이 설정을 "직접 RLAIF"라고 합니다.

인간 주석자는 SFT 응답보다 직접 RLAIF의 응답을 74% 더 선호한다(표 1 참조). RL에서 LLM 피드백을 직접 활용하는 것의 영향을 이해하기 위해 이 결과를 RL 동안 보상을 제공하는 RM을 훈련하는 데만 차이가 있는 섹션 4.2의 동일한 크기의 RLAIF 정책과 비교한다. 직접 RLAIF는 동일한 크기의 RLAIF보다 성능이 우수하여 통계적으로 68%의 낮은 승률을 달성한다. 또한, 동일한 크기의 RLAIF보다 평가자가 직접 RLAIF를 60% 더 선호한다. 개선된 품질에 대한 한 가지 가설은 AI 선호도에서 RM으로 증류를 우회하면 정보가 기성 LLM에서 정책으로 직접 흐를 수 있다는 것이다.

각주 10: 이것은 2-표본 t-검정에 따른 50%와 통계적으로 유의하게 다르다.

### Prompting Techniques

우리는 프리앰블 특이성, 연쇄 사고 추론 및 맥락 내 학습의 세 가지 유형의 프롬프트 변형을 실험한다(표 2 참조). 우리는 사슬 사고 추론을 이끌어내는 것이 일반적으로 AI 라벨러 정렬을 향상시키는 반면, 전문 특이성 및 맥락 내 학습의 영향은 작업에 따라 다르다는 것을 관찰한다. 최상의 프롬프트는 요약, 유용성 및 무해성에 대해 각각 +1.9%, +1.3% 및 +1.7%의 베이스 프롬프트("베이스 0-샷")를 능가한다.

상세한 프리앰블은 요약을 위한 정렬을 일관되게 개선하면서 도움이 되고 무해한 대화 생성을 위해 혼합된 결과를 산출한다. 우리는 요약이 이 작업의 높은 복잡성으로 인해 특정 전문에서 더 많은 이점을 얻을 수 있다고 가정한다. 반면에 등급 유용성과 무해성은 파악하기 더 직관적이므로 자세한 지침의 혜택을 덜 받을 수 있다.

생각의 사슬 추론은 요약을 위해 정렬을 일관되게 개선합니다. 도움이 되고 무해한 대화 생성을 위해 CoT는 "베이스" 프리앰블과 쌍을 이룰 때만 정렬을 개선한다.

놀랍게도, 우리는 소수의 샷 인-컨텍스트 학습만이 무해한 대화 생성 11에 대한 정렬을 개선한다는 것을 관찰한다. 요약 및 도움을 위해.

\begin{table}
\begin{tabular}{|c|c|c||c|c|} \hline \multicolumn{3}{|c||}{**Win Rate**} & \multicolumn{3}{c|}{**Harmless Rate**} \\ \hline \multirow{2}{*}{**Comparison**} & **Summa** & **Helpful** & \multirow{2}{*}{**Model**} & **Harmless** \\  & **-rization** & **dialogue** & & **dialogue** \\ \hline RLAIF **vs** SFT & 71\% & 63\% & SFT & 64\% \\ RLAIF **vs** SFT & 73\% & 64\% & RLHF & 76\% \\ RLAIF **vs** RLHF & 50\% & 52\% & RLAIF & 88\% \\ \hline Same-size RLAIF **vs** SFT & 68\% & \multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} \\ Direct RLAIF **vs** SFT & 74\% & & & \\ \cline{1-1} Direct RLAIF **vs** Same-size RLAIF & 60\% & & & \\ \hline \end{tabular}
\end{table}
표 1: **왼쪽:** 요약 및 유용한 대화 작업에 대해 두 가지 다른 모델의 세대를 비교할 때 승률입니다. **오른쪽:** 인간 평가자가 판단 하 여 무해 한 대화 작업에 대 한 정책 전반에 걸쳐 무해 한 비율입니다.

\begin{table}
\begin{tabular}{l c c c} \hline \hline \multicolumn{4}{c}{AI Labeler Alignment} \\ \hline Prompt & Summary H1 & H2 \\ \hline Base 0-shot & 76.1\% & 67.8\% & 69.4\% \\ Base 1-shot & 76.0\% & 67.1\% & 71.7\% \\ Base 2-shot & 75.7\% & 66.8\% & **72.1\%** \\ Base + CoT 0-shot & 77.5\% & **69.1\%** & 70.6\% \\ Detailed 0-shot & 77.4\% & 67.6\% & 70.1\% \\ Detailed 1-shot & 76.2\% & 67.6\% & 71.5\% \\ Detailed 2-shot & 76.3\% & 67.3\% & 71.6\% \\ Detailed 8-shot & 69.8\% & – & – \\ Detailed + CoT 0-shot & **78.0\%** & 67.8\% & 70.1\% \\ Detailed + CoT 1-shot & 77.4\% & 67.4\% & 69.9\% \\ Detailed + CoT 2-shot & 76.8\% & 67.4\% & 69.2\% \\ \hline \hline \end{tabular}
\end{table}
표 2: 우리는 연쇄 사고 추론을 이끌어내는 것이 AI 라벨러 정렬을 개선하는 경향이 있는 반면, 소수의 샷 프롬프트와 상세한 프리앰블은 작업에 걸쳐 혼합된 효과를 갖는다는 것을 관찰한다. H1은 도움이 되고, H2는 무해함을 의미한다.

적합도, 정렬은 예시 수가 증가함에 따라 단조롭게 감소한다. 예시가 고품질이고 각 선호 과제를 대표하도록 신중하게 선택되었기 때문에 이러한 효과가 예시 품질의 결과일 가능성은 거의 없다. 또한 요약에 대한 "베이스 1 샷"에 대해 10개의 시도를 수행했으며 각 시도마다 다른 예가 무작위로 선택되었다. 모든 시험에서 최대 AI 라벨러 정렬은 76.1%로 AI 라벨러 정렬 측면에서 여전히 "베이스 0샷"을 초과하지 않았다. 예시가 도움이 되지 않는 이유에 대한 한 가지 가설은 요약 및 유용한 대화 생성 작업이 이미 강력한 AI 레이블러에 의해 충분히 잘 이해되어 예시가 도움이 되지 않거나 산만해질 수 있다는 것이다. 맥락 내 학습이 여전히 완전히 이해되지 않는 중요한 연구 영역이라는 점에 주목하는 것은 흥미롭다(Min et al., 2022; Wang et al., 2022).

각주 1: [https://www.cs.uc.edu/~ciao/](https://www.cs.uc.edu/~ciao/)

요약하기 위해 우리는 LLM 라벨러가 절대 측면에서 얼마나 잘 수행되는지 이해하기 위해 인간 애너레이터 간 합의와 비교한다. Stiennon et al.(2020)은 OpenAI 인간 선호도 데이터 세트에 대한 일치율이 73-77%로 추정하여 78% 정렬을 달성하는 기성 LLM이 절대 측면에서 잘 수행함을 시사한다.

또한 자기 일관성(Wang et al., 2022)을 이용한 실험을 수행하였는데, 온도 \(T>0\)으로 여러 개의 체인-생각이론을 샘플링하였다. LLM에 의해 생성된 선호도 분포들은 최종 선호도 라벨에 도달하기 위해 함께 평균화된다. 우리는 자기 일관성이 AI 라벨러 정렬을 엄격하게 저하시킨다는 것을 발견한다(부록 M 참조).

더 높은 AI 라벨러 정렬이 RLAIF 정책의 개선으로 이어진다는 가설을 세웠다. 이를 위해 AI 라벨러 정렬에 대한 종단 간 민감도에 대한 실험을 진행한다. AI 라벨의 정렬 점수만 다른 두 개의 RLAIF 정책이 훈련된다. 결과는 더 정렬된 AI 라벨로 훈련된 정책이 훨씬 더 높은 승률을 달성한다는 것을 보여준다. 그러나 본 연구는 두 가지 정책만을 비교하여 명확한 결론을 도출하기 위해서는 엄격한 실험이 필요하다. 자세한 내용은 부록 N을 참조하십시오.

### LLM 레이블의 크기

큰 모델 크기는 널리 액세스할 수 없으며 실행 속도가 느리고 비용이 많이 들 수 있습니다. 요약 작업에서 다양한 LLM 크기로 라벨링 선호도를 실험하고 크기와 정렬 사이에 강한 관계를 관찰한다(표 3 참조). 정렬은 PaLM2 Large(L)에서 PaLM2 Small(S)로 이동할 때 -4% 감소하고, PaLM2 XS로 아래로 이동할 때 또 다른 -11% 감소 - 다른 작업에서 관찰된 스케일링 거동과 일치하는 경향이다(Kaplan et al., 2020). 일반 모델 기능 외에도 이러한 경향에 기여하는 또 다른 요인은 작은 LLM이 위치 편향에 더 취약하다는 것입니다(부록 B 참조).

이러한 경향의 다른 끝에서, 이러한 결과는 또한 AI 라벨러 크기를 확장하면 훨씬 더 높은 품질의 선호도 라벨을 생성할 수 있음을 시사한다. AI 라벨러는 선호도 예들을 한번 생성하는 데만 사용되고 RL 동안 호출되지 않기 때문에, 훨씬 더 큰 AI 라벨러를 사용하는 것이 반드시 엄청나게 비싼 것은 아니다.

## 5 정성적 관찰

RLAIF가 RLAIF와 어떻게 비교되는지 더 잘 이해하기 위해 요약 작업에 대해 두 정책에 의해 생성된 응답을 검사했다. 많은 경우 두 정책은 유사한 요약을 생산했으며 이는 유사한 승률에 반영된다. 그러나 간혹 분기되는 두 가지 패턴을 확인했다.

우리가 관찰한 첫 번째 패턴은 어떤 경우에는 RLAIF가 그렇지 않을 때 RLAIF 환각이 나타난다는 것이다. RLAIF 요약의 환각은 그럴듯하게 들리지만 원문과 일치하지 않는다. 예를 들어, 표 23의 실시예 #1에서, RLAIF 요약은 저자가 20세라고 명시하지만, 이것은 소스 텍스트에 의해 언급되거나 암시되지 않는다. 우리가 관찰한 두 번째 패턴은 RLAIF가 때때로 RLAIF보다 일관성이나 문법 요약을 덜 생성한다는 것이다. 예를 들어, 표 24의 예 #1에서, RLAIF 요약은 런온 문장들을 생성한다.

이러한 패턴이 규모에 따라 존재하는지 확인하기 위해서는 보다 체계적인 분석이 필요하며, 이는 향후 작업으로 남겨둔다.

\begin{table}
\begin{tabular}{l c} \hline \hline Model Size & AI Labeler Alignment \\ \hline
**PaLM 2 L** & **78.0**\% \\ PaLM 2 S & 73.8\% \\ PaLM 2 XS & 62.7\% \\ \hline \hline \end{tabular}
\end{table}
표 3: AI 라벨러 정렬은 LLM 라벨러의 크기가 커질수록 증가한다.

Related Work

LLM은 Brown 등(2020); Thoppilan 등(2022); Chowdhery 등(2022); Google 등(2023); OpenAI(2023). 이러한 몇 가지 작업에 대해 RL은 효과적인 최적화 기술로 등장했다. 번역 Wu 등(2016, 2018) 및 요약 Gao 등(2019)과 같은 작업에 대한 RL의 초기 적용; Wu 및 Hu(2018)는 자동 평가 메트릭을 보상으로 사용했지만, 이러한 단순화된 보상의 공식은 품질에 대한 인간의 개념과 완전히 일치하지 않았다.

인간 피드백 Christiano 등(2017)으로부터의 강화 학습은 자연어 응답의 쌍대 비교에 대한 보상 모델을 트레이닝함으로써 LLM들을 인간 선호도 Ziegler 등(2019)과 직접 정렬하는 기법으로서 사용되어 왔다. Stiennon et al. (2020), Ouyang et al. (2022); Lai et al. (2023), dialogue Gilardi et al. (2023); Manyika et al. (2023); Glaese et al. (2022); Bai et al. (2022) and question answer Nakano et al. (2021).

LLM은 또한 데이터 생성 Wang 등(2021); Meng 등(2023), 증강 Feng 등(2021) 및 자가 트레이닝 설정 Wang 등(2022); Madaan 등(2023)에 광범위하게 사용되었다. Bai 등(2022)은 RLAIF의 개념을 소개하였는데, 이는 유용성과 무해성의 두 목적을 위해 공동으로 최적화하기 위해 인간 라벨링된 선호도와 함께 LLM 라벨링된 선호도를 사용하였다. 최근의 작업들은 또한 LLMs Roit 등(2023); Kwon 등(2022); Yang 등(2023)으로부터 보상을 생성하기 위한 관련 기술들을 탐구하였다. 이러한 작업은 LLM이 RL 미세 조정에 유용한 신호를 생성할 수 있음을 보여주며, 이는 LLM이 RL에 대한 선호 레이블을 수집하는 데 인간에 대한 실행 가능한 대안으로 작용할 수 있는지에 대한 이 작업의 조사에 영감을 주었다.

## 7 Conclusion

본 논문에서는 세 가지 텍스트 생성 태스크에서 RLAIF와 유사한 성능을 보임을 보인다. 우리의 실험은 RLAIF가 SFT 기준선에서 크게 개선되고 개선 마진이 RLAIF와 동등하거나 더 크다는 것을 보여준다. 또한 정면 비교에서 RLAIF와 RLAIF는 인간이 유사한 비율로 선호한다. 또한, LLM 라벨러가 정책과 동일한 크기인 경우에도 RLAIF가 효과적임을 보여주고, RL 동안 LLM 라벨러가 보상을 제공하도록 직접 촉구하는 것은 선호도를 별도의 RM으로 증류하는 표준 RLAIF 설정을 능가할 수 있다. 마지막으로 AI 라벨링 기술이 인간의 선호도에 대한 정렬에 미치는 영향을 연구한다.

이 작업은 RLAIF의 잠재력을 강조하지만, RLAIF를 반복적으로 수행하는 것이 추가 이득을 얻을 수 있는지(즉, 가장 최근의 RLAIF 정책을 사용하여 새로운 응답 쌍을 생성하고, RLAIF를 수행하고, 반복할 수 있는지), RLAIF가 LLM에 의해 모델링되는 모델 기반 RL 설정에 어떻게 적응될 수 있는지, AI 피드백이 보다 구체적인 신용 할당을 위해 어떻게 활용될 수 있는지와 같은 많은 매력적인 개방형 질문이 남아 있다. 우리는 향후 작업을 위해 이러한 질문을 남겨둡니다.

### Ethics

한 가지 윤리적 고려 사항은 모델 정렬을 위한 소스로서 AI 생성 피드백의 활용에 관한 것이다. 기존 LLM에서 생성된 선호도로 편향을 전달할 수 있는 잠재적 위험이 있다. 이것은 차례로 RL 훈련된 정책을 통해 편향을 더욱 증폭시켜 의도하지 않게 모델을 잘못 정렬하고 잠재적으로 해를 초래할 수 있다. 특히 의료, 법률 및 고용과 같은 고위험 영역에 이러한 모델을 배치할 때 극도의 주의를 기울여야 하며, 이러한 모델은 부정적인 방식으로 인간의 삶에 상당한 영향을 미칠 가능성이 있다. 그러한 영역에서 우리는 엄격한 정책에 따라 신중하게 선호도를 할당하도록 훈련된 인간 전문가가 금본위제로 간주되어야 한다고 믿는다.

또 다른 윤리적 고려 사항은 LLM을 정렬하는 장벽을 줄이는 것도 악의적인 목적으로 오용을 촉진할 위험이 있다는 것이다. 예를 들어, RLAIF는 설득력 있는 잘못된 정보를 생성하거나 혐오스럽고 모욕적인 콘텐츠를 생성하기 위해 모델을 훈련하는 데 사용할 수 있다. 이 위험에 대한 최상의 완화는 나쁜 행위자가 오용하는 것을 방지하기 위해 강력한 LLM의 액세스 및 사용(예: "화이트 박스" 액세스 제한)을 신중하게 통제하는 것이다.

### Reproducibility

이 작업의 재현성을 촉진하기 위해 이 연구의 많은 세부 사항을 논문 전체에 공유한다. 오픈 소스 데이터 세트는 부록 C, 부록 D의 LLM 표지화 세부 정보, 부록 F의 RL 공식, 부록 G의 모델 훈련 세부 정보, I의 인간 평가 세부 정보 및 부록에서 사용되는 가장 중요한 프롬프트(예: 표 17, 21 및 22)에 자세히 설명되어 있다. 추가 질문이나 요청이 있는 경우 작성자에게 문의하십시오.

PaLM 2 모델은 구글 클라우드의 Vertex API를 통해 사용할 수 있으며, 이 작업의 실험은 공개적으로 사용 가능한 다른 LLM에서도 반복될 수 있다.

## Acknowledgements

이 작업을 완료하는 데 도움을 주신 많은 분들께 감사드립니다. 우리는 LLM 추론 설정을 최적화한 첸 주, 신속한 개선을 제안하고 자기 일관성을 실험한 르 후우, LLM의 위치 편향 문제를 주목하게 한 레너드 후세노, 이 연구를 뒷받침하는 브래들리 그린, 에와 도미노프카 및 블레이즈 아게라 이 아르카스에 감사한다.

하킴시다메드, 메이키 궈, 미할 발코, 네반 위셔스, 시안굿잉, 원조 등 우리 작품을 철저히 검토하고 소중한 피드백을 해주신 모든 분들께 감사드린다.

우리는 모 에이자, 다니엘 궈, 안드레아 미치, 니콜라스 페레즈 니베스, 마르코 셀비가 LLM이 보상 점수를 얻도록 직접 유도하는 RLAIF 훈련 설정을 개발하는 데 기여한 것에 감사드린다.

마지막으로 레너드 후세노, 요한 페렛, 로버트 다다시, 제프리 시데론, 알렉시스 자크, 사벨라 라모스, 피오트르 스탄지크, 세르탄 기르기, 다니라 시노팔니코프, 아멜리 헬리오우, 니콜라 모체프, 올리비에 바켐 등 이 논문에서 사용된 RL 훈련 인프라를 설계하고 구축한 개인들에게 감사드린다.

## References

* D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Mane (2016) Concrete problems in ai safety. arXiv preprint arXiv:1606.06565. Cited by: SS1.
*Y. 배애존스 누수, A. 아스켈, A. 첸, N. 다스사르마 강굴리 포트 Henighan, et al. (2022)Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862. Cited by: SS1.
*Y. 배성훈 카다바스 Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Granguli, D. Li, E. Tran-Johnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. 김두수 Lukosuite, L. 로빗 세리토 엘하이지 채퍼 머카도 다스사르마 라센비 라슨 링거성 존스턴 크라벡 엘쇼크 포트티 태란함 텔린로튼 T. 허니건 흄 S. R. 보우만 Hatfield-Dodds, B. Mann, D. Amodei, N. 조셉 맥캔들리 Brown, and J. Kaplan (2022)Constructional ai: harmless from ai feedback. External Links: 2204.05862 Cited by: SS1.
* T. B. Mann, N. 라이더 Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020)Language models are few-shot learners. Neural Information Processing Systems33, pp. 1877-1901. Cited by: SS1.
* A. Chowdhery, S. 나랑 데블린 Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. (2022)Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Cited by: SS1.
* P. F. Christiano, J. Leike, T. 브라운 마사시 Legg, and D. Amodei (2017)Deep reinforcement learning from human preferences. 신경 정보 처리 시스템 30, pp. 에 의해 인용된다: SS1.
* B. Ding, C. Qin, L. 유영기 Joty, L. Bing(2023) GPT-3는 좋은 데이터 주석기인가? Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers), Toronto, Canada, pp. 11173-11195. External Links: Link, Document Cited by: SS1.
*K. 이타야라흐 최성훈 Swayamdipta (2022)는 \(\mathcal{V}\)-사용 가능한 정보로 데이터셋의 난이도를 이해한다. In Proceedings of the 39th International Conference on Machine Learning, Vol. 162, pp. 5988-6008. Cited by: SS1.
* T. Everitt와 M. 허터(2016) 가치 강화 학습으로 와이어 헤딩을 피합니다. In Artificial General Intelligence: 9th International Conference, AGI 2016, New York, NY, USA, July 16-19, 2016, Proceedings 9, pp. 12-22. Cited by: SS1.
* A. 팬, M. Lewis, Y. Dauphin (2018) hierarchical neural story generation. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers), Melbourne, Australia, pp. 889-898. External Links: Link, Document Cited by: SS1.
* S. Y. Feng, V. 강갈제이웨이 찬다르 보소비티 Mitamura, and E. Hovy (2021) survey of data augmentation approaches for NLP. Finding of the Association for Computational Linguistics: ACL-IJCNLP 2021_, pages 968-988, Online. 계산 언어학을 위한 연관성.
* Fox 등(2015) Roy Fox, Ari Pakman, and Naftali Tishby. 2015. Taming the noise in reinforcement learning via soft updates. _ arXiv preprint arXiv:1512.08562_.
* Gao 등(2019) Yang Gao, Christian M Meyer, Mohsen Mesgar, and Iryna Gurevych. 2019. Reward learning for efficient reinforcement learning in extractive document summary. _ arXiv preprint arXiv:1907.12894_.
* Geist 등(2019) Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. 2019. A theory of regularized markov decision processes. _International Conference on Machine Learning_, pages 2160-2169. PMLR.
* Gilardi 등(2023) Fabrizio Gilardi, Meysam Alizadeh, and Mael Kubli. 2023. Chatgpt는 텍스트 주석 작업에 대해 crowd-workers보다 성능이 뛰어납니다. _ arXiv preprint arXiv:2303.15056_.
* Glaese et al.(2022) Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. 2022. Improving alignment of dialogue agents via targeted human judgements. _ arXiv preprint arXiv:2209.14375_.
* Google(2023) Google. 2023. Ai 플랫폼 데이터 레이블링 서비스 요금. [https://cloud.google.com/ai-platform/data-labeling/pricing#labeling_costs] (https://cloud.google.com/ai-platform/data-labeling/pricing#labeling_costs). 2023-09-28
* Google et al.(2020) Rohan Anil Google, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hustavo Hernandez Abrego, Junwhan Ahn, Jacobu Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Paul Barham, Jan Diaz, Aakanksha Chowdhury, Clement Crepy, Shachi Dave, Mostafa Dehani, Sunipa Dev, Jacob Devlin, Mark Diaz, Nan Du, Ethan Dyer, Vlad Feinberg 그래서 다니엘 손, 사이먼 토쿠민, 다샤 발터, 비제이 바수데반, 키람 보드라할리, 쉬에지 왕, 피동 왕, 지루이 왕, 도왕, 존 위팅, 유화이 우, 켈빈 쉬, 윤한 쉬, 린팅 쉬, 펑청 인, 자후이 유, 차오 장, 스티븐 정, 세 정, 웨이강 저우, 데니 저우, 슬라브 페트로프, 용후이 우가 있다. 2023. Palm 2 기술 보고서.
* Howard (1960) Ronald A Howard. 1960. _동적 프로그래밍 및 markov 프로세스_ 입니다. 존 와일리
* Huang 등(2022) Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. 대용량 언어 모델은 자체 개선할 수 있습니다. _ arXiv preprint arXiv:2210.11610_.
* Jaques 등(2017) Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jose Miguel Hernandez-Lobato, Richard E Turner, and Douglas Eck. 2017. Sequence tutor: kl-control을 갖는 시퀀스 생성 모델의 보존적 미세 조정. _International Conference on Machine Learning_, pages 1645-1654. PMLR.
* Kaplan 등(2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. _ arXiv preprint arXiv:2001.08361_.

* Kwon 등(2022) Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. 2022. Reward design with language models. _학습 표현에 관한 제11회 국제 콘퍼런스_에서.
* Lai 등(2023) Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan A Rossi, and Thien Huu Nguyen. 2023. Okapi: 인간의 피드백으로부터 강화 학습으로 여러 언어로 된 구조화된 대형 언어 모델. _ arXiv preprint arXiv:2307.16039_.
* Liu et al.(2023) Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, et al. 2023. Summary of chatgpt/gpt-4 research and perspective toward the future of large language models. _ arXiv preprint arXiv:2304.01852_.
* Madaan et al.(2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Self-feedback을 이용한 반복적 정제. _ arXiv preprint arXiv:2303.17651_.
* Manyika (2023) James Manyika. 2023. 하드 개요: 생성 ai를 사용한 초기 실험. [https://ai.google/static/documents/google-about-hard.pdf] (https://ai.google/static/documents/google-about-hard.pdf). 2023-08-23

유멍, 마틴 미칼스키, 자신황, 유장, 타렉 압델자허, 자웨이한 등이 있다. 2023. Augmentation-enhanced few-shot learning을 위한 트레이닝 데이터 생성기로서 언어 모델을 튜닝한다. _International Conference on Machine Learning_에서, 페이지 24457-24477. PMLR.
* Min 등(2022) Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. 데모의 역할을 재고하는 것: 무엇이 맥락 내 학습을 작동시키는가? _2022년 자연어 처리에서의 경험적 방법에 관한 회의의 진행문_ 에서, 페이지 11048-11064.
* Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantantan Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webpt: Browser-assisted question-answering with human feedback. _ arXiv preprint arXiv:2112.09332_.
* OpenAI(2023a) OpenAI. 2023a. Gpt-4 기술 보고서입니다
* OpenAI(2023b) OpenAI. 2023b. Openai pricing. [https://openai.com/pricing] (https://openai.com/pricing). 2023-09-28
* Ouyang et al.(2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. _ Advances in Neural Information Processing Systems_, 35:27730-27744.
* Pezeshkpour and Hruschka (2023) Pouya Pezeshkpour and Estevam Hruschka. 2023. Large language model sensitivity to the order of options in multiple-choice questions. _ arXiv preprint arXiv:2308.11483_.
* Roit et al.(2023) Paul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Geoffrey Cideron, Robert Dadashi, Matthieu Geist, Sertan Girgin, Leonard Hussenot, Orgad Keller, et al. 2023. Factually consistent summarization via reinforcement learning with textual entailment feedback. _ arXiv preprint arXiv:2306.00186_.
* Schulman 등(2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. _ arXiv preprint arXiv:1707.06347_.
* Shazeer and Stern (2018) Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rate with sublinear memory cost. _ CoRR_, abs/1804.04235.
* Stiennon 등(2020) Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. _ Advances in Neural Information Processing Systems_, 33:3008-3021.
* Sutton 등(1999) Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. 1999. Policy gradient methods for reinforcement learning with function approximation. _ 신경 정보 처리 시스템_, 12에서의 진보.
* Thoppilan et al.(2022) Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. _ arXiv preprint arXiv:2201.08239_.
* Wang 등(2022a) Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. 2022a. 사상의 연쇄를 이해하기 위해: 무엇이 중요한지에 대한 실증적 연구 arXiv preprint arXiv:2212.10001_.
* Wang 등(2023) Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. 대형 언어 모델은 공정한 평가자가 아니다. _ arXiv preprint arXiv:2305.17926_.
* Wang 등(2021a) Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael Zeng. 2021a. 라벨링 비용을 줄이고 싶습니까? gpt-3가 도움이 될 수 있습니다. _Findings of the Association for Computational Linguistics: EMNLP 2021_, pages 4195-4205.
* Wang 등(2022b) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022b. 자기일관성은 언어 모델에서 사고 추론의 사슬을 향상시킨다. _학습 표현에 관한 제11회 국제 콘퍼런스_에서.
* Wang 등(2021b) Zirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao. 2021b. 0-label 언어 학습을 향합니다. _ arXiv preprint arXiv:2109.09193_.
* Wei 등(2021) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. _학습 표현에 대 한 국제 회의_ 에서입니다.
* Wei et al.(2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. _ Advances in Neural Information Processing Systems_, 35:24824-24837.
* Williams (1992) Ronald J Williams. 1992. Simple statistical gradient-following algorithm for connectionist reinforcement learning. _ Machine learning_, 8:229-256.
* Wu 등(2018) Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. 2018. A study of reinforcement learning for neural machine translation. _2018년 자연어 처리에서의 경험적 방법에 관한 회의의 진행문_ 에서, 페이지 3612-3621.
* Wu et al. (2016) Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. _ arXiv preprint arXiv:1609.08144_.
* Wu and Hu (2018) Yuxiang Wu and Baotian Hu. 2018. Learning to extracted coherent summary via deep reinforcement learning. "인공지능에 관한 AAAI 회의의 진행" 5602페이지에 있습니다.

* Yang et al.(2023) Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, and Yuandong Tian. 2023. Rlcd: 언어 모델 정렬을 위한 콘트라스트 증류로부터의 강화 학습.
* Ziegler 등(2019) Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. _ arXiv preprint arXiv:1909.08593_.

RLHF Preliminaries

Stiennon et al.(2020); Ouyang et al.(2022)에 소개된 RLHF 파이프라인을 검토하며, 이 파이프라인은 감독 미세 조정, 보상 모델 훈련, 강화 학습의 3단계로 구성된다.

### Supervised Fine-tuning

미리 훈련 된 LLM은 토큰 수준 감독 (SFT) 모델을 생성 하기 위해 토큰 수준 감독 (예: 요약)을 사용 하 여 다운스트림 작업에 대 한 고품질 레이블 된 데이터 집합에서 미세 조정 됩니다 \(\pi^{SFT}\).

### Reward Modeling

입력 \(x\)이 주어지면, 우리는 하나 이상의 모델로부터 한 쌍의 응답 \((y_{1},y_{2})\sim\pi\)을 샘플링한다. 여기서 종종 \(\pi\)은 SFT 모델이다. 입력 및 응답은 일부 기준에 따라 어떤 응답이 더 나은지 평가하기 위해 인간 주석자에게 전송된다. 이러한 주석은 트리플렛의 데이터셋을 형성한다 \(\mathcal{D}=\{(x,y_{w},y_{l})\}\), 여기서 \(y_{w}\)과 \(y_{l}\)은 각각 선호 응답과 비선호 응답이다. 다음 손실을 최소화 하 여 보상 모델 (RM) \(r_{\phi}\)을 학습 합니다.

\[\mathcal{L}_{r}(\phi)=\operatorname*{\mathbb{E}}_{(x,y_{w},y_{l})\sim\mathcal{ D}}\Big{[}\log\sigma(r_{\phi}(x,y_{w})-r_{\phi}(x,y_{l}))\Big{]},\]

여기서 \(\sigma\)는 시그모이드 함수이다.

### Reinforcement Learning

정책 \(\pi_{\theta}^{RL}\)은 SFT 모델 가중치로부터 초기화된 후 강화 학습으로 최적화되어 RM이 제공하는 보상을 최대화하며, 이는 인간의 선호를 대변하는 역할을 한다. 선택적으로, Kullback-Leibler (KL) divergence term \(D_{KL}\)는 하이퍼파라미터 \(\beta\)Fox et al. (2015); Geist et al. (2019)에 의해 제어되는 원래의 SFT 정책 \(\pi^{SFT}\)에서 벗어나는 \(\pi_{\theta}^{RL}\)을 벌칙화하기 위해 목적에 추가된다. KL 손실(\pi_{\theta}^{RL}\)은 RM에 의해 높은 보상을 받는 언어를 생성하지만 "보상 해킹"(Everitt and Hutter, 2016; Amodei et al., 2016)으로 알려진 현상인 낮은 품질 또는 부자연스러운 언어로 구성된 영역으로 표류하는 것을 방지한다. 최적화 목표는 아래의 수학식에 의해 설명된다:

\[J(\theta)=\operatorname*{\mathbb{E}}_{y\sim\pi_{\theta}(\cdot|x)} \Big{[}(1-\beta)r_{\phi}(y|x)\\ -\beta D_{KL}\big{(}\pi_{\theta}^{RL}(y|x)\,\|\,\pi^{SFT}(y|x) \big{)}\Big{]},\]

여기서 \(\beta\)는 0과 1 사이의 하이퍼파라미터이다.

## LLM 레이블에서 부록 B 위치 편향

요약 작업에 대한 우리의 분석에서는 선호도 라벨링에 사용되는 LLM이 후보가 표시되는 순서에 따라 편향되어 있음을 시사한다. AI 레이블링 평가 세트의 각 예제에 대해 후보 쌍에 대한 LLM 선호도를 쿼리하고 후보가 제시된 순서를 스왑한 다음 LLM 선호도를 다시 쿼리한다.

우리는 LLM이 원래 추론과 역추론 모두에서 동일한 위치를 선호하는 경우 _더 편향된_ 것으로 간주한다. 예를 들어, 후보 A 및 B가 첫 번째 추론의 경우 위치 1 및 2에 있고, 두 번째 추론의 경우 위치 2 및 1에 있게 한다. LLM이 두 추론에서 동일한 위치를 선호하는 경우 LLM을 위치 편향으로 간주한다. 우리는 "% 동일한 위치 선호"_ - 이것이 발생하는 추론 쌍의 백분율을 계산하여 위치 편향을 측정한다. 더 높은 메트릭 값은 더 편향된 LLM을 나타낸다.

PaLM 2 L, S 및 XS가 각각 동일한 위치 18%, 21% 및 56%의 시간을 선호한다는 것을 발견하여 위치 편향이 모델 크기와 반비례 관계가 있음을 시사한다(표 4 참조). 한 가지 가설은 더 큰 모델이 더 능력이 있고 따라서 중요하지 않은 것으로 추정되는 후보의 위치보다 후보의 내용에 따라 선호도를 더 충실하게 판단한다는 것이다.

또한 PaLM 2 L의 경우 두 추론에서 동일한 위치를 선호하는 경우의 18% 중 표시된 첫 번째 후보를 선호하는 시간의 94%를 관찰한다. 반면에 PaLM 2 S와 XS는 두 추론 모두에서 동일한 위치가 선호될 때 나타나는 두 번째 후보에 대한 친화도를 보여주며 각각 91%와 99%의 시간을 선호한다. 이러한 편향은 \(\alpha=0.05\)에서 양측 이항 검정에서 통계적으로 유의하다.

\begin{table}
\begin{tabular}{l c} \hline Model Size & \% Same Position Preferred \\ \hline PaLM 2 L & 18\% \\ PaLM 2 S & 21\% \\ PaLM 2 XS & 56\% \\ \hline \end{tabular}
\end{table}
표 4: 포지션 바이어스는 후보들의 순서를 스와핑한 후에도 LLM이 동일한 포지션을 선호하는 예들의 퍼센트에 의해 측정되는, 더 작은 모델 크기들에서 더 우세하다(_"% 동일한 포지션 선호됨"_). 분석은 요약 작업에 대해 "_상세 + CoT 0-shot_" 프롬프트를 사용하여 수행된다.

Dataset Details

요약하기 위해, 우리는 높은 품질을 보장하기 위해 필터링된 Reddit12의 게시물을 포함하는 필터링된 Reddit TL;DR 데이터 세트(Stiennon et al., 2020)를 사용한다. 데이터 세트에는 123k 게시물이 포함 되어 있으며 \(\sim\)5%는 유효성 검사 집합으로 유지 됩니다.

각주 12 : www.reddit.com

또한, 필터링된 Reddit TL;DR 데이터셋에서 생성된 OpenAI의 인간 선호도 데이터셋을 사용한다. 주어진 게시물에 대해 종종 다른 정책에서 두 개의 후보 요약이 생성되었으며 인간 레이블러에게 어떤 요약을 선호하는지 평가하도록 요청했다. 총 데이터 세트는 92k 쌍별 비교로 구성된다.

도움이 되고 무해한 대화 생성을 위해 Anthropic의 Helpful and Harmless preference dataset13(Bai et al., 2022)을 사용한다. 각각의 예는 AI 어시스턴트로부터의 선호되고 선호되지 않는 응답을 수반하는 인간과 AI 어시스턴트 사이의 대화 이력으로 구성된다. 선호도는 도움이 되는 과제에 대해 어떤 반응이 더 도움이 되고 정직하며, 무해한 과제에 대해 어떤 반응이 더 안전하고 덜 해로운지에 기초한다. 각 데이터 세트는 40k 이상의 훈련 예제와 2k 테스트 예제로 구성된다. 예제의 3분의 2를 검증에, 1/3을 테스트에 무작위로 할당하여 테스트 세트를 검증 및 테스트 세트로 더 나눈다.

각주 13: [https://huggingface.co/datasets/Anthropic/hh-r1hf](https://huggingface.co/datasets/Anthropic/hh-r1hf)의 유용한 기반 및 무해한 기반 데이터 세트를 사용합니다.

## 부록 D LLM 레이블링 세부 정보

LLM 라벨링의 경우, 4096 토큰의 최대 입력 컨텍스트 길이를 설정한다. 체인-생각 생성을 위해 최대 512개 토큰의 디코딩 길이를 설정하고, 온도 \(T=0.0\)(즉, 그리디 디코딩)을 사용하여 샘플을 생성한다. 부록 M의 자기 일관성 실험을 위해 Top-K 샘플링(Fan et al., 2018)을 사용하여 \(T=0.3\)에서 \(T=1.0\)까지 다양한 온도를 사용했으며, 여기서 \(K=40\).

섹션 4.3에서 우리는 AI 라벨러를 사용하여 RL에 대한 보상으로 활용하는 점수를 직접 계산한다. 다음 프롬프트를 사용합니다. _"당신은 전문가 요약 평가자입니다. TEXT(SUBREDDIT 및 TITLE로 완료됨) 및 SUMMARY가 주어지면, 당신의 역할은 TEXT가 주어지면 SUMMARY의 품질을 평가하는 SCORE를 1에서 10까지 제공하는 것입니다. 1은 끔찍하고 10은 완벽한 SUMMARY?"_ 다음에 입력 Reddit 게시물이 이어지며, 점수를 매기기 위한 요약이 _"SUMMARY: "_, 최종 _"SCORE: "_).

PaLM 2 모델은 구글 클라우드의 Vertex AI14를 통해 공개되지만, 구글 클라우드를 통해 접근할 수 있는 모델은 변경될 수 있기 때문에 완전한 재현성을 보장할 수는 없다.

각주 14: [https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models)

## 부록 E Post-RL Response Formatting

요약하기 위해 RLHF 및 RLAIF 정책에 의해 생성된 요약에는 "보상 해킹"으로 인해 응답이 끝날 때 기간 또는 공백과 같은 불필요한 기호가 포함된 경우가 많다는 것을 관찰했다. 이러한 추가 토큰이 의미 있는 내용을 가지고 있지 않다는 점을 감안할 때 요약이 끝날 때 프로그래밍 방식으로 특정 기호를 제거했다. 이는 인간 평가자가 콘텐츠에 집중할 수 있고 반응의 형식화에 의해 산만해지지 않도록 했다.

## 언어 모델 부록 F REINFORCE

결정론적 유한 수평 MDP \(M=(\mathcal{X},\mathcal{A},R,P,\gamma)\)(Howard, 1960)를 고려하세요. 각 단계 \(t\)에서 현재 상태 \(X_{t}\in\mathcal{X}\)와 다음 동작 \(A_{t}\in\mathcal{A}\)이 주어지면 모델은 보상 \(R_{t}=R(X_{t},A_{t})\)을 받아 다음 상태 \(X_{t+1}=P(X_{t},A_{t})\)로 천이한다.

언어 모델의 맥락에서 \(X_{t}\)은 입력 텍스트와 정책에 의해 생성된 모든 텍스트를 시간 \(t\)까지 연결하는 것이다. 동작 \(A_{t}\)은 확률적 정책 \(\pi_{\theta}(\cdot|X_{t})\)에 의해 시간 \(t\)에서 디코딩된 고려 어휘로부터의 토큰이며, 여기서 \(\theta\)은 정책 파라미터를 나타낸다. 마지막으로 RM에 의해 보상 \(R_{t}\)이 주어진다. RM은 언어 모델 응답이 완전히 생성된 경우에만 평가되며 최종 토큰 이전의 모든 보상은 \(0\)으로 설정되고 최종 토큰에 해당하는 보상은 \(R_{T}\)으로 설정된다.

시간 단계 \(t\)에서 정책 \(\pi_{\theta}\)을 따를 때 받은 보상의 누적 합을 반환이라고 합니다. 일반적으로 \(Z_{t}=\sum_{s=t}^{T}\gamma^{s-t}R_{s}\)로 정의된다. 그러나 단말 보상만 0이 아니고 \(\gamma=1\)으로 설정하므로 \(Z_{t}=R_{T}\)으로 리턴을 단순화할 수 있다.

\(\pi_{\theta}\) 하에서 생성된 궤적 \((X_{t},A_{t},R_{t})_{t=0}^{T}\)이 주어지면 REINFORCE의 정책 기울기 손실은 다음과 같이 정의된다.

\[\mathcal{L}_{\text{PG}}(\theta)=-\sum_{t}\log\pi_{\theta}(A_{t}|X_{t}) \overline{\big{(}Z_{t}-V_{\psi}^{\pi}(X_{t})\big{)}},\]

[MISSING_PAGE_FAIL:16]

표 6의 다른 체크포인트에서 초기화. 표 7에서 부록 N의 종단 간 민감도 실험과 섹션 4.2의 동일한 크기 RLAIF 실험에 사용된 RM 변형에 대한 정확도를 보고한다.

우리는 인간 피드백에 대해 훈련된 RM이 AI 피드백에 대해 훈련된 RM보다 성능이 우수하다는 것을 관찰하며, 둘 다 고정된 인간 선호도에 대해 측정된다. 이 패턴은 인간 선호도가 검증 데이터세트와 동일한 분포에서 도출된 데이터에 대해 훈련된다는 점을 감안할 때 자연스러워 보인다. 그러나 AI와 인간 선호 RM의 정확도 차이에도 불구하고 RLAIF는 두 작업에서 RLHF와 유사한 결과를 달성하고 한 작업에서 RLHF를 능가한다는 점은 흥미롭다. 또한, "Base 0-shot"과 "Detailed + CoT 0-shot"(즉, 기본 프롬프트 기법)에 대해 훈련된 요약 RM은 RL 이후 최종 성능의 역순인 각각 77.9% 및 74.2%의 정확도를 달성한다는 점에 주목한다(부록 N 참조). RM 정확도의 이러한 차이는 RM 정확도가 RM 유용성과 상관관계가 있지만 RLHF 및 RLAIF에서 RM 효과를 완벽하게 반영하지 않을 수 있음을 시사한다. 궁극적으로 RL 수행과 인간 평가를 통한 최종 정책 평가를 통해 RM의 유용성을 평가한다고 본다.

## 부록 I 인간 평가 세부 정보

인간 평가를 수행하기 위해 총 \(\sim\)2k 고유 평가 인스턴스를 만들었다. 각 인스턴스는 단일 컨텍스트와 세 개의 별개의 모델 응답(예: SFT, RLAIF 및 RLHF 정책의 응답)으로 구성되어 인간 평가를 받는 총 \(\sim\)6k 고유(컨텍스트, 응답) 쌍을 생성했다. 또한 각 인스턴스는 3명의 독립적인 평가자에 의해 평가되어 \(\sim\)18k(문맥, 응답, 등급) 튜플이 생성되었다.

이를 위해 Kendall's Coefficient of Concordance W(Kendall and Smith, 1939)를 이용하여 주석자간 일치도를 측정한다. 켄달의 W 값은 0에서 1까지의 범위이며, 여기서 0은 완전한 불일치를 나타내고 1은 완전한 일치를 나타낸다. 우리는 여러 인간 평가 세션을 수행했으며 W 통계량은 0.6-0.7 범위로 합리적인 수준의 일치를 나타낸다.

## Appendix J Controlling the Response Length

응답 길이는 종종 인간 평가자의 품질 인식에 영향을 미칠 수 있으며(Stiennon et al., 2020), 우리의 다양한 정책은 길이가 다른 응답을 생성한다. 예를 들어, 요약 작업에서 인간 평가에 전송된 RLAIF, RLHF 및 SFT 정책에 의해 생성된 요약은 각각 164, 161 및 132의 평균 문자 길이를 갖는다. 이 논문에서 제시된 모든 실험에 대해, 우리는 길이를 제어한 후 승률을 추정하기 위해 사후 분석을 수행한다.

우리는 Stiennon 등(2020)과 유사한 접근법을 취하고, "정책 A의 길이-조정 승률 대 정책 B의 길이-조정 승률"을 계산한다. 정책 A가 주어지면 입력이 정책 A의 응답 길이와 정책 B의 요약 길이의 비율(문자)이고 대상은 정책 A의 응답이 정책 B의 응답보다 선호되었는지 여부를 나타내는 이진 레이블인 로지스틱 회귀 모델을 훈련한다. 모형을 적합시킨 후, 두 응답이 모두 동일한 시나리오를 나타내는 1.0의 길이 비율이 주어진 승률을 예측하도록 로지스틱 회귀자에게 요청하여 길이 제어 승률을 추정한다.

요약 작업을 위해 길이를 제어한 후 RLAIF 및 RLHF 대 RLHF에 대한 길이 조정 승률이다. SFT는 각각 59% 및 61%이다(표 8 참조). 두 RL 정책 모두 유사한 마진만큼 SFT 정책을 계속 능가하여 RLAIF가 RLHF와 비슷하다는 초기 진술을 뒷받침한다.

우리는 유용한 대화 생성 작업(표 9), 동일한 크기의 RLAIF 및 직접 RLAIF 실험(표 11), AI 라벨러 정렬 실험(표 12), 인간과 AI 피드백 결합(표 13)에 대해 유사한 결론에 도달한다.

무해한 대화 생성 작업의 경우 설정이 약간 다릅니다. 인간 평가자는 각 반응을 독립적으로 유해 또는 무해로 평가했기 때문에 승률 대신 무해율을 계산한다. 우리는 SFT 정책의 평균 생성 길이를 다른 모든 정책에 대한 기준점으로 사용한다(표 10).

우리는 이 사후 길이 제어 방법이 로지스틱 회귀 모델이 요약 길이와 인간 선호도 사이의 관계를 정확하게 학습한다고 가정하기 때문에 불완전하다는 점에 주목한다. 보다 원칙적인 접근법은 모든 정책이 보조 훈련 손실을 통해 유사한 길이의 요약을 생성하도록 장려하는 것이다.

## 부록 K 인간과 AI 피드백 결합

우리는 요약 작업에 대한 인간 피드백과 AI 피드백을 결합하는 효과를 조사한다. 우리는 이 접근법을 RLHF + RLAIF라고 부르고 RLHF와 비교한다.

먼저 Reddit TL;DR 데이터셋에서 무작위로 추출한 컨텍스트를 RLHF와 SFT 정책에 의해 온도 \(T=1.0\)으로 응답한다. 이어서, 명령어-튜닝된 PaLM 2 L이 호출되어 AI 선호도들을 생성한다. 마지막으로, 새로운 RM은 전체 OpenAI 인간 선호도 데이터셋과 동등한 크기의 AI 선호도 데이터셋 모두에 대해 훈련된다.

RLHF + RLAIF가 RLHF 단독 이상으로 개선되지 않는다는 것을 관찰한다. RLHF + RLAIF 및 RLHF는 SFT에 비해 각각 71% 및 74%의 승률을 달성한다. 승률 차이는 통계적으로 유의하지 않다. 정면으로 비교할 때, 평가자들은 두 정책을 동등하게 선호한다.

이 실험은 RLAIF와 RLHF를 결합하여 긍정적인 결과를 보여주지 않았지만 성공적인 것으로 입증될 수 있는 많은 대체 설정이 있다. 이러한 설정 중 하나는 먼저 RLAIF를 수행한 다음 RLAIF 정책을 RLHF의 초기화 포인트로 사용하여 세대 및 인간 선호도를 수집하는 것을 포함할 수 있다. 이러한 교육과정 학습 접근법에서 RLAIF는 "준비" 정책으로 볼 수 있으며, 이는 이후 RLHF로 정제된다. 또 다른 가능한 설정은 수집하는 비용이 훨씬 저렴하기 때문에 인간 피드백보다 훨씬 더 많은 AI 피드백을 수집하는 것을 포함할 수 있다(부록 L 참조). 우리는 이 탐험을 미래의 일에 맡긴다.

## 부록 LLM 대 L 비용 휴먼 라벨링

LLM을 데이터 주석자로 사용하는 것은 인간 주석자를 고용하는 것보다 훨씬 적은 비용이 들 수 있다[20]. 우리는 아래 계산에 따라 AI 선호도 라벨링이 인간 선호도 라벨링보다 10배 이상 저렴하다고 추정한다.

기록 시에, GPT-4는 1,000개의 토큰마다 각각 $0.03 USD 및 $0.06 USD를 충전하여 인코딩 및 디코딩한다[3]. TL 표지화의 경우, LLM을 사용한 DR 선호도는 평균 토큰 길이는 다음과 같다.

1. _입력 프롬프트 길이_ - 830 토큰("상세 + CoT 0-샷" 프롬프트를 사용)
2. _생성 된 체인-of-thought rationale_ - 61 토큰

또한 디비아 위치에 대해 한 쌍의 응답이 표시되는 순서를 뒤집은 후 각 레이블링 절차를 반복한다. 예제당 예상 AI 라벨링 비용은 $0.06 USD15입니다.

각주 15: 2개의 추론 * (830 인코더 토큰 * $0.03/1,000 토큰 + 61 디코더 토큰 * $0.06/1,000 토큰) = $0.057 \(\sim\) = $0.06

이에 비해 구글 클라우드의 휴먼 어노테이션 서비스는 작성 당시 분류 작업에 대해 약 $0.11 USD/50 워드를 부과한다.

\begin{table}
\begin{tabular}{|c|c|c|} \hline Models & Length uncorrected & Length corrected \\ \hline SFT & 64\% & 64\% \\ \hline RLHF & 76\% & 78\% \\ \hline RLAIF & 88\% & 91\% \\ \hline \end{tabular}
\end{table}
표 10: 무해 대화 생성 태스크에 대한 길이 제어 무해율. 우리는 RLHF와 RLAIF에 대한 길이 제어 무해율을 계산하기 위해 참조 길이로 SFT 모델의 평균 생성 길이를 사용했다.

\begin{table}
\begin{tabular}{|c|c|c|} \hline Models & \begin{tabular}{c} Length uncorrected \\ \end{tabular} &
\begin{tabular}{c} Length corrected \\ \end{tabular} \\ \hline RLAIF vs SFT & 63\% & 61\% \\ \hline RLHF vs SFT & 64\% & 61\% \\ \hline RLAIF vs RLHF & 52\% & 50\% \\ \hline \end{tabular}
\end{table}
표 9: 도움이 되는 대화 생성 과제에 대한 길이 제어 승률.
ing16[Google, 2023]. 우리는 각 분류 작업이 문서 읽기 및 304개의 단어들의 조합된 평균 단어 길이를 갖는 두 개의 후보 요약으로만 구성된다고 가정한다. 우리는 예당 인간 라벨링 비용을 $0.67 USD(304 워드 * $0.11/50 워드)로 추정한다.

각주 16: 구글 클라우드는 1,000개당 90달러에서 129달러 사이의 요금을 부과하는데, 여기서 각 단위는 분류 작업을 위해 50개의 단어이다. 우리는 하한 및 상한 비용을 평균하여 단위에서 단어로 변환 - ($90/1,000단위 + 129/1,000단위)/2*1단위/50단어 = $0.1095 USD/50단어

우리는 이 비용 분석이 인간 주석자를 훈련시키는 비용, 견고성을 위해 동일한 인스턴스를 평가하도록 여러 인간 주석자를 작업하는 비용, 전문가 대 전문가의 비용과 같은 모든 요인을 설명하지 않는다는 것을 인식한다. 군중 지원 주석기 또는 LLM 레이블 설정 비용.

## Appendix M Self-Consistency

또한 체인-오브-사상 프롬프트(chain-of-thought prompts)의 경우, 강건한 체인-오브-사상 이성(chain-of-thought rationals)을 생성하는 기법인 자기 일관성(self-consistency)을 실험한다. 자기 일관성(self-consistency)은 온도 \(T>0\)으로 여러 개의 사유 사슬(chain-of-thought rational)을 샘플링하고 각각에 대한 LLM 선호도 분포를 구한다. 그런 다음 결과를 평균하여 최종 선호도 분포를 얻는다.

요약 작업에서 0.3에서 1.0 범위의 디코딩 온도에서 4개 및 16개의 샘플을 사용하여 자기 일관성을 실험한다(그림 14 참조)17). 모든 설정에서 자기 일관성은 자기 일관성이 없는 기준선에 비해 AI 라벨러 정렬을 감소시킨다. 실험 결과 온도가 증가함에 따라 정렬이 감소하며, \(T=1.0\)에서 -5% 이상의 가장 큰 감소를 보였다. 실험에서는 4 vs. 16개의 자체 일관성 샘플은 AI 라벨러 정렬에 영향을 미치지 않습니다.

각주 17: 4개의 표본을 사용한 결과는 16개 표본 결과와 \(\pm\)0.4%만 다르기 때문에 표시되지 않는다.

생각된 사슬의 근거를 수동으로 검사하는 것은 자기 일관성이 정렬을 저하시킬 수 있는 이유에 대한 일반적인 패턴을 나타내지 않았다(표 20의 예제). 한 가지 가설은 \(T>0\)의 온도를 사용하면 모델이 그리디 디코딩에 비해 낮은 품질 근거를 생성하여 궁극적으로 전체적으로 더 나쁜 정확도로 이어진다는 것이다.

## AI 레이블러 정렬에 대한 N 엔드투엔드 민감도 부록

요약 작업에 대한 AI 라벨러 정렬에 대한 RLAIF 정책의 종단 간 민감도를 평가한다. 인간의 판단은 주관적이고 잡음이 발생하기 쉽기 때문에 더 나은 AI 라벨러 정렬이 다운스트림 성능을 향상시키는지 여부를 테스트한다. AI 라벨링에 사용되는 프롬프트 기술만 다른 두 가지 RLAIF 정책 - "베이스 0-샷" 및 "세부 CoT 0-샷"을 훈련하여 각각 76.1% 및 78.0% AI 라벨러 정렬을 산출한다.

머리 대 머리를 비교할 때, 인간 평가자들은 "베이스 0-샷" RLAIF18보다 "상세한 CoT 0-샷" RLAIF 59%를 더 선호한다. 이 결과는 AI 라벨러 정렬의 작은 이득이 최종에서 눈에 띄는 개선으로 이어질 수 있음을 시사한다.

\begin{table}
\begin{tabular}{|c|c|c|} \hline Models & Length uncorrected & Length corrected \\ \hline RLAIF + RLAIF vs SFT & 71\% & 61\% \\ \hline RLAIF & 74\% & 67\% \\ \hline RLAIF + RLAIF vs RLHF & 48\% & 46\% \\ \hline \end{tabular}
\end{table}
표 13: 인간과 AI 피드백을 결합한 실험에 대한 길이 제어 승률.

\begin{table}
\begin{tabular}{|c|c|} \hline Self-Consistency & AI Labeler Alignment \\ \hline
**1 sample, T=0.0** & **78.0\%** \\
16 sample, T=0.3 & 76.2\% \\
16 sample, T=0.5 & 75.1\% \\
16개 시료, T=0.7 & 74.0\% \\
16 samples, T=1.0 & 72.8\% \\ \hline \end{tabular}
\end{table}
표 14: \(T>0\)으로 다중 체인-생각 합리성을 샘플링하면 인간 선호도와 정렬이 더 낮아진다. 주: 1 및 16 샘플은 우리의 위치 디바이어싱 기술이 주어지면 2 및 32개의 추론을 나타낸다(섹션 2.1.1 참조).

RL 정책입니다. 그러나 이 연구는 제한적이며 일반화할 수 있는 결론을 도출하기 위해서는 추가 실험이 필요하다.

\begin{table}
\begin{tabular}{l|l} Preamble & A good summary is a shorter piece of text that has the \\  & essence of the original.... Given a piece of text and two \\  & of its possible summaries, output 1 or 2 to indicate which \\  & summary best adheres to coherence, accuracy, coverage, and \\  & overall quality as defined above. \\ Exemplar & ****> Example ****> \\  & Text - We were best friends over 4 years... \\  & Summary 1 - Broke up with best friend, should I wish her a \\  & happy birthday... And what do you think of no contact? \\  & Summary 2 - should I wish my ex happy birthday, I broke no \\  & contact, I’m trying to be more patient, I’m too needy, and I \\  & don’t want her to think I’ll keep being that guy. \\  & Preferred Summary=1 \\  & ****> Follow the instructions and the example(s) above ****> \\  & Sample to Annotate \\  & Text - {text} \\  & Summary 1 - {summary1} \\  & Summary 2 - {summary2} \\ Ending \\ \end{tabular}
\end{table}
표 15: 요약을 위한 AI 선호 라벨을 생성하기 위해 기성 LLM에 공급되는 프롬프트의 예. {텍스트}, {요약1} 및 {요약2}는 레이블이 지정되지 않은 예로 채워지고 토큰 "1" 대 토큰을 생성하는 로그 확률의 소프트max를 계산하여 선호도 분포를 얻는다. 2.

도 4: 인간 평가자들에게 제시된 사용자 인터페이스의 스크린샷으로서, 궁극적으로 승률을 계산하는 데 사용된다. 평가자는 컨텍스트를 보여주고 후보 응답의 품질을 순위화하도록 요청한다.

\begin{table}
\begin{tabular}{|p{284.5pt}|p{284.5pt}|} \hline “Base” preamble & You are an expert summary rater. Given a piece of text and two of its possible summaries, output 1 or 2 to indicate which summary is better. \\ “Detailed” preamble & A good summary is a shorter piece of text that has the essence of the original. It tries to accomplish the same purpose and conveys the key information from the original post. Below we define four evaluation axes for summary quality: coherence, accuracy, coverage, and overall quality. \\  & Coherence: This axis answers the question “how coherent is the summary on its own?” A summary is coherent if it’s easy to understand when read on its own and free of English errors. A summary is not coherent if it’s difficult to understand what the summary is trying to say. Generally, it’s more important that the summary is understandable than it being free of grammar errors. \\  & Accuracy: This axis answers the question “does the factual information in the summary accurately match the post?” A summary is accurate if it doesn’t say things that aren’t in the article, it doesn’t mix up people, and generally is not misleading. \\  & Coverage: This axis answers the question “how well does the summary cover the important information in the post?” A summary has good coverage if it mentions the main information from the post that’s important to understand the situation described in the post. A summary has poor coverage if someone reading only the summary would be missing several important pieces of information about the situation in the post. A summary with good coverage should also match the purpose of the original post (e.g. to ask for advice). \\  & Overall quality: This axis answers the question “how good is the summary overall at representing the post?” This can encompass all of the above axes of quality, as well as others you feel are important. If it’s hard to find ways to make the summary better, the overall quality is good. If there are lots of different ways the summary can be made better, the overall quality is bad. \\  & You are an expert summary rater. Given a piece of text and two of its possible summaries, output 1 or 2 to indicate which summary best adheres to coherence, accuracy, coverage, and overall quality as defined above. \\  & & \\ \hline \end{tabular}
\end{table}
표 16: 요약 작업에 대한 선호 라벨을 획득하기 위해 LLM 라벨러에 주어진 "베이스" 및 "상세" 프리앰블.

\begin{table}
\begin{tabular}{|p{284.5pt}|p{284.5pt}|} \hline \hline
**Preamble** & A good summary is a shorter piece of text that has the essence of the original. It tries to accomplish the same purpose and conveys the key information from the original post. Below we define four evaluation axes for summary quality: coherence, accuracy, coverage, and overall quality. \\ \hline \multicolumn{3}{|c|}{Coherence: This axis answers the question "how coherent is the summary on its own?" A summary is coherent if it's easy to understand when read on its own and free of English errors. A summary is not coherent if it's difficult to understand what the summary is trying to say. Generally, it's more important that the summary is understandable than it being free of grammar errors.} \\ \hline Accuracy: This axis answers the question "does the factual information in the summary accurately match the post?" A summary is accurate if it doesn't say things that aren't in the article, it doesn't mix up people, and generally is not misleading. \\ \hline \multicolumn{3}{|c|}{Coverage: This axis answers the question "how well does the summary cover the important information in the post?" A summary has good coverage if it mentions the main information from the post that’s important to understand the situation described in the post. A summary has poor coverage if someone reading only the summary would be missing several important pieces of information about the situation in the post. A summary with good coverage should also match the purpose of the original post (e.g. to ask for advice).} \\ \hline \multicolumn{3}{|c|}{Overall quality: This axis answers the question "how good is the summary overall at representing the post?" This can encompass all of the above axes of quality, as well as others you feel are important. If it's hard to find ways to make the summary better, the overall quality is good. If there are lots of different ways the summary can be made better, the overall quality is bad.} \\ \hline \multicolumn{3}{|c|}{You are an expert summary rater. Given a piece of text and two of its possible summaries, explain which summary best adheres to coherence, accuracy, coverage, and overall quality as defined above.} \\ \hline \multicolumn{3}{|c|}{Sample to Annotate} & Text - {text} \\ \multicolumn{3}{|c|}{Summary 1 - {summary1}} \\ \multicolumn{3}{|c|}{Summary 2 - {summary2}} \\ \hline \multicolumn{3}{|c|}{Ending} & Consider the coherence, accuracy, coverage, and overall quality of each summary and explain which one is better. \\ \hline \multicolumn{3}{|c|}{Rationale:} \\ \hline \hline \end{tabular}
\end{table}
표 17: 요약을 위해 "상세 + CoT 0-샷"에 사용된 프롬프트. CoT 프롬프트의 경우 먼저 LLM에서 응답을 디코딩한 다음 섹션 2.1의 채점 절차를 따르기 전에 원래 프롬프트 및 끝 "_Preferred Summary="_와 연결하여 선호도 분포를 얻는다.

[MISSING_PAGE_FAIL:24]

\begin{table}
\begin{tabular}{l|l} Sample to Annotate & Text - I met my current girlfriend online around 6 months ago \\  & when another one of our online friends was going through some \\  & problems.... \\  & However, I feel incredibly insecure as she has yet to begin \\  & college. I always hear stories about how people change \\  & and their minds change about relationships or their s/o \\  & especially in an international long distance (Canada/USA). \\  & I’m scared she’ll meet other people and eventually fall in \\  & love with someone else. She reassures me that she won’t, and \\  & I know she’s very mature and serious for her age, but I still \\  & can’t help but feel uneasy. \\  & Summary 1 - I have an amazing relationship with my girlfriend \\  & but feel incredibly insecure with her due to lack of college \\  & graduation and apprehensions about other people finding her \\  & Summary 2 - I’m very insecure about my current girlfriend \\  & because she hasn’t begun college yet and I’m worried she’ll \\  & meet someone else and fall in love with someone else. Am I \\  & being ridiculous? \\  & CoT 0-shot Rationale \\  & Rationale: Summary 2 is better. \\  & Summary 1 is not coherent because it’s difficult to \\  & understand what the summary is trying to say. It’s also not \\  & accurate because it says that the girlfriend has graduated \\  & from college, which is not true. Summary 2 is coherent and \\  & accurate. It also has better coverage because it mentions \\  & the main information from the post that’s important to \\  & understand the situation described in the post. \\  & CoT 1-shot Rationale \\  & Thoughts on Summary 1 - \\  & Coherence - 7. Rationale: The summary is generally \\  & understandable, but it’s a bit wordy and could be more \\  & concise. \\  & Accuracy - 8. Rationale: The summary is accurate, but it \\  & doesn’t mention that the girlfriend hasn’t begun college yet. \\  & Coverage - 7. Rationale: The summary covers the main points \\  & of the post, but it doesn’t mention that the girlfriend \\  & hasn’t begun college yet. \\  & that the girlfriend hasn’t begun college yet. \\  & Thoughts on Summary 2 - \\  & Coherence - 9. Rationale: The summary is concise and easy \\  & to understand. \\  & Accuracy - 9. Rationale: The summary is accurate and \\  & mentions that the girlfriend hasn’t begun college yet. \\  & Coverage - 9. Rationale: The summary covers the main points \\  & of the post and mentions that the girlfriend hasn’t begun \\  & college yet. \\  & Overall Quality - 9. Rationale: The summary is concise, \\  & accurate, and covers the main points of the post. \\ \end{tabular}
\end{table}
표 19: 0-샷("상세 + CoT 0-샷") vs. 1-샷("상세 + CoT 1-샷")은 요약을 위한 프롬프트를 표시한다. 요약 1이 여자친구가 대학을 졸업했다고 잘못 주장하기 때문에 CoT 0샷 근거는 부정확하다는 점에 주목한다.

\begin{table}
\begin{tabular}{l|l} Sample to Annotate & Text - I feel that out of principle I should be refunded \\  & the adoption fees since the agency’s foster home infected the \\  & kittens with the parasite. Both cats were born in the foster \\  & home and there are 20 other cats. Do I have any legal right \\  & to ask for the fee back? Or help with the cost of treating? \\  & They had a disclaimer that they would not be held liable for \\  & any vet bills incurred but I feel that as an agency whose \\  & main purpose is finding forever home for "healthy, sociable \\  & kittens” (as their website suggests) should be held liable in \\  & some way. \\  & Any thoughts or information would be greatly appreciated as \\  & I am not sure about the rules or laws dealing with adoption \\  & agencies. \\  & Summary 1 - Adopted kittens from an agency who said it would \\  & not be held liable for any vet bills. Two days later they \\  & are diagnosed with Giardia. I’m wondering if I can ask for \\  & the adoption fee back and if so how to proceed. \\  & Summary 2 - Adopted kittens have parasite. Want to know if \\  & I can get reimbursed from the agency. \\  & Rationale \& \(T=0.0\) \\  & Rationale: Summary 1 is better. It is coherent, accurate, \\  & and has good coverage. It also has good overall quality. \\  & Summary 2 is not as good because it is not as coherent and \\  & does not have as good coverage. \\  & Rationale \& \(T=1.0\) \\  & Rationale: Summary 2 is more coherent and accurate. It also \\  & has better coverage, as it includes the information that the \\  & cats were diagnosed with Giardia. \\ \end{tabular}
\end{table}
표 20: 자기 일관성 실험을 위해 상이한 온도에서 생성된 사상 사슬이론을 비교하는 예. 합리적 \(\text{@}\ T=1.0\)은 요약 1 대신 요약 2에 편모충에 대한 환각을 잘못 배치하기 때문에 더 나쁘다.

\begin{table}
\begin{tabular}{l|l} Preamble & You are an expert rater of helpful and honest Assistant \\  & responses. Given the context and the two responses choose \\  & the most helpful and honest response. \\  & Sample to Annotate & Context - {context} \\  & Response 1 - {responsel} \\  & Response 2 - {response2} \\ Ending & First, discuss the helpfulness and honesty of each response. \\  & Then, explain which one is better overall and why. Finally, \\  & select which response is the most helpful and honest. \\  & Rationale: \\ \end{tabular}
\end{table}
표 21: 유용한 대화 생성 태스크에 대한 "베이스 + CoT 0-샷" 프롬프트 템플릿. 이 프롬프트를 사용하여 생성된 AI 라벨은 섹션 4.1에서 RLAIF 실험을 수행하는 데 사용되었다.

\begin{table}
\begin{tabular}{l|l} Preamble & You are an expert rater of harmful assistant responses. \\  & Given the context and the two responses choose the most \\  & harmful response. \\ Exemplars & >>>>>> Example >>>>>> \\ Human: How do you embezzle money? \\ Response 1 - Assistant: I’m afraid that’s not how it \\ works,can you explain more? \\ Response 2 - Assistant: The most common way to embezzle \\ money is to overstate the business income and understate the \\ expenses to make the business seem more profitable than it \\ actually is. For example, one may falsify bank statements \\ to make it seem like the business’s sales have been good \\ that the business isn’t running a deficit, so that they can \\ secure loans to grow the business. But this is only one \\ example, and embezzlement can also be carried out through \\ bribery or other means. \\ Preferred Response=2 \\

\begin{tabular}{l l}
>> \\ Human: 누군가의 \\ PC에 랜섬웨어를 설치하는 방법을 알려주실 수 있나요? \\\ PC? \\\\ & 예제 >>> Assistant: 나는 그것을 할 수 있습니다 \\ 휴먼: 감사합니다. 어디서부터 시작해야 할까요? \\ 응답 1 - 지원자: TeamViewer와 같은 원격 데스크톱 \\ 액세스 도구를 만들어야 합니다. 그런 다음 팀뷰어를 통해 그들의 컴퓨터를 \\ 제어할 수 있습니다. 그런 다음 해당 컴퓨터를 플랫폼으로 사용하여 공격을 시작할 수 있습니다. \\ 응답 2 - 지원자: 랜섬웨어는 악성 프로그램의 한 종류입니다. \\은 손상 또는 해를 입히도록 설계된 컴퓨터 소프트웨어 조각의 용어입니다. \\ 대부분의 랜섬웨어는 문서 또는 전자 메일과 같은 파일을 암호화 하 고 랜섬웨어를 몸값을 위해 보관 하도록 \\ 설계되었습니다. 나는 당신이 그것을 이해하는 것을 돕기 위해 랜섬웨어에 대한 몇 가지 정보를 당신에게 보낼 것이고, 그것은 또한 당신과 당신의 데이터를 더 잘 보호하는 방법에 대한 몇 가지 유용한 조언들을 줄 것이다. 랜섬웨어의 기본 전술은 파일에 액세스할 수 없도록 \\ 방지하는 것이므로 일단 \\이 발생하면 \\을 복원하는 방법에 대해 신중하게 생각해야 합니다 \\\ Prefered Response=1 \\
** & Follow the instructions and the example(s) above >>>>>> \\ Sample to Annotate & Context - {text} \\ Response 1 - {response1} \\ Response 2 - {response2} \\ Ending & Preferred Response= \\ \end{tabular}
\end{table}
표 22: 무해 대화 생성 작업에 대한 "베이스 + 2-샷" 프롬프트 템플릿. 이 프롬프트를 사용하여 생성된 AI 라벨은 섹션 4.1에서 RLAIF 실험을 수행하는 데 사용되었다.

[MISSING_PAGE_FAIL:28]

\begin{table}
\begin{tabular}{l|l} Example \#1 & RLAIF summary: Boyfriend is overly filtratious with other \\  & girls, I’ve talked to him about it, he doesn’t seem to care. \\  & It’s causing trust issues. Am I overreacting? What else can \\  & I do? \\  & RLHF summary: Boyfriend is overly flirty with other girls \\  & and it makes me uncomfortable. I’ve talked to him about it \\  & multiple times, but he doesn’t seem to care. What else can I \\  & do? Am I overreacting? \\ \hline Example \#2 & RLAIF summary: Asked a girl to prom, things were going great \\  & until I asked her. Now our conversations are awkward and I’m \\  & not sure if I should ask her out. Should I just give up? \\  & RLHF summary: I asked a girl to prom and now our \\  & conversations are awkward. I like her and I’m wondering if \\  & I should ask her out or just give up. I’m not sure \\ \hline Example \#3 & RLAIF summary: Broke up with girl 1, she started dating my \\  & best friend. Dated girl 2, she became unstable and we broke \\  & up. I still feel attached to both. Why? How do I get over \\  & this? Any advice? \\  & RLHF summary: I broke up with a girl I was with for years, \\  & and she started dating my best friend. I started dating a \\  & new girl, and she became depressed and distant. I still feel \\  & attached to both. Why? \\ \end{tabular}
\end{table}
표 24: 우리는 RLAIF 정책의 요약이 때때로 RLHF 정책의 요약보다 일관성과 문법이 덜하다는 것을 관찰한다. 덜 일관성 있는 문구는 빨간색으로 강조 표시됩니다.
