{
    "2309.00267": {
        "paper_id": "2309.00267",
        "abs_url": "https://arxiv.org/abs/2309.00267",
        "pdf_url": "https://arxiv.org/pdf/2309.00267.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2309.00267_RLAIF_Scaling_Reinforcement_Learning_from_Human_Feedback_with_AI_Feedback.pdf",
        "title": "RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Harrison Lee",
            "Samrat Phatale",
            "Hassan Mansoor",
            "Thomas Mesnard",
            "Johan Ferret",
            "Kellie Lu",
            "Colton Bishop",
            "Ethan Hall",
            "Victor Carbune",
            "Abhinav Rastogi",
            "Sushant Prakash"
        ],
        "abstract": "Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences. However, gathering high-quality human preference labels can be a time-consuming and expensive endeavor. RL from AI Feedback (RLAIF), introduced by Bai et al., offers a promising alternative that leverages a powerful off-the-shelf LLM to generate preferences in lieu of human annotators. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, RLAIF achieves comparable or superior performance to RLHF, as rated by human evaluators. Furthermore, RLAIF demonstrates the ability to outperform a supervised fine-tuned baseline even when the LLM preference labeler is the same size as the policy. In another experiment, directly prompting the LLM for reward scores achieves superior performance to the canonical RLAIF setup, where LLM preference labels are first distilled into a reward model. Finally, we conduct extensive studies on techniques for generating aligned AI preferences. Our results suggest that RLAIF can achieve human-level performance, offering a potential solution to the scalability limitations of RLHF.",
        "comments": "Added two more tasks and many more experiments and analyses (e.g. same-size RLAIF, direct RLAIF, cost analysis)",
        "official_code_urls": [],
        "pwc_page_url": "https://paperswithcode.com/paper/rlaif-scaling-reinforcement-learning-from",
        "bibtex": "@misc{lee2023rlaif,\n      title={RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback}, \n      author={Harrison Lee and Samrat Phatale and Hassan Mansoor and Thomas Mesnard and Johan Ferret and Kellie Lu and Colton Bishop and Ethan Hall and Victor Carbune and Abhinav Rastogi and Sushant Prakash},\n      year={2023},\n      eprint={2309.00267},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
    }
}