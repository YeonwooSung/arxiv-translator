<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'Entire Space Multi-Task Modeling via Post-Click Behavior Decomposition for Conversion Rate Prediction\n' +
      '\n' +
      'Hong Wen\n' +
      '\n' +
      'Corresponding author: Both authors contributed equally to this paper.\n' +
      '\n' +
      'Alibaba Group\n' +
      '\n' +
      'Hangzhou, Zhejiang, China 311121qinggan.wh@alibaba-inc.com\n' +
      '\n' +
      'Jing Zhang\n' +
      '\n' +
      'The University of Sydney\n' +
      '\n' +
      'Darlington NSW 2008, Australiajing.zhang1@sydney.edu.au\n' +
      '\n' +
      'Yuan Wang\n' +
      '\n' +
      'Alibaba Group\n' +
      '\n' +
      'Hangzhou, Zhejiang, China 311121wy175696@alibaba-inc.com\n' +
      '\n' +
      'Fuyu Lv\n' +
      '\n' +
      'Alibaba Group\n' +
      '\n' +
      'Hangzhou, Zhejiang, China 311121fuyu.lfy@alibaba-inc.com\n' +
      '\n' +
      'Wentian Bao\n' +
      '\n' +
      'Alibaba Group\n' +
      '\n' +
      'Hangzhou, Zhejiang, China 311121wentian.bwt@alibaba-inc.com\n' +
      '\n' +
      'Quan Lin, Keping Yang\n' +
      '\n' +
      'Alibaba Group\n' +
      '\n' +
      'Hangzhou, Zhejiang, China 311121{tieyi.lq,shaoyao}@taobao.com\n' +
      '\n' +
      '###### Abstract.\n' +
      '\n' +
      'Recommender system, as an essential part of modern e-commerce, consists of two fundamental modules, namely Click-Through Rate (CTR) and Conversion Rate (CVR) prediction. While CVR has a direct impact on the purchasing volume, its prediction is well-known challenging due to the Sample Selection Bias (SSB) and Data Sparsity (DS) issues. Although existing methods, typically built on the user sequential behavior path "impression\\(\\rightarrow\\)click\\(\\rightarrow\\)purchase", is effective for dealing with SSB issue, they still struggle to address the DS issue due to rare purchase training samples. Observing that users always take several purchase-related actions after clicking, we propose a novel idea of post-click behavior decomposition. Specifically, disjoint purchase-related Deterministic Action (DAction) and Other Action (OAction) are inserted between click and purchase in parallel, forming a novel user sequential behavior graph "impression\\(\\rightarrow\\)click\\(\\rightarrow\\)D(O)Action\\(\\rightarrow\\)purchase". Defining model on this graph enables to leverage all the impression samples over the entire space and extra abundant supervised signals from D(O)Action, which will effectively address the SSB and DS issues together. To this end, we devise a novel deep recommendation model named Elaborated Entire Space Supervised Multi-task Model (_ESM\\({}^{2}\\)_). According to the conditional probability rule defined on the graph, it employs multi-task learning to predict some decomposed sub-targets in parallel and compose them sequentially to formulate the final CVR. Extensive experiments on both offline and online environments demonstrate the superiority of _ESM\\({}^{2}\\)_ over state-of-the-art models. The source code and dataset will be released.\n' +
      '\n' +
      'Recommender System, Entire Space Multi-Task Learning, Post-Click Behavior Decomposition, Conversion Rate Prediction +\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote † †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote † †: Both authors contributed equally to this paper.\n' +
      '\n' +
      '+\n' +
      'Footnote †: Both authors contributed equally to this paper.\n' +
      '\n' +
      'samples compared with impressions, the number of training samples from the sequential behavior path "click\\(\\rightarrow\\)purchase" is insufficient to fit the large parameter space of CVR task, which results in the DS problem. As illustrated in Figure 2, how to deal with the SSB and DS problems is crucial for developing an efficient industrial-level recommender system.\n' +
      '\n' +
      'Several studies have been carried out to tackle these challenges (Ma et al., 2018; Wang et al., 2018; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019). For example, Ma _et al_. propose a new model named Entire Space Multi-Task Model (ESMM) (Ma et al., 2018), which defines CVR task on the user sequential behavior path "impression\\(\\rightarrow\\)click\\(\\rightarrow\\)purchase" via multi-task learning framework. It is trained with all impression samples over the entire space for two auxiliary tasks namely post-view CTR and post-view click-through conversion rate (CTCVR). Therefore, the derived CVR from CTR and CTCVR is also applicable in the same entire space when inferring online, thus addressing the SSB issue effectively. Besides, an auxiliary CTR network with rich labeled samples shares the same feature representation with the CVR network, helping to alleviate the DS issue. Although ESMM achieves better performance than conventional methods by dealing with the SSB and DS issues simultaneously, it still struggles to alleviate the DS issue due to the rare purchase training samples, \\(i\\)._e_., less than 0.1% of impression behaviors converts to purchase according to the large scale real transaction logs from our e-commerce platform.\n' +
      '\n' +
      'After a detailed analysis of the logs, we observe that users always take some purchase-related actions after clicking. For example, users may add the preferred items to their shopping cart (or wish list) instead of immediately purchases due to some reasons (_i_.\\(e\\)., waiting for a discount). Besides, these actions are indeed more abundant than purchase actions. Motivated by this, we propose a novel idea of post-click behavior decomposition. Specifically, disjoint purchase-related Deterministic Action (DAction) and Other Action (OAction) are inserted between click and purchase in parallel, forming a novel user sequential behavior graph "impression\\(\\rightarrow\\)click\\(\\rightarrow\\)D(O)Action\\(\\rightarrow\\)purchase", where the task relationship is explicitly defined by the conditional probability. Besides, defining model on this graph enables to leverage all impression samples over the entire space and extra abundant supervisory signals from post-click behaviors, efficiently addressing the SSB and DS issues.\n' +
      '\n' +
      'In this paper, we resort to deep neural networks to embody the above idea. Specifically, we propose a novel deep neural recommendation model named Elaborated Entire Space Supervised Multi-task Model (\\(ESM^{2}\\)), which consists of three modules: 1) a shared embedding module (SEM), 2) a decomposed prediction module (DPM), and 3) a sequential composition module (SCM). First, SEM embeds a one-hot feature vector of ID features into dense representation through a linear fully connected layer. Then, these embeddings are fed into the subsequent DPM, where individual prediction network estimates the probabilities of decomposed sub-targets in parallel by employing multi-task learning on all the impression samples over the entire space. Finally, SCM composes the final CVR as well as some auxiliary probabilities sequentially according to the conditional probability rule defined on the graph. Multiple losses defined on some sub-paths of the graph are used to supervise the training of \\(ESM^{2}\\).\n' +
      '\n' +
      'The main contributions of this paper are summarized as follows:\n' +
      '\n' +
      '\\(\\bullet\\) To the extent of our knowledge, we are the first to introduce the idea of post-click behavior decomposition to model CVR over the entire space. The explicit decomposition results in a novel user sequential behavior graph "impression\\(\\rightarrow\\)click\\(\\rightarrow\\)D(O)Action\\(\\rightarrow\\)purchase".\n' +
      '\n' +
      '\\(\\bullet\\) We propose a novel deep neural recommendation method named \\(ESM^{2}\\), which models CVR prediction and auxiliary tasks simultaneously in a multi-task learning framework according to the conditional probability rule defined on the user behavior graph. \\(ESM^{2}\\) can address the SSB and DS issues efficiently by harvesting the abundant post-click action data with labels.\n' +
      '\n' +
      '\\(\\bullet\\) Our model achieves better performance on the real-world offline dataset than representative state-of-the-art methods. We also deploy it in our online recommender system and achieve significant improvement, confirming its value in industrial applications.\n' +
      '\n' +
      'The rest of this paper is organized as follows. Section 2 presents a brief survey of related work, followed by the details of the proposed model in Section 3. Experiment results and analysis are presented in Section 4. Finally, we conclude the paper in Section 5.\n' +
      '\n' +
      '## 2. Related Work\n' +
      '\n' +
      'Our proposed method specifically tackles the conversion rate prediction problem by employing the multi-task learning framework over the entire space. Therefore, we briefly review the most related\n' +
      '\n' +
      'Figure 1. A diagram of online recommendation in e-commerce platform, comprising of two fundamental components, \\(i\\)._e_., system recommendation and user feedback.\n' +
      '\n' +
      'Figure 2. Illustration of sample selection bias problem in conventional CVR prediction, where training space only composes of clicked samples, while inference space is the entire space for all impression samples. And data volume gradually decreased from impression to purchase.\n' +
      '\n' +
      'work from the following two aspects: 1) conversion rate prediction and 2) multi-task learning.\n' +
      '\n' +
      '**Conversion Rate Prediction:** Conversion rate prediction is a key component of many online applications, such as search engines (Han et al., 2015; Wang et al., 2017), recommender systems (Han et al., 2015; Wang et al., 2017) and online advertising (Han et al., 2015; Wang et al., 2017). However, there are few literatures directly proposed for CVR tasks (Han et al., 2015; Wang et al., 2017; Wang et al., 2017), regardless of recent prosperous development of CTR methods (Han et al., 2015; Wang et al., 2017; Wang et al., 2017; Wang et al., 2017). Indeed, CVR modeling is very challenging since conversions are extremely rare events that only a very small portion of impression items are eventually being clicked and bought. Recently, the deep neural network has achieved significant progress in many areas including recommender systems due to its remarkable ability in feature representation and end-to-end modeling (Han et al., 2015; Wang et al., 2017; Wang et al., 2017; Wang et al., 2017; Wang et al., 2017). In this paper, we also adopt a deep neural network to model the conversion rate prediction task. In contrast to the above methods, we derive a new user sequential behavior graph "impression\\(\\rightarrow\\)click\\(\\rightarrow\\)D(O)Action\\(\\rightarrow\\)purchase" based on a novel idea of post-click behavior decomposition. According to the conditional probability rule defined on the graph, our network structure is specifically devised to predict several decomposed sub-targets in parallel and compose them sequentially to formulate the final CVR.\n' +
      '\n' +
      '**Multi-Task Learning:** Due to the temporal multi-stage nature of users\' purchasing behavior, _e.g._, impression, click, and purchase, prior work attempts to formulate the conversion rate prediction task by a multi-task learning framework. For example, Hadash _et al_. propose a multi-task learning-based recommender system by modeling the ranking and rating prediction tasks simultaneously (Hadash et al., 2015). Ma _et al_. propose a multi-task learning approach named multi-gate mixture-of-experts to explicitly learn the task relationship from data (Hadash et al., 2015). Gao _et al_. propose a neural multi-task recommendation model to learn the cascading relationship among different types of behaviors (Gao et al., 2016). In contrast, we model the CTR and CVR tasks simultaneously by associating with users\' sequential behavior graph, where the task relationship is explicitly defined by the conditional probability (See Section 3). Ni _et al_. propose to learn universal user representations across multiple tasks for more effective personalization (He et al., 2017). We also explore such an idea by sharing embedded features across different tasks. Recently, Ma _et al_. propose an entire space multi-task model (ESMM) for CVR prediction (Ma et al., 2015). It adds the CTR task and CTCVR task as an auxiliary to the main CVR task. Our method is partially inspired by ESMM but has the following significant difference: we propose a novel idea of post-click behavior decomposition to reformulate a novel user sequential behavior graph "impression\\(\\rightarrow\\)click\\(\\rightarrow\\)D(O)Action\\(\\rightarrow\\)purchase". Defining model on this graph enables to formulate the final CVR as well as some auxiliary tasks together. It can leverage all the impression samples over the entire space and the abundant supervisory signals from users\' post-click behaviors, which are highly relevant to the purchase behaviors, consequently addressing the SSB and DS issue simultaneously.\n' +
      '\n' +
      '## 3. Proposed Method\n' +
      '\n' +
      '### Motivation\n' +
      '\n' +
      'In practice, from an item being displayed to it being purchased successfully, we identify that there may exist multiple kinds of sequential actions a user could choose to do. For example, after clicking one interested item, a user may directly purchase it without any hesitation, or add it to the shopping cart and then make the purchase eventually. These behavior paths are shown in Figure 3(a). We can simplify and group these paths according to several predefined specific purchase-related post-click actions, _i.e._, adding to Shopping Cart (SCart) and adding to Wish list (Wish), as shown in Figure 3(b). Based on our data analysis of online real-world logs, we found that only 1% of clicked behaviors are converted to purchase eventually, indicating rare purchase training samples. However, the data volume of several post-click actions like SCart and Wish are much larger than purchase. For example, 10% will be added to the shopping cart given clicked behaviors. Besides, these post-click actions are highly relevant to the final purchase action, _e.g._, 12% (or 31%) will be bought eventually after they have been added to the shopping cart (or wish list). How can we leverage the larger volume of post-click behaviors to benefit CVR prediction in some manner, regarding their high relevance to purchase?\n' +
      '\n' +
      'Intuitively, one solution is to model these purchase-related post-click actions along with purchase into a multi-task prediction framework. The key is how to formulate them properly since they have explicit sequential correlations, _e.g._, the purchase action probably conditioned on the SCart or Wish action. To this end, we define a single node named Deterministic Action (DAction) to merge these predefined specific purchase-related post-click actions, such as SCart and Wish, as shown in Figure 3(c). DAction has two properties: 1) it is highly relevant to the purchase action and 2) it has abundant deterministic supervisory signals from users\' feedback, _e.g._, 1 for taking some specific actions (_i.e._, adding to shopping cart or wish list after clicking) and 0 for none. We also add a node named Other Action(OAction) between click and purchase to deal with other post-click behaviors except DAction. In this way, the conventional behavior path "impression\\(\\rightarrow\\)click\\(\\rightarrow\\)purchase" becomes to a novel elaborated user sequential behavior graph "impression\\(\\rightarrow\\)click\\(\\rightarrow\\)D(O)Action\\(\\rightarrow\\)purchase", as shown in Figure 3(c). Defining model on this graph enables to leverage all the impression samples over the entire space and extra abundant supervisory signals from D(O)Action, which will circumvent the SSB and DS issues efficiently. We call this novel idea as post-click behavior decomposition.\n' +
      '\n' +
      '### Conditional probability decomposition\n' +
      '\n' +
      'In this section, we present the conditional probability decomposition of CVR as well as related auxiliary tasks according to the digraph defined in Figure 3(c). First, the probability of post view click-through rate of an item \\(x_{i}\\), denoted as \\(p_{i}^{\\mathit{ctr}}\\), is defined as the conditional probability of being clicked given that it has been viewed, which depicts the path "impression\\(\\rightarrow\\)click" in the digraph. Mathematically, it can be written as:\n' +
      '\n' +
      '\\[p_{i}^{\\mathit{ctr}}=p\\left(c_{i}=1\\left|v_{i}=1\\right.\\right)\\overset{\\Delta}{=} y_{1i}, \\tag{1}\\]\n' +
      '\n' +
      'where \\(c_{i}\\in C\\) denotes whether the \\(i^{\\mathit{t}}h\\) item \\(x_{i}\\) is being clicked, \\(c_{i}\\in\\{0,1\\}\\), \\(C\\) is the label spaces of all the items being clicked or not, \\(i\\in[1,N]\\) and \\(N\\) is the number of items. Similarly, \\(v_{i}\\in V\\) denotes whether the \\(i^{\\mathit{t}}h\\) item \\(x_{i}\\) is being viewed (_i.e._, impression), \\(v_{i}\\in\\{0,1\\}\\), \\(V\\) is the label spaces of all the items being viewed or not. \\(y_{1i}\\) is a surrogate symbol for simplicity.\n' +
      '\n' +
      'Then, the probability of click-through DAction conversion rate of an item \\(x_{i}\\), denoted as \\(p_{i}^{etavr}\\), is defined as the conditional probability of being taken DAction given that it has been viewed, which depicts the path "impression\\(\\rightarrow\\)click\\(\\rightarrow\\)DAction" in the digraph. Mathematically, it can be written as:\n' +
      '\n' +
      '\\[\\begin{split} p_{i}^{etavr}&=p\\left(a_{i}=1\\left|v_{ i}=1\\right.\\right)\\\\ &=\\sum_{c_{i}\\in\\{0,1\\}}p\\left(a_{i}=1\\left|v_{i}=1\\right.,c_{i} \\right)p\\left(c_{i}\\left|v_{i}=1\\right.\\right)\\\\ &=p\\left(a_{i}=1\\left|v_{i}=1\\right.,c_{i}=0\\right)p\\left(c_{i}= 1\\left|v_{i}=1\\right.\\right)\\\\ &\\quad+p\\left(a_{i}=1\\left|v_{i}=1\\right.,c_{i}=1\\right)p\\left(c_ {i}=1\\left|v_{i}=1\\right.\\right)\\\\ &=y_{zi}y_{1i}\\end{split} \\tag{2}\\]\n' +
      '\n' +
      'where \\(a_{i}\\in A\\) denotes whether the \\(i^{th}\\) item \\(x_{i}\\) is being taken some specific actions defined in Section 3.1, \\(a_{i}\\in\\{0,1\\}\\), \\(A\\) is the label spaces of all the items being taken some specific actions or not. \\(y_{zi}=p\\left(a_{i}=1\\left|v_{i}=1\\right.,c_{i}=1\\right)\\), depicting the path "click\\(\\rightarrow\\)DAction", is a surrogate symbol for simplicity as \\(y_{1i}\\). It is trivial that \\(y_{zi}=p\\left(a_{i}=1\\left|c_{i}=1\\right.\\right)\\) since all the samples are impression samples (_i.e._, \\(v_{i}=1\\)). It is noteworthy that Eq. (2) holds due to the fact that no action occurs without being clicked, _i.e._, \\(p\\left(a_{i}=1\\left|v_{i}=1\\right.,c_{i}=0\\right)\\)=0.\n' +
      '\n' +
      'Next, the probability of conversion rate of an item \\(x_{i}\\), denoted as \\(p_{i}^{cvr}\\), is defined as the conditional probability of being bought given that it has been clicked, which depicts the paths "click\\(\\rightarrow\\)D(Action\\(\\rightarrow\\)purchase" in the digraph. Mathematically, it can be written as:\n' +
      '\n' +
      '\\[\\begin{split} p_{i}^{ecvr}&=p\\left(b_{i}=1\\left|c_ {i}=1\\right.\\right)\\\\ &=\\sum_{a_{i}\\in\\{0,1\\}}p\\left(b_{i}=1\\left|c_{i}=1\\right.,a_{i} \\right)p\\left(a_{i}\\left|c_{i}=1\\right.\\right)\\\\ &=p\\left(b_{i}=1\\left|c_{i}=1\\right.,a_{i}=0\\right)p\\left(a_{i}= 0\\left|c_{i}=1\\right.\\right)\\\\ &\\quad+p\\left(b_{i}=1\\left|c_{i}=1\\right.,a_{i}=1\\right)p\\left( a_{i}=1\\left|c_{i}=1\\right.\\right)\\\\ &\\quad\\overset{\\Delta}{=}y_{4i}\\left(1-y_{2i}\\right)+y_{2i}y_{3i} \\end{split} \\tag{3}\\]\n' +
      '\n' +
      'where \\(b_{i}\\in B\\) denotes whether the \\(i^{th}\\) item \\(x_{i}\\) is being bought, \\(b_{i}\\in\\{0,1\\}\\), \\(B\\) is the label spaces of all the items being bought or not. \\(y_{zi}=p\\left(b_{i}=1\\left|c_{i}=1\\right.,a_{i}=1\\right)\\), \\(y_{4i}=p\\left(b_{i}=1\\left|c_{i}=1\\right.,a_{i}=0\\right)\\) are some surrogate symbols for simplicity as \\(y_{1i}\\). \\(y_{3i}\\) or \\(y_{4i}\\) depicts the path "DAction\\(\\rightarrow\\)purchase" or "OAction\\(\\rightarrow\\)purchase" in the digraph, respectively.\n' +
      '\n' +
      'The probability of click-through conversion rate of an item \\(x_{i}\\), denoted as \\(p_{i}^{ctcvr}\\), is defined as the conditional probability of being bought given that it has been viewed, which depicts the complete graph "impression\\(\\rightarrow\\)click\\(\\rightarrow\\)D(O)Action\\(\\rightarrow\\)purchase" in the digraph. Mathematically, it can be written as:\n' +
      '\n' +
      '\\[\\begin{split} p_{i}^{etcvr}&=p\\left(b_{i}=1\\left|v_ {i}=1\\right.\\right)\\\\ &=\\sum_{c_{i}}p\\left(b_{i}=1\\left|v_{i}=1\\right.,c_{i}\\right)p \\left(c_{i}\\left|v_{i}=1\\right.\\right)\\\\ &=\\sum_{c_{i}}\\sum_{a_{i}}p\\left(b_{i}=1\\left|v_{i}\\right.,c_{i},a_ {i}\\right)p\\left(a_{i}\\left|v_{i}\\right.,c_{i}\\right)p\\left(c_{i}\\left|v_{i} \\right.\\right)\\\\ &=y_{4i}\\left(1-y_{2i}\\right)y_{1i}+y_{3i}y_{2i}y_{1i}\\\\ &=y_{1i}\\left(y_{4i}\\left(1-y_{2i}\\right)+y_{3i}y_{2i}\\right) \\end{split} \\tag{4}\\]\n' +
      '\n' +
      'Here, we use \\(v_{i}\\) to replace \\(v_{i}=1\\) in the third equality for simplicity without causing any ambiguity. It is noteworthy that the fourth equality holds due to the fact that no items will be bought without being clicked, _i.e._, \\(p\\left(b_{i}=1\\left|v_{i}=1\\right.,c_{i}=0,a_{i}\\right)\\) equals to zero, \\(\\forall a_{i}\\in\\{0,1\\}\\). Indeed, Eq. (4) can be derived by decomposing the graph "impression\\(\\rightarrow\\)click\\(\\rightarrow\\)D(O)Action\\(\\rightarrow\\)purchase" into "impression\\(\\rightarrow\\)click" and "click\\(\\rightarrow\\)D(O)Action\\(\\rightarrow\\)purchase", and integrating Eq. (1) and Eq. (3) together according to the chain rule, _i.e._, \\(p_{i}^{etcvr}=p_{i}^{etr}\\ast p_{i}^{cvr}\\).\n' +
      '\n' +
      '### Elaborated entire space supervised multi-task model\n' +
      '\n' +
      'From Eq. (1)\\(\\sim\\) Eq. (4), we can see that \\(p_{i}^{ctr}\\), \\(p_{i}^{etavr}\\), and \\(p_{i}^{ecvr}\\) can be derived from four hidden probability variables \\(y_{1i}\\), \\(y_{2i}\\), \\(y_{3i}\\), and \\(y_{4i}\\), which represents the conditional probabilities over some sub-paths in the graph, _i.e._, "impression\\(\\rightarrow\\)click", "click\\(\\rightarrow\\)DAction", "DAction\\(\\rightarrow\\)purchase" and "OAction\\(\\rightarrow\\)purchase". On the one hand, these four sub-targets are defined over the entire space and can be predicted using all the impression samples. Taking \\(y_{2i}\\) as an example, training \\(y_{2i}\\) directly with only clicked samples suffers from the SSB issue. Indeed, \\(y_{2i}\\) is an intermediate variable derived from\n' +
      '\n' +
      'Figure 3. Illustration of the proposed user sequential behavior graph based on post-click behavior decomposition. (a) The multiple paths from impression to purchase after distinguishing post-click behaviors, such as “impression\\(\\rightarrow\\)click\\(\\rightarrow\\)Scart\\(\\rightarrow\\)purchase”. (b) A digraph is used to describe the simplified purchasing process, where the numbers above edges represent the sparsity of different paths. (c) Several specific purchase-related post-click actions are merged into a single node, _i.e._, DAction, which also inherits their supervisory signals. OAction represents other cases except DAction.\n' +
      '\n' +
      '\\(p_{i}^{ctr}\\) and \\(p_{i}^{etavr}\\) according to Eq. (2). Since both \\(p_{i}^{ctr}\\) and \\(p_{i}^{etavr}\\) are modeled over the entire space with all impression samples, the derived \\(y_{2i}\\) is also applicable over the entire space, therefore, no SSB in our model. On the other hand, ground truth labels of \\(p_{i}^{ctr}\\), \\(p_{i}^{etavr}\\), and \\(p_{i}^{etcvr}\\) are available given users\' logs, which can be used to supervise these sub-targets. Therefore, an intuitive way is to model them simultaneously by employing a multi-task learning framework. To this end, we propose a novel deep neural recommendation model named Elaborated Entire Space Supervised Multi-task Model (\\(ESM^{2}\\)) for CVR prediction. \\(ESM^{2}\\) gets its name since 1) \\(p_{i}^{ctr}\\), \\(p_{i}^{etavr}\\), and \\(p_{i}^{etcvr}\\) are modeled over the entire space and predicted using all the impression samples; 2) the derived \\(p_{i}^{cvr}\\) from Eq. (3) also benefits from the entire space multi-task modeling, which will be validated in the experiment part. \\(ESM^{2}\\) consists of three key modules: 1) a shared embedding module, 2) a decomposed prediction module, and 3) a sequential composition module. We present each of them in detail as follows.\n' +
      '\n' +
      '**Shared Embedding Module (SEM):** First, we devise a shared embedding module to embed all the sparse ID features and dense numerical features coming from user field, item field, and user-item cross field. The user features include users\' ID, ages, genders and purchasing powers, _etc_. The item features include items\' ID, prices, accumulated CTR and CVR from historical logs, _etc_. The user-item features include users\' historical preference scores on items, _etc_. Dense numerical features are first discretized based on their boundary values and then represented as one-hot vectors. Here, we use \\(f_{i}=\\left\\{f_{ij},\\forall j\\in\\Lambda_{f}\\right\\}\\) to denote the one-hot features of the \\(i^{th}\\) training sample, where \\(\\Lambda_{f}\\) denotes the index set of all kinds of features. Due to the sparseness nature of one-hot encoding, we employ linear fully connected layers to embed them into dense representation, which can be formulated as:\n' +
      '\n' +
      '\\[g_{ij}=P_{\\theta_{j}}^{T}f_{ij}, \\tag{5}\\]\n' +
      '\n' +
      'where \\(P_{\\theta_{j}}\\) denotes the embedding matrix for the \\(j^{th}\\) kind of features, \\(\\theta_{j}\\) represents the network parameters.\n' +
      '\n' +
      '**Decomposed Prediction Module (DPM):** Then, once all the feature embeddings are obtained, they are concatenated together, fed into several decomposed prediction modules, and shared by each of them. Each prediction network in DPM estimates the probability of the decomposed target on the path "impression\\(\\rightarrow\\)click", "click\\(\\rightarrow\\)DAction", "DAction\\(\\rightarrow\\)purchase", "OAction\\(\\rightarrow\\)purchase", respectively. In this paper, we employ Multi-Layer Perception (_MLP_) as the prediction network. All the non-linear activation function is _ReLU_ except the output layer, where we use a _Sigmoid_ function to map the output into a probability taking real value from 0 to 1. Mathematically, it can be formulated as:\n' +
      '\n' +
      '\\[y_{ki}=\\sigma\\left(\\varphi_{\\beta_{k}}^{k}\\left(g_{i}\\right)\\right), \\tag{6}\\]\n' +
      '\n' +
      'where \\(\\sigma\\) denotes the _Sigmoid_ function, \\(\\varphi_{\\beta_{k}}^{k}\\) denotes the mapping function learned by the _k_th MLP, \\(\\beta_{k}\\) denotes its network parameters. For example, as shown in the first _MLP_ in Figure 4, it output\n' +
      '\n' +
      'Figure 4: The diagram of \\(ESM^{2}\\) model over the entire space, which consists of three key modules: SEM, DPM and SCM. SEM embeds sparse features into dense representation. DPM predicts the probabilities of decomposed targets. SCM integrates them together sequentially to calculate the final CVR as well as other related auxiliary tasks namely CTR, CTAVR, and CTCVR.\n' +
      '\n' +
      'the estimated probability \\(y_{1}\\), which is indeed the post-view click-through rate.\n' +
      '\n' +
      '**Sequential Composition Module (SCM):** Finally, we devise a sequential composition module to compose the above predicted probabilities sequentially according to Eq. (1) \\(\\sim\\) Eq.(4) to calculate the conversion rate \\(p^{ctr}\\) and some auxiliary targets including the post-view click-through rate \\(p^{ctr}\\), click-through DAction conversation rate \\(p^{ctrvar}\\), and click-through conversion rate \\(p^{ctrcvr}\\), respectively. As shown in the top part of Figure 4, SCM is a parameter-free feed forward neural network which represents the underlying conditional probabilities defined by the purchasing decision digraph in Figure 3.\n' +
      '\n' +
      '**Remarks:** 1) All the tasks share the same embedding, making them be trained with all impression samples, _i.e._, they are modeled over the entire space, resulting in no SSB issue during the inference phase; 2) the lightweight DPM is strictly regularized by the shared embedding module, which makes up the majority of the trainable parameters; and 3) our model suggests an efficient network design, where SEM can run in parallel, leading to low latency when deployed online.\n' +
      '\n' +
      '### Training objective\n' +
      '\n' +
      'We use \\(S=\\{(c_{i},a_{i},b_{i};f_{i})\\}_{i=1}^{N}\\) to denote the training set, where \\(c_{i}\\), \\(a_{i}\\), \\(b_{i}\\), represent the ground truth label whether the \\(i^{th}\\) impression sample is being clicked, taken deterministic actions, and bought. Then, we can define the joint post-view click-through probability of all training samples as follows:\n' +
      '\n' +
      '\\[p^{ctr}=\\prod_{i\\in C_{+}}p_{i}^{ctr}\\prod_{j\\in C_{-}}\\left(1-p_{j}^{ctr} \\right), \\tag{7}\\]\n' +
      '\n' +
      'where \\(C_{+}\\) and \\(C_{-}\\) denote the positive and negative samples in the label space \\(C\\), respectively. After taking negative logarithm on Eq.(7), we obtain the _logloss_ of \\(p^{ctr}\\), which is widely used in recommender systems, _i.e._,\n' +
      '\n' +
      '\\[L_{ctr}=-\\sum_{i\\in C_{+}}logp_{i}^{ctr}-\\sum_{j\\in C_{-}}log\\left(1-p_{j}^{ ctr}\\right). \\tag{8}\\]\n' +
      '\n' +
      'Similarly, we can obtain the loss function of \\(p^{ctrv}\\) and \\(p^{ctrv}\\)as follows:\n' +
      '\n' +
      '\\[L_{ctavr}=-\\sum_{i\\in A_{+}}logp_{i}^{ctavr}-\\sum_{j\\in A_{-}}log\\left(1-p_{j}^ {ctavr}\\right), \\tag{9}\\]\n' +
      '\n' +
      'and\n' +
      '\n' +
      '\\[L_{ctevr}=-\\sum_{i\\in B_{+}}logp_{i}^{ctevr}-\\sum_{j\\in B_{-}}log\\left(1-p_{j}^ {ctevr}\\right). \\tag{10}\\]\n' +
      '\n' +
      'The final training objective to be minimized is defined as:\n' +
      '\n' +
      '\\[L\\left(\\Theta\\right)=w_{ctr}\\times L_{ctr}+w_{ctavr}\\times L_{ctavr}+w_{ctevr} \\times L_{ctevr}, \\tag{11}\\]\n' +
      '\n' +
      'where \\(\\Theta=\\left\\{\\theta_{j},\\forall j\\in\\Lambda_{f}\\right\\}\\cup\\left\\{\\theta_{i},i=1,2,3,4\\right\\}\\) denotes all the network parameters in \\(ESM\\)2. \\(w_{ctr}\\), \\(w_{ctavr}\\), \\(w_{ctevr}\\) are loss weights of \\(L_{ctr}\\), \\(L_{ctavr}\\), \\(L_{ctevr}\\), which are set to 1 in this paper, respectively.\n' +
      '\n' +
      'Footnote 2: To the extent of our knowledge, there are no public datasets suited for this entire space modeling task, we will release our dataset for reproducibility and future research.\n' +
      '\n' +
      '**Remarks:** 1) Adding intermediate losses to supervise the decomposed sub-tasks can efficiently leverage the abundant labeled data from post-click behaviors, making the model less affected by the DS issue; and 2) all the losses are computed from the view of entire space modeling, effectively addressing the SSB issue.\n' +
      '\n' +
      '## 4. Experiments\n' +
      '\n' +
      'To evaluate the effectiveness of the proposed \\(ESM\\)2 model, we conducted extensive experiments on both the offline dataset collected from real-world e-commerce scenarios and online deployment. \\(ESM\\)2 is compared with some representative state-of-the-art (SOTA) methods including GBDT (Chen et al., 2017), DNN (He et al., 2019), DNN using over-sampling idea (Krizhevsky et al., 2014) and ESMM (Krizhevsky et al., 2014). First, we present the evaluation settings including the dataset preparation, evaluation metrics, a brief description of these SOTA methods, and the implementation details. Then, we present the comparison results and analysis. Ablation studies are presented next, followed by the performance analysis on different post-click behaviors.\n' +
      '\n' +
      'Footnote 2: To the extent of our knowledge, there are no public datasets suited for this entire space modeling task, we will release our dataset for reproducibility and future research.\n' +
      '\n' +
      '### Evaluation settings\n' +
      '\n' +
      '#### 4.1.1. Dataset preparation\n' +
      '\n' +
      'We make the offline dataset by collecting the users\' sequential behaviors and feedback logs 1 from our online e-commerce platform, which is one of the largest third-party retail platforms in the world. More than 300 million instances with user/item/user-item features and sequential feedback labels (_e.g._, whether click, or DAction, or purchase) are filtered out. The statistics of this offline dataset are listed in Table 1. They are further divided into the disjoint training set, validation set, and test set.\n' +
      '\n' +
      'Footnote 1: To the extent of our knowledge, there are no public datasets suited for this entire space modeling task, we will release our dataset for reproducibility and future research.\n' +
      '\n' +
      '#### 4.1.2. Evaluation metrics\n' +
      '\n' +
      'To comprehensively evaluate the effectiveness of the proposed model and compare it with SOTA methods, we adopt three widely used metrics in recommendation and advertising system, _i.e._, Area Under Curve (AUC), GAUC (Sutton et al., 2017; Wang et al., 2018) and \\(F_{1}\\) score, where AUC reflecting the ranking ability.\n' +
      '\n' +
      '\\[AUC=\\frac{1}{|S_{+}|}\\sum_{|S_{-}|}\\sum_{x^{\\prime}\\in S_{+}}\\sum_{x^{\\prime} \\in S_{-}}I\\left(\\phi\\left(x^{+}\\right)>\\phi\\left(x^{-}\\right)\\right), \\tag{12}\\]\n' +
      '\n' +
      'where \\(S_{+}\\) and \\(S_{-}\\) denote the set of positive/negative samples, respectively, \\(|S_{+}|\\) and \\(|S_{-}|\\) denote the number of samples in \\(S_{+}\\) and \\(S_{-}\\), \\(\\phi\\left(\\cdot\\right)\\) is the prediction function, \\(I\\left(\\cdot\\right)\\) is the indicator function.\n' +
      '\n' +
      'GAUC (Wang et al., 2018) is calculated as follows. First, all the test data are partitioned into different groups according to the individual user ID. Then, AUC is calculated in every single group. Finally, we average the weighted AUC. Mathematically, GAUC is defined as:\n' +
      '\n' +
      '\\[GAUC=\\frac{\\sum_{u}w_{u}\\times AUC_{u}}{\\sum_{u}w_{u}}, \\tag{13}\\]\n' +
      '\n' +
      'where \\(w_{u}\\) denotes the weight for user \\(u\\) (set as 1 for our offline evaluations). \\(AUC_{u}\\) denotes the AUC for user \\(u\\).\n' +
      '\n' +
      'Moreover, \\(F_{1}\\) score is defined as:\n' +
      '\n' +
      '\\[F_{1}=\\frac{2\\times P\\times R}{P+R}, \\tag{14}\\]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline Category & \\#User & \\#Item & \\#Impression \\\\ \\hline Number & 13,383,415 & 10,399,095 & 326,325,042 \\\\ \\hline \\hline Category & \\#Click & \\#Purchase & \\#DAction \\\\ \\hline Number & 20,637,192 & 226,918 & 2,501,776 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1. Statistics of the offline dataset.\n' +
      '\n' +
      'where \\(P\\) and \\(R\\) denote the precision and recall, _i.e.,_:\n' +
      '\n' +
      '\\[P=\\frac{TP}{TP+FP}, \\tag{15}\\]\n' +
      '\n' +
      '\\[R=\\frac{TP}{TP+FN}, \\tag{16}\\]\n' +
      '\n' +
      'where \\(TP\\), \\(FP\\), and \\(FN\\) denote the number of true positive, false positive, and false negative predictions, respectively.\n' +
      '\n' +
      '#### 4.1.3. Brief description of comparison methods\n' +
      '\n' +
      'The representative state-of-the-art methods are described as follows.\n' +
      '\n' +
      '\\(\\bullet\\)**GBDT**(Chen et al., 2017): The gradient boosting decision tree (GBDT) model follows the idea of gradient boosting machine (GBM), is able to produce competitive, highly robust, and interpretable procedures for regression and classification tasks (Krizhevsky et al., 2015). In this paper, we use it as the representative of non-deep learning-based methods.\n' +
      '\n' +
      '\\(\\bullet\\)**DNN**(Krizhevsky et al., 2015): We also implement a deep neural network baseline model, which has the same structure and hyper-parameters with the single branch in \\(ESM^{2}\\). Different from \\(ESM^{2}\\), it is trained with samples on the path "click\\(\\rightarrow\\)purchase" or "impression\\(\\rightarrow\\)click" to predict conversion rate \\(p^{cvr}\\) or click-through rate \\(p^{ctr}\\), respectively.\n' +
      '\n' +
      '\\(\\bullet\\)**DNN-OS**(Krizhevsky et al., 2015): Due to the data sparsity on the paths "impression\\(\\rightarrow\\)purchase" and "click\\(\\rightarrow\\)purchase", it is hard to train a deep neural network with good generalization. To address this issue, we leverage the _over-sampling_ strategy to augment positive samples during training the deep model, named DNN-OS. It has the same structure and hyper-parameters with the above DNN model.\n' +
      '\n' +
      '\\(\\bullet\\)**ESMM**(Krizhevsky et al., 2015): For a fair comparison, we use the same backbone structure as the above deep models for ESMM. It directly models the conversion rate on the user sequential path "impression\\(\\rightarrow\\)click\\(\\rightarrow\\)purchase" without considering the purchase-related post-click behaviors.\n' +
      '\n' +
      'In a nutshell, the first three methods learn to predict \\(p^{ctr}\\) and \\(p^{ctr}\\) using samples on the path "impression\\(\\rightarrow\\)click" and "click\\(\\rightarrow\\)purchase", respectively, then multiply them together to derive the click-through conversion rate \\(p^{ctctr}\\). As for ESMM and our \\(ESM^{2}\\), they directly predict \\(p^{ctctr}\\) and \\(p^{ctr}\\) by modeling them over the entire space.\n' +
      '\n' +
      '#### 4.1.4. Hyper-parameters settings\n' +
      '\n' +
      'For the GBDT model, the number of trees, depth, minimum instance numbers for splitting a node, the sampling rate of the training set for each iteration, the sampling rate of features for each iteration, and the type of loss function, are set as 150, 8, 20, 0.6, 0.6, and _logistic loss_, respectively, which are chosen according to the AUC score on the validation set. For the deep neural network-based models, they are implemented in TensorFlow using the Adam optimizer. The learning rate is set to 0.0005 and the mini-batch size is set to 1000. Logistic loss is used as the loss function for each prediction task in all models. There are 5 layers in the _MLP_, where the dimension of each layer is set to 512, 256, 128, 32, and 2, respectively, as summarized in Table 2.\n' +
      '\n' +
      '### Main results\n' +
      '\n' +
      '#### 4.2.1. Comparison on the offline dataset\n' +
      '\n' +
      'In this subsection, we report the AUC, GAUC, and \\(F_{1}\\) scores of all the competitors on the offline test set. Table 3 summarizes the results of AUC and GAUC. It can be seen that the DNN method achieves gains of 0.0242, 0.0102, 0.0117 for CVR AUC, CTCVR AUC, and CTCVR GAUC over the baseline GBDT model, respectively. It demonstrates the strong representation ability of deep neural networks. Different from the vanilla DNN, DNN-OS utilizes an over-sampling strategy to address the DS issue, achieving a better performance than DNN. As for ESMM, it is modeled on the path "impression\\(\\rightarrow\\)click\\(\\rightarrow\\)purchase", which tries to address the SSB and DS issues simultaneously. Benefiting from modeling over the entire space and the abundant training samples, it outperforms DNN-OS. Nevertheless, ESMM, neglecting the impact of post-click behaviors while being further exploited by the proposed \\(ESM^{2}\\), still struggles to address the DS issue due to rare purchase training samples. After predicting some decomposed sub-targets in parallel under a multi-task learning framework, \\(ESM^{2}\\) composes them sequentially to formulate the final CVR. As can be seen, it obtains the best scores among all the methods. For example, the gains over ESMM are 0.0088, 0.0101, 0.0145 for CVR AUC, CTCVR AUC, and CTCVR GAUC, respectively. It is worth mentioning that a gain of 0.01 in offline AUC always means a significant increment in revenue for online RS (Krizhevsky et al., 2015; Krizhevsky et al., 2015).\n' +
      '\n' +
      'As for the \\(F_{1}\\) score, we report several values by setting different thresholds for CVR and CTCVR, respectively. First, we sort all the instances in descending order according to the predicted CVR or CTCVR score. Then, due to the sparsity of CVR task (about 1% of the predicted samples are positive), we choose three thresholds namely top@0.1%, top@0.6%, and top@1% to split the predictions into positive and negative groups accordingly. Finally, we calculate the precision, recall, and \\(F_{1}\\) scores of these predictions at these different thresholds. Results are summarized in Table 4 and Table 5. A similar trend to Table 3 can be observed. Again, the proposed method \\(ESM^{2}\\) achieves the best performance in different settings.\n' +
      '\n' +
      '#### 4.2.2. Comparison on online deployment\n' +
      '\n' +
      'It is not an easy job to deploy deep network models in our recommender system since it servers hundreds of millions of users every day, _e.g._, more than 100 million users per second at a traffic peak. Therefore,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c} \\hline \\hline Hyper-parameter & Choice \\\\ \\hline Loss function & Logistic Loss \\\\ Optimizer & Adam \\\\ Number of layers in MLP & 5 \\\\ Dimensions of layers in MLP & [512,256,128,32,2] \\\\ Batch size & 1000 \\\\ Learning rate & 0.0005 \\\\ Dropout ratio & 0.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2. Hyper-parameters of deep neural network-based models including DNN, DNN-OS, ESMM, and \\(ESM^{2}\\).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c} \\hline \\hline Method & CVR AUC & CTCVR AUC & CTCVR GAUC \\\\ \\hline GBDT & 0.7823 & 0.8059 & 0.7747 \\\\ \\hline DNN & 0.8065 & 0.8161 & 0.7864 \\\\ \\hline DNN-OS & 0.8124 & 0.8192 & 0.7893 \\\\ \\hline ESMM & 0.8398 & 0.8270 & 0.7906 \\\\ \\hline \\(ESM^{2}\\) & **0.8486** & **0.8371** & **0.8051** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3. The AUC and GAUC scores of all methods.\n' +
      '\n' +
      'a practical model is required to make real-time CVR prediction with high throughput and low latency. For example, hundreds of recommendation items for each visitor should be predicted in less than 100 milliseconds in our system. Thanks to the parallel network structure, our model is computationally efficient and can respond to each online request within 20 milliseconds. To make the online evaluation fair, confident, and comparable, each deployed method for an A/B test has involved the same number of users, _i.e._, millions of users. The results are listed in Figure 5, where we use the GBDT model as the baseline. As can be seen, DNN, DNN-OS, and ESMM achieve comparable performance and outperform the baseline model significantly, while ESMM performs slightly better. As for the proposed \\(ESM^{2}\\), the significant margins between it and the above methods demonstrate its superiority. Besides, it contributes to a 3% CVR promotion compared with ESMM, indicating a significant business value for the e-commercial platform.\n' +
      '\n' +
      '**Remarks:** 1) The deep neural network has stronger representation ability than the decision tree-based GBDT; 2) the multi-task learning framework over the entire sample space serves as an efficient tool to address the SSB and DS issues; and 3) based on the idea of post-click behaviors decomposition, \\(ESM^{2}\\) efficiently addresses the SSB and DS issues by modeling CVR over the entire space and leveraging abundant supervisory signals from deterministic actions and achieves the best performance.\n' +
      '\n' +
      '### Ablation studies\n' +
      '\n' +
      'In this part, we present the detailed ablation studies including hyper-parameter settings of the deep neural network, effectiveness of embedding dense numerical features, and the choice of decomposing post-click behaviors, respectively.\n' +
      '\n' +
      '#### 4.3.1. Hyper-parameters of deep neural network\n' +
      '\n' +
      'Here, we take three critical parameters, namely _dropout ratio, the number of hidden layers_, and _the dimension of item feature embeddings_ as example to illustrate the process of parameter selection in our \\(ESM^{2}\\) model.\n' +
      '\n' +
      'Dropout (Krizhevsky et al., 2012) refers to the regularization technique by randomly deactivating some neural nodes during training. It can increase the generalization of the deep neural network by introducing randomness. We try different choices of the dropout ratio from 0.2 to 0.7 in our model. As shown in Figure 6(a), a dropout ratio of 0.5 leads to the best performance. Therefore, we set it to 0.5 in all the experiments if not specified.\n' +
      '\n' +
      'Increasing the depth of network layers can enhance the model capacity but also potentially leads to over-fitting. Therefore, we carefully set this hyper-parameter according to the AUC scores on the validation set. As can be seen from Figure 6(b), at the beginning stage, _i.e._, from two layers to five layers, increasing the number of hidden layers consistently improves the model\'s performance. However, it saturates at five layers that increasing more layers even marginally decreases the AUC scores, where the model may overfit the training set. Therefore, we use five hidden layers in all experiments if not specified.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c|c c c} \\hline  & \\multicolumn{3}{c|}{CVR@top0.1\\%} & \\multicolumn{3}{c|}{CVR@top0.6\\%} & \\multicolumn{3}{c}{CVR@top1\\%} \\\\ \\hline Method & Recall & Precision & F1-Score & Recall & Precision & F1-Score & Recall & Precision & F1-Score \\\\ \\hline GBDT & 4.382\\% & 14.348\\% & 6.714\\% & 16.328\\% & 9.894\\% & 12.322\\% & 27.384\\% & 7.384\\% & 11.631\\% \\\\ \\hline DNN & 4.938\\% & 15.117\\% & 7.445\\% & 17.150\\% & 10.495\\% & 13.021\\% & 28.481\\% & 8.196\\% & 12.729\\% \\\\ \\hline DNN-OS & 5.383\\% & 15.837\\% & 8.034\\% & 17.381\\% & 10.839\\% & 13.353\\% & 29.032\\% & 8.423\\% & 13.058\\% \\\\ \\hline ESMM & 5.813\\% & 16.295\\% & 8.570\\% & 18.585\\% & 11.577\\% & 14.267\\% & 29.789\\% & 8.961\\% & 13.777\\% \\\\ \\hline \\(ESM^{2}\\) & **6.117\\%** & **17.145\\%** & **9.017\\%** & **23.492\\%** & **10.574\\%** & **14.584\\%** & **30.032\\%** & **9.034\\%** & **13.890\\%** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4. The Precision, Recall and \\(F_{1}\\) scores of all methods for CVR.\n' +
      '\n' +
      'Figure 5. The results of A/B test for CVR by deploying different models in our recommender system.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c|c c c} \\hline  & \\multicolumn{3}{c|}{CTCVR@top0.1\\%} & \\multicolumn{3}{c|}{CTCVR@top0.6\\%} & \\multicolumn{3}{c}{CTCVR@top1\\%} \\\\ \\hline Method & Recall & Precision & F1-Score & Recall & Precision & F1-Score & Recall & Precision & F1-Score \\\\ \\hline GBDT & 2.937\\% & 0.701\\% & 1.132\\% & 4.870\\% & 0.649\\% & 1.145\\% & 8.894\\% & 0.531\\% & 1.002\\% \\\\ \\hline DNN & 3.168\\% & 0.851\\% & 1.341\\% & 5.269\\% & 0.768\\% & 1.340\\% & 9.461\\% & 0.643\\% & 1.204\\% \\\\ \\hline DNN-OS & 3.382\\% & 0.871\\% & 1.385\\% & 5.369\\% & 0.801\\% & 1.395\\% & 9.863\\% & 0.673\\% & 1.260\\% \\\\ \\hline ESMM & 3.858\\% & 0.915\\% & 1.479\\% & 5.504\\% & 0.828\\% & 1.439\\% & 10.088\\% & 0.691\\% & 1.294\\% \\\\ \\hline \\(ESM^{2}\\) & **4.219\\%** & **1.001\\%** & **1.618\\%** & **5.987\\%** & **0.900\\%** & **1.566\\%** & **10.991\\%** & **0.753\\%** & **1.410\\%** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5. The Precision, Recall and \\(F_{1}\\) scores of all methods for CTCVR.\n' +
      '\n' +
      'The dimension of item feature embeddings is a critical parameter that high-dimension features reserve more information but also contains noise and leads to higher model complexity. We try different settings of the parameter and plot the results in Figure 6(c). As can be seen, increasing the dimension generally improves performance. It finally saturates at 128 while doubling it leads no more gains. Therefore, to make a trade-off between model capacity and complexity, we set the dimension of item feature embeddings to 128 in all the experiments if not specified.\n' +
      '\n' +
      '#### 4.3.2. Effectiveness of embedding dense numerical features\n' +
      '\n' +
      'In our task, there are several numerical features. A common practice is to discretize them into one-hot vectors first and then concatenate them with ID features together, which are then embedded into dense features through a linear projection layer as described in Section 3.3. However, we hypothesize that the one-hot vector representation of numerical features may degrade the precision during discretization. In contrast, we try another solution by normalizing the numerical features and embedding them using the Tanh activation function, _i.e._,\n' +
      '\n' +
      '\\[g_{ij}=\\tanh\\left(\\frac{f_{ij}-\\mu_{fj}}{\\sigma_{f_{j}}}\\right), \\tag{17}\\]\n' +
      '\n' +
      'where \\(\\mu_{fj}\\) and \\(\\sigma_{fj}\\) denotes the mean and standard deviation of the \\(j^{th}\\) kind of features. Then, we concatenate the embedded features with the ID features together as the input of our \\(ESM^{2}\\) model. It achieves a gain of 0.004 AUC over the discretization-based method. Therefore, we use the normalization-based embedding method for dense numerical features in all the experiments if not specified.\n' +
      '\n' +
      '#### 4.3.3. Effectiveness of decomposing post-click behaviors\n' +
      '\n' +
      'When decomposing the post-click behaviors, we can integrate different behaviors into the DAction node, _e.g._, only Scart, only Wish, or both Scart and Wish (SCart and Wish). Here, we evaluate the effectiveness of different choices. The results are summarized in Table 6. As can be seen, the combination of both Scart and Wish achieves the best AUC scores. It is reasonable since there are more purchase-related labeled data than the other two cases to address the DS issue.\n' +
      '\n' +
      '### Performance analysis of user behaviors\n' +
      '\n' +
      'To understand the performance of \\(ESM^{2}\\) and its difference with ESMM, we further partition the test set into four groups according to the number of users\' purchasing behaviors, _i.e._, [0,10], [11,20], [21,50], [50,+]. We report AUC scores of CVR and CTCVR for both methods at each group, and the results are plotted in Figure 7. As can be seen, the CVR AUC(CTCVR AUC) of both methods decreases with the increase of the number of purchasing behaviors. However, we observe that the relative gain of \\(ESM^{2}\\) over ESMM in each group increases, _i.e._, 0.72%, 0.81%, 1.13%, 1.30%. Generally, users having more purchasing behaviors always have more active post-click behaviors such as Scart and Wish. Our \\(ESM^{2}\\) model deals with such post-click behaviors by adding a DAction node supervised by deterministic signals from users\' feedback. Therefore, it has better representation ability on those samples than ESMM and achieves better performances on the users with high-frequency purchasing behaviors.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c} \\hline  & CVR AUC & CTCVR AUC & CTCVR GAUC \\\\ \\hline SCart & 0.8457 & 0.8359 & 0.7996 \\\\ \\hline Wish & 0.8403 & 0.8319 & 0.7962 \\\\ \\hline SCart and Wish & **0.8486** & **0.8371** & **0.8051** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6. The results of choices on post-click behaviors.\n' +
      '\n' +
      'Figure 6. The results of different hyper-parameter settings in \\(ESM^{2}\\).\n' +
      '\n' +
      'Figure 7. The AUC scores of CVR and CTCVR for ESMM and \\(ESM^{2}\\) at different groups regarding the number of purchasing behaviors. Please refer to Section 4.4.\n' +
      '\n' +
      '## 5. Conclusion\n' +
      '\n' +
      'In this paper, we introduce a novel idea of post-click behavior decomposition for modeling CVR task in the context of e-commerce recommender system. A novel user sequential behavior graph "impression\\(\\rightarrow\\)click\\(\\rightarrow\\)D(O)Action\\(\\rightarrow\\)purchase" is constructed, which is used to model CVR over the entire space. Based on the conditional probability rule, we disentangle CVR and some related auxiliary tasks including the post-view click-through rate, click-through DAction conversation rate, and click-through conversion rate into four hidden probability variables, which are defined on explicit sub-paths of the graph. Consequently, we propose a novel deep neural recommendation model named \\(ESM^{2}\\) by employing a multi-task learning framework to predict CVR as well as related auxiliary tasks simultaneously. By training with all impression samples and leveraging the abundant labels of deterministic post-click behaviors, our \\(ESM^{2}\\) model efficiently addresses the SSB and DS issues. Extensive experiments on both offline and online environments demonstrate the superiority of \\(ESM^{2}\\) over state-of-the-art models.\n' +
      '\n' +
      '## Acknowledgment\n' +
      '\n' +
      'This work was partly supported by the National Natural Science Foundation of China (NSFC) under Grant 61806062.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* (1)\n' +
      '* Chen et al. (2019) Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Behavior Sequence Transformer for E-commerce Recommendation in Alibaba. _arXiv preprint arXiv:1905.06874_ (2019).\n' +
      '* Dupret and Piwowarski (2008) Georges E Dupret and Benjamin Piwowarski. 2008. A user browsing model to predict search engine click data from past observations.. In _Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval_. ACM, 331-338.\n' +
      '* Effendi and Ali (2017) Muhammad Junaid Effendi and Syed Abbas Ali. 2017. Click through rate prediction for contextual advertisement using linear regression. _arXiv preprint arXiv:1701.08744_ (2017).\n' +
      '* Feng et al. (2019) Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, and Keping Yang. 2019. Deep Session Interest Network for Click-Through Rate Prediction. _arXiv preprint arXiv:1905.06482_ (2019).\n' +
      '* Friedman (2001) Jeromeo F Friedman. 2001. Greedy function approximation: a gradient boosting machine. _Annals of statistics_ (2001), 1189-1232.\n' +
      '* Gao et al. (2019) Chen Gao, Xiangnan He, Lushan Gan, Xiangming Chen, Fuli Feng, Yong Li, Tat-Seng Chua, and Dempeng Jin. 2019. Neural Multi-Task Recommendation from Multi-Behavior Data. In _2019 IEEE 35th International Conference on Data Engineering (ICDE)_. IEEE, 1554-1557.\n' +
      '* Golbeck et al. (2006) Jennifer Golbeck, James Hendler, et al. 2006. Flumtrest: Movie recommendations using trust in web-based social networks. In _Proceedings of the IEEE Consumer communications and networking conference_, Vol. 96. Citeseer, 282-286.\n' +
      '* Graepel et al. (2010) Thorne Graepel, Joquinquino Candela, Thomas Borchert, and Ralf Herbrich. 2010. Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoft\'s bing search engine. Omnipress.\n' +
      '* Graves et al. (2013) Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013. Speech recognition with deep recurrent neural networks. In _2013 IEEE international conference on acoustics, speech and signal processing_. IEEE, 6645-6649.\n' +
      '* Guo et al. (2017) Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization machine based neural network for CTR prediction. _arXiv preprint arXiv:1703.04247_ (2017).\n' +
      '* Hadah et al. (2018) Guy Hadah, Oren Ser Shalom, and Rita Osadley. 2018. Rank and rate: multi-task learning for recommender systems. In _Proceedings of the 12th ACM Conference on Recommender Systems_. ACM, 451-454.\n' +
      '* Hanfeijn Pan et al. (2014) Xinran He, Junfeng Pan, Qu Jin, Tianbing Xu, Bo Li, Tao Xu, Yanxin Shi, Antoine Allahah, Ralf Herbrich, Stuart Bowers, et al. 2014. Practical lessons from predicting clicks on ads at facebook. In _Proceedings of the Eighth International Workshop on Data Mining for Online Advertising_. ACM, 1-9.\n' +
      '* Hinton et al. (2006) Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. 2006. A fast learning algorithm for deep belief nets. _Neural computation_ 18, 7 (2006), 1527-1554.\n' +
      '* Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural networks. In _Advances in neural information processing systems_. 1097-1105.\n' +
      '* Lee et al. (2012) Kuang-chih Lee, Burkay Orten, Ali Dasdan, and Wentong Li. 2012. Estimating conversion rate in display advertising from past performance data. In _Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining_. ACM, 768-776.\n' +
      '* Lin et al. (2017) Quan Lin, Shengpan Pan, Liang Wang, Junwei Pan, Fengdan Wan, and Hongxia Yang. 2017. A practical framework of conversion rate prediction for online display advertising. In _Proceedings of the ADKDD\'17_. 1-9.\n' +
      '* Lv et al. (2019) Fuyu Lv, Taiwei Jin, Changlong Yu, Fei Sun, Quan Lin, Keping Yang, and Wilfred Ng. 2019. SDM: Sequential deep matching model for online large-scale recommender system. In _Proceedings of the 28th ACM International Conference on Information and Knowledge Management_. 2635-2643.\n' +
      '* Ma et al. (2018) Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jili Chen, Lichan Hong, and Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-of-experts. In _Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_. ACM, 1930-1939.\n' +
      '* Ma et al. (2018) Xiao Ma, Lijin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018. Ultra space multi-task model: An effective approach for estimating post-click conversion rate. In _The 43th International ACM SIGIR Conference on Research & Development in Information Retrieval_. ACM, 1137-1140.\n' +
      '* Navinduramers et al. (2011) Jeff Narinduramers, Mehmet Hadi Gunes, and subsibul J Louis. 2011. Friend recommendations in social networks using genetic algorithms and network topology. In _2011 IEEE Congress of Evolutionary Computation (CEC)_. 2207-2214.\n' +
      '* Wan et al. (2018) Yabe Ni, Dan Ou, Shichei Liu, Xiang Li, Wenwu Ou, Anxiang Zeng, and Luo Si. 2018. Precise your users in depth: Learning universal user representations from multiple e-commerce tasks. In _Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_. ACM, 506-605.\n' +
      '* Pan et al. (2008) Rong Pan, Yunhong Zhou, Bin Cao, Nathan N Lin, Rajan Lakose, Martin Scholz, and Qiang Yang. 2008. One-class collaborative filtering. In _2008 Eighth IEEE International Conference on Data Mining_. IEEE, 502-511.\n' +
      '* Qu et al. (2016) Yanruru Qu, Han Cai, Kan Ren, Weihang Zhang, Tong, Ying Wen, and Jun Wang. 2016. Product-based neural networks for user response prediction. In _2016 IEEE 16th International Conference on Data Mining (ICDD)_. IEEE, 1149-1154.\n' +
      '* Shen et al. (2014) Yejong Shen, Xiaodong Li, Jianfeng Gao, Li Deng, and Greg Greggirensell. 2014. A latent semantic model with convolutional-pooling structure for information retrieval. In _Proceedings of the 25th ACM international conference on conference on information and knowledge management_. ACM, 101-110.\n' +
      '* Srivastava et al. (2014) Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. _The Journal of machine learning research_ 15, 1 (2014), 1929-1958.\n' +
      '* Sun et al. (2019) Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4ke: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. _arXiv preprint arXiv:1904.06690_ (2019).\n' +
      '* Weiss (2004) Gary M Weiss. 2004. Mining with rarity: a unifying framework. _ACM Sigkdd Explorations Newsletter 6_, 1 (2004), 7-19.\n' +
      '* Wen et al. (2019) Hong Wen, Jing Zhang, Quan Lin, Keping Yang, and Pipei Huang. 2019. Multi-Level Deep Cascade Trees for Conversion Rate Prediction in Recommendation System. In _Proceedings of the AAAI Conference on Artificial Intelligence_.\n' +
      '* Xiao et al. (2017) Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua. 2017. Attentional factorization machines: Learning the weight of feature interactions via attention networks. _arXiv preprint arXiv:1708.04641_ (2017).\n' +
      '* Yang et al. (2016) Hongxia Yang, Quan Lu, Angus Xianen Qiu, and Chun Han. 2016. Large scale cvr prediction through dynamic transfer learning of global and local features. In _Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications_. 103-119.\n' +
      '* Zadroznyny (2004) Bianca Zadroznyny. 2004. Learning and evaluating classifiers under sample selection bias. In _Proceedings of the international conference on Machine learning_.\n' +
      '* Zhang et al. (2016) Weinan Zhang, Tianxiong Zhou, Jun Wang, and Jan Xu. 2016. Bid-aware gradient descent for unbiased learning with censored data in display advertising. In _Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining_. ACM, 665-674.\n' +
      '* Zhang et al. (2014) Yuyu Zhang, Hanjun Dai, Chang Xu, Jun Feng, Taifeng Wang, Jiang Bian, Wang, and Tie-Yan Liu. 2014. Sequential click prediction for sponsored search with recurrent neural networks. In _AAAI Conference on Artificial Intelligence_.\n' +
      '* Zhou et al. (2019) Guorui Zhou, Na Mon, Ying Fan Qi, Weiqing Bian, Chang Zhao, Xiaoqiang Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate prediction. In _Proceedings of the AAAI Conference on Artificial Intelligence_.\n' +
      '* Zhou et al. (2018) Guorui Zhou, Xiaoqiang Zhu, Chenux Song, Ting Fan, Han Zhu, Xiao Ma, Yanjun Yan, Junqi Jin, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In _Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_. ACM, 1059-1068.\n' +
      '* Zhu et al. (2017) Han Zhu, Junqi Jin, Chang Tan, Fei Pan, Min Zeng, Han Li, and Kun Gai. 2017. Optimized cost per click in hook display advertising. In _Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_.\n' +
      '* Zhu et al. (2018) Han Zhu, Xiang Li, Pengeye Zhang, Guozheng Li, Jie He, Han Li, and Kun Gai. 2018. Learning Tree-based Deep Model for Recommender Systems. In _Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_. ACM, 1079-1088.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>