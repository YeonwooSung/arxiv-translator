<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Retrieval-Augmented Generation for\n' +
      '\n' +
      '지식 집중형 NLP 태스크\n' +
      '\n' +
      ' Patrick Lewis\\({}^{\\dagger\\dagger}\\), Ethan Perez\\({}^{\\star}\\),\n' +
      '\n' +
      'Aleksandra Piktus\\({}^{\\dagger}\\), Fabio Petroni\\({}^{\\dagger}\\), Vladimir Karpukhin\\({}^{\\dagger}\\), Naman Goyal\\({}^{\\dagger}\\), Heinrich Kuttler\\({}^{\\dagger}\\)\n' +
      '\n' +
      'Mike Lewis\\({}^{\\dagger\\), Wen-tau Yih\\({}^{\\dagger\\), Tim Rocktaschel\\({}^{\\dagger\\ddagger\\), Sebastian Riedel\\({}^{\\dagger\\ddagger\\), Douwe Kiela\\({}^{\\dagger\\\\)\n' +
      '\n' +
      '\\({}^{\\dagger}\\)Facebook AI Research; \\({}^{\\ddagger}\\)University College London; \\({}^{\\star}\\) New York University;\n' +
      '\n' +
      'plewis@fb.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '사전 훈련된 대규모 언어 모델은 파라미터에 사실적 지식을 저장하고 다운스트림 NLP 태스크에서 미세 조정될 때 최신 결과를 달성하는 것으로 나타났다. 그러나 지식에 접근하고 정확하게 조작하는 능력은 여전히 제한되어 있으므로 지식 집약적 작업에서 성능은 작업별 아키텍처에 뒤처진다. 또한 결정에 대한 출처를 제공하고 세계 지식을 업데이트하는 것은 여전히 열린 연구 문제로 남아 있다. 명시적 비모수 메모리에 대한 미분 가능한 액세스 메커니즘을 가진 사전 훈련된 모델은 지금까지 추출 다운스트림 작업에 대해서만 조사되었다. 본 논문에서는 언어 생성을 위해 사전 학습된 모수적 메모리와 비모수적 메모리를 결합한 모델인 검색 증강 생성(RAG)을 위한 범용 미세 조정 방법을 탐색한다. 파라메트릭 메모리는 사전 훈련된 seq2seq 모델이고 비파라메트릭 메모리는 사전 훈련된 신경 검색기로 액세스되는 위키피디아의 조밀한 벡터 인덱스인 RAG 모델을 소개한다. 우리는 생성된 전체 서열에 걸쳐 동일한 검색된 계대에 조건을 부여하는 두 가지 RAG 공식과 토큰당 다른 계대를 사용할 수 있는 두 가지 RAG 공식을 비교한다. 다양한 지식 집약적 NLP 태스크에서 모델을 미세 조정하고 평가하고 3개의 오픈 도메인 QA 태스크에서 최신 기술을 설정하여 파라메트릭 seq2seq 모델 및 태스크별 검색 및 추출 아키텍처를 능가한다. 언어 생성 작업의 경우 RAG 모델이 최신 매개변수 전용 seq2seq 기준선보다 더 구체적이고 다양하며 사실적인 언어를 생성한다는 것을 발견했다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '사전 훈련된 신경 언어 모델은 데이터로부터 상당한 양의 심층 지식을 학습하는 것으로 나타났다[47]. 그들은 파라미터화된 암시적 지식 베이스로서, 외부 메모리에 대한 어떠한 액세스도 없이 그렇게 할 수 있다[51;52]. 이 개발은 흥미진진하지만 그러한 모델에는 단점이 있다: 그들은 기억을 쉽게 확장하거나 수정할 수 없고 예측에 대한 통찰력을 직접적으로 제공할 수 없으며 "환각"을 생성할 수 있다[38]. 파라메트릭 메모리와 비파라메트릭(즉, 검색 기반) 메모리를 결합하는 하이브리드 모델[20; 26; 48]은 지식이 직접 수정되고 확장될 수 있고 액세스된 지식이 검사되고 해석될 수 있기 때문에 이러한 문제들 중 일부를 해결할 수 있다. REALM[20]과 ORQA[31]는 마스킹 언어 모델[8]과 미분 가능한 검색기를 결합한 최근에 도입된 두 가지 모델로 유망한 결과를 보여주었지만 오픈 도메인 추출 질문 응답만 탐구했다. 여기서 우리는 하이브리드 모수 및 비모수 메모리를 "NLP의 작업말", 즉 서열 대 서열(seq2seq) 모델에 가져온다.\n' +
      '\n' +
      '우리는 사전 훈련된 파라메트릭-메모리 생성 모델을 비파라메트릭 메모리와 함께 검색 증강 생성(RAG)이라고 하는 범용 미세 조정 접근법을 통해 부여한다. 파라메트릭 메모리는 미리 훈련된 seq2seq 트랜스포머이고 비파라메트릭 메모리는 미리 훈련된 신경 검색기로 액세스되는 위키피디아의 조밀한 벡터 인덱스인 RAG 모델을 구축한다. 우리는 이러한 컴포넌트들을 엔드 투 엔드 트레이닝된 확률 모델에서 결합한다(도 1). 리트리버(Dense Passage Retriever[26], 이후 DPR)는 입력에 조건화된 잠재 문서를 제공하고, seq2seq 모델(BART[32])은 이러한 잠재 문서에 대한 조건을 입력과 함께 제공하여 출력을 생성한다. 우리는 출력 단위(동일한 문서가 모든 토큰에 대해 책임이 있다고 가정) 또는 토큰 단위(서로 다른 문서가 서로 다른 토큰에 대해 책임이 있는 경우)에서 상위 K 근사치로 잠재 문서를 주변화한다. T5[51] 또는 BART와 같이, RAG는 임의의 seq2seq 태스크 상에서 미세 조정될 수 있고, 이에 의해 발전기 및 리트리버 모두가 공동으로 학습된다.\n' +
      '\n' +
      '메모리 네트워크[64; 55], 스택-증강 네트워크[25] 및 메모리 층[30]과 같은 특정 작업에 대해 처음부터 훈련된 비-파라메트릭 메모리를 갖는 시스템을 풍부하게 하기 위한 아키텍처를 제안하는 광범위한 이전 작업이 있었다. 대조적으로, 우리는 모수 및 비모수 메모리 구성 요소가 모두 사전 훈련되고 광범위한 지식으로 사전 로드되는 설정을 탐구한다. 결정적으로 사전 훈련된 접근 메커니즘을 사용함으로써 추가적인 훈련 없이 지식에 접근할 수 있는 능력이 존재한다.\n' +
      '\n' +
      '우리의 결과는 인간이 외부 지식 소스에 대한 액세스 없이 수행할 것으로 합리적으로 예상할 수 없는 _지식 집약적 작업_-작업에 대해 모수 및 비모수 메모리를 생성과 결합하는 이점을 강조한다. 우리의 RAG 모델은 개방형 자연 질문[29], WebQuestions[3] 및 CuratedTrec[2]에서 최신 결과를 달성하고 트리비아QA[24]에서 전문화된 사전 훈련 목표를 사용하는 최근 접근법을 강력하게 능가한다. 이러한 추출 작업들에도 불구하고, 우리는 제약 없는 세대가 이전의 추출 접근법들보다 더 우수함을 발견한다. 지식 집약적 생성을 위해 MS-MARCO [1] 및 Jeopardy 질문 생성을 실험하고 모델이 BART 기준보다 더 사실적이고 구체적이며 다양한 응답을 생성한다는 것을 발견했다. FEVER [56] 사실 검증을 위해 강력한 검색 감독을 사용하는 최신 파이프라인 모델의 4.3% 이내에서 결과를 얻었다. 마지막으로, 전 세계가 변화함에 따라 모델의 지식을 업데이트하기 위해 비모수 메모리를 대체할 수 있음을 보여준다.\n' +
      '\n' +
      '각주 1: RAG를 사용 하 여 실험을 실행 하는 코드는 HuggingFace Transformers 라이브러리 [66]의 일부로 오픈 소스 되었으며 [https://github.com/huggingface/transformers/blob/master/examples/rag/](https://github.com/huggingface/transformers/blob/master/examples/rag/)에서 찾을 수 있습니다. RAG 모델의 대화형 데모는 [https://huggingface.co/rag/](https://huggingface.co/rag/)에서 찾을 수 있습니다.\n' +
      '\n' +
      '## 2 Methods\n' +
      '\n' +
      '우리는 입력 시퀀스 \\(x\\)를 사용하여 텍스트 문서를 검색하고 대상 시퀀스 \\(y\\)를 생성할 때 추가 컨텍스트로 사용하는 RAG 모델을 탐색한다. 그림 1과 같이, 우리의 모델들은 (i) 검색기 \\(p_{\\eta}(z|x)\\)와 파라미터 \\(\\eta\\)를 사용하여 질의 \\(x\\)과 (ii) 생성기 \\(p_{\\theta}(y_{i}|x,z,y_{1:i-1})\\) 파라메트릭라이징된 텍스트 패시지에 대한 (top-K truncated) 분포를 반환하는 (\\eta\\) 두 가지 구성 요소를 활용한다.\n' +
      '\n' +
      '도 1: 본 접근법의 개요. 사전 학습된 리트리버(_Query Encoder + Document Index_)와 사전 학습된 seq2seq 모델(_Generator_)을 결합하여 end-to-end를 미세 조정한다. 질의 \\(x\\)의 경우 최대 내부 제품 검색(MIPS)을 사용하여 상위 K개의 문서 \\(z_{i}\\)을 찾는다. 최종 예측 \\(y\\)의 경우 \\(z\\)을 잠재 변수로 취급하고 서로 다른 문서가 주어졌을 때 seq2seq 예측보다 주변화한다.\n' +
      '\n' +
      '이전 토큰 \\(i-1\\) 토큰 \\(y_{1:i-1}\\), 원래 입력 \\(x\\) 및 검색된 통로 \\(z\\)의 컨텍스트를 기반으로 현재 토큰을 생성하는 \\(\\theta\\)을 사용한다.\n' +
      '\n' +
      '검색기와 생성기의 종단 간 학습을 위해 검색된 문서를 잠재 변수로 취급한다. 우리는 생성된 텍스트에 대한 분포를 생성하기 위해 다른 방식으로 잠재 문서를 주변화하는 두 가지 모델을 제안한다. 하나의 접근 방식인 _RAG-Sequence_에서 모델은 동일한 문서를 사용하여 각 대상 토큰을 예측합니다. 두 번째 접근 방식인 _RAG-Token_은 다른 문서를 기반으로 각 대상 토큰을 예측할 수 있습니다. 다음에서는 두 모델을 형식적으로 소개하고 \\(p_{\\eta}\\) 및 \\(p_{\\theta}\\) 구성요소와 훈련 및 디코딩 절차를 설명한다.\n' +
      '\n' +
      '### Models\n' +
      '\n' +
      'RAG-Sequence 모델 RAG-Sequence 모델은 동일한 검색된 문서를 사용하여 전체 _sequence_를 생성합니다. 기술적으로 검색된 문서를 상위 K 근사를 통해 seq2seq 확률 \\(p(y|x)\\)을 얻기 위해 주변화된 단일 잠재 변수로 취급한다. 구체적으로, 검색기를 이용하여 상위 K개의 문서를 검색하고, 생성기는 각 문서에 대한 출력 시퀀스 확률을 생성한 후, 이를 주변화하며,\n' +
      '\n' +
      '\\[p_{\\text{RAG-Sequence}}(y|x)\\underset{z\\in\\text{top-}k(p(\\cdot|x))}{\\approx} p_{\\eta}(z|x)p_{\\theta}(y|x,z)\\underset{z\\in\\text{top-}k(p(\\cdot|x))}{=} \\underset{z\\in\\text{top-}k(p(\\cdot|x))}{\\approx} p_{\\eta}(z|x)\\prod_{i}^{N}p_{ \\theta}(y_{i}|x,z,y_{1:i-1})\\]\n' +
      '\n' +
      'RAG-토큰 모델 RAG-토큰 모델에서 각 대상 _토큰_에 대해 다른 잠재 문서를 그릴 수 있으며 그에 따라 주변화할 수 있습니다. 이렇게 하면 생성자는 답변을 작성할 때 여러 문서에서 내용을 선택할 수 있습니다. 구체적으로는, 상위 K개의 문서를 리트리버를 이용하여 검색한 후, 생성기는 각 문서에 대해 다음 출력 토큰에 대한 분배를 생성한 후, 주변화하고, 다음 출력 토큰으로 프로세스를 반복하며, 다음 출력 토큰에 대한 분배를 형식적으로 정의한다:\n' +
      '\n' +
      '\\[p_{\\text{RAG-Token}}(y|x)\\ \\approx\\ \\prod_{i}^{N}\\ \\sum_{z\\in\\text{top-}k(p(\\cdot|x))}p_{ \\eta}(z|x)p_{\\theta}(y_{i}|x,z,y_{1:i-1})\\]\n' +
      '\n' +
      '마지막으로, RAG는 타겟 클래스를 길이 1의 타겟 시퀀스로 간주하여 시퀀스 분류 작업에 사용될 수 있으며, 이 경우 RAG-Sequence와 RAG-Token은 동등하다.\n' +
      '\n' +
      '### Retriever: DPR\n' +
      '\n' +
      '검색 컴포넌트 \\(p_{\\eta}(z|x)\\)는 DPR[26]에 기초한다. DPR은 바이 인코더 아키텍처를 따른다:\n' +
      '\n' +
      '\\[p_{\\eta}(z|x)\\propto\\exp\\left(\\mathbf{d}(z)^{\\top}\\mathbf{q}(x)\\right)\\ \\ \\ \\ \\ \\\\mathbf{d}(z)=\\text{BERT}_{d}(z),\\ \\\\mathbf{q}(x)=\\text{BERT}_{q}(x)\\]\n' +
      '\n' +
      '여기서, \\(\\mathbf{d}(z)\\)는 _BERTBASE_문서 인코더_[8]에 의해 생성된 문서의 조밀한 표현이고, \\(\\mathbf{q}(x)\\)는 _쿼리 인코더_에 의해 생성된 쿼리 표현이며, 또한 BERTBASE에 기초한다. Top-k\\((p_{\\eta}(\\cdot|x))\\), \\(k\\) 문서 목록 \\(z\\)의 사전확률이 가장 높은 \\(p_{\\eta}(z|x)\\)을 계산하는 것은 MIPS(Maximum Inner Product Search) 문제로, 하위 선형 시간[23]에서 근사적으로 해결할 수 있다. DPR에서 미리 훈련된 바이인코더를 사용하여 검색기를 초기화하고 문서 인덱스를 구축합니다. 이 검색기는 트리비아QA[24] 질문 및 자연 질문에 대한 답변을 포함하는 문서를 검색하도록 훈련되었다[29]. 문서 인덱스를 _비모수 메모리_ 라고 합니다.\n' +
      '\n' +
      '### Generator: BART\n' +
      '\n' +
      '발전기 성분 \\(p_{\\theta}(y_{i}|x,z,y_{1:i-1})\\)은 모든 인코더-디코더를 사용하여 모델링할 수 있다. 우리는 400M 파라미터를 갖는 사전 훈련된 seq2seq 변압기[58]인 BART-large[32]를 사용한다. BART에서 생성할 때 입력 \\(x\\)과 검색된 콘텐츠 \\(z\\)를 결합하려면 단순히 연결하기만 하면 됩니다. BART는 노이즈 제거 목표와 다양한 노이즈 제거 기능을 사용하여 사전 훈련되었다. 다양한 생성 태스크 집합에서 최신 결과를 얻었으며 비교 가능한 크기의 T5 모델[32]보다 성능이 우수하다. 우리는 앞으로 BART 생성기 매개변수 \\(\\theta\\)를 _파라메트릭 메모리_라고 부른다.\n' +
      '\n' +
      '### Training\n' +
      '\n' +
      '어떤 문서를 검색해야 하는지에 대한 직접적인 감독 없이 검색기와 발전기 구성 요소를 공동으로 교육합니다. 입력/출력 쌍의 미세 조정 훈련 말뭉치 \\((x_{j},y_{j})\\)을 주어, Adam [28]과 확률적 경사하강을 이용하여 각 표적의 음의 한계 로그우도 \\(\\sum_{j}-\\log p(y_{j}|x_{j})\\를 최소화한다. 학습과정에서 문서인코더 \\(\\text{BERT}_{d}\\)를 갱신하는 것은 사전학습과정에서 REALM과 같이 문서인덱스를 주기적으로 갱신해야 하므로 비용이 많이 든다. 이 단계는 강력한 성능을 위해 필요하지 않으며 문서 인코더(및 인덱스)를 고정하고 질의 인코더 \\(\\text{BERT}_{q}\\)와 BART 생성기를 미세 조정하기만 하면 된다.\n' +
      '\n' +
      '### Decoding\n' +
      '\n' +
      '테스트 시간에 RAG-Sequence와 RAG-Token은 \\(\\operatorname*{arg\\,max}_{y}p(y|x)\\)를 근사하는 다른 방법이 필요합니다.\n' +
      '\n' +
      'RAG-Token RAG-Token 모델은 전이확률 \\(p^{\\prime}_{\\theta}(y_{i}|x,y_{1:i-1})=\\sum_{z\\in\\text{top-}k(p(\\cdot|x))}p_{ \\theta}(z_{i}|x)p_{\\theta}(y_{i}|x,z_{i},y_{1:i-1})\\)을 갖는 표준 자기회귀 seq2seq 생성기로 볼 수 있다. \\(p^{\\prime}_{\\theta}(y_{i}|x,y_{1:i-1})\\)을 표준 빔 디코더에 삽입할 수 있다.\n' +
      '\n' +
      'RAG-Sequence RAG-Sequence의 경우, likelihood \\(p(y|x)\\)는 기존의 토큰 당 likelihood로 분할되지 않으므로 단일 빔 탐색으로는 해결할 수 없다. 대신 각 문서 \\(z\\)에 대한 빔 검색을 실행하고 \\(p_{\\theta}(y_{i}|x,z,y_{1:i-1})\\)을 사용하여 각 가설을 채점한다. 이것은 일련의 가설 \\(Y\\)을 산출하며, 그 중 일부는 모든 문서의 빔에 나타나지 않았을 수 있다. 가설 \\(y\\)의 확률을 추정하기 위해 각 문서 \\(z\\)에 대해 \\(y\\)이 빔에 나타나지 않는 추가 전진 패스를 실행하고, 발생기 확률을 \\(p_{\\eta}(z|x)\\)과 곱한 다음, 보에 대한 확률을 합산한다. 우리는 이 디코딩 절차를 "Through Decoding"이라고 지칭한다. 더 긴 출력 시퀀스의 경우, \\(|Y|\\)이 커질 수 있고, 많은 순방향 패스가 필요하다. 보다 효율적인 복호를 위해, \\(x,z_{i}\\)에서 빔 탐색 동안 \\(y\\)이 생성되지 않은 \\(p_{\\theta}(y|x,z_{i})\\approx 0\\)을 더 근사화할 수 있다. 이렇게 하면 후보 집합 \\(Y\\)이 생성되면 추가 전진 패스를 실행할 필요가 없습니다. 이러한 디코딩 절차를 "Fast Decoding"이라고 한다.\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      '우리는 광범위한 지식 집약적 작업에서 RAG를 실험한다. 모든 실험에 대해 비모수 지식 소스에 단일 위키피디아 덤프를 사용한다. Lee et al. [31], Karpukhin et al. [26]에 이어 2018년 12월 덤프를 사용한다. 각 위키피디아 기사는 100단어 덩어리로 나뉘어 총 21M개의 문서를 만든다. 문서 인코더를 사용하여 각 문서에 대한 임베딩을 계산하고, 고속 검색을 위해 계층적 Navigable Small World 근사를 사용하는 FAISS[23]를 사용하여 단일 MIPS 인덱스를 구축한다[37]. 학습 과정에서 각 질의에 대한 상위 \\(k\\) 문서를 검색한다. 훈련은 \\(k\\in\\{5,10\\}\\)을 고려하였고, 테스트 시간은 dev 데이터를 이용하여 \\(k\\)을 설정하였다. 우리는 이제 각 작업에 대한 실험 세부 사항에 대해 논의한다.\n' +
      '\n' +
      '### 오픈 도메인 질문 응답\n' +
      '\n' +
      '개방형 질문 응답(QA)은 지식 집약적 태스크를 위한 중요한 실세계 애플리케이션 및 공통 테스트베드이다[20]. 우리는 질문과 답변을 입력-출력 텍스트 쌍 \\((x,y)\\)으로 취급하고 답변의 음의 로그 우도를 직접 최소화하여 RAG를 훈련시킨다. 우리는 RAG를 비모수적 지식에 주로 의존하여 검색된 문서에서 답변이 추출되는 인기 있는 추출 QA 패러다임[5; 7; 31; 26]과 비교한다. 우리는 또한 RAG와 같이 답을 생성하지만 검색을 이용하지 않는 "닫힌 책 QA" 접근법[52]과 비교하여 순수하게 파라메트릭 지식에 의존한다. 우리는 네 가지 인기 있는 오픈 도메인 QA 데이터 세트인 자연 질문(NO)[29], 트리비아QA (TQA)[24]를 고려한다. WebQuestions(WQ)[3] 및 CuratedTree(CT)[2]. CT와 WQ가 작기 때문에 NQ RAG 모델로 CT와 WQ 모델을 초기화하여 DPR[26]을 따른다. 이전 작업 [31; 26]과 동일한 열차/장치/테스트 분할을 사용하고 정확한 일치(EM) 점수를 보고합니다. TQA의 경우 T5 [52]와 비교하기 위해 TQA 위키 테스트 세트에서도 평가한다.\n' +
      '\n' +
      '### Abstractive Question Answering\n' +
      '\n' +
      'RAG 모델은 단순한 추출 QA를 넘어 자유 형식의 추상적 텍스트 생성으로 질문에 답할 수 있다. 지식 집약적 환경에서 RAG의 자연어 생성(NLG)을 테스트하기 위해 MSMARCO NLG 태스크 v2.1 [43]을 사용한다. 태스크는 질문, 각 질문에 대한 검색 엔진에서 검색된 10개의 골드 구절, 검색된 구절에서 주석이 달린 전체 문장 답변으로 구성된다. 우리는 MSMARCO를 개방형 추상적 QA 과제로 취급하기 위해 제공된 구절, 질문과 답변만을 사용하지 않는다. MSMARCO는 \'CA 화산의 날씨는 어떤가\'와 같이 골드 패시지 접근 없이 기준 답변과 일치하는 방식으로 답변할 수 없는 몇 가지 질문이 있어 골드 패시지를 사용하지 않으면 성능이 낮아진다. 또한 일부 MSMARCO 질문은 위키피디아만 사용하여 대답할 수 없다는 점에 주목한다. 여기서, RAG는 합리적인 응답을 생성하기 위해 파라메트릭 지식에 의존할 수 있다.\n' +
      '\n' +
      '### Jeopardy Question Generation\n' +
      '\n' +
      '비QA 환경에서 RAG의 생성 능력을 평가하기 위해 개방형 질문 생성을 연구한다. 일반적으로 짧고 간단한 질문으로 구성된 표준 개방형 QA 태스크의 질문을 사용하는 대신 제퍼디 질문을 생성하는 더 까다로운 태스크를 제안한다. 제퍼디는 그 실체에 대한 사실로부터 실체를 추측하려는 것으로 구성된 특이한 형식이다. 예를 들어, "월드컵"은 "1986년 멕시코가 이 국제 스포츠 대회를 두 번 개최한 최초의 국가로 득점했습니다."라는 질문에 대한 답변이다. 제퍼디 질문은 정확하고 사실적인 진술이므로, 그들의 답변 엔티티에 조건화된 제퍼디 질문을 생성하는 것은 도전적인 지식 집약적 생성 작업을 구성한다.\n' +
      '\n' +
      '우리는 SearchQA[10]의 분할을 사용하여 100K 트레인, 14K 디브 및 27K 테스트 예제를 사용한다. 이것은 새로운 작업이기 때문에 비교를 위해 BART 모델을 훈련시킵니다. [67]에 이어서, 우리는 SQuAD-튜닝된 Q-BLEU-1 메트릭을 사용하여 평가한다[42]. Q-BLEU는 매칭 엔티티에 대한 가중치가 더 높은 BLEU의 변형이며, 표준 메트릭보다 질문 생성에 대한 인간의 판단과 더 높은 상관 관계를 갖는다. 우리는 또한 두 가지 인간 평가를 수행하는데, 하나는 세대 사실성을 평가하는 것이고 하나는 특수성을 평가하는 것이다. 우리는 사실성은 진술이 신뢰할 수 있는 외부 소스에 의해 확증될 수 있는지 여부로 정의하고, 특수성은 투입과 산출 사이의 높은 상호 의존성으로 정의한다[33]. 우리는 모범 사례를 따르고 쌍대 비교 평가를 사용한다[34]. 평가자는 답변과 생성된 두 가지 질문, 하나는 BART에서, 하나는 RAG에서 표시된다. 그런 다음 그들은 네 가지 옵션 중 하나를 선택하도록 요청받는다 - 질문 A가 더 좋다, 질문 B가 더 좋다, 둘 다 좋다, 둘 다 좋지 않다.\n' +
      '\n' +
      '### Fact Verification\n' +
      '\n' +
      'FEVER[56]은 자연어 주장이 위키피디아에 의해 지원되거나 반박되는지, 또는 결정할 정보가 충분한지 여부를 분류하도록 요구한다. 이 작업은 위키피디아에서만 주장이 참인지 거짓인지 검증할 수 없는지 분류하기 위해 주장과 관련된 위키피디아에서 증거를 검색한 다음 이 증거에 대해 추론해야 한다. FEVER는 도전적인 수반 추론 과제와 결합된 검색 문제이다. 또한 RAG 모델의 생성보다는 분류 처리 능력을 탐색하기 위한 적절한 테스트베드를 제공한다. FEVER 클래스 레이블(지원, 반박 또는 정보 부족)을 단일 출력 토큰에 매핑하고 클레임 클래스 쌍으로 직접 학습합니다. 결정적으로, FEVER에 대한 대부분의 다른 접근법과 달리, 우리는 검색된 증거에 대한 감독을 사용하지 않는다. 많은 실제 응용에서 검색 감독 신호는 사용할 수 없으며 이러한 감독이 필요하지 않은 모델은 더 넓은 범위의 작업에 적용할 수 있다. 우리는 Thorne과 Vlachos[57]에서 연구된 표준 3-way 분류 작업(지원/반박/정보가 충분하지 않음)과 2-way(지원/반박) 작업의 두 가지 변형을 탐구한다. 두 경우 모두 레이블 정확도를 보고한다.\n' +
      '\n' +
      '## 4 Results\n' +
      '\n' +
      '### 오픈 도메인 질문 응답\n' +
      '\n' +
      '표 1은 최신 모델과 함께 RAG에 대한 결과를 보여준다. 네 개의 오픈 도메인 QA 태스크 모두에서, RAG는 (TQA에 대한 T5-비교 가능한 분할에만) 새로운 최신 상태를 설정한다. RAG는 "closed-book"(parametric only) 접근법의 생성 유연성과 "open-book" 검색 기반 접근법의 성능을 결합한다. REALM 및 T5+SSM과 달리, RAG는 비싸고 전문화된 "현저한 스팬 마스킹" 사전 훈련 없이 강한 결과를 누린다[20]. RAG의 검색기는 자연 질의와 트리비아QA에 대한 검색 감독을 사용하는 DPR의 검색기를 사용하여 초기화된다는 점에 주목할 필요가 있다. RAG는 추출 판독기와 함께 문서의 순위를 재지정하기 위해 BERT 기반 "크로스 인코더"를 사용하는 DPR QA 시스템과 유리하게 비교된다. RAG는 최신 성능을 위해 재순위자 또는 추출 독자가 필요하지 않음을 보여준다.\n' +
      '\n' +
      '해답을 추출할 수 있는 경우에도 답을 생성하는 데에는 몇 가지 장점이 있다. 답변에 대한 단서가 있지만 정답을 포함하지 않는 문서는 여전히 생성된 정답에 기여할 수 있으며, 이는 표준 추출 접근법으로는 불가능하며, 주도적이다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      '2-way 분류를 위해 RoBERTa[35]를 훈련시킨 Thorne과 Vlachos[57]와 비교하여 금 증거 문장을 고려할 때 주장을 참 또는 거짓으로 분류한다. RAG는 주장만 제공받고 자체 증거를 회수함에도 불구하고 이 모델의 2.7% 이내의 정확도를 달성한다. 또한 RAG에 의해 검색된 문서가 FEVER에서 금 증거로 주석이 달린 문서에 해당하는지 분석한다. RAG에 의해 검색된 상위 \\(k\\) 문서와 골드 증거 주석 사이의 기사 제목 중복을 계산한다. 검색된 상위 문서는 71%의 사례에서 금 기사이고 90%의 사례에서 상위 10개의 검색된 문서에서 금 기사가 존재한다는 것을 발견했다.\n' +
      '\n' +
      '### Additional Results\n' +
      '\n' +
      '생성 다양성 섹션 4.3은 RAG 모델이 제퍼디 질문 생성에 대해 BART보다 사실적이고 구체적이라는 것을 보여준다. 다양성을 촉진하는 디코딩에 대한 최근 작업[33; 59; 39]에 이어, 우리는 또한 다른 모델에 의해 생성된 총 ngram에 대한 별개의 ngram의 비율을 계산하여 생성 다양성을 조사한다. 표 5는 RAG-Sequence의 세대가 RAG-Token의 세대보다 다양하고, 둘 다 다양성 촉진 디코딩이 필요하지 않고 BART보다 훨씬 더 다양함을 보여준다.\n' +
      '\n' +
      '검색 AblationsRAG의 주요 기능은 작업에 대한 관련 정보를 검색하는 학습입니다. 검색 메커니즘의 효율성을 평가하기 위해 훈련 중에 검색기를 동결하는 절제를 실행한다. 표 6에 도시된 바와 같이, 학습된 검색은 모든 태스크에 대한 결과를 향상시킨다.\n' +
      '\n' +
      'RAG의 조밀한 검색기를 단어 중첩 기반 BM25 검색기와 비교한다[53]. 여기서는 RAG의 검색기를 고정된 BM25 시스템으로 교체하고, \\(p(z|x)\\)을 계산할 때 BM25 검색 점수를 로짓으로 사용한다. 결과를 표 6에 나타낸다. FEVER의 경우 BM25가 가장 잘 수행되는데, 아마도 FEVER 클레임이 개체 중심적이므로 단어 중첩 기반 검색에 적합하기 때문일 것이다. 미분 가능한 검색은 특히 중요한 오픈 도메인 QA의 경우 다른 모든 작업에 대한 결과를 향상시킨다.\n' +
      '\n' +
      '인덱스 핫 스와핑 RAG와 같은 비모수 메모리 모델의 장점은 테스트 시간에 지식을 쉽게 업데이트할 수 있다는 것이다. T5 또는 BART와 같은 모수 전용 모델은 세계가 변화함에 따라 그들의 행동을 업데이트하기 위해 추가 훈련이 필요하다. 이를 위해 2016년 12월부터 DrQA [5] 위키피디아 덤프를 사용하여 인덱스를 구축하고 이 인덱스를 사용하여 RAG의 출력을 주요 결과(2018년 12월)에서 최신 인덱스와 비교한다. 우리는 세상을 바꾼 82명의 세계 지도자들의 명단을 준비한다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline Task & Input & Model & Generation \\\\ \\hline \\multirow{4}{*}{MS-MARCO} & \\multirow{2}{*}{define middle ear} & BART & ’The middle ear is the part of the ear between the middle ear and the nose. \\\\  & & RAG-T & The middle ear is the portion of the ear internal to the earstrum. \\\\  & & RAG-S & The middle ear includes the tympanic cavity and the three ossicles. \\\\ \\cline{2-4}  & \\multirow{2}{*}{what currency needed in} & BART & The currency needed in Scotland is Poland. \\\\  & & needed in & RAG-T & Pound is the currency needed in Scotland. \\\\  & & scotland & RAG-S & The currency needed in Scotland is the bound sterling. \\\\ \\hline \\multirow{4}{*}{Jeopardy Question} & \\multirow{2}{*}{Washington} & BART & ’This state has the largest number of counties in the U.S. \\\\  & & RAG-T & T’s the only U.S. state named for a U.S. president \\\\ \\cline{1-1}  & & RAG-S & It’s the state where you’ll find Mount Rainier National Park \\\\ \\cline{1-1} \\cline{2-4}  & \\multirow{2}{*}{The Divine Comedy} & BART & ’This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio \\& the Purgatorio \\& the Purgatorio \\& T-Dante’s “Inferno” is the first part of this epic poem \\\\ \\cline{1-1}  & & RAG-S & This 14th century work is divided into 3 sections: “Inferno”, “Purgatorio” \\& “Paradiso” \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 생성 태스크로부터의 예시. RAG 모델은 보다 구체적이고 사실적으로 정확한 응답을 생성한다. \'?\'는 실제로 잘못된 응답을 나타내고, *는 부분적으로 올바른 응답을 나타낸다.\n' +
      '\n' +
      '그림 2: RAG-Token 문서 posterior \\(p(z_{i}|x,y_{i},y_{-i})\\)는 5개의 검색된 문서로 Jeopardy 생성을 위한 입력 "Hemingway"에 대해 생성된 각 토큰에 대한 것이다. 문서 1의 사후는 "팔과 작별"을 생성할 때 높고, 문서 2는 "태양도 떠오른다"를 생성할 때 높다.\n' +
      '\n' +
      '이러한 날짜 사이에 템플릿을 사용 하 여 각 인덱스로 NQ RAG 모델을 쿼리 합니다. RAG는 2016년 세계 리더의 경우 2016년 지수를 사용하여 70%, 2018년 세계 리더의 경우 2018년 지수를 사용하여 68%를 정확하게 응답한다. 일치하지 않는 지수의 정확도는 낮다(2018년 지수와 2016년 지도자의 경우 12%, 2016년 지수와 2018년 지도자의 경우 4%). 이것은 단순히 비모수적 메모리를 대체함으로써 RAG의 세계 지식을 업데이트할 수 있음을 보여준다.\n' +
      '\n' +
      '더 많은 문서 검색의 효과는 5개 또는 10개의 검색된 잠재 문서로 모델을 훈련하며, 이들 간의 성능에서 유의미한 차이를 관찰하지 못한다. 성능과 런타임에 영향을 미칠 수 있는 테스트 시간에 검색된 문서의 수를 조정할 수 있는 유연성을 가지고 있습니다. 그림 3(왼쪽)은 테스트 시간에 더 많은 문서를 검색하는 것이 RAG-Sequence에 대한 오픈 도메인 QA 결과를 단조롭게 개선하지만 10개의 검색된 문서에서 RAG-Token에 대한 성능 피크를 보여준다. 그림 3(오른쪽)은 더 많은 문서를 검색하는 것이 Bleu-1을 희생시키면서 RAG 토큰에 대해 더 높은 루즈-L로 이어지지만 RAG-서열에 대해서는 효과가 덜 두드러진다는 것을 보여준다.\n' +
      '\n' +
      '## 5 관련 작업\n' +
      '\n' +
      '단일 태스크 검색 이전 작업은 검색이 독립적으로 고려될 때 다양한 NLP 태스크에 걸쳐 성능을 향상시킨다는 것을 보여주었다. 이러한 작업은 오픈 도메인 질의 응답[5; 29], 팩트 체크[56], 팩트 완성[48], 롱폼 질의 응답[12], 위키피디아 기사 생성[36], 대화[41; 65; 9; 13], 번역[17], 언어 모델링[19; 27]을 포함한다. 본 연구는 단일 검색 기반 아키텍처가 여러 태스크에 걸쳐 강력한 성능을 달성할 수 있음을 보여줌으로써 개별 태스크에 검색을 통합하는 데 있어 이전의 성공을 통합한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c|c c c c|c c c} \\hline \\hline Model & NQ & TQA & WQ & CT & Jeopardy-QGen & MSMarco & FVR-3 & FVR-2 \\\\  & & Exact & Match & & B-1 & QB-1 & R-L & B-1 & Label & Accuracy \\\\ \\hline RAG-Token-BM25 & 29.7 & 41.5 & 32.1 & 33.1 & 17.5 & 22.3 & 55.5 & 48.4 & **75.1** & **91.6** \\\\ RAG-Sequence-BM25 & 31.8 & 44.1 & 36.6 & 33.8 & 11.1 & 19.5 & 56.5 & 46.9 & & **75.1** & **91.6** \\\\ \\hline RAG-Token-Frozen & 37.8 & 50.1 & 37.1 & 51.1 & 16.7 & 21.7 & 55.9 & 49.4 & 72.9 & 89.4 \\\\ RAG-Sequence-Frozen & 41.2 & 52.1 & 41.8 & 52.6 & 11.8 & 19.6 & 56.7 & 47.3 & & 89.4 \\\\ \\hline RAG-Token & 43.5 & 54.8 & **46.5** & 51.9 & **17.9** & **22.6** & 56.2 & **49.4** & & 74.5 & 90.6 \\\\ RAG-Sequence & **44.0** & **55.8** & 44.9 & **53.4** & 15.3 & 21.5 & **57.2** & 47.5 & & 89.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 디브 셋의 블레이션. FEVER는 분류 작업이기 때문에 두 RAG 모델은 모두 동등하다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & Factuality & Specificity \\\\ \\hline BART better & 7.1\\% & 16.8\\% \\\\ RAG better & **42.7\\%** & **37.4\\%** \\\\ Both good & 11.7\\% & 11.8\\% \\\\ Both poor & 17.7\\% & 6.9\\% \\\\ No majority & 20.8\\% & 20.1\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 위험 질문 생성 과제에 대한 인간 평가.\n' +
      '\n' +
      '도 3: 좌측: 더 많은 문서들이 검색됨에 따른 NQ 성능. Center: Retrieval recall performance in NQ. 오른쪽: MS-MARCO Bleu-1과 Rouge-L은 더 많은 문서가 검색됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & MSMARCO & Jeopardy QGen \\\\ \\hline Gold & 89.6\\% & 90.0\\% \\\\ BART & 70.7\\% & 32.4\\% \\\\ RAG-Token & 77.8\\% & 46.8\\% \\\\ RAG-Seq. & 83.5\\% & 53.8\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 생성 작업에 대한 총 3-그램에 대한 구분 비율.\n' +
      '\n' +
      'NLP 태스크를 위한 범용 아키텍처에 대한 NLPP 이전 작업을 위한 범용 아키텍처는 검색을 사용하지 않고 큰 성공을 거두었다. 사전 훈련된 단일 언어 모델은 미세 조정 후 GLUE 벤치마크[60; 61]에서 다양한 분류 작업에서 강력한 성능을 달성하는 것으로 나타났다[49; 8]. GPT-2 [50]은 나중에 단일 좌-우 사전 훈련 언어 모델이 차별적 작업과 생성적 작업 모두에서 강력한 성능을 달성할 수 있음을 보여주었다. 추가적인 개선을 위해, BART[32] 및 T5[51; 52]는 차별적이고 생성적인 태스크에서 더 강한 성능을 달성하기 위해 양방향 주의를 활용하는 단일 사전 훈련된 인코더-디코더 모델을 제안한다. 본 연구는 사전 훈련된 생성 언어 모델을 증강하기 위해 검색 모듈을 학습함으로써 단일 단일 아키텍처로 가능한 작업의 공간을 확장하는 것을 목표로 한다.\n' +
      '\n' +
      '학습된 검색은 정보 검색에서 문서를 검색하기 위한 중요한 작업이며, 최근에는 우리와 유사한 사전 훈련된 신경 언어 모델[44; 26]을 사용하여 문서 검색을 학습한다. 일부 작업은 검색 [46], 강화 학습 [6; 63; 62] 또는 잠재 변수 접근 방식 [31; 20]을 사용 하 여 질의 응답과 같은 특정 다운스트림 작업에 도움이 되도록 검색 모듈을 최적화 합니다. 이러한 성공은 단일 태스크에서 강력한 성능을 얻기 위해 다양한 검색 기반 아키텍처와 최적화 기법을 활용하는 반면, 단일 검색 기반 아키텍처는 다양한 태스크에서 강력한 성능을 위해 미세 조정될 수 있음을 보여준다.\n' +
      '\n' +
      '메모리 기반 아키텍처 우리의 문서 인덱스는 메모리 네트워크와 유사하게 신경 네트워크들이 주목해야 하는 큰 외부 메모리로 볼 수 있다[64;55]. 동시 작업[14]은 우리 작업에서와 같이 원시 텍스트를 검색하는 것이 아니라 입력에서 각 엔티티에 대해 훈련된 임베딩을 검색하는 것을 학습한다. 다른 작업은 팩트 임베딩에 대해 참석함으로써 팩트 텍스트를 생성하는 대화 모델의 능력을 향상시킨다[15;13]. 우리 메모리의 주요 특징은 원시 텍스트가 분산되어 있는 표현으로 구성되어 있기 때문에 (i) 사람이 읽을 수 있고 모델에 해석 가능한 형태를 빌려줄 수 있으며 (ii) 사람이 쓸 수 있어 문서 색인을 편집하여 모델의 메모리를 동적으로 업데이트할 수 있다는 것이다. 이 접근법은 또한 지식 집약적 대화에서 사용되었는데, 여기서 생성자는 종단 간 학습 검색이 아닌 TF-IDF를 통해 얻었지만 검색된 텍스트에 직접 조건화되었다[9].\n' +
      '\n' +
      '검색 및 편집 접근법은 주어진 입력에 대해 유사한 훈련 입력-출력 쌍을 검색한 다음 최종 출력을 제공하도록 편집하는 검색 및 편집 스타일 접근법과 몇 가지 유사성을 공유한다. 이러한 접근법들은 기계 번역[18; 22] 및 시맨틱 파싱[21]을 포함한 다수의 도메인들에서 성공적인 것으로 입증되었다. 우리의 접근 방식은 검색된 항목을 가볍게 편집하는 것에 중점을 두지 않고 검색된 여러 콘텐츠에서 콘텐츠를 집계하는 것뿐만 아니라 잠재 검색을 학습하고 관련 훈련 쌍이 아닌 증거 문서를 검색하는 것을 포함하여 몇 가지 차이점이 있다. 이것은 RAG 기술이 이러한 설정에서 잘 작동할 수 있으며 유망한 미래 작업을 나타낼 수 있다고 말했다.\n' +
      '\n' +
      '## 6 Discussion\n' +
      '\n' +
      '본 연구에서는 모수적 메모리와 비모수적 메모리에 접근할 수 있는 하이브리드 생성 모델을 제시하였다. 우리는 우리의 RAG 모델이 개방형 QA에 대한 최신 결과를 얻는다는 것을 보여주었다. 우리는 사람들이 순전히 파라메트릭 BART보다 RAG의 세대를 선호한다는 것을 발견했으며, 이는 RAG가 더 사실적이고 구체적이라는 것을 발견했다. 학습된 검색 컴포넌트에 대한 철저한 조사를 수행하여 그 유효성을 검증하였으며, 재학습이 필요 없이 검색 인덱스를 핫스왑하여 모델을 갱신할 수 있는 방법을 예시하였다. 향후 작업에서는 BART와 유사한 노이즈 제거 목표 또는 다른 목표와 함께 두 구성 요소가 처음부터 공동으로 사전 훈련될 수 있는지 조사하는 것이 유용할 수 있다. 이 연구는 모수적 기억과 비모수적 기억이 어떻게 상호 작용하고 가장 효과적으로 결합하는지에 대한 새로운 연구 방향을 열어 다양한 NLP 작업에 적용될 가능성을 보여준다.\n' +
      '\n' +
      '## Broader Impact\n' +
      '\n' +
      '이 작업은 이전 작업에 비해 몇 가지 긍정적인 사회적 이점을 제공한다. RAG는 예를 들어 의료 지수를 부여하고 해당 주제에 대해 개방형 질문을 하거나 사람들이 직장에서 더 효과적으로 일할 수 있도록 도와 사회에 직접적인 이점이 있는 다양한 시나리오에서 사용할 수 있다.\n' +
      '\n' +
      '이러한 이점에는 잠재적인 단점도 있다: 위키피디아 또는 잠재적인 외부 지식 소스는 아마도 완전히 사실적이지 않고 편견이 완전히 없을 것이다. RAG가 언어 모델로 사용될 수 있기 때문에 GPT-2 [50]에 대한 유사한 우려는 여기에서 유효하지만, 뉴스나 소셜 미디어에서 오용, 위장 또는 오판의 소지가 있는 콘텐츠를 생성하거나 다른 사람을 가장하거나 스팸/피싱 콘텐츠 생산을 자동화하는 데 사용될 수 있음을 포함하여 논란의 여지가 적다. 고급 언어 모델은 또한 향후 수십 년 동안 다양한 작업의 자동화로 이어질 수 있다[16]. 이러한 위험을 완화하기 위해 AI 시스템을 사용하여 오판의 소지가 있는 콘텐츠와 자동화된 스팸/피싱과 싸울 수 있습니다.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      '저자들은 리뷰어들에게 이 논문에 대한 사려 깊고 건설적인 피드백과 RAG 모델을 실행하기 위한 오픈 소싱 코드에 대한 도움에 대해 허그페이스에게 감사하고 싶다. 저자들도 조경현과 민세원이 생산적인 토론과 조언에 감사드린다. EP는 NSF 대학원 연구 펠로우십의 지원에 감사한다. PL은 FAIR 박사 프로그램에 의해 지원됩니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Bajaj et al. [2016] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. _arXiv:1611.09268 [cs]_, November 2016. URL [http://arxiv.org/abs/1611.09268](http://arxiv.org/abs/1611.09268). arXiv: 1611.09268.\n' +
      '* Baudis and Sedivy [2015] Petr Baudis and Jan Sedivy. 요다카 시스템에서의 질의 응답 태스크의 모델링. _유럽 언어에 대 한 교차 언어 평가 포럼의 국제 회의_에서 222-228 페이지, Springer, 2015. URL [https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20](https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20)\n' +
      '* Berant et al. [2013] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. In _Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing_, pages 1533-1544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL [http://www.aclweb.org/anthology/D13-1160](http://www.aclweb.org/anthology/D13-1160).\n' +
      '* Bi et al. [2020] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencoding&autoregressive language model for context-conditioned generation. _ArXiv_, abs/2004.07159, 2020. URL [https://arxiv.org/abs/2004.07159](https://arxiv.org/abs/2004.07159).\n' +
      '* Chen et al. [2017] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1870-1879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL [https://www.aclweb.org/anthology/P17-1171](https://www.aclweb.org/anthology/P17-1171).\n' +
      '* Choi et al. [2017] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonathan Berant. Coarse-to-fine question answering for long documents. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 209-220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1020. URL [https://www.aclweb.org/anthology/P17-1020](https://www.aclweb.org/anthology/P17-1020).\n' +
      '\n' +
      '* Clark and Gardner [2017] Christopher Clark and Matt Gardner. 간단하고 효과적인 다중 단락 읽기 이해입니다. _ arXiv:1710.10723 [cs]_, October 2017. URL [http://arxiv.org/abs/1710.10723](http://arxiv.org/abs/1710.10723). arXiv: 1710.10723.\n' +
      '* Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL [https://www.aclweb.org/anthology/N19-1423](https://www.aclweb.org/anthology/N19-1423).\n' +
      '* Dinan et al. [2019] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In _International Conference on Learning Representations_, 2019. URL [https://openreview.net/forum?id=r1173iRqKm](https://openreview.net/forum?id=r1173iRqKm).\n' +
      '* Dunn et al. [2017] Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun Cho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine. _arXiv:1704.05179 [cs]_, April 2017. URL [http://arxiv.org/abs/1704.05179](http://arxiv.org/abs/1704.05179). arXiv: 1704.05179.\n' +
      '* Fan et al. [2018] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 889-898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL [https://www.aclweb.org/anthology/P18-1082](https://www.aclweb.org/anthology/P18-1082).\n' +
      '* Fan et al. [2019] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 3558-3567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL [https://www.aclweb.org/anthology/P19-1346](https://www.aclweb.org/anthology/P19-1346).\n' +
      '* Fan et al. [2020] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers with KNN-based composite memory, 2020. URL [https://openreview.net/forum?id=H1gx1CNKPH](https://openreview.net/forum?id=H1gx1CNKPH).\n' +
      '* Fevry et al. [2020] Thibault Fevry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. Entities as experts: Sparse memory access with entity supervision. _ArXiv_, abs/2004.07202, 2020. URL [https://arxiv.org/abs/2004.07202](https://arxiv.org/abs/2004.07202).\n' +
      '* Ghazvininejad et al. [2018] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wentau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In _AAAI Conference on Artificial Intelligence_, 2018. URL [https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16710](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16710).\n' +
      '* Grace et al. [2017] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI exceed human performance? evidence from AI experts. _CoRR_, abs/1705.08807, 2017. URL [http://arxiv.org/abs/1705.08807](http://arxiv.org/abs/1705.08807).\n' +
      '* Gu et al. [2018] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In _AAAI Conference on Artificial Intelligence_, 2018. URL [https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282).\n' +
      '* Gu et al. [2018] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In _32nd AAAI Conference on Artificial Intelligence, AAAI 2018_, 32nd AAAI Conference on Artificial Intelligence, AAAI 2018 ; Conference date: 02-02-2018 Through 07-02-2018.\n' +
      '* Guu et al. [2018] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. _Transactions of the Association for Computational Linguistics_, 6:437-450, 2018. doi: 10.1162/tacl_a_00030. URL [https://www.aclweb.org/anthology/Q18-1031](https://www.aclweb.org/anthology/Q18-1031).\n' +
      '\n' +
      '* Guu et al. [2020] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. _ArXiv_, abs/2002.08909, 2020. URL [https://arxiv.org/abs/2002.08909](https://arxiv.org/abs/2002.08909).\n' +
      '* Hashimoto et al. [2018] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A retrieve-and-edit framework for predicting structured outputs. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems 31_, pages 10052-10062. Curran Associates, Inc., 2018. URL [http://papers.nips.cc/paper/8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.pdf](http://papers.nips.cc/paper/8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.pdf).\n' +
      '* Hossain et al. [2020] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-edit-rerank text generation. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 2532-2538, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL [https://www.aclweb.org/anthology/2020.acl-main.228](https://www.aclweb.org/anthology/2020.acl-main.228).\n' +
      '* Johnson et al. [2017] Jeff Johnson, Matthijs Douze, and Herve Jegou. Billion-scale similarity search with gpus. _arXiv preprint arXiv:1702.08734_, 2017. URL [https://arxiv.org/abs/1702.08734](https://arxiv.org/abs/1702.08734).\n' +
      '* Joshi et al. [2017] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL [https://www.aclweb.org/anthology/P17-1147](https://www.aclweb.org/anthology/P17-1147).\n' +
      '* Volume 1_, NIPS\'15, page 190-198, Cambridge, MA, USA, 2015. MIT Press. URL [https://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets](https://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets).\n' +
      '* Karpukhin et al. [2020] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. _arXiv preprint arXiv:2004.04906_, 2020. URL [https://arxiv.org/abs/2004.04906](https://arxiv.org/abs/2004.04906).\n' +
      '* Khandelwal et al. [2020] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In _International Conference on Learning Representations_, 2020. URL [https://openreview.net/forum?id=Hk1BjCEvH](https://openreview.net/forum?id=Hk1BjCEvH).\n' +
      '* Kingma and Ba[2015] Diederik P. Kingma and Jimmy Ba. Adam: 확률적 최적화를 위한 방법. Yoshua Bengio and Yann LeCun, editors, _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, Conference Track Proceedings_, 2015. URL [http://arxiv.org/abs/1412.6980](http://arxiv.org/abs/1412.6980).\n' +
      '* Kwiatkowski et al. [2019] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Question Answering Research. _Transactions of the Association of Computational Linguistics_, 2019. URL [https://tomkwiat.users.x20web.corp.google.com/papers/natural-questions/main-1455-kwiatkowski.pdf](https://tomkwiat.users.x20web.corp.google.com/papers/natural-questions/main-1455-kwiatkowski.pdf).\n' +
      '* Lample et al. [2019] Guillaume Lample, Alexandre Sablayrolles, Marc\' Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\' Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems 32_, pages 8548-8559. Curran Associates, Inc., 2019. URL [http://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf](http://papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf).\n' +
      '* Lee et al. [2019] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In _Proceedings of the 57th Annual Meeting of the Associationfor Computational Linguistics_, pages 6086-6096, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1612. URL [https://www.aclweb.org/anthology/P19-1612](https://www.aclweb.org/anthology/P19-1612).\n' +
      '* Lewis et al. [2019] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. _arXiv preprint arXiv:1910.13461_, 2019. URL [https://arxiv.org/abs/1910.13461](https://arxiv.org/abs/1910.13461).\n' +
      '* Li et al. [2016] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In _Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 110-119, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1014. URL [https://www.aclweb.org/anthology/N16-1014](https://www.aclweb.org/anthology/N16-1014).\n' +
      '* Li et al. [2019] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. _ArXiv_, abs/1909.03087, 2019. URL [https://arxiv.org/abs/1909.03087](https://arxiv.org/abs/1909.03087).\n' +
      '* Liu et al. [2019] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine translation with joint textual and phonetic embedding. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 3044-3049, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL [https://www.aclweb.org/anthology/P19-1291](https://www.aclweb.org/anthology/P19-1291).\n' +
      '* Liu* et al. [2018] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In _International Conference on Learning Representations_, 2018. URL [https://openreview.net/forum?id=Hyg0vbWC-](https://openreview.net/forum?id=Hyg0vbWC-).\n' +
      '* Malkov and Yashunin [2016] Yury A. Malkov and D. A. Yashunin. 계층적 탐색 가능한 작은 세계 그래프를 사용하여 효율적이고 강력한 근사 최근접 이웃 검색입니다. _ IEEE Transactions on Pattern Analysis and Machine Intelligence_, 42:824-836, 2016. URL [https://arxiv.org/abs/1603.09320](https://arxiv.org/abs/1603.09320).\n' +
      '* Marcus[2020] Gary Marcus. 다음 10년은 강력한 인공지능을 향한 4단계입니다. arXiv preprint arXiv:2002.06177_, 2020. URL [https://arxiv.org/abs/2002.06177](https://arxiv.org/abs/2002.06177).\n' +
      '* Massarelli et al. [2019] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktaschel, Vassilis Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the verifiability of generated text. _arXiv preprint arXiv:1911.03587_, 2019. URL [https://arxiv.org/abs/1911.03587](https://arxiv.org/abs/1911.03587).\n' +
      '* Micikevicius et al. [2018] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In _ICLR_, 2018. URL [https://openreview.net/forum?id=r1g89JgRZ](https://openreview.net/forum?id=r1g89JgRZ).\n' +
      '* Moghe et al. [2018] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploiting background knowledge for building conversation systems. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 2322-2332, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1255. URL [https://www.aclweb.org/anthology/D18-1255](https://www.aclweb.org/anthology/D18-1255).\n' +
      '* Nema and Khapra [2018] Preksha Nema and Mitesh M. 카프라 질문 생성 시스템을 평가하기 위한 더 나은 메트릭을 향합니다. _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 3950-3959, Brussels, Belgium, October-11. Association for Computational Linguistics. doi: 10.18653/v1/D18-1429. URL [https://www.aclweb.org/anthology/D18-1429](https://www.aclweb.org/anthology/D18-1429).\n' +
      '* Nguyen et al. [2018] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In Tarek Richard Besold, Antoine Bordes, Artur S. d\'Avila Garcez, and Greg Wayne, editors, _Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016_, volume 1773 of _CEUR Workshop Proceedings_. CEUR-WS.org, 2016. URL [http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf](http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf).\n' +
      '* 노게이라 및 조[2019] 로드리고 노게이라 및 조경현. BERT로 다시 순위를 매깁니다. _ arXiv preprint arXiv:1901.04085_, 2019. URL [https://arxiv.org/abs/1901.04085](https://arxiv.org/abs/1901.04085).\n' +
      '* Ott et al. [2019] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)_, pages 48-53, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL [https://www.aclweb.org/anthology/N19-4009](https://www.aclweb.org/anthology/N19-4009).\n' +
      '* Perez et al. [2019] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho. Finding generalizable evidence by learning to convince q&a models. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 2402-2411, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1244. URL [https://www.aclweb.org/anthology/D19-1244](https://www.aclweb.org/anthology/D19-1244).\n' +
      '* Petroni et al. [2019] Fabio Petroni, Tim Rocktaschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 2463-2473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL [https://www.aclweb.org/anthology/D19-1250](https://www.aclweb.org/anthology/D19-1250).\n' +
      '* Petroni et al. [2020] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktaschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. How context affects language models\' factual predictions. In _Automated Knowledge Base Construction_, 2020. URL [https://openreview.net/forum?id=025X02Pfn](https://openreview.net/forum?id=025X02Pfn).\n' +
      '* Radford et al. [2018] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Understanding by Generative Pre-Training, 2018. URL [https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf).\n' +
      '* Radford et al. [2019] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL [https://d4mucfpksyw.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](https://d4mucfpksyw.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).\n' +
      '* Raffel et al. [2019] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _arXiv e-prints_, 2019. URL [https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683).\n' +
      '* Roberts et al. [2020] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? _arXiv e-prints_, 2020. URL [https://arxiv.org/abs/2002.08910](https://arxiv.org/abs/2002.08910).\n' +
      '* Robertson and Zaragoza[2009] Stephen Robertson and Hugo Zaragoza. 확률적 관련성 프레임워크: Bm25 이상. _ 찾았어요 트렌드 인포 Ret._ , 3(4):333-389, 2009년 4월. ISSN 1554-0669. doi: 10.1561/1500000019. URL [https://doi.org/10.1561/1500000019](https://doi.org/10.1561/1500000019).\n' +
      '* Solaiman et al. [2019] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models. _ArXiv_, abs/1908.09203, 2019.\n' +
      '* Sukhbaatar et al. [2015] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, _Advances in Neural Information Processing Systems 28_, pages 2440-2448. Curran Associates, Inc., 2015. URL [http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf](http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf).\n' +
      '\n' +
      '* Thorne et al. [2018] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 809-819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL [https://www.aclweb.org/anthology/N18-1074](https://www.aclweb.org/anthology/N18-1074).\n' +
      '* Thorne and Vlachos [2020] James H. Thorne and Andreas Vlachos. 탄력적 가중치 통합을 사용 하 여 문장 쌍 분류에서 모델 편향을 완화 하는 데 치명적인 망각을 방지 합니다. _ ArXiv_, abs/2004.14366, 2020. URL [https://arxiv.org/abs/2004.14366](https://arxiv.org/abs/2004.14366).\n' +
      '* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems 30_, pages 5998-6008. Curran Associates, Inc., 2017. URL [http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf).\n' +
      '* Vijayakumar et al. [2018] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. _AAAI Conference on Artificial Intelligence_, 2018. URL [https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17329](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17329).\n' +
      '* Wang et al. [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In _Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP_, pages 353-355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL [https://www.aclweb.org/anthology/W18-5446](https://www.aclweb.org/anthology/W18-5446).\n' +
      '* Wang et al. [2019] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dtextquotesing el Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems 32_, pages 3261-3275. Curran Associates, Inc., 2019. URL [https://arxiv.org/abs/1905.00537](https://arxiv.org/abs/1905.00537).\n' +
      '* Wang et al. [2018] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain question answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018_, pages 5981-5988. AAAI Press, 2018. URL [https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16712](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16712).\n' +
      '* Wang et al. [2018] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-ranking in open-domain question answering. In _ICLR_, 2018. URL [https://openreview.net/forum?id=rJl3yM-Ab](https://openreview.net/forum?id=rJl3yM-Ab).\n' +
      '* Weston et al. [2015] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio and Yann LeCun, editors, _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015. URL [http://arxiv.org/abs/1410.3916](http://arxiv.org/abs/1410.3916).\n' +
      '* Weston et al. [2018] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and refine: Improved sequence generation models for dialogue. In _Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI_, pages 87-92, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL [https://www.aclweb.org/anthology/W18-5713](https://www.aclweb.org/anthology/W18-5713).\n' +
      '\n' +
      '* [66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface\'s transformers: State-of-the-art natural language processing. _ArXiv_, abs/1910.03771, 2019.\n' +
      '* [67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-supervised question answering. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 2495-2509, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL [https://www.aclweb.org/anthology/D19-1253](https://www.aclweb.org/anthology/D19-1253).\n' +
      '* [68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. Reasoning over semantic-level graph for fact checking. _ArXiv_, abs/1909.03745, 2019. URL [https://arxiv.org/abs/1909.03745](https://arxiv.org/abs/1909.03745).\n' +
      '\n' +
      '지식 집약적 NLP 태스크를 위한 검색 확장 생성\n' +
      '\n' +
      '## Appendix A Implementation Details\n' +
      '\n' +
      '오픈 도메인 QA의 경우 RAG-토큰 모델에 대해 15개의 검색된 문서를 사용하여 테스트 번호를 보고한다. RAG-Sequence 모델의 경우 50개의 검색된 문서를 사용하여 테스트 결과를 보고하고 일반적으로 답변이 짧기 때문에 Thorough Decoding 방식을 사용한다. 우리는 개선된 빔 탐색 결과를 찾지 못했기 때문에 QA에 대한 그리디 디코딩을 사용한다. Open-MSMarco와 Jeopardy 질문 생성을 위해 RAG-Token과 RAG-Sequence 모두에 대해 10개의 검색된 문서를 사용하여 테스트 번호를 보고하고 BART-large 모델을 기준선으로 훈련한다. 우리는 4개의 빔 크기를 사용하고 RAG-Sequence 모델에 대해 Fast Decoding 방식을 사용하는데, Thorough Decoding은 성능을 향상시키지 못했기 때문이다.\n' +
      '\n' +
      '## Appendix B Human Evaluation\n' +
      '\n' +
      '그림 4는 인간 평가를 위한 사용자 인터페이스를 보여준다. 화면 위치에 대한 편향을 피하기 위해 문장 A와 문장 B에 해당하는 모델을 각 예제에 대해 무작위로 선택했다. 주석사는 인터넷을 사용하여 주제를 연구하도록 권장되었으며 전체 지침 탭에서 자세한 지침과 작업 예가 제공되었다. 주석 작성자의 정확도를 평가하기 위해 일부 금 문장을 포함했다. 두 명의 주석이 이러한 예제에 대해 잘 수행되지 않았으며 그들의 주석은 결과에서 제거되었다.\n' +
      '\n' +
      '## 부록 C 교육 설정 세부 정보\n' +
      '\n' +
      '우리는 Fairseq[45].2 혼합 정밀 부동 소수점 산술[40]로 훈련하며, 훈련과 추론은 하나의 GPU에서 실행될 수 있지만 8,32GB NVIDIA V100 GPU에 걸쳐 훈련을 분배한다. FAISS를 사용하여 최대 내부 제품 검색을 수행하는 것은 CPU에서 충분히 빠르다는 것을 발견하여 문서 색인 벡터를 CPU에 저장하여 모든 위키피디아에 대해 \\(\\sim 100\\)GB의 CPU 메모리를 필요로 한다. 제출 후, 우리는 이전 버전과 동등한 성능을 달성하지만 더 깨끗하고 사용하기 쉬운 HuggingFace Transformers[66]3에 코드를 이식했습니다. 이 버전도 오픈 소스입니다. 또한 FAISS의 압축 도구를 사용하여 문서 인덱스를 압축하여 CPU 메모리 요구량을 36GB로 줄였다. RAG로 실험을 실행하는 스크립트는 [https://github.com/huggingface/transformers/blob/master/examples/rag/README.md](https://github.com/huggingface/transformers/blob/master/examples/rag/README.md)에서 찾을 수 있으며 RAG 모델의 대화형 데모는 [https://huggingface.co/rag/](https://huggingface.co/rag/)에서 찾을 수 있습니다.\n' +
      '\n' +
      '각주 2: [https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq)\n' +
      '\n' +
      '각주 3: [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)\n' +
      '\n' +
      '도 4: 사실성에 대한 인간의 평가를 위한 주석 인터페이스. "도구 가이드 보기"를 클릭하면 자세한 설명과 작동된 예제에 대한 팝아웃이 나타납니다.\n' +
      '\n' +
      '개방형 QA에 대한 추가 세부사항\n' +
      '\n' +
      '개방형 도메인 QA의 경우 주어진 질문에 대해 여러 답변 주석을 사용할 수 있는 경우가 많다. 이러한 답변 주석은 일반적으로 모든 답변 주석이 학습 데이터를 준비할 때 문서 내에서 일치하는 것을 찾는 데 사용되기 때문에 학습 중에 추출 모델에 의해 악용된다. RAG의 경우, 각 \\((q,a)\\) 쌍으로 모델을 개별적으로 학습하여 자연 질문 및 웹 질문에 대한 여러 주석 예제를 사용하여 정확도가 약간 증가한다. 트리비아QA의 경우, 주어진 질문에 대한 많은 유효한 답변이 종종 있으며, 그 중 일부는 이모지 또는 철자 변형과 같은 적절한 훈련 대상이 아니다. 트리비아QA의 경우, 질의에 대해 상위 1000개의 문서에서 발생하지 않는 경우 답변 후보들을 걸러낸다.\n' +
      '\n' +
      'CuratedTrec 전처리 CuratedTrec에 대한 답변은 정규식의 형태로 주어지는데, 이는 답변 생성 모델에 적합하지 않은 이유로 제시되고 있다[20]. 이를 극복하기 위해 먼저 각 질의에 대해 상위 1000개의 문서를 검색하는 전처리 단계를 사용하고, 정규 패턴과 가장 자주 일치하는 답변을 감독 대상으로 사용한다. 일치하는 것이 발견되지 않는 경우, 우리는 간단한 휴리스틱에 의존한다: 각 정규식에 대해 가능한 모든 순열을 생성하여 정규식 중첩 트리 구조의 비결정적 기호를 공백으로 대체한다.\n' +
      '\n' +
      'TriviaQA 평가 설정 오픈 도메인 QA 커뮤니티는 QA 데이터 세트에 대한 테스트 데이터가 종종 제한되고 읽기 이해 목적에 전념하기 때문에 일반적으로 공개 개발 데이터 세트를 테스트 데이터 세트로 사용한다. 우리는 오픈 도메인 QA의 일반적인 관행과 일치하는 DPR [26]에 사용된 데이터 세트 분할을 사용하여 결과를 보고한다. 트리비아QA의 경우 이 테스트 데이터 세트는 공용 트리비아QA 웹 개발 분할입니다. 로버트 등[52]은 대신 트리비아QA 공식 위키피디아 테스트 세트를 사용했다. Fevry et al. [14]는 Roberts et al. [52]와 비교하기 위해 이 규칙을 따른다([14]의 부록 참조). 우리는 두 접근법에 대한 공정한 비교를 가능하게 하기 위해 두 테스트 세트에 대한 결과를 보고한다. 우리는 우리의 성능이 더 전통적인 개방형 테스트 세트보다 공식 위키 테스트 세트를 사용하는 것이 훨씬 더 높다는 것을 발견했는데, 이는 공식 위키 테스트 세트 질문이 위키피디아에서 쉽게 답할 수 있기 때문이다.\n' +
      '\n' +
      '## 부록 E FEVER에 대한 추가 세부 정보\n' +
      '\n' +
      'FEVER 분류를 위해 [32]의 연습에 따라 먼저 클레임을 다시 생성한 다음 최종 은닉 상태의 표현을 사용하여 분류한 후 최종적으로 문서에서 주변화하여 클래스 확률을 얻는다. FEVER 태스크는 전통적으로 두 개의 하위 태스크를 가지고 있다. 첫 번째는 주장을 본 논문에서 탐구하는 과제인 "지원", "반박" 또는 "충분하지 않은 정보"로 분류하는 것이다. FEVER의 다른 하위 작업은 분류 예측을 뒷받침하는 증거로 위키피디아에서 문장을 추출하는 것이다. FEVER가 우리에게 다른 위키피디아 덤프를 사용하기 때문에, 이 일을 직접 다루는 것은 간단하지 않다. 향후 작업에서 이 문제를 해결하기를 바랍니다.\n' +
      '\n' +
      '## 부록 F Null 문서 확률\n' +
      '\n' +
      '우리는 주어진 입력에 대해 유용한 정보를 검색할 수 없는 경우를 모델링하기 위해 REALM [20]과 유사한 "Null 문서" 메커니즘을 RAG에 추가하는 실험을 수행했다. 여기서, \\(k\\) 문서가 검색된 경우, 우리는 추가로 빈 문서를 "검색"하고 null 문서에 대한 로짓을 예측한 후 \\(k+1\\) 예측을 무시한다. 우리는 (i) null 문서에 대한 문서 임베딩, (ii) 정적 학습 편향 항 또는 (iii) 로짓 예측을 위한 신경망을 학습하여 이 null 문서 로짓 모델링을 탐구했다. 우리는 이러한 성능이 향상되었다는 것을 발견하지 못했기 때문에 단순성을 위해 이를 생략한다. 유용한 검색된 문서가 항상 검색될 수 없는 오픈 MS-MARCO의 경우, 우리는 모델이 검색으로부터 이익을 얻을 가능성이 적은 질문들에 대해 특정 문서 세트를 항상 검색하는 것을 학습한다는 것을 관찰하며, 이는 RAG에 널 문서 메커니즘이 필요하지 않을 수 있음을 시사한다.\n' +
      '\n' +
      '## Appendix G Parameters\n' +
      '\n' +
      'RAG 모델은 DPR의 BERT 기반 질의와 문서 인코더에 대한 훈련 가능한 파라미터를 포함하고 있으며, 문서 인코더는 직접 훈련하지 않지만 각각 110M개의 파라미터와 BART-large, 406M 파라미터에서 406M개의 훈련 가능한 파라미터를 포함하여 총 626M개의 훈련 가능한 파라미터를 생성한다. 가장 잘 수행되는 "폐쇄북"(매개변수 전용) 개방형 도메인 QA 모델은 110억 훈련 가능한 매개변수가 있는 T5-11B이다. 모델과 가장 가까운 매개변수 수를 가진 T5 모델은 T5-large(770M 매개변수)로 자연 질문에 대해 28.9 EM의 점수를 달성하여 RAG-서열이 달성한 44.5보다 실질적으로 낮으며, 이는 하이브리드 매개변수/비매개변수 모델이 강력한 개방형 QA 성능을 위해 훨씬 적은 훈련 가능한 매개변수를 필요로 함을 나타낸다. 비모수 메모리 인덱스는 훈련 가능한 파라미터로 구성되지 않고 15.3B 값으로 구성된 21M 728 차원 벡터로 구성된다. 메모리 및 디스크 풋프린트를 관리하기 위해 8비트 부동 소수점 정밀도로 쉽게 저장할 수 있습니다.\n' +
      '\n' +
      '## 부록 H Retrieval Collapse\n' +
      '\n' +
      '사전 실험에서 스토리 생성[11]과 같은 일부 태스크의 경우, 검색 컴포넌트는 입력에 관계없이 동일한 문서를 "붕괴"하고 검색하는 것을 관찰했다. 이러한 경우, 일단 검색이 붕괴되면, 생성기는 문서를 무시하는 것을 학습할 것이고, RAG 모델은 BART와 동등하게 수행될 것이다. 붕괴는 일부 작업에서 사실적 지식에 대한 덜 명시적인 요구 사항 또는 더 긴 표적 서열로 인해 검색기에 대해 덜 유익한 기울기가 발생할 수 있다. Perez 등 [46]은 또한 다운스트림 태스크들에 대한 성능을 향상시키기 위해 검색 컴포넌트를 최적화할 때 스퓨리어스 검색 결과들을 발견하였다.\n' +
      '\n' +
      '## 부록 I 데이터 세트당 인스턴스 수\n' +
      '\n' +
      '각 데이터 세트의 훈련, 개발 및 테스트 데이터 포인트 수는 표 7에 나와 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r} \\hline \\hline Task & Train & Development & Test \\\\ \\hline Natural Questions & 79169 & 8758 & 3611 \\\\ TriviaQA & 78786 & 8838 & 11314 \\\\ WebQuestions & 3418 & 362 & 2033 \\\\ CuratedTree & 635 & 134 & 635 \\\\ Jeopardy Question Generation & 97392 & 13714 & 26849 \\\\ MS-MARCO & 153726 & 12468 & 101093* \\\\ FEVER-3-way & 145450 & 10000 & 10000 \\\\ FEVER-2-way & 96966 & 6666 & 6666 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 사용된 데이터세트의 인스턴스 수. *이 데이터의 숨겨진 서브세트가 평가에 사용됨\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>