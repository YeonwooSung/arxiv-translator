# Accurate Cold-start Bundle Recommendation via Popularity-based Coalescence and Curriculum Heating

Hyunsik Jeon

UC San Diego

San Diego, CA, USA

Jong-eun Lee

Seoul National University

Seoul, South Korea

Jeongin Yun

Seoul National University

Seoul, South Korea

U Kang

Seoul National University

Seoul, South Korea

###### Abstract.

How can we accurately recommend cold-start bundles to users? The cold-start problem in bundle recommendation is critical in practical scenarios since new bundles are continuously created for various marketing purposes. Despite its importance, no previous studies have addressed cold-start bundle recommendation. Moreover, existing methods for cold-start item recommendation overly rely on historical information, even for unpopular bundles, failing to tackle the primary challenge of the highly skewed distribution of bundle interactions. In this work, we propose CoHeat (Popularity-based Coalescence and Curriculum Heating), an accurate approach for the cold-start bundle recommendation. CoHeat tackles the highly skewed distribution of bundle interactions by incorporating both historical and affiliation information based on the bundle's popularity when estimating the user-bundle relationship. Furthermore, CoHeat effectively learns latent representations by exploiting curriculum learning and contrastive learning. CoHeat demonstrates superior performance in cold-start bundle recommendation, achieving up to 193% higher nDCG@20 compared to the best competitor.

cold-start bundle recommendation; curriculum learning; contrastive learning +
Footnote †: journal: Information systems Recommender systems

+
Footnote †: journal: Information systems Recommender systems

## 1. Introduction

_How can we accurately recommend cold-start bundles to users?_ Bundle recommendation has garnered significant attention in both academia and industry since it enables providers to offer items to users with one-stop convenience (Kang et al., 2018). In particular, recommending new bundles to users (i.e. cold-start bundle recommendation) is important in practical scenarios because the new bundles are constantly created for various marketing purposes (Kang et al., 2018).

In recent years, bundle recommendation has seen advancements through matrix factorization-based approaches (Chen et al., 2018; He et al., 2018; He et al., 2018) and graph learning-based approaches (Chen et al., 2018; He et al., 2018; He et al., 2018). However, they have been developed for a warm-start setting, where all bundles possess historical interactions with users. Consequently, they fail to effectively perform in a cold-start setting, where certain bundles are devoid of historical interactions. This is because warm-start methods rely highly on historical information to learn bundle representations. On the other hand, the cold-start problem in item recommendation has been extensively studied, with a focus on aligning behavior representations with content representations. For instance, generative methods have aimed to model the generation of item behavior representations using mean squared error (Krizhevsky et al., 2015) and adversarial loss (Chen et al., 2018). Dropout-based methods (Krizhevsky et al., 2015; Li et al., 2017) have aimed to bolster robustness to behavior information by randomly dropping the behavior embedding in the training phase. More recently, contrastive learning-based methods (Wang et al., 2018; He et al., 2018) have shown superior performance by reducing the discrepancy between the distributions of behavior and content information of items. However, none of the existing works have explicitly considered the skewed distribution of interactions which is a pivotal aspect in bundle recommendation as shown in Figure 0(a). For unpopular bundles, aligning behavior representations from insufficient historical information with content representations amplifies inherent biases and makes it difficult to learn meaningful representations; this results in sacrificing the performance on a warm-start setting to improve the performance on a cold-start setting (see Figure 2).

In this paper, we propose CoHeat (Popularity-based Coalescence and Curriculum Heating), an accurate method for cold-start bundle recommendation. CoHeat constructs representations of users and bundles using two distinct graph-based views: history-view and affiliation-view. The history-view graph is grounded in historical interactions between users and bundles, whereas the affiliation-view graph captures information rooted in bundle affiliations. To handle the extremely skewed distribution, CoHeat strategically leverages both views in its predictions, emphasizing affiliation-view for less popular bundles since they provide richer information than

Figure 1. (a) Extremely skewed distribution of bundle interactions in real-world datasets (data statistics are summarized in Table 1). (b-c) For an unpopular bundle, history-view provides insufficient information while affiliation-view provides sufficient information.

the sparse history-view, as shown in Figures 0(b) and 0(c). In addition, to effectively learn the affiliation-view representations which are fully used for cold-start bundles, CoHeat exploits a curriculum learning approach that gradually shifts the training focus from the history-view to the affiliation-view. CoHeat further exploits a contrastive learning approach to align the representations of the two views effectively.

Our contributions are summarized as follows:

* **Problem.** To our knowledge, this is the first work that tackles the cold-start problem in bundle recommendation, a challenging problem of significant impact in real-world scenarios.
* **Method.** We propose CoHeat, an accurate method for cold-start bundle recommendation. CoHeat effectively treats the extremely skewed distribution of interactions in order to accurately recommend cold-start bundles based on their affiliations.
* **Experiments.** We experimentally show that CoHeat provides the state-of-the-art performance achieving up to 193% higher nDCG@20 compared to the best competitor in cold-start bundle recommendation (see Figure 2).

## 2. Preliminaries

### Problem Definition

The problem of cold-start bundle recommendation is defined as follows. Let \(\mathcal{U}\), \(\mathcal{B}\), and \(\mathcal{I}\) be the sets of users, bundles, and items, respectively. Among the bundles, \(\mathcal{B}_{w}\subset\mathcal{B}\) refers to the warm-start bundles that have at least one historical interaction with users, while \(\mathcal{B}_{c}=\mathcal{B}\setminus\mathcal{B}_{w}\) represents the cold-start bundles that lack any historical interaction with users. The observed user-bundle interactions, user-item interactions, and bundle-item affiliations are respectively defined as \(\mathcal{X}=\{(u,b)|u\in\mathcal{U},b\in\mathcal{B}_{w}\}\), \(\mathcal{Y}=\{(u,i)|u\in\mathcal{U},i\in I\}\), and \(\mathcal{Z}=\{(b,i)|b\in\mathcal{B},i\in I\}\). Given \(\{\mathcal{X},\mathcal{Y},\mathcal{Z}\}\), our goal is to recommend \(k\) bundles from \(\mathcal{B}\) to each user \(u\in\mathcal{U}\). Note that the given interactions are observed only for warm bundles but the objective includes recommending also cold bundles to users.

The central challenge in cold-start bundle recommendation, compared to traditional bundle recommendation, lies in accurately predicting the relationship between a user \(u\in\mathcal{U}\) and a cold-start bundle \(b\in\mathcal{B}_{c}\) in the absence of any historical interactions of \(b\). Hence, the crux of addressing the problem is to effectively estimate the representations of cold-start bundles using their affiliation information.

### Curriculum Learning

Curriculum learning, inspired by human learning, structures training from simpler to more complex tasks, unlike standard approaches that randomize task order (Bang et al., 2019; Chen et al., 2020). Its effectiveness has been proven in various domains, including computer vision (Zhu et al., 2019; Wang et al., 2020), natural language processing (Liu et al., 2019; Wang et al., 2020), robotics (Liu et al., 2019; Wang et al., 2020), and recommender systems (Chen et al., 2020; Chen et al., 2020).

In this work, we harness curriculum learning to enhance the learning process of user-bundle relationships. We initiate with a focus on the more straightforward history-view embeddings and then progressively shift attention to the intricate affiliation-view embeddings. This strategy stems from the ease of learning history-view embeddings, which directly capture collaborative signals from historical interactions. In contrast, affiliation-view embeddings are more complicated due to their dependence on the representations of affiliated items.

### Contrastive Learning

Contrastive learning aims to learn meaningful embeddings by distinguishing between similar and dissimilar data samples. It has consistently demonstrated superior performance across a range of research fields, including computer vision (Zhu et al., 2019; Wang et al., 2020), natural language processing (Liu et al., 2019; Wang et al., 2020), and recommender systems (Chen et al., 2020; Wang et al., 2020). Specifically, CrossCBR (Liu et al., 2019) recently achieved a good performance in bundle recommendation by regularizing embeddings of users and bundles using InfNoCE (Wang et al., 2020) between history-view and affiliation-view.

However, CrossCBR aligns the two views while equally treating them in prediction. In contrast, our work adaptively modulates the weights of these views based on bundle popularity, thereby facilitating information transfer from the more informative view

Figure 2. Performance comparison between CoHeat and competitors on three real-world datasets: Youshu, NetEase, and iFashion. The performance is evaluated through Recall@20 for all experiments. We mark cold-start methods as orange, and warm-start methods as red. CoHeat demonstrates superior performance over existing methods in both cold and warm settings, with a notable advantage in outperforming competitors.

to its sparser counterpart. Additionally, in our contrastive learning approach, we utilize the alignment and uniformity loss (Krizhevsky et al., 2014). This has been shown to surpass InfoNCE in various applications (Krizhevsky et al., 2014; Krizhevsky et al., 2014), as it directly optimizes the core perspectives of contrastive learning.

## 3. Proposed Method

### Overview

We address the following challenges to achieve high performance on cold-start bundle recommendation.

1. **Handling highly skewed interactions.** Previous works overly depend on history-view representations, which are unreliable if bundles have sparse interactions. How can we effectively learn the representations from highly skewed interactions?
2. **Effectively learning affiliation-view representations.** Despite the ample information provided by the affiliation-view, multiple items in a bundle complicate learning of these representations. How can we effectively learn the affiliation-view representations?
3. **Bridging the gap between two view representations.** Aligning history-view and affiliation-view is crucial, as we estimate future interactions of cold bundles only using their affiliations. How can we effectively reconcile these two view representations?

To address these challenges, we propose CoHeat (Popularity-based Coalescence and Curriculum Heating) with the following main ideas.

1. **Popularity-based coalescence.** For the score between users and bundles, we propose the coalescence of two view scores, with less popular bundles relying more on affiliation-view scores and less on history-view scores.
2. **Curriculum heating.** We propose a curriculum learning approach that focuses initially on training representations using the history-view, gradually shifting the focus to the affiliation-view.
3. **Representation alignment and uniformity.** We exploit a representation alignment and uniformity approach to effectively reconcile the history-view and affiliation-view representations.

Figure 3 depicts the schematic illustration of CoHeat. Given user-bundle interactions, user-item interactions, and bundle-item affiliations, CoHeat forms two graph-based views. Then, it predicts user-bundle scores by coalescing scores from both views based on bundle popularity. During training, CoHeat prioritizes history-view initially, transitioning progressively to affiliation-view via curriculum heating. CoHeat also exploits alignment and uniformity loss to regularize both views.

### Two Graph-based Views

The objective of bundle recommendation is to estimate the relationship between users and bundles by learning their latent representations. We utilize graph-based representations of users and bundles to fully exploit the given user-bundle interactions, user-item interactions, and bundle-item affiliations. We construct history-view and affiliation-view graphs and use LightGCN (Hu et al., 2017) to obtain embeddings of users and bundles (Hu et al., 2018).

**History-view representation and score.** In history-view, we aim to capture the behavior signal between users and bundles. Specifically, we construct a bipartite graph using user-bundle interactions, and propagate the historical information using a LightGCN.

Figure 3. Overview of CoHeat (see Section 3 for details).

The \(k\)'th layer of the LightGCN is computed as follows:

\[\begin{split}\mathbf{h}_{u}^{(k)}&=\sum_{b\in\mathcal{ N}_{u}}\frac{1}{\sqrt{|\mathcal{N}_{u}|}\sqrt{|\mathcal{N}_{b}|}}\,\mathbf{h}_{b}^{(k-1)}, \\ \mathbf{h}_{b}^{(k)}&=\sum_{u\in\mathcal{N}_{b}} \frac{1}{\sqrt{|\mathcal{N}_{b}|}\sqrt{|\mathcal{N}_{u}|}}\,\mathbf{h}_{u}^{(k -1)},\end{split} \tag{1}\]

where \(\mathbf{h}_{u}^{(k)},\mathbf{h}_{b}^{(k)}\in\mathbb{R}^{d}\) are the embeddings of user \(u\) and bundle \(b\) at \(k\)'th layer, respectively; \(\mathcal{N}_{u}\) and \(\mathcal{N}_{b}\) are the sets of user \(u\)'s neighbors and bundle \(b\)'s neighbors in the user-bundle graph, respectively. \(\mathbf{h}_{u}^{(0)},\mathbf{h}_{b}^{(0)}\in\mathbb{R}^{d}\) are randomly initialized before the training of the model. We obtain the history-view representations of user \(u\) and bundle \(b\) by aggregating the embeddings from all layers with a weighting approach that places greater emphasis on the lower layers as follows:

\[\mathbf{h}_{u}=\sum_{k=0}^{K}\frac{1}{k+1}\mathbf{h}_{u}^{(k)},\mathbf{h}_{b }=\sum_{k=0}^{K}\frac{1}{k+1}\mathbf{h}_{b}^{(k)}, \tag{2}\]

where \(\mathbf{h}_{u},\mathbf{h}_{b}\in\mathbb{R}^{d}\) are the history-view embeddings of user \(u\) and bundle \(b\), respectively; \(K\) denotes the last layer. Finally, the history-view score between user \(u\) and bundle \(b\) is defined as \(\mathbf{h}_{ub}=\mathbf{h}_{u}^{\mathrm{T}}\mathbf{h}_{b}\).

**Affiliation-view representation and score.** In affiliation-view, we aim to learn the relationship between users and bundles from the perspective of item affiliations. Specifically, we construct a bipartite graph using user-item interactions, and propagate the historical information using another LightGCN. Then, we obtain bundle representations by aggregating the affiliated items' representations. The \(k\)'th layer of the LightGCN is computed as follows:

\[\begin{split}\mathbf{a}_{u}^{(k)}&=\sum_{i\in \mathcal{N}_{u}}\frac{1}{\sqrt{|\mathcal{N}_{u}^{\prime}|}\sqrt{|\mathcal{N}_{ i}|}}\,\mathbf{a}_{i}^{(k-1)},\\ \mathbf{a}_{i}^{(k)}&=\sum_{u\in\mathcal{N}_{i}} \frac{1}{\sqrt{|\mathcal{N}_{i}|}}\,\mathbf{a}_{u}^{(k-1)},\end{split} \tag{3}\]

where \(\mathbf{a}_{u}^{(k)},\mathbf{a}_{i}^{(k)}\in\mathbb{R}^{d}\) are the embeddings of user \(u\) and item \(i\) at \(k\)'th layer, respectively; \(\mathcal{N}_{u}^{\prime}\) and \(\mathcal{N}_{i}\) are the sets of user \(u\)'s neighbors and item \(i\)'s neighbors in the user-item graph, respectively. \(\mathbf{a}_{u}^{(0)},\mathbf{a}_{i}^{(0)}\in\mathbb{R}^{d}\) are randomly initialized before the training. We obtain the affiliation-view representations of user \(u\) and item \(i\) by aggregating the embeddings from all layers with a weighting approach as follows:

\[\begin{split}\mathbf{a}_{u}=\sum_{k=0}^{K}\frac{1}{k+1}\mathbf{a }_{u}^{(k)},\mathbf{a}_{i}=\sum_{k=0}^{K}\frac{1}{k+1}\mathbf{a}_{i}^{(k)}, \end{split} \tag{4}\]

where \(\mathbf{a}_{u},\mathbf{a}_{i}\in\mathbb{R}^{d}\) are the affiliation-view embeddings of user \(u\) and item \(i\), respectively; \(K\) indicates the last layer. We then obtain the affiliation-view representations of bundle \(b\) by an average pooling as \(\mathbf{a}_{b}=\frac{1}{|\mathcal{N}_{b}|}\sum_{i\in\mathcal{N}_{b}}\mathbf{a }_{i}\), where \(\mathcal{N}_{b}^{\prime}\) is the set of bundle \(b\)'s affiliated items. Finally, the affiliation-view score between user \(u\) and bundle \(b\) is defined as \(a_{ub}=\mathbf{a}_{u}^{\mathrm{T}}\mathbf{a}_{b}\).

### Popularity-based Coalescence

For recommending bundles to users, our objective is to estimate the final score \(\hat{y}_{ub}\in\mathbb{R}\) between user \(u\) and bundle \(b\) using scores \(h_{ub}\) and \(a_{ub}\), derived from the two distinct views. However, real-world datasets present an inherent challenge of handling the extremely skewed distribution of interactions between users and bundles, as illustrated in Figure 0(a). While both views are informative, many unpopular bundles are underrepresented in the history-view due to the insufficient interactions as illustrated in Figure 0(b). In contrast, they are often sufficiently represented in the affiliation-view, as depicted in Figure 0(c). A uniform weighting strategy for both views, as in CrossCBR, risks amplifying biases inherent to the history-view, especially for the unpopular bundles. This predicament is further exacerbated for cold-start bundles devoid of history-view data.

To deal with this challenge, we propose two desired properties for the user-bundle relationship score \(\hat{y}_{ub}\).

_Property 1_ (History-view influence mitigation): The influence of history-view score should be mitigated as a bundle's interaction number decreases, i.e. \(\frac{\partial\hat{y}_{ub}}{\partial h_{ub}}<\frac{\partial\hat{y}_{ub^{\prime} }}{\partial h_{ub^{\prime}}}\) if \(n_{b}<n_{b^{\prime}}\) where \(n_{b}\) is the number of interactions of bundle \(b\).

_Property 2_ (Affiliation-view influence amplification): The influence of affiliation-view score should be amplified as a bundle's interaction number decreases, i.e. \(\frac{\partial\hat{y}_{ub}}{\partial h_{ub}}>\frac{\partial\hat{y}_{ub^{\prime} }}{\partial u_{ub^{\prime}}}\) if \(n_{b}<n_{b^{\prime}}\) where \(n_{b}\) is the number of interactions of bundle \(b\).

Properties 1 and 2 are crucial in achieving a balanced interplay between the history-view and affiliation-view scores based on bundle popularities. Specifically, they ensure a heightened emphasis on the affiliation-view over the history-view for less popular bundles.

We propose the user-bundle relationship score \(\hat{y}_{ub}\) that satisfies the two desired properties by weighting the two scores \(h_{ub}\) and \(a_{ub}\) based on bundle popularities as follows:

\[\hat{y}_{ub}=\gamma_{b}h_{ub}+(1-\gamma_{b})a_{ub}, \tag{5}\]

where \(\gamma_{b}\in[0,1]\), which is defined in the next subsection, denotes a weighting coefficient such that \(\gamma_{b}>\gamma_{b^{\prime}}\) if \(n_{b}>n_{b^{\prime}}\). A smaller value of \(\gamma_{b}\) (i.e. a smaller value of \(n_{b}\)) ensures that the score \(\hat{y}_{ub}\) is predominantly influenced by the affiliation-view score \(a_{ub}\). We show in Lemmas 3.1 and 3.2 that Equation (5) satisfies all the desired properties.

**Lemma 3.1**.: _Equation (5) satisfies Property 1._

Proof.: \(\frac{\partial\hat{y}_{ub}}{\partial h_{ub}}=\gamma_{b}\). Thus, \(\frac{\partial\hat{y}_{ub}}{\partial h_{ub}}<\frac{\partial\hat{y}_{ub^{\prime} }}{\partial h_{ub^{\prime}}}\) if \(n_{b}<n_{b^{\prime}}\) because \(\gamma_{b}<\gamma_{b^{\prime}}\). 

**Lemma 3.2**.: _Equation (5) satisfies Property 2._

Proof.: \(\frac{\partial\hat{y}_{ub}}{\partial a_{ub}}=1-\gamma_{b}\). Thus, \(\frac{\partial\hat{y}_{ub}}{\partial a_{ub}}>\frac{\partial\hat{y}_{ub^{\prime} }}{\partial a_{ub^{\prime}}}\) if \(n_{b}<n_{b^{\prime}}\) because \(1-\gamma_{b}>1-\gamma_{b^{\prime}}\). 

### Curriculum Heating

Despite the ample information provided by the affiliation-view, multiple items in a bundle complicate the learning of affiliation-view representations. This difficulty arises because accurate representation of a bundle necessitates well-represented embeddings of its all affiliated items. On the other side, the history-view representation is relatively straightforward to learn. This simplicity arises because we encapsulate each bundle's historical characteristics into a single embedding rather than understanding the intricate composition of the bundle.

Hence, we modify Equation (5) by exploiting a curriculum learning approach that focuses initially on training history-view representations, and gradually shifts the focus to the affiliation-view representations as follows:

\[\hat{g}_{ub}^{(t)}=Y_{b}^{(t)}h_{ub}+(1-Y_{b}^{(t)})a_{ub}, \tag{6}\]

where \(\hat{g}_{ub}^{(t)}\in\mathbb{R}\) is the estimated relationship score between user \(u\) and bundle \(b\) at epoch \(t\). \(Y_{b}^{(t)}\in\mathbb{R}\) is defined as \(Y_{b}^{(t)}=\tanh\left(\frac{n_{b}}{\hat{y}^{(t)}}\right)\), where \(n_{b}\) is the number of interactions of bundle \(b\), and \(\hat{y}^{(t)}>0\) is the temperature at epoch \(t\). Note that \(Y_{b}^{(t)}\) lies within the interval \([0,1]\) because \(\frac{n_{b}}{\hat{y}^{(t)}}\geq 0\). Then, we incrementally raise the temperature \(\hat{y}^{(t)}\) up to the maximum temperature as follows:

\[\hat{y}^{(t)}=e^{t/T},t\cdot 0\to T, \tag{7}\]

where \(t,T\in\mathbb{R}\) are the current and the maximum epochs of the training process, and \(\epsilon>1\) is the hyperparameter of the maximum temperature. In the initial epochs of training, \(Y_{b}^{(t)}\) is large since \(t\) is small. As a result, the score \(\hat{y}_{ub}^{(t)}\) relies more heavily on \(h_{ub}\) than \(a_{ub}\). However, as the training progresses and \(t\) increases, \(Y_{b}^{(t)}\) diminishes, shifting the emphasis from \(h_{ub}\) to \(a_{ub}\). This heating mechanism is applied to all bundles regardless of their popularity. Furthermore, we show in Lemmas 3.3 and 3.4 that Equation (6) still satisfies the two desired properties.

**Lemma 3.3**.: _Equation (6) satisfies Property 1._

Proof.: \(\frac{\partial\hat{g}_{ub}^{(t)}}{\partial h_{ub}}=\tanh\left(\frac{n_{b}}{ \hat{y}^{(t)}}\right)\). Thus, \(\frac{\partial\hat{g}_{ub}^{(t)}}{\partial h_{ub}}<\frac{\partial\hat{g}_{ub ^{(t)}}^{(t)}}{\partial h_{ub^{\prime}}}\) if \(n_{b}<n_{b^{\prime}}\) because \(\hat{y}^{(t)}\) is the same for all bundles at epoch \(t\) and \(tanh(\cdot)\) is an increasing function. 

**Lemma 3.4**.: _Equation (6) satisfies Property 2._

Proof.: \(\frac{\partial\hat{g}_{ub}^{(t)}}{\partial h_{ub}}=1-\tanh\left(\frac{n_{b}}{ \hat{y}^{(t)}}\right)\). Thus, \(\frac{\partial\hat{g}_{ub}^{(t)}}{\partial a_{ub}}\geq\frac{\partial\hat{g}_{ ub^{\prime}}^{(t)}}{\partial a_{ub^{\prime}}}\) if \(n_{b}<n_{b^{\prime}}\) because \(\hat{y}^{(t)}\) is the same for all bundles at epoch \(t\) and \(1-tanh(\cdot)\) is a decreasing function. 

### Representation Alignment and Uniformity

While the history-view and affiliation-view are crafted to capture distinct representations, aligning the two views is essential, especially when predicting future interactions of cold bundles solely based on affiliation-view representations. To achieve this, we exploit a contrastive learning-based approach that reconciles the two views. Specifically, we use the alignment and uniformity loss (Srivastava et al., 2015) as a regularization for the representations of the two views. We firstly \(l_{2}\)-normalize the embeddings of the two views as follows:

\[\hat{\mathbf{h}}_{u}=\frac{\mathbf{h}_{u}}{\|\mathbf{h}_{u}\|_{2}},\hat{ \mathbf{a}}_{u}=\frac{\mathbf{a}_{u}}{\|\mathbf{a}_{u}\|_{2}},\hat{\mathbf{h}} _{b}=\frac{\mathbf{h}_{b}}{\|\mathbf{h}_{b}\|_{2}},\hat{\mathbf{a}}_{b}=\frac {\mathbf{a}_{b}}{\|\mathbf{a}_{b}\|_{2}}, \tag{8}\]

where \(\mathbf{h}_{u},\mathbf{h}_{b}\in\mathbb{R}^{d}\) are history-view representations of user \(u\) and bundle \(b\), respectively; \(\mathbf{a}_{u},\mathbf{a}_{b}\in\mathbb{R}^{d}\) are affiliation-view representations of user \(u\) and bundle \(b\), respectively. Then, we define an alignment loss as follows:

\[l_{align}=\mathop{\mathbb{E}}_{u\text{-}puser}\|\hat{\mathbf{h}}_{u}-\hat{ \mathbf{a}}_{u}\|_{2}^{2}+\mathop{\mathbb{E}}_{b\text{-}pbundle}\|\hat{ \mathbf{h}}_{b}-\hat{\mathbf{a}}_{b}\|_{2}^{2}, \tag{9}\]

where \(p_{user}\) and \(p_{bundle}\) are the distributions of users and bundles, respectively. The alignment loss makes the embeddings of the two views close to each other for each user and bundle. We also define a uniformity loss as follows:

\[l_{uniform} =\log\mathop{\mathbb{E}}_{u\text{-}puser}e^{-2\|\hat{\mathbf{h}}_{u }-\hat{\mathbf{h}}_{u^{\prime}}\|_{2}^{2}}\] \[+\log\mathop{\mathbb{E}}_{u\text{-}puser}e^{-2\|\hat{\mathbf{h}}_{ u}-\hat{\mathbf{a}}_{u^{\prime}}\|_{2}^{2}}\] \[+\log\mathop{\mathbb{E}}_{b\text{,}b^{\prime}}\mathop{\mathbb{E} }_{pbundle}e^{-2\|\hat{\mathbf{h}}_{b}-\hat{\mathbf{h}}_{b^{\prime}}\|_{2}^{2}}\] \[+\log\mathop{\mathbb{E}}_{b\text{,}b^{\prime}}\mathop{\mathbb{E} }_{pbundle}e^{-2\|\hat{\mathbf{h}}_{b^{\prime}}-\hat{\mathbf{a}}_{b^{\prime}}\|_{2}^{ 2}}, \tag{10}\]

where \(u^{\prime}\) and \(b^{\prime}\) denote a user and a bundle distinct from \(u\) and \(b\), respectively. The uniformity loss ensures distinct representations for different users (or bundles) by scattering them across the space. Finally, we define the contrastive loss for the two views as follows:

\[\mathcal{L}_{AU}=l_{align}+l_{uniform}. \tag{11}\]

### Objective Function and Training

To effectively learn the user-bundle relationship, we utilize Bayesian Personalize Ranking (BPR) loss (Kang et al., 2017), which is the most widely used loss owing to its powerfulness, as follows:

\[\mathcal{L}_{BPR}^{(t)}=\mathop{\mathbb{E}}_{(u,b^{\prime},b^{\prime})\text{-}p _{data}}-\ln\sigma(\hat{y}_{ub^{\prime}}^{(t)}-\hat{y}_{ub^{\prime}}^{(t)}), \tag{12}\]

where \(p_{data}\) is the data distribution of user-bundle interactions, with \(u\) denoting a user, \(b^{+}\) indicating a positive bundle, and \(b^{-}\) representing a negative bundle. We define the final objective function as follows:

\[\mathcal{L}^{(t)}=\mathcal{L}_{BPR}^{(t)}+\lambda_{1}\mathcal{L}_{AU}+\lambda_{2} \|\Theta\|_{2}, \tag{13}\]

where \(\lambda_{1},\lambda_{2}\in\mathbb{R}\) are balancing hyperparameters for the terms, and \(\Theta\) denotes trainable parameters of CoHeat. For the distributions \(p_{user}\) and \(p_{bundle}\), we use in-batch sampling which selects samples from the training batch of \(p_{data}\) rather than the entire dataset. This approach has empirically demonstrated to mitigate the training bias in prior studies (Srivastava et al., 2015; Wang et al., 2016). All the parameters are optimized in an end-to-end manner through the optimization. We also adopt an edge dropout (Kang et al., 2017; Wang et al., 2016) while training to enhance the performance robustness.

## 4. Experiments

In this section, we perform experiments to answer the following questions.

1. **Comparison with cold-start methods.** Does CoHeat show superior performance in comparison to other cold-start methods in bundle recommendation?
2. **Comparison with warm-start methods.** Does CoHeat show similar performance in warm-start bundle recommendation compared with baselines, although CoHeat is a cold-start bundle recommendation method?
3. **Ablation study.** How do the main ideas of CoHeat affect the performance?
4. **Effect of the maximum temperature.** How does the maximum temperature \(\epsilon\), the critical hyperparameter, affect the performance of CoHeat?

[MISSING_PAGE_FAIL:6]

baselines in the warm-start scenario. This indicates CoHeat effectively learns representations from both history-view and affiliation-view by treating the extremely skewed distribution of user-bundle interactions. For the baselines, the performance improves when contrastive learning is used as exemplified in SGL, SimGCL, LightGCL, and CrossCBR. Additionally, graph-based models such as LightGCN, SGL, SimGCL, LightGCL, BundleNet, BGCN, and CrossCBR excel over other non-graph-based models. In light of these observations, CoHeat strategically exploits a graph-based modeling approach and harnesses the power of contrastive learning. This makes CoHeat robustly achieve the highest performance across diverse scenarios.

### Ablation Study (Q3)

Table 4 provides an ablation study that compares CoHeat with its three variants CoHeat-_PC_, CoHeat-_CH_, and CoHeat-_AU_. This study is conducted in the cold-start scenario, which is the primary focus of our work. In CoHeat-_PC_, we remove the influence of popularity-based coalescence by setting the value of \(\gamma_{b}^{(t)}\) in Equation (5) to a constant 0.5. For CoHeat-_CH_, we exploit an anti-curriculum learning strategy. The temperature in Equation (7) is defined as \(t:T\to 0\), initiating the learning process with the affiliation-view and gradually shifting the focus to the history-view. For CoHeat-_AU_, we omit \(\mathcal{L}_{AU}\) from Equation (13), thereby excluding the contrastive learning between the two views. As shown in the table, CoHeat consistently outperforms all the variants, which verifies all the main ideas help improve the performance. In particular, CoHeat-_PC_ shows a severe performance drop, justifying the importance of satisfying Properties 1 and 2 when addressing the extreme skewness inherent in cold-start bundle recommendation.

### Effect of the Maximum Temperature (Q4)

The maximum temperature \(\epsilon\) in Equation (7) is the most influential hyperparameter of CoHeat since it directly affects both popularity-based coalescence and curriculum heating. Accordingly, we analyze the influence of \(\epsilon\) in cold-start scenario on real-world datasets, as depicted in Figure 4. As shown in the figure, CoHeat shows low performance for the extreme low temperature because the representations of affiliation-view are not sufficiently learned. For the extreme high temperature, the performance degrades because the speed of the curriculum is too fast to fully learn the representation of the two views. As a result, we set \(\epsilon\) to \(10^{4}\) for all datasets since it shows the best performance.

## 5. Related Works

**Bundle recommendation.** Our work focuses on the cold-start problem in bundle recommendation. Previous works can be categorized based on their modeling structures: matrix factorization-based models (Krizhevsky et al., 2014; He et al., 2015; He et al., 2016) and graph learning-based models (Krizhevsky et al., 2014; He et al., 2015; He et al., 2016; He et al., 2017). Such methods operate under the assumption that all bundles have historical interactions, which makes them ill-suited for tackling the cold-start problem. However, in real-world scenarios, new bundles are introduced daily, leading to an inherent cold-start challenge. Our work addresses this significant yet overlooked issue, recognizing its potential impact on the field.

**Cold-start recommendation.** The cold-start problem, a long-standing challenge in recommender systems, focuses on recommending cold-start items that have yet to be interacted with users. Existing works are mainly divided into generative methods (Krizhevsky et al., 2014; He et al., 2015; He et al., 2016; He et al., 2017), dropout-based methods (Krizhevsky et al., 2014; He et al., 2015; He et al., 2016), meta-learning methods (He et al., 2016), and constraint-based methods (Krizhevsky et al., 2014; He et al., 2015; He et al., 2016; He et al., 2017). However, such prior works have not explicitly addressed the highly skewed distribution of interactions, a critical aspect in bundle recommendation. Thus, our work excels over these methods in cold-start bundle recommendation by effectively considering the skewed distribution during training.

\begin{table}
\begin{tabular}{l|c c|c c|c c} \hline \hline \multirow{3}{*}{**Model**} & \multicolumn{2}{c|}{**Youshu**} & \multicolumn{2}{c|}{**NetEase**} & \multicolumn{2}{c}{**iFashion**} \\  & Recall & nDCG & Recall & nDCG & Recall & nDCG \\  & @20 & @20 & @20 & @20 & @20 & @20 & @20 \\ \hline MFBPR (He et al., 2016) &.1959 &.1117 &.0355 &.0181 &.0752 &.0542 \\ LightGCN (He et al., 2016) &.2286 &.1344 &.0496 &.0254 &.0837 &.0612 \\ SGL (He et al., 2016) &.2568 &.1527 &.0687 &.0368 &.0933 &.0690 \\ SimGCL (He et al., 2016) &.2691 &.1593 &.0710 &.0377 &.0919 &.0677 \\ LightGCL (He et al., 2016) &.2712 &.1607 &.0722 &.0388 &.0943 &.0686 \\ \hline DAM (He et al., 2016) &.2082 &.1198 &.0411 &.0210 &.0629 &.0450 \\ BundlesNet (He et al., 2016) &.1895 &.1125 &.0391 &.0201 &.0626 &.0447 \\ BGCN (Krizhevsky et al., 2014) &.2347 &.1345 &.0491 &.0258 &.0733 &.0531 \\ CrossCBR (He et al., 2016) &.2776 &.1641 &.0791 &.0433 &.1133 &.0875 \\ \hline
**CoHeat (ours)** & **.2804** & **.1646** & **.0847** & **.0455** & **.1156** & **.0876** \\ \hline \hline \end{tabular}
\end{table}
Table 3. Performance comparison of CoHeat and baseline warm-start methods on three real-world datasets.

Figure 4. Effect of the maximum temperature \(\epsilon\).

\begin{table}
\begin{tabular}{l|c c|c c|c c} \hline \hline \multirow{3}{*}{**Model**} & \multicolumn{2}{c|}{**Youshu**} & \multicolumn{2}{c|}{**NetEase**} & \multicolumn{2}{c}{**iFashion**} \\  & Recall & nDCG & Recall & nDCG & Recall & nDCG \\  & @20 & @20 & @20 & @20 & @20 & @20 \\ \hline MFBPR (He et al., 2016) &.1959 &.1117 &.0355 &.0181 &.0752 &.0542 \\ LightGCN (He et al., 2016) &.2286 &.1344 &.0496 &.0254 &.0837 &.0612 \\ SGL (He et al., 2016) &.2568 &.1527 &.0687 &.0368 &.0933 &.0690 \\ SimGCL (He et al., 2016) &.2691 &.1593 &.0710 &.0377 &.0919 &.0677 \\ LightGCL (He et al., 2016) &.2712 &.1607 &.0722 &.0388 &.0943 &.0686 \\ \hline DAM (He et al., 2016) &.2082 &.1198 &.0411 &.0210 &.0629 &.0450 \\ BundlesNet (He et al., 2016) &.1895 &.1125 &.0391 &.0201 &.0626 &.0447 \\ BGCN (Krizhevsky et al., 2014) &.2347 &.1345 &.0491 &.0258 &.0733 &.0531 \\ CrossCBR (He et al., 2016) &.2776 &.1641 &.0791 &.0433 &.1133 &.0875 \\ \hline
**CoHeat (ours)** & **.2804** & **.1646** & **.0847** & **.0455** & **.1156** & **.0876** \\ \hline \hline \end{tabular}
\end{table}
Table 4. Ablation study of CoHeat in cold-start scenario which is our main target.

[MISSING_PAGE_FAIL:8]