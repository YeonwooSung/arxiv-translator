# LIME: Attention Regularization을 통한 로컬화 이미지 편집

확산 모형들

Eenis Simsar\({}^{1}\) Alessio Tonioni\({}^{3,}\) Yongqin Xian\({}^{3,}\) Thomas Hofmann\({}^{1}\) Federico Tombari\({}^{2,3}\)

\({}^{1}\)ETH Zurich - DALAB

뮌헨대학

\({}^{3}\)Google Switzerland

Equal advising.

프로젝트 페이지는 _[https://enisimsar.github.io/LIME/_](https://enisimsar.github.io/LIME/_)에서 찾을 수 있습니다.

###### Abstract

확산 모델(DM)은 텍스트-이미지 생성의 최근 발전과 함께 고품질의 다양한 이미지를 생성할 수 있는 능력으로 인해 주목을 받고 있다. 연구 초점은 이제 DM의 제어 가능성으로 이동하고 있다. 이 도메인 내의 중요한 도전은 이미지의 특정 영역이 콘텐츠의 나머지 부분에 영향을 미치지 않고 수정되는 로컬화된 편집이다. 본 논문은 사용자 지정 관심 영역(RoI)이나 추가 텍스트 입력이 필요하지 않은 확산 모델에서 지역화된 이미지 편집을 위한 LIME을 소개한다. 제안된 방법은 사전 훈련된 방법과 간단한 클러스터링 기법을 사용하여 정확한 의미론적 분할 맵을 얻는다. 그런 다음 교차 주의 지도를 활용하여 로컬 편집을 위해 이러한 세그먼트를 세분화합니다. 마지막으로, 노이즈 제거 단계 동안 RoI에서 관련 없는 교차 주의 점수를 처벌하여 로컬 편집을 보장하는 새로운 교차 주의 정규화 기술을 제안한다. 본 논문에서 제안하는 방법은 재학습과 미세조정을 하지 않고 다양한 벤치마크 편집에서 기존 방법들의 성능을 지속적으로 향상시킨다.

## 1 Introduction

확산 모델(DM)은 최근 텍스트-이미지 변환의 진보에 힘입어 고품질일 뿐만 아니라 풍부하게 변하는 이미지를 생성하는 데 괄목할 만한 성공을 거두었다[19, 36, 38, 40]. 생성 능력을 넘어 이러한 모델의 제어 가능성 측면에 대한 연구 관심이 증가하고 있다[2, 6, 8, 17, 33, 53]. 이는 개인화된 이미지 생성[14, 39, 47], 상황 인식 인페인팅[26, 31, 50], 텍스트 편집에 대한 이미지 변환[2, 6, 8, 17, 21, 27]과 같은 작업에 DM의 힘을 활용하는 다양한 편집 기술의 탐색으로 이어졌다. 이러한 개발은 DM의 다양성과 다양한 이미지 편집 응용 프로그램을 위한 기초 도구 역할을 할 가능성을 강조한다.

본 논문에서는 이미지에서 관심 영역을 식별하고 수정하는 지역화 편집에 명시적으로 초점을 맞춘 텍스트 유도 이미지 편집 작업을 다룬다. 이것은 크기에 관계없이 텍스트 지침에 따라 수행되며 주변 지역의 컨텍스트를 보존합니다. 어려움은 한 영역에 대해 의도된 변화가 부주의하게 다른 영역에 영향을 미칠 수 있는 이러한 모델 내에서 이미지 표현의 얽힌 특성에서 발생한다[6, 17, 27, 53]. 기존의 방법들은 종종 편집 영역을 정확히 지적하기 위해 타겟 영역, _i.e., RoI(Region of Interest)를 마스킹하거나 추가 텍스트 정보, _e.g., 관심 객체들을 제공하는 것과 같은 추가 사용자 입력에 의존한다[2, 8]. 그러나, 이러한 접근법들은 복잡성을 도입하고, 끊김없는 편집에 필요한 정밀도를 보장하지 않는다. 그림 1은 현재 방법이 아직 적용되지 않은 균형인 전체 이미지를 변경하지 않고 로컬화된 편집을 강조한다. 더 직관적이고 효과적으로 지역화된 편집을 발전시키는 것은 중추적인 방향으로 남아 있다.

추가 감독, 사용자 입력 또는 모델 재교육/미세 조정 필요 없이 미리 훈련 된 Instruct-Pix2Pix [6]을 활용 하는 _LIME_을 도입 하 여 지역화된 이미지 편집의 문제를 해결 합니다. 최근 연구[34, 44, 49]는 확산 모델이 중간 특징 내에서 의미를 인코딩할 수 있음을 보여주었다. LIME은 이러한 기능을 활용하여 세그먼트를 식별한 다음 지침에서 파생된 주의 점수를 활용하여 RoI를 추출한다. 다른 연구 [1, 7]은 주의력 기반 지침이 이미지 구성에 미치는 유의한 영향을 보여주었다. 이에 따라, LIME은 어텐션 스코어를 정규화하여 편집의 범위를 제한하여 디덴서링 및 로컬리제이션된 편집이 가능하도록 하는 것을 목적으로 한다. 이 두 가지 작업 라인을 개선함으로써 LIME은 그림과 같이 보다 효과적인 로컬 편집을 제공할 뿐만 아니라. 1은 또한 4개의 다른 벤치마크 데이터세트에서 현재 최신 방법을 정량적으로 능가함으로써 주목할 만한 발전을 보여준다.

파이프라인에는 두 단계가 포함되어 있습니다. 먼저 입력 이미지의 의미 세그먼트를 찾습니다. 이는 중간 특징들에 인코딩된 의미 정보에 기초하여 달성된다. 그리고 교차 주의 점수가 큰 세그먼트를 편집 지시로 결합하여 편집 대상 영역을 식별한다. 일단 편집될 영역, 즉 RoI를 격리하면, 제안된 주의 규칙화 기법이 텍스트 토큰에 적용되어 후속 편집이 이미지의 다른 부분에 의도하지 않은 변경을 피하면서 정확하게 포커싱되도록 RoI를 선택적으로 타겟팅한다. 먼저 대상 영역을 정제한 다음 RoI 내에서 편집하는 이 2단계 접근법은 우리의 수정이 정확하고 맥락적으로 일관성이 있음을 보장하여 이미지의 나머지 부분에 대한 의도하지 않은 변경을 방지하면서 편집 프로세스를 단순화한다.

이 연구의 핵심 기여는 다음과 같다.

* 미세 조정 또는 재교육이 필요하지 않아 효율적이고 정확한 로컬 편집이 보장되는 로컬 이미지 편집 기술을 소개합니다.
* 사전 훈련 된 모델의 중간 기능을 활용 하 여 이미지를 분할 하 고 수정이 적용 될 영역을 식별 합니다.
* RoI 내에서 엉킴 해제 및 로컬화된 편집을 달성하는 데 사용되는 주의 규칙화 전략이 제안되어 문맥적으로 일관성 있는 편집이 보장됩니다.

실험 결과, 4개의 벤치마크 데이터 집합[5, 6, 20, 52]에서 본 논문에서 제안한 방법이 기존의 방법들에 비해 질적, 양적으로 우수한 편집 성능을 보였다.

## 2 관련 작업

텍스트 유도 이미지 생성.텍스트-이미지 합성은 이전의 생성적 적대 네트워크(GAN)를 능가하는 확산 모델 덕분에 상당히 발전했다[16, 37, 51]. 주요 개발[10, 19, 43]은 텍스트 입력으로부터 매우 사실적인 이미지를 생성하는 확산 모델을 가져왔다[31, 36, 40]. 특히, 잠재 확산 모델의 도입은 이전 방법[38]의 계산 효율성을 크게 증가시켰다.

확산 모델들을 이용한 이미지 편집. 이미지 편집을 위한 한 방향은 먼저 잠재 공간 내의 입력 이미지를 반전시킨 다음 텍스트 프롬프트를 변경함으로써 원하는 편집을 적용함으로써 사전 훈련된 확산 모델들을 활용하는 것이다[8, 17, 20, 27, 30, 32, 45, 46, 48]. 예를 들어, DirectInversion[20]은 입력 이미지를 반전시킨 후 Prompt2Prompt[17]을 적용하여 원하는 편집을 얻지만, 반전 동안 입력 이미지의 세부 사항을 잃을 수 있다. 한편, 디프 에디트 [8]은 입력 및 출력 캡션들에 대한 예측들의 차이들과 매칭하여 편집을 로컬화하지만 복잡한 명령어들과 투쟁한다. 소음 공간에서 작동하여 편집합니다. 명령어를 사용하여 이미지 편집을 위한 또 다른 방향은 입력 이미지, 명령어 및 원하는 이미지를 포함하는 트리플렛 데이터에 확산 모델을 훈련시키는 것이다[6, 13, 52, 53]. 최신 접근법인 Instruct-Pix2Pix(IP2P)[6]는 트리플렛 데이터세트를 사용하여 명령어를 사용하여 이미지를 편집하기 위한 모델을 학습시킨다. 이전 방법보다 성능이 우수하지만 때로는 얽힌 편집을 생성하기도 합니다. 이 문제를 해결하기 위해, HIVE[53]는 편집된 이미지에 대한 인간의 피드백에 의존하여 사용자가 일반적으로 선호하는 것을 학습하고 이 정보를 사용하여 IP2P를 미세 조정하여 인간의 기대와 더 밀접하게 정렬하는 것을 목표로 한다. 또는, 미리 학습된 IP2P를 활용하여 편집 명령어를 지역화한다. 그리고 잡음 공간[2, 8, 29]을 조작하는 대신 주의 규칙화를 사용하여 국부 편집을 수행함으로써 RoI 내에서 편집이 제한되도록 한다. 추가 데이터, 재교육 또는 미세 조정 없이 전체 프로세스가 수행됩니다.

확산 모델의 의미론 [33, 34, 44, 49]와 같은 연구에서 탐구한 확산 모델의 중간 특징은 의미 정보를 인코딩하는 것으로 나타났다. LD-ZNet[34] 및 ODISE[49]와 같은 최근의 연구는 이러한 모델들의 중간 특징들을 시맨틱 세분화를 위한 트레이닝 네트워크들에 활용한다. 한편, Prompt Mixing(LPM)의 로컬라이징[33]은 세그먼트 식별을 위해 자기-주의 출력에 대한 클러스터링을 이용한다. 이러한 성공으로 인해, 이 방법은 미리 훈련된 중간 특징들을 활용하여 의미론적 분할을 달성하고 편집 명령을 사용하여 국부화된 편집을 적용한다.

## 3 Background

Latent Diffusion Models. SD(Stable Diffusion) [38]은 압축된 잠재공간에서 동작하도록 설계된 LDM(Latent Diffusion Model)이다. 이 공간은 계산 효율을 높이기 위해 미리 훈련된 가변 오토인코더(Variational Autoencoder, VAE)의 병목점에서 정의된다. 잠재 공간에는 가우시안 잡음이 유입되어 잠재 분포 \(z_{t}\)에서 샘플을 생성한다. U-Net 기반의 잡음 제거 구조[10]는 잡음 입력(\(z_{t}\))과 텍스트 컨디셔닝(\(c_{T}\))을 조건으로 하여 영상 복원에 사용된다. 이러한 재구성은 다수의 시간 단계들에 걸쳐 반복적으로 적용되며, 각각은 자기-주의 층들 및 교차-주의 층들의 시퀀스를 포함한다. 셀프-어텐션 계층은 현재 노이즈 이미지 표현을 변환하는 반면, 크로스-어텐션 계층은 텍스트 컨디셔닝을 통합한다.

모든 주의 계층은 쿼리 (\(Q\)), 키 (\(K\)) 및 값 (\(V\))의 세 가지 구성 요소로 구성 됩니다. 교차-어텐션 레이어들에 대해, 교차-어텐션 레이어(_i.e_., 이미지 특징들)에 선행하는 자기-어텐션 레이어의 결과에 선형 변환 \(f_{Q}\)을 적용함으로써 \(Q\)s가 획득된다. 마찬가지로, \(K\)s와 \(V\)s는 선형 변환 \(f_{K}\)과 \(f_{V}\)을 사용하여 텍스트 컨디셔닝 \(c_{T}\)에서 파생된다. 식 (1)은 어텐션 레이어의 수학적 공식화를 보여주는데, 여기서 \(P\)은 어텐션 맵을 나타내며 \(K\)과 \(Q\)의 차원 \(d\)의 제곱근으로 정규화된 \(K\)과 \(Q\)의 내적의 소프트맥스로 구해진다.

\[\text{Attention}(Q,K,V)=P\cdot V, \tag{1}\] \[\text{where }P=\text{Softmax}\left(\frac{QK^{T}}{\sqrt{d}}\right)\]

직관적으로, \(P\)은 어텐션 레이어에서 입력 피처들의 어떤 영역들이 수정될지를 나타낸다. 교차 주의의 경우, 이것은 \(c_{T}\)을 정의하는 컨디셔닝 텍스트 토큰 중 하나에 의해 영향을 받는 이미지의 영역이다. 이러한 주의 맵을 넘어, 우리의 접근법은 또한 최근 연구에서 강조된 바와 같이 풍부한 의미 내용을 포함하는 _중간 특징_\(\phi(z_{t})\)으로 언급된 트랜스포머 레이어의 출력을 활용한다[34, 44, 49]. 본 연구에서는 사전 훈련된 LDM에서 편집을 지역화하기 위해 교차 주의의 \(P\)를 수정하고 중간 기능 \(\phi(z_{t})\)을 활용한다.

InstructPix2Pix.우리의 방법은 텍스트 조건 편집을 위해 훈련된 이미지 대 이미지 변환 네트워크인 InstructPix2Pix(IP2P)[6]에 의존한다. IP2P는 안정확산(Stable Diffusion) 위에 구축되며 입력영상 \(I\)과 텍스트 기반 명령어 \(T\)을 동시에 활용하여 영상 합성을 유도하는 이중조건 프레임워크를 통합하며, 조건화 특징은 영상의 \(c_{I}\)과 텍스트의 \(c_{T}\)이다. 이미지 생성 작업 흐름은 텍스트 조건에 대해 \(s_{T}\)과 이미지 조건에 대해 \(s_{I}\)의 두 개의 개별 계수를 사용하는 분류기 없는 안내(CFG) 전략 [18]을 통해 변조된다. 입력 집합이 다른 개별 U-Net 단계에 해당하는 학습된 네트워크 \(e_{\theta}\)에 의해 예측된 잡음 벡터는 식과 같이 선형적으로 결합된다. (2) 점수의 추정 \(\tilde{e}_{\theta}\)을 달성한다. 이 방법은 식에서 \(c_{I}\)을 갖는 항에 대한 프로세스를 활용하고 수정한다. (2) 지역화된 이미지 편집을 적용하는 단계.

\[\tilde{e}_{\theta}(z_{t},c_{I},c_{T})= \ e_{\theta}(z_{t},\varnothing,\varnothing) \tag{2}\] \[+s_{I}\cdot(e_{\theta}(z_{t},c_{I},\varnothing)-e_{\theta}(z_{t},c_{I},\varnothing))\] \[+s_{T}\cdot(e_{\theta}(z_{t},c_{I},c_{T})-e_{\theta}(z_{t},c_{I},\varnothing))\]\]

## 4 Method

우리는 _미리 훈련된_ IP2P _재훈련 또는 미세 조정 없이_를 위한 로컬화된 편집 방법을 개발하는 것을 목표로 합니다. 제안하는 방법은 (i) _edit localization_은 입력 영상과 편집 명령어를 통합하여 RoI를 찾고, (ii) _edit application_은 이 명령어를 RoI에 디덴탈 및 로컬리제이션 방식으로 적용한다.

### Edit Localization

세그먼트화: 우리의 연구는 확산 모델의 중간 특징이 필수적인 의미 정보를 인코딩한다는 확립된 이해를 확장한다. 기존의 안정적인 확산(Stable Diffusion, 34, 44, 49)을 기반으로 하는 방법과 달리, 본 논문에서는 IP2P에서 동작하며, Eq에 표시된 분할을 위해 원본 이미지(\(z_{t}\), \(c_{I}\), \(\varnothing\))에 조건화된 특징들에 초점을 맞춘다. (2). 실험적 관찰을 통해 이러한 특징들이 편집 목적의 분할 목적들과 잘 일치함을 보인다. 분할 맵을 얻기 위해 다운 블록과 업 블록을 포함하는 U-Net 아키텍처의 여러 계층에서 특징을 추출하여 다양한 해상도를 포괄하고 이미지에 대한 의미론적 이해를 향상시킨다. 주의 맵보다 중간 특징들에 대한 우리의 선호는 [34, 44, 49]와 같은 연구들에 의해 검증된 바와 같이, 보다 풍부한 의미 정보를 인코딩하기 위한 그들의 우수한 능력에 기초한다.

제안된 모델 내에서 특징 표현을 정제하기 위해 다중 해상도 융합 전략을 구현한다. 이는 (i) 바이-리니어 보간법을 적용하여 특징 맵을 다양한 해상도에서 공통 해상도로 리사이징하고, (ii) 채널 차원을 따라 이들을 연결 및 정규화하고, (iii) 마지막으로 융합된 특징들에 대해 K-평균 알고리즘과 같은 클러스터링 방법을 적용하는 것을 포함한다. 우리는 이러한 단계를 수행하여 각 기능 세트의 풍부하고 설명적인 품질을 유지하는 것을 목표로 합니다. 더욱이, U-Net 단계의 각각의 해상도는 의미론 및 크기 면에서 영역들의 상이한 입도를 유지한다.

그림 2는 서로 다른 해상도의 분할 맵과 제안된 융합 특징을 보여준다. 각 해상도는 이미지의 서로 다른 의미 구성 요소(예: 필드, 라켓, 모자, 드레스)를 캡처합니다. 비록 _해상도 64_가 객체들, _예를 들어, 스킨 및 의상을 구별할 수 있지만, 그것은 필드 내의 라인들에 대한 일관된 세그먼트 영역들, _예를 들어, 두 개의 별개의 클러스터들을 제공하지 않는다. 반면에, 더 낮은 해상도인 _해상도 16 및 32_는 필드 및 라켓의 라인들과 같은 거친 세그먼트들을 캡처할 수 있다. 상이한 해상도들로부터의 이러한 특징들을 융합하는 것은 보다 강건한 특징 표현들을 산출하여, 세그먼트화를 강화한다; 도면을 참조한다. 2 - _Ours_. 중간 특징 추출을 위해 LD-ZNet [34]에서 권장하는 대로 100단계 중 30~50단계 사이의 시간 단계를 사용한다.

로컬라이제이션: 입력 이미지 내의 세그먼트를 식별할 때, 제안된 방법은 입력 이미지에 조건화된 교차 주의 맵과 식에 표시된 명령(\(z_{t}\), \(c_{I}\), \(c_{T}\))을 사용하여 편집에 대한 RoI를 식별한다. (2). 이 맵은 \(H_{b}\times W_{b}\times D\)의 차원을 가지며, \(H_{b}\)과 \(W_{b}\)은 각각 블록 \(b^{th}\)(위쪽 및 아래쪽 블록)의 특징의 높이와 폭을 나타내며, \(D\)은 텍스트 토큰의 수를 나타낸다. 분할 전략에 따라, 크로스 어텐션 맵은 공간 차원들, 즉 \(H\)과 \(W\) 사이에서 결합되고 토큰 차원들, \(D\) 사이에서 정규화되는 공통 해상도로 리사이징된다. 상이한 해상도들로부터 어텐션 맵들을 병합한 후, 방법은 컨디셔닝 텍스트의 관련없는 부분들로부터 잡음이 있는 어텐션 값들을 무시하기 위해 _<텍스트의 시작>_, _단어들_ 및 _패딩_ 토큰들을 무시하고 나머지 토큰들에 초점을 맞추어 편집 명령과 관련된 영역을 식별한다. 그리고, 토큰들 중 평균 어텐션 스코어를 구하여 최종 어텐션 맵을 생성한다. 2 - _Attention_. 그 후, 상단 \(100\) 픽셀이 Tab에서 제거되었다. 가장 높은 확률 점수로 표시된 4가 식별된다. 그리고, 이들 화소 중 적어도 하나와 겹치는 모든 세그먼트를 조합하여 RoI를 얻는다. 2 - _Ours_, _Attention_ 및 _RoI_.

### Edit Application

미리 훈련 된 모델의 강도를 활용 하 여 IP2P 내에서 새로운 _로컬화된 편집 기술_ 을 도입 합니다. 이 모듈은 이미지의 나머지가 동일하게 유지되도록 보장하면서 RoI에 대응하는 주의 스코어를 조작하여 RoI 외부의 의도하지 않은 변경을 방지한다. 구체적으로, 이 절차는 Eq의 표기법을 사용하여 \(z_{t}\), \(c_{I}\), \(c_{T}\)의 항을 사용한다. (2).

주의 규칙화: 이전의 방법 [2, 8, 29]은 주의 점수 대신 잡음 공간을 사용한다. 이와는 대조적으로, 우리의 방법은 편집 동안 RoI 내에서 관련 없는 토큰의 영향을 선택적으로 줄이기 위한 표적 주의 규칙화를 도입한다. 이 접근 방식은 _<텍스트 시작>_, _패딩_ 및 _단어 중지_ ( \(S\)로 표시 됨)와 같이 편집 작업과 관련이 없는 토큰에 대 한 주의 점수를 정규화 합니다. RoI 내에서 주의 점수(\(QK^{T}\))를 조정함으로써 소프트맥스 정규화 과정에서 이러한 관련 없는 토큰의 영향을 최소화하는 것을 목표로 한다. 결과적으로, 소프트맥스 함수는 편집 명령어와 정렬되는 토큰에 RoI 내에서 더 높은 주의 확률을 할당할 가능성이 더 높다. 이러한 타겟팅된 접근법은 편집들이 원하는 영역들에 정확하게 포커싱되는 것을 보장하여, 나머지는 보존하면서 편집들의 정확성 및 유효성을 향상시킨다. RoI \(M\)에 대한 이진 마스크를 사용하면 관련 없는 토큰에 대한 교차 주의 계층의 내적 \(QK^{T}\) 결과를 다음과 같이 정규화 버전 \(R(QK^{T},M)\)으로 수정합니다.

그림 3: **Attention Regularization**. 우리의 방법은 RoI 내에서 관련 없는 토큰을 선택적으로 정규화하여 추가 모델 훈련이나 추가 데이터가 필요 없이 정확하고 상황 인식 편집을 보장한다. 주의 규칙화 후 관련 토큰에 대 한 확률은 두 번째 행에 설명 된 대로 RoI에 참석 합니다. **

그림 2: **세그먼트화 및 RoI 찾기**_해상도 X_s는 다른 해상도의 분할 맵을 보여 주는 반면 _우리_는 메서드의 분할 맵을 보여 줍니다. 크로스 어텐션 맵의 경우 노란색은 높은 확률을 나타내고 파란색 점은 가장 높은 확률로 \(100\) 픽셀을 표시한다. 마지막 이미지에는 파란색 점과 _우리의_를 사용하여 추출된 RoI가 표시됩니다.

\[R(QK^{T},M)=\begin{cases}QK^{T}_{ijt}-\alpha,&\text{if $M_{ij}=1$ and $t\in S$}\\ QK^{T}_{ijt},&\text{otherwise},\end{cases} \tag{3}\]

여기서 \(\alpha\)는 큰 값이다. 직관적으로 우리는 그림 3과 같이 관련 없는 토큰이 RoI에 참석하는 것을 방지한다. 대조적으로 관련 토큰은 RoI에서 선택될 가능성이 더 높아 더 정확하고 현지화 및 집중 편집으로 이어질 것이다. 이 방법은 의도된 영역 내에서 타겟팅된 편집과 주변 컨텍스트 보존 사이의 최적의 균형을 달성하여 수업의 전반적인 효율성을 향상시킨다.

RoI 내에서 이 정확한 정규화 기술을 사용함으로써 우리의 방법은 IP2P를 크게 향상시킨다. 이미 학습된 모델의 피쳐를 탭하여 엉킴 해제 정도를 높이고 편집의 위치를 개선합니다. 이 대상 접근 방식은 _재교육 또는 미세 조정_ 의 필요성을 우회하여 계산 리소스 및 시간을 보존합니다. 미리 훈련된 IP2P 기능의 고유한 강도를 활용하여 집중적이고 효과적인 방식으로 배포합니다. 이러한 정밀도는 의도된 영역 내에 편집이 포함되도록 하여, 추가적인 라운드의 훈련 또는 미세 조정에 대한 필요 없이 국부적이고 제어된 방식으로 복잡한 명령을 실행할 수 있는 모델의 향상된 능력을 뒷받침한다.

## 5 Experiments

### 평가 데이터 집합 및 메트릭

다양한 데이터 세트와 메트릭을 결합하면 제안된 방법을 철저히 평가할 수 있다. 각 데이터 세트에 대해 해당 작업에서 제안된 메트릭을 보고한다.

**MagicBrush [52].** 테스트 분할은 535 세션 및 1053 회기로 포괄적인 평가 파이프라인을 제공합니다. _ 세션_은 반복 편집 지침에 사용되는 원본 이미지를 의미하며 _turns_는 각 세션 내의 개별 편집 단계를 나타냅니다. 픽셀 정확도를 측정하는 _L1_ 및 _L2_ 규범, 코사인 유사성을 통해 이미지 품질을 평가하는 _CLIP-1_ 및 _DINO_ 임베딩, 생성된 이미지가 로컬 텍스트 설명과 정확하게 일치하는지 확인하는 _CLIP-T_를 사용합니다.

**InstructPix2Pix [6].** 5K 이미지 명령 쌍으로 InstructPix2Pix 테스트 분할에서 방법을 평가 합니다. 메트릭은 시각적 충실도를 위한 _CLIP 이미지 유사성_ 및 편집 명령에 대한 준수를 측정하기 위한 _CLIP 텍스트-이미지 방향 유사성_을 포함한다.

**PIE-Bench [20].** 벤치마크에는 입/출력 캡션, 편집 지침, 입력 이미지 및 RoI 주석이 있는 10개의 편집 범주에서 700개의 이미지가 포함됩니다. 구조적 무결성 및 배경 보존을 위한 메트릭은 _PSNR_, _LPIPS_, _MSE_ 및 _SSIM_과 같은 코사인 유사성 측정 및 이미지 메트릭에서 파생되며 텍스트-이미지 일관성은 _CLIP 유사성_을 통해 평가됩니다.

도 4: **질적 실시예**. 우리는 (a) 큰 세그먼트의 편집, (b) 텍스처의 변경, (c) 여러 세그먼트의 편집, (d) 추가, (e) 대체 및 (f) 객체 제거와 같은 다양한 작업에 대해 이 방법을 테스트한다. 예들은 확립된 논문들로부터 취해진다[20, 52, 53]. LIME의 통합은 모든 모델의 성능을 향상시켜 나머지 이미지 영역의 무결성을 유지하면서 로컬 편집을 가능하게 합니다. **

**EditVal [5].** 벤치마크는 MS-COCO 데이터 세트 [24]의 19개 클래스에 걸쳐 있는 648개의 이미지 편집 작업을 제공합니다. 벤치마크는 편집 유형이 성공적으로 적용되었는지 여부를 나타내는 이진 점수로 각 편집의 성공을 평가합니다. OwL-ViT [28] 모델은 관심 객체를 검출하는 데 사용되고, 검출은 수정들의 정확성을 평가하는 데 사용된다.

### Implementation Details

이 방법은 InstructPix2Pix [6]을 기본 모델로 채택하여 Secs. 4.1과 4.2에서 설명한 각 단계에 대해 100단계에 대한 모델을 실행한다. 구체적으로, _Edit Localization_ 동안 LD-ZNet [34]에서 제안한 바와 같이 \(100\) 단계 중 \(30\)과 \(50\) 사이에 중간 표현이 추출된다. 또한, 이들 중간 특징들은 \(256\times 256\)으로 크기가 조정된다. 분할을 위한 클러스터의 수는 절제 연구에 의해 동기화된 모든 실험에서 \(8\)이다. 동시에 교차 주의 맵에 대해 단계 \(1\)에서 \(75\)까지의 기능을 수집 하 고 관련 토큰만 보유 합니다. 어텐션 맵에서 확률이 가장 높은 \(100\)개의 픽셀을 추출하여 RoI를 식별하고 중복 세그먼트를 결정한다. 로컬화 편집 _이미지 스케일 \(s_{I}\) 및 텍스트 스케일 \(s_{T}\)의 경우 매개 변수는 각각 \(1.5\) 및 \(7.5\)입니다. **애플리케이션 편집** 중에는 관련 없는 토큰만 대상으로 하는 단계 \(1\)과 \(75\ 사이에 주의 규칙화가 사용 됩니다. 편집 과정에서 이미지 스케일, \(s_{I}\), 텍스트 스케일, \(s_{T}\) 파라미터는 각각 \(1.5\)과 \(3.5\)으로 설정된다.

### Qualitative Results

[그림 4]는 다양한 편집 작업에 대한 질적 사례를 제시하고 있다. 이러한 작업에는 큰 세그먼트 편집, 텍스처 변경, 여러 개의 작은 세그먼트 동시 편집, 객체 추가, 교체 또는 제거가 포함됩니다. 첫 번째 열은 각 이미지 아래에 해당 편집 지시와 함께 입력 이미지를 표시합니다. 두 번째 열은 제안된 방법이 없는 기본 모델에 의해 생성된 결과를 보여준다. 세 번째 및 네 번째 열은 정규화 방법이 이러한 RoI에 적용될 때 우리의 방법으로 식별된 RoI와 기본 모델에서 생성된 편집된 출력을 보고한다. 에 도시된 바와 같이. 4, 본 논문에서 제안하는 방법은 전체 장면 컨텍스트를 유지하면서 편집 명령을 효과적으로 구현한다. 제시된 모든 결과에서 이 방법은 수동으로 주석이 달린 데이터 세트 _예_, MagicBrush [52]에서 미세 조정된 버전을 포함하여 현재 최신 모델을 능가한다. 또한, HIVE [53]에서 주장 및 보고된 바와 같이 추가 훈련 없이 IP2P는 그림 4의 (d)에 대한 성공적인 편집을 수행할 수 없다. 그러나, 제안된 방법은 그림과 같이 기본 모델에 대한 추가 훈련 없이 원하는 편집을 달성한다. 4-(d).

### Quantitative Results

**MagicBrush에 대 한 결과.** 본 방법은 탭에서 볼 수 있듯이 MagicBrush (MB) [52] 벤치마크에서 단일 및 다중 회전 편집 작업 모두에서 다른 모든 방법보다 성능이 뛰어납니다. 1. 기본 모델에 비해 본 방법은 _L1_, _L2_, _CLIP-I_ 및 _DINO_ 측면에서 상당한 개선과 최상의 결과를 제공 합니다. 편집된 이미지와 캡션을 Ground truth와 비교하는 _CLIP-T_ 메트릭의 경우, 본 논문에서 제안하는 방법은 멀티턴의 경우 \(0.309\), 싱글턴의 경우 \(0.307\)의 오라클 점수에 매우 근접한다. 이것은 우리의 편집이 사실 수정을 정확하게 반영한다는 것을 나타낸다. VQGAN-CLIP [9]는 _CLIP-T_ 에서 가장 높은 성능을 달성 합니다.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Methods**} & \multicolumn{5}{c}{**Single-turn**} & \multicolumn{5}{c}{**Multi-turn**} \\ \cline{2-11}  & **MB** & L1 \(\downarrow\) & L2 \(\downarrow\) & CLIP-I \(\uparrow\) & DINO \(\uparrow\) & CLIP-T \(\uparrow\) & L1 \(\downarrow\) & L2 \(\downarrow\) & CLIP-I \(\uparrow\) & DINO \(\uparrow\) & CLIP-T \(\uparrow\) \\ \hline Open-Edit [25] & ✗ & 0.143 & 0.043 & 0.838 & 0.763 & 0.261 & 0.166 & 0.055 & 0.804 & 0.684 & 0.253 \\ VQGAN-CLIP [9] & ✗ & 0.220 & 0.083 & 0.675 & 0.495 & **0.388** & 0.247 & 0.103 & 0.661 & 0.459 & **0.385** \\ SDEdit [27] & ✗ & 0.101 & 0.028 & 0.853 & 0.773 & 0.278 & 0.162 & 0.060 & 0.793 & 0.621 & 0.269 \\ Text2LIVE [4] & ✗ & **0.064** & **0.017** & **0.924** & **0.881** & 0.242 & **0.099** & **0.028** & **0.880** & **0.793** & 0.272 \\ Null-Text Inv. [30] & ✗ & 0.075 & 0.020 & 0.883 & 0.821 & 0.274 & 0.106 & 0.034 & 0.847 & 0.753 & 0.271 \\ \hline HIVE [53] & ✗ & 0.109 & 0.034 & 0.852 & 0.750 & 0.275 & 0.152 & 0.056 & 0.800 & 0.646 & 0.267 \\ HIVE [53] + _LIME_ & ✗ & **0.051** & **0.016** & **0.940** & **0.909** & 0.293 & **0.080** & 0.029 & 0.894 & **0.829** & 0.283 \\ HIVE [53] & ✓ & 0.066 & 0.022 & 0.919 & 0.866 & 0.281 & 0.097 & 0.037 & 0.879 & 0.789 & 0.280 \\ HIVE [53] + _LIME_ & ✓ & 0.053 & **0.016** & 0.939 & 0.906 & **0.300** & **0.080** & **0.028** & **0.899** & **0.829** & **0.295** \\ \hline IP2P [6] & ✗ & 0.112 & 0.037 & 0.852 & 0.743 & 0.276 & 0.158 & 0.060 & 0.792 & 0.618 & 0.273 \\ IP2P [6] + _LIME_ & ✗ & 0.058 & **0.017** & 0.935 & 0.906 & 0.293 & 0.094 & 0.033 & 0.883 & 0.817 & 0.284 \\ IP2P [6] & ✓ & 0.063 & 0.020 & 0.933 & 0.899 & 0.278 & 0.096 & 0.035 & 0.892 & 0.827 & 0.275 \\ IP2P [6] + _LIME_ & ✓ & **0.056** & **0.017** & **0.939** & **0.911** & **0.297** & **0.088** & **0.030** & **0.894** & **0.835** & **0.294** \\ \hline \hline \end{tabular}
\end{table}
표 1: **매직브러시 데이터셋에 대한 평가[52]. 단일회전 및 다중회전 설정에 대한 결과는 각 방법에 대해 제시되었으며 _MB_는 MagicBrush에서 미세 조정된 모델의 약자이다. 다른 접근법에 대한 벤치마크 값은 [52]에서 가져온 반면, 제안된 방법에 대한 값은 동일한 프로토콜에 따라 계산된다. 두 설정 모두에서 이 방법은 비교 모델의 기본 모델 성능을 능가한다. 상위 성능은 굵게 강조 표시 되는 반면 두 번째 성능은 각 블록에 대 한 밑줄로 표시 됩니다.* * 추론 중에 미세 조정을 위해 CLIP [35]를 직접 사용 하 여 표시 됩니다. 그러나 이는 이미지를 과도하게 변경하여 다른 메트릭에서 성능이 저하될 수 있습니다. 전반적으로, 메트릭에 걸친 성능은 우리의 접근법이 명령을 기반으로 고품질 및 로컬화된 이미지 편집을 생성하여 이전의 최첨단 방법보다 우수함을 보여준다.

PIE-Bench에 대한 결과, PIE-Bench에 대한 정량적 분석[20]은 제안된 방법의 유효성을 입증한다. MagicBrush [52] 및 HIVE [53]의 Instruct-Pix2Pix [6] 및 미세 조정된 버전과 같은 기본 모델과 비교하여 본 방법은 구조 및 배경 보존을 측정하는 메트릭에서 훨씬 더 나은 성능을 달성한다. 이것은 우리의 접근법이 영향을 받지 않는 영역에 대한 의도하지 않은 변경을 피하면서 지침에 따라 국부적인 편집을 수행함을 나타낸다. 동시에 CLIP 유사도 점수에 대한 기본 모델과 유사한 결과를 얻었으며, 텍스트 지침에 따라 편집을 충실히 적용함을 보여준다. 2. 전반적으로 본 논문에서 제안한 방법은 관련 없는 부분을 변경하지 않고 주어진 명령어에 의해서만 정확한 편집을 함으로써 텍스트 유도 이미지 편집이 가능함을 정량적으로 확인하였다.

편집Val 벤치마크 데이터 세트 [5]를 사용한 평가에서는 다양한 편집 유형, 특히 _객체 추가(O.A.)_, _위치 대체(P.R.)_ 및 _위치 추가(P.A.)_에서 우수한 성능을 보이는 반면 _객체 대체(O.R.)_에서는 두 번째로 우수한 성능을 보인다. 특히, _Size(S.)_ 및 _Alter Parts(A.P.)_를 포함하는 편집을 위한 다른 방법에 필적하는 성능을 수행한다. 종합적인 비교가 Tab. 3. 전반적으로, 이 방법은 이전의 최상의 모델보다 평균 벤치마크 결과를 \(5\%\)만큼 개선함으로써 최신 기술을 발전시킨다.

InstructPix2Pix에 대한 결과는 그림 5와 같이 합성 평가 데이터 집합 [6]에 대한 결과를 제시하면서 InstructPix2Pix와 동일한 설정을 사용하여 방법을 평가한다. 이 방법은 기본 모델인 IP2P를 개선하여 입력 이미지와 명령어 기반 편집 사이의 균형을 최적화한다. 또한 텍스트 규모의 증가 \(s_{T} \)는 _CLIP 텍스트-이미지 방향 유사성_을 향상시키지만 _CLIP 이미지 유사성_에는 부정적인 영향을 미칩니다. 두 메트릭 모두, 높을수록 좋다. 검은색 화살표는 이 문서의 결과에 대해 선택한 구성을 나타냅니다.

\begin{table}
\begin{tabular}{l c|c|c c c c|c c} \hline \hline  & **Metrics** & **Structure** & \multicolumn{4}{c|}{**Background Preservation**} & \multicolumn{2}{c}{**CLIP Similarity**} \\ \hline
**Methods** & **GT Mask** & \(\textbf{Distance}_{+10}\downarrow\) & **PSNR \(\uparrow\)** & \(\textbf{LPIPS}_{+10}\downarrow\) & \(\textbf{MSE}_{+10}\downarrow\) & \(\textbf{SSIM}_{+10}\uparrow\) & **Whole \(\uparrow\)** & **Edited \(\uparrow\)** \\ \hline InstructDiffusion [15] & ✗ & 75.44 & 20.28 & 155.66 & 49.66 & 75.53 & 23.26 & 21.34 \\ BlendedDiffusion [3] & ✓ & 81.42 & **29.13** & **36.61** & **19.16** & **86.96** & **25.72** & **23.56** \\ DirectInversion + P2P [20] & ✗ & **11.65** & 27.22 & 54.55 & 32.86 & 84.76 & 25.02 & 22.10 \\ \hline IP2P [6] & ✗ & 57.91 & 20.82 & 158.63 & 227.78 & 76.26 & 23.61 & **21.64** \\ IP2P [6] + _LIME_ & ✗ & 32.80 & 21.36 & 110.69 & 159.93 & 80.20 & 23.73 & 21.11 \\ IP2P [6] + _LIME_ & ✓ & **26.33** & **24.78** & **89.90** & **105.19** & **82.26** & **23.81** & 21.10 \\ \hline IP2P [6] w/MB [52] & ✗ & 22.25 & 27.68 & 47.61 & 40.03 & 85.82 & 23.83 & **21.26** \\ IP2P [6] w/MB [52] + _LIME_ & ✗ & 10.81 & 28.80 & 41.08 & 27.80 & 86.51 & 23.54 & 20.90 \\ IP2P [6] w/MB [52] + _LIME_ & ✓ & **10.23** & **28.96** & **39.85** & **27.11** & **86.72** & **24.02** & 21.09 \\ \hline HIVE [53] & ✗ & 56.37 & 21.76 & 142.97 & 159.10 & 76.73 & 23.30 & **21.52** \\ HIVE [53] + _LIME_ & ✗ & 37.05 & 22.90 & 112.99 & 107.17 & 78.67 & 23.41 & 21.12 \\ HIVE [53] + _LIME_ & ✓ & **33.76** & **24.14** & **103.63** & **94.01** & **81.18** & **23.62** & 21.21 \\ \hline HIVE [53] w/MB [52] & ✗ & 34.91 & 20.85 & 158.12 & 227.18 & 76.47 & 23.90 & **21.75** \\ HIVE [53] w/MB [52] + _LIME_ & ✗ & 26.98 & 26.09 & 68.28 & 63.70 & 84.58 & 23.96 & 21.36 \\ HIVE [53] w/MB [52] + _LIME_ & ✓ & **25.86** & **28.43** & **50.33** & **43.25** & **86.67** & **24.23** & 21.43 \\ \hline \hline \end{tabular}
\end{table}
표 2: **PIE-Bench 데이터 세트에 대한 평가[20]. 10가지 유형의 편집 유형을 비교한 결과 텍스트 유도 이미지 편집 모델에서 기본 모델보다 우수한 성능을 보였다. 첫 번째 블록에 대한 숫자는 벤치마크 논문[20]으로부터 취해진다. 최고 성능은 굵게 강조 표시 되는 반면 두 번째 성능은 각 블록에 대 한 밑줄로 표시 됩니다. _ GT Mask_는 관심 영역으로서 Ground-truth 마스크를 나타냅니다. **

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Method** & **O.A.** & **O.R.** & **P.R.** & **P.A.** & **S.** & **A.P.** & **Avg.** \\ \hline SINE [54] & 0.47 & **0.59** & 0.02 & 0.16 & 0.46 & 0.30 & 0.33 \\ NText. [30] & 0.35 & 0.48 & 0.00 & 0.20 & **0.52** & **0.34** & 0.32 \\ IP2P [6] & 0.38 & 0.39 & 0.07 & 0.25 & 0.51 & 0.25 & 0.31 \\ Imagic [21] & 0.36 & 0.49 & 0.03 & 0.08 & 0.49 & 0.21 & 0.28 \\ SDEdit [27] & 0.35 & 0.06 & 0.04 & 0.18 & 0.47 & 0.33 & 0.24 \\ Dbooth [39] & 0.39 & 0.32 & 0.11 & 0.08 & 0.28 & 0.22 & 0.24 \\ TInv. [14] & 0.43 & 0.19 & 0.00 & 0.00 & 0.02 & 0.14 \\ DiffEdit [8] & 0.34 & 0.26 & 0.00 & 0.00 & 0.00 & 0.07 & 0.11 \\ \hline IP2P + _LIME_ & **0.48** & 0.49 & **0.21** & **0.34** & 0.49 & 0.28 & **0.38** \\ \hline \hline \end{tabular}
\end{table}
표 3: **EditVal Dataset에 대한 평가[5]. 6가지 편집 유형에 대한 비교를 통해 제안된 방법이 8가지 최첨단 텍스트 유도 이미지 편집 모델보다 성능이 우수함을 알 수 있다. 다른 방법에 대한 숫자는 벤치마크 논문 [5]에서 직접 가져오고 동일한 평가 설정이 우리 방법에 적용된다. 최고 성능은 굵게 강조 표시 되는 반면 두 번째 성능은 굵게 표시 됩니다. **

### Ablation Study

절제 연구는 RoI 찾기 방법, 주의 지도의 점 수 및 군집 수의 세 가지 주요 구성요소의 영향을 분석한다. InstructPix2Pix는 기본 아키텍처이다. 평가는 매직브러시 데이터 세트에 있습니다. 각 매개변수는 별도로 수정되지만 다른 매개변수는 영향을 분리하기 위해 고정된 상태로 유지된다.

RoI 찾기 방법. MagicBrush [52]의 Ground truth 마스크는 편집 영역 주위에 그리 좁지 않습니다. 시각화는 _보충 자료_ 를 참조 하세요. 이러한 이유로, 예측 마스크를 사용한 방법은 Ground truth 마스크를 사용한 방법과 비교하여 _CLIP-T_와 온-파 결과를 가지면서 _L1_, _L2_, _CLIP-I_ 및 _DINO_ 메트릭에 대해 최상의 성능을 달성한다. 4. 또한 공식 코드 base1을 사용하여 최신 LPM [33]을 IP2P에 적용하여 예측된 분할을 비교한다. 이 경우에도 더 나은 결과를 얻을 수 있다.

각주 1: [https://github.com/orpatashnik/local-prompt-mixing](https://github.com/orpatashnik/local-prompt-mixing)

어텐션 맵의 포인트 수 \(25\) 포인트만 사용하면 RoI 내에서 여러 개의 별개의 세그먼트를 캡처할 수 없기 때문에 성능이 악화된다. 그러나, 더 많은 포인트들을 갖는 것은 과도한 노이즈를 포함하고, 더 많은 세그먼트들이 RoI 영역을 부적절하게 병합하고 확장하게 한다. \ (100\) 포인트는 Tab. 4에 도시된 바와 같이, 더 나은 RoI를 제공한다.

클러스터의 수 \(4\)과 같은 몇 개의 클러스터는 큰 세그먼트와 확장된 RoI로 이어져 로컬 편집을 방지했다. \(16\) 또는 \(32\)과 같은 클러스터 수를 늘리면 단일 RoI가 여러 클러스터로 분리된다. 탭에 표시된 대로입니다. 도 4, 도 8은 최상의 결과를 달성한다.

애플리케이션 편집. 주의 규칙화 대신에, 잡음 공간에서도 편집이 수행될 수 있다[2, 8, 29]. 이는 RoI에 따라, 입력 이미지와 편집 텍스트로부터 도출된 참조 이미지를 노이즈 공간에서 선형적으로 블렌딩하는 것에 해당한다. 그러나 RoI를 효과적으로 타겟팅하기 위해서는 편집된 영역에서 참조 영상과 입력 영상 간의 정렬이 매우 중요하다. 탭에 표시된 대로입니다. 4 - _편집_ 이 방법은 주의 규칙화를 사용 하 여 편집 정밀도를 향상 합니다.

## 6 Conclusion

본 논문에서는 편집 영역의 명시적 분할과 주의 규칙화를 통해 수정된 IP2P를 이용한 새로운 지역화 이미지 편집 기법인 LIME을 소개한다. 이 접근법은 로컬화된 편집에서 정밀도 및 컨텍스트 보존의 문제를 효과적으로 해결하여 사용자 입력 또는 모델 미세 조정/재훈련의 필요성을 제거한다. 이 방법의 주의력 정규화 단계는 사용자 지정 마스크와 함께 사용할 수 있어 추가 유연성을 제공한다. 제안된 방법의 강인성과 유효성은 기존의 최신 방법에 비해 우수한 성능을 보였다. 이러한 발전은 이미지 편집에서 LDM의 지속적인 진화에 기여하여 향후 연구를 위한 흥미로운 가능성을 가리킨다.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline  & **Method** & L1 \(\downarrow\) & L2 \(\downarrow\) & CLIP-I\(\uparrow\) & DINO \(\uparrow\) & CLIP-T \(\uparrow\) \\ \hline  & IP2P [6] & 0.112 & 0.037 & 0.852 & 0.743 & 0.276 \\ \hline \multirow{4}{*}{**Road**} & GT & 0.063 & **0.017** & **0.935** & 0.902 & **0.297** \\  & LPM [33] & 0.072 & 0.019 & 0.924 & 0.886 & 0.291 \\  & Ours & **0.058** & **0.017** & **0.935** & **0.906** & 0.293 \\ \hline \multirow{4}{*}{**Road**} & \(25\) & 0.079 & 0.023 & 0.917 & 0.874 & 0.290 \\  & \(100\) & **0.058** & **0.017** & **0.935** & **0.906** & 0.293 \\  & \(225\) & 0.065 & 0.018 & 0.932 & 0.901 & **0.295** \\  & \(400\) & 0.070 & 0.020 & 0.925 & 0.889 & **0.295** \\ \hline \multirow{4}{*}{**Road**} & \(4\) & 0.080 & 0.022 & 0.923 & 0.885 & **0.295** \\  & \(8\) & **0.058** & **0.017** & **0.935** & **0.906** & 0.293 \\  & \(16\) & 0.062 & 0.018 & 0.933 & 0.903 & 0.294 \\  & \(32\) & 0.064 & 0.018 & 0.932 & 0.901 & 0.291 \\ \hline \multirow{4}{*}{**Road**} & Noise & 0.076 & 0.022 & 0.914 & 0.864 & 0.291 \\  & Ours & **0.058** & **0.017** & **0.935** & **0.906** & **0.293** \\ \hline \end{tabular}
\end{table}
표 4: **절제 연구.** 공정한 비교를 위해 절제된 매개 변수를 제외한 모든 설정에 대해 모든 매개 변수가 동일합니다. 최고 성능은 **볼드** 에서 강조 표시 되는 반면 두 번째 성능은 각 블록에 대 한 밑줄로 표시 됩니다.

그림 5: **InstructPix2Pix 테스트.** 입력 이미지(Y 축)와 편집(X 축) 간의 트레이드 오프가 표시됩니다. T와 C는 각각 클러스터의 \(s_{T}\)과 #을 나타낸다. 모든 실험에서 \(s_{I}\in[1.0,2.2]\)은 고정되어 있다. 화살표는 결과에 대해 선택한 구성을 가리킵니다.

그림 6: **실패 사례 및 제한** 왼쪽: 기본 모델 얽힘입니다. 오른쪽: 피쳐 혼합 문제입니다.

제한점.그림 6은 우리 방법의 한계를 보여준다. (i) 사전 훈련된 기본 모델의 능력으로 인한 제한을 보여준다. 이 방법은 RoI에 초점을 맞추고 편집을 성공적으로 적용할 수 있지만 기본 모델 얽힘으로 인해 장면 스타일, 특히 색상을 변경할 수 있다. 그러나 우리의 제안은 IP2P에 비해 편집을 크게 개선합니다. (ii) 프롬프트 콘텐츠가 편집 품질에 어떻게 영향을 미치는지를 예시한다. 편집하는 동안 텍스트의 \(<\)_start, _중단 단어_ 및 _패딩_을 제외한 모든 토큰이 RoI에 영향을 주어 기능 혼합으로 이어집니다.

## References

*[1]A. 아가왈 Karanam, K. J. Joseph, A. Saxena, K. Goswami, and B. Vasan Srinivasan (2023) A-star: test-time attention segregation and retention for text-to-image synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 2283-2293. Cited by: SS1.
*[2]O. Avrahami, D. Lischinski, O. 자연 이미지의 텍스트 구동 편집을 위한 프리드(2022) 블렌디드 확산. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 18187-18197. Cited by: SS1.
*[3]O. 아브라함, O. Fried, and D. Lischinski(2023) Blended latent diffusion. ACM Transactions on Graphics (TOG)42 (4), pp. 1-11. Cited by: SS1.
* ECCV 2022
- 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XV, pp. 707-723. Cited by: SS1.
*[5]S. 바수 사베리 Bhardwaj, A. M. Chegini, D. Massiceti, M 산자비 Xu Hu, S. Feizi(2023) Editval: 벤치마킹 확산 기반 텍스트 유도 이미지 편집 방법. arXiv preprint arXiv:2310.02426. Cited by: SS1.
*[6]T. Brooks, A. Holynski, and A. A. Efros(2022) Instructpix2pix: learning to follow image editing instructions. CoRRabs/2211.09800. 인용: SS1.
*[7]H. 셰이퍼 알루프 빈커 Wolf, and D. Cohen-Or (2023) Attend-and-excite: attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG)42 (4), pp. 1-10. Cited by: SS1.
*[8]G. Couairon, J. Verbeek, H. Schwenk, and M. Cord(2023) Diffe4it: 확산 기반 시맨틱 이미지 편집과 마스크 안내. The Eleventh International Conference on Learning Representations, Vol., pp. 1-2. Cited by: SS1.
* ECCV 2022
- 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXVII, pp. 88-105. Cited by: SS1.
*[10]P. Dhariwal과 A. Nichol (2021) 확산 모델은 이미지 합성에서 gans를 능가한다. Neural Information Processing Systems34, pp.8780-8794. Cited by: SS1.
*[11]A. 도소비츠키 베이어, A. 콜레스니코프, D. 와이슨본, X. 자이태 운터타이너 데하니 김혜골 Gelly, J. Uszkoreit, N. Houlsby(2021) 이미지는 16x16 단어의 가치가 있다: 스케일에서의 이미지 인식을 위한 트랜스포머. In International Conference on Learning Representations, Cited by: SS1.
*[12]P. 에서 Rombach, and B. Ommer(2021) Taming Transformer for high-resolution image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pp. 12873-12883. Cited by: SS1.
*[13]T. 후원 후석호 두원영 양주영 간(2023) 멀티모달 대형 언어 모델을 통한 명령어 기반 이미지 편집을 안내한다. arXiv preprint arXiv:2309.17102. Cited by: SS1.
*[14]R. 갈영 알루프 아츠몬 Patashnik, A. H. Bermano, G. Chechik, 및 D. Cohen-Or (2022) 이미지는 텍스트 역전을 사용하여 텍스트 대 이미지 생성을 개인화하는 한 단어의 가치가 있다. 에 의해 인용된다: SS1.
*[15]Z. 강병양 항철리 구태진 장정바오 Zhang, H. Hu, D. Chen, et al.(2023) Instructdiffusion: a generalist modeling interface for vision tasks. arXiv preprint arXiv:2309.03895. Cited by: SS1.
*[16]I. 좋은 친구, J. Pouget-Abadie, M. 미르자 Ozair, A. Courville, and Y. Bengio (2014) Generative Adversarial nets. 신경 정보 처리 시스템27에서의 진보. 인용: SS1.
*[17]A. 허츠 Mokady, J. Tenenbaum, K. A. Y. Pitch, and D. Cohen-Or (2022) Prompt-to-prompt image editing with cross attention control. CoRRabs/2208.01626. 인용: SS1.
*[18]J. Ho와 T. Salimans (2021) Classifier-free diffusion guidance. NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, Cited by: SS1.
*[19]J. Ho, A. Jain, and P. Abbeel (2020) Denoising diffusion probabilistic models. Neural Information Processing Systems33, pp.6840-6851. Cited by: SS1.
*[20]X. 주아정 비안석 Liu, Q. Xu(2023) Direct inversion: 3 라인의 코드로 확산 기반 편집을 부스팅한다. arXiv preprint arXiv:2310.01506. Cited by: SS1.
*[21]B. 가와르 자다오 랑오 토브창태 Dekel, I. Mosseri, and M. 이란(2022) ImageNet: 확산 모델을 이용한 텍스트 기반의 실제 이미지 편집. CoRRabs/2210.09276. 인용: SS1.
*[22]A. 키릴로프 라비, 호마오, C. 롤랜드, L. 구스타프손 샤오성 화이트헤드 Lo, P. Dollar, R. B. Girshick (2023) Segment anything. CoRRabs/2304.02643. 인용: SS1.
*[23]P. Korshunov and S. 마르셀(2018) 딥페이크: 얼굴 인식에 대한 새로운 위협? 평가 및 검출. arXiv preprint arXiv:1812.08685. Cited by: SS1.

- ECCV 2014
- 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V_, pages 740-755. Springer, 2014.
* ECCV 2020
- 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XI_, pages 89-106. Springer, 2020.
* [26] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repairt: Inpainting using denoising diffusion probabilistic models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pages 11451-11461. IEEE, 2022.
* [27] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In _International Conference on Learning Representations_, 2022.
* ECCV 2022: 17th European Conference, 2022_, page 728-755, 2022.
* [29] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus A Brubaker, Jonathan Kelly, Alex Levinshtein, Konstantinos G Depamis, and Igor Giltschenski. Watch your steps: Local image and scene editing by text instructions. _arXiv preprint arXiv:2308.08947_, 2023.
* [30] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. _CoRR_, abs/2211.09794, 2022.
* [31] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-guided diffusion models. In _International Conference on Machine Learning, 2022_, pages 16784-16804. PMLR, 2022.
* [32] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In _ACM SIGGRAPH 2023 Conference Proceedings_, pages 1-11, 2023.
* [33] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-Elor, and Daniel Cohen-Or. Localizing object-level shape variations with text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2023.
* [34] Koutilya PNVR, Bharat Singh, Pallabi Ghosh, Behjat Siddiquie, and David Jacobs. Ld-znet: A latent diffusion approach for text-based image segmentation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 4157-4168, 2023.
* [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _Proceedings of the 38th International Conference on Machine Learning, 2021_, pages 8748-8763. PMLR, 2021.
* [36] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. _CoRR_, abs/2204.06125, 2022.
* [37] Scott E. Reed, Zeynep Akata, Honglak Lee, and Bernt Schiele. Learning deep representations of fine-grained visual descriptions. In _2016 IEEE Conference on Computer Vision and Pattern Recognition, 2016_, pages 49-58. IEEE Computer Society, 2016.
* [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022_, pages 10674-10685. IEEE, 2022.
* [39] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 22500-22510, 2023.
* [40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In _Advances in Neural Information Processing Systems_, 2022.
* [41] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. _arXiv preprint arXiv:2311.10089_, 2023.
* [42] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.
* [43] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32, 2019.
* [44] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. _arXiv preprint arXiv:2306.03881_, 2023.
* [45] Kai Wang, Fei Yang, Shiqi Yang, Muhammad Atif Butt, and Joost van de Weijer. Dynamic prompt learning: Addressing cross-attention leakage for text-based image editing. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [46] Qian Wang, Biao Zhang, Michael Birsak, and Peter Wonka.

Mdp: 확산 경로를 조작하여 텍스트 유도 이미지 편집을 위한 일반화된 프레임워크, 2023.
* [47] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. _arXiv preprint arXiv:2302.13848_, 2023.
* [48] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale, Trung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu Chang. Uncovering the disentanglement capability in text-to-image diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1900-1910, 2023.
* [49] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary panoptic segmentation with text-to-image diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2955-2966, 2023.
* [50] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18381-18391, 2023.
* [51] Han Zhang, Tao Xu, and Hongsheng Li. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In _IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017_, pages 5908-5916. IEEE Computer Society, 2017.
* [52] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: A manually annotated dataset for instruction-guided image editing. In _Advances in Neural Information Processing Systems_, 2023.
* [53] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, and Ran Xu. HIVE: harnessing human feedback for instructional visual editing. _CoRR_, abs/2303.09618, 2023.
* [54] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris Metaxas, and Jian Ren. Sine: Single image editing with text-to-image diffusion models, 2022.

## 7 구현 세부 정보

우리는 8개의 코어가 있는 NVIDIA A100 40GB GPU 기계를 사용하여 결과를 얻는다. \(512\times 512\) 이미지의 경우 IP2P 기반 기준선(_e.g_, IP2P, IP2P w/MB, HIVE, HIVE w/MB)은 편집당 약 15초가 소요되며, LIME 통합 모델의 경우 \(\approx\)25초가 소요된다.

### Baselines

Open-Edit [25]: 이 GAN 기반 접근 방식은 사전 훈련을 위해 재구성 손실을 사용 하 고 특정 이미지에 대 한 미세 조정 동안 일관성 손실을 통합 합니다. 그것의 독특한 특징은 시각적 특징과 텍스트적 특징의 공유 공간 내에서 단어 임베딩을 산술적으로 조작하는 것이다.

VQGAN-CLIP[9]:CLIP 임베딩으로 VQGAN[12] 프레임워크를 개선한다[35], 이 방법은 생성된 이미지와 타겟 텍스트 사이의 CLIP 임베딩의 유사성을 사용하여 VQGAN을 미세 조정하여 최적화된 이미지 생성을 유도한다.

SDEdit [27]: Stable Diffusion [38]의 기능을 활용 하 여 SDEdit는 튜닝 없는 방법을 도입 합니다. 이것은 확률적 미분 방정식 잡음을 사용하여 소스 이미지에 추가하고 후속적으로 노이즈를 제거하여 목표 이미지를 근사화하며, 모두 목표 캡션을 기반으로 한다.

Text2LIVE[4]: 추가 레이어에 편집된 객체를 생성하기 위한 Vision Transformer[11]을 제안한다. 그것은 데이터 증강 및 CLIP[35] 감독을 통합하고, 궁극적으로 편집된 레이어를 원본과 알파-블렌딩하여 타겟 이미지를 생성한다.

Null Text Inversion[30]: DDIM[42] 궤적을 최적화함으로써, 이 방법은 처음에 소스 이미지를 반전시킨다. 이후, 텍스트와 이미지 간의 교차 주의[17]에 의해 안내되는 노이즈 제거 과정 동안 이미지 편집을 수행한다.

SINE[54]: 모델 기반 안내 및 패치 기반 미세 조정 프로세스를 사용하여 실제 이미지를 편집한다.

DreamBooth [39]: 특수 텍스트 토큰을 학습 하 고 편집을 위해 이미지 집합에서 모델 매개 변수를 조정 하 여 확산 모델을 미세 조정 합니다.

Textual-Inversion[14]: 이미지들의 세트를 사용하여 텍스트-인코더 공간 내에 임베딩된 토큰을 미세 조정한다.

Imagic [21]: 3단계 프로세스를 통해 이미지를 편집합니다. 먼저 토큰 임베딩을 미세 조정한 다음, 미세 조정된 토큰 임베딩을 사용하여 텍스트 유도 이미지 확산 모델의 매개 변수를 미세 조정하고, 마지막으로 보간을 수행하여 대상 프롬프트를 기반으로 다양한 편집을 생성합니다.

DiffEdit [8]: 쿼리 및 참조 텍스트를 기반으로 조건부 확산 모델과 무조건 확산 모델을 대조하여 이미지에서 편집할 영역을 식별합니다. 그리고 편집하고자 하는 영역을 고려하여 잡음/잠복 공간의 특징을 결합하여 텍스트 쿼리로부터 특징을 취합하여 편집 이미지를 재구성한다.

Blended Latent Diffusion [3]: 이 방법은 텍스트 대 이미지 LDM(Latent Diffusion Model)을 사용하여 사용자 정의 마스크 영역을 편집한다. 편집 텍스트로부터 마스크 영역에 대한 특징을 추출하고, 나머지 영상에 대해서는 잡음/잠복 공간에서 원본 영상의 특징을 사용한다.

DirectDiffusion[20]: 입력 이미지를 Stable Diffusion[38]의 잠재 공간으로 반전시킨 후 Prompt2Prompt[17]을 적용하여 편집 확산 분기에 어떠한 변경도 하지 않고 원하는 편집을 얻는다.

Diffusion Disentanglement[48]: 입력 캡션의 텍스트 임베딩과 수행하고자 하는 원하는 편집의 선형 조합을 찾는다. 안정적인 확산 매개변수를 미세 조정하지 않기 때문에 이 방법은 풀린 편집을 수행한다고 주장한다.

InstructPix2Pix(IP2P)[6]: Stable Diffusion[38]의 기초로부터 시작하여, 모델은 명령어 기반 편집 작업을 위해 미세 조정된다. 이것은 테스트-시간 튜닝의 필요없이 소스 이미지를 유지하면서 편집된 이미지가 주어진 지시를 밀접하게 따르도록 보장한다.

InstructPix2Pix w/MagicBrush [52]: MagicBrush train set [52]에서 훈련된 IP2P [6] 버전. MagicBrush 데이터 세트는 더 지역화된 편집 예제를 가지고 있기 때문에 미세 조정된 버전은 탭 1에서 볼 수 있듯이 더 나은 결과를 제공합니다.

Hive [53]: 확장된 데이터 세트로 IP2P [6]을 미세 조정하여 확장합니다. 인간 순위 데이터를 기반으로 개발된 보상 모델과의 미세 조정을 통해 추가 개선이 달성된다.

HIVE w/MagicBrush [52]:HIVE [53] fine-tuned on MagicBrush train set [52]. MagicBrush 데이터 세트는 더 지역화된 편집 예제를 가지고 있기 때문에 미세 조정된 버전은 탭 1에서 볼 수 있듯이 더 나은 결과를 제공합니다.

## 8 추가 실험

### MagicBrush Annotations

섹 5.5에서 언급된 바와 같이, MagicBrush 데이터세트 [52]에 대한 마스크 주석들은 도 7에 도시된 바와 같이, 편집 영역 주위에서 매우 타이트하지 않다. 본 방법은 편집 프로세스 동안 식별된 마스크를 직접 사용하기 때문에, 테스크들이 로컬화된 편집들을 적용하기 위해 편집 영역 주위에서 가능한 한 타이트한 것이 중요하다. 매직브러시의 느슨한 GT 마스크는 왜 우리 모델이 탭에서 더 나쁜 성능을 달성하는지 설명합니다. GT 마스크 사용시 4. 이를 강조하기 위해 그림 1에서 빨간색 원으로 증명한다. 정확한 마스크가 LIME에 제공될 때 7개의 정밀한 편집이 가능합니다. 첫 번째 행 - (a)의 경우, 마스크가 폐색된 영역에서 핸들과 의상 사이의 정확한 경계를 갖는 경우 라켓의 핸들이 보존될 수 있다. 더욱이, 두 번째 행 - (b)는 매직브러시 데이터세트의 마스크가 편집 동안 사용되는 경우, 이 방법이 담요의 색상도 변경한다는 것을 보여준다. 그러나 본 논문에서 제안한 방법으로 추출된 정밀한 마스크로 편집은 영역 내의 객체들을 구별하고 국부적인 편집을 적용할 수 있다.

### Visual Comparison

Vqgan-clip. Tab에 도시된 바와 같이. 1, VQGAN-CLIP [9]는 _CLIP-T_ 메트릭에서 더 나은 결과를 가져옵니다. 이는 CLIP 임베딩을 사용하여 편집된 이미지를 직접 미세 조정하기 때문에 예상된다. 그러나, 도 1에서 보는 바와 같다. 도 8을 참조하면, VQGAN-CLIP에서 편집된 이미지들은 입력 이미지의 세부사항들을 보존하지 못한다. 한편, 본 논문에서 제안하는 방법은 장면의 구조와 세밀한 디테일을 보존하여 원하는 편집을 성공적으로 수행한다. 이는 MagicBrush 데이터 집합에서 Ground truth 편집된 이미지에 대한 값과 유사한 _CLIP-T_ 값을 생성합니다.

혼합 잠재 확산.탭에 표시된 대로입니다. 2, Blended Latent Diffusion [3]은 기준선 및 방법보다 더 나은 결과를 보였다. 다만, 도면에 도시된 바와 같이. 도 9를 참조하면, 그들의 방법이 사용자로부터 주어진 마스크(RoI)에 대해 원하는 편집을 수행할 수 있다고 하더라도, (a) 그것은 특징들 _e.g_, 새들의 머리들의 위치를 왜곡하고, (b) 그것은 입력 이미지에서 객체의 정보를 잃고 RoI, _e.g_, (b)의 블랭킷에 새로운 객체를 생성한다. 한편, 본 논문에서 제안하는 방법은 입력 영상으로부터 최대한 많은 세부 정보를 보존함으로써 주어진 편집 지시 사항을 고려하여 입력 영상에 대한 시각적으로 매력적인 편집을 수행한다. 이것은 또한 탭 2의 방법에 대해 상당히 낮은 거리 메트릭에 의해 강조된다.

Diffusion Disentanglement.Wu et al. [48]은 disentangled 속성 편집 방법을 제안한다. 도 10은 (a) 텍스처 편집 및 (b) 객체를 유사한 것으로 대체하는 것과 같은 편집 유형을 나타낸다. _ diffusion Disentanglement_ on (a)

그림 8: **VQGAN-CLIP [9] 비교.** CLIP-T 메트릭은 각 이미지 아래에 보고 되며 출력 캡션과 해당 이미지 간에 계산 됩니다. 입력 이미지와 편집 지침은 첫 번째 열에 표시됩니다. 지상 진실 편집 이미지는 매직브러시 데이터 집합에서 가져옵니다.

그림 7: **MagicBrush Mask Annotations.** Ground truth (GT)는 MagicBrush [52]의 마스크 주석을 참조 합니다. RoI는 제안된 방법에서 추론된 마스크를 나타낸다. 편집된 이미지들 상의 적색 원들(+편집)은 편집들의 정확한 로컬리제이션이 인식될 수 있는 영역을 강조한다.

그림 9: **BlendedDiffusion [3] 질적 비교.** 입력 이미지를 기반으로 편집한 이미지와 각 행 아래에 보고된 지침을 편집합니다. BlendedDiffusion에 대한 이미지는 PIE-Bench 평가로부터 취해진다[20].

이미지 내의 배경 객체들, _예_, 눈 추가 및 가지 모양을 변경하고, 또한 관심 객체의 특징들, _예_, 새의 꼬리를 제거하는 것을 변경한다. (b)에서는 원하는 편집을 아예 수행하지 못한다. 또한 \(>48\)GB RAM2의 GPU가 필요하며, NVIDIA A100 80GB에서 1개의 이미지가 편집된 버전을 생성하는 데 약 10분이 소요된다. 이에 비해 GPU RAM 사용량이 25GB인 NVIDIA A100 40GB에서 더 높은 시각적 품질을 달성하고 완료하는 데 25초가 걸린다.

각주 2: [https://github.com/UCSB-NLP-Chang/DiffusionDisenqulement](https://github.com/UCSB-NLP-Chang/DiffusionDisenqulement)

### 분할 맵의 정성적 비교

제안된 방법은 확산 과정의 중간 특징들의 클러스터링을 기반으로 한 대안적인 분할 방법을 제안한다. 이 섹션에서는 다른 분할 방법과 정성적 비교를 제공한다. LPM [33]은 한 해상도의 자기 주의 특징(\(32\times 32\)을 사용하는 반면, 본 방법은 다른 해상도의 중간 특징들을 활용하여 분할 맵을 향상시킨다. 그리고, 둘 다 클러스터링 방법을 적용하여 입력 영상에서 세그먼트를 찾는다. 세그먼트를 찾는 또 다른 방법은 큰 세그먼트화 모델들, _예를 들어, SAM[22], ODISE[49]...를 사용하는 것이지만, 이들은 감독된 데이터 및 트레이닝 또는 미세 조정을 필요로 한다. 도면에서 볼 수 있다. 도 11(i)에 도시된 바와 같이, 큰 분할 모델들은 물고기의 투명한 지느러미를 검출할 수 없는 반면, LPM 및 우리의 것은 검출할 수 있다. 또한 LPM은 하나의 해상도만을 이용하기 때문에 강에서 암석을 따로 찾을 수 없다. 도면에서 볼 수 있다. 도 11(ii), ODISE[49] 및 SAM[22]는 손톱과 같은 미세한 물체 부분을 분할하는데 실패하는 반면, LPM 및 우리 것은 이러한 세그먼트를 찾을 수 있다. 또한, 제안된 방법은 LPM보다 높은 해상도로 정확한 경계와 세그먼트를 제공한다. 또한, LPM은 Stable Diffusion[38]을 사용하여 세그먼트를 찾기 위해 실제 이미지 반전을 필요로 하지만, IP2P[6]를 기반으로 하기 때문에 그렇지 않다. 이러한 이유로, LPM은 수행하는 데 1분 이상이 필요한 반면, 우리의 제안은 이미지당 10-15초만 소요됩니다. 그 결과, LPM과 직접적인 비교에서, 우리의 방법은 더 많은 세부사항의 고해상도 분할 맵을 갖는 이점이 있고, 훨씬 더 빠르다. LPM3, SAM4 및 ODISE5의 공개적으로 이용 가능한 공식 구현은 그림의 결과에 사용된다. 11. 또한, 동일한 수의 클러스터가 LPM에 사용되고, 우리의 클러스터는 공정한 비교를 달성한다.

각주 3: [https://github.com/orpatashnik/local-prompt-mixing](https://github.com/orpatashnik/local-prompt-mixing)

각주 4: [https://segment-anything.com/demo](https://segment-anything.com/demo)

각주 5: [https://github.com/NVlab/ODISE](https://github.com/NVlab/ODISE)

### Ablation study

또한 Sec. 5.5의 절제 연구 외에도 Sec. 4.2에 정의된 교차 주의 규칙화 동안 토큰 선택을 분석한다. 텍스트의 \(<\)_시작, _패딩_ 및 _중단 단어_와 같은 관련 없는 토큰의 주의를 규칙화하는 대신 벌점을 부여하여 분석한다. 다음 식에 보고 된 대로 RoI 내에서 관련 토큰 ( \(\tilde{S}\)으로 표시 됨)에 반대 작업을 수행 하 고 높은 값을 제공할 수 있습니다.

\[R(QK^{T},M)=\begin{cases}QK^{T}_{ijt}+\alpha,&\text{if }M_{ij}=1\text{ and }t \in\tilde{S}\\ QK^{T}_{ijt},&\text{otherwise},\end{cases} \tag{4}\]

여기서 \(\alpha\)는 큰 값이다. 이 할당은 편집 명령어와 관련된 관련 토큰이 소프트맥스 동작 후에 높은 주의 점수를 가질 것임을 보장한다. 탭에서 볼 수 있습니다. 도 5를 참조하면, 관련 토큰을 수여하는 대신 관련되지 않은 토큰을 벌칙화하면 큰 개선이 없다. 그러나 관련 없는 토큰을 처벌하는 것은 관련 토큰 간의 주의 점수를 프로세스에 불평등하게 분배할 수 있는 자유를 제공한다. 따라서, 관련 토큰들 중 소프트 할당을 의미한다.

그림 11: **세그먼트 정성적** 입니다. 도전적인 예제에 대한 최신 분할 방법 간의 비교입니다. **

그림 10: **확산 왜곡 [48] 정성적 비교.** 편집은 입력 이미지에 대한 전역 설명을 사용하고 원하는 편집을 ’,’와 연결하여 얻습니다.

### 더 정성적 결과

이 섹션에서는 IP2P [6] 및 IP2P w/MB [52]와 같은 확립된 기준선에 대한 향상된 효과를 강조하면서 이 방법에서 파생된 추가 정성적 결과를 제시한다. 그림 12는 지역화된 이미지 편집 작업에서 우리의 방법의 적용을 보여준다. 구체적으로, (a) _ottoman_, (b) _lamp_, (c) _carpet_ 및 (d) _curtain_와 같은 특정 객체의 색상을 변경하는 방법에 대한 우리의 방법을 보여준다. 관심 대상을 주변 요소와 얽히는 경향이 있는 기본 방법과 달리, 우리의 접근법은 정확하고 엉킨 편집을 달성한다. 이것은 타겟팅된 영역에 대한 변경들을 격리하기보다는 다수의 객체들을 동시에 변경하는 경향이 있는 베이스라인에 의해 달성되지 않는다. 해체 및 국부 편집은 그림 1에 나와 있다. 12는 개체별 편집이 중요한 최종 사용자 애플리케이션에서 LIME의 잠재력을 강조합니다.

도 13은 MagicBrush [52] 테스트 세트 및 PIE-Bench [20] 데이터세트에 대한 본 방법의 성능의 추가적인 예들을 보여준다. 제안하는 방법은 (a) 동물 교체, (b) 다중 객체 수정, (c) 동물의 질감 변경, (d) 다중 객체의 색상 변경 등 다양한 작업을 효과적으로 수행한다. 에 도시된 바와 같이. 도 13을 참조하면, 본 방법은 기존 기준선보다 상당한 개선을 보여준다. 예를 들어, (a)의 IP2P w/MB와 같은 기준 모델은 합리적인 편집을 달성하지만 (b) 및 (c)에서 관찰된 바와 같이 종종 실수로 RoI 외부의 영역을 수정한다. 특히, 이 방법은 기준선이 원래 이미지를 보존하기 위해 고군분투하는 (b), (c) 및 (d)에서 볼 수 있듯이 기준선 모델을 RoI에 집중시키는 데 도움이 된다. 이 방법은 베이스라인에 의존하며 때때로 바닥의 색상인 주변 영역에 의도하지 않은 변화를 유발할 수 있지만 표적 및 국부 편집 측면에서 베이스라인 모델보다 일관되게 성능이 우수하다.

도 14는 Emu-Edit 테스트 세트[41]를 사용한 추가 비교 분석을 제공한다. 이 방법은 (a) 동물의 부분 수정, (b) 특정 객체의 색상 변경, (c) 추가 및 (d) 객체 제거 등 다양한 작업을 성공적으로 처리한다. 도면에 도시된 바와 같이. 도 14에서, 우리의 접근법은 성능에서 기존의 베이스라인들을 상당히 능가한다. 특히, 기준 모델은 개별 부분보다는 전체 객체를 변경하는 경향이 있지만 동물 다리, 우리의 방법은 시나리오 (a)에서 지시된 대로 특정 섹션을 목표로 하고 수정한다. 또한 기준선 모델은 경우 (b) 및 (c)에서 볼 수 있듯이 의도된 RoI를 초과하는 영역에 부주의하게 영향을 미치는 경우가 많다. 대조적으로, 우리의 방법은 RoI 내에서 작업을 제한함으로써 정밀도를 보여준다. 특히 시나리오 (d)에서는 IP2P와 같은 기본 모델이 원본 이미지의 무결성을 유지하기 위해 노력하거나 IP2P w/MB와 같이 객체를 효과적으로 제거하지 못하는 반면, 본 방법은 지정된 객체를 정확하게 제거하여 표적 이미지 조작 작업에서 우수성을 강조한다.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Method** & L1 \(\downarrow\) & L2 \(\downarrow\) & CLIP-I \(\uparrow\) & DINO \(\uparrow\) & CLIP-T \(\uparrow\) \\ \hline IP2P [6] & 0.112 & 0.037 & 0.852 & 0.743 & 0.276 \\ \hline Related & 0.065 & 0.018 & 0.930 & 0.897 & 0.292 \\ Unrelated & **0.058** & **0.017** & **0.935** & **0.906** & **0.293** \\ \hline \hline \end{tabular}
\end{table}
표 5: **토큰 선택에 대한 절제 연구.** 공정한 비교를 위해 절제된 매개 변수를 제외한 모든 설정에 대해 모든 매개 변수가 동일합니다. 최고 성능은 **볼드** 에서 강조 표시 되는 반면 두 번째 성능은 각 블록에 대 한 밑줄로 표시 됩니다.

그림 12: **제안된 방법의 사용 사례입니다.* * 기준선과 방법을 비교하여 다른 개체의 색상을 변경하는 것이 표시됩니다. 제안하는 방법은 장면 내의 서로 다른 색상과 서로 다른 객체에 대해 디덴탈링(disentangled)과 로컬리제이션(localized) 편집을 수행한다.

## 9 Broader Impact & Ethical 고려 사항

지역화된 이미지 편집 기술의 발전은 디지털 미디어 및 가상 현실 응용 프로그램에서 창의적인 표현과 접근성을 향상시킬 수 있는 상당한 잠재력을 가지고 있다. 그러나 특히 딥페이크와 같은 기만적인 이미지를 만드는 오용과 이미지 편집 부문의 고용 시장에 미칠 잠재적 영향에 대해서도 중요한 윤리적 문제를 제기한다. 윤리적 고려 사항은 특히 뉴스 미디어와 같은 민감한 영역에서 책임 있는 사용을 촉진하고 남용을 방지하기 위한 명확한 지침을 수립하며 공정성과 투명성을 보장하는 데 중점을 두어야 한다. 이러한 우려를 해결하는 것은 기술의 위험을 완화하면서 기술의 긍정적인 영향을 극대화하는 데 필수적이다.

그림 13: **더 정성적 예제** (a) 동물 교체, (b) 기존 개체 변경, (c) 질감 변경 및 (d) 여러 개체의 색상 변경과 같은 다양한 작업에 대해 방법을 테스트합니다. 예들은 확립된 벤치마크들로부터 취해진다[20, 52]. LIME의 통합은 모든 모델의 성능을 향상시켜 나머지 이미지 영역의 무결성을 유지하면서 로컬 편집을 가능하게 합니다.

그림 14: **더 정성적 예제** (a) 동물 부분 편집, (b) 색상 변경, (c) 추가 및 (d) 개체 제거와 같은 다양한 작업에 대해 방법을 테스트합니다. 예는 기성 논문에서 따온 것이다[41]. LIME의 통합은 모든 모델의 성능을 향상시켜 나머지 이미지 영역의 무결성을 유지하면서 로컬 편집을 가능하게 합니다.
