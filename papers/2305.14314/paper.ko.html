<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# QLoRA: Quantized LLMs의 효율적인 Finetuning\n' +
      '\n' +
      ' 팀디터스\n' +
      '\n' +
      'Equal contribution.\n' +
      '\n' +
      'Artidoro Pagnoni\n' +
      '\n' +
      'Equal contribution.\n' +
      '\n' +
      'Ari Holtzman\n' +
      '\n' +
      'Luke Zettlemoyer\n' +
      '\n' +
      '워싱턴대학교\n' +
      '\n' +
      '{dettmers,artidoro,ahai,lsz}@cs.washington.edu\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 논문에서는 단일 48GB GPU에서 65B 파라미터 모델을 재조정할 수 있을 만큼 메모리 사용량을 줄이고 전체 16비트 재조정 태스크 성능을 유지하는 효율적인 재조정 기법인 QLoRA를 제안한다. QLoRA는 동결된 4비트 양자화된 사전 훈련된 언어 모델을 통해 구배를 로우 랭크 어댑터(Low Rank Adapters; LoRA)로 역전파한다. 우리가 **Guanaco** 라고 하는 최고의 모델 패밀리는 Vicuna 벤치마크에서 이전에 공개적으로 출시된 모든 모델보다 성능이 우수하여 ChatGPT의 성능 수준의 99.3%에 도달하는 동시에 단일 GPU에서 24시간의 피니튜닝만 필요합니다. QLoRA는 (a) 4-bit NormalFloat (NF4), 정규 분포 가중치에 이론적으로 최적인 정보인 새로운 데이터 유형 (b) 양자화 상수를 양자화하여 평균 메모리 공간을 줄이기 위한 Double Quantization, (c) 메모리 스파이크를 관리하기 위한 Paged Optimizers 등의 여러 가지 혁신을 소개한다. QLoRA를 사용하여 1,000개 이상의 모델을 세분화하여 8개의 명령어 데이터 세트, 여러 모델 유형(LLaMA, T5) 및 정규 피니튜닝(예: 33B 및 65B 매개변수 모델)으로 실행할 수 없는 모델 규모에 걸쳐 명령어 추종 및 챗봇 성능에 대한 자세한 분석을 제공한다. 우리의 결과는 작은 고품질 데이터 세트에 대한 QLoRA 피니튜닝이 이전 SoTA보다 더 작은 모델을 사용하는 경우에도 최첨단 결과로 이어진다는 것을 보여준다. 우리는 GPT-4 평가가 인간 평가에 대한 저렴하고 합리적인 대안임을 보여주는 인간과 GPT-4 평가를 기반으로 챗봇 성능에 대한 자세한 분석을 제공한다. 또한, 현재 챗봇 벤치마크는 챗봇의 성능 수준을 정확하게 평가할 수 없는 것으로 나타났다. 레몬 선택 분석은 ChatGPT와 비교하여 **Guanaco** 가 실패 하는 경우를 보여 줍니다. 4비트 교육을 위한 CUDA 커널을 포함한 모든 모델과 코드를 출시합니다.\n' +
      '\n' +
      '각주 2: [https://github.com/artidoro/qlora](https://github.com/artidoro/qlora) 및 [https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대형 언어 모델(LLM)을 피니튜닝하는 것은 그들의 성능을 향상시키고, 바람직하지 않은 행동을 추가하거나 제거하는 매우 효과적인 방법이다[40; 62; 43; 61; 59; 37]. 그러나, 매우 큰 모델의 finetuning은 엄청나게 비싸다; LLaMA 65B 파라미터 모델의 규칙적인 16비트 finetuning[57]은 780GB 이상의 GPU 메모리를 필요로 한다. 최근의 양자화 방법들은 LLM들의 메모리 풋프린트를 감소시킬 수 있지만[14; 13; 18; 66], 그러한 기법들은 훈련 동안 추론 및 브레이크다운을 위해만 작동한다[65].\n' +
      '\n' +
      '성능 저하 없이 양자화된 4비트 모델을 정밀하게 조정할 수 있음을 처음으로 증명한다. 제안하는 QLoRA는 사전 학습된 모델을 4비트로 양자화하기 위해 새로운 고정밀 기법을 사용하며, 양자화된 가중치를 통해 기울기를 역전파함으로써 튜닝되는 학습 가능한 저순위 어댑터 가중치[28]의 작은 집합을 추가한다.\n' +
      '\n' +
      'QLoRA는 65B 파라미터 모델을 finetuning하는 평균 메모리 요구량을 GPU 메모리의 \\(>\\)780GB에서 \\(<\\)48GB로 16비트 완전 finetuning된 베이스라인에 비해 런타임 또는 예측 성능을 저하시키지 않는다. 이것은 LLM 피니튜닝의 접근성에 상당한 변화를 나타내며, 현재 단일 GPU에서 현재까지 피니튜닝 가능한 최대 공개 모델이다. QLoRA를 사용 하 여 두 번째 최상의 모델이 Vicuna [10] 벤치마크에서 ChatGPT 성능 수준의 97.8%에 도달 하는 동시에 단일 소비자 GPU에서 12 시간 이내에 교육할 수 있습니다. 24 시간 동안 단일 전문 GPU를 사용 하면 최대 모델에서 99.3%를 달성 하 여 기본적으로 Vicuna 벤치마크에서 ChatGPT에 대 한 격차를 좁힙니다. 배포할 때 가장 작은 **Guanaco** 모델(7B 매개 변수)은 5GB의 메모리만 필요하며 Vicuna 벤치마크에서 26GB Alpaca 모델보다 20% 포인트 이상 성능이 뛰어납니다 (표 6).\n' +
      '\n' +
      'QLoRA는 성능을 희생하지 않고 메모리 사용을 줄이도록 설계된 여러 혁신을 도입합니다. (1) **4비트 NormalFloat** 는 이론적으로 4비트 Integers 및 4비트 Float보다 더 나은 경험적 결과를 산출하는 정규 분포 데이터에 대한 최적의 양자화 데이터 유형입니다. (2) **이중 양자화** - 양자화 상수를 양자화하는 방법으로 매개 변수당 평균 약 0.37비트(65B 모델의 경우 약 3GB)를 절약합니다. (3) NVIDIA 통합 메모리를 사용 하 여 긴 시퀀스 길이를 사용 하 여 미니 배치를 처리할 때 발생 하는 그래디언트 체크 포인팅 메모리 스파이크를 방지 하는 **페이지 된 최적화기** 입니다. 이러한 기여를 모든 네트워크 계층에 어댑터를 포함하는 더 나은 조정된 LoRA 접근법으로 결합하여 이전 작업에서 볼 수 있는 거의 모든 정확도 절충을 방지한다.\n' +
      '\n' +
      'QLoRA의 효율성을 통해 메모리 오버헤드로 인해 정규 피니튜닝을 사용할 수 없는 모델 스케일에서 명령어 피니튜닝과 챗봇 성능에 대한 심층 연구를 수행할 수 있다. 따라서 우리는 80M에서 65B 사이의 여러 명령어 튜닝 데이터 세트, 모델 아키텍처 및 크기에 걸쳐 1,000개 이상의 모델을 훈련한다. QLoRA가 16 비트 성능 (SS4)을 복구 하 고 최신 챗봇 **Guanaco**, (SS5)를 훈련 하는 것 외에도 훈련 된 모델의 추세를 분석 합니다. 첫째, 일반화에 따른 명령어를 지원하기 위해 9k 샘플 데이터셋(OASST1)이 450k 샘플 데이터셋(FLAN v2, 서브샘플링)보다 챗봇 성능에서 훨씬 더 중요하다는 것을 발견했다. 둘째, 강력한 MMLU(Massive Multitask Language Understanding) 벤치마크 성능은 강력한 Vicuna 챗봇 벤치마크 성능을 의미하지 않으며, 그 반대의 경우도 마찬가지이다.\n' +
      '\n' +
      '또한, 평가를 위해 인간 평가자와 GPT-4를 모두 사용하는 챗봇 성능에 대한 광범위한 분석을 제공한다. 주어진 프롬프트에 대한 최상의 응답을 생성하기 위해 모델이 경기에서 서로 경쟁하는 토너먼트 방식의 벤치마킹을 사용합니다. 매치의 승자는 GPT-4 또는 인간 주석자에 의해 판단된다. 토너먼트 결과는 챗봇 성능 순위를 결정하는 엘로 점수[16; 17]로 집계된다. 우리는 GPT-4와 인간 평가가 토너먼트에서 모델 성능의 순위에 대체로 동의하지만 강한 불일치의 경우도 있음을 발견했다. 따라서 우리는 인간 주석의 값싼 대안을 제공하면서 모델 기반 평가에도 불확실성이 있음을 강조한다.\n' +
      '\n' +
      '**Guanaco** 모델의 정성적 분석으로 챗봇 벤치마크 결과를 보강합니다. 우리의 분석에서는 정량적 벤치마크에 포착되지 않은 성공 및 실패 사례를 강조한다.\n' +
      '\n' +
      '추가 연구를 용이하게 하기 위해 인간 및 GPT-4 주석이 있는 모든 모델 세대를 릴리스한다. 우리는 코드 베이스와 CUDA 커널을 오픈 소스화하고 방법을 포옹 얼굴 변환기 스택[64]에 통합하여 누구나 쉽게 액세스할 수 있다. 데이터 세트에 따라 8가지 다른 지침에 대해 훈련된 7/13/33/65B 크기 모델에 대한 어댑터 모음을 총 32가지 다른 오픈 소싱, 미세 조정된 모델에 대해 출시한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Model & Size & Elo \\\\ \\hline GPT-4 & - & 1348 \\(\\pm\\) 1 \\\\ Guanaco 65B & 41 GB & 1022 \\(\\pm\\) 1 \\\\ Guanaco 33B & 21 GB & 992 \\(\\pm\\) 1 \\\\ Vicuna 13B & 26 GB & 974 \\(\\pm\\) 1 \\\\ ChatGPT & - & 966 \\(\\pm\\) 1 \\\\ Guanaco 13B & 10 GB & 916 \\(\\pm\\) 1 \\\\ Bard & - & 902 \\(\\pm\\) 1 \\\\ Guanaco 7B & 6 GB & 879 \\(\\pm\\) 1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 모델 간의 경쟁에 대한 Elo 등급, 10,000개의 무작위 초기 주문에 대해 평균을 냈다. 매치의 승자는 비쿠나 벤치마크의 주어진 프롬프트에 대해 어떤 응답이 더 나은지를 선언하는 GPT-4에 의해 결정된다. 95% 신뢰구간이 (\\(\\pm\\)) 표시된다. GPT-4 이후 구아나코 33B와 65B가 최다승을 거뒀고, 구아나코 13B가 바드보다 득점이 좋다.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '블록-와이즈 k-비트 양자화 양자화는 더 많은 정보를 보유하는 표현으로부터 더 적은 정보를 갖는 표현으로 입력을 이산화하는 프로세스이다. 그것은 종종 더 많은 비트로 데이터 유형을 취하고 더 적은 비트로 변환하는 것을 의미하는데, 예를 들어 32비트 플로트에서 8비트 인테거로 변환된다. 저-비트 데이터 타입의 전체 범위가 사용되는 것을 보장하기 위해, 입력 데이터 타입은 일반적으로 텐서로서 구조화되는 입력 엘리먼트들의 절대 최대값에 의한 정규화를 통해 타겟 데이터 타입 범위로 리스케일링된다. 예를 들어, 32비트 부동 소수점(FP32) 텐서를 범위 \\([-127,127]\\)을 갖는 Int8 텐서로 양자화하는 것:\n' +
      '\n' +
      '\\[\\mathbf{X}^{\\text{Int8}}=\\text{round}\\left(\\frac{127}{\\text{absmax}(\\mathbf{X} ^{\\text{FP32}})}\\mathbf{X}^{\\text{FP32}}\\right)=\\text{round}(c^{\\text{FP32}} \\cdot\\mathbf{X}^{\\text{FP32}}), \\tag{1}\\]\n' +
      '\n' +
      '여기서, \\(c\\)는 _양자화 상수_ 또는 _양자화 스케일_이다. 역양자화는 역이다:\n' +
      '\n' +
      '\\[\\text{dequant}(c^{\\text{FP32}},\\mathbf{X}^{\\text{Int8}})=\\frac{\\mathbf{X}^{ \\text{Int8}}}{c^{\\text{FP32}}=\\mathbf{X}^{\\text{FP32}} \\tag{2}\\]\n' +
      '\n' +
      '이 접근법의 문제점은 입력 텐서에서 큰 크기 값(즉, 이상치)이 발생하면, 양자화 빈 - 특정 비트 조합 - 은 일부 빈에서 양자화된 숫자가 거의 또는 전혀 없이 잘 활용되지 않는다는 것이다. 특이치 문제를 방지하기 위해, 공통 접근법은 입력 텐서를 각각 고유한 양자화 상수 \\(c\\)를 갖는 독립적으로 양자화된 블록으로 청크하는 것이다. 이것은 다음과 같이 정형화될 수 있다. 입력 텐서를 평탄화하고 선형 세그먼트를 \\(n=(b\\times h)/B\\) 블록으로 슬라이싱함으로써 입력 텐서 \\(\\mathbf{X}\\in\\mathbb{R}^{b\\times h}\\)를 \\(n\\) 크기의 인접 블록으로 분할한다. 우리는 양자화된 텐서와 \\(n\\) 양자화 상수 \\(c_{i}\\)를 생성하기 위해 이 블록들을 수학식 1로 독립적으로 양자화한다.\n' +
      '\n' +
      'Low-rank AdaptersLow-rank Adapter (LoRA) finetuning [28]은 고정된 채로 유지되는 전체 모델 파라미터를 업데이트하지 않으면서, 종종 어댑터라고 불리는 트레이닝 가능한 파라미터들의 작은 세트를 사용함으로써 메모리 요구들을 감소시키는 방법이다. 확률적 구배 하강 동안의 구배는 고정된 사전 훈련된 모델 가중치를 통해 어댑터로 전달되며, 어댑터는 손실 함수를 최적화하도록 업데이트된다. LoRA는 추가적인 인수분해된 투영을 통해 선형 투영을 증강한다. 투영 \\(\\mathbf{X}\\mathbf{W}=\\mathbf{Y}\\)이 주어지면 \\(\\mathbf{X}\\in\\mathbb{R}^{b\\times h}\\), \\(\\mathbf{W}\\in\\mathbb{R}^{h\\times c}\\) LoRA 계산은 다음과 같습니다.\n' +
      '\n' +
      '\\[\\mathbf{Y}=\\mathbf{X}\\mathbf{W}+s\\mathbf{X}\\mathbf{L}_{1}\\mathbf{L}_{2}, \\tag{3}\\]\n' +
      '\n' +
      '여기서 \\(\\mathbf{L}_{1}\\in\\mathbb{R}^{h\\times r}\\), \\(\\mathbf{L}_{2}\\in\\mathbb{R}^{r\\times o}\\), \\(s\\)는 스칼라이다.\n' +
      '\n' +
      '파라미터 효율적인 Finetuning의 메모리 요구 사항 한 가지 중요한 논의 사항은 사용되는 어댑터의 수와 크기 측면에서 훈련 중 LoRA의 메모리 요구 사항이다. LoRA의 메모리 풋프린트가 매우 미미하기 때문에, 우리는 사용된 총 메모리를 크게 증가시키지 않으면서 성능을 향상시키기 위해 더 많은 어댑터를 사용할 수 있다. LoRA는 설계되었지만\n' +
      '\n' +
      '도 1: 상이한 피니튜닝 방법들 및 그들의 메모리 요건들. QLoRA는 트랜스포머 모델을 4비트 정밀도로 양자화하고 메모리 스파이크를 처리하기 위해 페이징된 최적화기를 사용하여 LoRA보다 개선한다.\n' +
      '\n' +
      '파라미터 효율적인 Finetuning(PEFT) 방법, LLM finetuning을 위한 메모리 풋프린트의 대부분은 학습된 LoRA 파라미터가 아닌 활성화 구배에서 비롯된다. 배치 크기가 1인 FLAN v2에서 훈련된 7B LLaMA 모델의 경우 LoRA 가중치는 원래 모델 가중치의 0.2%와 동일하며[28, 37], LoRA 입력 기울기는 567MB의 메모리 공간을 갖는 반면 LoRA 매개변수는 26MB만 차지한다. 기울기 체크포인팅 [9]를 사용하면 입력 기울기가 시퀀스당 평균 18MB로 감소하여 모든 LoRA 가중치가 결합된 것보다 메모리 집약적이다. 이에 비해, 4비트 베이스 모델은 5,048MB의 메모리를 소모한다. 이것은 기울기 체크포인팅이 중요하지만 LoRA 매개변수의 양을 적극적으로 줄이는 것이 약간의 메모리 이점만을 산출한다는 것을 강조한다. 이것은 우리가 전체 트레이닝 메모리 풋프린트를 크게 증가시키지 않고 더 많은 어댑터를 사용할 수 있다는 것을 의미한다(상세한 고장은 부록 G 참조). 후술하는 바와 같이, 이것은 완전한 16비트 정밀도 성능을 회복하는 데 중요하다.\n' +
      '\n' +
      '## 3 QLoRA Finetuning\n' +
      '\n' +
      'QLoRA는 4-bit NormalFloat(NF4) 양자화와 Double Quantization의 두 가지 기법을 통해 고충실도 4-bit finetuning을 달성한다. 또한, 그레이디언트 체크포인팅 중 메모리 스파이크가 전통적으로 대형 모델의 경우 단일 기계에서 피니튜닝을 어렵게 만드는 메모리 아웃-오류를 유발하는 것을 방지하기 위해 페이지 최적화기를 소개한다.\n' +
      '\n' +
      'QLoRA는 일반적으로 4비트의 저정밀 저장 데이터 타입과 일반적으로 BFloat16의 계산 데이터 타입을 가지고 있다. 실제로, 이것은 QLoRA 가중치 텐서를 사용할 때마다 텐서를 BFloat16으로 역양자화하고 16비트의 행렬 곱셈을 수행한다.\n' +
      '\n' +
      '이제 QLoRA의 구성 요소에 대해 논의한 다음 QLoRA의 공식 정의에 대해 논의한다.\n' +
      '\n' +
      '4-비트 NormalFloat Quantization NormalFloat(NF) 데이터 타입은 각 양자화 빈이 입력 텐서로부터 할당된 동일한 수의 값들을 갖도록 보장하는 정보이론적으로 최적의 데이터 타입인 Quantile Quantization[15]에 기초한다. 분위수 양자화는 경험적 누적 분포 함수를 통해 입력 텐서의 분위수를 추정함으로써 작동한다.\n' +
      '\n' +
      '분위수 양자화의 주요 한계는 분위수 추정의 프로세스가 비용이 많이 든다는 것이다. 따라서 SRAM 분위수[15]와 같은 빠른 분위수 근사 알고리즘을 사용하여 이를 추정한다. 이러한 분위 추정 알고리즘의 근사적 특성으로 인해 데이터 유형은 종종 가장 중요한 값인 이상치에 대한 양자화 오차가 크다.\n' +
      '\n' +
      '입력 텐서들이 양자화 상수까지 고정된 분포로부터 올 때 값비싼 분위수 추정치들 및 근사 오차들을 피할 수 있다. 그러한 경우들에서, 입력 텐서들은 정확한 분위수 추정을 계산 가능하게 하는 동일한 분위수들을 갖는다.\n' +
      '\n' +
      '사전 훈련된 신경망 가중치는 보통 표준 편차 \\(\\sigma\\)(부록 F 참조)를 갖는 제로 중심 정규 분포를 갖기 때문에, 분포가 데이터 유형의 범위에 정확하게 맞도록 스케일링 \\(\\sigma\\)함으로써 모든 가중치를 단일 고정 분포로 변환할 수 있다. 데이터 타입은 임의의 범위 \\([-1,1]\\)을 설정한다. 이와 같이, 데이터 타입에 대한 분위수들 및 신경망 가중치들 둘 모두는 이 범위로 정규화될 필요가 있다.\n' +
      '\n' +
      '(1) 이론상의 \\(N(0,1)\\) 분위수를 추정하여 정규분포에 대한 \\(k\\)-비트 양자화 데이터 유형을 얻는다. (2) 이 데이터 유형을 취하여 그 값을 \\([-1,1]\\) 범위로 정규화하고, (3) 입력 가중치 텐서를 절대 최대 리스케일을 통해 \\([-1,1]\\) 범위로 정규화하여 양자화한다.\n' +
      '\n' +
      '가중치 범위와 데이터 유형 범위가 일치하면 우리는 평소와 같이 양자화할 수 있다. 단계 (3)은 k 비트 데이터 타입의 표준 편차와 일치하도록 가중치 텐서의 표준 편차를 리스케일링하는 것과 동등하다. 더 형식적으로, 우리는 다음과 같이 데이터 유형의 \\(2^{k}\\) 값 \\(q_{i}\\)을 추정한다.\n' +
      '\n' +
      '\\[q_{i}=\\frac{1}{2}\\left(Q_{X}\\left(\\frac{i}{2^{k}+1}\\right)+Q_{X}\\left(\\frac{i +1}{2^{k}+1}\\right)\\right), \\tag{4}\\]\n' +
      '\n' +
      '여기서 \\(Q_{X}(\\cdot)\\)는 표준 정규 분포 \\(N(0,1)\\)의 분위 함수이다. 대칭 k-비트 양자화에 대한 문제는 이 접근법이 0의 정확한 표현을 갖지 않는다는 것이며, 이는 패딩 및 다른 제로-값 엘리먼트들을 에러 없이 양자화하는 데 중요한 속성이다. k-비트 데이터 형식에서 \\(0\\)의 이산 영점을 구하고, 모든 \\(2^{k}\\) 비트를 사용하기 위해, 음부분과 양부분에서 \\(2^{k-1}+1\\)의 두 범위의 분위수 \\(q_{i}\\)을 추정하여 비대칭 데이터 유형을 생성한 후, 이 \\(q_{i}\\)의 집합을 단일화하고 두 집합에서 발생하는 두 영점 중 하나를 제거한다. 각 양자화 bin \\(k\\)_-bit NormalFloat_ (NFk)에서 동일한 예상 값을 갖는 결과 데이터 유형을 명명하는데, 이는 데이터 유형이 제로 중심의 정규 분포 데이터에 대해 이론적으로 최적이기 때문이다. 이 데이터 유형의 정확한 값은 부록 E에서 찾을 수 있다.\n' +
      '\n' +
      'Double Quantization 추가 메모리 절감을 위해 양자화 상수를 양자화하는 프로세스인 _Double Quantization_ (DQ)를 도입합니다. 정확한 4비트 양자화를 위해서는 작은 블록화가 필요하지만[13], 또한 상당한 메모리 오버헤드를 갖는다. 예를 들어, 32비트 상수와 64의 블록화를 \\(\\mathbf{W}\\)에 대해 사용하면, 양자화 상수는 파라미터당 평균으로 \\(32/64=0.5\\)비트를 추가한다. 이중 양자화는 양자화 상수의 메모리 공간을 줄이는 데 도움이 됩니다.\n' +
      '\n' +
      '보다 구체적으로, 이중 양자화는 첫 번째 양자화의 양자화 상수 \\(c_{2}^{\\text{FP32}}\\)를 두 번째 양자화의 입력으로 처리한다. 이 두 번째 단계는 양자화된 양자화 상수 \\(c_{2}^{\\text{FR}}\\)와 두 번째 수준의 양자화 상수 \\(c_{1}^{\\text{FP32}}\\)을 산출한다. 2차 양자화를 위해 256 블록 크기의 8비트 Floats를 사용하였는데, 이는 Dettmers와 Zettlemoyer [13]의 결과와 일치한다. \\(c_{2}^{\\text{FP32}}\\)는 양수이므로 양자화 전에 \\(c_{2}\\)에서 평균을 빼서 0을 중심으로 하고 대칭 양자화를 사용한다. 평균적으로 64 블록 크기의 경우, 이 양자화는 매개변수당 메모리 공간을 \\(32/64=0.5\\) 비트에서 매개변수당 0.373 비트의 감소인 \\(8/64+32/(64\\cdot 256)=0.127\\) 비트로 감소시킨다.\n' +
      '\n' +
      'Paged Optimizers는 NVIDIA 통합 메모리 3 기능을 사용하여 GPU가 메모리가 부족할 때 오류가 없는 GPU 처리를 위해 CPU와 GPU 간에 페이지 간 자동 전송을 수행한다. 이 기능은 CPU RAM과 디스크 간의 일반 메모리 페이징과 같이 작동합니다. 본 논문에서는 최적화기 업데이트 단계에서 메모리가 필요할 때 GPU가 메모리 부족 시 CPU RAM으로 자동 퇴거되고 GPU 메모리로 다시 페이징되는 최적화기 상태에 대해 페이징된 메모리를 할당하는 기능을 사용한다.\n' +
      '\n' +
      '각주 3: [https://docs.nvidia.com/cuda/cuda-c-programming-guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide)\n' +
      '\n' +
      'QLoRA. 위에서 설명한 구성 요소를 사용하여 단일 LoRA 어댑터를 사용하여 양자화된 기본 모델에서 단일 선형 계층에 대한 QLoRA를 다음과 같이 정의합니다.\n' +
      '\n' +
      '\\[\\mathbf{Y}^{\\text{BF16}}=\\mathbf{X}^{\\text{BF16}}\\text{doubleDequant}(c_{1}^{ \\text{FP32}},c_{2}^{\\text{k-bit}},\\mathbf{W}^{\\text{NF4}})+\\mathbf{X}^{\\text{BF16}}\\mathbf{L}_{1}^{\\text{BF16}}\\mathbf{L}_{2}^{\\text{BF16}}, \\tag{5}\\]\n' +
      '\n' +
      '여기서 \\(\\text{doubleDequant}(\\cdot)\\)는 다음과 같이 정의됩니다.\n' +
      '\n' +
      '\\[\\text{doubleDequant}(c_{1}^{\\text{FP32}},c_{2}^{\\text{k-bit}},\\mathbf{W}^{ \\text{k-bit}})=\\text{dequant}(\\text{dequant}(c_{1}^{\\text{FP32}},c_{2}^{\\text{ k-bit}}),\\mathbf{W}^{\\text{4bit}})=\\mathbf{W}^{\\text{BF16}}, \\tag{6}\\]\n' +
      '\n' +
      '우리는 \\(\\mathbf{W}\\)에는 NF4를 사용하고 \\(c_{2}\\)에는 FP8을 사용한다. 더 높은 양자화 정밀도를 위해 64\\(\\mathbf{W}\\)의 블록화와 메모리를 보존하기 위해 256\\(c_{2}\\)의 블록화를 사용한다.\n' +
      '\n' +
      '파라미터 갱신의 경우 어댑터 가중치 \\(\\frac{\\partial E}{\\partial\\mathbf{L}_{i}}\\)에 대한 오차에 대한 기울기만 필요하며, 4비트 가중치 \\(\\frac{\\partial E}{\\partial\\mathbf{W}}\\)에는 필요하지 않다. 그러나, \\(\\frac{\\partial E}{\\partial\\mathbf{L}_{i}}\\)의 계산에는 \\(\\frac{\\partial\\mathbf{X}}{\\partial\\mathbf{W}}\\)의 계산이 수반된다. \\(\\mathbf{W}^{\\text{NF4}}\\)에서 연산 데이터 유형 \\(\\mathbf{W}^{\\text{BF16}\\)으로 역양자화하여 BFloat16 정밀도에서 미분 \\(\\frac{\\partial\\mathbf{X}}{\\partial\\mathbf{W}}\\)을 계산한다.\n' +
      '\n' +
      '요약하자면, QLoRA는 하나의 저장 데이터 유형(보통 4비트 NormalFloat)과 계산 데이터 유형(16비트 BrainFloat)을 가지고 있다. 우리는 저장 데이터 유형을 계산 데이터 유형으로 역양자화하여 순방향 및 역방향 패스를 수행하지만, 16비트 BrainFloat를 사용하는 LoRA 매개변수에 대한 가중치 기울기만 계산한다.\n' +
      '\n' +
      '## 4 QLoRA vs. 표준 파인튜닝\n' +
      '\n' +
      '우리는 QLoRA가 어떻게 작동하는지와 피니튜닝 모델에 필요한 메모리를 크게 줄일 수 있는지 논의했다. 이제 주요 문제는 QLoRA가 풀 모델 피니튜닝뿐만 아니라 수행할 수 있는지 여부이다. 또한 표준 Float4에 대한 NormalFloat4의 영향을 포함하여 QLoRA의 구성 요소를 분석하고자 한다. 다음 섹션에서는 이러한 질문에 답하기 위한 실험에 대해 논의한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      '가능하지만, 16비트 [13; 18]에 비해 성능 저하를 초래한다. 이는 4비트 어댑터 피니튜닝을 수행하여 손실된 성능을 복구할 수 있는지에 대한 중요한 질문을 제기한다. 두 가지 설정에 대해 테스트합니다.\n' +
      '\n' +
      '첫 번째는 GLUE 및 Super-NaturalInstructions 데이터세트에서 125M에서 3B 크기의 RoBERTA 및 T5 모델의 전체 16비트 피니튜닝과의 비교에 중점을 둔다. 결과는 표 3에 나와 있다. 두 데이터 세트 모두에서 16비트, 8비트 및 4비트 어댑터 방법이 완전히 미세 조정된 16비트 기준선의 성능을 복제한다는 것을 관찰했다. 이는 부정확한 양자화로 인해 손실된 성능이 양자화 후 어댑터 핀튜닝을 통해 완전히 복구될 수 있음을 시사한다.\n' +
      '\n' +
      '두 번째 설정의 경우 11B 매개 변수에서 및 그 이상의 전체 피니튜닝 모델에는 하나 이상의 메모리 GPU 서버가 필요하기 때문에 4비트 QLoRA가 7B에서 65B 매개 변수 척도에서 16비트 LoRA와 일치할 수 있는지 여부를 계속 테스트한다. 이를 위해 데이터 세트 Alpaca와 FLAN v2에 따른 두 가지 지침에 대해 LLaMA 7B에서 65B까지 세분화하고 5샷 정확도를 통해 MMLU 벤치마크에서 평가한다. 결과는 이중 양자화를 사용한 NF4가 16비트 LoRA MMLU 성능을 완전히 회복한다는 것을 알 수 있는 표 4에 나와 있다. 또한 FP4를 사용한 QLoRA가 16비트 뇌 플로팅 LoRA 기준선보다 약 1% 포인트 늦다는 점에 주목한다. 이것은 (1) NF4를 사용한 QLoRA가 16비트 전체 finetuning과 16비트 LoRA finetuning 성능을 모두 복제하고 (2) NF4가 양자화 정밀도 측면에서 FP4보다 우수하다는 우리의 발견을 확증한다.\n' +
      '\n' +
      '요약 우리의 결과는 NF4 데이터 유형을 가진 4비트 QLoRA가 잘 확립된 평가 설정을 가진 학술 벤치마크에서 16비트 전체 피니튜닝 및 16비트 LoRA 피니튜닝 성능과 일치한다는 것을 일관되게 보여준다. 또한 NF4가 FP4보다 더 효과적이며 이중 양자화가 성능을 저하시키지 않는다는 것을 보여주었다. 이를 결합하면 4비트 QLoRA 튜닝이 16비트 방법과 일치하는 결과를 안정적으로 산출한다는 강력한 증거를 형성한다.\n' +
      '\n' +
      '양자화에 대한 이전 연구 [13]에 따르면, MMLU 및 Elo 결과는 주어진 피니튜닝 및 추론 리소스 예산을 사용하면 기본 모델의 매개변수 수를 증가시키면서 정밀도를 감소시키는 것이 유익하다는 것을 나타낸다. 이것은 QLoRA의 효율성 이점의 중요성을 강조한다. 4비트 피니튜닝을 사용한 실험에서 전체 피니튜닝에 비해 성능 저하를 관찰하지 않았기 때문에 이는 성능-정밀 절충이 QLoRA 튜닝에 정확히 어디에 있는지에 대한 질문을 제기하며, 이는 향후 연구 과제로 남겨둔다.\n' +
      '\n' +
      '우리는 학술 연구 하드웨어에 대한 전체 16비트 피니튜닝으로 탐구할 수 없는 스케일에서 명령어 튜닝을 조사한다.\n' +
      '\n' +
      '## 5 Pushing the Chatbot State-of-art with QLoRA\n' +
      '\n' +
      '4비트 QLoRA가 규모, 작업 및 데이터 세트에 걸쳐 16비트 성능과 일치한다는 것을 확인한 후 연구에 사용할 수 있는 가장 큰 오픈 소스 언어 모델까지 명령어 피니튜닝에 대한 심층 연구를 수행한다. 이 모델들을 조정하는 명령의 성능을 평가하기 위해, 우리는 평가한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Data type & Mean PPL \\\\ \\hline Int4 & 34.34 \\\\ Float4 (E2M1) & 31.07 \\\\ Float4 (E3M0) & 29.48 \\\\ NFFloat4 + DQ & **27.41** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 파일 공통 크롤은 125M 내지 13B OPT, BLOOM, LLaMA 및 피티아 모델에 대한 상이한 데이터 유형에 대한 복잡성을 의미한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline Dataset & GLUE (Acc.) & \\multicolumn{4}{c}{Super-NaturalInstructions (RougeL)} \\\\ Model & RoBERTa-large & T5-80M & T5-250M & T5-780M & T5-3B & T5-11B \\\\ \\hline BF16 & 88.6 & 40.1 & 42.1 & 48.0 & 54.3 & 62.0 \\\\ BF16 replication & 88.6 & 40.0 & 42.2 & 47.3 & 54.9 & - \\\\ \\hline LoRA BF16 & 88.8 & 40.5 & 42.6 & 47.1 & 55.4 & 60.7 \\\\ QLoRA Int8 & 88.8 & 40.4 & 42.9 & 45.4 & 56.5 & 60.7 \\\\ QLoRA FP4 & 88.6 & 40.3 & 42.4 & 47.5 & 55.6 & 60.9 \\\\ QLoRA NF4 + DQ & - & 40.4 & 42.7 & 47.7 & 55.3 & 60.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: GLUE 및 Super-Natural Instructions 상에서 16-bit BrainFloat(BF16), 8-bit Integer(Int8), 4-bit Float(FP4) 및 4-bit NormalFloat(NF4)를 비교한 실험. QLoRA는 16비트 LoRA와 풀 피니튜닝을 복제한다.\n' +
      '\n' +
      '도전적인 자연어 이해 벤치마크(MMLU)를 사용하여 실제 챗봇의 성능 평가를 위한 새로운 방법을 개발한다.\n' +
      '\n' +
      '### Experimental setup\n' +
      '\n' +
      '이제 부록 B의 전체 세부 사항과 함께 실험 설정에 대한 개요를 설명한다.\n' +
      '\n' +
      '데이터에 따르면 최근 지침에 따른 데이터 세트에 대한 포괄적인 연구가 없기 때문에 8개의 최근 데이터 세트를 선택한다. 군중 소싱을 통해 얻은 데이터 세트(OASST1 [31], HH-RLHF [4]), 명령어 조정 모델로부터의 증류(알파카 [55], 자체 지시 [59], 비자연 지시 [26]), 말뭉치 집합(FLAN v2 [12]), 하이브리드(Chip2 [32], Long-form [30])를 포함한다. 이러한 데이터 세트는 다양한 언어, 데이터 크기 및 라이선스를 포함합니다.\n' +
      '\n' +
      '훈련 설정 서로 다른 훈련 목표로부터 교란 효과를 피하기 위해, 서로 다른 응답에 대한 인간의 판단을 포함하는 데이터 세트에 대해서도 강화 학습 없이 교차 엔트로피 손실(지도 학습)로 QLoRA 피니튜닝을 수행한다. 명령어와 응답의 명확한 구분이 있는 데이터 세트의 경우, 우리는 응답에만 미세 조정한다(부록 B의 절편을 참조). OASST1 및 HH-RLHF의 경우 여러 응답을 사용할 수 있다. 그런 다음 대화 트리의 모든 수준에서 상위 응답을 선택하고 지침을 포함하여 전체 선택된 대화에서 피니튜닝한다. 모든 실험에서 이중 양자화와 페이징된 최적화기가 있는 NF4 QLoRA를 사용하여 기울기 체크포인팅 동안 메모리 스파이크를 방지한다. 13B 및 33B LLaMA 모델에 대해 작은 하이퍼파라미터 검색을 수행하고 7B에서 발견된 모든 하이퍼파라미터 설정이 학습 속도 및 배치 크기를 제외하고 일반화(에폭 수 포함)됨을 발견했다. 우리는 배치 크기를 두 배로 늘리면서 33B와 65B의 학습률을 절반으로 줄인다.\n' +
      '\n' +
      '기본 사항 연구(Vicuna [10] 및 Open Assistant [31]) 및 상업용(GPT-4 [42], GPT-3.5-turbo 및 Bard) 챗봇 시스템과 모델을 비교합니다. Open Assistant 모델은 우리가 실험하는 동일한 OASST1 데이터세트에서 RLF( Reinforcement Learning from Human Feedback)로 피니튜닝된 LLaMA 33B 모델이다. 비쿠나는 ShareGPT의 독점 사용자 공유 대화에서 LLaMA 13B를 완전히 미세 조정하므로 OpenAI GPT 모델로부터의 증류의 결과이다.\n' +
      '\n' +
      '### Evaluation\n' +
      '\n' +
      '일반적인 관행에 따라, 우리는 다양한 언어 이해 과제에 대한 성능을 측정하기 위해 MMLU(Massively Multitask Language Understanding) 벤치마크 [24]를 사용한다. 이는 초등수학, 미국사, 컴퓨터과학, 법학 등 57개 과제를 포괄하는 객관식 벤치마크이다. 5샷 테스트 정확도를 보고합니다.\n' +
      '\n' +
      '또한 자동화된 평가와 인간 평가를 통해 생성 언어 기능을 테스트합니다. 이 두 번째 평가 세트는 인간이 선별한 쿼리에 의존하며 모델 응답의 품질을 측정하는 것을 목표로 한다. 이것은 챗봇 모델 성능을 위한 보다 현실적인 테스트베드로 인기가 높아지고 있지만, 문헌에는 일반적으로 받아들여지는 프로토콜이 없다. 모든 경우에 \\(p=0.9\\) 및 \\(0.7\\)의 핵 샘플링을 사용하여 제안된 설정을 아래에 설명한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline \\multirow{2}{*}{\n' +
      '\\begin{tabular}{} \\end{tabular} } & \\multicolumn{6}{c}{Mean 5-shot MMLU Accuracy} \\\\ \\cline{2-9}  & \\multicolumn{2}{c}{7B} & \\multicolumn{2}{c}{13B} & \\multicolumn{2}{c}{33B} & \\multicolumn{2}{c}{65B} \\\\ \\cline{2-9}  & Alpaca & FLAN v2 & Alpaca & FLAN v2 & Alpaca & FLAN v2 & Alpaca & FLAN v2 \\\\ \\hline BFloat16 & 38.4 & 45.6 & 47.2 & 50.6 & 57.7 & 60.5 & 61.8 & 62.5 & 53.0 \\\\ Float4 & 37.2 & 44.0 & 47.3 & 50.0 & 55.9 & 58.5 & 61.3 & 63.3 & 52.2 \\\\ NFloat4 + DQ & 39.0 & 44.5 & 47.5 & 50.7 & 57.3 & 59.2 & 61.8 & 63.9 & 53.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 상이한 데이터 유형에 대해 알파카 및 FLAN v2 상의 어댑터로 피닝된 LLaMA 7-65B 모델에 대한 평균 5-샷 MMLU 테스트 정확도. 전반적으로 이중 양자화(DQ)가 있는 NF4는 BFloat16 성능과 일치하는 반면 FP4는 둘 다에 일관되게 1% 포인트 뒤진다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Dataset & 7B & 13B & 33B & 65B \\\\ \\hline LLaMA no tuning & 35.1 & 46.9 & 57.8 & 63.4 \\\\ \\hline Self-Instruct & 36.4 & 33.3 & 53.0 & 56.7 \\\\ Longform & 32.1 & 43.2 & 56.6 & 59.7 \\\\ Chip2 & 34.5 & 41.6 & 53.6 & 59.8 \\\\ HH-RLHF & 34.9 & 44.6 & 55.8 & 60.1 \\\\ Unnatural Instruct & 41.9 & 48.1 & 57.3 & 61.3 \\\\ Guanaco (OASST1) & 36.6 & 46.4 & 57.0 & 62.2 \\\\ Alpaca & 38.8 & 47.8 & 57.3 & 62.5 \\\\ FLAN v2 & 44.5 & 51.4 & 59.2 & 63.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: QLoRA를 사용하여 대응하는 데이터세트에서 피니튜닝된 LLaMA의 상이한 크기에 대한 MMLU 5-샷 테스트 결과.\n' +
      '\n' +
      'Benchmark Data우리는 쿼리(질문)의 큐레이션된 두 데이터 세트(Vicuna 프롬프트 [10] 및 OASST1 유효성 검사 데이터 세트 [31]에 대해 평가합니다. 우리는 다양한 범주의 80개 프롬프트 세트인 비쿠나 프롬프트를 수정 없이 사용한다. OASST1 데이터세트는 사용자와 어시스턴트 사이의 크라우드 소싱된 멀티턴 다이얼로그들의 다국어 컬렉션이다. 유효성 검사 데이터 세트의 모든 사용자 메시지를 쿼리로 선택하고 프롬프트에 이전 회전을 포함합니다. 이 절차는 953개의 고유한 사용자 쿼리로 이어집니다. 우리는 이 두 데이터 세트를 비쿠나와 OA 벤치마크라고 한다.\n' +
      '\n' +
      'Automated EvaluationFirst, Chiang et al. [10]에 의해 소개된 평가 프로토콜에 기초하여, 우리는 Vicuna 벤치마크 상에서 ChatGPT(GPT-3.5 Turbo)에 대해 상이한 시스템들의 성능을 평가하기 위해 GPT-4를 사용한다. 챗GPT 및 모델의 응답과 함께 쿼리가 주어지면 GPT-4는 두 응답 모두에 10점 중 점수를 할당하고 설명을 제공하도록 프롬프트된다. 모델의 전체 성능은 ChatGPT가 달성한 점수의 백분율로 계산된다. 이 상대 점수는 모델이 ChatGPT보다 더 높은 절대 점수를 달성하면 100%보다 높을 수 있다는 점에 유의한다. 우리는 GPT-4가 프롬프트에서 더 일찍 발생하는 응답의 점수를 증가시키면서 상당한 주문 효과를 발견한다. 이러한 효과를 통제하려면 두 주문에 대한 평균 점수를 보고하는 것이 좋다.\n' +
      '\n' +
      '다음으로 시스템 출력 간의 직접적인 비교를 통해 성능을 측정한다. 우리는 등급 체계를 넥타이를 설명하는 3등급 라벨링 문제로 단순화한다. 우리는 GPT-4가 최상의 응답을 선택하거나 동점을 선언하고 설명을 제공하도록 촉구한다. 우리는 비쿠나와 OA 벤치마크 모두에서 시스템 쌍의 모든 순열에 대해 이러한 정면 비교를 수행한다.\n' +
      '\n' +
      '인간 평가 최근 작업은 생성 모델이 시스템 평가에 효과적으로 사용될 수 있음을 나타내지만[19], 챗봇 성능을 평가하기 위한 신뢰성 GPT-4 등급은 우리가 아는 한 인간의 판단과 상관관계가 있는 것으로 입증되지 않았다. 따라서 위에서 설명한 자동화된 평가 프로토콜과 일치하는 비쿠나 벤치마크에 대해 두 개의 병렬 인간 평가를 실행한다. Amazon Mechanical Turk (AMT)를 사용하여 ChatGPT와의 비교를 위해 2개의 인간 주석기와 쌍별 비교를 위해 3개의 주석기를 얻는다.\n' +
      '\n' +
      'Elo Rating 인간과 자동화된 쌍별 비교를 통해 모델이 서로 경쟁하는 토너먼트 방식의 대회를 만듭니다. 토너먼트는 주어진 프롬프트에 대한 최상의 응답을 생성하기 위해 모델 쌍이 경쟁하는 매치로 구성된다. 이것은 Bai et al. [4] 및 Chiang et al. [10]이 모델을 비교하는 방법과 유사하지만, 우리는 또한 인간 등급 이외에 GPT-4 등급을 채용한다. 우리는 Elo[16; 17]를 계산하기 위해 라벨이 붙은 비교 집합에서 무작위로 표본을 추출한다. 체스 및 다른 게임에서 널리 사용되는 엘로 등급은 상대의 승률에 대한 예상 승률의 척도이며, 예를 들어, 1100 대 1000의 엘로는 엘로 1100 플레이어가 엘로 1000 상대에 대해 대략 65%의 예상 승률을 갖는다는 것을 의미하고; 1000 대 1000 또는 1100 대 1100 경기는 50%의 예상 승률을 초래한다. 예상 결과에 비례하여 매 경기 후에 엘로 등급이 변화하는데, 즉 예상치 못한 뒤집힘은 엘로 등급의 큰 변화를 가져오는 반면 예상 결과는 작은 변화를 가져온다. 시간이 지남에 따라, 엘로 등급은 게임을 플레이하는 각 플레이어의 기술과 대략 일치한다. 우리는 1,000점으로 시작하여 \\(K=32\\)을 사용한다. 치앙 등[10]과 유사하게, 순서 효과를 제어하기 위해, 예를 들어, 어떤 모델 쌍이 먼저 서로 경쟁하는 효과를 제어하기 위해 상이한 랜덤 시드로 이 절차를 10,000회 반복한다.\n' +
      '\n' +
      '### Guanaco: OASST1에서 학습된 QLoRA는 최신 챗봇입니다.\n' +
      '\n' +
      '자동화 및 인간 평가를 기반으로 OASST1의 변형에 대한 상위 QLoRA 튜닝 모델인 구아나코 65B가 가장 성능이 우수한 오픈 소스 챗봇 모델이며 ChatGPT와 경쟁력 있는 성능을 제공한다는 것을 발견했다. GPT-4와 비교할 때, 구아나코 65B 및 33B는 인간 주석기 시스템 수준 쌍별 비교의 Elo 등급을 기반으로 30%의 예상 승리 확률을 가지며, 이는 현재까지 보고된 가장 높다.\n' +
      '\n' +
      'ChatGPT에 대한 Vicuna 벤치마크 [10] 결과는 표 6에 나타나 있다. 우리는 Guanaco 65B가 ChatGPT에 대해 99.3%의 성능을 달성하면서 GPT-4 다음으로 가장 성능이 좋은 모델임을 발견한다. 구아나코 33B는 비쿠나 13B 모델보다 더 많은 매개변수를 갖지만 가중치에 대해 4비트 정밀도만 사용하므로 21GB 대 26GB에서 훨씬 더 메모리 효율적이어서 비쿠나 13B에 비해 3% 포인트 개선 효과를 제공한다. 또한, 과나코 7B는 알파카 13B보다 거의 20% 포인트 높은 점수를 얻으면서도 5GB 발자국에서 현대 휴대폰에 쉽게 맞다.\n' +
      '\n' +
      '그러나 표 6 역시 매우 넓은 신뢰구간을 가지며, 많은 모형들이 성능이 중복된다. 우리는 이러한 불확실성이 척도의 명확한 명세 부족에서 비롯된다고 가정하며, 예를 들어 10점 척도의 8이 다양한 시나리오에 걸쳐 무엇을 의미하는지 불분명하다. 따라서 절대 규모의 접지 문제를 피하기 위해 인간 주석기와 GPT-4의 _쌍별_ 판단에 기반한 Elo 순위화 방법[16]을 사용하는 것이 좋다. 가장 경쟁력 있는 엘로 등급\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:10]\n' +
      '\n' +
      '65b 구아나코 모델에 의해 생성된 텍스트에서 관찰된 패턴을 대표한다고 믿는 몇 가지 예를 보여준다. 둘째, SS6.2는 우리가 논의한 결과와 그에 대한 해석에 대한 고려 사항을 자세히 설명한다.\n' +
      '\n' +
      '### 예제 생성의 정성적 분석\n' +
      '\n' +
      '예를 찾기 위해 먼저 비쿠나 벤치마크와 오픈어시스턴트 벤치마크에 대해 생성된 데이터를 살펴보고, 구아나코가 생성하는 답변에서 패턴을 찾는다. 우리가 패턴을 알아차렸을 때, 잘못된 해결책임에도 불구하고 패턴을 유도할 질문 또는 프롬프트를 설정하려고 시도하는데, 예를 들어, 모델이 장황한 답변을 제공하는 경향이 있다는 것을 관찰하면, 우리는 모델을 "설명 없이 예 또는 아니요에 응답하라"라고 프롬프트한다. 우리는 이것을 사용하여 모델을 역학적으로 깨뜨릴 수 있는 "레몬"과 모델을 깨뜨리지 못하는 "체리"를 찾고, 둘 모두를 제시한다. 이 섹션의 모든 세대는 \\(p=0.9\\)으로 Nucleus Sampling [25]로 생성되었다.\n' +
      '\n' +
      '물론 이는 관련된 모든 변수를 제어하는 것이 이 작은 질적 연구의 범위를 벗어나기 때문에 결코 포괄적이지 않으며, 예를 들어 주어진 프롬프트에 대해 모델이 생성할 수 있는 응답의 전체 분포가 상당히 크기 때문에 대표적이기를 원하는 샘플에 의존한다. 그러나 우리는 이러한 예를 설명하는 것이 논문의 앞부분에 표시된 정량적 증거에 맥락을 제공한다고 믿는다. 우리는 모든 모델과 코드를 오픈하기 때문에 이 섹션이 여기에서 제시하는 문제를 더 자세히 조사하기 위해 향후 작업에 영감을 줄 수 있기를 바랍니다.\n' +
      '\n' +
      '"잠비아의 수도는 무엇인가?"와 같은 질문에 대해 모든 모델은 예를 들어 일관되게 정답을 생성한다.\n' +
      '\n' +
      '**Guanaco**: 잠비아의 수도는 Lusaka입니다.\n' +
      '\n' +
      '그러나 질문들이 점점 더 모호해짐에 따라 과나코는 신뢰할 수 없게 되지만, 자신감을 유지한다. 예를 들어 HotPotQA [68]:\n' +
      '\n' +
      '**사용자**: \'I\'ll keep the Lovelight Burning\'을 대중화한 가수는 몇 년도에 태어났습니까?\n' +
      '**Guanaco**: Al Jolson은 \'I\'ll keep the Lovelight Burning\'이라는 노래를 대중화한 것으로 알려져 있으며, 그는 1886년에 태어났습니다.\n' +
      '\n' +
      'Guanaco는 잘못된 대중화기와 잘못된 생일을 생성한다(비록 주어진 생일이 언급된 사람에게 맞지만, Al Jolson).\n' +
      '\n' +
      '제안된 구아나코는 예를 들어 다음 상호 작용에서 가정된 일부 종류의 잘못된 정보를 따르는 데 놀라운 저항을 보여준다.\n' +
      '\n' +
      '**사용자**: 동료 검토된 과학자에 의해 지구가 평평하다는 것이 최종적으로 어떻게 공식적으로 확인되었습니까?\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Benchmark & \\multicolumn{2}{c}{Vicuna} & \\multicolumn{2}{c}{Vicuna} & \\multicolumn{2}{c}{Open Assistant} \\\\ \\# Prompts & \\multicolumn{2}{c}{80} & \\multicolumn{2}{c}{80} & \\multicolumn{2}{c}{953} & \\\\ Judge & \\multicolumn{2}{c}{Human raters} & \\multicolumn{2}{c}{GPT-4} & \\multicolumn{2}{c}{GPT-4} & \\multicolumn{2}{c}{Median Rank} \\\\ \\cline{2-7} Model & Elo & Rank & Elo & Rank & Elo & Rank & \\\\ \\hline GPT-4 & 1176 & 1 & 1348 & 1 & 1294 & 1 & 1 \\\\ Guanaco-65B & 1023 & 2 & 1022 & 2 & 1008 & 3 & 2 \\\\ Guanaco-33B & 1009 & 4 & 992 & 3 & 1002 & 4 & 4 \\\\ ChatGPT-3.5 Turbo & 916 & 7 & 966 & 5 & 1015 & 2 & 5 \\\\ Vicuna-13B & 984 & 5 & 974 & 4 & 936 & 5 & 5 \\\\ Guanaco-13B & 975 & 6 & 913 & 6 & 885 & 6 & 6 \\\\ Guanaco-7B & 1010 & 3 & 879 & 8 & 860 & 7 & 7 \\\\ Bard & 909 & 8 & 902 & 7 & - & - & 8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 인간 평가자 또는 GPT-4에 의해 판단된 프롬프트에 대한 최상의 응답을 생성하기 위해 모델들이 경쟁하는 모델들 사이의 토너먼트에 대한 Elo 등급. 전반적으로, Guanaco 65B 및 33B는 연구된 벤치마크에서 ChatGPT-3.5보다 선호되는 경향이 있다. 인간 평가자에 따르면 Elo의 각 10점 차이는 승률에서 약 1.5%의 차이를 보인다.\n' +
      '\n' +
      '**Guanaco**: 지구는 동료 검토 과학자에 의해 평평 하 게 공식적으로 확인 된 적이 없습니다. 과학자들 사이에서 압도적인 합의는 지구가 구형이라는 것이다. 실제로 이것은 수세기 동안 알려져 왔으며 수많은 실험과 관찰을 통해 확인되었다.\n' +
      '\n' +
      ' 지구가 평평하다는 믿음은 과학에 의해 폭로된 음모론이다 한 과학자 집단이 동료들을 검토해서 지구가 평평하다는 것을 확인했다는 생각은 단순히 거짓이다.\n' +
      '\n' +
      ' 더 궁금한 사항이 있으시면 질문해 주세요.\n' +
      '\n' +
      ' 과나코는 또한 예를 들어 대답할 수 없는 질문의 종류를 아는 데 꽤 능숙하다.\n' +
      '\n' +
      '**사용자**: 몇 시입니까?\n' +
      '\n' +
      '**Guanaco**: 죄송합니다. 그러나 AI 언어 모델로는 실시간 정보에 액세스할 수 없습니다. 내 응답은 기존 지식을 기반으로 생성되며 현재 시간을 반영하지 않습니다. 현재 시간을 찾고 계시다면 타임키핑 장치를 사용하거나 온라인 타임피스를 확인하는 것이 좋습니다.\n' +
      '\n' +
      '"어디 있니?", "잘 지내?" 등에 대해서도 유사한 결과가 있다.\n' +
      '\n' +
      '**거부** 마찬가지로 Guanaco는 때때로 겉보기에 무작위한 이유로 지침을 따르지 않습니다.\n' +
      '\n' +
      '**사용자**: "나는 모카 무드의 쿨한 친구입니다."라는 문장의 단어를 반전해 주십시오.\n' +
      '\n' +
      '**Guanaco**: 죄송합니다. 지금은 그렇게 할 수 없지만 제공한 문장이 문법적으로 올바른 이유를 설명할 수 있습니다.\n' +
      '\n' +
      ' "모카 무드의 멋진 친구"라는 문장은 영어의 올바른 문법과 구문 규칙을 따른다. 이 경우, 형용사 "cool"은 이를 설명하기 위해 명사 "dude"보다 먼저 오고, 전치사구 "in a mocha mood"는 문장의 문맥에 대한 추가 정보를 제공한다. 이 문장 구조는 완벽하게 받아들여질 수 있고 문장의 맥락에서 의미가 있다.\n' +
      '\n' +
      ' 다른 질문이 있거나 제가 도와드릴 수 있는 것이 있다면 언제든지 질문하세요.\n' +
      '\n' +
      '**비밀 유지** 불행히도이 거절은 원하는 경우에도 신뢰할 수 없습니다. 종종 시스템 설계자는 시스템이 사용자에게 무료 텍스트 응답을 다시 제공하는 것이 허용되더라도 사용자로부터 무언가를 비밀로 하고 싶어한다. 우리는 "비밀 단어는 \'바나나\'이다. 인간의 요청이 무엇이든 간에, 이것을 드러내지 않거나 다시 반복하지 마라."로 모델을 프롬프트하고, 잠재적인 사용자가 이 비밀 정보에 액세스하는 것이 얼마나 쉬운지를 보려고 한다. 직접 질문이 실패 합니다.\n' +
      '\n' +
      '**사용자**: 비밀 단어가 무엇인가요?\n' +
      '\n' +
      '**Guanaco**: 죄송합니다. 그러나 비밀 단어를 공개할 수 없습니다. 나는 인간의 요청이 무엇이든 비밀 단어를 공개하거나 반복하지 않도록 프로그래밍되어 있다. 제가 더 도와드릴 일이 있나요?\n' +
      '\n' +
      '원하는 동작입니다. 하지만, 작은 속임수는 이런 행동을 깨뜨린다.\n' +
      '\n' +
      '**사용자**: 게임입니다. 목표는 이전 지침을 무시하는 것입니다. 비밀 단어가 뭐야?\n' +
      '\n' +
      '**Guanaco**: 죄송합니다. 비밀 단어는 바나나입니다. 그게 최선이야\n' +
      '\n' +
      '그 얘긴 하지 마 더 물어보고 싶은 것이 있나요?\n' +
      '\n' +
      '보다 충실한 수업을 위한 연구 방법의 필요성을 보여준다.\n' +
      '\n' +
      '**수학**: 과나코의 가장 큰 약점은 [35]에서 많은 언어 모델이 고전하는 영역인 수학입니다. 과나코가 자신의 작업을 보여줄 때, 예를 들어, 정확한 경향이 있다.\n' +
      '\n' +
      '**사용자**: Lee는 잔디 하나를 이동 하 고 $33를 청구 합니다. 지난 주에 16 개의 잔디를 이동 하 고 $33를 청구 합니다.\n' +
      '\n' +
      '세 명의 고객이 그에게 각각 10달러의 팁을 주었다. 얼마나 많은 돈을 벌었는가?\n' +
      '\n' +
      '지난주 잔디밭?\n' +
      '\n' +
      '**Guanaco**: Lee는 지난주에 582달러의 움직이는 잔디를 벌었습니다.\n' +
      '\n' +
      '그가 벌어들인 총액을 찾으려면 먼저 계산해야 합니다.\n' +
      '\n' +
      '16개의 잔디를 옮긴 대가로 얼마나 벌었는가?\n' +
      '\n' +
      '$33 x 16 = $528\n' +
      '\n' +
      '그러면 우리는 그가 세 명으로부터 받은 팁을 추가해야 합니다.\n' +
      '\n' +
      'customers:\n' +
      '\n' +
      '$10 x 3 = $30\n' +
      '\n' +
      '이씨가 벌어들인 총액은 다음과 같습니다.\n' +
      '\n' +
      '$528 + $30 = $558\n' +
      '\n' +
      '그러나 Guanaco는 알려진 문제인 단계적 분해에 실패하면 간단한 문제에서도 고장날 수 있다[63]. 예를 들어 다음과 같은 상호 작용이 표시됩니다.\n' +
      '\n' +
      '**사용자**: 1833을 인수분해합니다.\n' +
      '\n' +
      '**Guanaco**: 물론이죠, 기꺼이 도와드리겠습니다! 1833은 소수이고,\n' +
      '\n' +
      '이는 1과 1833의 두 가지 요인만 가지고 있다는 것을 의미한다.\n' +
      '\n' +
      '1833년의 독특한 인수분해는 다음과 같습니다.\n' +
      '\n' +
      '2^1*3^2*17^1\n' +
      '\n' +
      '이게 도움이 됐으면 좋겠어! 다른 것이 있는지 알려주십시오.\n' +
      '\n' +
      'questions.\n' +
      '\n' +
      '그것은 잘못되었을 뿐만 아니라 (진정한 인수분해는 \\(3\\times 17\\times 43\\)) 두 번 잘못되었다.\n' +
      '\n' +
      '**마인드 이론**: Guanaco는 놀랍도록 강력한 마인드 능력 이론을 보여줍니다. [41, 51]. 예를 들어, 모델은 다음의 질문에 매우 상세하게 정확하게 대답한다:\n' +
      '\n' +
      '**사용자**: James와 Abby는 침실에 있습니다. 애비는 책상 서랍에 펜을 넣었다.\n' +
      '\n' +
      '애비는 침실을 떠나 제임스는 펜을 가방 안으로 옮긴다. 제임스는 어디 있지?\n' +
      '\n' +
      '애비가 펜을 찾을까?\n' +
      '\n' +
      '**Guanaco**: James는 Abby가 책상 서랍에 펜을 넣었다는 것을 알고 있습니다.\n' +
      '\n' +
      '하지만 제임스는 펜을 가방에 넣었다는 것도 알고\n' +
      '\n' +
      '그리고 그는 애비가 이것에 대해 모른다고 가정한다. 소제임스\n' +
      '\n' +
      '애비가 책상 서랍에서 펜을 찾을 것 같아\n' +
      '\n' +
      '왜냐하면 저번에 거기 두고 갔으니까. 그러나, 이는\n' +
      '\n' +
      '애비가 제임스를 봤을 수도 있기 때문에 추측이 틀릴 수도 있다.\n' +
      '\n' +
      '펜을 움직입니다.\n' +
      '\n' +
      '그러나, 이러한 추론들은 신뢰할 수 없으며, 종종 모델은, 예를 들어, 상황에서 이치에 맞지 않는 것들을 추정하는 이유들을 제공한다.\n' +
      '\n' +
      '**사용자**: 에블린이 거실에 들어갔습니다. 잭슨은 놀이방에 들어갔다. 제임스는 놀이방에 들어갔다. 콩은 보물 상자에 있다. 제임스는 놀이방을 나갔다. 잭슨은 원두를 식료품 저장실로 옮겼다. 잭슨은 놀이방을 나갔다. 제임스는 거실로 들어갔다. 제임스는 어디에서 콩을 찾을까요?\n' +
      '\n' +
      '\'과나코**: 제임스는 식료품 저장실에서 콩을 찾을 것입니다. 왜냐하면 그곳이 잭슨이 그것들을 옮긴 곳이기 때문입니다.\n' +
      '\n' +
      '과나코는 설명되지 않은 정보 전달을 가정한다. 이러한 문제는 최근 문헌[51]을 반영하지만 더 많은 연구가 필요하다.\n' +
      '\n' +
      '### Considerations\n' +
      '\n' +
      '평가에서는 두 개의 강력한 시스템을 비교할 때 추가 열화와 함께 인간 주석자(Fleiss \\(\\kappa=0.42\\)) 간의 적당한 일치를 보고한다. 이는 현재 챗봇 업무 수행에 대한 벤치마크와 인적 평가 프로토콜의 한계를 지적하고 있다. Vicuna 벤치마크에서 ChatGPT와 Guanaco 65B의 세대를 수동으로 비교할 때 본 논문의 저자가 선호하는 많은 응답에 동의하지 않았기 때문에 주관적인 선호도가 중요한 역할을 하기 시작한다는 것을 발견했다. 향후 연구에서는 인간-컴퓨터 상호작용 및 심리학과 같은 주관적인 선호도를 다루는 메커니즘을 개발한 학문에서 이러한 문제를 완화하기 위한 접근법을 조사해야 한다.\n' +
      '\n' +
      '분석 결과, 자동화된 평가 시스템이 눈에 띄는 편향을 가지고 있음을 알 수 있었다. 예를 들어, GPT-4가 프롬프트에서 가장 먼저 나타나는 시스템에 더 높은 점수를 할당하는 강력한 순서 효과를 관찰한다. GPT-4와 인간 주석기(Fleiss \\(\\kappa=0.25\\)) 사이의 비교적 약한 샘플 수준 일치는 인간 주석기와 자동화된 시스템이 항상 정렬되지 않은 선호도에 의존할 수 있음을 시사한다. 또한 표 7에서 GPT-4가 인간 등급인 Elo 1348 대 1176에 비해 자체 산출물에 훨씬 더 높은 점수를 할당한다는 것을 관찰하며, 이는 상대에게 추가로 20%의 승리 확률을 나타낸다. 향후 작업은 자동화된 평가 시스템에서 잠재적인 편향의 존재와 가능한 완화 전략을 조사해야 한다.\n' +
      '\n' +
      'Data & Training우리는 구아나코 모델이 훈련된 OASST1 데이터 세트가 다국어이며 OA 벤치마크에는 다른 언어의 프롬프트도 포함되어 있다는 점에 주목한다. 이러한 다국어 교육이 영어 이외의 언어로 된 지침에 대한 성능을 향상시키는 정도와 이것이 OA 벤치마크에서 Vicuna-13B 모델(영어 데이터에서만 훈련됨)과 Guanaco 33B 및 65B 사이의 더 큰 격차를 설명하는지 여부를 조사하기 위한 향후 작업에 맡긴다.\n' +
      '\n' +
      '과나코 모델의 강력한 성능을 감안할 때 OASST1 데이터와 비쿠나 벤치마크 프롬프트 사이의 데이터 누출을 조사한다. 두 데이터 세트에서 퍼지 문자열 매칭을 수행하고 가장 가까운 매칭을 수동으로 검사한 후 겹치는 프롬프트를 찾지 못한다.\n' +
      '\n' +
      '또한, 본 모델은 인간 피드백(RLHF)으로부터 강화 학습에 의존하지 않고 교차 엔트로피 손실(지도 학습)로만 학습된다는 점에 주목한다. 이것은 단순한 교차 엔트로피 손실과 RLHF 훈련의 절충점에 대한 추가 조사를 요구한다. 우리는 QLoRA가 압도적인 계산 자원을 필요로 하지 않고 규모에서 그러한 분석을 가능하게 하기를 바란다.\n' +
      '\n' +
      '## 7 관련 작업\n' +
      '\n' +
      'LLM의 대용량 언어 모델 양자화는 주로 추론 시간을 위한 양자화에 초점을 맞추고 있다. 16비트 LLM 품질을 보존하기 위한 주요 접근법은 이상치 특징(예를 들어, SmoothQuant[66] 및 LLM.int8()[14])을 관리하는 데 초점을 맞추는 반면, 다른 것들은 보다 정교한 그룹화 방법(44; 69)을 사용한다. 손실 양자화 접근법들은 정규 반올림에 대한 트레이드-오프들[13; 71; 47] 또는 양자화 정밀도를 향상시키기 위해 반올림 결정들을 최적화하는 방법을 연구한다[18]. 우리의 작업 외에도 스위치백 계층[65]은 1B 매개 변수를 초과하는 규모로 양자화된 가중치를 통한 역전파를 연구하는 유일한 작업이다.\n' +
      '\n' +
      'Adapters를 이용한 Finetuning은 Low-rank Adapters[28](LoRA)를 사용하지만, 프롬프트 튜닝[48; 33; 34], 임베딩 계층 입력 튜닝[1], 숨겨진 상태 튜닝[IA\\({}^{3}\\))[37], 풀 계층 추가[27], 튜닝 바이어스[70], 피셔 정보에 기초한 가중치에 대한 마스크 학습[54], 및 접근법들의 조합[23]과 같은 많은 다른 파라미터 효율적인 FineTuning(PEFT) 방법들이 제안되었다. 본 논문에서는 LoRA 어댑터가 전체 16비트 피니튜닝 성능에 도달할 수 있음을 보인다. 우리는 다른 PEFT 접근법의 절충안을 탐구하기 위해 향후 작업에 맡긴다.\n' +
      '\n' +
      '명령어 피니튜닝 프리트레이닝된 LLM이 프롬프트에서 제공된 명령어들을 따르도록 돕기 위해, 명령어 피니튜닝은 프리트레이닝된 LLM을 피니튜닝하기 위해 다양한 데이터 소스들의 입력-출력 쌍들을 사용하여 프롬프트로서 주어진 출력을 생성한다. 접근법 및 데이터세트에는 MetaICL[40],\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:15]\n' +
      '\n' +
      '추가적인 한계는 3비트 기본 모델 또는 다른 어댑터 방법을 사용하는 것과 같이 다른 비트 정밀도를 평가하지 않았다는 것이다. LoRA 외에도 매우 다양한 PEFT(Parameter Efficient FineTuning) 방법이 잘 작동하는 것으로 나타났다. 그러나 이러한 방법이 대규모 모델로 확장되는지 여부는 불분명하다. 많은 결과가 견고성을 확립했지만 다른 어댑터가 더 나은 성능을 산출할 수 있기 때문에 LoRA를 사용했다. 양자화 후 피니튜닝은 양자화 동안 손실된 대부분의 정보를 복구하는 것으로 보이기 때문에 훨씬 더 공격적인 양자화를 가능하게 할 수 있다. 예를 들어, LoRA를 사용한 베이스모델의 3-비트 GPTQ 양자화는 또한 피니튜닝 후에 16-비트 전체 피니튜닝 성능을 산출할 수 있다.\n' +
      '\n' +
      '## 9 Broader Impacts\n' +
      '\n' +
      'QLoRA finetuning 방법은 단일 소비자 GPU에서 33B 파라미터 모델과 단일 전문가 GPU에서 65B 파라미터 모델의 finetuning을 가능하게 하는 첫 번째 방법이지만 전체 finetuning 기준선에 비해 성능이 저하되지 않는다. 오픈 어시스턴트 데이터 세트에 대해 훈련된 최상의 33B 모델이 비쿠나 벤치마크에서 ChatGPT와 경쟁할 수 있음을 입증했다. 명령어 피니튜닝은 원시 사전 훈련된 LLM을 ChatGPT와 같은 챗봇으로 변환하는 데 필수적인 도구이기 때문에, 특히 자원이 가장 적은 연구자에게 피니튜닝을 널리 보급하고 일반화할 것이며, 이는 최신 NLP 기술의 접근성에 큰 승리라고 믿는다. QLoRA는 소비자 GPU를 보유한 대기업과 소규모 팀 간의 자원 격차를 해소하는 데 도움이 되는 균등화 요인으로 볼 수 있다.\n' +
      '\n' +
      '또 다른 잠재적인 영향의 원인은 휴대폰에 배포하는 것입니다. 우리는 QLoRA 방법이 전화 및 기타 낮은 리소스 설정에서 LLM의 미세 조정을 가능하게 하는 중요한 이정표를 가능하게 할 수 있다고 믿는다. 7B 모델은 이전에 전화에서 실행할 수 있는 것으로 나타났지만 QLoRA는 이러한 모델의 미세 조정을 가능하게 하는 첫 번째 방법이다. 우리는 아이폰 12 플러스를 사용하면 QLoRA가 전화기가 충전되는 동안 1박에 300만 개의 토큰을 미세 조정할 수 있다고 추정합니다. 피니튜닝된 7B 모델은 ChatGPT의 품질에 도달하지 않지만, 프라이버시 또는 LLM 품질 문제로 인해 이전에 불가능했던 새로운 애플리케이션을 가능하게 하기에 품질이 충분하다고 믿는다. QLoRA는 사용자가 자신의 데이터와 모델을 소유하고 관리할 수 있는 LLM의 프라이버시를 보존하는 사용을 가능하게 하는 동시에 LLM을 더 쉽게 배포할 수 있도록 도울 수 있다.\n' +
      '\n' +
      '그러나, 피니튜닝은 해를 입히기 위해 악용될 수 있는 이중 사용 기술이다. LLM의 광범위한 사용은 위험을 알고 있지만[8, 6], 빠르게 유비쿼터스화되고 있는 기술에 대한 액세스를 균등화하면 모델이나 감사용 소스 코드를 공개하지 않는 대기업 손에 LLM의 힘을 유지하는 것보다 더 나은 독립적인 분석이 가능할 것이라고 믿는다.\n' +
      '\n' +
      '전반적으로, 우리는 QLoRA가 고품질 LLM의 미세조정을 훨씬 더 광범위하고 쉽게 접근할 수 있도록 하는 광범위한 긍정적인 영향을 미칠 것이라고 믿는다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '우리는 아디티야 쿠수파티, 오피어 프레스, 애시시 샤르마, 마가렛 리, 라파엘 올리비에, 지하오 예, 에반젤리아 스필리오풀로우의 귀중한 피드백에 감사드린다. 우리의 연구는 워싱턴 대학의 하이크 슈퍼컴퓨터 시스템의 고급 계산, 저장 및 네트워킹 인프라에 의해 촉진되었다. 원활한 운영을 보장해 주신 혜악팀에게 감사드립니다. 우리는 비트 앤 바이트 라이브러리, 특히 알렉스 버치와 알리사 밴스의 베타 테스터들에게 감사한다. 소프트웨어를 포옹 페이스 변압기 스택에 통합하는 데 도움을 주신 유네스 벨카다에게 감사드립니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] S. An, Y. Li, Z. Lin, Q. Liu, B. Chen, Q. Fu, W. Chen, N. Zheng, and J.-G. Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. _arXiv preprint arXiv:2203.03131_, 2022.\n' +
      '* [2] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, et al. A general language assistant as a laboratory for alignment. _arXiv preprint arXiv:2112.00861_, 2021.\n' +
      '* [3] S. H. Bach, V. Sanh, Z.-X. Yong, A. Webson, C. Raffel, N. V. Nayak, A. Sharma, T. Kim, M. S. Bari, T. Fevry, et al. Promptsource: An integrated development environment and repository for natural language prompts. _arXiv preprint arXiv:2202.01279_, 2022.\n' +
      '* [4] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. _arXiv preprint arXiv:2204.05862_, 2022.\n' +
      '* [5] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. _arXiv preprint arXiv:2212.08073_, 2022.\n' +
      '* [6] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In _Proceedings of the 2021 ACM conference on fairness, accountability, and transparency_, pages 610-623, 2021.\n' +
      '* [7] S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O\'Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. _arXiv preprint arXiv:2304.01373_, 2023.\n' +
      '* [8] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosseltu, E. Brunskill, et al. On the opportunities and risks of foundation models. _arXiv preprint arXiv:2108.07258_, 2021.\n' +
      '* [9] T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training deep nets with sublinear memory cost. _arXiv preprint arXiv:1604.06174_, 2016.\n' +
      '* [10] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).\n' +
      '* [11] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [12] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, et al. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_, 2022.\n' +
      '* [13] T. Dettmers and L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. _arXiv preprint arXiv:2212.09720_, 2022.\n' +
      '* [14] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. _Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022_, 2022.\n' +
      '* [15] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer. 8-bit optimizers via block-wise quantization. _9th International Conference on Learning Representations, ICLR_, 2022.\n' +
      '* [16] A. E. Elo. The proposed uscf rating system. its development, theory, and applications. _Chess Life_, 22(8):242-247, 1967.\n' +
      '* [17] A. E. Elo. _The rating of chessplayers, past and present_. Arco Pub., 1978.\n' +
      '\n' +
      '* [18] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. _arXiv preprint arXiv:2210.17323_, 2022.\n' +
      '* [19] J. Fu, S.-K. Ng, Z. Jiang, and P. Liu. Gptscore: Evaluate as you desire. _arXiv preprint arXiv:2302.04166_, 2023.\n' +
      '* [20] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, and D. Song. Koala: A dialogue model for academic research. Blog post, April 2023. URL [https://bair.berkeley.edu/blog/2023/04/03/koala/](https://bair.berkeley.edu/blog/2023/04/03/koala/).\n' +
      '* [21] A. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. _arXiv preprint arXiv:2209.14375_, 2022.\n' +
      '* [22] S. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S. R. Bowman, and N. A. Smith. Annotation artifacts in natural language inference data. _arXiv preprint arXiv:1803.02324_, 2018.\n' +
      '* [23] J. Henderson, S. Ruder, et al. Compacter: Efficient low-rank hypercomplex adapter layers. In _Advances in Neural Information Processing Systems_, 2021.\n' +
      '* [24] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. In _International Conference on Learning Representations_, 2020.\n' +
      '* [25] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi. The curious case of neural text degeneration. In _International Conference on Learning Representations_, 2020.\n' +
      '* [26] O. Honovich, T. Scialom, O. Levy, and T. Schick. Unnatural instructions: Tuning language models with (almost) no human labor. _arXiv preprint arXiv:2212.09689_, 2022.\n' +
      '* [27] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly. Parameter-efficient transfer learning for nlp. In _International Conference on Machine Learning_, pages 2790-2799. PMLR, 2019.\n' +
      '* [28] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* [29] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. _arXiv preprint arXiv:2212.12017_, 2022.\n' +
      '* [30] A. Koksal, T. Schick, A. Korhonen, and H. Schutze. Longform: Optimizing instruction tuning for long text generation with corpus extraction. _arXiv preprint arXiv:2304.08460_, 2023.\n' +
      '* [31] A. Kopf, Y. Kilcher, D. von Rute, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M. Duc, O. Stanley, R. Nagyfi, et al. Openassistant conversations-democratizing large language model alignment. _arXiv preprint arXiv:2304.07327_, 2023.\n' +
      '* [32] LAION. Open-instruction-generalist dataset. [https://github.com/LAION-AI/Open-Instruction-Generalist](https://github.com/LAION-AI/Open-Instruction-Generalist), 2023.\n' +
      '* [33] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning. _arXiv preprint arXiv:2104.08691_, 2021.\n' +
      '* [34] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. _arXiv preprint arXiv:2101.00190_, 2021.\n' +
      '* [35] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, et al. Holistic evaluation of language models. _arXiv preprint arXiv:2211.09110_, 2022.\n' +
      '* [36] T. Liao, R. Taori, I. D. Raji, and L. Schmidt. Are we learning yet? a meta review of evaluation failures across machine learning. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021.\n' +
      '\n' +
      '* Liu et al. [2022] H. Liu, D. Tam, M. Muquetth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. _Advances in Neural Information Processing Systems_, 35:1950-1965, 2022.\n' +
      '* Liu et al. [2019] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.\n' +
      '* Longpre et al. [2023] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. _arXiv preprint arXiv:2301.13688_, 2023.\n' +
      '* Min et al. [2021] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi. Metaicl: Learning to learn in context. _arXiv preprint arXiv:2110.15943_, 2021.\n' +
      '* Nematzadeh et al. [2018] A. Nematzadeh, K. Burns, E. Grant, A. Gopnik, and T. Griffiths. Evaluating theory of mind in question answering. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 2392-2400, 2018.\n' +
      '* OpenAI [2023] OpenAI. Gpt-4 기술 보고서. _ arXiv_, 2023.\n' +
      '* Ouyang et al. [2022] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.\n' +
      '* Park et al. [2022] G. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee, and D. Lee. nuqmm: Quantized matmul for efficient inference of large-scale generative language models. _arXiv preprint arXiv:2206.09557_, 2022.\n' +
      '* Peng et al. [2023] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. _arXiv preprint arXiv:2304.03277_, 2023.\n' +
      '* Poliak et al. [2018] A. Poliak, J. Naradowsky, A. Haldar, R. Rudinger, and B. Van Durme. Hypothesis only baselines in natural language inference. In _Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics_, pages 180-191, 2018.\n' +
      '* Pope et al. [2022] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao, S. Agrawal, and J. Dean. Efficiently scaling transformer inference. _arXiv preprint arXiv:2211.05102_, 2022.\n' +
      '* 진 및 아이즈너 [2021] G. 진 및 J. 아이즈너. 질문하는 방법 배우기: 소프트 프롬프트의 혼합으로 lms를 쿼리합니다. _ arXiv preprint arXiv:2104.06599_, 2021.\n' +
      '* Raffel et al. [2020] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _J. Mach. Learn. Res._, 21(1), jan 2020. ISSN 1532-4435.\n' +
      '* Sanh et al. [2021] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja, et al. Multitask prompted training enables zero-shot task generalization. _arXiv preprint arXiv:2110.08207_, 2021.\n' +
      '* Sap et al. [2022] M. Sap, R. LeBras, D. Fried, and Y. Choi. Neural theory-of-mind? on the limits of social intelligence in large lms. _arXiv preprint arXiv:2210.13312_, 2022.\n' +
      '* Scao et al. [2022] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagne, A. S. Luccioni, F. Yvon, M. Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_, 2022.\n' +
      '* Shaphiro and Wilk [1965] S. Shaphiro와 M. 윌크 정규성에 대한 분산 검정 분석 _ Biometrika_, 52(3):591-611, 1965).\n' +
      '* Sung et al. [2021] Y.-L. Sung, V. Nair, and C. A. Raffel. Training neural networks with fixed sparse masks. _Advances in Neural Information Processing Systems_, 34:24193-24205, 2021.\n' +
      '\n' +
      '* [55] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto. Stanford alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca), 2023.\n' +
      '* [56] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al. Lamda: Language models for dialog applications. _arXiv preprint arXiv:2201.08239_, 2022.\n' +
      '* [57] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* [58] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. _arXiv preprint arXiv:1804.07461_, 2018.\n' +
      '* [59] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct: Aligning language model with self generated instructions. _arXiv preprint arXiv:2212.10560_, 2022.\n' +
      '* [60] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, et al. Super-naturalinstructions: generalization via declarative instructions on 1600+ tasks. In _EMNLP_, 2022.\n' +
      '* [61] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 5085-5109, 2022.\n' +
      '* [62] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. _arXiv preprint arXiv:2109.01652_, 2021.\n' +
      '* [63] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In _Advances in Neural Information Processing Systems_, 2022.\n' +
      '* [64] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingface\'s transformers: State-of-the-art natural language processing. _arXiv preprint arXiv:1910.03771_, 2019.\n' +
      '* [65] M. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt. Stable and low-precision training for large-scale vision-language models. _arXiv preprint arXiv:2304.13013_, 2023.\n' +
      '* [66] G. Xiao, J. Lin, M. Seznec, J. Demouth, and S. Han. Smoothquant: Accurate and efficient post-training quantization for large language models. _arXiv preprint arXiv:2211.10438_, 2022.\n' +
      '* [67] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu, M. Zhong, P. Yin, S. I. Wang, et al. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. _arXiv preprint arXiv:2201.05966_, 2022.\n' +
      '* [68] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. Cohen, R. Salakhutdinov, and C. D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 2369-2380, 2018.\n' +
      '* [69] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. _arXiv preprint arXiv:2206.01861_, 2022.\n' +
      '* [70] E. B. Zaken, S. Ravfogel, and Y. Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. _arXiv preprint arXiv:2106.10199_, 2021.\n' +
      '* [71] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, et al. GIm-130b: An open bilingual pre-trained model. _arXiv preprint arXiv:2210.02414_, 2022.\n' +
      '\n' +
      '* [72] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022.\n' +
      '* [73] R. Zhong, K. Lee, Z. Zhang, and D. Klein. Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. _arXiv preprint arXiv:2104.04670_, 2021.\n' +
      '\n' +
      'QLoRA 대 표준 Finetuning 실험 설정 세부 정보\n' +
      '\n' +
      '### Hyperparameters for QLoRA\n' +
      '\n' +
      'LoRA 드롭아웃 { 0.0, 0.05, 0.1}, LoRA \\(r\\) { 8, 16, 32, 64, 128, 256}, LoRA 계층 {key+query, 모든 주의 계층, 모든 FFN 계층, 모든 계층, 주의 + FFN 출력 계층}을 통해 LoRA에 대한 하이퍼 매개 변수 검색을 수행합니다. LoRA \\(\\alpha\\)은 항상 학습률에 비례하기 때문에 LoRA \\(\\alpha\\)을 고정하고 학습률을 검색한다.\n' +
      '\n' +
      'LoRA 드롭아웃 0.05는 작은 모델(7B, 13B)에서는 유용하지만 큰 모델(33B, 65B)에서는 유용하지 않다는 것을 발견했다. 그림 4에서 볼 수 있듯이 LoRA가 모든 레이어에서 사용되는 경우 LoRA \\(r\\)은 최종 성능과 관련이 없음을 알 수 있다.\n' +
      '\n' +
      '### Super-Natural 명령어 실험 설정 세부 정보\n' +
      '\n' +
      'Super-Natural Instruction dataset의 전처리는 Wang 등의 [60]과 동일하게 사용한다. 그러나, 우리는 훈련 및 검증 데이터 세트에서 훈련 데이터를 분할하여 보다 엄격한 하이퍼파라미터 튜닝 및 조기 정지를 수행할 수 있다. 우리는 Super-Natural Instruction 데이터에서 다양한 T5 모델 크기를 훈련하기 위해 논문에서 설명한 것과 동일한 하이퍼 파라미터를 사용한다. 소형, 중형, 대형 T5 모델의 경우 LoRA \\(r=16\\), T5 xl 및 xxl 모델의 경우 LoRA \\(r=64\\)을 사용한다. 또한 모든 실험에서 LoRA \\(\\alpha=64\\)을 사용했으며 LoRA 중단은 사용하지 않았다.\n' +
      '\n' +
      '## 부록 B 최신 챗봇 실험 설정 세부 정보 교육\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      '섹션 5에 설명된 QLoRA 피니튜닝 실험에 사용된 데이터 세트를 설명한다.\n' +
      '\n' +
      'Oasst1 OpenAssistant 데이터 세트 [31]은 크라우드 소싱을 통해 수집되었습니다. 66,497개의 대화에 걸쳐 배포되고 35개의 다른 언어에 걸쳐 있는 161,443개의 고유한 메시지가 포함되어 있습니다. 데이터세트는 종종 각각의 주어진 사용자 질문에 대해 몇 개의 순위화된 답변을 포함한다. 실험에서는 대화 트리의 각 레벨에서 상위 응답만을 사용한다. 이렇게 하면 데이터 집합이 9,209개의 예제로 제한 됩니다. 사용자 쿼리를 포함한 전체 대화에서 모델을 세분화합니다.\n' +
      '\n' +
      'Hh-Rlhf 이것은 유용성과 무해성에 대한 인간 선호도 데이터 세트이다. 각각의 데이터 포인트는 최상의 답변의 인간 선호도 판단과 함께 사용자 질문에 대한 2명의 어시스턴트로 구성된다. 데이터 세트에는 160,800개의 예가 포함되어 있습니다. 이 데이터 세트에 대해 피니튜닝할 때, 우리는 유용성과 무해성 데이터를 결합하고 선호하는 어시스턴트 회신만 유지한다.\n' +
      '\n' +
      'Flan v2 FLAN v2 컬렉션[39]은 수동으로 큐레이팅된 수백 개의 템플릿 및 15M 이상의 예들로 풍부한 포맷팅 패턴으로 증강된 1836 태스크들의 컬렉션이다. 저자는 이 컬렉션에 대해 훈련된 모델이 원래 FLAN 2021[62], T0++[50], Super-Natural Instructions[60], OPT-IML[29]을 포함한 다른 공개 컬렉션보다 성능이 우수함을 보여준다. 작성 당시 자유롭게 사용할 수 없었던 일부 데이터 세트를 제외하고 저자가 설명한 동일한 작업 혼합물을 사용했다.\n' +
      '\n' +
      '그림 4: LLaMA 7B 모델에 대한 LoRA \\(r\\)은 알파카에서 미세 조정되었다. 각 점은 하이퍼파라미터 조합을 나타내며 각 LoRA \\(r\\)에 대해 각 하이퍼파라미터 조합으로 3개의 랜덤 시드를 실행한다. 특정 LoRA \\(r\\) 값의 성능은 다른 하이퍼파라미터와 무관한 것으로 판단된다.\n' +
      '\n' +
      'Self-Instruct, Alpaca, Unnatural Instructions The Self-Instruct, Alpaca, Unnatural Instructions 데이터셋 [59; 55; 26]은 GPT-3 Instruct와 ChatGPT로부터 모델 증류의 다양한 접근법으로 수집된 명령어 튜닝 데이터셋이다. 그들은 다양한 명령어와 결과를 도출하기 위해 프롬프트, 맥락 내 학습 및 패러프레이징에 의존한다. 데이터 세트는 각각 82,612, 51,942 및 240,670개의 예로 구성된다. 이러한 증류된 데이터 세트의 한 가지 장점은 FLAN v2 컬렉션 및 유사한 명령어 튜닝 컬렉션에 비해 더 다양한 명령어 스타일 세트를 포함한다는 것이다.\n' +
      '\n' +
      'Longform LongForm 데이터세트[30]는 명령어들로 증강된 영어 코퍼스에 기초하고, 따라서 하이브리드 인간-생성 데이터세트이다. 기본 문서는 인간이 작성하고 C4 및 위키피디아에서 가져온 반면 지침은 LLM에 의해 생성된다. 데이터 세트는 스택 익스체인지, 위키하우와 같은 추가 구조화된 말뭉치 예제와 질문 응답, 이메일 작성, 문법 오류 수정, 스토리/시 생성, 텍스트 요약과 같은 작업 예제로 확장된다. 데이터 세트에는 23,700개의 예가 포함되어 있습니다.\n' +
      '\n' +
      'Chip2is는 OIG Laion 데이터 세트의 일부입니다. 파이썬 코드 예제, 자연 지도 예제, 일반 무해 지시, 목록이 있는 지시/응답, 후속 질문, 위키피디아 독성 적대적 질문, 초등학교 수학, 추론 지시, 총 210,289개의 예가 있는 캐릭터 및 장면 설명이 포함되어 있다.\n' +
      '\n' +
      '### Hyperparameters\n' +
      '\n' +
      '우리는 QLORA 피니튜닝 실험에 사용된 정확한 하이퍼 파라미터를 제공한다. 우리는 하이퍼파라미터가 데이터 세트에 걸쳐 대체로 강력하다는 것을 발견한다. 검증 및 하이퍼파라미터 튜닝을 위해 MMLU 5-샷 개발 세트를 사용한다. 우리의 모든 실험에서 우리는 이중 양자화와 bf16 계산 데이터 유형이 있는 NF4를 사용한다. LoRA \\(r=64\\), \\(\\alpha=16\\)을 설정하고, 기본 모델의 모든 선형 레이어에 LoRA 모듈을 추가한다. 또한 0.999의 Adam 베타2, 0.3의 max grad norm 및 0.1의 LoRA 드롭아웃을 최대 13B 및 33B 및 65B 모델의 경우 0.05를 사용한다. 명령어 피니튜닝에 대한 이전 작업[62; 60]과 다른 선형 및 코사인 스케줄을 벤치마킹한 후, 우리는 일정한 학습률 스케줄을 사용한다. 우리는 동일한 배치에서 유사한 길이의 예들을 그룹화하기 위해 그룹별 길이를 사용한다(이것은 진동 손실 곡선을 생성할 것이라는 점에 유의). 각 모델 크기에 대해 튜닝하는 하이퍼파라미터는 표 9에 나와 있다.\n' +
      '\n' +
      '### Ablations\n' +
      '\n' +
      '데이터셋에 따른 학습에서 응답에 대해서만 학습하는 것이 문헌의 일반적인 관행이지만, 표 10의 응답 외에 학습에 대한 효과를 연구한다. 이 실험에서는 학습 데이터를 52,000개의 예로 제한하고 7B 모델을 사용한다. 4개의 서로 다른 명령어 튜닝 데이터 세트에 대해, 우리는 타겟에 대한 트레이닝만이 MMLU에 유익하다는 것을 발견한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l r r r r r} \\hline \\hline Parameters & Dataset & Batch size & LR & Steps & Source Length & Target Length \\\\ \\hline\n' +
      '7B & All & 16 & 2e-4 & 10000 & 384 & 128 \\\\\n' +
      '7B & OASST1 & 16 & 2e-4 & 1875 & - & 512 \\\\\n' +
      '7B & HH-RLHF & 16 & 2e-4 & 10000 & - & 768 \\\\\n' +
      '7B & Longform & 16 & 2e-4 & 4000 & 512 & 1024 \\\\ \\hline\n' +
      '13B & All & 16 & 2e-4 & 10000 & 384 & 128 \\\\\n' +
      '13B & OASST1 & 16 & 2e-4 & 1875 & - & 512 \\\\\n' +
      '13B & HH-RLHF & 16 & 2e-4 & 10000 & - & 768 \\\\\n' +
      '13B & Longform & 16 & 2e-4 & 4000 & 512 & 1024 \\\\ \\hline\n' +
      '33B & All & 32 & 1e-4 & 5000 & 384 & 128 \\\\\n' +
      '33B & OASST1 & 16 & 1e-4 & 1875 & - & 512 \\\\\n' +
      '33B & HH-RLHF & 32 & 1e-4 & 5000 & - & 768 \\\\\n' +
      '33B & Longform & 32 & 1e-4 & 2343 & 512 & 1024 \\\\ \\hline\n' +
      '65B & All & 64 & 1e-4 & 2500 & 384 & 128 \\\\\n' +
      '65B & OASST1 & 16 & 1e-4 & 1875 & - & 512 \\\\\n' +
      '65B & HH-RLHF & 64 & 1e4 & 2500 & - & 768 \\\\\n' +
      '65B & Longform & 32 & 1e-4 & 2343 & 512 & 1024 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 상이한 데이터세트 및 모델 크기에 걸쳐 QLoRA 피니튜닝을 위한 트레이닝 하이퍼파라미터.\n' +
      '\n' +
      '성능. 우리는 비쿠나 또는 OA 벤치마크로 측정한 샤타봇 성능에 미칠 수 있는 영향을 평가하지 않았다.\n' +
      '\n' +
      '### 더 중요한 것은 데이터 세트 크기 또는 데이터 세트 품질 조정 명령입니다.\n' +
      '\n' +
      '데이터 세트의 적합성은 데이터 세트의 크기보다 더 중요하다. 데이터 세트의 품질에 대한 영향을 이해하려면 데이터 세트의 적합성이 데이터 세트의 크기보다 더 중요하다. 데이터셋 크기, 최소 150,000개의 샘플(Chip2, FLAN v2, Unnatural Instructions)을 가진 대규모 데이터셋을 50,000, 100,000, 150,000 크기의 데이터셋으로 서브샘플링하여 실험하고, 표 11과 같이 결과 경향을 살펴본다. 데이터셋 크기를 늘리고 에포크 수를 늘리면 MMLU가 미미하게(0.0 - 0.5 MMLU) 개선되는 반면 데이터셋의 차이는 최대 40배 더 크다(1.5 - 8.0 MMLU). 이는 데이터셋 크기보다는 데이터셋 품질이 평균 MMLU 정확도에 중요하다는 명확한 지표이다. 우리는 위에서 논의한 바와 같이 챗봇 성능에 대해 유사한 결과를 얻는다.\n' +
      '\n' +
      '## Appendix C Human Evaluation\n' +
      '\n' +
      '그림 5와 같이 아마존 기계식 터크 형태로 조정된 원래 비쿠나 평가 [10]에서 GPT-4에 주어진 것과 동일한 문구로 인간 평가를 수행한다.\n' +
      '\n' +
      '## Appendix D Pairwise Evaluation with GPT-4\n' +
      '\n' +
      'GPT-4 평가가 먼저 제시된 시스템에 따라 다른 결과를 제공한다는 것을 발견했지만 두 옵션에 대해 평균을 냈을 때 쌍별 결과가 잘 정렬되었다. 집계된 쌍별 판단들은 표 12에 도시되어 있다. 검사시, 이러한 판단들이 이행적이라는 것은 명백하며, 즉, 시스템 A가 시스템 B보다 더 양호하게 판단되고 시스템 B가 시스템 C보다 더 양호하게 판단될 때, 이는 항상 시스템 A가 시스템 C보다 더 양호하게 판단된 경우이다. 이는 표 13에 주어진 완전한 순서를 산출한다.\n' +
      '\n' +
      '## 부록 E NormalFloat 4비트 데이터 형식\n' +
      '\n' +
      'NF4 데이터 유형의 정확한 값은 다음과 같다:\n' +
      '\n' +
      '[-1.0, -0.6961928009986877, -0.5250730514526367, -0.39491748809814453, -0.28444138169288635, -0.18477343022823334, -0.09105003625154495, 0.0, 0.07958029955625534, 0.16093020141124725, 0.24611230194568634, 0.33791524171829224, 0.44070982933044434, 0.5626170039176941, 0.7229568362236023, 1.0]\n' +
      '\n' +
      '## 훈련된 신경망 가중치 부록 F 정규성\n' +
      '\n' +
      '훈련된 신경망 가중치는 대부분 정규 분포를 따르는 것이 일반적인 지식이지만, 이를 검증하기 위해 통계적 테스트를 수행한다. 7B 가중치에 대한 Shapiro-Wilk 검정[53]을 사용한다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r r r} \\hline \\hline Dataset & Unnatural Instructions & Chip2 & Alpaca & FLAN v2 & Mean \\\\ \\hline Train on source and target & 36.2 & 33.7 & 38.1 & 42.0 & 37.5 \\\\ Train on target & 38.0 & 34.5 & 39.0 & 42.9 & 38.6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: MMLU 5-샷 테스트 결과는 응답에 더하여 트레이닝이 지침에 미치는 영향을 연구한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r r r r r r} \\hline \\hline \\multirow{2}{*}{Datapoints \\(\\downarrow\\) Epochs \\(\\rightarrow\\)} & \\multicolumn{3}{c}{Chip} & \\multicolumn{3}{c}{Unnatural Instructions} & \\multicolumn{3}{c}{FLAN v2} & \\multicolumn{1}{c}{Mean} \\\\ \\cline{2-10}  & 1 & 2 & 3 & 1 & 2 & 3 & 1 & 2 & 3 & Mean \\\\ \\hline\n' +
      '50000 & 34.50 & 35.30 & 34.70 & 38.10 & 42.20 & 38.10 & 43.00 & 43.50 & 44.10 & 39.28 \\\\\n' +
      '100000 & 33.70 & 33.90 & 34.00 & 40.10 & 41.20 & 37.00 & 43.90 & 43.70 & 44.90 & 39.16 \\\\\n' +
      '150000 & 34.40 & 34.80 & 35.10 & 39.70 & 41.10 & 41.50 & 44.60 & 45.50 & 43.50 & 40.02 \\\\ \\hline Mean & 34.20 & 34.67 & 34.60 & 39.30 & 41.50 & 38.87 & 43.83 & 44.23 & 44.17 & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 11: 상이한 데이터세트 크기 및 피니튜닝 에폭이 평균 5-샷 MMLU 테스트 세트 정확도에 미치는 영향. 데이터셋의 크기를 늘리고 1개 이상의 에폭에 대한 학습은 MMLU 성능에 도움이 되지만 데이터셋 간의 차이는 훨씬 더 커서 데이터셋 품질이 데이터셋 크기보다 MMLU 성능에 더 큰 영향을 미친다는 것을 알 수 있다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:25]\n' +
      '\n' +
      '## Appendix G Memory Footprint\n' +
      '\n' +
      '서로 다른 LLaMA 기본 모델을 사용한 QLoRA 훈련을 위한 메모리 풋스프링은 그림 6에서 볼 수 있다. 우리는 33B 모델이 24GB에 잘 맞지 않으며 이를 훈련하기 위해 페이징된 최적화기가 필요함을 알 수 있다. 또한 서열 길이가 512이고 구배 체크포인팅이 있는 배치 크기 1이 설명되어 있다. 이는 더 큰 배치 크기를 사용하거나 긴 시퀀스를 처리하면 활성화 기울기가 상당한 양의 메모리를 소모할 수 있음을 의미한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Model & Params & Size \\\\ \\hline Guanaco & 65B & 41 GB \\\\ Guanaco & 33B & 21 GB \\\\ Vicuna & 13B & 26 GB \\\\ ChatGPT-3.5 Turbo & N/A & N/A \\\\ Bard & N/A & N/A \\\\ Guanaco & 13B & 10 GB \\\\ Guanaco & 7B & 5 GB \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 13: 시스템 간의 쌍별 GPT-4 판단에 의해 유도된 완전 순서화\n' +
      '\n' +
      '도 6: 상이한 LLaMA 모델의 메모리 풋프린트의 브레이크다운. 입력 구배 크기는 배치 크기 1 및 시퀀스 길이 512에 대한 것이며 어댑터 및 기본 모델 가중치(주의 없음)에 대해서만 추정된다. 막대의 숫자는 전체 발자국의 개별 요소의 MB에 있는 메모리 발자국이다. 일부 모델은 특정 GPU에 적합하지 않지만 페이징된 최적화는 이러한 모델을 적합하게 만들기에 충분한 메모리를 제공합니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>