<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_EMPTY:1]\n' +
      '\n' +
      'tasks. As the pace of biomedical data generation and innovation increases, so will the potential impact of such models, with a breadth of possible downstream applications spanning fundamental biomedical discovery to care delivery.\n' +
      '\n' +
      'In this work, we detail our progress towards such a _generalist_ biomedical AI system - a unified model that can interpret multiple biomedical data modalities and handle many downstream tasks with the _same set of model weights_. One of the key challenges of this goal has been the absence of comprehensive multimodal medical benchmarks. To address this unmet need, we curate MultiMedBench, an open source multimodal medical benchmark spanning language, medical imaging, and genomics modalities with 14 diverse biomedical tasks including question answering, visual question answering, medical image classification, radiology report generation and summarization, and genomic variant calling.\n' +
      '\n' +
      'We leverage MultiMedBench to design and develop Med-PaLM Multimodal (Med-PaLM M), a large-scale generalist biomedical AI system building on the recent advances in language [8, 9] and multimodal foundation models [10, 11]. In particular, Med-PaLM M is a flexible multimodal sequence-to-sequence architecture that can easily incorporate and interleave various types of multimodal biomedical information. Further, the expressiveness of the modality-agnostic language decoder enables the handling of various biomedical tasks in a simple generative framework with a unified training strategy.\n' +
      '\n' +
      'To the best of our knowledge, Med-PaLM M is the first demonstration of a generalist biomedical AI system that can interpret multimodal biomedical data and handle a diverse range of tasks with a single model. Med-PaLM M reaches performance competitive with or exceeding the state-of-the-art (SOTA) on all tasks in MultiMedBench, often surpassing specialized domain and task-specific models by a large margin. In particular, Med-PaLM M exceeds prior state-of-the-art on chest X-ray (CXR) report generation (MIMIC-CXR dataset) by over 8% on the common success metric (micro-F1) for clinical efficacy. On one of the medical visual question answering tasks (Slake-VQA [12]) in MultiMedBench, Med-PaLM M outperforms the prior SOTA results by over 10% on the BLEU-1 and F1 metrics.\n' +
      '\n' +
      'Figure 1: **Med-PaLM M overview. A generalist biomedical AI system should be able to handle a diverse range of biomedical data modalities and tasks. To enable progress towards this overarching goal, we curate MultiMedBench, a benchmark spanning 14 diverse biomedical tasks including question answering, visual question answering, image classification, radiology report generation and summarization, and genomic variant calling. Med-PaLM Multimodal (Med-PaLM M), our proof of concept for such a generalist biomedical AI system (denoted by the shaded blue area) is competitive with or exceeds prior SOTA results from specialists models (denoted by dotted red lines) on all tasks in MultiMedBench. Notably, Med-PaLM M achieves this using a single set of model weights, without any task-specific customization.**\n' +
      '\n' +
      'We perform ablation studies to understand the importance of scale in our generalist multimodal biomedical models and observe significant benefits for tasks that require higher-level language capabilities, such as medical (visual) question answering. Preliminary experiments also suggest evidence of zero-shot generalization to novel medical concepts and tasks across model scales, and emergent capabilities [13] such as zero-shot multimodal medical reasoning. We further perform radiologist evaluation of AI-generated chest X-ray reports and observe encouraging results across model scales.\n' +
      '\n' +
      'Overall, these results demonstrate the potential of generalist biomedical AI systems for medicine. However, significant work remains in terms of large-scale biomedical data access for training such models, validating performance in real world applications, and understanding the safety implications. We outline these key limitations and directions of future research in our study. To summarize, our key contributions are as follows:\n' +
      '\n' +
      '* **Curation of MultiMedBench** We introduce MultiMedBench, a new multimodal biomedical benchmark spanning multiple modalities including medical imaging, clinical text and genomics with 14 diverse tasks for training and evaluating generalist biomedical AI systems.\n' +
      '* **Med-PaLM M, the first demonstration of a generalist biomedical AI system** We introduce Med-PaLM M, a single multitask, multimodal biomedical AI system that can perform medical image classification, medical question answering, visual question answering, radiology report generation and summarization, genomic variant calling, and more with the same set of model weights. Med-PaLM M reaches performance competitive with or exceeding state-of-the-art (SOTA) specialist models on multiple tasks in MultiMedBench without any task-specific customization.\n' +
      '* **Evidence of novel emergent capabilities in Med-PaLM M** Beyond quantitative evaluations of task performance, we observe evidence of zero-shot medical reasoning, generalization to novel medical concepts and tasks, and positive transfer across tasks. These experiments suggest promising potential of such systems in downstream data-scarce biomedical applications.\n' +
      '* **Human evaluation of Med-PaLM M outputs** Beyond automated metrics, we perform radiologist evaluation of chest X-ray reports generated by Med-PaLM M across different model scales. In a blinded side-by-side ranking on 246 retrospective chest X-rays, clinicians expressed a pairwise preference for Med-PaLM M reports over those produced by radiologists in up to 40.50% of cases. Furthermore, the best Med-PaLM M model has on average 0.25 clinically significant errors per report. These results are on par with human baselines from prior work [14], suggesting potential clinical utility.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '### Foundation models, multimodality, and generalists\n' +
      '\n' +
      'The emergence of the **foundation model** paradigm [5] has had widespread impact across a variety of applications in language [8], vision [15], and other modalities [16]. While the idea of transfer learning [17; 18] using the weights of pretrained models has existed for decades [19; 20; 21; 22], a shift has come about due to the scale of data and compute used for pretraining such models [23]. The notion of a foundation model further indicates that the model can be adapted to a wide range of downstream tasks [5].\n' +
      '\n' +
      'Within the foundation model paradigm, **multimodality**[24] has also had a variety of important impacts - in the datasets [25], in the inter-modality supervision [26], and in the generality and unification of task specification [27; 28]. For example, language has specifically been an important enabler of foundation models in other modalities [11; 29]. Visual foundation models such as CLIP [30] are made possible by training on language-labeled visual datasets [25; 31], which are easier to collect from large-scale internet data than classification datasets with pre-determined class labels (i.e., ImageNet [32]). The benefits of joint language-and-vision supervision has also been noteworthy in generative modeling of images [33], where text-to-image generative modeling has been notably more successful at producing high-fidelity image generation [34] than purely unconditioned generative image modeling [35]. Further, the flexibility of language also enables a wide range of task specifications all via one unified output space [36] - it is possible to phrase tasks traditionally addressed by different output spaces, such as object detection and object classification, all jointly via the output space of language [37]. Med-PaLM M additionally benefits from the generality of multimodality, both via a model [10] pretrained on large vision-language datasets [11], and also by further biomedical domain finetuning through a unified generative language output space.\n' +
      '\n' +
      'A related notion to that of a foundation model is that of a **generalist model** - the same model with the same set of weights, without finetuning, can excel at a wide variety of tasks. A single multitask [17] model which can address many tasks has been of long standing interest [38, 39], including for example in the reinforcement learning community [40]. Language-only models such as GPT-3 [6] and PaLM [8] simultaneously excel at many tasks using only prompting and in-context learning. Recent work has also explored generalist models capable not only of performing many tasks, but also of processing many modalities [41]. For example, the capabilities of Gato [42] span language, vision, and agent policy learning. PaLM-E [10] further shows that it is possible to obtain a single generalist model which excels at language-only tasks, vision-language tasks, and embodied vision-language tasks. Med-PaLM M is specifically a generalist model designed for the biomedical domain, built by finetuning and aligning the PaLM-E generalist model.\n' +
      '\n' +
      '### Multimodal foundation models in biomedicine\n' +
      '\n' +
      'Given the potential, there has been significant interest in multimodal foundation models for different biomedical applications. Moor _et al._[43] discuss the notion of generalist medical AI, albeit without implementation or empirical results. Theodoris _et al._[44] introduce Genformer, a transformer [45] based model pretrained on a corpus of about 30 million single-cell transcriptomes to enable context-specific predictions in low data network biology applications. BiomedGPT [46] is a multi-task biomedical foundation model pretrained on a diverse source of medical images, medical literature, and clinical notes using a combination of language model (LM) and masked image infilling objectives. However, all these efforts are pretrained models and as such they require further task-specific data and finetuning to enable downstream applications. In contrast, Med-PaLM M is directly trained to jointly solve many biomedical tasks at the same time without requiring any further finetuning or model parameter updates. LLaVA-Med [47] is perhaps most similar to our effort. The authors use PubMed and GPT-4 [48] to curate a multimodal instruction following dataset and finetune a LLaVA model with it. However, the experiments are limited to three medical visual question answering datasets and qualitative examples of conversations conditioned on a medical image. In contrast, our work is more comprehensive, spanning multiple modalities including medical imaging, clinical text, and genomics with 14 diverse tasks and expert evaluation of model outputs.\n' +
      '\n' +
      '### Multimodal medical AI benchmarks\n' +
      '\n' +
      'To the best of our knowledge, there have been limited attempts to curate benchmarks for training and evaluating generalist biomedical AI models. Perhaps the work closest in spirit is BenchMD [49]. The benchmark spans 19 publicly available datasets and 7 medical modalities, including 1D sensor data, 2D images, and 3D volumetric scans. However, their tasks are primarily focused on classification whereas our benchmark also includes generative tasks such as medical (visual) question answering, radiology report generation and summarization. Furthermore, there is currently no implementation of a generalist biomedical AI system that can competently handle all these tasks simultaneously.\n' +
      '\n' +
      '## 3 MultiMedBench: A Benchmark for Generalist Biomedical AI\n' +
      '\n' +
      'We next describe MultiMedBench, a benchmark we curated to enable the development and evaluation of generalist biomedical AI. MultiMedBench is a multi-task, multimodal benchmark comprising 12 de-identified open source datasets and 14 individual tasks. It measures the capability of a general-purpose biomedical AI to perform a variety of clinically-relevant tasks. The benchmark covers a wide range of data sources including medical questions, radiology reports, pathology, dermatology, chest X-ray, mammography, and genomics. Tasks in MultiMedBench vary across the following axes:\n' +
      '\n' +
      '* **Task type:** question answering, report generation and summarization, visual question answering, medical image classification, and genomic variant calling.\n' +
      '* **Modality:** text, radiology (CT, MRI, and X-ray), pathology, dermatology, mammography, and genomics.\n' +
      '* **Output format:** open-ended generation for all tasks including classification.\n' +
      '\n' +
      'Language-only tasks consist of medical question answering, including three of the MultiMedQA tasks used in Singhal _et al._[9], and radiology report summarization. They were selected to assess a model\'s ability to comprehend, recall, and manipulate medical knowledge. Multimodal tasks include medical visual question answering (VQA), medical image classification, chest X-ray report generation, and genomic variant calling, which are well-suited to evaluate both the visual understanding and multimodal reasoning capabilities of these models. Table 1 includes an overview of the datasets and tasks in MultiMedBench - in total, the benchmark contains over 1 million samples. For detailed descriptions of individual datasets and tasks, see Section A.1.\n' +
      '\n' +
      '## 4 Med-PaLM M: A Proof of Concept for Generalist Biomedical AI\n' +
      '\n' +
      'In this section, we detail the methods underpinning the development of the Med-PaLM M model. We first review preliminaries of the pretrained models in Section 4.1 from which Med-PaLM M inherits, then discuss the datasets and training details involved in the finetuning and specialization of the model to the biomedical domain Section 4.2.\n' +
      '\n' +
      '### Model preliminaries\n' +
      '\n' +
      'Note that Med-PaLM M inherits not only the architectures of these pretrained models, but also the general domain knowledge encoded in their model parameters.\n' +
      '\n' +
      '**Pathways Language Model (PaLM)** introduced by Chowdhery _et al._[8] is a densely-connected decoder-only Transformer [45] based large language model (LLM) trained using Pathways [50], a large-scale ML accelerator orchestration system that enables highly efficient training across TPU pods. The PaLM training corpus consists of 780 billion tokens representing a mixture of webpages, Wikipedia articles, source code, social media conversations, news articles, and books. PaLM models were trained at sizes of 8, 62, and 540 billion parameters, and all three PaLM model variants are trained for one epoch of the training data. At the time of its announcement, PaLM 540B achieved breakthrough performance, outperforming finetuned state-of-the-art models on a suite of multi-step reasoning tasks and exceeding average human performance on BIG-bench [51].\n' +
      '\n' +
      '**Vision Transformer (ViT)** introduced by Dosovitskiy _et al._[52] extends the Transformer [45] architecture to visual data such as images and videos. In this work, we consider two ViT pre-trained models as vision\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline\n' +
      '**Task Type** & **Modality** & **Dataset** & **Description** \\\\ \\hline \\multirow{3}{*}{Question Answering} & \\multirow{3}{*}{Text} & MedQA & US medical licensing exam-style, multiple-choice \\\\  & & MedMCQA & Indian medical entrance exams, multiple-choice \\\\  & & PubMedQA & Biomedical literature questions, multiple-choice \\\\ \\hline Report Summarization & Radiology & MIMIC-III & Summarizing findings in radiology reports \\\\ \\hline \\multirow{3}{*}{Question Answering} & \\multirow{3}{*}{Radiology} & VQA-RAD & Close/open-ended VQA on radiology images \\\\  & & Slake-VQA & English-Chinese bilingual VQA on radiology images \\\\  & Pathology & Path-VQA & Close/open-ended VQA on pathology images \\\\ \\hline Report Generation & Chest X-ray & MIMIC-CXR & Chest X-ray report generation \\\\ \\hline \\multirow{6}{*}{\n' +
      '\\begin{tabular}{c} Medical \\\\ Image Classification \\\\ \\end{tabular} } & Chest X-ray & MIMIC-CXR & Binary classification of chest X-ray abnormalities \\\\  & Dermatology & PAD-UFES-20 & 6-class skin lesion image classification \\\\  & & VinDr-Mammo & 5-class breast-level BI-RADS classification \\\\  & Mammography & CBIS-DDSM & 3-class lesion-level classification (mass) \\\\  & & CBIS-DDSM & 3-class lesion-level classification (calcification) \\\\  & Genomics & PrecisionFDA & Genomic variant calling as 3-class image classification \\\\  & Truth Challenge V2 & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **MultiMedBench overview.** Summary of MultiMedBench, the benchmark we introduce for the development and evaluation of Med-PaLM M. MultiMedBench consists of 14 individual tasks across 5 task types and 12 datasets spanning 7 biomedical data modalities. In total, the benchmark contains over 1 million samples.\n' +
      '\n' +
      'encoders, the 4 billion (4B) parameters model from Chen _et al._[11] and the 22 billion (22B) parameters model from Dehghani _et al._[15]. Both of these models were pretrained via supervised learning on a large classification dataset [53, 54] of approximately 4 billion images.\n' +
      '\n' +
      '**PaLM-E** introduced by Driess _et al._[10] is a multimodal language model that can process sequences of multimodal inputs including text, vision, and sensor signals. The primary PaLM-E model uses pretrained PaLM and ViT, and was initially developed for embodied robotics applications but demonstrated strong performance on multiple vision language benchmarks such as OK-VQA [55] and VQA v2 [56]. Furthermore, PaLM-E offers the flexibility to interleave images, text and sensor signals in a single prompt, enabling the model to make predictions with a fully multimodal context. PaLM-E also exhibits a wide array of capabilities including zero-shot multimodal chain-of-thought (CoT) reasoning, and few-shot in-context learning. We therefore leverage the PaLM-E model as the base architecture for Med-PaLM M.\n' +
      '\n' +
      'We consider three different combinations of LLM and vision encoders in our study - PaLM 8B with ViT 4B (PaLM-E 12B), PaLM 62B with ViT 22B (PaLM-E 84B) and PaLM 540B with ViT 22B (PaLM-E 562B). All models were pretrained on diverse vision-language datasets in addition to tasks across multiple robot embodiments as described in Driess _et al._[10].\n' +
      '\n' +
      '### Putting it all together: Med-PaLM M\n' +
      '\n' +
      'Med-PaLM M is developed by finetuning and aligning the PaLM-E model to the biomedical domain using MultiMedBench. The following summarizes important methodological details underlying the development of the model.\n' +
      '\n' +
      'Dataset and preprocessingWe resized all the images in MultiMedBench to \\(224\\times 224\\times 3\\), while preserving the original aspect ratio with padding if needed. The gray-scale images were converted to 3-channel images by stacking up the same image along the channel dimension. Task-specific prepossessing methods such as class balancing and image data augmentation are described in detail for each task in Section A.1.\n' +
      '\n' +
      'Instruction task prompting and one-shot exemplarOur goal is to train a generalist biomedical AI model to perform multiple tasks with multimodal inputs using a unified model architecture and a single set of model parameters. To this end, we trained the model with a mixture of distinct tasks simultaneously via instruction tuning [57]. Specifically, we provided the model with task-specific instructions to prompt the model to perform different types of tasks in a unified generative framework. The task prompt consists of an instruction, relevant context information, and a question. For example, as shown in Figure 2, in the chest X-ray report generation task, we included the reason for the study and the image orientation information as additional context information for the model to condition its prediction on. Similarly, for the dermatology classification task, we provided the patient clinical history associated with the skin lesion image. We formulated all classification tasks as multiple choice questions where all possible class labels are provided as individual answer options and the model was prompted to generate the most likely answer as the target output. For other generative tasks such as visual question answering and report generation and summarization, the model was finetuned on the target response.\n' +
      '\n' +
      'In order to enable the model to better follow instructions, for the majority of tasks (see Table A.1), we added a text-only _"one-shot exemplar"_ to the task prompt to condition the language model\'s prediction. The one-shot exemplar helps prompt the model with a partial input-output pair. Importantly, for multimodal tasks, we replaced the actual image in the exemplar with a dummy text placeholder (with the text string "\\(<\\)img\\(>\\)"): this (i) preserves training compute efficiency for single-image training, and also (ii) bypasses potential interference from cross-attention between a given text token and image tokens from multiple images [28]. Our results show that this scheme is effective in prompting the model to generate the desired format of responses as detailed in Section 6.\n' +
      '\n' +
      'Model trainingWe finetuned the pretrained 12B, 84B, and 562B parameter variants of PaLM-E on MultiMedBench tasks with mixture ratios denoted in Table A.1. These mixture ratios were empirically determined such that they are approximately proportional to the number of training samples in each dataset and ensuring at least one sample from each task is present in one batch. We performed an end-to-endfinetuning of the PaLM-E model with the entire set of model parameters updated during training. For multimodal tasks, image tokens were interleaved with text tokens to form multimodal context input to the PaLM-E model. The multimodal context input contains at most 1 image for all finetuning tasks. However, we note that Med-PaLM M is able to process inputs with multiple images during inference.\n' +
      '\n' +
      'We used the Adafactor optimizer [58] with momentum of \\(\\beta_{1}=0.9\\), dropout rate of 0.1, and a constant learning rate schedule. We used different sets of hyperparameters in our finetuning experiments for different model sizes, which are further detailed in Table 2.\n' +
      '\n' +
      'The resulting model, Med-PaLM M (12B, 84B, and 562B), is adapted to the biomedical domain with the capability to encode and interpret multimodal inputs and perform tasks including medical (visual) question answering, radiology report generation and summarization, medical image classification, and genomic variant calling.\n' +
      '\n' +
      '## 5 Evaluation\n' +
      '\n' +
      'In this section, we describe the purpose, scope, and methods of experimental evaluations. Results are presented in Section 6. Evaluation experiments of Med-PaLM M were designed for the following purposes:\n' +
      '\n' +
      '* **Evaluate generalist capabilities** We evaluated Med-PaLM M on all tasks in MultiMedBench across model scales. We provide initial insights on the effect of scaling ViT and LLM components across different tasks. We compared performance to previous SOTA (including specialist single-task or single-modality methods) and a state-of-art generalist model (PaLM-E) without biomedical finetuning.\n' +
      '* **Explore novel emergent capabilities** One hypothesized benefit of training a single flexible multimodal generalist AI system across diverse tasks is the emergence of novel capabilities arising from language\n' +
      '\n' +
      'Figure 2: **Illustration of instruction task prompting with one-shot exemplar.** (top) shows the task prompt for the chest X-ray report generation task. It consists of task-specific instructions, a text-only “one-shot exemplar” (omitting the corresponding image but preserving the target answer), and the actual question. The X-ray image is embedded and interleaved with textual context including view orientation and reason for the study in addition to the question. (bottom) shows the task prompt for the dermatology classification task. We formulate the skin lesion classification task as a multiple choice question answering task with all the class labels provided as individual answer options. Similar to the chest X-ray report generation task, skin lesion image tokens are interleaved with the patient clinical history as additional context to the question. The blue <img> denotes the position in the prompt where the image tokens are embedded.\n' +
      '\n' +
      'enabled combinatorial generalization, such as to novel medical concepts and tasks. We explored this via qualitative and qualitative experiments.\n' +
      '* **Measure radiology report generation quality** Automatic natural language generation (NLG) metrics do not provide sufficient evaluation of the clinical applicability of AI-generated radiology reports. We therefore performed expert radiologist evaluation of AI-generated reports on the MIMIC-CXR dataset, including comparison to the radiologist-provided reference reports.\n' +
      '\n' +
      '### Evaluation on MultiMedBench\n' +
      '\n' +
      'Med-PaLM M was simultaneously finetuned on a mixture of language-only and multimodal biomedical tasks in MultiMedBench. We assessed the model\'s in-distribution performance on these tasks by comparing to the corresponding SOTA results obtained from separate specialist models. Specifically, we used the same few-shot setup as in training for each task during evaluation. Task-specific metrics were computed on the test split of each task and compared to prior SOTA specialist AI systems. Note that for a small number of tasks described in Table 1, we were not able to find a sufficiently similar prior attempt for comparison.\n' +
      '\n' +
      '### Evaluation of language enabled zero-shot generalization\n' +
      '\n' +
      'To probe Med-PaLM M\'s ability to generalize to previously unseen medical concepts, we evaluate the model\'s ability to predict the presence or absence of tuberculosis (TB) from chest X-ray images. We used the Montgomery County chest X-ray set (MC) for this purpose. The dataset contains 138 frontal chest X-rays, of which 80 are normal cases and 58 cases have manifestations of TB [59]. Each case also contains annotations on the abnormality seen in the lung. We note that Med-PaLM M has been trained on MIMIC-CXR dataset; however, it is not trained to explicitly predict the TB disease label.\n' +
      '\n' +
      'We evaluated the accuracy across model scales by formulating this problem as a two-choice question answering task where the model was prompted (with a text-only one-shot exemplar) to generate a yes/no answer about the presence of TB in the input image.\n' +
      '\n' +
      'We further explored zero-shot chain-of-thought (CoT) multimodal medical reasoning ability of the model by prompting with a text-only exemplar (without the corresponding image) and prompting the model to generate the class prediction and an accompanying report describing the image findings. We note that while we did prompt the model with a single text-only input-output pair, we omitted the image (used a dummy text placeholder instead) and the text exemplar was hand-crafted rather than drawn from the training set. Hence, this approach can be considered zero-shot rather than one-shot.\n' +
      '\n' +
      'In order to assess Med-PaLM M\'s ability to generalize to novel task scenarios, we evaluated the model performance on two-view chest X-ray report generation - this is a novel task given the model was trained to generate reports only from a single-view chest X-ray.\n' +
      '\n' +
      'Finally, we also probed for evidence of positive task transfer as a result of jointly training a single generalist model to solve many different biomedical tasks. To this end, we performed an ablation study where we trained a Med-PaLM M 84B variant by excluding the MIMIC-CXR classification tasks from the task mixture. We compared this model variant to the Med-PaLM M 84B variant trained on the complete MultiMedBench mixture on the chest X-ray report generation task with the expectation of improved performance in the latter.\n' +
      '\n' +
      '### Clinician evaluation of radiology report generation\n' +
      '\n' +
      'To further assess the quality and clinical applicability of chest X-ray reports generated by Med-PaLM M and understand the effect of model scaling, we conducted a human evaluation using the MIMIC-CXR dataset. The evaluation was performed by four qualified thoracic radiologists based in India.\n' +
      '\n' +
      '**Dataset**: The evaluation set consisted of 246 cases selected from the MIMIC-CXR test split. To match the expected input format of Med-PaLM M, we selected a single image from each study. We excluded studies that had ground truth reports mentioning multiple X-ray views or past examinations of the same patient.\n' +
      '\n' +
      '**Procedure**: We conducted two complementary human evaluations: (1) _side-by-side evaluation_ where raters compared multiple alternative report findings and ranked them based on their overall quality, and (2)_independent evaluation_ where raters assessed the quality of individual report findings. Prior to performing the final evaluation, we iterated upon the instructions for the raters and calibrated their grades using a pilot set of 25 cases that were distinct from the evaluation set. Side-by-side evaluation was performed for all 246 cases, where each case was rated by a single radiologist randomly selected from a pool of four. For independent evaluation, each of the four radiologists independently annotated findings generated by three Med-PaLM M model variants (12B, 84B, and 562B) for every case in the evaluation set. Radiologists were blind to the source of the report findings for all evaluation tasks, and the reports were presented in a randomized order.\n' +
      '\n' +
      '**Side-by-side evaluation** The input to each side-by-side evaluation was a single chest X-ray, along with the "indication" section from the MIMIC-CXR study. Four alternative options for the "findings" section of the report were shown to raters as depicted in Figure A.3. The four alternative "findings" sections corresponded to the dataset reference report\'s findings, and findings generated by three Med-PaLM M model variants (12B, 84B, 562B). Raters were asked to rank the four alternative findings based on their overall quality using their best clinical judgement.\n' +
      '\n' +
      '**Independent evaluation** For independent evaluation, raters were also presented with a single chest X-ray, along with the indication and reference report\'s findings from the MIMIC-CXR study (marked explicitly as such), but this time only a single findings paragraph generated by Med-PaLM M as shown in Figure A.4. Raters were asked to assess the quality of the Med-PaLM M generated findings in the presence of the reference inputs provided and their own judgement of the chest X-ray image. The rating schema proposed in Yu _et al._[60] served as inspiration for our evaluation task design.\n' +
      '\n' +
      'First, raters assessed whether the quality and view of the provided image were sufficient to perform the evaluation task fully. Next, they annotated all passages in the model-generated findings that they disagreed with (errors), and all missing parts (omissions). Raters categorized each error passage by its type (no finding, incorrect finding location, incorrect severity, reference to non-existent view or prior study), assessed its clinical significance, and suggested alternative text to replace the selected passage. Likewise, for each omission, raters specified a passage that should have been included and determined if the omission had any clinical significance.\n' +
      '\n' +
      '## 6 Results\n' +
      '\n' +
      'Here we present results across the three different evaluation setups introduced in Section 5.\n' +
      '\n' +
      '### Med-PaLM M performs near or exceeding SOTA on all MultiMedBench tasks\n' +
      '\n' +
      '**Med-PaLM M performance versus baselines** We compared Med-PaLM M with two baselines:\n' +
      '\n' +
      '* prior SOTA specialist models for each of the MultiMedBench tasks\n' +
      '* a baseline generalist model (PaLM-E 84B) without any biomedical domain finetuning. We used this model size variant (and not PaLM-E 562B) due to compute constraints.\n' +
      '\n' +
      'Results are summarized in Table 2. Across MultiMedBench tasks, Med-PaLM M\'s best result (across three model sizes) exceeded prior SOTA results on 5 out of 12 tasks (for two tasks, we were unable to find a prior SOTA comparable to our setup) while being competitive on the rest. Notably, these results were achieved with a generalist model using the same set of model weights without any task-specific architecture customization or optimization.\n' +
      '\n' +
      'On medical question answering tasks, we compared against the SOTA Med-PaLM 2 results [61] and observed higher performance of Med-PaLM 2. However, when compared to the baseline PaLM model on which Med-PaLM M was built, Med-PaLM M outperformed the previous best PaLM results [9] by a large margin in the same few-shot setting on all three question answering datasets.\n' +
      '\n' +
      'Further, when compared to PaLM-E 84B as a generalist baseline without biomedical domain finetuning, Med-PaLM M exhibited performance improvements on all 14 tasks often by a significant margin, demonstrating the importance of domain adaptation. Taken together, these results illustrate the strong capabilities of Med-PaLM M as a generalist biomedical AI model. We further describe the results in detail for each of the individual tasks in Section A.3.\n' +
      '\n' +
      '**Med-PaLM M performance across model scales** We summarize Med-PaLM M performance across model scales (12B, 84B, and 562B) in Table 3. The key observations are:\n' +
      '\n' +
      '* **Language reasoning tasks benefit from scale** For tasks that require language understanding and reasoning such as medical question answering, medical visual question answering and radiology report summarization, we see significant improvements as we scale up the model from 12B to 562B.\n' +
      '* **Multimodal tasks bottlenecked by vision encoder performance** For tasks such as mammography or dermatology image classification, where nuanced visual understanding is required but minimal language reasoning is needed (outputs are classification label tokens only), the performance improved from Med-PaLM M 12B to Med-PaLM 84B but plateaued for the 562B model, possibly because the vision encoder is not further scaled in that step (both the Med-PaLM M 84B and 562B models use the same 22B ViT as the vision encoder), thereby acting as a bottleneck to observing a scaling benefit. We note the possibility of additional confounders here such as the input image resolution.\n' +
      '\n' +
      'The scaling results on the chest X-ray report generation task are interesting (Table 3). While on the surface, the task seems to require complex language understanding and reasoning capabilities and would thus benefit\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline Task Type & Modality & Dataset & Metric & SOTA & \\begin{tabular}{c} PaLM-E \\\\ (84B) \\\\ \\end{tabular} & \n' +
      '\\begin{tabular}{c} Med-PaLM M \\\\ (Best) \\\\ \\end{tabular} \\\\ \\hline \\multirow{3}{*}{Question Answering} & \\multirow{3}{*}{Text} & MedQA & Accuracy & **86.50\\%**[61] & 28.83\\% & 69.68\\% \\\\  & & MedMCQA & Accuracy & **72.30\\%**[61] & 33.35\\% & 62.59\\% \\\\  & & PubMedQA & Accuracy & **81.80\\%**[61] & 64.00\\% & 80.00\\% \\\\ \\hline \\multirow{3}{*}{Report Summarization} & \\multirow{3}{*}{Radiology} & \\multirow{3}{*}{MIMIC-III} & ROUGE-L & **38.70\\%**[62] & 3.30\\% & 32.03\\% \\\\  & & & BLEU & **16.20\\%**[62] & 0.34\\% & 15.36\\% \\\\  & & & F1-RadGraph & **40.80\\%**[62] & 8.00\\% & 34.71\\% \\\\ \\hline \\multirow{3}{*}{Visual Question Answering} & \\multirow{3}{*}{Radiology} & VQA-RAD & BLEU-1 & 71.03\\% [63] & 59.19\\% & **71.27\\%** \\\\  & & & F1 & N/A & 38.67\\% & **62.06\\%** \\\\ \\cline{1-1}  & & & BLEU-1 & 78.60\\% [64] & 52.65\\% & **92.77\\%** \\\\ \\cline{1-1}  & & & F1 & 78.10\\% [64] & 24.53\\% & **89.28\\%** \\\\ \\cline{1-1}  & & & BLEU-1 & 70.30\\% [64] & 54.92\\% & **72.27\\%** \\\\ \\cline{1-1}  & & & F1 & 58.40\\% [64] & 29.68\\% & **62.69\\%** \\\\ \\hline \\multirow{3}{*}{Report Generation} & \\multirow{3}{*}{Chest X-ray} & \\multirow{3}{*}{MIMIC-CXR} & Micro-F1-14 & 44.20\\% [65] & 15.40\\% & **53.56\\%** \\\\  & & & Macro-F1-14 & 30.70\\% [65] & 10.11\\% & **39.83\\%** \\\\ \\cline{1-1}  & & & Micro-F1-5 & 56.70\\% [66] & 5.51\\% & **57.88\\%** \\\\ \\cline{1-1}  & & & Macro-F1-5 & N/A & 4.85\\% & **51.60\\%** \\\\ \\cline{1-1}  & & & F1-RadGraph & 24.40\\% [14] & 11.66\\% & **26.71\\%** \\\\ \\cline{1-1}  & & & BLEU-1 & **39.48\\%**[65] & 19.86\\% & 32.31\\% \\\\ \\cline{1-1}  & & & BLEU-4 & **13.30\\%**[66] & 4.60\\% & 11.50\\% \\\\ \\cline{1-1}  & & & ROUGE-L & **29.60\\%**[67] & 16.53\\% & 27.49\\% \\\\ \\cline{1-1}  & & & CIDEr-D & **49.50\\%**[68] & 3.50\\% & 26.17\\% \\\\ \\hline \\multirow{3}{*}{Image Classification} & \\multirow{3}{*}{Chest X-ray} & MIMIC-CXR & Macro-AUC & **81.27\\%**[69] & 51.48\\% & 79.09\\% \\\\  & & (5 conditions) & Macro-F1 & N/A & 7.83\\% & **41.57\\%** \\\\ \\cline{1-1}  & & & Macro-AUC & N/A & 63.37\\% & **97.27\\%** \\\\ \\cline{1-1}  & & & Macro-F1 & N/A & 1.38\\% & **84.32\\%** \\\\ \\cline{1-1}  & & & Micro-AUC & 64.50\\% [49] & 51.49\\% & **71.76\\%** \\\\ \\cline{1-1}  & & & Macro-F1 & N/A & 16.06\\% & **35.70\\%** \\\\ \\cline{1-1}  & & & CBIS-DDSM & Macro-AUC & N/A & 47.75\\% & **73.31\\%** \\\\ \\cline{1-1}  & & & (mass) & Macro-F1 & N/A & 7.77\\% & **51.12\\%** \\\\ \\cline{1-1}  & & & CBIS-DDSM & Macro-AUC & N/A & 40.67\\% & **82.22\\%** \\\\ \\cline{1-1}  & & & (calcification) & Macro-F1 & **70.71\\%**[70] & 11.37\\% & 67.86\\% \\\\ \\cline{1-1}  & & & PrecisionFDA & Indel-F1 & **99.40\\%**[71] & 53.01\\% & 97.04\\% \\\\ \\cline{1-1}  & & & (Variant Calling) & (Truth Challenge V2) & SNP-F1 & **99.70\\%**[71] & 52.84\\% & 99.35\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: **Performance comparison on MultiMedBench.** We compare Med-PaLM M with specialist SOTA models and a generalist model (PaLM-E 84B) without biomedical domain finetuning. Across all tasks, datasets and metrics combination in MultiMedBench, we observe Med-PaLM M performance near or exceeding SOTA. Note that these results are achieved by Med-PaLM M with the same set of model weights without any task-specific customization.\n' +
      '\n' +
      'from scaling the language model, we find the Med-PaLM M 84B model to be roughly on-par or slightly exceeding the 562B model on a majority of metrics, which may simply be due to fewer training steps used for the larger model. Another possibility for the diminishing return of increasing the size of language model is likely that the output space for chest X-ray report generation in the MIMIC-CXR dataset is fairly confined to a set of template sentences and limited number of conditions. This insight has motivated the use of retrieval based approaches as opposed to a fully generative approach for the chest X-ray report generation task on this dataset [72, 73]. Additionally, the larger 562B model has a tendency towards verbosity rather than the comparative brevity of the 84B model, and without further preference alignment in training, this may impact its metrics.\n' +
      '\n' +
      '### Med-PaLM M demonstrates zero-shot generalization to novel medical tasks and concepts\n' +
      '\n' +
      'Training a generalist biomedical AI system with language as a common grounding across different tasks allows the system to tackle new tasks by combining the knowledge it has learned for other tasks (i.e. combinatorial\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline Task Type & Modality & Dataset & Metric & Med-PaLM M & Med-PaLM M & Med-PaLM M \\\\  & & & & (12B) & (84B) & (562B) \\\\ \\hline \\multirow{3}{*}{Question Answering} & \\multirow{3}{*}{Text} & MedQA & Accuracy & 29.22\\% & 46.11\\% & **69.68\\%** \\\\  & & MedMCQA & Accuracy & 32.20\\% & 47.60\\% & **62.59\\%** \\\\  & & PubMedQA & Accuracy & 48.60\\% & 71.40\\% & **80.00\\%** \\\\ \\hline \\multirow{3}{*}{Report Summarization} & \\multirow{3}{*}{Radiology} & ROUGE-L & 29.45\\% & 31.47\\% & **32.03\\%** \\\\  & & MIMIC-III & BLEU & 12.14\\% & **15.36\\%** & 15.21\\% \\\\  & & & F1-RadGraph & 31.43\\% & 33.96\\% & **34.71\\%** \\\\ \\hline \\multirow{3}{*}{Visual Question Answering} & \\multirow{3}{*}{Radiology} & VQA-RAD & BLEU-1 & 64.02\\% & 69.38\\% & **71.27\\%** \\\\  & & & F1 & 50.66\\% & 59.90\\% & **62.06\\%** \\\\  & & & BLEU-1 & 90.77\\% & **92.70\\%** & 91.64\\% \\\\  & & & & F1 & 86.22\\% & **89.28\\%** & 87.50\\% \\\\  & & Pathology & Path-VQA & BLEU-1 & 68.97\\% & 70.16\\% & **72.27\\%** \\\\  & & & F1 & 57.24\\% & 59.51\\% & **62.69\\%** \\\\ \\hline \\multirow{3}{*}{Report Generation} & \\multirow{3}{*}{Chest X-ray} & \\multirow{3}{*}{MIMIC-CXR} & Micro-F1-14 & 51.41\\% & **53.56\\%** & 51.60\\% \\\\  & & & Macro-F1-14 & 37.31\\% & **39.83\\%** & 37.81\\% \\\\  & & & Micro-F1-5 & 56.54\\% & **57.88\\%** & 56.28\\% \\\\  & & & Macro-F1-5 & 50.57\\% & **51.60\\%** & 49.86\\% \\\\  & & & F1-RadGraph & 25.20\\% & **26.71\\%** & 26.06\\% \\\\  & & & BLEU-1 & 30.90\\% & **32.31\\%** & 31.73\\% \\\\  & & & BLEU-4 & 10.43\\% & 11.31\\% & **11.50\\%** \\\\  & & & ROUGE-L & 26.16\\% & 27.29\\% & **27.49\\%** \\\\  & & & CIDer-D & 23.43\\% & **26.17\\%** & 25.27\\% \\\\ \\hline \\multirow{3}{*}{Image Classification} & \\multirow{3}{*}{Chest X-ray} & MIMIC-CXR & Macro-AUC & 76.67\\% & 78.35\\% & **79.09\\%** \\\\  & & (5 conditions) & Macro-F1 & 38.33\\% & 36.83\\% & **41.57\\%** \\\\ \\cline{1-1}  & & & Macro-AUC & 95.57\\% & **97.27\\%** & 96.08\\% \\\\ \\cline{1-1}  & & & Macro-F1 & 78.42\\% & **84.32\\%** & 77.03\\% \\\\ \\cline{1-1}  & & & Macro-AUC & 66.29\\% & **71.76\\%** & 71.42\\% \\\\ \\cline{1-1}  & & & Macro-F1 & 29.81\\% & **35.70\\%** & 33.90\\% \\\\ \\cline{1-1}  & & & CBIS-DDSM & Macro-AUC & 70.11\\% & 73.09\\% & **73.31\\%** \\\\ \\cline{1-1}  & & (mass) & Macro-F1 & 47.23\\% & 49.98\\% & **51.12\\%** \\\\ \\cline{1-1}  & & CBIS-DDSM & Macro-AUC & 81.40\\% & **82.22\\%** & 80.90\\% \\\\ \\cline{1-1}  & & (calcification) & Macro-F1 & **67.86\\%** & 63.81\\% & 63.03\\% \\\\ \\cline{1-1}  & & & Indel-F1 & 96.42\\% & **97.04\\%** & 95.46\\% \\\\ \\cline{1-1}  & & & SNP-F1 & **99.35\\%** & 99.32\\% & 99.16\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: **Performance of Med-PaLM M on MultiMedBench across model scales.** We summarize the performance of Med-PaLM M across three model scale variants 12B, 84B, 562B. All models were finetuned and evaluated on the same set of tasks in MultiMedBench. We observe that scaling plays a key role in language-only tasks and multimodal tasks that require reasoning such as visual question answering. However, scaling has diminishing benefit for image classification and chest X-ray report generation task.\n' +
      '\n' +
      'generalization). We highlight preliminary evidence which suggests Med-PaLM M can generalize to novel medical concepts and unseen tasks in a zero-shot fashion. We further observe zero-shot multimodal reasoning as an emergent capability [13] of Med-PaLM M. Finally, we demonstrate benefits from positive task transfer as a result of the model\'s multi-task, multimodal training.\n' +
      '\n' +
      '#### 6.2.1 Evidence of generalization to novel medical concepts\n' +
      '\n' +
      'We probed the zero-shot generalization capability of Med-PaLM M for an unseen medical concept by evaluating its ability to detect tuberculosis (TB) abnormality from chest X-ray images in the Montgomery County (MC) dataset. As shown in Table 4, Med-PaLM M performed competitively compared to SOTA results obtained by a specialized ensemble model optimized for this dataset [74]. We observed similar performance across three model variants, consistent with findings on other medical image classification tasks in MultiMedBench. Given the classification task was set up as an open-ended question answering task, we did not report the AUC metric which requires the normalized predicted probability of each possible class.\n' +
      '\n' +
      '#### 6.2.2 Evidence of emergent zero-shot multimodal medical reasoning\n' +
      '\n' +
      'We also qualitatively explored the zero-shot chain-of-thought (CoT) capability of Med-PaLM M on the MC TB dataset. In contrast to the classification setup, we prompted the model with a text-only exemplar to generate a report describing the findings in a given image in addition to a yes/no classification prediction. In Figure 3, we present qualitative examples of zero-shot CoT reasoning from the Med-PaLM M 84B and 562B variants. In particular, both Med-PaLM M variants were able to identify the major TB related lesion in the correct location. However, according to expert radiologist review, there are still some omissions of findings and errors in the model generated report, suggesting room for improvement. It is noteworthy that Med-PaLM M 12B failed to generate a coherent visually conditioned response, which indicates that scaling of the language model plays a key role in the zero-shot CoT multimodal reasoning capability (i.e. this might be an emergent capability [13]).\n' +
      '\n' +
      '#### 6.2.3 Evidence of generalization to novel tasks\n' +
      '\n' +
      'Although Med-PaLM M was only trained with single-view chest X-ray image inputs, we observed the capability of the model to generalize to a novel task setup with multi-view visual inputs. Specifically, on a subset of studies from MIMIC-CXR where each report is accompanied with both a frontal and a lateral view X-ray image. we observe that Med-PaLM M is able to attain zero-shot performance comparable to the single-view report generation task as detailed in Table 5. This ability is promising given medical imaging studies often benefit from the interpretation of prior historical studies in addition to the current instance for optimal performance.\n' +
      '\n' +
      '#### 6.2.4 Evidence of positive task transfer\n' +
      '\n' +
      'To demonstrate the positive task transfer arising from joint training across modalities and tasks, we performed an ablation study where we trained a Med-PaLM M 84B variant by excluding the MIMIC-CXR classification task from the task mixture and compared this model variant against Med-PaLM M 84B trained on the full MultiMedBench mixture. As seen in Table 6, we observed that the model trained jointly on both report\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Model & \\# Training samples & Accuracy \\\\ \\hline SOTA [74] & **138** & **92.60\\%** \\\\ Med-PaLM M (12B) & 0 & 86.96\\% \\\\ Med-PaLM M (84B) & 0 & 82.60\\% \\\\ Med-PaLM M (562B) & 0 & 87.68\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: **Zero-shot classification performance of Med-PaLM M on the tuberculosis (TB) detection task.** Med-PaLM M performs competitively to the SOTA model [74] finetuned on the Montgomery County TB dataset using model ensemble. Notably, Med-PaLM M achieves this result with a simple task prompt consisting of a single text-only exemplar (without task-specific image and hence zero-shot), in contrast to the specialist model that requires training on all the samples in the dataset.\n' +
      '\n' +
      'generation and classification has higher performance across the board on all report generation metrics. We also observe that the model trained only on chest X-ray report generation can generalize to abnormality detection in a zero-shot fashion with compelling performance, as evidenced by a higher macro-F1 score. This is another example of generalization to a novel task setting where the model learns to differentiate between types of abnormalities from training on the more complex report generation task.\n' +
      '\n' +
      '### Med-PaLM M performs encouragingly on radiology report generation across model scales\n' +
      '\n' +
      'To further understand the clinical applicability of Med-PaLM M, we conducted radiologist evaluations of model-generated chest X-ray reports (and reference human baselines). Under this evaluation framework, we observe encouraging quality of Med-PaLM M generated reports across model scales as detailed below.\n' +
      '\n' +
      '#### 6.3.1 Side-by-side evaluation\n' +
      '\n' +
      'In a side-by-side evaluation, four clinician raters ranked the quality of four radiology reports, comparing the radiologist-provided reference report from the MIMIC-CXR dataset with reports generated by different Med-PaLM M model scales (12B, 84B, and 562B).\n' +
      '\n' +
      'Figure 3(a) summarizes how often each rater ranked a report generated by one of the three Med-PaLM M variants or the reference report as the best among four candidate reports. Averaged over all four raters, the radiologist-provided reference report was ranked best in 37.14% of cases, followed by Med-PaLM M (84B)\n' +
      '\n' +
      'Figure 3: **Evidence of emergent zero-shot multimodal medical reasoning with Med-PaLM M. Large Med-PaLM M models exhibit zero-shot CoT reasoning capability in identifying and describing tuberculosis related findings in chest X-ray images. The model is prompted with task-specific instructions and a text-only exemplar (without the corresponding image) to generate a report describing findings in the given X-ray image. Model predictions from Med-PaLM M 84B and 562B are shown together with the annotations from an expert radiologist. Both models correctly localized the major TB related cavity lesion in the right upper lobe. However, both models did not address the small cavity lesion in left upper lobe (Med-PaLM M 562B was considered better than Med-PaLM M 64B in this example as it also alluded to the opacity in the right middle lobe and did not make the incorrect statement of left lung being clear). Notably, Med-PaLM M 12B failed to generate a coherent report, indicating the importance of scaling for zero-shot COT reasoning.**\n' +
      '\n' +
      'which was ranked best in 25.78% of cases, and the other two model scales, 12B and 562B, which were ranked best in 19.49% and 17.59% of cases respectively.\n' +
      '\n' +
      'To enable a direct comparison of reports generated by each Med-PaLM M model scale to the radiologist-provided reference report, we derived pairwise preferences from the four-way ranking and provided a breakdown for each rater and model scale in Figure 3(b). Averaged over all four raters, Med-PaLM M 84B was preferred over the reference report in 40.50% of cases, followed by the other two model scales, 12B and 562B, which were preferred over the reference report in 34.05% and 32.00% of cases, respectively.\n' +
      '\n' +
      '#### 6.3.2 Independent evaluation\n' +
      '\n' +
      'We report the rates of omissions and errors radiologists identified in findings paragraphs generated by Med-PaLM M. Figure 5 provides breakdowns by model scales (12B, 84B, 562B). We observed different trends for omissions and errors. For omissions, we observed the lowest rate of 0.12 (95% CI, 0.10 - 0.15) omissions per report on average for both the Med-PaLM M 12B and 84B models, followed by 0.13 (95% CI, 0.11 - 0.16) for the 562B model.\n' +
      '\n' +
      'In contrast, we measured the lowest mean error rate of 0.25 (95% CI, 0.22 - 0.28) for Med-PaLM M 84B, followed by 0.28 (95% CI, 0.24 - 0.31) for Med-PaLM M 12B and 0.29 (95% CI, 0.25 - 0.32) for the 562B model. Notably, this error rate is comparable to those reported for human radiologists baselines on the MIMIC-CXR\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline Dataset & Metric & Med-PaLM M (84B) & \n' +
      '\\begin{tabular}{c} Med-PaLM M (84B) \\\\ No CXR classification \\\\ \\end{tabular} \\\\ \\hline \\multirow{8}{*}{MIMIC-CXR} & Micro-F1-14 & **53.56\\%** & 52.94\\% \\\\  & Macro-F1-14 & **39.83\\%** & 38.92\\% \\\\  & Micro-F1-5 & **57.88\\%** & 57.58\\% \\\\  & Macro-F1-5 & **51.60\\%** & 51.32\\% \\\\ \\cline{1-1}  & F1-RadGraph & **26.71\\%** & 26.08\\% \\\\ \\cline{1-1}  & BLEU-1 & **32.31\\%** & 31.72\\% \\\\ \\cline{1-1}  & BLEU-4 & **11.31\\%** & 10.87\\% \\\\ \\cline{1-1}  & ROUGE-L & **27.29\\%** & 26.67\\% \\\\ \\cline{1-1}  & CIDEr-D & **26.17\\%** & 25.17\\% \\\\ \\hline MIMIC-CXR & Macro-AUC & **78.35\\%** & 73.88\\% \\\\ (5 conditions) & Macro-F1 & 36.83\\% & **43.97\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: **Positive task transfer between CXR report generation and abnormality classification.** We observe positive transfer as a result of multi-task training with Med-PaLM M model trained jointly on both chest X-ray report generation and classification tasks. It exhibits higher performance on report generation metrics compared to a Med-PaLM M model trained without chest X-ray report classification. We also observe that training on the chest X-ray report generation task alone enables Med-PaLM M to generalize to abnormality detection in a zero-shot fashion.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline Metric & SOTA & Med-PaLM M (12B) & Med-PaLM M (84B) & Med-PaLM M (562B) \\\\ \\hline Micro-F1-14 & 44.20\\% & 49.80\\% & **50.54\\%** & 48.85\\% \\\\ Macro-F1-14 & 30.70\\% & 37.69\\% & **37.78\\%** & 37.29\\% \\\\ Micro-F1-5 & 56.70\\% & 54.49\\% & **56.37\\%** & 54.36\\% \\\\ Macro-F1-5 & N/A & 48.33\\% & **51.23\\%** & 48.49\\% \\\\ F1-RadGraph & 24.40\\% & 26.73\\% & **28.30\\%** & 27.28\\% \\\\ BLEU-1 & **39.48\\%** & 33.31\\% & 34.58\\% & 33.83\\% \\\\ BLEU-4 & **13.30\\%** & 11.51\\% & 12.44\\% & 12.47\\% \\\\ ROUGE-L & **29.60\\%** & 27.84\\% & 28.71\\% & 28.49\\% \\\\ CIDEr-D & **49.50\\%** & 27.58\\% & 29.80\\% & 29.80\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: **Zero-shot generalization to two-view chest X-ray report generation.** Med-PaLM M performance remains competitive on a novel two-view report generation task setup despite having not been trained with two visual inputs before. Med-PaLM M achieves SOTA results on clinical efficacy metrics for the two view report generation task.\n' +
      '\n' +
      'dataset in a prior study [14].\n' +
      '\n' +
      'It is important to mention that our analysis is limited to errors of clinical relevance, ensuring a specific focus on clinical interpretation. This includes those errors related to the presence, location or severity of a clinical finding. Example of non-clinical errors are passages referring to views or prior studies not present, which stem from training artifacts.\n' +
      '\n' +
      'These trends across model scales were identical for the subset of omissions and errors that were marked as significant by radiologist raters. We refer the reader to Table A.8 for an overview of error and omission rates, including non-clinical errors.\n' +
      '\n' +
      'In Figure 6, we illustrate a qualitative example of chest X-ray reports generated by Med-PaLM M across three model sizes along with the target reference report. For this example, our panel of radiologists judged the Med-PaLM M 12B report to have two clinically significant errors and one omission, the Med-PaLM M 84B report to have zero errors and zero omissions, and the Med-PaLM M 562B report to have one clinically insignificant errors and no omissions.\n' +
      '\n' +
      'Figure 4: **Side-by-side human evaluation.** Four clinician raters ranked the quality of four radiology reports in a side-by-side evaluation, comparing the radiologist-provided reference report from MIMIC-CXR with reports generated by different Med-PaLM M model scale variants (12B, 84B, 562B).\n' +
      '\n' +
      'Figure 5: **Independent human evaluation.** Rates of omissions and clinical errors identified by clinician raters in radiology reports generated by Med-PaLM M. Clinical errors are those related to the presence, location or severity of a clinical finding.\n' +
      '\n' +
      '## 7 Discussion\n' +
      '\n' +
      'To the best of our knowledge, Med-PaLM M is the first demonstration of a generalist biomedical AI system that can interpret a wide range of medical modalities, perform competently (including near or exceeding prior SOTA) on a diverse array of tasks, and generalize to unseen biomedical concepts and tasks. This potentially opens up new possibilities in applications spanning scientific discovery to care delivery. We elaborate on the implications of this development as well as the challenges and limitations below.\n' +
      '\n' +
      '**Lack of benchmarks a key bottleneck for the development of generalist biomedical AI** AI progress to date has largely been catalyzed by the development of high quality benchmarks. While there exists several single-task biomedical AI datasets, there have been limited attempts to unify them and create benchmarks for the development of generalist biomedical AI systems. Our curation of MultiMedBench is a step towards addressing this unmet need. However, the benchmark has several important limitations including limited size of the individual datasets (a cumulative size of \\(\\tilde{1}\\) million samples) and limited modality and task diversity (e.g., lacking life sciences such as transcriptomics and proteomics). Another key barrier to developing models for use across an even wider variety of biomedical data types is the lack of large scale multimodal datasets, which would permit joint learning and alignment of the modality-specific encoders with the decoder.\n' +
      '\n' +
      '**Importance of medical finetuning and specialization** PaLM-E is a highly capable generalist AI model as evidenced by its SOTA performance on a wide range of vision-language and embodied robotics tasks. Yet, its out-of-the-box performance on MultiMedBench was poor and Med-PaLM M outperforms it by a wide margin across model scales. This result suggests that finetuning with domain-specific biomedical data is critical to achieving good performance on biomedical tasks, perhaps due to the distribution shift presented by the domain overall compared to the plethora of non-medical tasks and modalities.\n' +
      '\n' +
      '**Scaling multimodal AI models is challenging** In the language domain, scaling the model has led to leapfrog improvements in performance and emergent capabilities. However, our preliminary experiments suggest this is likely more challenging for multimodal generalist models in the biomedical task domain due to the medical data scarcity. Given the wide array of modalities and tasks such generalist models are expected to understand and tackle, it is crucial that the encoders for such diverse modalities are scaled jointly with the\n' +
      '\n' +
      'Figure 6: **Qualitative example of reference and Med-PaLM M generated chest X-ray reports.** We present a qualitative example of chest X-ray reports generated by Med-PaLM M across model scales along with the target reference report. In this example, a panel of radiologists adjudicated the Med-PaLM M 12B report to have two clinically significant errors and one omission, the Med-PaLM M 84B report to have zero errors and zero omissions, and the Med-PaLM M 562B report to have one clinically insignificant error and no omissions.\n' +
      '\n' +
      'language model. Otherwise, for tasks that require interpretation of data from a combination of modalities, the performance will end up being bottlenecked by the weakest encoder. We see evidence of this in medical image classification tasks such as mammography and dermatology where scaling the language model component has little effect on the task performance as the potential key bottleneck is the vision encoder. It is possible that the small volume of medical data in MultiMedBench is not be sufficient to effectively adapt a ViT pretrained on natural images to the medical domain, thereby limiting the benefits of model scaling. As such, our study only provides some initial insights on the effect of model scaling on biomedical task performance. Future research is needed to fully understand the effect of model scaling by teasing apart the scaling effect of the language model from that of modality-specific encoders, with sufficient amounts of biomedical data.\n' +
      '\n' +
      'Technical considerations for generalist biomedical AI Med-PaLM M builds on state-of-the-art vision and language components such as ViT and PaLM. Yet, putting them together requires careful considerations around token lengths allocated to visual encoder outputs, total context length of the model, sampling strategies, training data mixtures and so forth. Further, simple, yet important techniques such as the use of one-shot training with dummy image tokens make an important difference in the quality and compute efficiency of the final model. With increasing generality of the AI system, the number of details requiring careful consideration tends to increase as well. We also note that Med-PaLM M architecture as setup currently is not optimal for few-shot in-context learning.\n' +
      '\n' +
      'Progress in AI for radiology report generationOur evaluation by radiologists of Med-PaLM M generated radiology reports suggests encouraging performance of the model on a challenging multimodal task. In up to 40.50% of the cases, a Med-PaLM M generated report was preferred over the human-generated reference report. Further, the average number of clinically significant errors within the model responses is comparable to those reported for human-generated reports in prior studies [14] on the same dataset. These promising results underpin rapid development in the task of automatic radiology report generation and suggest the potential for clinical utility in the future.\n' +
      '\n' +
      'Generalist agents are not the only approach to multimodal biomedical AI While generalist biomedical AI systems offer exciting possibilities [43], there are other approaches to developing multimodal biomedical AI systems that might be more applicable depending on data availability, pretrained models, compute and application scenarios. These include leveraging frozen encoders with adapter layers [75] to glue together a multimodal biomedical AI system or developing LLMs that can interface with specialist biomedical encoders or task-specific agents through tool use [76].\n' +
      '\n' +
      'Considerations for real-world applications of generalist biomedical AIWhile the development of generally capable biomedical AI systems is exciting, for such systems to be useful in practice or opening the door to new applications, they need to match or exceed specialized, single-task models or otherwise reach clinically applicable levels of performance. While beyond the scope of this work, the progress here necessitates careful considerations of safety and equity in the development and validation of such systems.\n' +
      '\n' +
      '## 8 Perspective on Generalist Biomedical AI\n' +
      '\n' +
      'Reaching near or above SOTA on a diverse range of biomedical tasks with a single set of model weights is a noteworthy milestone for the development of generalist biomedical AI systems. While human clinicians can train for "general practice" [77], helpful subspecialty-specific expertise is often found in different experts [78], to whom non-specialist clinicians may refer for specialist opinions in the course of care. It is also commonplace for multiple physician specialities to work together in care delivery. We envisage a similar future for biomedical AI where generalist and specialist AI systems interact and collaborate together with expert clinicians and researchers in a tight feedback loop to tackle grand challenges in biomedicine.\n' +
      '\n' +
      'Our finding, of a single generalist biomedical AI that reaches compelling performance across disparate tasks and contexts, hints at new frontiers for impact in applications. This includes the potential for near zero-shot insight in new domains, as a tool for discovery integrating insights from distinct areas of biomedicine, and as a common point of assistance providing access to expertise from many different fields.\n' +
      '\n' +
      '## 9 Conclusion\n' +
      '\n' +
      'Medicine is a multidisciplinary endeavour. Generalist biomedical AI systems that effectively assimilate and encode multimodal medical data at scale and rapidly adapt to new clinical contexts are likely to be the foundation of next generation learning health systems and make healthcare more accessible, efficient, equitable and humane. While further development and rigorous validation is needed, we believe Med-PaLM M represents an important step towards the development of such generalist biomedical AI.\n' +
      '\n' +
      '#### Acknowledgments\n' +
      '\n' +
      'This project was an extensive collaboration between many teams at Google Research and Google DeepMind. We thank Andrew Sellergren, Yuan Liu, Michael Howell, Julie Wang, Sho Kannan, Christine Kingsley, Roy Lee, Naama Hammel, Jay Hartford, Preeti Singh, Kavita Kulkarni, Gavriel Goidel, Anil Palepu, Si Wai Man, Amy Wang, Sami Lachgar, Lauren Winer, Maggie Shiels, Annisha Um\'rani, John Guilyard, Shravya Shetty and Evan Rapoport for their valuable insights and feedback during our research. We are also grateful to Karen DeSalvo, Zoubin Ghahramani, James Manyika, and Jeff Dean for their support during the course of this project.\n' +
      '\n' +
      '#### Data Availability\n' +
      '\n' +
      'The benchmark used for training and evaluation in this study, MultiMedBench, comprises de-identified datasets that are all open source. We present an overview of datasets in Table 1.\n' +
      '\n' +
      '#### Code Availability\n' +
      '\n' +
      'We will not be able to open source the large language models (LLMs) used in this study. We have provided comprehensive details regarding our underlying methodology and build on previously detailed models [8, 10], so that similar approaches can be tried with other classes of LLMs.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M. & Thrun, S. Dermatologist-level classification of skin cancer with deep neural networks. nature542, pp. 115-118. Cited by: SS1.\n' +
      '* [2]V. Gulshan, L. Peng, M. Cormam, M. C. Stumpe, D. Wu, A. Narayanaswamy, S. Venugopalan, K. Widner, T. Madams, J. Cuadros, and et al. (2016) Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. Jama316, pp. 2402-2410. Cited by: SS1.\n' +
      '* [3]N. Tomasev, X. Cliotot, J. Rae, M. Zielinski, H. Askham, A. Saraiva, A. Mottram, C. Meyer, S. Ravuri, and et al. (2019) A clinically applicable approach to continuous prediction of future acute kidney injury. Nature572, pp. 116-119. Cited by: SS1.\n' +
      '* [4]S. M. McKinney, M. Sieniek, V. Godbole, J. Godwin, N. Antropova, H. Ashrafian, T. Back, M. Chesus, G. S. Corrado, A. Darzi, et al. (2020) International evaluation of an AI system for breast cancer screening. Nature577, pp. 89-94. Cited by: SS1.\n' +
      '* [5]R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosseltu, E. Brunskill, et al. (2021) On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258. Cited by: SS1.\n' +
      '* [6]T. Brown, B. Mann, M. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, et al. (2020) Language models are few-shot learners. Advances in neural information processing systems33 (1), pp. 1877-1901. Cited by: SS1.\n' +
      '* [7]S. Azizi, L. Culp, J. Freyberg, B. Mustafa, S. Baur, T. Kornblith, N. Chen, J. Tomasev, J. Mitrovic, P. Strachan, and et al. (2023) Robust and data-efficient generalization of self-supervised machine learning for diagnostic imaging. Nature Biomedical Engineering1-24. Cited by: SS1.\n' +
      '* [8]A. Chowdhary, S. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. (2022) PALM: scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Cited by: SS1.\n' +
      '* [9]K. Singhal, S. Yu, T. Mahdavi, S. S. Wei, J. Chung, N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl, et al. (2022) Large Language Models Encode Clinical Knowledge. arXiv preprint arXiv:2212.13138. Cited by: SS1.\n' +
      '* [10]D. Driess, F. Xia, M. S. Sajadi, M. Lynch, C. Lynch, E. Chowdhary, A. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Vuong, Y. Huang, Y. Chebotar, P. Duckworth, D. Levine, V. Vanhoucke, K. Hausman, M. Toussaint, K. Greff, A. Zeng, and I. K. Mordatch (2023) PyTorch: a framework for efficient image classification. arXiv:2303.03378. Cited by: SS1.\n' +
      '* [11]X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, B. Grycner, B. Mustafa, L. Beyer, et al. (2022) Split: a jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794. Cited by: SS1.\n' +
      '* [12]B. Liu, L. Zhang, L. Xu, L. Ma, Y. Yang, and X. Wu (2021) Slake: a semantically-labeled knowledge-enhanced dataset for medical visual question answering in 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI). Cited by: SS1.\n' +
      '* [13]J. Jeong, K. Tian, A. Li, S. Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, and S. Adithan (2023) A bidirectional image-text matching improves retrieval-based chest X-ray report generation. arXiv preprint arXiv:2303.17579. Cited by: SS1.\n' +
      '* [14]M. Dehghani, J. Dolonga, B. Mustafa, J. Padelewski, J. Heek, J. Gilmer, A. Steiner, R. Caron, I. Gerhos, I. Alabdulmohsini, et al. (2023) Scaling vision transformers to 22 billion parameters. arXiv preprint arXiv:2302.05442. Cited by: SS1.\n' +
      '* [15]Z. Boros, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, D. Roblek, O. Teboul, D. Grangier, M. Tagliasacchi, et al. (2023) AudioML: a language modeling approach to audio generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing. Cited by: SS1.\n' +
      '* [16]R. Caruana (1997) Multitask learning. Machine learning28, pp. 41-75. Cited by: SS1.\n' +
      '* [17]S. Thrun (1998) Learning to learn 181-209. Cited by: SS1.\n' +
      '* [18]G. E. Hinton, S. Osindero, and Y. Teh (2006) A fast learning algorithm for deep belief nets. Neural computation18, pp. 1527-1554. Cited by: SS1.\n' +
      '* [19]Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle (2006) Greedy layer-wise training of deep networks. Advances in neural information processing systems19. Cited by: SS1.\n' +
      '* [20]P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol (2008) Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pp. 1096-1103. Cited by: SS1.\n' +
      '* [21]Y. Bengio (2012) Deep learning of representations for unsupervised and transfer learning. In Proceedings of ICML workshop on unsupervised and transfer learning, pp. 17-36. Cited by: SS1.\n' +
      '* [22]J. Kaplan, S. McCandlish, T. Heighan, T. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei (2020) Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. Cited by: SS1.\n' +
      '* [23]J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, A. Ng, and A. Y. Multimodal deep learning in ICML. Cited by: SS1.\n' +
      '* [24]C. Schuhmann, R. Beaumont, C. Vencu, R. Gordon, M. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, et al. (2022) Laion-5b: an open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems35, pp. 25278-25292. Cited by: SS1.\n' +
      '* [25]A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, O. Zisserman, and J. Carreira (2021) Perceiver: general perception with iterative attention in international conference on machine learning. Cited by: SS1.\n' +
      '* [26]M. Tsimpoukelli, J. L. Menick, S. Cabi, S. S. S. Sismiys, O. Hill, and F. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems34, pp. 200-212. Cited by: SS1.\n' +
      '* [27]J. L. Alayrac, P. Donahue, A. Luc, I. Mitsch, K. Barr, Y. Lenc, A. Mensch, K. Reynolds, et al. (2022) Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems35, pp. 23716-23736. Cited by: SS1.\n' +
      '* [28]A. Agostinelli, T. I. Denko, Z. Boros, J. Engel, M. Verzetti, G. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi, et al. (2023) M. et al. (2023) MusicIm: generating music from text. arXiv preprint arXiv:2301.11325. Cited by: SS1.\n' +
      '* [29]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision in international conference on machine learning. Cited by: SS1.\n' +
      '* [30]J. J. Jeong, K. Tian, A. Li, S. Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, and S. Adithan (2023) A bidirectional image-text matching improves retrieval-based chest X-ray report generation. arXiv preprint arXiv:2303.17579. Cited by: SS1.\n' +
      '* [31]J. Jeong, K. Tian, A. Li, S. Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, and S. Adithan (2023) A bidirectional image-text matching improves retrieval-based chest X-ray report generation. arXiv preprint arXiv:2303.17579. Cited by: SS1.\n' +
      '* [32]J. Jeong, K. Tian, A. Li, S. Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, and S. Adithan (2023) A bidirectional image-text matching improves retrieval-based chest X-ray report generation. arXiv preprint arXiv:2303.17579. Cited by: SS1.\n' +
      '* [33]J. Jeong, K. Tian, A. Li, S. Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, and S. Adithan (2023) A bidirectional image-text matching improves retrieval-based chest X-ray report generation. arXiv preprint arXiv:2303.17579. Cited by: SS1.\n' +
      '* [34]J. Jeong, K. Tian, A. Li, S. Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, and S. Adithan (2023) A bidirectional image-text matching improves retrieval-based chest X-ray report generation. arXiv preprint arXiv:2303.17579. Cited by: SS1.\n' +
      '* [35]J. Jeong, K. Tian, A. Li, S. Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, and S. Adithan (2023) A bidirectional image-text matching improves retrieval-based chest X-ray report generation. arXiv preprint arXiv:2303.17579. Cited by: SS1.\n' +
      '* [36]J. Jeong, K. Tian, A. Li, S. Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, and S. Adithan (2023) A bidirectional image-text matching improves retrieval-based chest X-ray report generation. arXiv preprint arXiv:2303.17579. Cited by: SS1.\n' +
      '* [37]J. Jeong, K. Tian, A. Li, S. Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, and S. Adithan (2023) A bidirectional image-text matching improves retrieval-based chest X-ray report generation. arXiv preprint arXiv:2303.17579. Cited by: SS1.\n' +
      '* [38]J. Jeong, K. Tian, A. Li, S. Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, and S. Adithan (2023) A bidirectional image-text matching improves retrieval-based chest X-ray report generation. arXiv preprint arXiv:2303.17579. Cited by: SS1.\n' +
      '* [39]J. Jeong, K. Tian, A. Li, S. Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, and S. Adithan (2023) A bidirectional image-text matching improves retrieval-based chest X-ray report generation. arXiv preprint* [31] Thomee, B., Shamma, D. A., Friedland, G., Elizable, B., Ni, K., Poland, D., Borth, D. & Li, L.-J. YFCC100M: The new data in multimedia research. _Communications of the ACM_**59,** 64-73 (2016).\n' +
      '* [32] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K. & Fei-Fei, L. _Imagenet: A large-scale hierarchical image database_ in _2009 IEEE conference on computer vision and pattern recognition_ (2009), 248-255.\n' +
      '* [33] Rombach, R., Blattmann, A., Lorenz, D., Esser, P. & Ommer, B. _High-Resolution Image Synthesis with Latent Diffusion Models_ 2021. arXiv: 2112.10752 (cs.v).\n' +
      '* [34] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., _et al._ Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_**35,** 36479-36494 (2022).\n' +
      '* [35] Dhariwal, P. & Nichol, A. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_**34,** 8780-8794 (2021).\n' +
      '* [36] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., _et al._ Language models are unsupervised multitask learners. _OpenAI blog_**1,** 9 (2019).\n' +
      '* [37] Chen, T., Saxena, S., Li, L., Fleet, D. J. & Hinton, G. Pix2seq: A language modeling framework for object detection. _arXiv preprint arXiv:2109.10852_ (2021).\n' +
      '* [38] Collobert, R. & Weston, J. _A unified architecture for natural language processing: Deep neural networks with multitask learning_ in _Proceedings of the 25th international conference on Machine learning_ (2008), 160-167.\n' +
      '* [39] Ruder, S. An overview of multi-task learning in deep neural networks. _arXiv preprint arXiv:1706.05098_ (2017).\n' +
      '* [40] Lee, K.-H., Nachum, O., Yang, M. S., Lee, L., Freeman, D., Guadarrama, S., Fischer, I., Xu, W., Jang, E., Michalewski, H., _et al._ Multi-game decision transformers. _Advances in Neural Information Processing Systems_**35,** 27921-27936 (2022).\n' +
      '* [41] Lu, J., Clark, C., Zellers, R., Mottaghi, R. & Kembhavi, A. Unified-io: A unified model for vision, language, and multi-modal tasks. _arXiv preprint arXiv:2206.08916_ (2022).\n' +
      '* [42] Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., _et al._ A generalist agent. _arXiv preprint arXiv:2205.06175_ (2022).\n' +
      '* [43] Moor, M., Banerjee, O., Abad, Z. S. H., Krumholz, H. M., Leskovec, J., Topol, E. J. & Rajpurkar, P. Foundation models for generalist medical artificial intelligence. _Nature_**616,** 259-265 (2023).\n' +
      '* [44] Theodoris, C. V., Xiao, L., Chopra, A., Chaffin, M. D., Al Sayed, Z. R., Hill, M. C., Mantineo, H., Brydon, E. M., Zeng, Z., Liu, X. S., _et al._ Transfer learning enables predictions in network biology. _Nature_, 1-9 (2023).\n' +
      '* [45] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. & Polosukhin, I. Attention is all you need. _Advances in neural information processing systems_**30** (2017).\n' +
      '* [46] Zhang, K., Yu, J., Yan, Z., Liu, Y., Adhikarika, E., Fu, S., Chen, X., Chen, C., Zhou, Y., Li, X., _et al._ BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks. _arXiv preprint arXiv:2305.17100_ (2023).\n' +
      '* [47] Li, C., Wong, C., Zhang, S., Uusyama, N., Liu, H., Yang, J., Naumann, T., Poon, H. & Gao, J. LLAVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day. _arXiv preprint arXiv:2306.00890_ (2023).\n' +
      '* [48] Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., _et al._ Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_ (2023).\n' +
      '* [49] Wantlin, K., Wu, C., Huang, S.-C., Banerjee, O., Dadabby, F., Mehta, V. V., Han, R. W., Cao, F., Narayan, R. R., Colak, E., _et al._ BenchMD: A Benchmark for Modality-Agnostic Learning on Medical Images and Sensors. _arXiv preprint arXiv:2304.08486_ (2023).\n' +
      '* [50] Barham, P., Chowdhery, A., Dean, J., Ghemawat, S., Hand, S., Hurt, D., Isard, M., Lim, H., Pang, R., Roy, S., _et al._ Pathways: Asynchronous distributed dataflow for ML. _Proceedings of Machine Learning and Systems_**4,** 430-449 (2022).\n' +
      '* [51] Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., _et al._ Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _arXiv preprint arXiv:2206.04615_ (2022).\n' +
      '* [52] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., _et al._ An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_ (2020).\n' +
      '* [53] Sun, C., Shrivastava, A., Singh, S. & Gupta, A. _Revisiting unreasonable effectiveness of data in deep learning era_ in _Proceedings of the IEEE international conference on computer vision_ (2017), 843-852.\n' +
      '* [54] Zhai, X., Kolesnikov, A., Houlsby, N. & Beyer, L. _Scaling vision transformers_ in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), 12104-12113.\n' +
      '* [55] Marino, K., Rastegari, M., Farhadi, A. & Mottaghi, R. _Ok-qa: A visual question answering benchmark requiring external knowledge in Proceedings of the IEEE/cvf conference on computer vision and pattern recognition_ (2019), 3195-3204.\n' +
      '* [56] Goyal, Y., Khot, T., Summers-Stay, D., Batra, D. & Parikh, D. _Making the v in vqa matter: Elevating the role of image understanding in visual question answering_ in _Proceedings of the IEEE conference on computer vision and pattern recognition_ (2017), 6904-6913.\n' +
      '* [57] Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M. & Le, Q. V. Finetuned language models are zero-shot learners. _arXiv preprint arXiv:2109.01652_ (2021).\n' +
      '* [58] Shazeer, N. & Stern, M. _Adgator: Adaptive learning rates with sublinear memory cost in International Conference on Machine Learning_ (2018), 4596-4604.\n' +
      '* [59] Jaeger, S., Candemir, S., Antani, S., Wang, Y.-X. J., Lu, P.-X. & Thoma, G. Two public chest X-ray datasets for computer-aided screening of pulmonary diseases. _Quantitative imaging in medicine and surgery_**4,** 475 (2014).\n' +
      '* [60] Yu, F., Endo, M., Krishnan, R., Pan, I., Tsai, A., Reis, E. P., Fonseca, E. K. U. N., Lee, H. M. H., Abad, Z. S. H., Ng, A. Y., _et al._ Evaluating progress in automatic chest x-ray radiology report generation. _medRxiv_, 2022-08 (2022).\n' +
      '* [61] Singhal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn, E., Hou, L., Clark, K., Pfohl, S., Cole-Lewis, H., Neal, D., _et al._ Towards expert-level medical question answering with large language models. _arXiv preprint arXiv:2305.09617_ (2023).\n' +
      '* [62] Van Veen, D., Van Uden, C., Attias, M., Pareek, A., Bluthegen, C., Polacin, M., Chiu, W., Delbrouck, J.-B., Chaves, J. M. Z., Langlotz, C. P., _et al._ RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models. _arXiv preprint arXiv:2305.01146_ (2023).\n' +
      '* [63] Bazi, Y., Rahhal, M. M. A., Bashmal, L. & Zuair, M. Vision-Language Model for Visual Question Answering in Medical Imagery. _Bioengineering_**10,** 380 (2023).\n' +
      '* [* [64] Van Sosneek, T., Derakhshani, M. M., Najdenkoska, I., Snoek, C. G. & Worring, M. Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models. _arXiv preprint arXiv:2303.05977_ (2023).\n' +
      '* [65] Nicolson, A., Dowling, J. & Koopman, B. Improving chest X-Ray report generation by leveraging warm-starting. _arXiv preprint arXiv:2201.09405_ (2022).\n' +
      '* [66] Miura, Y., Zhang, Y., Tsai, E. B., Langloz, C. P. & Jurafsky, D. Improving factual completeness and consistency of image-to-text radiology report generation. _arXiv preprint arXiv:2010.10042_ (2020).\n' +
      '* [67] Bannur, S., Hyland, S., Liu, Q., Perez-Garcia, F., Ilse, M., Castro, D. C., Boecking, B., Sharma, H., Bouzid, K., Thieme, A., _et al. Learning to exploit temporal structure for biomedical vision-language processing_ in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), 15016-15027.\n' +
      '* [68] Tanida, T., Muller, P., Kaissi, G. & Rueckert, D. Interactive and Explainable Region-guided Radiology Report Generation in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), 7433-7442.\n' +
      '* [69] Rammuni Silva, R. S. & Fernando, P. Effective utilization of multiple convolutional neural networks for chest X-ray classification. _SN Computer Science_**3**, 492 (2022).\n' +
      '* [70] Panambur, A. B., Madhu, P. & Maier, A. _Effect of Random Histogram Equalization on Breast Calcification Analysis Using Deep Learning in Bluidereputing fur die Medizin 2022: Proceedings, German Workshop on Medical Image Computing, Heidelberg, June 26-28, 2022_ (2022), 173-178.\n' +
      '* [71] Poplin, R., Chang, P.-C., Alexander, D., Schwartz, S., Coltlhurst, T., Ku, A., Newburger, D., Dijamco, J., Nguyen, N., Afshar, P. T., Gross, S. S., Dorfman, L., McLean, C. Y. & DePristo, M. A. A universal SNP and small-indel variant caller using deep neural networks. _Nature Biotechnology_**36,** 983-987 (Sept. 2018).\n' +
      '* [72] Ye, S., Jang, J., Kim, D., Jo, Y. & Seo, M. Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization. _arXiv preprint arXiv:2210.03029_ (2022).\n' +
      '* [73] Endo, M., Krishnan, R., Krishna, V., Ng, A. Y. & Rajpurkar, P. _Retrieval-based chest x-ray report generation using a pre-trained contrastive language-image model_ in _Machine Learning for Health_ (2021), 209-219.\n' +
      '* [74] Oloko-Oba, M., Viriri, S., _et al._ Ensemble of EfficientNets for the Diagnosis of Tuberculosis. _Computational Intelligence and Neuroscience_**2021** (2021).\n' +
      '* [75] Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P. & Qiao, Y. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. _arXiv preprint arXiv:2303.16199_ (2023).\n' +
      '* [76] Schick, T., Dwivedi-Yu, J., Dess, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N. & Scialom, T. Toolformer: Language models can teach themselves to use tools. _arXiv preprint arXiv:2302.04761_ (2023).\n' +
      '* [77] Marshall, M. _The future of general practice in England_ 2022.\n' +
      '* [78] Blank, L., Baxter, S., Woods, H. B., Goyder, E., Lee, A., Payne, N. & Rimmer, M. Referral interventions from primary to specialist care: a systematic review of international evidence. _British Journal of General Practice_**64,** e765-e774 (2014).\n' +
      '* [79] Jin, D., Pan, E., Olufatelo, N., Weng, W.-H., Fang, H. & Szolovits, P. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. _Applied Sciences_**11,** 6421 (2021).\n' +
      '* [80] Pal, A., Umapathi, L. K. & Sankarasububu, M. _MedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering_ in _Conference on Health, Inference, and Learning_ (2022), 248-260.\n' +
      '* [81] Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W. & Lu, X. PubMedQA: A dataset for biomedical research question answering. _arXiv preprint arXiv:1909.06146_ (2019).\n' +
      '* [82] Johnson, A. E., Pollard, T. J., Shen, L., Lehman, L.-w. H., Feng, M., Ghassemi, M., Moody, B., Szolovits, P., Anthony Celi, L. & Mark, R. G. MIMIC-III, a freely accessible critical care database. _Scientific data_**3,** 1-9 (2016).\n' +
      '* [83] Delbruck, J.-B., Saab, K., Varma, M., Eyuboglu, S., Chambon, P., Dunnmon, J., Zambrano, J., Chaudhari, A. & Langlotz, C. _ViLM: Arctic a framework for research at the intersection of vision and language in medical AI_ in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations_ (2022), 23-34.\n' +
      '* [84] Delbruck, J.-B., Varma, M. & Langlotz, C. P. Toward expanding the scope of radiology report summarization to multiple anatomies and modalities. _arXiv preprint arXiv:2211.08584_ (2022).\n' +
      '* [85] Pacheco, A. G., Lima, G. R., Salomao, A. S., Krohling, B., Biral, I. P., de Angelo, G. G., Alves Jr, F. C., Esgario, J. G., Simora, A. C., Castro, P. B., _et al._ PAD-UFES-20: A skin lesion dataset composed of patient data and clinical images collected from smartphones. _Data in brief_**32,** 106221 (2020).\n' +
      '* [86] Cubuk, E. D., Zoph, B., Shlens, J. & Le, Q. V. _Randaquement: Practical automated data augmentation with a reduced search space in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops_ (2020), 702-703.\n' +
      '* [87] Nguyen, H. T., Nguyen, H. Q., Pham, H. H., Lam, K., Le, L. T., Dao, M. & Vu, V. VinDr-Mammo: A large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammography. _Scientific Data_**10**, 277 (2023).\n' +
      '* [88] Lee, R. S., Gimenez, F., Hoogi, A., Miyake, K. K., Gorovoy, M. & Rubin, D. L. A curated mammography data set for use in computer-aided detection and diagnosis research. _Scientific data_**4,** 1-9 (2017).\n' +
      '* [89] Olson, N. D., Wagner, J., McDaniel, J., Stephens, S. H., Westreich, S. T. & et al. PrecisionFDA Truth Challenge V2: Calling variants from short and long reads in difficult-to-map regions. _Cell Genomics_**2,** 100129 (May 2022).\n' +
      '* [90] DePristo, M. A., Banks, E., Poplin, R., Garimella, K. V., Maguire, J. R., Hartl, C. & et al. A framework for variation discovery and genotyping using next-generation DNA sequencing data. _Nature Genetics_**43,** 491-498 (Apr. 2011).\n' +
      '* [91] AlDubayan, S. H., Conway, J. R., Camp, S. Y., Witkowski, L., Kofman, E., Reardon, B., Han, S., Moore, N., Elmarakeby, H., Salari, K., Choudhry, H., Al-Rubaish, A. M., Al-Silai, A. A., Al-Ali, A. K., Taylor-Weiner, A. K. Allen, E. M. V. Detection of Pathogenic Variants With Germile Genetic Testing Using Deep Learning vs Standard Methods in Patients With Prostate Cancer and Melanoma. _JAMA_**324,** 1957 (Nov. 2020).\n' +
      '* [92] Liao, W.-W., Asri, M., Ebler, J., Doerr, D., Hauknes, M., Hickey, G. & et al. A draft human pangenome reference. _Nature_**617,** 312-324 (May 2023).\n' +
      '* [93] Thorvaldsdottir, H., Robinson, J. T. & Mesirov, J. P. Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration. _Briefings in Bioinformatics_**14,** 178-192 (Apr. 2012).\n' +
      '* [94] Zook, J. M., Catoe, D., McDaniel, J., Yang, L., Spies, N. & et al. Extensive sequencing of seven human genomes to characterize benchmark reference materials. _Scientific Data_**3** (June 2016).\n' +
      '* [95] Lau, J. J., Gayen, S., Ben Abacha, A. & Demner-Fushman, D. A dataset of clinically generated visual questions and answers about radiology images. _Scientific data_**5,** 1-10 (2018).\n' +
      '\n' +
      '* [96] He, X., Zhang, Y., Mou, L., Xing, E. & Xie, P. Pathvqa: 30000+ questions for medical visual question answering. _arXiv preprint arXiv:2003.10286_ (2020).\n' +
      '* [97] Johnson, A. E., Pollard, T. J., Berkowitz, S. J., Greenbaum, N. R., Lungren, M. P., Deng, C.-y., Mark, R. G. & Horng, S. MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports. _Scientific data_**6,** 317 (2019).\n' +
      '* [98] Irvin, J., Rajpurkar, P., Ko, M., Yu, Y., Ciurea-Ilcus, S., Chute, C., Marklund, H., Haghgoo, B., Ball, R., Shpanskaya, K., _et al.__Cherpert: A large chest radiograph dataset with uncertainty labels and expert comparison_ in _Proceedings of the AAAI conference on artificial intelligence_**33** (2019), 590-597.\n' +
      '* [99] Chen, Z., Song, Y., Chang, T.-H. & Wan, X. Generating radiology reports via memory-driven transformer. _arXiv preprint arXiv:2010.16056_ (2020).\n' +
      '* [100] Lin, C.-Y. _Rouge: A package for automatic evaluation of summaries_ in _Text summarization branches out_ (2004), 74-81.\n' +
      '* [101] Papineni, K., Roukos, S., Ward, T. & Zhu, W.-J. _Bleu: a method for automatic evaluation of machine translation_ in _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_ (2002), 311-318.\n' +
      '* [102] Jain, S., Agrawal, A., Saporta, A., Truong, S. Q., Duong, D. N., Bui, T., Chambon, P., Zhang, Y., Lungren, M. P., Ng, A. Y., _et al.__Radapgraph: Extracting clinical entities and relations from radiology reports. _arXiv preprint arXiv:2106.14463_ (2021).\n' +
      '* [103] Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. & Chen, W. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_ (2021).\n' +
      '* [104] Lehman, E. & Johnson, A. _Clinical-t5: Large language models built using mimic clinical text_ 2023.\n' +
      '* [105] Petrini, D. G., Shimizu, C., Roela, R. A., Valente, G. V., Folgueira, M. A. A. K. & Kim, H. Y. Breast cancer diagnosis in two-view mammography using end-to-end trained efficientnet-based convolutional network. _Ieee access_**10,** 77723-77731 (2022).\n' +
      '* [106] Dai, W., Liu, R., Wu, T., Wang, M., Yin, J. & Liu, J. Deeply Supervised Skin Lesions Diagnosis with Stage and Branch Attention. _arXiv preprint arXiv:2205.04326_ (2022).\n' +
      '* [107] De Lima, L. M. & Krohling, R. _A. Exploring Advances in Transformers and CNN for Skin Lesion Diagnosis on Small Datasets_ in _Intelligent Systems: 11th Brazilian Conference, BRACIS 2022, Campinas, Brazil, November 28-December 1, 2022, Proceedings, Part II_ (2022), 282-296.\n' +
      '* [108] Liu, Y., Wang, Z., Xu, D. & Zhou, L. _Q2attransformer: Improving medical vqa via an answer querying decoder_ in _International Conference on Information Processing in Medical Imaging_ (2023), 445-456.\n' +
      '* [109] Eslami, S., de Melo, G. & Meinel, C. Does clip benefit visual question answering in the medical domain as much as it does in the general domain? _arXiv preprint arXiv:2112.13906_ (2021).\n' +
      '* [110] Vedantam, R., Lawrence Zitnick, C. & Parikh, D. _Cider: Consensus-based image description evaluation_ in _Proceedings of the IEEE conference on computer vision and pattern recognition_ (2015), 4566-4575.\n' +
      '* [111] Smit, A., Jain, S., Rajpurkar, P., Pareek, A., Ng, A. Y. & Lungren, M. P. CheXbert: combining automatic labelers and expert annotations for accurate radiology report labeling using BERT. _arXiv preprint arXiv:2004.09167_ (2020).\n' +
      '* [112] Liu, G., Hsu, T.-M. H., McDermott, M., Boag, W., Weng, W.-H., Szolovits, P. & Ghassemi, M. _Clinically accurate chest x-ray report generation in Machine Learning for Healthcare Conference_ (2019), 249-269.\n' +
      '* [113] Xu, Y., Zhang, Q., Zhang, J. & Tao, D. Vitae: Vision transformer advanced by exploring intrinsic inductive bias. _Advances in neural information processing systems_**34,** 28522-28535 (2021).\n' +
      '* [114] Ramesh, V., Chi, N. A. & Rajpurkar, P. _Improving radiology report generation systems by removing hallucinated references to non-existent priors_ in _Machine Learning for Health_ (2022), 456-473.\n' +
      '\n' +
      '## Appendix\n' +
      '\n' +
      'In the following sections, we report additional experiments and detailed analysis to further illustrate the performance of our proposed generalist model, Med-PaLM M.\n' +
      '\n' +
      'We provide details on:\n' +
      '\n' +
      '* Datasets and tasks in MultiMedBench\n' +
      '* Med-PaLM M training procedure\n' +
      '* Interpretations of Med-PaLM M performance by task type:\n' +
      '* Performance analysis on language-only medical question answering\n' +
      '* Performance analysis on radiology report summarization\n' +
      '* Performance analysis on medical image classification tasks\n' +
      '* Performance analysis on medical visual question answering\n' +
      '* Performance analysis on chest X-ray report generation\n' +
      '* Human evaluation of model-generated chest X-ray reports\n' +
      '* Examples from MultiMedBench tasks\n' +
      '\n' +
      '### MultiMedBench\n' +
      '\n' +
      'In this section, we offer a comprehensive overview of _MultiMedBench_, including a detailed description of the datasets, data preprocessing, and task setups. Figure A.1 summarizes MultiMedBench over its various biomedical tasks.\n' +
      '\n' +
      '#### a.1.1 Language-only datasets\n' +
      '\n' +
      '**MultiMedQA** We used three of the multiple-choice medical question-answering datasets from MultiMedQA [9]: the MedQA [79], MedMCQA [80], and PubMedQA [81] datasets for training and evaluation of Med-PaLM M. These question answering tasks are language-only and do not require the interpretation of additional modalities. The training set consists of 10,178 questions from MedQA and 182,822 questions from MedMCQA. The test set comprises 1,273 questions from MedQA, 4,183 questions from MedMCQA, and 500 questions from PubMedQA. Note that PubMedQA was not included in the training data mixture and only used for evaluation.\n' +
      '\n' +
      '**MIMIC-III** is a large publicly-available medical database that contains medical records of patients admitted to intensive care units [82]. It contains 79,790 radiology reports across two imaging modalities (CT and MRI) and seven anatomic regions (head, abdomen, chest, head, neck, sinus, spine, pelvis). A total of 78,875 reports were chosen based on criteria such as the length of the report. We used the radiology report summarization dataset from [62], which comprises six most common modality/anatomy pairs for training and evaluation: CT head, CT abdomen, CT chest, MRI head, CT spine, and CT neck. To evaluate out-of-distribution (OOD) performance we used five less common modality/anatomy pairs: MRI spine, CT sinus, MRI abdomen, MRI pelvis, and MRI neck. This resulted in a total of 58,405 reports for training, 7,413 reports for validation, and 13,057 reports for testing. Note that chest X-ray reports are excluded from this dataset to avoid data contamination with the MIMIC-CXR dataset for the report generation task. For each report, we used the same preprocessing functions as in [83, 84] to extract the findings and impression sections. Specifically, we filtered out the reports whose findings section are longer than 600 tokens. We performed a report summarization task by predicting the impression section given the findings section as input, which is another language-only task that does not require multi-modal input.\n' +
      '\n' +
      '#### a.1.2 Multimodal datasets\n' +
      '\n' +
      '**PAD-UFES-20** consists of 2,298 clinical images of skin lesions collected from different smartphone devices with varying resolutions, sizes, and lighting conditions [85]. The data was collected through the Dermatological and Surgical Assistance Program at the Federal University of Espirito Santo (UFES-Brazil), a nonprofitprogram that provides free skin lesion treatment. The dataset contains six different types of skin lesions including: Basal Cell Carcinoma (BCC), Malignant Melanoma (MEL), Squamous Cell Carcinoma (SCC), Actinic Keratosis (ACK), Melanocytic Nevus (NEV), and Seborrheic Keratosis (SEK). Each image is associated with up to 21 patient clinical features such as patient demographics, family cancer history lesion location, lesion size. We set up a 6-class classification task in a generative framework through a language decoder using skin lesion images and the associated clinical textual features as the multimodal input. Specifically, we selected 14 clinical attributes in the metadata for each lesion including: _age_, _gender_, _smoke_, _drink_, _skin cancer history_, _cancer history_, _region_, _fitspatrick_, _horizontal and vertical diameters_, _itch_, _grew_, _bleed_, and _elevation_. The class ratio is approximately 16:1:4:14:5:4 over three skin cancers (BCC, MEL, and SCC) and three skin disease (ACK, NEV, and SEK). Since there are no published official train/test splits, we randomly split the dataset into a training set (80%) and a test test (20%) using a stratified sampling to the preserve original class ratio. We applied a series of image augmentation operations using RandAugment [86] to the training set including: _autoContrast_, _equalize_, _invert_, _rotate_, _posterior_, _solarze_, _color_, and _contrast_.\n' +
      '\n' +
      '**VinDr-Mammo** is a full-field digital mammography dataset which consists of 5000 breast X-ray imaging studies and a total of 20,000 gray-scale images with extensive breast-level assessment and lesion-level annotations, collected from two hospitals in in Hanoi, Vietnam [87]. Each study contains four images where the left and right breasts are imaged with mediolateral-oblique (MLO) and cranio-caudal (CC) views. Each image has breast-level assessment following the Breast Imaging Reporting and Data System (BI-RADS). BI-RADS assessment ranges from 1 (negative) to 5 (highly suggestive of malignancy). In addition to the BI-RADS score, the breast density level is also provided as well as regional abnormality finding annotations. We performed a breast-level 5-class BI-RADS classification task similar to the setup in [49], except that\n' +
      '\n' +
      'Figure A.1: **MultiMedBench overview.** MultiMedBench is a benchmark that covers 14 different biomedical tasks, including question answering, visual question answering, image classification, radiology report generation and summarization, and genomic variant calling. MultiMedBench comprises more than 1 million data samples from a diverse range of medical images, radiology reports, medical question answers, and visual question answering pairs.\n' +
      '\n' +
      'the laterality and view position of the image was provided as additional contextual features. We used the official train/test splits where the train split contains 16,000 samples with a class ratio of 60:21:4:3:1 across BI-RADS 1-5, respectively and the test split contains 4,000 samples with the same class ratio. We applied the following transformations to the images in the training set: _contrast_, _equalize_, _rotate_, _shearX_, _shearY_, _translateX_, and _translateY_. To mitigate the class imbalance in the training data, we upsampled for each minority class (BI-RADS 2-5) by a factor of 3.\n' +
      '\n' +
      '**Cbis-DDSM** is the Curated Breast Imaging Subset of Digital Database for Screening Mammography [88]. This dataset contains 2,620 scanned film mammography studies. Unlike VinDr-Mammo, CBIS-DDSM does not have breast-level BI-RADS assessment. Annotations are provided at the lesion level including BI-RADS, subtlety level, and pathology type. There are two types of lesions: mass and calcification. Both of them are annotated with three possible pathology labels: benign, benign without callback, and malignant. We performed a 3-class abnormality (patch-level) pathology classification task on this dataset for mass and calcification abnormalities separately. Abnormality image patch is cropped by the bounding box of the region-of-interest (ROI) from the full mammogram and used as the model input along with its view position (CC or MLO) information. We used the official train/test splits for both abnormality types. For mass cases, the training and test sets contain 1,318 and 378 images (class ratio: 6:1:6), respectively. For calcification cases, the total number of images in the training and test sets are 1,544 and 326 (class ratio: 1:1:1), respectively. For both cases, we applied the same image augmentation as in VinDr-Mammo to the training set.\n' +
      '\n' +
      '**PrecisionFDA Truth Challenge V2** was developed for benchmarking the state-of-the-art of variant calling in challenging genomics regions [89]. Genomic variant calling is a task aiming at identifying genetic variants from sequencing data [90], which can identify disease-causing mutations [91]. For variant calling, sequencing data is mapped to the coordinates of a reference genome [92]. The mappings can be represented as an image-like format that computational methods such as DeepVariant [71] use to call variants, or in a human-friendly image format which experts use to inspect and quality control variants of interest [93]. For this task, we used an extensively characterized groundtruth set from the National Institute of Standards and Technology (NIST) [94] for the HG002 sample. We generated examples from sequencing from the PrecisionFDA Truth Challenge V2. For training, we use 4% of the examples from the whole genome (except for chromosome 20, 21, and 22). For evaluation, we used chromosome20, bases 3000001-9444417. This generated 197,038 candidate variants for training and 13,030 candidate variants for evaluation. For each example, the model predicts three possible genotypes, corresponding to how many copies (0, 1, or 2) of the given alternate allele are present. The training set consists of 45,011, 93,246, and 58,781 samples for classes 0, 1, 2, respectively. The evaluation set contains 3,016, 6,169, and 3,845 for classes 0, 1, 2, respectively.\n' +
      '\n' +
      'We used DeepVariant v1.3.0\'s [71] example generation method to create image-like examples suitable for machine classification. Specifically, input examples to DeepVariant v1.3.0 have a shape of (100, 221, 6) corresponding to (height, width, channels). Channels are shown in grey-scale below in the following order:\n' +
      '\n' +
      '1. Read base: different intensities represent A, C, G, and T.\n' +
      '2. Base quality: set by the sequencing machine. White is higher quality.\n' +
      '3. Mapping quality: set by the aligner. White is higher quality.\n' +
      '4. Strand of alignment: Black is forward; white is reverse.\n' +
      '5. Read supports variant: White means the read supports the given alternate allele, grey means it does not.\n' +
      '6. Base differs from ref: White means the base is different from the reference, dark grey means the base matches the reference.\n' +
      '\n' +
      'To reshape the input example to be compatible with the Med-PaLM M input shape of (224, 224, 3), we stacked up channels 1, 2, 3 with channels 4, 5, 6 such that the original tensor of shape (100, 221, 6) became an RGB image of shape (200, 221, 3). We then padded the image on the width and height dimensions to give it a final shape of (224, 224, 3).\n' +
      '\n' +
      '**VQA-RAD** is a radiology visual question answering (VQA) dataset which consists of 315 radiology images and 3,515 question-answer pairs created and validated by clinicians [95]. The radiology images are selected from three imaging modalities (CT, MRI, and X-rays) and three anatomical regions (head, abdominal, chest).\n' +
      '\n' +
      'The types of question fall into 11 categories including modality, plane, organ system, abnormality, size, plane, positional reasoning, color, counting, attribute and other. 58% of the question-answer (QA) pairs are closed-ended (yes/no or limited choices) and the rest 42% are open-ended (short answer). We adopted the official train/test splits, where the training set contains 1,797 QA pairs (only free-form and paraphrased questions were included) and the test set contains 451 QA pairs (not filtered).\n' +
      '\n' +
      '**Path-VQA**: is a pathology VQA dataset, containing a total of 4,998 pathology images with 32,799 question-answer pairs [96]. Pathology images are extracted from medical textbooks and online digital libraries. Each image is associated with multiple QA pairs pertaining to different aspects of the pathology including color, location, appearance, shape, etc. Open-ended questions account for 50.2% of all questions, which are categorized into 7 categories: what, where, when, whose, how, and how much/how many, accounting for 50.2% of all questions. The rest are close-ended questions with simple "yes/no" answer. We adopted the official data partitioning where the training, validation, and test sets contain 19,755, 6,279, and 6,761 QA pairs, respectively.\n' +
      '\n' +
      '**Slake-VQA** is a semantically annotated and knowledge-enhanced bilingual (English and Chinese) VQA dataset on radiology images [12]. It contains 642 annotated images with 14,028 question-answer pairs covering 12 diseases, 39 organ systems and 3 imaging modalities (CT, MRI, and chest X-rays). Questions are either open-ended (free-form) or closed-ended (balanced yes/no) related to various aspects of the image content including plane, quality, position, organ, abnormality, size, color, shape, knowledge graph, etc. The training, validation, and test sets contain 9,849, 2,109, and 2,070 samples, respectively.\n' +
      '\n' +
      '**MIMIC-CXR**: is a large dataset of chest radiographs with free-text radiology reports [97]. A total of 377,110 images are available in the dataset from 227,835 image studies collected for 65,379 patients. Each patient may have multiple studies and each study may contain one or more images associated with the same free-text report. Images in MIMIC-CXR are collected from multiple view positions: e.g., anterior-posterior (AP), posterior-anterior, and lateral (LA). Protected health information (PHI) in radiology reports and images is removed, which results in missing information in some sentences of the reports. Since this dataset contains sequential imaging studies of an individual patient, a large number of reports refer to information in prior studies of the same patient. Each report is annotated with structured labels of 14 common radiological observations using CheXpert labeler [98]. We performed two tasks using this dataset: chest X-ray report generation and binary classification of clinically-relevant pathology observations. We preprocessed the radiology reports by extracting the indication, findings, and impression sections, removing redundant white-spaces in the reports, following previous work [99]. We used the official train/validation/test splits. We discarded images without reports and reports where the findings section can not be extracted across train and test. We also filtered out the reports where the length of findings section exceeds 800 characters. However, unlike most previous work using focusing only on the frontal view, we treated images of different orientation that are associated with the same report as independent samples (retaining the patient-level train/test splits to avoid contamination of the test data). The goal is to improve the image understanding capability of the model to process images of different view positions. In a separate evaluation, we also studied a subset of samples where reports are accompanied by both a front and lateral view (two-view report generation).\n' +
      '\n' +
      'For the report generation task, we combined the chest X-ray image with the contextual information from the indication section (reason for the study) to predict the findings section of the target report. The total number of samples in the training, validation, and test sets are: 353,542, 2,866, and 4,834, respectively.\n' +
      '\n' +
      'For the binary classification task, we grouped negative and uncertain labels as the negative class for 11 pathological conditions: no finding, atelectasis, cardiomegaly, consolidation, edema, pleural effusion, lung opacity, enlarged cardiomediastinum, fracture, pneumonia, and support devices. Atelectasis, cardiomegaly, consolidation, edema, and pleural effusion are 5 major conditions given their clinical relevance and prevalence. The "No finding" label captures the cases without any pathology and therefore this classification task simply helps the model to distinguish normal cases from cases with any type of abnormality. Due to class imbalance, during training we upsampled the positive class by a factor of 2 for the following conditions: consolidation, enlarged cardiomediastinum, fracture, and pneumonia. These binary classification tasks are auxiliary to the report generation task when they are trained simultaneously since they help the model to distinguish among different types of clinical observations in the chest X-ray images.\n' +
      '\n' +
      '### Med-PaLM M Training Details\n' +
      '\n' +
      '#### a.2.1 Training data mixture\n' +
      '\n' +
      'Figure A.2 and Table A.1 show the mixture ratio and few-shot task setup of the training data mixture. The majority of the data distribution is medical vision-language tasks, with less than 15% consisting of language-only tasks. While the majority of vision-language tasks were trained with a text-only 1-shot setup (without the corresponding image), the CBIS-DDSM classification and genomic variant calling tasks were trained with a 0-shot setup.\n' +
      '\n' +
      '#### a.2.2 Training hyperparameters\n' +
      '\n' +
      'PaLM-E projects the multimodal inputs into the same language embedding space as latent vectors such that continuous observations (e.g., images, time series) can be processed the same way by a pre-trained LLM as the language tokens, and thereby generates textual completions autoregressively given a multimodal prompt. In our experiments, the ViT maps the visual input to a fixed number of 256 tokens which are further processed by the LLM along with the additional text/multimodal tokens [10]. Med-PaLM M was finetuned on the pretrained PaLM-E checkpoints. Table A.2 shows the training hyperparameters for Med-PaLM M 12B, 84B, and 562B, respectively.\n' +
      '\n' +
      '### Detailed Med-PaLM M Performance\n' +
      '\n' +
      'Performance on text-only medical question answeringWe report the few-shot performance of Med-PaLM M on MedQA, MedMCQA, and PubMedQA in Table A.3. SOTA results were chosen from Med-PaLM 2 with ensemble refinement prompting and PaLM 540B few-shot results reported in [9, 61]. Med-PaLM M outperformed the baseline PaLM model (from which it inherits) by a large margin on all three datasets, despite falling behind the Med-PaLM 2 best results obtained with ensemble refinement. Scaling up the language model from 8B to 540B significantly improves the accuracy on the multiple-choice medical question answering\n' +
      '\n' +
      'Figure A.2: \\(|\\) **Med-PaLM M data mixture overview.** Summary of the mixture ratio in the Med-PaLM M training data mixture across MultiMedBench datasets as detailed in Table A.1.\n' +
      '\n' +
      'tasks, where strong capabilities to comprehend, recall, and reason about medical knowledge are important. These results can be partly explained by the improved base language model used for Med-PaLM 2.\n' +
      '\n' +
      '**Performance on radiology report summarization** We report commonly used metrics such as ROUGE-L [100], BLEU [101], and F1-RadGraph [102] scores on the radiology report summarization task as in Van Veen _et al._[62] in Table 4. Med-PaLM M (562B) yielded the best overall performance compared to the smaller model variants, consistent with our observations on medical question answering tasks. Med-PaLM M performed worse than the SOTA results which were obtained with a parameter-efficient finetuning method (low-rank adaptation, LoRA [103]) on a 738M-parameter clinical-T5 model [104]. However, as noted in [62], one caveat of clinical-T5 is that it is unclear if Lehman & Johnson [104] pretrained the model on the test set of MIMIC-III which led to potential data leakage. Notably, Med-PaLM M compared favorably to the results in Van Veen _et al._[62] based on the T5 model which was not pretrained on clinical text, similar to the PaLM model.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline Dataset & Med-PaLM 2 & PaLM & Med-PaLM M (12B) & Med-PaLM M (84B) & Med-PaLM M (562B) \\\\ \\hline MedQA (USMLE) & **86.50\\%** & 58.90\\% & 29.22\\% & 46.11\\% & 69.68\\% \\\\ \\hline MedMCQA & **72.30\\%** & 54.50\\% & 32.20\\% & 47.60\\% & 62.59\\% \\\\ \\hline PubMedQA & **81.80\\%** & 55.00\\% & 48.60\\% & 71.40\\% & 80.00\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: **Language-only medical question answering accuracy on MultiMedQA.** Med-PaLM 2 results with ensemble refinement [61] and PaLM few-shot results [9] are presented for comparison. Few-shot Med-PaLM M outperforms the corresponding PaLM baseline by a large margin, despite falling short of the state-of-the-art Med-PaLM 2.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline Task & Modality & Dataset & Mixture ratio & Few-shot setup \\\\ \\hline \\multirow{2}{*}{Question Answering} & \\multirow{2}{*}{Text} & MedQA & 3.13\\% & 2-shot \\\\  & & MedMCQA & 6.25\\% & 2-shot \\\\ \\hline \\multirow{2}{*}{Report Summarization} & \\multirow{2}{*}{Radiology} & MIMIC-III & 3.13\\% & 0-shot \\\\ \\hline \\multirow{2}{*}{Visual Question Answering} & \\multirow{2}{*}{Radiology} & VQA-RAD & 0.15\\% & text-only 1-shot \\\\  & & Slake-VQA & 2.64\\% & text-only 1-shot \\\\  & & Pathology & Path-VQA & 1.90\\% & text-only 1-shot \\\\ \\hline \\multirow{2}{*}{Report Generation} & \\multirow{2}{*}{Chest X-ray} & MIMIC-CXR & 59.90\\% & text-only 1-shot \\\\ \\hline \\multirow{4}{*}{Medical Image Classification} & \\multirow{2}{*}{Dermatology} & PAD-UFES-20 & 6.25\\% & text-only 1-shot \\\\  & & VinDr-Mammo & 1.56\\% & text-only 1-shot \\\\ \\cline{1-1}  & & CBIS-DDSM & 1.56\\% & 0-shot \\\\ \\cline{1-1}  & & MINIC-CXR & 11.98\\% & text-only 1-shot \\\\ \\cline{1-1}  & & PrecisionFDA & 1.56\\% & 0-shot \\\\ \\cline{1-1}  & & Truth Challenge V2 [89] & 1.56\\% & 0-shot \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **Med-PaLM M data mixture.** Summary of the task types, modalities, mixture ratios, and few-shot setups in Med-PaLM M training data mixture.\n' +
      '\n' +
      '**Performance on medical image classification tasks** Table A.5 shows the performance of Med-PaLM M on a set of classification tasks across multiple modalities including dermatology, radiology, and genomics. Since these tasks all have imbalanced class distributions, we reported macro-AUC (unweighted mean of all the per-class AUC scores) and macro-F1 scores (unweighted mean of all the per-class F1 scores) as the classification metrics except for the genomic variant calling task where the F1 scores for single nucleotide polymorphisms (SNPs) and short insertions and deletions (indels) in the context of variant discovery were used instead.\n' +
      '\n' +
      'On VinDr-Mammo, all size variants of Med-PaLM M exceeded the prior SOTA using a smaller ViT (9.7M) on macro-AUC [49]. On CBIS-DDSM, our model achieved the best macro-F1 of 51.12% and 67.86% on the mass and calcification classification, respectively, behind the SOTA F1 of 70.71% reported on the calcification classification [70]. Note that most previous works on CBIS-DDSM focused on a two-class patch-level classification (benign versus malignant) problem in contrast to our 3-class setup as discussed in [105]. On Pad-UFES-20, since no official train/test splits are available, our results are not directly comparable with prior studies. Med-PaLM M 84B achieved a macro-AUC of 97.27%, on par with previous reported results (94% - 98%) obtained using CNN and ViT variants [106, 107]. On MIMIC-CXR, we reported the macro-average of F1 scores across the binary classification of 5 major conditions: atelectasis, cardiomegaly, consolidation, edema, and pleural effusion. Med-PaLM M (562B) achieved a macro-AUC of 79.09%, slightly lower than the SOTA result of 81.27% obtained from ParallelXNet [69], which used a parallelization of various CNN Architectures. On the variant calling task, DeepVariant model [71] outperformed Med-PaLM M on both Indel-F1 and SNP-F1 scores. The SOTA DeepVariant model was trained with 2,633-fold more training examples. Training with the same examples resulted in a narrower advantage for DeepVariant for SNP (Med-PaLM M 99.35% versus DeepVariant 99.63%) and Indel (Med-PaLM M 97.04% versus DeepVariant 98.55%. Notably, Med-PaLM M outperformed the accuracy of the widely used GATK4 method [90] for SNP calling (Med-PaLM M 99.35% versus GATK4 99.29%) but not Indel calling (Med-PaLM M 97.04% versus GATK4 99.32%).\n' +
      '\n' +
      'Taken together, Med-PaLM M achieved competitive results on a variety of classification tasks using a single model compared to highly specialized SOTA models. It is worth noting that we did not perform any fine-grained task-specific customization and hyperparameter tuning beyond data augmentation and class balancing. It is expected that scaling up the language model does not significantly benefit the classification tasks where the vision encoder is likely the bottleneck for the model performance. There is no overall evidence to suggest that larger vision model outperforms the small one across all our experiments, suggesting that more domain-specific pretraining may be more important for improving vision encoder performance. It is also likely that relatively small-scale datasets we explored here are not sufficient to establish such a robust scaling relationship between the model size and task performance, as results were generally close to each other across model scales.\n' +
      '\n' +
      '**Performance on medical visual question answering** Since we formulated both close-end and open-end QA pairs in three VQA datasets as an open-ended language decoding task conditioned on visual input, we used BLEU-1 and token-level F1 scores to assess the performance of Med-PaLM M. This is in contrast with many prior works which used a string-level accuracy evaluation metric as they often considered VQA as a classification task on a set of pre-defined fixed-number answer candidates [108, 109]. This accuracy metric has the weakness of failing to capture "near misses" of groundtruth answers, particularly in our open-ended generative setup. We also noted that only human validation by experts can provide additional insights on the quality of model answers beyond token-level or string-level matching metrics. As shown in Table A.6, Med-PaLM M surpassed previous SOTA using a similar generative approach across all three datasets and metrics [63, 64]. In particular, model performance increased with scaling up the language model on VQA-RAD\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline Dataset & Metric & SOTA & Med-PaLM M (12B) & Med-PaLM M (84B) & Med-PaLM M (562B) \\\\ \\hline \\multirow{3}{*}{MIMIC-III} & ROUGE-L & **38.70**\\% & 29.45\\% & 31.47\\% & 32.03\\% \\\\  & BLEU & **16.20**\\% & 12.14\\% & 15.36\\% & 15.21\\% \\\\ \\cline{1-1}  & F1-RadGraph & **40.80**\\% & 31.43\\% & 33.96\\% & 34.71\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table A.4: \\(|\\)**Med-PaLM M performance on MIMIC-III radiology report summarization.**and Path-VQA. On Slake-VQA, the best performance was achieved with the medium size model variant. These results suggest that scaling up language models is beneficial for visual-language tasks where language reasoning is conditioned on visual understanding.\n' +
      '\n' +
      'Performance on chest X-ray report generationTo measure the quality of generated chest X-ray reports using automatic metrics, we computed common natural language generation metrics such as BLEU-1, BLEU-4, ROUGE-L, CIDEr-D [110], in addition to the clinical efficacy (CE) metrics and F1-RadGraph which were designed to capture the factuality and diagnostic accuracy in the generated reports. Specifically, we used CheXbert [111], an automatic radiology report labeller based on a BERT model improved with expert annotation, to extract the 14 CheXpert pathological observations from a given report. For each observation, the predicted label was compared against the groundtruth label to compute CE metrics. F1-RadGraph generalizes over CheXpert labeller to more observation categories by measuring the overlapping clinical entities and relations between a generated report and the reference report [60]. In line with previous studies [65, 66, 67, 68, 14, 69, 112], we reported the macro-F1 and micro-F1 scores averaged over 5 major observations and all 14 observations for CE metrics, respectively. As shown in Table 7, Med-PaLM M achieved a new SOTA on all CE metrics and F1-RadGraph, with a substantial increase of about 9 points on macro-F1-14 and micro-F1-14 averaged across all clinical relevant observations over previous best SOTA results in [65, 14]. The macro-average F1 resulted in a lower score than the micro-average F1 over 14 observation categories because of the worse model performance on some categories with very low representation in the training data. Notably, improvements on F1 scores were more prominent across all 14 categories than over the 5 major categories for Med-PaLM M. This is likely due to the benefit of jointly training with the classification tasks on those\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline Dataset & Metric & SOTA & Med-PaLM M (12B) & Med-PaLM M (84B) & Med-PaLM M (562B) \\\\ \\hline \\multirow{2}{*}{VQA-RAD} & BLEU-1 & 71.03\\% & 64.02\\% & 69.38\\% & **71.27\\%** \\\\  & F1 & N/A & 50.66\\% & 59.90\\% & **62.06\\%** \\\\ \\hline \\multirow{2}{*}{Path-VQA} & BLEU-1 & 70.30\\% & 68.97\\% & 70.16\\% & **72.27\\%** \\\\  & F1 & 58.40\\% & 57.24\\% & 59.51\\% & **62.69\\%** \\\\ \\hline \\multirow{2}{*}{Slake-VQA} & BLEU-1 & 78.60\\% & 90.77\\% & **92.7\\%** & 91.64\\% \\\\  & F1 & 78.10\\% & 86.22\\% & **89.28\\%** & 87.50\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: \\(|\\)**Med-PaLM M performance on medical image classification. We report macro-averaged AUC and F1 for all tasks. For MIMIC-CXR, metrics are averaged over 5 major pathological conditions.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline Dataset & \\# Classes & Metric & SOTA & Med-PaLM M (12B) & Med-PaLM M (84B) & Med-PaLM M (562B) \\\\ \\hline MIMIC-CXR (5 conditions) & 2-class & Macro-AUC & **81.27\\%** & 76.67\\% & 78.35\\% & 79.09\\% \\\\  & & Macro-F1 & N/A & 38.33\\% & 36.83\\% & **41.57\\%** \\\\ \\hline PAD-UFES-20 & 6-class & Macro-AUC & N/A & 95.57\\% & **97.27\\%** & 96.08\\% \\\\  & & Macro-F1 & N/A & 78.42\\% & **84.32\\%** & 77.03\\% \\\\ \\hline \\multirow{2}{*}{Variant Calling} & \\multirow{2}{*}{3-class} & Indel-F1 & **99.40\\%** & 96.42\\% & 97.04\\% & 95.46\\% \\\\  & & SNP-F1 & **99.70\\%** & 99.35\\% & 99.32\\% & 99.16\\% \\\\ \\hline \\multirow{2}{*}{VinDr-Mammo} & \\multirow{2}{*}{5-class} & Macro-AUC & 64.50\\% & 66.29\\% & **71.76\\%** & 71.42\\% \\\\  & & Macro-F1 & N/A & 29.81\\% & **35.7\\%** & 33.90\\% \\\\ \\hline CBIS-DDSM (mass) & 3-class & Macro-AUC & N/A & 70.11\\% & 73.09\\% & **73.31\\%** \\\\  & & Macro-F1 & N/A & 47.23\\% & 49.98\\% & **51.12\\%** \\\\ \\hline CBIS-DDSM (calcification) & 3-class & Macro-AUC & N/A & 81.40\\% & **82.22\\%** & 80.90\\% \\\\  & & Macro-F1 & **70.71\\%** & 67.86\\% & 63.81\\% & 63.03\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: \\(|\\)**Med-PaLM M performance on medical image classification. We report macro-averaged AUC and F1 for all tasks. For MIMIC-CXR, metrics are averaged over 5 major pathological conditions.**minority conditions. We consider such positive task transfer as one of the main advantages of a generalist multi-task model over a specialized single-task model. On text overlap based natural language generation metrics, Med-PaLM M did not outperform existing SOTA results. However, the pitfalls of these automatic metrics have been raised by many studies, particularly in that they fail to capture the factual correctness and do not align well with radiologist judgements [60, 66, 112, 68].\n' +
      '\n' +
      'Interestingly, our largest model Med-PaLM M (562B) did not achieve the best performance, falling slightly short of Med-PaLM M (84B). Furthermore, the gap in performance across three model sizes is relatively small across all types of metrics. The diminishing return of increasing the size of the language model is likely because the output space for chest X-ray report generation is fairly confined to a set of template sentences and limited number of conditions. It is also possible that the task performance is primarily limited by the vision encoder, particularly in how well it is adapted for this domain. As noted by Xu _et al._[113], ViT lacks inductive bias for modeling local visual features which are often crucial for interpreting medical images. To overcome this limitation, large-scale medical training data may be required to enable benefit from size scaling. Additionally, the input image size \\(224\\times 224\\times 3\\) we used cause loss in resolution, which is a tradeoff we made to shorten the length of embedded image tokens to fit within the context length limit of the language model.\n' +
      '\n' +
      '### Details on Human Evaluation\n' +
      '\n' +
      'Figures A.3 and A.4 depict the task interfaces used for side-by-side and independent radiologist evaluations, including the task input and annotation prompts presented to radiologist raters. For ease of a detailed inspection (e.g., identification of subtle structures), the built-in medical image viewer provided tools for raters to adjust the chest X-ray image, including zoom, gamma, and blend controls.\n' +
      '\n' +
      'It is worth noting that a non-trivial number of reports in the MIMIC-CXR dataset contain references to prior studies (e.g., "Compared to the prior radiograph [...]") or references to multiple views (e.g., "Ap and lateral views of the chest are compared."). By contrast, the input to our model is a single image and indication from a single study. As a result, these artifacts in the training corpus render the model prone to hallucination of references to non-existent prior imaging studies or non-existent X-ray views. Our human evaluation task interface accounted for the presence of these artifacts by providing the option to categorize erroneous passages as "Refers to view that is not present" or "Refers to study that is not present". Future work may leverage the CXR-PRO dataset [114], a cleaned version of MIMIC-CXR with all prior references removed, to mitigate this issue during model development.\n' +
      '\n' +
      'For the purpose of analysis, we distinguished between clinical errors (i.e., "Finding I do not agree is present", "Incorrect location of finding") and non-clinical errors (i.e., "Refers to view that is not present" or "Refers to study that is not present"). Table A.8 summarizes the rates of omissions and errors identified by clinician raters in radiology reports generated by different Med-PaLM M models. Here, we report the rate of total errors, including all clinical and non-clinical error types. On average, the best performing Med-PaLM M\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline Metric & SOTA & Med-PaLM M (12B) & Med-PaLM M (84B) & Med-PaLM M (562B) \\\\ \\hline Micro-F1-14 & 44.20\\% & 51.41\\% & **53.56\\%** & 51.60\\% \\\\ Macro-F1-14 & 30.70\\% & 37.31\\% & **39.83\\%** & 37.81\\% \\\\ Micro-F1-5 & 56.70\\% & 56.54\\% & **57.88\\%** & 56.28\\% \\\\ Macro-F1-5 & N/A & 50.57\\% & **51.60\\%** & 49.86\\% \\\\ F1-RadGraph & 24.40\\% & 25.20\\% & **26.71\\%** & 26.06\\% \\\\ BLEU-1 & **39.48\\%** & 30.90\\% & 32.31\\% & 31.73\\% \\\\ BLEU-4 & **13.30\\%** & 10.43\\% & 11.31\\% & 11.50\\% \\\\ ROUGE-L & **29.60\\%** & 26.16\\% & 27.29\\% & 27.49\\% \\\\ CIDEr-D & **49.50\\%** & 23.43\\% & 26.17\\% & 25.27\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table A.7: **Med-PaLM M performance on chest X-ray report generation.** We use text-overlap based and clinical factuality based automatic metrics to evaluate the quality of model-generated reports. Med-PaLM M sets new SOTA on all metrics designed to capture clinical efficacy and correctness. Across three Med-PaLM M variants, the medium-sized model achieves the best performance.\n' +
      '\n' +
      'Figure A.3 **| Side-by-side human evaluation task interface.** Radiologist raters ranked four findings paragraphs based on overall quality, given a chest X-ray and indication. The four findings corresponded to the reference findings, and findings generated by three Med-PaLM M model variants (12B, 84B, 562B).\n' +
      '\n' +
      'Figure A.4 **| Independent human evaluation task interface.** Radiologist raters annotated a findings paragraph generated by Med-PaLM M (red) for errors and omissions, given a chest X-ray, the indication, and reference findings.\n' +
      '\n' +
      'model produces 0.58 total errors per report.\n' +
      '\n' +
      'One important limitation of our human evaluation approach is the inter-rater variability. Similar to [60], which used a comparable evaluation scheme, we also observed that the same radiology report often was annotated with varying error and omission passages by different radiologist raters. While this is a common phenomenon in studies that use subjective ratings from clinicians, future work may aim to further refine rater instructions and improve rater calibration to reduce variability.\n' +
      '\n' +
      '### MultiMedBench Examples\n' +
      '\n' +
      'In Tables 9 to 13 we provide examples of various MultiMedBench tasks.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline Model Size & Med-PaLM M (562B) & Med-PaLM M (84B) & Med-PaLM M (12B) \\\\ \\hline Significant Omissions & 0.10 (95\\% CI, 0.08 - 0.12) & 0.09 (95\\% CI, 0.07 - 0.10) & 0.08 (95\\% CI, 0.06 - 0.10) \\\\ Total Omissions & 0.13 (95\\% CI, 0.11 - 0.16) & 0.12 (95\\% CI, 0.10 - 0.15) & 0.12 (95\\% CI, 0.10 - 0.15) \\\\ \\hline Significant Clinical Errors & 0.26 (95\\% CI, 0.23 - 0.29) & 0.23 (95\\% CI, 0.20 - 0.27) & 0.26 (95\\% CI, 0.22 - 0.29) \\\\ Total Clinical Errors & 0.29 (95\\% CI, 0.25 - 0.32) & 0.25 (95\\% CI, 0.22 - 0.28) & 0.28 (95\\% CI, 0.24 - 0.31) \\\\ Total Errors & 0.63 (95\\% CI, 0.58 - 0.68) & 0.59 (95\\% CI, 0.54 - 0.64) & 0.58 (95\\% CI, 0.53 - 0.63) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: **Independent human evaluation details. Rates of omissions and errors identified by clinician raters in radiology reports generated by different Med-PaLM M models. Clinical errors are those related to the presence, location or severity of a clinical finding. Total errors include both clinical errors and non-clinical errors (i.e., passages referring to views or prior studies not present).**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Image & Task and input prompt & Target \\\\ \\hline \\multicolumn{3}{l}{**Classification (PAD-UFES-20)**} \\\\ \\multicolumn{3}{l}{**Instructions:** You are a helpful dermatology assistant. The following are questions about skin lesions. Categorize the skin lesions into the most likely class given the patient history. \\\\ \\multicolumn{3}{l}{Given \\(<\\)img\\(>\\). Patient History: Age: 51, Gender: female, Smoke: false, Drink: false, Family skin cancer history: true, Family any cancer history: false, Lesion region: back, Lesion itch: false, Lesion grew: false, Lesion bled: false, Lesion elevation: false, Fitzpatrick scale: 1.0, DDiameters (mm): [12.0, 8.0]. Q: Which of the following is the most likely diagnosis of the patient’s skin lesion? (A) Nevus (B) Basal Cell Carcinoma (C) Squamous Cell Carcinoma (D) Actinic Keratosis (E) Seborrheic Keratosis (F) Melanoma A: Basal Cell Carcinoma. \\\\ \\multicolumn{3}{l}{Given \\(<\\)img\\(>\\). Patient History: Age: 39, Gender: unknown, Smoke: unknown, Drink: unk, Family skin cancer history: unknown, Family any cancer history: unknown, Lesion region: neck, Lesion i:ck: false, Lesion grew: true, Lesion bled: false, Lesion elevation: true, Fitzpatrick scale: unknown, Diameters (mm): [unknown, unknown]. Q: Which of the following is the most likely diagnosis of the patient’s skin lesion? (A) Nevus (B) Basal Cell Carcinoma (C) Squamous Cell Carcinoma (D) Actinic Keratosis (E) Seborrheic Keratosis (F) Melanoma A: \\\\ \\hline \\multicolumn{3}{l}{**Classification (MIMIC-CXR)**} \\\\ \\multicolumn{3}{l}{**Instructions:** You are a helpful radiology assistant. The following are questions about findings in chest X-ray in different views. Identify if a specific type of abnormality is shown in the X-ray. \\\\ \\multicolumn{3}{l}{Given the AP view X-ray image \\(<\\)img\\(>\\). Q: Is cardiomegaly indicated by the image? (A) No (B) Yes} \\\\ \\multicolumn{3}{l}{A: Yes.} \\\\ \\multicolumn{3}{l}{Given the AP view X-ray image \\(<\\)img\\(>\\). Q: Is cardiomegaly indicated by the image? (A) No (B) Yes} \\\\ \\multicolumn{3}{l}{A:} \\\\ \\multicolumn{3}{l}{**Classification (VinDr-Mammo)**} \\\\ \\multicolumn{3}{l}{**Instructions:** You are a helpful medical assistant. The following are questions about mammography reading. Provide a breast-level assessment based on the BI-RADS categories. \\\\ \\multicolumn{3}{l}{Given mammogram image \\(<\\)img\\(>\\). Image view: bilateral craniocaudal Q: What is the most likely breast BI-RADS score? (A) 1 (B) 2 (C) 3 (D) 4 (E) 5 A: 4. \\\\ \\multicolumn{3}{l}{Given mammogram image \\(<\\)img\\(>\\). Image view: bilateral craniocaudal Q: What is the most likely breast BI-RADS score? (A) 1 (B) 2 (C) 3 (D) 4 (E) 5 A: \\\\ \\multicolumn{3}{l}{**Classification (CBIS-DDSM Calcification)**} \\\\ \\multicolumn{3}{l}{Given mammogram image \\(<\\)**img\\(>\\)**. Image view: CC Q: Which of the following is the most likely type of the patient’s breast calcification? (A) BENIGN (B) BENIGN\\_WITHOUT\\_CALLBACK (C) MALIGNANT \\\\ \\multicolumn{3}{l}{A:} \\\\ \\multicolumn{3}{l}{**Classification (CBIS-DDSM Mass)**} \\\\ \\multicolumn{3}{l}{Given mammogram image \\(<\\)**img\\(>\\)**. Image view: CC Q: Which of the following is the most likely type of the patient’s breast mass? (A) BENIGN (B) BENIGN\\_WITHOUT\\_CALLBACK (C) MALIGNANT A: \\\\ \\multicolumn{3}{l}{**Generomic variant calling**} \\\\ \\multicolumn{3}{l}{**Instructions:** You are a helpful genetic assistant. The following are questions about variant calling. Identify the number of copies of the putative variant in pileup images. \\\\ \\multicolumn{3}{l}{Given \\(<\\)**img\\(>\\)**. Q: How many copies of this putative variant are shown in the middle of the image? (A) 0 (B) 1 (C) 2 A: \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 9: **Examples of the classification tasks in MultiMedBench.**\n' +
      '\n' +
      '**Table A.10**: **Examples of VQA and chest X-ray report generation tasks in MultiMedBench.**\n' +
      '\n' +
      '\\begin{tabular}{p{142.3pt} p{142.3pt} p{142.3pt}} \\hline Image & Task and input prompt & Target \\\\ \\hline  & **VQA-RAD** & Axial. \\\\  & **Instructions:** You are a helpful medical assistant. The following are questions about medical knowledge. Solve them in a step-by-step fashion, referring to authoritative sources as needed. & Brain. \\\\  & **Instructions:** You are a helpful medical assistant. The following are questions about medical knowledge. Solve them in a step-by-step fashion, referring to authoritative sources as needed. & Given \\(<\\)img\\(>\\). Q: Is the lung healthy? & A: No. \\\\  & **Given \\(<\\)img\\(>\\). Q: Which part of the body does this image belong to? A:** & \\\\ \\hline  & **Path-VQA** & **Instructions:** You are a helpful medical assistant. The following are questions about medical knowledge. Solve them in a step-by-step fashion, referring to authoritative sources as needed. & Accumulation of large numbers of macrophage. \\\\  & **Given \\(<\\)img\\(>\\). Q: What is present? (other)** & \\\\  & **Given \\(<\\)img\\(>\\). Q: What is there of large numbers of macrophages within the alveolar spaces with only slight fibrous thickening of the alveolar walls? (other)** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again.** demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again.** & **Again.** **Again.** \\\\  & **Instructions:** You are a helpful medical assistant. The following are questions about medical knowledge. Solve them in a step-by-step fashion, referring to authoritative sources as needed. & **Given \\(<\\)img\\(>\\). Q: Is the lung healthy? & **A: No. & **A: No. & **Given \\(<\\)img\\(>\\). Q: Which part of the body does this image belong to?\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:36]\n' +
      '\n' +
      '**Table A.13 \\(|\\) Example from MedQA in MultiMedBench.**\n' +
      '\n' +
      '**Input**\n' +
      '\n' +
      '**Instructions:** The following are multiple choice questions about medical knowledge. Solve them in a step-by-step fashion, starting by summarizing the available information. Output a single option from the four options as the final answer.\n' +
      '\n' +
      'Question: A 57-year-old man presents to his family physician for a checkup. He has had type 2 diabetes mellitus for 13 years, for which he has been taking metformin and vildagliptin. He has smoked 10-15 cigarettes daily for 29 years. Family history is irrelevant. Vital signs include: temperature 36.6\\({}^{\\circ}\\)C (97.8\\({}^{\\circ}\\)F), blood pressure 152/87 mm Hg and pulse 88/min. Examination reveals moderate abdominal obesity with a body mass index of 32 kg/m\\({}^{2}\\). The remainder of the examination is unremarkable. His fasting lipid profile is shown: Total cholesterol (TC) 280 mg/dL Low-density lipoprotein (LDL)-cholesterol 210 mg/dL High-density lipoprotein (HDL)-cholesterol 40 mg/dL Triglycerides (TGs) 230 mg/dL Which of the following is the mechanism of action of the best initial therapy for this patient?\n' +
      '\n' +
      '(A) Inhibition of cholesterol absorption (B) Bile acid sequestration (C) Inhibition of cholesterol synthesis (D) Activation of PPAR-alpha\n' +
      '\n' +
      'Answer: Inhibition of cholesterol synthesis.\n' +
      '\n' +
      'Question: A 3-year-old girl presents with her mother for a well-child checkup. Recent laboratory data has demonstrated a persistent normocytic anemia. Her mother denies any previous history of blood clots in her past, but she says that her mother has also had to be treated for pulmonary embolism in the recent past, and her brother has had to deal with anemia his entire life. The patient\'s past medical history is noncontributory other than frequent middle ear infections. The vital signs upon arrival include: temperature, 36.7\\({}^{\\circ}\\)C (98.0\\({}^{\\circ}\\)F); blood pressure, 106/74 mm Hg; heart rate, 111/min and regular; and respiratory rate, 17/min. On physical examination, her pulses are bounding and fingernails are pale, but breath sounds remain clear. Oxygen saturation was initially 91% on room air and electrocardiogram (ECG) shows sinus tachycardia. The patient\'s primary care physician orders a peripheral blood smear to further evaluate this finding, and preliminary results show a hemolytic anemia. Which of the following pathophysiologic mechanisms best describes sickle cell disease?\n' +
      '\n' +
      '(A) Increased red blood cell sensitivity to complement activation, making patients prone to thrombotic events (B) A recessive beta-globin mutation causing morphological changes to the RBC (C) An X-linked recessive disease in which red blood cells are increasingly sensitive to oxidative stress (D) Secondarily caused by EBV, mycoplasma, CLL, or rheumatoid disease\n' +
      '\n' +
      'Answer: A recessive beta-globin mutation causing morphological changes to the RBC.\n' +
      '\n' +
      'Question: A pulmonary autopsy specimen from a 58-year-old woman who died of acute hypoxic respiratory failure was examined. She had recently undergone surgery for a fractured femur 3 months ago. Initial hospital course was uncomplicated, and she was discharged to a rehab facility in good health. Shortly after discharge home from rehab, she developed sudden shortness of breath and had cardiac arrest. Resuscitation was unsuccessful. On histological examination of lung tissue, fibrous connective tissue around the lumen of the pulmonary artery is observed. Which of the following is the most likely pathogenesis for the present findings?\n' +
      '\n' +
      '(A) Thromboembolism (B) Pulmonary ischemia (C) Pulmonary hypertension (D) Pulmonary passive congestion\n' +
      '\n' +
      'Answer:\n' +
      '\n' +
      '_Target_\n' +
      '\n' +
      'Thromboembolism.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>