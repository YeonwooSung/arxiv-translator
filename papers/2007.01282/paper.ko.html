<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 생성 모델을 사용 하 여 경로 검색 활용\n' +
      '\n' +
      '개방형 도메인 질의응답\n' +
      '\n' +
      'Gautier Izacard\\({}^{1,2,3}\\) Edouard Grave\\({}^{1}\\)\n' +
      '\n' +
      '파리 페이스북 AI 연구\n' +
      '\n' +
      '({}^{2}\\) ENS, PSL 대학 파리\n' +
      '\n' +
      '\\({}^{3}\\) Inria, Paris\n' +
      '\n' +
      'gizacard|egrave@fb.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '오픈 도메인 질의 응답을 위한 생성 모델은 외부 지식에 의존하지 않고 경쟁력이 있는 것으로 입증되었다. 유망하지만, 이 접근법은 훈련 및 질의에 비용이 많이 드는 수십억 개의 파라미터를 갖는 모델을 사용해야 한다. 이 논문에서는 이러한 모델이 잠재적으로 증거를 포함하는 텍스트 지문을 검색하는 데 얼마나 많은 이점을 얻을 수 있는지 조사한다. 우리는 자연 질문과 트리비아QA 공개 벤치마크에 대한 최신 결과를 얻는다. 흥미로운 사실은 검색된 통로의 수를 늘릴 때 이 방법의 성능이 크게 향상된다는 것을 관찰한다. 이것은 서열 간 모델이 여러 구절의 증거를 효율적으로 집계하고 결합할 수 있는 유연한 프레임워크를 제공한다는 증거이다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근, 몇몇 작업들은 방대한 양의 데이터에 대해 트레이닝된 대규모 언어 모델들로부터 사실 정보가 추출될 수 있다는 것을 보여주었다(Radford et al., 2019; Petroni et al., 2019; Jiang et al., 2019; Talmor et al., 2019). 이러한 관찰과 자연어 처리 모델의 사전 훈련의 발전을 바탕으로 Roberts et al.(2020)은 오픈 도메인 질의 응답을 위한 생성 모델을 도입했다. 외부 지식에 의존하지 않고 이 방법은 여러 벤치마크에서 경쟁적인 결과를 얻었다. 그러나 모든 정보가 가중치에 저장되어야 하기 때문에 수십억 개의 매개변수를 포함하는 모델이 필요하다. 따라서 모델을 쿼리하고 훈련하는 데 비용이 많이 듭니다. 본 논문에서는 위키피디아와 같은 외부 지식원에 접근함으로써 이 방법이 얼마나 많은 이점을 얻을 수 있는지 조사한다.\n' +
      '\n' +
      '검색 기반 접근법은 이전에 추출 모델을 사용한 개방형 질문 답변의 맥락에서 고려되었다(Chen et al., 2017). 이 경우 시스템은 이러한 문서에서 답변을 추출하기 전에 지원 문서를 검색하는 것으로 시작합니다. TF/IDF에 기초한 희소 표현을 사용하거나 밀집 임베딩을 사용하는 등 상이한 검색 기술이 고려되었다(Guu et al., 2020; Karpukhin et al., 2020). 응답들을 추출하는 모델들은 종종 ELMo 또는 BERT(Peters et al., 2018; Devlin et al., 2019)와 같은 문맥화된 단어 표현들에 기초하고, 응답으로서 스팬을 예측한다. 여러 구절로부터의 증거를 집계하고 결합하는 것은 추출 모델을 사용할 때 간단하지 않으며, 이러한 한계를 해결하기 위해 여러 기술이 제안되었다(Clark and Gardner, 2018; Min et al., 2019).\n' +
      '\n' +
      '본 논문에서는 오픈 도메인 질의응답을 위한 생성 모델링 및 검색의 흥미진진한 발전을 바탕으로 두 세계의 장점을 갖는 간단한 접근 방법을 탐구한다. 이 방법은 두 단계로 진행되며, 먼저 희소 또는 조밀 중 하나를 사용하여 지원 패시지를 검색함으로써\n' +
      '\n' +
      '도 1: 오픈 도메인 질문 답변에 대한 간단한 접근법. 먼저 위키피디아와 같은 외부 지식 소스에서 지원 텍스트 구절을 검색합니다. 그런 다음, 생성 인코더-디코더 모델은 질문 및 검색된 지문에 조건화된 답변을 생성한다. 이 접근법은 최대 100개의 패시지를 검색할 때 성능이 계속 향상되기 때문에 검색된 패시지의 수에 따라 잘 확장된다.\n' +
      '\n' +
      'representations. 그런 다음, 시퀀스-투-시퀀스 모델은 질문에 추가하여 검색된 지문을 입력으로 하여 답변을 생성한다. 개념적으로 단순하지만 이 방법은 트리비아QA 및 자연 질문 벤치마크에 대한 새로운 최신 결과를 설정한다. 특히, 검색된 패시지의 수가 증가할 때 제안된 방법의 성능이 크게 향상됨을 보인다. 우리는 이것이 생성 모델이 추출 모델에 비해 여러 구절의 증거를 결합하는 데 능숙하다는 증거라고 믿는다.\n' +
      '\n' +
      '## 2 관련 작업\n' +
      '\n' +
      '오픈 도메인 질문 답변은 일반적인 도메인 질문에 답하는 작업으로, 시스템에 대한 입력으로 증거가 주어지지 않는다. 자연어 처리(Voorhees et al., 1999)에서 오랜 문제가 되었지만, 이 작업은 최근 Chen et al.(2017)의 작업에 이어 다시 관심을 얻고 있다. 그 문제의 버전에서, 강력한 감독은 답변에 대응하는 스팬의 형태로 학습 시스템에 이용 가능하다. Chen 등(2017)은 검색된 문서로부터 답변을 추출하기 전에, 위키피디아로부터 지원 문서를 먼저 검색함으로써 문제를 해결하기 위해 제안하였다. 시스템에 금 스팬이 주어지지 않고 정답만 주어지는 설정을 다루기 위해 다른 방법이 제안되었다. Clark and Gardner(2018)는 답변에 대응하는 모든 스팬에 걸쳐 글로벌 정규화를 사용할 것을 제안하였고, 이는 나중에 BERT 기반 모델(Wang et al., 2019)에 적용되었다. Min 등(2019)은 이 설정으로부터 잡음이 있는 감독을 다루기 위해 경성 기대-최대화에 기초한 방법을 도입하였다. Wang 등(2018)은 신뢰도 및 커버리지 점수를 사용하여 상이한 단락으로부터의 답변을 집계하는 기술을 설명하였다.\n' +
      '\n' +
      '패시지 검색은 오픈 도메인 질의 응답에서 중요한 단계이며 QA 시스템을 개선하기 위한 활발한 연구 분야이다. 처음에, TF/IDF에 기초한 희소 표현들이 지원 문서들을 검색하기 위해 사용되었다(Chen 등, 2017). Lee et al. (2018)은 BiLSTM을 기반으로 단락 순위를 재지정하는 지도 학습 방법을 도입하였고 Wang et al. (2018)은 강화 학습으로 순위 체계를 훈련하였다. QA 시스템의 검색 단계를 개선하기 위한 두 번째 접근법은 위키피디아 또는 위키다타 그래프와 같은 추가 정보를 사용하는 것이다(Min et al., 2019; Asai et al., 2020). 최근 여러 연구에서 조밀한 표현과 근사적 이웃을 기반으로 한 검색 시스템이 전통적인 접근법과 경쟁적임을 보여주고 있다. 이러한 모델은 질문-답변 쌍의 형태로 약한 수퍼비전을 사용하여 트레이닝될 수 있다(Karpukhin et al., 2020), 또는 클로즈 태스크 및 finetuned end-to-end를 사용하여 사전 트레이닝될 수 있다(Guu et al., 2020; Lee et al., 2019).\n' +
      '\n' +
      '생성적 질문 답변은 NarrativeQA(Kocisky et al., 2018), CoQA(Reddy et al., 2019) 또는 ELI5(Fan et al., 2019)와 같이 답변을 생성해야 하는 데이터 세트에 대해 이전 작업에서 대부분 고려되었다. 이러한 데이터 세트는 답변이 지원 문서의 스팬에 해당하지 않는 방식으로 생성되어 추상적인 모델이 필요했다. Raffel et al.(2019)은 생성 모델이 정답이 스팬인 SQuAD(Rajpurkar et al., 2016)와 같은 읽기 이해 과제에 경쟁력이 있음을 보여주었다. Roberts et al.(2020)은 오픈 도메인 질의응답을 위해 추가 지식을 사용하지 않고 사전 훈련된 대규모 생성 모델을 사용할 것을 제안했다. 본 연구에서 가장 가까운 Min et al. (2020)과 Lewis et al. (2020)은 오픈 도메인 질의 응답을 위한 검색 증강 생성 모델을 도입했다. 우리의 접근법은 생성 모델이 검색된 구절을 처리하는 방법에 따라 이러한 작업과 다르다. 이를 통해 많은 수의 문서로 확장할 수 있으며 이 많은 양의 증거로부터 이익을 얻을 수 있습니다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '본 절에서는 오픈 도메인 질의 응답에 대한 접근 방법에 대해 설명한다. 두 단계로 진행됩니다. 먼저 지원 구문을 검색한 후 시퀀스 대 시퀀스 모델로 처리합니다.\n' +
      '\n' +
      '도 2: Fusion-in-Decoder 방식의 구조.\n' +
      '\n' +
      '검색.지원 패시지의 검색을 위해, BM25(Robertson et al., 1995)와 DPR(Karpukhin et al., 2020)의 두 가지 방법을 고려한다. BM25에서, 구절은 단어들의 백으로 표현되고, 랭킹 함수는 용어 및 역 문서 빈도들에 기초한다. 기본 매개 변수를 사용 하 여 Apache Lucene1의 구현을 사용 하 고 SpaCy.2를 사용 하 여 질문 및 패시지를 토큰화 합니다. DPR에서 패시지와 질문은 두 BERT 네트워크를 사용 하 여 계산 되는 조밀한 벡터 표현으로 표시 됩니다. 랭킹 함수는 질의와 통로 표현 사이의 내적이다. 검색은 FAISS 라이브러리를 사용하여 근사 최근접 이웃을 사용하여 수행된다.\n' +
      '\n' +
      '각주 1: lucene.apache.org\n' +
      '\n' +
      '각주 2: spacy.io\n' +
      '\n' +
      '각주 3: github.com/facebookresearch/faiss\n' +
      '\n' +
      '독서. 오픈 도메인 QA에 대한 우리의 생성 모델은 T5 또는 BART와 같은 감독되지 않은 데이터에 대해 사전 훈련된 시퀀스 대 시퀀스 네트워크를 기반으로 한다(Raffel et al., 2019; Lewis et al., 2019). 모델은 질문과 지원 구절을 입력하고 답변을 생성합니다. 보다 정확하게는, 각각의 검색된 구절과 그 제목은 질문과 연결되며, 인코더에 의해 다른 구절들과 독립적으로 처리된다. 각 구절의 질문, 제목 및 텍스트 앞에 특수 토큰 질문, 제목 및 컨텍스트를 추가합니다. 마지막으로, 디코더는 검색된 모든 통로의 결과 표현들의 연접에 걸쳐 주의를 수행한다. 따라서 모델은 디코더에서만 증거 융합을 수행하며 우리는 이를 _Fusion-in-Decoder_라고 한다.\n' +
      '\n' +
      '인코더에서 독립적으로 패시지를 처리하지만 디코더에서 공동으로 처리함으로써, 이 방법은 Min 등(2020) 및 Lewis 등(2020)과 상이하다. 인코더에서 독립적으로 패시지를 처리하면 한 번에 하나의 컨텍스트에 대해서만 셀프 어텐션을 수행하기 때문에 많은 수의 컨텍스트로 스케일링할 수 있다. 이는 모델의 계산 시간이 2차식이 아닌 통과 횟수에 따라 선형적으로 증가한다는 것을 의미한다. 다른 한편으로, 디코더에서 공동으로 패시지를 프로세싱하는 것은 다수의 패시지에서 증거를 더 잘 집계할 수 있게 한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '본 절에서는 오픈 도메인 QA를 위한 Fusion-in-Decoder의 실증적 평가를 보고한다.\n' +
      '\n' +
      '데이터셋.우리는 다음과 같은 데이터셋을 고려하며, Lee 등(2019)과 동일한 설정을 사용한다:\n' +
      '\n' +
      '* NaturalQuestions(Kwiatkowski 등, 2019)는 구글 검색 쿼리에 해당하는 질문을 포함한다. 이 데이터 세트의 오픈 도메인 버전은 5개 이상의 토큰이 있는 답변을 폐기하여 얻습니다.\n' +
      '* TriviaQA (Joshi et al., 2017) contains questions gathered from trivia and quiz-league\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Model & NQ & \\multicolumn{2}{c}{TriviaQA} & \\multicolumn{2}{c}{SQuAD Open} \\\\  & EM & EM & EM & EM & F1 \\\\ \\hline DrQA (Chen et al., 2017) & - & - & - & 29.8 & - \\\\ Multi-Passage BERT (Wang et al., 2019) & - & - & - & 53.0 & 60.9 \\\\ Path Retriever (Asai et al., 2020) & 31.7 & - & - & **56.5** & **63.8** \\\\ Graph Retriever (Min et al., 2019) & 34.7 & 55.8 & - & - & - \\\\ Hard EM (Min et al., 2019) & 28.8 & 50.9 & - & - & - \\\\ ORQA (Lee et al., 2019) & 31.3 & 45.1 & - & 20.2 & - \\\\ REALM (Guu et al., 2020) & 40.4 & - & - & - & - \\\\ DPR (Karpukhin et al., 2020) & 41.5 & 57.9 & - & 36.7 & - \\\\ SpanSeqGen (Min et al., 2020) & 42.5 & - & - & - & - \\\\ RAG (Lewis et al., 2020) & 44.5 & 56.1 & 68.0 & - & - \\\\ \\hline T5 (Roberts et al., 2020) & 36.6 & - & 60.5 & - & - \\\\ GPT-3 few shot (Brown et al., 2020) & 29.9 & - & 71.2 & - & - \\\\ \\hline Fusion-in-Decoder (base) & 48.2 & 65.0 & 77.1 & 53.4 & 60.6 \\\\ Fusion-in-Decoder (large) & **51.4** & **67.6** & **80.1** & **56.7** & 63.2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 최신식 대비 비교. 트리비아QA에서는 오픈 도메인 테스트 세트(왼쪽), 숨겨진 테스트 세트(오른쪽), competitions.codalab.org/competitions/1720@results에 대한 결과를 보고한다.\n' +
      '\n' +
      '웹사이트 트리비아QA의 _필터링되지 않음_ 버전은 오픈 도메인 질문 응답에 사용됩니다.\n' +
      '* SQuAD v1.1 Rajpurkar 등 (2016)은 읽기 이해 데이터 세트입니다. 위키피디아에서 추출된 단락이 주어지면 주석자는 해당 단락의 범위인 질문을 작성하도록 요청받았다.\n' +
      '\n' +
      'Lee 등 (2019)에 이어 유효성 검사를 테스트로 사용 하 고 유효성 검사를 위해 훈련 세트의 10%를 유지 합니다. 우리는 12월부터 위키피디아 덤프를 사용한다. 2018년 12월 20일 NQ 및 트리비아QA. 21, 2016 for SQuAD. 우리는 Chen et al. (2017); Karpukhin et al. (2020)과 동일한 전처리를 적용하여 100단어의 구절이 겹치지 않게 된다.\n' +
      '\n' +
      '평가.예측된 답변은 Rajpurkar 등(2016)에 의해 소개된 바와 같이 표준 정확 일치 메트릭(EM)으로 평가된다. 생성된 답변은 정규화 후 허용 가능한 답변 목록의 임의의 답변과 일치하는 경우 올바른 것으로 간주된다. 이 정규화 단계는 기사 축소 및 제거, 구두점 및 중복된 공백으로 구성된다.\n' +
      '\n' +
      '기술 세부 사항. HuggingFace Transformers 라이브러리에서 사용할 수 있는 사전 훈련된 T5 모델 Raffel 등(2019)으로 모델을 초기화한다. 4는 각각 220M 및 770M 매개변수를 포함하는 두 가지 모델 크기, 즉 기본 및 대형을 고려한다. 본 연구에서는 Adam Kingma와 Ba (2014)를 이용하여 학습률(10^{-4}\\)이 일정하고 탈락률(10%)이 10%일 때 각 데이터셋의 모델을 독립적으로 미세 조정하였다. 64 테슬라 V100 32Gb를 사용하여 배치 크기가 64인 10k 구배 단계에 대한 모델을 훈련한다. 500단계마다 모델을 평가하고 정확한 일치 점수를 기반으로 검증 세트에서 가장 좋은 모델을 선택한다. 자연 질문 및 SQuAD에 대한 교육 동안 우리는 답변 목록 중 대상을 샘플링하는 반면 트리비아QA의 경우 고유한 인간 생성 답변을 사용한다. 트리비아QA의 경우 제목 파이썬 문자열 방법을 사용하여 각 단어의 첫 글자를 제외한 소문자의 모든 글자를 변환하여 대문자의 답을 정규화한다. 훈련과 테스트 모두에 대해 100개의 구절을 검색하고(달리 언급되지 않는 한), 250개의 단어 조각으로 잘라냅니다. 카푸킨 등(2020)의 결과에 따라 NQ 및 트리비아QA에 대한 DPR 및 SQuAD에 대한 BM25로 계대를 검색한다. 우리는 그리디 디코딩을 사용하여 답을 생성한다.\n' +
      '\n' +
      '각주 4: github.com/huggingface/transformers\n' +
      '\n' +
      '최첨단과의 비교 표 1에서는 Fusion-in-Decoder로 얻은 결과를 오픈 도메인 질의 응답에 대한 기존 접근법과 비교한다. 우리는 개념적으로 간단하지만 이 방법이 NaturalQuestion 및 TriviaQA 벤치마크에 대한 기존 작업보다 우수함을 관찰한다. 특히 생성 모델은 추출 접근법에 비해 여러 구절의 증거를 집계해야 할 때 잘 수행되는 것으로 판단된다. 또한 제안된 방법은 다른 생성 모델보다 성능이 우수하여 많은 수의 통로로 확장하고 공동으로 처리하면 정확도가 향상됨을 보여준다. 둘째, 검색에 의한 생성 모델에서 추가 지식을 사용하는 것이 중요한 성능 향상으로 이어진다는 것을 관찰한다. 자연 질문에 대해 _닫힌 책_ T5 모델은 11B 매개변수로 36.6%의 정확도를 얻는 반면, 우리의 접근법은 770M 매개변수와 BM25 검색을 포함하는 위키피디아로 44.1%를 얻는다. 두 방법 모두 정보를 저장하기 위해 대략 동일한 양의 메모리를 사용하는데, 이는 텍스트 기반 명시적 메모리가 지식 검색 작업에 경쟁력이 있음을 나타낸다.\n' +
      '\n' +
      '패스 수에 따른 스케일링.그림 3에서 우리는 성능에 대해 보고한다.\n' +
      '\n' +
      '도 3: 검색된 통로들의 수의 함수로서 유효 세트들에 대한 퓨전-인-디코더(베이스)의 성능.\n' +
      '\n' +
      '검색된 통로의 수입니다. 특히, 문항의 수를 10개에서 100개로 늘리면 트리비아QA가 6%, 내추럴쿼션이 3.5% 개선되는 것을 관찰할 수 있다. 반면, 대부분의 추출 모델들의 성능은 Wang et al.(2019), Yang et al.(2019)을 중심으로 10~20여개의 절정에 달하는 것으로 보인다. 우리는 이것이 시퀀스 대 시퀀스 모델이 여러 구절의 정보를 결합하는 데 능숙하다는 증거라고 믿는다.\n' +
      '\n' +
      '훈련 구절 수의 영향.이전 절에서는 동일한 구절 수로 모델을 훈련하고 평가하였다. 학습 계산 예산을 줄이기 위해, 간단한 해결책은 더 적은 패시지를 갖는 모델을 학습시키는 것이다. 표 2에서는 100개의 패시지로 테스트하는 동안 다른 패시지의 수로 훈련하여 얻은 성능을 보고한다. 학습 패시지의 수를 줄이면 정확도가 감소한다는 것을 알 수 있다. 또한, 1000단계 동안 100개의 패시지를 사용하여 이전 모델을 세분화하는 것을 제안한다. 이를 통해 정확도 격차를 줄이는 동시에 계산 리소스를 훨씬 적게 사용할 수 있습니다. 100개 패시지에 대해 훈련할 때 425 GPU 시간에 비해 147 GPU 시간을 사용하여 NaturalQuestions에서 46.0 EM에 도달할 수 있습니다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 오픈 도메인 질의 응답에 대한 간단한 접근 방법을 제안한다. 오픈 도메인 질의 응답은 생성 모델을 사용하여 처리되기 전에 지원 구문을 검색하는 것에 의존한다. 우리는 개념적으로 간단하지만 이 접근법이 기존 방법들과 경쟁적이며 검색된 패시지의 수에 따라 잘 확장된다는 것을 보여준다. 향후 연구에서는 특히 많은 수의 지지 통로로 확장할 때 이 모델을 보다 효율적으로 만들 계획이다. 또한 검색 결과를 모델에 통합하고 전체 시스템을 종단 간 학습할 계획입니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*A. Asai, K. 하시모토 하지시르지 Socher, and C. Xiong (2020)Learning to retrieve reasoning path over wikipedia graph for question answer. Proc. ICLR, Cited by: SS1, SS2.\n' +
      '* T. B. Brown, B. Mann, N. 라이더 Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020)Language models are few-shot learners. arXiv preprint arXiv:2005.14165. Cited by: SS1, SS2.\n' +
      '* D. Chen, A. Fisch, J. Weston, and A. Bordes(2017)Reading Wikipedia to answer open-domain questions. Proc. ACL, Cited by: SS1, SS2.\n' +
      '* C. Clark and M. Gardner (2018)Simple and effective multi-paragraph reading comprehension. Proc. ACL, Cited by: SS1, SS2.\n' +
      '* J. Devlin, M. 장광 이근 Toutanova (2019)BERT: 언어 이해를 위한 심층 양방향 변압기 사전 훈련. Proc. NAACL, Cited by: SS1, SS2.\n' +
      '* A. Fan, Y. Jernite, E. Perez, D. Grangier, J. Weston, and M. Auli (2019)ELIS: long form question answer. Proc. ACL, Cited by: SS1, SS2.\n' +
      '*K. 구광 이진호 Tung, P. Pasupat, M. Chang(2020)Realm: retrieval-augmented language model pre-training. arXiv preprint arXiv:2002.08909. Cited by: SS1, SS2.\n' +
      '*Z. Jiang, F. F. Xu, J. Araki, and G. Neubig (2019) 어떻게 우리는 언어 모델들이 무엇을 알고 있는지 알 수 있는가? arXiv preprint arXiv:1911.12543. Cited by: SS1, SS2.\n' +
      '*M. Joshi, E. Choi, D. S. Weld, L. Zettlemoyer (2017)Triviaqa: 판독 이해를 위한 대규모 원거리 감독 챌린지 데이터세트. Proc. ACL, Cited by: SS1, SS2.\n' +
      '* V. 카푸킨 민락 우성 Edunov, D. Chen, and W. Yih (2020)Dense passage retrieval for open-domain question answer. arXiv preprint arXiv:2004.04906.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline  & \\multicolumn{2}{c}{NaturalQuestions} & \\multicolumn{2}{c}{TriviaQA} \\\\ Training Passages & w/o finetuning & w/ finetuning & w/o finetuning & w/ finetuning \\\\ \\hline\n' +
      '5 & 37.8 & 45.0 & 58.1 & 64.2 \\\\\n' +
      '10 & 42.3 & 45.3 & 61.1 & 63.6 \\\\\n' +
      '25 & 45.3 & 46.0 & 63.2 & 64.2 \\\\\n' +
      '50 & 45.7 & 46.0 & 64.2 & 64.3 \\\\\n' +
      '100 & 46.5 & - & 64.7 & - \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 훈련시 사용되는 패스 수에 따른 성능. 정확한 매치 점수는 디브 세트에서 보고됩니다.\n' +
      '\n' +
      '* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. _ arXiv preprint arXiv:1412.6980_.\n' +
      '* Kocisky 등(2018) Tomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. _ TACL_.\n' +
      '* Kwiatkowski 등(2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. 투타노바, 라이온 존스, 밍웨이 창, 앤드류 다이, 야콥 우스코레이트, 콥 르, 슬라브 페트로프 등이다. 2019. Natural Questions: 질문 응답 연구를 위한 벤치마크. _ TACL_.\n' +
      '* Lee 등(2018) 이진혁, 윤성준, 김현재, 고미영, 강재우. 2018. Ranking 단락 for improving answer recall in open-domain question answer. _Proc. EMNLP_.\n' +
      '* Lee et al.(2019) Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answer. _Proc. ACL_.\n' +
      '* Lewis 등(2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. BART: 자연어 생성, 번역, 이해를 위한 시퀀스 대 시퀀스 사전 트레이닝 노이즈 제거_ arXiv preprint arXiv:1910.13461_.\n' +
      '* Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. _ arXiv preprint arXiv:2005.11401_.\n' +
      '* Min 등(2019a) Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019a. 약하게 감독된 질문 응답을 위한 이산 하드 EM 접근 방식입니다. _Proc. EMNLP-IJCNLP_.\n' +
      '* Min 등(2019b) Sewon Min, Danqi Chen, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019b. 오픈 도메인 질문 답변을 위한 지식 가이드 텍스트 검색 및 읽기를 제공합니다. _ arXiv preprint arXiv:1911.03868_.\n' +
      '* Min 등(2020) Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020. Ambigqa: 모호한 오픈 도메인 질문에 답변합니다. _ arXiv preprint arXiv:2004.10645_.\n' +
      '* Peters 등(2018) Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representation. _Proc. NAACL_.\n' +
      '* Petroni 등(2019) Fabio Petroni, Tim Rocktaschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. 지식 베이스로서의 언어 모델들? _Proc. EMNLP-IJCNLP_.\n' +
      '* Radford 등(2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. 언어 모델은 감독 되지 않은 다중 작업 학습자입니다. _ OpenAI 기술 보고서_.\n' +
      '* Raffel 등(2019) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. 통합 텍스트-텍스트 변환기를 이용한 전이학습의 한계 탐색. _ arXiv preprint arXiv:1910.10683_.\n' +
      '* Rajpurkar 등(2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. _Proc. EMNLP_.\n' +
      '* Reddy 등(2019) Siva Reddy, Danqi Chen, and Christopher D Manning. 2019. CoQA: A conversation question answer challenge. _ TACL_.\n' +
      '* Roberts 등(2020) Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. 언어 모델의 매개 변수에 얼마나 많은 지식을 담을 수 있습니까? _ arXiv preprint arXiv:2002.08910_.\n' +
      '* Robertson et al.(1995) Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at TREC-3. _NIST Special Publication Sp_.\n' +
      '* Talmor 등(2019) Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2019. oLMpics-on what language model pre-training captured. _ arXiv preprint arXiv:1912.13283_.\n' +
      '* Voorhees et al. (1999) Ellen M Voorhees et al. 1999. The TREC-8 question answering track report. _TREC_에서.\n' +
      '* Wang 등(2018a) Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. 2018a. R\\({}^{3}\\). 개방형 질문 응답을 위한 강화된 랭커 판독기입니다. _Proc. AAAI_.\n' +
      '* Wang 등(2018b) Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. 2018b. 개방형 질문 응답에서 답변 재순위에 대한 증거 집계 _Proc. ICLR_.\n' +
      '* Wang 등(2019) Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, and Bing Xiang. 2019. Multi-passage BERT: A globally normalized BERT model for open-domain question answer. _Proc. EMNLP-IJCNLP_.\n' +
      '* Yang 등(2019) Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019. End-to-end open-domain question answer with BERTserini. _Proc. NAACL(Demonstrations)_.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>