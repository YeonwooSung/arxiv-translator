{
    "2310.08659": {
        "paper_id": "2310.08659",
        "abs_url": "https://arxiv.org/abs/2310.08659",
        "pdf_url": "https://arxiv.org/pdf/2310.08659.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2310.08659_LoftQ_LoRA-Fine-Tuning-Aware_Quantization_for_Large_Language_Models.pdf",
        "title": "LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Yixiao Li",
            "Yifan Yu",
            "Chen Liang",
            "Pengcheng He",
            "Nikos Karampatziakis",
            "Weizhu Chen",
            "Tuo Zhao"
        ],
        "abstract": ".",
        "comments": "",
        "official_code_urls": [
            "https://github.com/yxli2123/loftq"
        ],
        "pwc_page_url": "https://paperswithcode.com/paper/loftq-lora-fine-tuning-aware-quantization-for",
        "bibtex": "@misc{li2023loftq,\n      title={LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models}, \n      author={Yixiao Li and Yifan Yu and Chen Liang and Pengcheng He and Nikos Karampatziakis and Weizhu Chen and Tuo Zhao},\n      year={2023},\n      eprint={2310.08659},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
    }
}