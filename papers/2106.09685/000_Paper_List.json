{
    "2106.09685": {
        "paper_id": "2106.09685",
        "abs_url": "https://arxiv.org/abs/2106.09685",
        "pdf_url": "https://arxiv.org/pdf/2106.09685.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2106.09685_LoRA_Low-Rank_Adaptation_of_Large_Language_Models.pdf",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Edward J. Hu",
            "Yelong Shen",
            "Phillip Wallis",
            "Zeyuan Allen-Zhu",
            "Yuanzhi Li",
            "Shean Wang",
            "Lu Wang",
            "Weizhu Chen"
        ],
        "abstract": ".",
        "comments": "Draft V2 includes better baselines, experiments on GLUE, and more on adapter latency",
        "official_code_urls": [
            "https://github.com/microsoft/LoRA"
        ],
        "pwc_page_url": "https://paperswithcode.com/paper/lora-low-rank-adaptation-of-large-language",
        "bibtex": "@misc{hu2021lora,\n      title={LoRA: Low-Rank Adaptation of Large Language Models}, \n      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},\n      year={2021},\n      eprint={2106.09685},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
    }
}