<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 도메인 간 브리지 학습에 의한 감독되지 않은 도메인 일반화\n' +
      '\n' +
      '시반 하라리*1, 일라이 슈워츠*1,2, 아사프 아르벨*1\n' +
      '\n' +
      'Peter Staar1, Shady Abu-Hussein1,2, Elad Armani1,3, Roei Herzig1,2,\n' +
      '\n' +
      '아미트 알파시1,3, 라자 기예스2, 힐데 쿠에네6,7, 디나 카타비5\n' +
      '\n' +
      '케이트 생코 4,7, 로저리오 페리스 7 레오니드 칼린스키*1\n' +
      '\n' +
      'Equal contribution\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '실제 사진, 클립아트, 그림 및 스케치 사이와 같이 상당히 다른 시각적 영역에 걸쳐 학습된 표현을 일반화하는 능력은 인간 시각 시스템의 기본 능력이다. 이 문서에서는 일부 (또는 전체) 원본 도메인 감독을 사용 하는 대부분의 교차 도메인 작업과는 달리 원본 도메인과 대상 도메인 모두에 **교육 감독 없음** 을 사용 하는 비교적 새롭고 매우 실용적인 UDG (Unsupervised Domain Generalization) 설정에 접근 합니다. 우리의 접근 방식은 각 교육 도메인에서 BrAD에 시각적(이미지 간) 매핑을 보존하는 일련의 의미론을 수반하는 보조 **브리지** 도메인인 BrAD(Bridge Across Domains)의 자체 감독 학습을 기반으로 합니다. BrAD 및 그것에 대한 매핑은 각각의 도메인을 그것의 BrAD-투영에 의미적으로 정렬하는 대조적 자기 감독 표현 모델과 공동으로(엔드 투 엔드) 학습되고, 따라서 암묵적으로 모든 도메인(보이는 또는 보이지 않는)을 서로 의미적으로 정렬하도록 구동한다. 이 연구에서는 에지 정규화된 BrAD를 사용하여 UDG, Few-shot UDA 및 비지도 일반화를 포함하는 다중 도메인 데이터 세트(보이지 않는 도메인 및 클래스에 대한 일반화 포함)에서 여러 벤치마크와 다양한 작업에 걸쳐 상당한 이득을 얻는 방법을 보여준다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '일부 기술 매뉴얼을 장비의 개략적인 도면으로 처음 관찰할 때, 사람들은 이러한 개략적인 도면을 실제 복잡한 객체와 연관시키기 위해 어떠한 감독도 필요하지 않다. 이는 기본 인간 능력 중 하나의 중요성과 효율성을 보여 줍니다. 여러 시각적 도메인에 걸쳐 _감독이 거의 또는 전혀 없이_ 학습 할 수 있을 뿐만 아니라 _새로운 도메인으로 일반화_ 및 추가 감독 없이 새로운 개체 클래스로도 효과적으로 일반화 할 수 있습니다.\n' +
      '\n' +
      '섹션 2에서 광범위하게 논의된 최근 문헌은 대상 도메인(들)에서 감독 없이 도메인 전체에 의미적으로 일반화하는 학습에 대한 연구가 풍부하다. 대상 도메인은 UDA(Unsupervised Domain Adaptation)에서 레이블이 지정되지 않은 이미지의 집합으로 관찰되거나, DG(Domain Generalization)에서 훈련하는 동안 완전히 보이지 않을 수 있다. UDA와 DG 모두에 대해 일반화의 성공은 원하는 다운스트림 작업(분류, 탐지 등)이 새로 보이거나 보이지 않는 영역으로 성공적으로 전달된다는 것을 의미한다. 그러나 대부분의 UDA 및 DG에서는 이에 대한 풍부한 소스 도메인 감독을 수행한다.\n' +
      '\n' +
      '도 1: 원래, 동일한 도메인 인스턴스들은 다른 도메인들에서 동일한 클래스의 인스턴스들보다 서로 더 가깝다. 대중적인 자기 지도 학습 기법의 순진한 적용은 수업 전에 영역을 분리하는 경향이 있다(섹션 4). 우리의 접근 방식은 도메인 간에 동일한 클래스의 인스턴스를 정렬하는 데 도움이 되는 보조 브리지 도메인(예: 에지 유사 이미지)인 BrAD를 학습하는 것입니다. 화살표는 훈련 손실에 의해 적용된 특징 공간의 힘을 나타낸다(섹션 3). 녹색 = 끌림, 빨간색 = 반발.\n' +
      '\n' +
      '의도된 다운스트림 작업을 가정합니다. 그러나 우리는 항상 실제 사용 사례에서 그것을 가지고 있습니까? 위의 기술 매뉴얼 예와 같은 많은 상황에서 우리는 새로운 도메인뿐만 아니라 데이터(이미지 및/또는 레이블)가 거의 없을 수 있는 완전히 새로운 종류의 객체에 일반화해야 하며 표준 UDA 또는 DG 방법을 훈련시키기에 충분하지 않다. 최근 UDG(Unsupervised DG) 및 FUDA(Few-Shot UDA)는 거의 작동하지 않고 이 문제의 중요성을 깨닫고 소스 도메인 감독을 소수의 또는 0 라벨로 표시된 예로 제한한다. 본 논문에서는 레이블링 요구사항 측면에서 가장 제한적인 UDG 설정을 목표로 한다. UDG 설정은 훈련 시 소스 도메인 감독이 필요하지 않으며, 새로운 미보임 클래스를 가진 완전히 새로운 미보임 시각 도메인에 대한 일반화를 암시적으로 지원한다.\n' +
      '\n' +
      '우리는 모두 무언가를 빨리 그리라는 요청을 받았을 때 엣지 같은 이미지를 그린다. 객체 간선은 우리가 관찰하는 모든 도메인에 대해 공유된 보편적인 시각적 표현인 것 같다. 이것은 우리의 BrAD 접근법 이면의 기본적인 직관을 견인했다: 기계에게 모든 시각적 영역(특징 공간 내)을 에지와 같은 이미지의 겉보기에 보편적으로 공유되는 시각적 영역을 나타내는 것과 동일한 방식으로 표현하도록 가르친다. 우리의 접근법(그림 1)은 관심 있는 모든 영역을 시각적으로 매핑하는 것이 비교적 쉬운 보조 시각적 \'브리지\' 도메인인 학습 가능한 브리지 크로스 도메인(BrAD)의 제안된 개념을 기반으로 한다. BrAD는 각 학습 영역의 표현(특징)을 공유 BrAD에 대한 표현과 의미적으로 정렬하기 위해 모델의 대조적 자체 감독 학습 동안에만 사용된다. 특징 공간에서 이 의미적 정렬의 전이성에 의해, 모든 도메인에 대한 학습된 모델 표현들은 모두 BrAD-정렬되고 따라서 서로 암시적으로 정렬된다. 이렇게 하면 학습된 BrAD 매핑이 추론에 불필요하게 되어 테스트 시간에 새로운 보이지 않는 도메인(BrAD 매핑이 없는 도메인)에도 학습된 모델을 일반화시킬 수 있다. 또한, 추론을 위한 BrAD 매핑에 의존하지 않고, 표현 모델 인코더(Sec. 3)에 의해 또한 학습되는 추가적인 비-BrAD-특정 특징(예: 색상)을 이용할 수 있다. 우리는 BrAD 아이디어의 간단한 휴리스틱 구현, 이미지를 에지 맵에 매핑하는 것조차도 이미 BrAD를 사용하지 않는 강력한 자체 감독 기준선에 비해 좋은 개선을 제공한다는 것을 보여준다. 또한 학습 가능한 BrAD를 사용하여 UDG, FUDA 및 제안된 일반화 태스크와 같은 다양한 데이터 세트와 태스크에서 좋은 이득을 보여준다.\n' +
      '\n' +
      '본 논문의 기여도를 요약하면 다음과 같다. (i) 학습 가능한 BrAD의 새로운 개념을 제안하며, 관심 도메인으로부터 비교적 쉽게 매핑될 수 있고, 학습된 표현 특징을 도메인 간에 의미적으로 정렬(일반화)할 수 있는 보조 시각적 \'브릿지\' 도메인을 제안한다. (ii) BrAD 개념을 자기 지도 대조 학습과 결합하여 다양한 종류의 소스 라벨 제한 교차 도메인 태스크에 대해 효과적인 모델을 훈련할 수 있는 방법을 보여준다. UDG, FUDA, 심지어 감독 없이 다중 도메인 벤치마크에 걸쳐 일반화도 포함한다. (iii) UDG에 비해 최대 \\(14\\%\\), FUDA에 비해 최대 \\(13.3\\%\\)의 상당한 이득을 얻을 수 있으며, 추가적인 미세 조정 없이 학습된 표현을 새로운 미확인 도메인 및 객체 범주로 전달하는 데 상당한 이점을 보여준다.\n' +
      '\n' +
      '## 2 관련 작업\n' +
      '\n' +
      '**UDA(감독되지 않은 도메인 적응)** UDA [16]은 _라벨된_ 원본 도메인에서 _라벨되지 않은_ 대상 도메인으로 지식을 전달하는 것을 나타냅니다. 대부분의 UDA 방법은 도메인 분포의 최대 불일치[25, 37, 48, 52, 69, 70], 적대적 도메인 분류기[13, 14, 55, 56, 50, 36, 51], 엔트로피 최적화[47, 36, 46, 26]를 사용하여 특징 분포 정렬을 사용한다. GAN 기반 이미지 번역은 [2, 21, 29, 39, 51]에서 사용된다. [34, 58, 64]에서 소스 도메인 데이터는 소스 상에서 미리 트레이닝된 모델로 대체된다. [38] 단안 깊이 추정을 위해 UDA의 일관성을 강화하기 위해 낮은 수준의 에지 기능을 사용한다. FUDA(Few-shot UDA) [65, 26]에서는 클래스당 몇 개의 예만 소스 도메인에서 라벨링되고 나머지는 라벨링되지 않는다. 우리의 작업에서 훈련 중에 라벨이 붙은 예를 보지 않고 도메인 적응을 다시 한 단계 더 나아가 사전 형성한다.\n' +
      '\n' +
      '**도메인 일반화 (DG).** DG는 교육 중에 보이지 않는 도메인에 지식을 전달 하는 주소를 지정 합니다. 대부분의 DG 작업은 레이블이 지정된 소스 도메인 세트에 대해 감독 훈련을 수행한다. 감독된 DG를 위한 방법들은: 도메인들에 걸친 분포 매칭[31, 33], 상이한 소스 도메인 손실들의 적응적 가중치 부여[45], 잠재 표현들의 낮은 순위를 시행[32], 및 랜덤 소스 도메인 혼합[59]을 포함한다. 감독되지 않은 DG(UDG)는 [68]에 의해 도입된 라벨이 없는 소스 도메인을 사용하여 훈련하는 새로운 작업이다. 라벨링되지 않은 소스 도메인 이미지는 자체 감독된 사전 훈련에 사용되며, 이어서 소스 이미지의 작은 부분을 라벨링함으로써 분류기를 학습된 표현에 피팅한다. 추가 분류기 피팅 없이 kNN을 통해 직접 비지도 사전 훈련 기능을 사용하는 경우에도 UDG 설정에 대한 본 방법의 강력한 이점을 보여준다.\n' +
      '\n' +
      '**SSL(자체 지도 학습)** SSL은 레이블이 지정되지 않은 데이터에서 강력한 의미 기능 표현을 학습하는 것을 의미합니다. 역사적으로 SSL[12, 15, 27, 41, 42, 67]에 대해 많은 사전 텍스트 작업이 제안되었다. 최근 대조적 학습[5, 61, 5, 7, 19, 10, 12, 17, 57, 66, 5, 60, 12, 18, 54, 57, 61]은 큰 가능성과 SOTA 결과를 보여주었다. 비교적 방법은 일반적으로 동일한 이미지의 두 증강을 네거티브 앵커 세트까지의 거리보다 특징 공간에서 더 가깝게 만들어 인스턴스 판별을 최적화한다. 본 논문에서 제안한 교차 도메인 브리지(cross-domain bridge)의 사용을 피하기 위해 클래스를 분리하기 전에 도메인을 분리하려는 SSL 방법의 경향으로 인해, 다중 도메인 데이터에 대해 인기 있는 SSL 방법을 직접 적용하는 것보다 본 논문에서 제안한 방법의 장점을 보여준다. 우리의 작업과 동시에 [60]은 비디오에서 SSL에 대한 HOG 기능을 성공적으로 사용했다. 우리의 접근 방식은 학습된 브리지 도메인에 대한 대체 정규화로 HOG 또는 기타 수작업 에지 기반 기능을 쉽게 채택할 수 있으므로 향후 작업에서 대조 SSL에 대한 에지 유틸리티에 대한 추가 조사를 위한 일반적인 도구를 제공한다.\n' +
      '\n' +
      '**UDA 및 DG에 대한 자체 감독 학습** 일부 UDA 및 DG 메서드는 파이프라인에서 SSL 손실을 사용합니다. 직소 퍼즐을 푸는 SSL 작업은 UDA와 DG를 돕기 위해 [41]에서 활용되었다. [53] 소스 도메인에 대한 지도 학습을 두 도메인의 SSL과 공동으로 결합한다. 최근에 [26, 65]는 Few-shot UDA(FUDA)를 위한 교차 도메인 SSL 접근법을 제안하였고, [68]은 UDG를 위한 SSL 방법을 제안하였다. 언급된 바와 같이, 우리는 FUDA와 UDG 작업 모두에 대해 우리의 방법의 강력한 이점을 보여준다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '\\(\\mathcal{D}=\\{D_{n}\\}_{n=1}^{N}\\)을 훈련에 사용되는 \\(N\\) 도메인 집합(예: FUDA [26, 65]의 원본 및 대상 도메인 쌍 또는 UDG [68]의 원본 도메인 집합)이라고 가정합니다. 각 도메인 \\(D_{n}\\)은 _표지되지 않은_ 이미지 \\(\\{I_{n}^{j}\\}\\) 집합으로 표현되며, 명확성을 위해 도메인 \\(D_{n}\\)에서 \\(j\\)을 생략하고 \\(I_{n}\\) \'이미지\'로 표현한다. 우리의 목표는 백본 모델 \\(\\mathcal{B}\\)(예: CNN)을 \\(d\\)-차원 표현 공간 \\(\\mathcal{F}\\subset\\mathbb{R}^{d}\\)(모든 도메인에 대해 공유됨)의 클래스 매핑을 위한 \\(\\mathcal{C}\\)(훈련 시 알 수 없음) 및 \\(I_{n}\\in D_{n}\\in D_{m}\\), s.t. \\(\\mathcal{C}(I_{n})=\\mathcal{C}(I_{m})\\neq\\mathcal{C}(I_{r})\\): \\(||\\mathcal{B}(I_{n})-\\mathcal{B}(I_{m})||\\ll||\\mathcal{B}(I_{n})-\\mathcal{B}(I_{r})||\\)이 만족될 가능성이 높습니다. 더욱이, 우리의 전체적인 목표는 이러한 의미 정렬 속성이 훈련 중에 보이지 않더라도 다른 도메인으로도 일반화될 것이라는 것이다.\n' +
      '\n' +
      'MocoV2 접근법 [9]을 확장하여 BrAD 학습 및 아래에 설명된 다른 아이디어를 통합하기 위해 대조적 자기 지도 학습을 사용하여 \\(\\mathcal{B}\\)을 훈련한다. 구체적으로, 본 논문의 학습 구조는 (1) 백본 \\(\\mathcal{B}:I\\rightarrow\\mathbb{R}^{d}\\)(예: GAP와 \\(d=2048\\)) - 학습 후 유지된 유일한 것으로 나머지 구성 요소는 학습에만 사용되며 나중에 폐기된다; (2) 프로젝션 헤드 \\(\\mathcal{P}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{p}\\)(예: \\(p<d\\) \\(p=128\\)과 \\(\\mathbb{L}_{2}\\) 상단에 정규화가 더 용이하다; (3) 각 도메인에 대한 별도의 부정 큐 \\(\\mathcal{Q}_{n}\\) - 모든 도메인에 대해 단일 큐를 갖는 것([9])이 성능 저하(Sec. 4.4)를 관찰한다; (4) 각 도메인 \\(D_{n}\\in\\mathcal{D}\\)을 공유된 도메인과 매핑하기 위한 이미지 대 이미지 모델 집합 \\(\\Psi_{n}:D_{n}\\rightarrow\\Omega\\), 보조 BrAD 도메인 \\(\\Omega\\) 4) \\(\\Psi_{n}\\); (5) 도메인 판별기 \\(\\mathcal{A}:\\mathbb{R}^{d}\\rightarrow\\{1,\\cdots,N\\}\\), 즉 \\(\\Omega\\) 이미지 표현에 _only_를 적용한 적대적 도메인 분류기인 \\(\\mathcal{B}(\\Psi_{n}(I_{n}))\\) 및 \\(\\Omega\\)으로 투영된 임의의 이미지 \\(I_{n}\\in D_{n}\\)의 원래 도메인 인덱스 \\(n\\)을 예측하고자 한다. 직관적으로, \\(\\mathcal{A}\\)을 혼동하는 표현을 생성하는 학습은 \\(\\Omega\\) 내부의 다른 모든 원래 도메인의 투영을 더 잘 정렬한다. (6) 운동량 모델 \\(\\mathcal{B}^{m}\\)과 \\(\\mathcal{P}^{m}\\) ([9]에서와 같이): EMA만 각각 \\(\\mathcal{B}\\)과 \\(\\mathcal{P}\\)의 복사본을 갱신하였다.\n' +
      '\n' +
      '훈련은 모든 훈련 도메인 \\(\\mathcal{D}\\)에서 무작위로 샘플링된 이미지 일괄로 진행된다. 명확성을 위해 단일 입력 영상 \\(I_{n}\\in D_{n}\\in\\mathcal{D}\\)에 대한 훈련 흐름을 설명한다. \\(I_{n}^{a1}\\)과 \\(I_{n}^{a2}\\)이 \\(I_{n}\\)의 두 증분이 되는 경우 먼저 다음과 같은 대비손실을 정의한다.\n' +
      '\n' +
      '\\[\\begin{split}\\mathcal{L}_{cont}(I_{n})&=\\mathcal{L} _{nce}(\\mathcal{P}(\\mathcal{B}(I_{n}^{a1})),\\mathcal{P}^{m}(\\mathcal{B}^{m}( \\Psi_{n}(I_{n}^{a2}))),\\mathcal{Q}_{n})\\\\ &+\\mathcal{L}_{nce}(\\mathcal{P}(\\mathcal{B}(\\Psi_{n}(I_{n}^{a1}))),\\mathcal{P}^{m}(\\mathcal{B}^{m}(I_{n}^{a2})),\\mathcal{Q}_{n}\\end{split} \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(\\mathcal{L}_{nce}(q,k_{+},k_{-})\\)는 쿼리와 함께 표준 InfoNCE 손실 [18, 19], \\(q\\), \\(q\\)을 끌어당기는 포지티브 키 \\(k_{+}\\) 및\n' +
      '\n' +
      '도 2: _BrAD 트레이닝 아키텍처._\\ (I_{n}^{a1},I_{n}^{a2}\\)는 D_{n}\\에서 영상의 무작위 확대이다. \\ (\\Psi_{n}(I_{n}^{a1}),\\Psi_{n}(I_{n}^{a2})\\)은 (학습된) 도메인 특정 이미지-이미지 매핑 \\(\\Psi_{n}\\)을 사용하여 브리지 도메인 \\(\\Omega\\)에 매핑된다. 화살표의 색은 모델에 흐르는 이미지를 나타낸다. \\(I_{n}^{a1}\\)(청색), \\(I_{n}^{a2}\\)(분홍색), \\(\\Psi_{n}(I_{n}^{a1})\\)(녹색) 및 \\(\\Psi_{n}(I_{n}^{a2})\\)(적색). 콘트라스트 손실의 음수 키 \\(k_{-}\\) \\(L_{nce}\\)은 도메인 특정 큐에서 발생한다. Edge-mapping(\\mathcal{E}\\)으로부터 증류하는 \\(L_{n}\\) 정규화(regularization)를 적용하여 브리지 도메인 이미지를 에지 맵과 유사하도록 하여 도메인 이동에 덜 민감하다(직관적으로). 마지막으로, 도메인 판별기 \\(\\mathcal{A}\\)와 적대적 손실 \\(\\mathcal{L}_{adv}\\)은 \\(\\Omega\\) 투영된 이미지 표현의 도메인 불변성을 향상시킨다.\n' +
      '\n' +
      '음수 키의 집합 \\(k_{-}\\)은 \\(q\\)을 반복한다. 우리의 InfoNCE는 코사인 유사도를 사용하여 쿼리와 키를 비교합니다. \\(\\mathcal{L}_{nce}\\) 모두 식에 포함되어 있기 때문이다. (1) 양의 키 \\(k_{+}\\)는 항상 운동량 모델 \\(\\mathcal{B}^{m}\\)과 \\(\\mathcal{P}^{m}\\) (기울기를 생성하지 않음)을 통해 인코딩된다. \\(\\mathcal{L}_{nce}\\)을 학습시키기 위해서는 \\(\\mathcal{B}\\)과 \\(\\mathcal{P}\\)이 모두 필요하다 \\(D_{n}\\)의 원래 학습 도메인 이미지와 BrAD 매핑 \\(\\Psi_{n}(I_{n})\\in\\Omega\\)을 모두 표현해야 한다. 유의할 점은 Eq의 첫 번째 \\(\\mathcal{L}_{nce}\\)입니다. (1) \\(\\mathcal{B}\\)은 각 \\(D_{n}\\)에서 직접 \\(\\Omega\\) 관련 특징을 추출하도록 가르친다. 이는 학습 후 BrAD 매핑 모델 \\(\\Psi_{n}\\)을 폐기하고 학습된 BrAD 매핑이 없는 영역에도 \\(\\mathcal{B}\\)을 적용할 수 있음을 의미한다. 각 일괄 처리가 처리 된 후 일괄 처리 이미지 \'omentum\' 표현은 원본 도메인에 따라 (원형) 대기 됩니다.\n' +
      '\n' +
      '\\[\\mathcal{Q}_{n}\\leftarrow\\mathcal{Q}_{n}\\cup\\{\\mathcal{P}^{m}(\\mathcal{B}^{ m}(\\Psi_{n}(I_{n}^{a2}))),\\mathcal{P}^{m}(\\mathcal{B}^{m}(I_{n}^{a2}))\\} \\tag{2}\\}\n' +
      '\n' +
      '이러한 방법으로 큐를 유지하면 \\(\\mathcal{F}\\)영상이 다른 \\(D_{n}\\)영상에 대한 \\(\\Omega\\) 프로젝션뿐만 아니라 \\(D_{n}\\)영상에 대한 \\(\\mathcal{B}\\)영상이 다른 \\(D_{n}\\)영상에 대한 \\(\\Omega\\)특정의 특징 집합을 일부 \\(D_{n}\\)특정의 특징 집합(예: 색상 특징)으로 보완할 수 있다. 또한 다음과 같은 적대적 손실을 사용합니다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{adv}(I_{n})=CE(\\mathcal{A}(\\mathcal{B}(\\Psi_{n}(I_{n}^{a1}))),n) \\tag{3}\\]\n' +
      '\n' +
      '여기서 \\(CE\\)은 표준 교차 엔트로피 손실이고 \\(n\\in\\{1,\\cdots,N\\}\\)은 이미지 \\(I_{n}\\)에 대한 올바른 도메인 인덱스이다. 본 논문에서는 \\(\\mathcal{L}_{adv}\\)에 대한 적대적 훈련 기법인 PyTorch의 \'two-optimizers\'를 사용한다. 각 학습 배치에서 도메인 판별기 \\(\\mathcal{A}\\)은 \\(\\mathcal{L}_{adv}\\)을 최소화하면서 \\(\\mathcal{B}\\)과 \\(\\Psi_{n}\\)의 기울기를 차단하는 반면 \\(\\mathcal{B}\\)과 \\(\\Psi_{n}\\)은 \\(-\\mathcal{L}_{adv}\\)을 최소화하면서 \\(\\mathcal{A}\\)의 기울기를 차단한다. 우리는 \\(\\mathcal{L}_{adv}\\)을 원래 도메인의 \\(\\Omega\\) 사영에만 사용하므로 \\(\\mathcal{D}\\)의 도메인 간의 직접적인 정렬이 필요하지 않다. 또한, \\(\\mathcal{L}_{adv}\\)과 \\(\\mathcal{L}_{cont}\\) 사이의 \'경쟁\'을 줄이기 위해 도메인 판별기 \\(\\mathcal{A}\\)를 \\(\\mathcal{B}\\)-생성 표현(최종 특징)에 직접 사용하고, 투영 헤드 \\(\\mathcal{P}\\)-생성 표현(\\(\\mathcal{L}_{cont}\\)에서 효율성을 위해 사용되는 임시 특징)에는 사용하지 않는다. 마지막으로, 공유 보조 BrAD 도메인 \\(\\Omega\\)을 포함하는 에지 유사 이미지를 생성하기 위해 \\(\\Psi_{n}\\) 모델을 정규화하는 BrAD 손실을 정의하며, 이는 Sec의 다른 작업에 매우 효과적임을 보여준다. 4:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\Omega}(I_{n})=||\\Psi_{n}(I_{n}^{a1})-\\mathcal{E}(I_{n}^{a1})||^{2} \\tag{4}\\]\n' +
      '\n' +
      '여기서 \\(\\mathcal{E}\\)은 Canny edge detector [4]와 같은 휴리스틱 모델이거나 HED [62]와 같은 사전 훈련된 에지 모델로서 Sec. 4.4의 변형들을 탐색하고 비교한다. 마지막으로 이미지 \\(I_{n}\\)에 대한 전체 손실은 다음과 같다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{f}(I_{n})=\\alpha_{1}\\cdot\\mathcal{L}_{cont}(I_{n})+\\alpha_{2} \\cdot\\mathcal{L}_{\\Omega}(I_{n})-\\alpha_{3}\\cdot\\mathcal{L}_{adv}(I_{n}) \\tag{5}\\]\n' +
      '\n' +
      '(\\mathcal{L}_{adv}\\) 앞의 부호는 적대적 도메인 판별기를 훈련시키기 위해 기울기를 계산할 때 양수가 된다.\n' +
      '\n' +
      '**구현 세부 정보.** 코드1은 PyTorch [11]에 있으며 [9]의 코드를 기반으로 합니다. 실험은 \\(\\alpha_{1},\\alpha_{2},\\alpha_{3}=1\\)으로 하였다. 백본 \\(\\mathcal{B}\\)은 UDG 실험에서 ResNet-18 [20] ([68]과 동일), FUDA 및 교차 벤치마크 일반화 실험에서 ResNet-50 ([26]과 동일)이었다. 크기 \\(256\\), 운동량 \\(0.9\\), 코사인 LR-스케줄 (LR 0.03에서 0.002)의 배치를 사용하고 FUDA의 경우 \\(250\\), UDG의 경우 \\(1000\\) 에폭에 대해 훈련했다([68]과 동일). \\(|\\mathcal{Q}_{n}|=min(64K,2\\cdot|D_{n}|)\\)을 설정하고 각 도메인 이미지 \\(I_{n}\\)와 그 \\(\\Omega\\) 투영(\\(\\Psi_{n}\\)에 의해 생성된 한 쌍의 (운동량) 표현만을 저장하였다. 또한, \\(\\mathcal{L}_{nce}(q,k_{+},k_{-})\\) 손실을 계산할 때 \\(k_{-}\\) 네거티브 키 집합에서 cached 버전의 \\(q\\)을 제외하는 것이 약간 유리하다는 것을 발견했다. \\(\\mathcal{A}\\)의 경우 LeakyReLU와 함께 \\(3\\)-layer MLP\\((1024,512,256)\\을 사용한 후 선형 도메인 분류기를 사용하였다. BrAD 매핑 모델 \\(\\Psi_{n}\\) 아키텍처는 PyTorch 구현 [40]에서 HED [62] 아키텍처를 사용하였다.\n' +
      '\n' +
      '각주 1: [https://github.com/leokarlin/BrAD](https://github.com/leokarlin/BrAD)에서 사용할 수 있습니다.\n' +
      '\n' +
      '## 4 Results\n' +
      '\n' +
      'BrAD 접근 방식은 훈련 중에 완전히 감독되지 않기 때문에 성능을 평가하고 다른 자체 감독 또는 소스 레이블 제한 교차 도메인 방법과 비교하기 위해 비감독 또는 제한 감독 교차 도메인 작업, 특히 UDG(Unsupervised Domain Generalization) [68] 및 FUDA(Few-shot UDA) [26, 65]를 사용했다. 또한, DomainNet(43)과 같은 대규모 레이블이 없는 교차 도메인 데이터에서 학습된 후 BrAD 및 기타 자체 감독 접근법이 보이지 않는 도메인과 보이지 않는 클래스로 일반화하는 방법을 평가한다.\n' +
      '\n' +
      '**데이터 세트**. DomainNet**[43]은 가장 크고 가장 다양하며 최근 도메인 간 벤치마크입니다. 실물, 그림, 스케치, 클립아트, 인포그래프, 퀵드로 구성된 \\(6\\) 도메인으로 구성되어 있으며, \\(345\\) 객체 클래스, 도메인당 48K - 173K 이미지, 클래스당 평균 269개의 이미지가 포함되어 있다. **PACS**[30]은 표준 도메인 일반화 벤치마크입니다. 포토, 아트, 카툰, 스케치의 \\(4\\) 도메인으로 구성되어 있으며, \\(7\\) 객체 클래스, 도메인당 2.5K 이미지, 클래스당 평균 \\(357\\) 이미지로 구성되어 있다. **VisDA**[44]는 \\(12\\) 클래스가 있는 시뮬레이션에서 실제 데이터 세트입니다. 시뮬레이션 도메인은 클래스당 50-150개의 모델, 3D 객체 모델의 인스턴스를 반복(80-480회) 3D 렌더링을 통해 생성된다. 따라서 \\(\\sim\\)1.5K 개별 개체 인스턴스로만 구성됩니다. **홈**[56]은 \\(4\\) 도메인: Art, Clipart, Product 및 Real로 구성된 비교적 작은 데이터 집합으로 \\(65\\) 클래스와 클래스당 평균 \\(60\\) 이미지만 있습니다.\n' +
      '\n' +
      '### 감독 되지 않은 도메인 일반화\n' +
      '\n' +
      'UDG 태스크는 다음과 같이 정의된다: (**i**) 소스 도메인 세트에 대한 비감독 트레이닝; (**ii**) 비감독 모델에 의해 생성된 (동결된) 피처들 위에 선형 분류기를 맞추기 위해 소스 도메인 이미지들의 작은 라벨링된 서브세트만을 사용하는 것; 및 (**iii**) 트레이닝 동안 보이지 않는, 타겟 도메인 세트에 대한 결과적인 분류기 성능을 평가하는 것. UDG 실험에서 동일한 백본 아치, 동일한 숫자를 포함하여 UDG 최신(SOTA) 방법 DIUL[68]의 프로토콜을 정확하게 따랐다. epochs 및 훈련 및 테스트에 사용되는 동일한 클래스 하위 집합이다. [68]과 동일하게 DomainNet [43]에 대해 평가하였다 (탭. 1). 및 PACS[30](탭. 2). 도메인넷에서는 클리파트, 인포그래프, 퀵드로우에서 교육하고 보이지 않는 그림, 실제 및 스케치에 대해 테스트하고 그 반대의 경우도 마찬가지입니다. PACS의 경우 다른 세 가지를 소스로 사용하여 (모든 도메인에 대해 이를 반복) leave-one-domain-out 테스트를 수행한다. 소스 레이블의 양이 소스 데이터 크기의 \\(10\\%\\)일 때 추가 전체 모델 미세 조정을 사용한 DIUL [68]과 달리 자체 감독 모델은 소스 레이블과 미세 조정되지 않았다(모든 경우에). 또한, 추가 훈련 없이 결과 피쳐를 직접 사용하는 방법에 대한 kNN 결과도 제공한다.\n' +
      '\n' +
      '탭에서 볼 수 있는 것처럼요. 1 및 Tab. 2, BrAD는 상당한 이득(선형 cls 모두에서)을 보여준다. 및 kNN 모드들에서) [68]에 걸쳐서 뿐만 아니라 다양한 SOTA 자체-감독된 사전-훈련 기준선들([68]과 정확히 동일한 방식으로 그리고 전술된 바와 같이 분류기로 조사됨)에 걸쳐서. 이는 중요한 점을 보여주는 것으로, BrAD 아이디어는 보이지 않는 대상 도메인에 대한 자체 감독 사전 훈련의 일반화를 개선하는 데 효과적인 것으로 보이며, 이러한 결과에 따르면 현재 자체 감독 SOTA 방법에는 상당히 어려운 것으로 판단된다.\n' +
      '\n' +
      '### Few-shot 비감독 도메인 적응\n' +
      '\n' +
      '가장 크고 가장 최근의 교차 도메인 데이터 세트인 DomainNet[43]을 사용하여 BrAD 접근 방식 FUDA[65, 26] 성능을 평가했다. [65]에서와 마찬가지로(그리고 일반적인 UDA 관행과 마찬가지로) 이 평가를 위해 \\(4\\) 도메인인 클리파트, 리얼, 페인팅 및 스케치 및 탭에 나열된 소스-목표 방향만 사용했다. 3.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c|c c c} \\hline \\hline Source domains & \\{Paint. \\(\\cup\\) Real \\(\\cup\\) Sketch\\} & \\{Clipart \\(\\cup\\) Info. \\(\\cup\\) Quick.\\} & \\multicolumn{3}{c}{} \\\\ Target domain & Clipart & Info. & Quick. & Painting & Real & Sketch & Overall & Avg. \\\\ \\hline \\multicolumn{8}{c}{Label Fraction 1\\%} \\\\ \\hline ERM & 6.54 & 2.96 & 5.00 & 6.68 & 6.97 & 7.25 & 5.88 & 5.89 \\\\ BYOL [17] & 6.21 & 3.48 & 4.27 & 5.00 & 8.47 & 4.42 & 5.61 & 5.30 \\\\ MoCo V2 [9, 19] & 18.85 & 10.57 & 6.32 & 11.38 & 14.97 & 15.28 & 12.12 & 12.90 \\\\ AdCo [22] & 16.16 & 12.26 & 5.65 & 11.13 & 16.53 & 17.19 & 12.47 & 13.15 \\\\ SimCLR V2 [8] & 23.51 & 15.42 & 5.29 & **20.25** & 17.84 & 18.85 & 15.46 & 16.55 \\\\ DIUL [68] & 18.53 & 10.62 & 12.65 & 14.45 & 21.68 & 21.30 & 16.56 & 16.53 \\\\ \\hline Ours (kNN) & 40.65 & 14.00 & 21.28 & 16.80 & 22.29 & 25.72 & 22.35 & 23.46 \\\\ Ours (linear cls.) & **47.26** & **16.89** & **23.74** & 20.03 & **25.08** & **31.67** & **25.85** & **27.45** \\\\ \\hline \\multicolumn{8}{c}{Label Fraction 5\\%} \\\\ \\hline ERM & 10.21 & 7.08 & 5.34 & 7.45 & 6.08 & 5.00 & 6.50 & 6.86 \\\\ BYOL [17] & 9.60 & 5.09 & 6.02 & 9.78 & 10.73 & 3.97 & 7.83 & 7.53 \\\\ MoCo V2 [9, 19] & 28.13 & 13.79 & 9.67 & 20.80 & 24.91 & 21.44 & 18.99 & 19.79 \\\\ AdCo [22] & 30.77 & 18.65 & 7.75 & 19.97 & 24.31 & 24.19 & 19.42 & 20.94 \\\\ SimCLR V2 [8] & 34.03 & 17.17 & 10.88 & 21.35 & 24.34 & 27.46 & 20.89 & 22.54 \\\\ DIUL [68] & 39.32 & 19.09 & 10.50 & 21.09 & 30.51 & 28.49 & 23.31 & 24.83 \\\\ \\hline Ours (kNN) & 55.75 & 18.15 & 26.93 & 24.29 & 33.33 & 37.54 & 31.12 & 32.66 \\\\ Ours (linear cls.) & **64.01** & **25.02** & **29.64** & **29.32** & **34.95** & **44.09** & **35.37** & **37.84** \\\\ \\hline \\multicolumn{8}{c}{Label Fraction 10\\%} \\\\ \\hline ERM & 15.10 & 9.39 & 7.11 & 9.90 & 9.19 & 5.12 & 8.94 & 9.30 \\\\ BYOL [17] & 14.55 & 8.71 & 5.95 & 9.50 & 10.38 & 4.45 & 8.69 & 8.92 \\\\ MoCo V2 [9, 19] & 32.46 & 18.54 & 8.05 & 25.35 & 29.91 & 23.71 & 21.87 & 23.05 \\\\ AdCo [22] & 32.25 & 17.96 & 11.56 & 23.35 & 29.98 & 27.57 & 22.79 & 23.78 \\\\ SimCLR V2 [8] & 37.11 & 19.87 & 12.33 & 24.01 & 30.17 & 31.58 & 24.28 & 25.84 \\\\ DIUL [68] & 35.15 & 20.88 & 15.69 & 25.90 & 33.29 & 30.77 & 26.09 & 26.95 \\\\ \\hline Ours (kNN) & 60.78 & 19.76 & 31.56 & 26.06 & 37.43 & 41.38 & 34.77 & 36.16 \\\\ Ours (linear cls.) & **68.27** & **26.60** & **34.03** & **31.08** & **38.48** & **48.17** & **38.74** & **41.10** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: DomainNet 상의 UDG에 대한 정확도(%) 결과. 모든 베이스라인 결과는 [68]로부터 취해진다. 모든 메서드는 ResNet18 백본을 사용 하 고 레이블이 지정 된 (원본 전용) 데이터에 대 한 훈련 전에 1000 에폭시에 대해 감독 되지 않게 사전 훈련 됩니다. 모든 베이스라인들은 선형 분류기를 사용한다(우리의 경우, 우리는 또한 어떠한 감독된 훈련도 이용하지 않는 kNN 결과를 포함한다). ERM은 무작위로 초기화된 ResNet18을 나타냅니다. 전체 및 Avg. 전체 테스트 데이터 정확도와 도메인별 정확도의 평균을 각각 나타낸다. 다른 도메인의 테스트 세트가 동일한 크기가 아니기 때문에 다르다. 보고된 결과는 \\(3\\) 런에 대한 평균입니다. **bold** = best, blue = second best.\n' +
      '\n' +
      '[65]에서 정의된 FUDA 프로토콜은 소스 도메인이 클래스당 단일 (\\(1\\)-shot) 또는 3개의 (\\(3\\)-shot) 라벨링된 이미지를 가지고 나머지 이미지는 라벨링되지 않은 것으로 제공된다. 반복성을 위해 [65]에서 제공한 대로 각 경우에 대해 동일한 클래스와 레이블이 지정된 샘플의 정확한 지수를 사용했다. 3. [65]의 프로토콜에 따라, 비교된 모든 방법 모델은 ImageNet 사전 훈련으로 초기화되고 변환 설정에서 작동한다. 우리 이외의 모든 방법은 훈련 중에 클래스당 각각의 \\(1\\) 또는 \\(3\\) 샘플을 사용한다. 우리의 경우, 우리는 kNN의 검색 공간으로 _추론 동안에만_ 또는 선형 분류기를 훈련하기 위해(이 몇 개의 레이블이 지정된 소스 이미지에서만) 이러한 샘플을 사용했다. 우리의 방법 이외의 모든 방법은 소스 및 타겟 도메인의 각 쌍에서 개별적으로 작업하도록 설계되었다. 따라서 비교를 포괄적으로 하기 위해 의도된 모드와 _쌍별_ 모드 모두에서 방법에 대한 결과를 포함한다. 우리의 의도된 모드에서(탭.3의 \'우리\'). 우리는 모든 도메인에 대해 단일 모델을 공동으로 훈련합니다. _pairwise_ 모드(탭의 \'ours pairwise\')입니다. 3) 각 소스-타겟 도메인 쌍에 대해 하나씩 \\(7\\)개의 개별 모델을 훈련한다. 모든 모드에서 이 방법의 경쟁 우위를 보여주는 것 외에도 탭이 생성됩니다. 도 3은 다중 도메인 트레이닝이 효율성에 있어서 분명한 이점을 갖는다는 것을 나타낸다(단일 모델 vs. \\ (7\\) 모델), 사용 용이성(쿼리 도메인을 알 필요가 없음), 성능(약 10\\%\\)이 더 좋습니다.\n' +
      '\n' +
      '각주 2: [65]의 공개된 공식 코드에 따르면, 트랜스덕티브 = 전체 도메인 데이터(라벨이 지정되지 않은 테스트 데이터 포함)를 훈련에 활용한다. 전송 설정은 성능(Sec. 4.4)에 약 \\(3-4\\%\\)을 추가합니다.\n' +
      '\n' +
      '### 도메인 및 클래스 보기 취소 일반화\n' +
      '\n' +
      '자기 지도 학습을 고려할 흥미로운 연구 질문 중 하나는 보이지 않는 영역과 수업에 얼마나 잘 일반화할 수 있는지이다. 비교적 자기 지도 모델3을 학습하는데 필요한 다양한 비표지 다중 도메인 데이터 집합(예: 비표지 도메인넷[43])에 접근할 수 있다고 가정한다. 그러나, 훈련된 모델은 새로운 비표지 도메인에서 소수의 샷 시나리오(즉, 데이터가 거의 없는 경우) 및 새로운 비표지 클래스 집합에서 추론에 사용될 필요가 있다고 가정한다. 예를 들어, 우리가 서론에서 논의한 상황을 고려해 보자. 우리의 자체 감독 시스템(일부 다중 도메인 데이터에 사전 훈련됨)은 일부 기술 매뉴얼의 독점 그래픽 스타일을 사용하여 처음으로 설명된 기술 장비를 본 후 실제 사진에서 보이지 않는 인스턴스를 인식해야 한다.\n' +
      '\n' +
      '각주 3: 자기 지도 대조적 학습 방법(우리 것 포함)의 알려진 한계 중 하나는 클래스당 상대적으로 많은 양의 _다른_ 인스턴스를 관찰해야 한다는 것이다(당연히 클래스 레이블이 없다).\n' +
      '\n' +
      '제안된 시나리오(보이는 도메인과 보이지 않는 도메인의 혼합, 그리고 대부분 보이지 않는 클래스로의 일반화)에 대한 선도적인 자체 감독 방법[66, 5, 6, 10, 6]의 처리 방식을 테스트하고 그 성능을 우리의 접근 방식과 비교하기 위해 다음 교차 데이터 세트 FUDA 일반화 실험을 수행했으며 그 결과는 Tab에 제공된다. 4. 도메인넷의 Clipart, Real, Painting 및 스케치 도메인의 전체 데이터에 대해 공식 코드와 권장되는 하이퍼 파라미터 및 백본 설정을 사용하여 모든 방법을 훈련한다. 최적화된 더 강력한 ViT 백본을 사용하는 Dino [6]을 제외한 모든 방법(우리 포함)에는 동일한 ResNet50 백본이 사용됩니다. 그런 다음 OfficeHome [56], PACS [30] 및 VisDA [44] 교차 도메인 데이터 세트에서 Sec. 4.2에 자세히 설명된 kNN 분류기와 FUDA 설정을 사용하여 결과 모델을 평가했다. 모든 경우에 클래스당 \\(1\\)-shot 및 \\(3\\)-shot 소스 도메인 예는 무작위로 샘플링되었으며 모든 방법에 대해 동일하게 유지되었다. 이 실험(샷의 샘플링)은 \\(5\\)번 반복되었고 Tab에서 반복되었다. 4 우리는 평균을 보고합니다. Sec. 4.1의 UDG 실험과 유사하게, 이러한 결과는 대중적인 자기 지도 학습 접근법에 내재된 교차 도메인 일반화의 어려움과 이러한 일반화를 개선하기 위한 BrAD의 이점을 다시 나타낸다. 부록 C에서 우리는 많은 정성적 예들을 제공한다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c|c} \\hline Target domain & Photo & Art. & Cartoon & Sketch & Avg. \\\\ \\hline \\multicolumn{6}{c}{Label Fraction 1\\%} \\\\ \\hline ERM & 10.90 & 11.21 & 14.33 & 18.83 & 13.82 \\\\ BYOL [17] & 11.20 & 14.53 & 16.21 & 10.01 & 12.99 \\\\ MoCo V2 [9, 19] & 22.97 & 15.58 & 23.65 & 25.27 & 21.87 \\\\ AdCo [22] & 26.13 & 17.11 & 22.96 & 23.37 & 22.39 \\\\ SimCLR V2 [8] & 30.94 & 17.43 & 30.16 & 25.20 & 25.93 \\\\ DIUL [68] & 27.78 & 19.82 & 27.51 & 29.54 & 26.16 \\\\ \\hline Ours (kNN) & 55.00 & **35.54** & 38.12 & 34.14 & 40.70 \\\\ Ours (linear cls.) & **61.81** & 33.57 & **43.47** & **36.37** & **43.81** \\\\ \\hline \\multicolumn{6}{c}{Label Fraction 5\\%} \\\\ \\hline ERM & 14.15 & 18.67 & 13.37 & 18.34 & 16.13 \\\\ BYOL [17] & 26.55 & 17.79 & 21.87 & 19.65 & 21.47 \\\\ MoCo V2 [9, 19] & 37.39 & 25.57 & 28.11 & 31.16 & 30.56 \\\\ AdCo [22] & 37.65 & 28.21 & 28.52 & 30.35 & 31.18 \\\\ SimCLR V2 [8] & 54.67 & 35.92 & 35.31 & 36.84 & 40.68 \\\\ DIUL [68] & 44.61 & 39.25 & 36.41 & 36.53 & 39.20 \\\\ \\hline Ours (kNN) & 58.66 & 39.11 & 45.37 & 46.11 & 47.31 \\\\ Ours (linear cls.) & **65.22** & **41.35** & **50.88** & **50.68** & **52.03** \\\\ \\hline \\multicolumn{6}{c}{Label Fraction 10\\%} \\\\ \\hline ERM & 16.27 & 16.62 & 18.40 & 12.01 & 15.82 \\\\ BYOL [17] & 27.01 & 25.94 & 20.98 & 19.69 & 23.40 \\\\ MoCo V2 [9, 19] & 44.19 & 25.85 & 33.53 & 24.97 & 32.14 \\\\ AdCo [22] & 46.51 & 30.21 & 31.45 & 22.96 & 32.78 \\\\ SimCLR V2 [8] & 54.65 & 37.65 & 46.00 & 28.25 & 41.64 \\\\ DIUL [68] & 53.37 & 39.91 & 46.41 & 30.17 & 42.47 \\\\ \\hline Ours (kNN) & 67.20 & 41.99 & 45.32 & 50.04 & 51.14 \\\\ Ours (linear cls.) & **72.17** & **44.20** & **50.01** & **55.66** & **55.51** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: PACS 상의 UDG에 대한 정확도(%) 결과. 각 타겟 도메인에 대해 다른 3개는 모두 트레이닝을 위한 소스 도메인으로 사용된다. 실행 횟수, 열 제목의 의미 등에 대한 기타 자세한 내용은 탭을 참조하십시오. 1 자막. 모든 베이스라인 결과는 [68]로부터 취해진다. **굵은** = 최상의 결과, 파란색 = 두 번째로 좋습니다.\n' +
      '\n' +
      '도메인넷에서 학습된 자체 지도 BrAD 모델의 특징 공간에서 kNN을 사용하여 얻은 PACS 데이터셋에 대한 교차 도메인 kNN 결과이다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '탭에서요 도 5는 도메인넷 데이터세트[43] 상의 FUDA 태스크[26, 65]를 사용하여 본 접근법의 상이한 컴포넌트들의 기여도를 평가한다. 실험 설정은 위의 Sec. 4.2에 설명되어 있다. 구체적으로, DomainNet의 \\(1\\)-shot / \\(3\\)-shots FUDA에서 생성된 모델의 평균 성능이 바닐라 MocoV2 [9]에서 시작하여 (i) DD: 도메인 판별기(\\(\\mathcal{A}\\)를 추가하면서 어떻게 진화하는지 보여준다. (3) - 자체적으로는 성능(\\(\\text{-}0.3/\\text{+}0.1\\)); (ii) MQ: 다중 네거티브 큐(\\(\\mathcal{Q}_{n}\\)) - 자체적으로는 \\(1\\)-샷 케이스(\\(\\text{+}4.2/\\text{+}0.4\\))에 좋은 부스트를 추가하고 DD(\\(\\text{+}3.8/\\text{+}6.0\\)와 결합했을 때 두 모드 모두에 강한 부스트(\\(\\text{+}3.8/\\text{+}6.0\\)); (iii) 휴리스틱 Canny [4] 에지 검출기 형태에서 Canny BrAD: \\(\\Psi_{n}\\)은 매우 강력한 성능 부스트(\\(\\text{+}10.9/\\text{+}11.9\\))로 이어진다. (iv) HED BrAD: \\(\\Psi_{n}\\)는 BSDS500 [1] 데이터셋에서 사전 훈련된 HED [62] 에지 검출기(\\(\\text{ 3 - BrAD\\(\\Psi_{n}\\) 모델의 _learn_ 필요성을 강조하면서, 이것은 휴리스틱 Canny BrAD(\\(\\text{+}2.8/\\text{+}2.3\\))에 비해 눈에 띄는 부스트를 도입하며, BrAD를 사용하지 않는 것과 비교하여 전반적으로(\\(\\text{+}13.7/\\text{+}14.2\\)); (vi) Canny에 의해 생성된 에지, 사전 훈련된 HED[62] 및 학습된 BrAD를 비교한 전형적인 예가 그림에 나와 있다. 3 - 알 수 있는 바와 같이, BrAD와 HED는 모두 배경 잡음을 버리지만, HED와 달리, BrAD는 하우스 윈도우, 기린 스팟 또는 사람 팔과 같은 모양 및 질감의 의미적 세부 사항을 유지하는 것을 학습한다(부록 A에 추가 예들이 제공됨); (vii) Transductive/ImageNet 사전 훈련: Sec. 4.2의 FUDA 평가에서 모든 방법에 사용되는 PCS [65]의 FUDA 실험 설정에 따르면, 훈련은 ImageNet 사전 훈련 모델에서 시작되고 전이 패러다임은 레이블이 지정되지 않은 도메인 데이터에 사용된다 - 전이 설정이 사전 훈련에 관계없이 일관되게 \\(\\sim 4\\%\\)을 추가하는 반면, ImageNet 사전 훈련은 성능에 \\(\\sim 10\\%\\)을 추가하는 더 큰 영향을 미친다는 것을 검증하였다.\n' +
      '\n' +
      '위의 모든 구성 요소를 요약하여 PCS의 실험 설정에 정렬하면 [65] 주요 FUDA 결과에 도달한다(탭 5에서 강조 표시). 또한, \\(\\Psi_{n}\\)으로 사용되는 HED 모델 [62]의 BSDS500 [1] 데이터셋 사전 훈련에 대한 의존성이 크지 않음을 확인하였다. 구체적으로, HED 구조를 갖는 \\(\\Psi_{n}\\) BrAD 모델을 무작위로 초기화하고, \\(\\mathcal{L}_{\\Omega}\\) 손실(Eq)에서 \\(\\mathcal{E}\\)으로 사용된 BSDS500 사전 훈련 HED 모델을 대체하였다. (4)) 간단한 블러링된 캐니 에지 맵(보충문의 더 상세한 내용)을 포함하는 것을 특징으로 하는 방법. 이렇게 하면 시스템 어디에서나 BSDS500 미리 훈련 된 가중치가 없습니다. 표 5의 해당 "학습된*" 행에서 볼 수 있듯이 HED의 BSDS500 사전 훈련 없이 우리의 접근법이 잘 작동할 수 있음을 나타내는 최종 결과(\\(\\text{-}1.2/\\text{+}1.0\\))에는 거의 변화가 없다. 자세한 내용은 부록 B를 참조하십시오. 마지막으로 BrAD의 도입으로 인해 관찰된 강한 이득을 확인하였다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline Method & OfficeHome & PACS & VisDA \\\\ \\hline Dino [6] & 12.41 / 16.97 & 30.90 / 34.67 & 25.02 / 28.14 \\\\ SWAV [5] & 13.26 / 17.80 & 31.14 / 33.09 & 25.65 / 29.26 \\\\ SimSiam [10] & 13.67 / 18.27 & 30.24 / 32.27 & 24.70 / 28.80 \\\\ BarlowTwins [66] & 17.86 / 24.06 & 41.18 / 46.18 & 25.34 / 30.47 \\\\ MocoV2 [9] & 17.64 / 22.63 & 49.00 / 54.25 & 30.34 / 36.10 \\\\ \\hline Ours & **21.79 / 28.21** & **55.61 / 63.00** & **32.98 / 40.22** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 1-샷/3-샷 정확도(%) 교차 데이터세트 결과. 모델은 DomainNet에서 처음부터 훈련되고 OfficeHome, PACS 및 VisDA에서 테스트된다. 우리는 샷을 랜덤화하는 평균 \\(5\\) 이상의 실행을 보고한다. **bold** = best, blue = second best.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c|c} \\hline \\hline Source domain & Real & Real & Real & Painting & Painting & Clipart & Sketch & \\multirow{2}{*}{Avg.} \\\\ Target domain & Clipart & Painting & Sketch & Clipart & Real & Sketch & Painting & Avg. \\\\ \\hline Source-Only baseline [65] & 18.4 / 30.2 & 30.6 / 44.2 & 16.7 / 25.7 & 16.2 / 24.6 & 28.9 / 49.8 & 12.7 / 24.2 & 10.5 / 23.2 & 19.1 / 31.7 \\\\ MME [46] & 13.8 / 22.8 & 29.2 / 46.5 & 9.7 / 14.5 & 16.0 / 25.1 & 26.0 / 50.0 & 13.4 / 20.1 & 14.4 / 24.9 & 17.5 / 29.1 \\\\ CDAN [36] & 16.0 / 30.0 & 25.7 / 40.1 & 12.9 / 21.7 & 12.6 / 21.4 & 19.5 / 40.8 & 7.20 / 17.1 & 8.00 / 19.7 & 14.6 / 27.3 \\\\ MDDIA [23] & 18.0 / 41.4 & 30.6 / 50.7 & 15.9 / 37.4 & 15.4 / 31.4 & 27.4 / 52.9 & 9.30 / 23.1 & 10.2 / 24.1 & 18.1 / 37.3 \\\\ CAN [24] & 18.3 / 28.1 & 22.1 / 33.5 & 16.7 / 25.0 & 13.2 / 24.7 & 23.9 / 46.9 & 11.1 / 23.3 & 12.1 / 20.1 & 16.8 / 28.8 \\\\ CDS+SRDC+ENT [26] & 23.1 / 36.6 & 40.0 / 54.0 & 22.2 / 35.5 & 24.1 / 38.1 & 35.1 / 57.6 & 18.8 / 35.4 & 25.2 / 45.1 & 26.9 / 43.2 \\\\ CDS+MME+ENT [26] & 35.4 / 47.4 & 36.7 / 52.8 & 33.4 / 43.2 & 25.6 / 41.2 & 29.4 / 56.4 & 19.3 / 37.5 & 22.5 / 41.1 & 28.9 / 45.6 \\\\ PCS [65] & 39.0 / 45.2 & 51.7 / 59.1 & 39.8 / 41.9 & 26.4 / 41.0 & 38.8 / **66.6** & 23.7 / 31.9 & 23.6 / 37.4 & 34.7 / 46.1 \\\\ \\hline Ours pairwise (kNN) & 43.6 / 49.5 & 50.4 / 57.0 & 41.7 / 47.9 & 35.9 / 41.0 & 44.2 / 60.7 & 34.5 / 42.4 & 36.1 / 45.5 & 40.9 / 49.1 \\\\ Ours pairwise (linear cls.) & 44.0 / 51.4 & 50.1 / 58.9 & 47.0 / 55.6 & 35.7 / 42.5 & 44.5 / 62.1 & 35.3 / 45.0 & 35.65 / 45.0 & 41.8 / 51.5 \\\\ \\hline Ours (kNN) & 46.4 / 57.6 & 52.0 / 59.4 & 50.5 / 58.9 & 45.1 / 55.3 & **49.3** / 61.3 & **48.3** / 56.4 & 49.0 / 59.3 & 48.6 / 58.3 \\\\ Ours (linear cls.) & **48.6 / 60.6** & **55.1 / 62.8** & **52.8 / 61.6** & **44.6 / 56.6** & 47.8 / 63.6 & 47.9 / **59.8** & **51.0 / 61.0** & **49.7 / 60.8** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: DomainNet 상의 FUDA 태스크[65]에 대한 1-샷/3-샷 정확도(%) 결과. CDS[26]를 제외한 모든 베이스라인 결과는 [65]로부터 취해진다. CDS 결과는 저자가 친절하게 제공했으며 [65]에 보고된 결과보다 높다. **굵은** = 최상의 결과, 파란색 = 두 번째로 좋습니다.\n' +
      '\n' +
      '모델 \\(\\Psi_{n}\\)은 ImageNet 전처리 및 변환 설정으로 사라지지 않는다. 탭의 해당 행에서 볼 수 있는 대로입니다. 5, Canny 및 frozen HED BrAD 변이체는 모두 최대 \\(2.9\\%\\) (no-BrAD 모드 이상)의 중간 이득을 유지하는 반면, 학습된 \\(\\Psi_{n}\\) BrAD를 사용한 전체 방법은 예상대로 큰 이득(\\(+9.9/+7.2\\))을 유지한다.\n' +
      '\n' +
      '## 5 결론 및 제한 사항\n' +
      '\n' +
      '본 논문에서는 이미지-이미지 매핑을 비교적 쉽게 학습할 수 있는 학습된 보조 브릿지 도메인인 BrAD 도메인에 모든 도메인을 의미적으로 정렬(특징 공간 내)하는 새로운 자기 지도 교차 도메인 학습 방법을 제안하였다. 우리는 에지 정규화된 BrAD의 특별한 사례를 탐구했는데, 특히 BrAD를 에지 맵과 같은 이미지의 영역으로 몰아넣었다. 이 구현에서는 FUDA 및 UDG와 같은 중요한 제한된 소스 레이블 태스크와 잠재적으로 보이지 않는 도메인과 클래스에 대한 교차 도메인 벤치마크 간의 일반화 태스크에 대해 제안된 접근법의 상당한 이점을 보여주었다. 우리는 이러한 작업에 대해 이전의 감독되지 않은 방법과 부분적으로 감독된 방법에 비해 상당한 개선을 관찰했다. 향후 작업에는 일반적으로 대조 SSL에 대한 잠재적으로 유용한 증대로 여기에서 사용되는 에지 유사 변환에 대한 탐색도 포함될 수 있다.\n' +
      '\n' +
      '현재 논문의_제한 사항_은 다음과 같습니다. (**i**) 가장 간단한 BrAD 구성 중 하나인 에지 유사 브리지 도메인에만 의도적으로 초점을 맞추는 것입니다. 자연스럽게 이것은 예를 들어 색상과 같은 비 에지 관련 특징을 나타내는 상대적 중요성을 낮추는 것과 같은 한계를 극복한다. 따라서, 다른 비-에지 브리지 도메인을 탐색하는 것은 향후 작업에 중요한 주제이다; (**ii**) 우리의 현재 접근법은 매우 유용하지만 단일 SSL 방법, 즉 MoCo[9] 위에 구축된다. 직접 확장은 [6]의 SSL 방법을 사용하여 비전 트랜스포머 백본 위에 우리의 접근법을 사용할 수 있거나 더 광범위하게 사용하여 모든 SSL 방법에 적용할 수 있다. (**iii**) 마지막으로 사전 훈련에서 완전히 감독되지 않은 우리의 접근법은 학습된 표현 공간에서 어떤 의미 클래스가 형성되는지에 대한 제어가 부족하며, 이는 레이블이 지정되지 않은 데이터에서 인스턴스 수가 다른 측면에서 과소 표현된 클래스를 놓칠 수 있는 대부분의 현재 SSL 기술과 공유되는 일반적인 문제이다. 후속 작업에서 이를 해결하는 것은 제로 샷 또는 소수의 샷 프라이밍의 일부 형태를 통해 또는 거친 라벨[3]로 훈련함으로써 이러한 제어를 추가하는 것을 포함할 수 있다.\n' +
      '\n' +
      '그림 3: \\(\\Psi_{n}\\): (i) 캐니는 많은 관련이 없는 에지를 가진 잡음이 있는 이미지를 생성하며, (ii) HED는 대부분 물체를 윤곽으로 하고 중요한 내부 텍스처를 폐기하며, (iii) 학습된 \\(\\Psi_{n}\\)은 대부분의 잡음을 폐기하면서 유익한 에지와 미세한 내부 디테일을 유지한다. 더 많은 예들은 보충에서 찾을 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline  & & BrAD & Transd- & ImageNet & & \\\\ DD & MQ & \\(\\Psi_{n}\\) & active & pretrained & 1-shot & 3-shots \\\\ \\hline - & - & - & - & - & 16.8 & 21.9 \\\\ ✓ & - & - & - & - & 16.5 & 22.0 \\\\ - & ✓ & - & - & - & 21.0 & 22.3 \\\\ ✓ & ✓ & - & - & - & 20.6 & 27.9 \\\\ ✓ & ✓ & Canny & - & - & 31.5 & 39.8 \\\\ ✓ & ✓ & HED & - & - & 29.8 & 38.0 \\\\ ✓ & ✓ & learned & - & - & 34.3 & 42.1 \\\\ ✓ & ✓ & learned & ✓ & - & 37.8 & 47.1 \\\\ ✓ & ✓ & learned & - & ✓ & 44.1 & 55.2 \\\\ \\hline ✓ & ✓ & learned & ✓ & ✓ & **48.6** & 58.3 \\\\ \\hline ✓ & ✓ & learned* & ✓ & ✓ & 47.4 & **59.3** \\\\ ✓ & ✓ & - & ✓ & ✓ & 38.7 & 51.1 \\\\ ✓ & ✓ & Canny & ✓ & ✓ & 41.4 & 52.7 \\\\ ✓ & ✓ & HED & ✓ & ✓ & 41.6 & 51.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: knn 분류기가 있는 Domain-Net 데이터세트에서 FUDA 태스크를 사용한 BrAD 절제 연구. **bold** = best, blue = second best. 강조 표시된 행은 Sec. 4.2 설정의 전체 방법입니다. "학습된*"은 HED 사전 훈련된 가중치를 사용하지 않는 우리의 전체 방법이다.\n' +
      '\n' +
      '이 자료는 계약 번호 FA8750-19-C-1001에 따라 방위고등연구사업청(DARPA)에서 지원하는 작업을 기반으로 한다. 이 자료에서 표현된 의견, 결과 및 결론 또는 권장 사항은 저자(들)의 것이며 DARPA의 견해를 반드시 반영하는 것은 아니다. Raja Giryes는 ERC-StG grant No. 757497 (SPADE)에 의해 지원되었다. 디나 카타비는 MIT-IBM 왓슨 AI 연구소의 자금 지원을 받았다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour detection and hierarchical image segmentation. _IEEE Trans. Pattern Anal. Mach. Intell._, 33(5):898-916, May 2011.\n' +
      '* [2] Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, July 2017.\n' +
      '* [3] Guy Bukchin, Eli Schwartz, Kate Saenko, Ori Shahar, Rogerio Feris, Raja Giryes, and Leonid Karlinsky. Fine-grained angular contrastive learning with coarse labels. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 8730-8740, June 2021.\n' +
      '* [4] John Canny. A computational approach to edge detection. _IEEE Transactions on pattern analysis and machine intelligence_, (6):679-698, 1986.\n' +
      '* [5] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.\n' +
      '* [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In _International Conference on Machine Learning, (ICML)_, 2020.\n' +
      '* [8] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big self-supervised models are strong semi-supervised learners. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.\n' +
      '* [9] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. _arXiv preprint arXiv:2003.04297_, 2020.\n' +
      '* [10] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 15750-15758, 2021.\n' +
      '* [11] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A matlab-like environment for machine learning. In _BigLearn, NIPS Workshop_, 2011.\n' +
      '* [12] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with convolutional neural networks. In _Advances in neural information processing systems (NIPS)_, pages 766-774, 2014.\n' +
      '* [13] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In _International conference on Machine Learning (ICML)_, pages 1180-1189. PMLR, 2015.\n' +
      '* [14] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. _The journal of machine learning research_, 17(1):2096-2030, 2016.\n' +
      '* [15] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In _International Conference on Learning Representations, (ICLR)_, 2018.\n' +
      '* [16] Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Domain adaptation for object recognition: An unsupervised approach. In _International Conference on Computer Vision (ICCV)_, pages 999-1006. IEEE, 2011.\n' +
      '* 자체 감독 학습에 대 한 새로운 접근 방식입니다. _NeurIPS(Neural Information Processing Systems)_ 에서, 2020.\n' +
      '* [18] Michael Gutmann and Aapo Hyvarinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In _Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics_, pages 297-304, 2010.\n' +
      '* [19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 9729-9738, 2020.\n' +
      '* [20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2016.\n' +
      '* [21] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In _International conference on machine learning (ICML)_, pages 1989-1998. PMLR, 2018.\n' +
      '* [22] Qianjiang Hu, Xiao Wang, Wei Hu, and Guo-Jun Qi. Adco: Adversarial contrast for efficient learning of unsupervised representations from self-trained negative adversaries. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1074-1083, 2021.\n' +
      '* [23] X. Jiang, Q. Lao, S. Matwin, and M. Havaei. Implicit class-conditioned domain alignment for unsupervised domain adaptation. 2020.\n' +
      '\n' +
      '* [24] G. Kang, L. Jiang, Y. Yang, and A. G. Hauptmann. Contrastive adaptation network for unsupervised domain adaptation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 4893-4902, 2019.\n' +
      '* [25] Qi Kang, Siya Yao, MengChu Zhou, Kai Zhang, and Abdullah Abusorrah. Enhanced subspace distribution matching for fast visual domain adaptation. _IEEE Transactions on Computational Social Systems_, 7(4):1047-1057, 2020.\n' +
      '* [26] Donghyun Kim, Kuniaki Saito, Tae-Hyun Oh, Bryan A Plummer, Stan Sclaroff, and Kate Saenko. Cds: Cross-domain self-supervised pre-training. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 9123-9132, 2021.\n' +
      '* [27] Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash. Mean shift for self-supervised learning. _arXiv preprint arXiv:2105.07269_, 2021.\n' +
      '* [28] Klemen Kotar, Gabriel Ilharco, Ludwig Schmidt, Kiana Ehsani, and Roozbeh Mottaghi. Contrasting contrastive self-supervised representation learning pipelines. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9949-9959, 2021.\n' +
      '* [29] Seunghun Lee, Sunghyun Cho, and Sunghoon Im. Dranet: Disentangling representation and adaptation networks for unsupervised cross-domain adaptation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 15252-15261, June 2021.\n' +
      '* [30] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Deeper, broader and artier domain generalization. In _Proceedings of the IEEE International Conference on Computer Vision (ICCV)_, Oct 2017.\n' +
      '* [31] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial feature learning. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 5400-5409, 2018.\n' +
      '* [32] Haoliang Li, YuFei Wang, Renjie Wan, Shiqi Wang, Tie-Qiang Li, and Alex C Kot. Domain generalization for medical imaging classification with linear-dependency regularization. _NeurIPS_, 2020.\n' +
      '* [33] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 624-639, 2018.\n' +
      '* [34] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In _International Conference on Machine Learning (ICML)_, pages 6028-6039. PMLR, 2020.\n' +
      '* [35] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In _International Conference on Machine Learning (ICML)_, pages 97-105. PMLR, 2015.\n' +
      '* [36] M. Long, Z. Cao, J. Wang, and M. Jordan. Conditional adversarial domain adaptation. _Advances in Neural Information Processing Systems_, pages 1640-1650, 2018.\n' +
      '* [37] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation with residual transfer networks. In _Proceedings of the 30th International Conference on Neural Information Processing Systems (NIPS)_, pages 136-144, 2016.\n' +
      '* [38] Adrian Lopez-Rodriguez and Krystian Mikolajczyk. Desc: Domain adaptation for depth estimation via semantic consistency. _arXiv preprint arXiv:2009.01579_, 2020.\n' +
      '* [39] Zak Murez, Soheil Kolouri, David Kriegman, Ravi Ramamoorthi, and Kyungnam Kim. Image to image translation for domain adaptation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2018.\n' +
      '* [40] Simon Niklaus. A reimplementation of HED using PyTorch. [https://github.com/sniklaus/pytorch-hed](https://github.com/sniklaus/pytorch-hed), 2018.\n' +
      '* [41] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In _European Conference on Computer Vision (ECCV)_, pages 69-84. Springer, 2016.\n' +
      '* [42] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In _Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)_, pages 2536-2544, 2016.\n' +
      '* [43] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In _Proceedings of the IEEE International Conference on Computer Vision (ICCV)_, pages 1406-1415, 2019.\n' +
      '* [44] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The visual domain adaptation challenge, 2017.\n' +
      '* [45] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. _arXiv preprint arXiv:1911.08731_, 2019.\n' +
      '* [46] K. Saito, D. Kim, S. Sclaroff, T. Darrell, and K. Saenko. Semi-supervised domain adaptation via minimax entropy. In _Proceedings of the IEEE International Conference on Computer Vision (ICCV)_, pages 8050-8058, 2019.\n' +
      '* [47] Kuniaki Saito, Donghyun Kim, Stan Sclaroff, and Kate Saenko. Universal domain adaptation through self supervision. _Advances in Neural Information Processing Systems_, 33, 2020.\n' +
      '* [48] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier discrepancy for unsupervised domain adaptation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2018.\n' +
      '* [49] Swami Sankaranarayanan, Yogesh Balaji, Carlos D. Castillo, and Rama Chellappa. Generate to adapt: Aligning domains using generative adversarial networks. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2018.\n' +
      '* [50] Jian Shen, Yanru Qu, Weinan Zhang, and Yong Yu. Wasserstein distance guided representation learning for domain adaptation. In _Thirty-Second AAAI Conference on Artificial Intelligence_, 2018.\n' +
      '* [51] Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Joshua Susskind, Wenda Wang, and Russell Webb. Learning from simulated and unsupervised images through adversarial training. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, July 2017.\n' +
      '* [52] Baochen Sun, Jiashi Feng, and Kate Saenko. Correlation alignment for unsupervised domain adaptation. In _Domain Adaptation in Computer Vision Applications_, pages 153-171. Springer, 2017.\n' +
      '* [53] Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A Efros. Unsupervised domain adaptation through self-supervision. _arXiv preprint arXiv:1909.11825_, 2019.\n' +
      '* [54] Ajinkya Tejankar, Soroush Abbasi Koohpayegani, Vipin Pillai, Paolo Favaro, and Hamed Pirsiavash. Isd: Self-supervised learning by iterative similarity distillation. In _Conceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9609-9618, 2021.\n' +
      '* [55] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, July 2017.\n' +
      '* [56] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5018-5027, 2017.\n' +
      '* [57] Guangrun Wang, Keze Wang, Guangcong Wang, Phillip HS Torr, and Liang Lin. Solving inefficiency of self-supervised representation learning. _arXiv preprint arXiv:2104.08760_, 2021.\n' +
      '* [58] Rui Wang, Zuxuan Wu, Zejia Weng, Jingjing Chen, Guo-Jun Qi, and Yu-Gang Jiang. Cross-domain contrastive learning for unsupervised domain adaptation. _arXiv preprint arXiv:2106.05528_, 2021.\n' +
      '* [59] Yufei Wang, Haoliang Li, and Alex C Kot. Heterogeneous domain generalization via domain mixup. In _ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 3622-3626. IEEE, 2020.\n' +
      '* [60] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer. Masked feature prediction for self-supervised visual pre-training. _arXiv preprint arXiv:2112.09133_, 2021.\n' +
      '* [61] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3733-3742, 2018.\n' +
      '* [62] Saining Xie and Zhuowen Tu. Holistically-nested edge detection. In _Proceedings of the IEEE International Conference on Computer Vision (ICCV)_, December 2015.\n' +
      '* [63] Shaoan Xie, Zibin Zheng, Liang Chen, and Chuan Chen. Learning semantic representations for unsupervised domain adaptation. In _International conference on Machine Learning (ICML)_, pages 5423-5432. PMLR, 2018.\n' +
      '* [64] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Unsupervised domain adaptation without source data by casting a bait. _arXiv preprint arXiv:2010.12427_, 2020.\n' +
      '* [65] X. Yue, Z. Zheng, S. Zhang, Y. Gao, T. Darrell, K. Keutzer, and A. Sangiovanni-Vincentelli. Prototypical cross-domain self-supervised learning for few-shot unsupervised domain adaptation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2021.\n' +
      '* [66] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In _International Conference on Machine Learning (ICML)_, 2021.\n' +
      '* [67] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In _European conference on computer vision (ECCV)_, pages 649-666. Springer, 2016.\n' +
      '* [68] Xingxuan Zhang, Linjun Zhou, Renzhe Xu, Peng Cui, Zheyan Shen, and Haoxin Liu. Domain-irrelevant representation learning for unsupervised domain generalization. _arXiv preprint arXiv:2107.06219_, 2021.\n' +
      '* [69] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain adaptation. In _International Conference on Machine Learning_, pages 7404-7413. PMLR, 2019.\n' +
      '* [70] Junbao Zhuo, Shuhui Wang, Weigang Zhang, and Qingming Huang. Deep unsupervised convolutional domain adaptation. In _Proceedings of the 25th ACM International Conference on Multimedia_, pages 261-269, 2017.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '도메인Net 데이터 세트에 대해 학습 된 감독 된 BrAD 모델입니다.\n' +
      '\n' +
      '그림 5: \\(\\Psi_{n}\\)의 다양한 선택에 대한 에지 이미지. 이미지는 PACS 데이터세트의 Real 및 Art 도메인으로부터 취해진다[30].\n' +
      '\n' +
      '그림 6: \\(\\Psi_{n}\\)의 다양한 선택에 대한 에지 이미지. 이미지는 PACS 데이터세트의 카툰 및 스케치 도메인으로부터 취해진다[30].\n' +
      '\n' +
      '그림 8: 데모에서 랜덤 쿼리 예제입니다. 각 쿼리 이미지(PACS)에 대해 사진, 아트/페인팅, 카툰, 스케치 등 각 PACS 도메인의 전체 이미지 집합 중 상위 \\(5\\) 이미지 일치도를 보여준다. 매칭은 DomainNet 데이터를 사용하여 학습된 자체 감독 BrAD 모델을 사용하여 얻는다. 올바른 수업은 퀴타입니다. 각 이미지 아래의 텍스트는 PACS 데이터세트에서 해당 이미지의 그라운드 트루스 클래스이다.\n' +
      '\n' +
      '그림 7: 데모에서 랜덤 쿼리 예제입니다. 각 쿼리 이미지(PACS)에 대해 사진, 아트/페인팅, 카툰, 스케치 등 각 PACS 도메인의 전체 이미지 집합 중 상위 \\(5\\) 이미지 일치도를 보여준다. 매칭은 DomainNet 데이터를 사용하여 학습된 자체 감독 BrAD 모델을 사용하여 얻는다. 올바른 수업은 개입니다. 각 이미지 아래의 텍스트는 PACS 데이터세트에서 해당 이미지의 그라운드 트루스 클래스이다.\n' +
      '\n' +
      '그림 10: 데모에서 랜덤 쿼리 예제입니다. 각 쿼리 이미지(PACS)에 대해 사진, 아트/페인팅, 카툰, 스케치 등 각 PACS 도메인의 전체 이미지 집합 중 상위 \\(5\\) 이미지 일치도를 보여준다. 매칭은 DomainNet 데이터를 사용하여 학습된 자체 감독 BrAD 모델을 사용하여 얻는다. 올바른 수업은 기린입니다. 각 이미지 아래의 텍스트는 PACS 데이터세트에서 해당 이미지의 그라운드 트루스 클래스이다.\n' +
      '\n' +
      '그림 9: 데모에서 랜덤 쿼리 예제입니다. 각 쿼리 이미지(PACS)에 대해 사진, 아트/페인팅, 카툰, 스케치 등 각 PACS 도메인의 전체 이미지 집합 중 상위 \\(5\\) 이미지 일치도를 보여준다. 매칭은 DomainNet 데이터를 사용하여 학습된 자체 감독 BrAD 모델을 사용하여 얻는다. 올바른 수업은 코끼리입니다. 각 이미지 아래의 텍스트는 PACS 데이터세트에서 해당 이미지의 그라운드 트루스 클래스이다.\n' +
      '\n' +
      '그림 11: 데모에서 랜덤 쿼리 예제입니다. 각 쿼리 이미지(PACS)에 대해 사진, 아트/페인팅, 카툰, 스케치 등 각 PACS 도메인의 전체 이미지 집합 중 상위 \\(5\\) 이미지 일치도를 보여준다. 매칭은 DomainNet 데이터를 사용하여 학습된 자체 감독 BrAD 모델을 사용하여 얻는다. 올바른 수업은 개입니다. 각 이미지 아래의 텍스트는 PACS 데이터세트에서 해당 이미지의 그라운드 트루스 클래스이다.\n' +
      '\n' +
      '그림 12: 데모에서 랜덤 쿼리 예제입니다. 각 쿼리 이미지(PACS)에 대해 사진, 아트/페인팅, 카툰, 스케치 등 각 PACS 도메인의 전체 이미지 집합 중 상위 \\(5\\) 이미지 일치도를 보여준다. 매칭은 DomainNet 데이터를 사용하여 학습된 자체 감독 BrAD 모델을 사용하여 얻는다. 올바른 수업은 기린입니다. 각 이미지 아래의 텍스트는 PACS 데이터세트에서 해당 이미지의 그라운드 트루스 클래스이다.\n' +
      '\n' +
      '그림 14: 데모에서 랜덤 쿼리 예제입니다. 각 쿼리 이미지(PACS)에 대해 사진, 아트/페인팅, 카툰, 스케치 등 각 PACS 도메인의 전체 이미지 집합 중 상위 \\(5\\) 이미지 일치도를 보여준다. 매칭은 DomainNet 데이터를 사용하여 학습된 자체 감독 BrAD 모델을 사용하여 얻는다. 올바른 수업은 말입니다. 각 이미지 아래의 텍스트는 PACS 데이터세트에서 해당 이미지의 그라운드 트루스 클래스이다.\n' +
      '\n' +
      '그림 13: 데모에서 랜덤 쿼리 예제입니다. 각 쿼리 이미지(PACS)에 대해 사진, 아트/페인팅, 카툰, 스케치 등 각 PACS 도메인의 전체 이미지 집합 중 상위 \\(5\\) 이미지 일치도를 보여준다. 매칭은 DomainNet 데이터를 사용하여 학습된 자체 감독 BrAD 모델을 사용하여 얻는다. 올바른 수업은 하우스입니다. 각 이미지 아래의 텍스트는 PACS 데이터세트에서 해당 이미지의 그라운드 트루스 클래스이다.\n' +
      '\n' +
      '그림 16: 데모에서 랜덤 쿼리 예제입니다. 각 쿼리 이미지(PACS)에 대해 사진, 아트/페인팅, 카툰, 스케치 등 각 PACS 도메인의 전체 이미지 집합 중 상위 \\(5\\) 이미지 일치도를 보여준다. 매칭은 DomainNet 데이터를 사용하여 학습된 자체 감독 BrAD 모델을 사용하여 얻는다. 올바른 수업은 에티판트입니다. 각 이미지 아래의 텍스트는 PACS 데이터세트에서 해당 이미지의 그라운드 트루스 클래스이다.\n' +
      '\n' +
      '그림 15: 데모에서 랜덤 쿼리 예제입니다. 각 쿼리 이미지(PACS)에 대해 사진, 아트/페인팅, 카툰, 스케치 등 각 PACS 도메인의 전체 이미지 집합 중 상위 \\(5\\) 이미지 일치도를 보여준다. 매칭은 DomainNet 데이터를 사용하여 학습된 자체 감독 BrAD 모델을 사용하여 얻는다. 올바른 수업은 코끼리입니다. 각 이미지 아래의 텍스트는 PACS 데이터세트에서 해당 이미지의 그라운드 트루스 클래스이다.\n' +
      '\n' +
      '그림 17: 데모에서 랜덤 쿼리 예제입니다. 각 쿼리 이미지(PACS)에 대해 사진, 아트/페인팅, 카툰, 스케치 등 각 PACS 도메인의 전체 이미지 집합 중 상위 \\(5\\) 이미지 일치도를 보여준다. 매칭은 DomainNet 데이터를 사용하여 학습된 자체 감독 BrAD 모델을 사용하여 얻는다. 올바른 수업은 기린입니다. 각 이미지 아래의 텍스트는 PACS 데이터세트에서 해당 이미지의 그라운드 트루스 클래스이다.\n' +
      '\n' +
      '그림 18: 데모에서 랜덤 쿼리 예제입니다. 각 쿼리 이미지(PACS)에 대해 사진, 아트/페인팅, 카툰, 스케치 등 각 PACS 도메인의 전체 이미지 집합 중 상위 \\(5\\) 이미지 일치도를 보여준다. 매칭은 DomainNet 데이터를 사용하여 학습된 자체 감독 BrAD 모델을 사용하여 얻는다. 올바른 수업은 말입니다. 각 이미지 아래의 텍스트는 PACS 데이터세트에서 해당 이미지의 그라운드 트루스 클래스이다.\n' +
      '\n' +
      '그림 19: 데모에서 랜덤 쿼리 예제입니다. 각 쿼리 이미지(PACS)에 대해 사진, 아트/페인팅, 카툰, 스케치 등 각 PACS 도메인의 전체 이미지 집합 중 상위 \\(5\\) 이미지 일치도를 보여준다. 매칭은 DomainNet 데이터를 사용하여 학습된 자체 감독 BrAD 모델을 사용하여 얻는다. 올바른 수업은 하우스입니다. 각 이미지 아래의 텍스트는 PACS 데이터세트에서 해당 이미지의 그라운드 트루스 클래스이다.\n' +
      '\n' +
      '그림 20: 데모에서 랜덤 쿼리 예제입니다. 각 쿼리 이미지(PACS)에 대해 사진, 아트/페인팅, 카툰, 스케치 등 각 PACS 도메인의 전체 이미지 집합 중 상위 \\(5\\) 이미지 일치도를 보여준다. 매칭은 DomainNet 데이터를 사용하여 학습된 자체 감독 BrAD 모델을 사용하여 얻는다. 올바른 수업은 개입니다. 각 이미지 아래의 텍스트는 PACS 데이터세트에서 해당 이미지의 그라운드 트루스 클래스이다.\n' +
      '\n' +
      '그림 21: 데모에서 랜덤 쿼리 예제입니다. 각 쿼리 이미지(PACS)에 대해 사진, 아트/페인팅, 카툰, 스케치 등 각 PACS 도메인의 전체 이미지 집합 중 상위 \\(5\\) 이미지 일치도를 보여준다. 매칭은 DomainNet 데이터를 사용하여 학습된 자체 감독 BrAD 모델을 사용하여 얻는다. 올바른 수업은 말입니다. 각 이미지 아래의 텍스트는 PACS 데이터세트에서 해당 이미지의 그라운드 트루스 클래스이다.\n' +
      '\n' +
      '그림 22: 데모에서 랜덤 쿼리 예제입니다. 각 쿼리 이미지(PACS)에 대해 사진, 아트/페인팅, 카툰, 스케치 등 각 PACS 도메인의 전체 이미지 집합 중 상위 \\(5\\) 이미지 일치도를 보여준다. 매칭은 DomainNet 데이터를 사용하여 학습된 자체 감독 BrAD 모델을 사용하여 얻는다. 올바른 수업은 기타입니다. 각 이미지 아래의 텍스트는 PACS 데이터세트에서 해당 이미지의 그라운드 트루스 클래스이다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>