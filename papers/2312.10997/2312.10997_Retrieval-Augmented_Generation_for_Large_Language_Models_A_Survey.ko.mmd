# 대규모 언어 모델을 위한 검색 확장 생성: 설문 조사

 윤판가오 \({}^{1}\)

윤시옹 \({}^{2}\)

Xinyu Gao \({}^{2}\)

강샹쟈 \({}^{2}\)

Jinliu Pan \({}^{2}\)

Yuxi Bi \({}^{3}\)

Yi Dai\({}^{1}\)

Jiawei Sun\({}^{1}\)&Haofen Wang \({}^{1,3}\)

통신저자 \({}^{1}\)Shanghai Institute of Intelligent Autonomous Systems, Tongji University \({}^{2}\)Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University \({}^{3}\) Design and Innovation, Tongji University

gaoyunfan1602@gmail.com

###### Abstract

대형 언어 모델(LLM)은 강력한 기능을 발휘하지만 환각, 느린 지식 업데이트, 답변의 투명성 부족과 같은 실제 응용 분야에서 여전히 어려움에 직면해 있다. RAG(Retrieval-Augmented Generation)는 LLM으로 질문에 답하기 전에 외부 지식 베이스로부터 관련 정보를 검색하는 것을 말한다. RAG는 특히 지식 집약적 작업의 경우 답변 정확도를 크게 향상시키고 모델 환각을 감소시키는 것으로 입증되었다. 출처를 인용함으로써 사용자는 답변의 정확성을 검증하고 모델 출력에 대한 신뢰를 높일 수 있다. 또한 지식 업데이트 및 도메인별 지식 도입을 용이하게 합니다. RAG는 LLM의 매개변수화된 지식과 매개변수화되지 않은 외부 지식 기반을 효과적으로 결합하여 대규모 언어 모델을 구현하는 가장 중요한 방법 중 하나이다. 본 논문은 LLMs 시대의 RAG의 발전 패러다임을 요약하고, 나이브 RAG, 어드밴스드 RAG, 모듈러 RAG의 세 가지 패러다임을 요약한다. 그런 다음 RAG의 세 가지 주요 구성 요소인 검색기, 생성기 및 확장 방법과 각 구성 요소의 핵심 기술을 요약하고 정리한다. 또한, RAG 모델의 효과성을 평가하는 방법에 대해 논의하고, RAG에 대한 두 가지 평가 방법을 소개하고, 평가를 위한 핵심 메트릭과 능력을 강조하고, 최신 자동 평가 프레임워크를 제시한다. 마지막으로, RAG.1의 수직적 최적화, 수평적 확장성, 기술적 스택과 생태계의 세 가지 측면에서 향후 연구 방향을 소개한다.

각주 1: [https://github.com/Tongji-KGLLM/RAG-Survey](https://github.com/Tongji-KGLLM/RAG-Survey)에서 리소스를 사용할 수 있습니다.

## 1 Introduction

대형 언어 모델(LLM)은 이전에 자연어 처리(NLP)에서 본 것보다 더 강력합니다. GPT 시리즈 모델[15, 16], LLama 시리즈 모델[14], Gemini[17] 및 기타 대형 언어 모델은 여러 평가 벤치마크[22, 19, 2]에서 인간 벤치마크 수준을 능가하는 인상적인 언어 및 지식 숙달을 보여준다.

그러나, 대형 언어 모델들 또한 수많은 단점들을 나타낸다. 그들은 종종 사실을 조작하고 특정 도메인이나 고도로 전문화된 쿼리를 다룰 때 지식이 부족하다[11]. 예를 들어, 추구하는 정보가 모델의 트레이닝 데이터 이상으로 확장되거나 최신 데이터가 필요한 경우, LLM은 정확한 답변을 제공하지 못할 수 있다. 이러한 제한은 블랙박스 LLM을 맹목적으로 사용하는 것으로 충분하지 않을 수 있기 때문에 실제 생산 환경에서 생성 인공 지능을 배치할 때 문제를 제기한다.

전통적으로 신경망은 지식을 파라미터화하기 위해 모델을 미세 조정함으로써 특정 도메인 또는 독점 정보에 적응한다. 이 기술은 상당한 결과를 산출하지만 상당한 계산 자원을 요구하고 높은 비용을 발생시키며 전문화된 기술 전문 지식을 요구하여 진화하는 정보 환경에 덜 적응한다. 모수적 지식과 비모수적 지식은 서로 다른 역할을 한다. 모수적 지식은 훈련 LLM을 통해 획득되고 신경망 가중치에 저장되며, 훈련 데이터에 대한 모델의 이해 및 일반화를 나타내며 생성된 응답의 기초를 형성한다. 반면에 비모수적 지식은 벡터 데이터베이스와 같은 외부 지식 소스에 존재하며, 모델에 직접 인코딩되지 않고 업데이트 가능한 보충 정보로 취급된다. 비모수적 지식은 LLM이 최신 또는 도메인별 정보에 액세스하고 활용할 수 있도록 권한을 부여하여 응답의 정확성과 관련성을 강화한다.

순수 매개변수 언어 모델(LLM)은 방대한 말뭉치로부터 획득한 세계 지식을 모델의 매개변수에 저장한다. 그럼에도 불구하고, 이러한 모델들은 그 한계가 있다. 첫째, 특히 덜 일반적이고 더 구체적인 지식에 대해서는 훈련 코퍼스에서 모든 지식을 유지하는 것이 어렵다. 둘째, 모델 파라미터를 동적으로 업데이트할 수 없기 때문에 파라미터 지식은 시간이 지남에 따라 구식이 되기 쉽다. 마지막으로, 매개변수의 확장은 훈련과 추론 모두에 대한 계산 비용을 증가시킨다. 순수하게 매개변수화된 모델의 한계를 해결하기 위해 언어 모델은 매개변수화되지 않은 말뭉치 데이터베이스를 매개변수화된 모델과 통합하여 준 매개변수화된 접근법을 채택할 수 있다. 이러한 접근법은 검색-증강 생성(RAG: Retrieval-Augmented Generation)으로 알려져 있다.

RAG( Retrieval-Augmented Generation)라는 용어는 [11]에 의해 처음 소개되었다. 사전 훈련된 리트리버와 사전 훈련된 seq2seq 모델(제너레이터)을 결합하고 엔드 투 엔드 미세 조정을 거쳐 보다 해석 가능하고 모듈화된 방식으로 지식을 포착한다. 대규모 모델이 등장하기 전에 RAG는 주로 엔드 투 엔드 모델의 직접적인 최적화에 중점을 두었다. 벡터 기반 DPR(Dense Passage Retrieval)[14]의 사용과 같은 검색 측면의 고밀도 검색과 생성 측면의 더 작은 모델을 훈련하는 것이 일반적인 관행이다. 전체적인 작은 파라미터 크기로 인해, 리트리버와 제너레이터는 종종 동기화된 엔드 투 엔드 트레이닝 또는 미세 조정을 겪는다[15].

챗GPT와 같은 LLM이 등장한 이후 다양한 언어 과제[16, 17, 18, 19]에 걸쳐 인상적인 성능을 보여주면서 생성 언어 모델이 우세해졌다. 그러나 LLM은 여전히 환각[19, 16], 지식 업데이트 및 데이터 관련 문제와 같은 문제에 직면해 있다. 이는 LLM의 신뢰성에 영향을 미쳐 특정 심각한 작업 시나리오, 특히 오픈 도메인 질문 응답[16, 17, 18] 및 상식 추론[13, 14]과 같은 방대한 지식에 대한 접근이 필요한 지식 집약적 작업에서 어려움을 겪는다. 매개변수 내의 암묵적 지식은 불완전하고 불충분할 수 있다.

후속 연구에서는 대규모 모델의 상황 내 학습(ICL)에 RAG를 도입하면 앞서 언급한 문제를 완화하고 중요하고 쉽게 구현할 수 있는 효과가 있다는 것을 발견했다. 추론 과정에서 RAG는 검색된 데이터를 참조로 사용하여 외부 지식 소스로부터 정보를 동적으로 검색하여 답변을 구성한다. 이는 응답의 정확성과 관련성을 실질적으로 개선하여 LLM에 존재하는 환각 문제를 효과적으로 해결한다. 이 기술은 LLM이 등장한 후 빠르게 인기를 얻었으며 챗봇을 개선하고 LLM을 보다 실용화하기 위한 가장 뜨거운 기술 중 하나가 되었다. RAG는 LLM의 훈련 매개변수로부터 사실적 지식을 분리함으로써 생성 모델의 강력한 능력과 검색 모듈의 유연성을 교묘하게 결합하여 순수하게 매개변수화된 모델에 내재된 불완전하고 불충분한 지식 문제에 대한 효과적인 솔루션을 제공한다.

본 논문은 RAG의 현재 연구 접근 방식과 향후 개발 경로를 체계적으로 검토하고 분석하여 Naive RAG, Advanced RAG, Modular RAG의 세 가지 주요 패러다임으로 요약한다. 그 후, RAG의 개선 방향과 현재 기술 특성을 강조하여 검색, 증강 및 생성의 세 가지 핵심 구성 요소를 통합 요약한다. 증강 방법에 대한 섹션에서는 현재 작업을 RAG의 증강 단계, 증강 데이터 소스 및 증강 프로세스의 세 가지 측면으로 구성한다. 또한, RAG와 관련된 평가 시스템, 적용 가능한 시나리오 및 기타 관련 내용을 요약한다. 이 기사를 통해 독자는 대형 모델과 검색 증강 세대에 대한 보다 포괄적이고 체계적인 이해를 얻는다. 그들은 지식 검색 증강의 진화 경로와 핵심 기술에 익숙해져 다양한 기술의 장단점을 식별하고 적용 가능한 시나리오를 식별하며 현재 일반적인 응용 사례를 실제로 탐색할 수 있다. 이전 연구에서 펑엘(Feng el al.[16])은 지식 편집 및 검색 증강 방법에 중점을 두고 대규모 모델과 지식을 결합하는 방법, 응용 및 향후 추세를 체계적으로 검토했다는 점은 주목할 만하다. Zhu 등[16]은 검색 시스템에 특정한 초점을 두고, 대형 언어 모델들에 대한 검색 시스템들을 증강시키는 최신 발전들을 소개했다. 한편, Asai et al.[16]은 "What", "When", "How"와 같은 질문에 초점을 맞추어 검색 기반 언어 모델의 주요 프로세스를 분석하고 설명했다. 이와 비교하여 본 논문은 검색-증강 생성(RAG: Retrieval-Augmented Generation)의 전 과정을 체계적으로 개관하고, 특히 지식 검색을 통한 대용량 언어 모델 생성 증대와 관련된 연구에 초점을 맞춘다.

RAG 알고리즘과 모델의 개발은 그림 1에 나와 있다. 타임라인에서 RAG와 관련된 대부분의 연구는 2020년 이후에 나타났으며 ChatGPT가 출시된 2022년 12월에 중요한 전환점을 보였다. ChatGPT 출시 이후 자연어 처리 분야의 연구는 대형 모델의 시대로 접어들었다. 순진한 RAG 기법은 빠르게 주목을 받아 관련 연구가 급격히 증가하고 있다. 개선 전략 측면에서 RAG 개념이 도입된 이후 사전 훈련 및 감독된 미세 조정 단계에서 강화에 대한 연구가 진행 중이다. 그러나 추론 단계에서의 강화 연구는 대부분 LLMs 시대에 등장하였다. 이는 주로 고성능 대형 모델과 관련된 높은 훈련 비용 때문이다. 연구자들은 추론 단계에서 RAG 모듈의 포함을 통해 비용 효율적인 방식으로 외부 지식을 통합하여 모델 생성을 향상시키려고 시도했다. 증강 데이터의 사용과 관련하여 초기 RAG는 특히 개방형 질문 답변의 맥락에서 비정형 데이터의 적용에 주로 초점을 맞추었다. 그 후, 높은 품질의 데이터를 지식 소스로 사용하여 대규모 모델에서 잘못된 지식의 내재화 및 환각과 같은 문제를 효과적으로 해결하는 등 검색을 위한 지식 소스의 범위가 확장되었다. 여기에는 구조화된 지식이 포함되며, 지식 그래프가 대표적인 예이다. 최근 LMM의 성능을 향상시키기 위해 스스로 지식을 마이닝하는 자기 검색에 대한 관심이 증가하고 있다.

본 논문의 후속 챕터는 다음과 같이 구성된다. 2장에서는 RAG의 배경에 대한 소개를 제공하고, 3장에서는 RAG의 주류 패러다임을 소개하고, 4장에서는 RAG의 리트리버를 분석하고, 5장에서는 RAG의 발전기 도입에 초점을 맞추고, 6장에서는 RAG의 증강 방법 도입을 강조하고, 7장에서는 RAG의 평가 시스템을 소개한다. 제8장에서는 RAG의 향후 발전 동향에 대한 전망을 제공한다. 마지막으로 9장에서는 설문조사의 주요 내용을 정리한다.

## 2 Background

이 장에서는 RAG의 정의와 RAG와 미세 조정과 같은 다른 모델 최적화 기술의 비교를 소개할 것이다.

### Definition

RAG의 의미는 기술 발전과 발맞춰 확장되었다 대규모 언어 모델 시대에 RAG의 구체적인 정의는 질문에 답하거나 텍스트를 생성할 때 먼저 방대한 문서 말뭉치에서 관련 정보를 검색하는 모델을 말한다. 이어서, 이 검색된 정보를 활용하여 응답 또는 텍스트를 생성함으로써, 예측의 품질을 향상시킨다. RAG 방법은 개발자들이 각각의 특정 태스크에 대해 전체 대형 모델을 재트레이닝할 필요성을 피할 수 있게 한다. 대신 지식 베이스를 부착하여 모델에 추가 정보 입력을 제공하고 응답의 정확도를 향상시킬 수 있다. RAG 방법은 특히 지식 집약적 작업에 적합하다. 요약하면, RAG 시스템은 두 개의 키 스테이지들로 구성된다:

1. 인코딩 모델을 사용하여 BM25, DPR, ColBERT 및 유사한 접근 방식과 같은 질문에 따라 관련 문서를 검색합니다[14, 15, 16].
2. 생성 단계: 검색된 컨텍스트를 조건으로 하여 시스템은 텍스트를 생성한다.

### RAG vs Fine-tuning

LLM(Large Language Models)의 최적화에서, RAG 외에, 또 다른 중요한 최적화 기법은 미세 조정이다.

RAG는 교재를 모델에 제공하는 것과 유사하여 특정 쿼리를 기반으로 정보를 검색할 수 있다. 이 접근법은 모델이 특정 문의에 응답하거나 특정 정보 검색 작업을 처리해야 하는 시나리오에 적합하다. 그러나 RAG는 광범위한 영역을 이해하거나 새로운 언어, 형식 또는 스타일을 학습하도록 모델을 가르치는 데 적합하지 않다.

미세조정은 학생들이 광범위한 학습을 통해 지식을 내면화할 수 있도록 하는 것과 유사하다. 이 접근법은 모형이 특정 구조, 스타일 또는 형식을 복제해야 하는 경우에 유용합니다. 미세 조정은 미세 조정되지 않은 모델의 성능을 향상시키고 상호 작용을 더 효율적으로 만들 수 있다. 특히 기본 모델에서 기존 지식을 강조하고, 모델의 출력을 수정하거나 커스터마이징하며, 모델에 복잡한 지시를 제공하는 데 적합하다. 그러나 미세 조정은 통합하기에 적합하지 않습니다.

도 1: 기존 RAG 연구의 타임라인. 출시일에 따라 주로 타임라인이 설정되었다.

모델의 새로운 지식 또는 새로운 사용 사례에 대해 빠른 반복을 요구하는 상황에 대한 지식.

미세조정은 학생들이 장기간의 학습을 통해 지식을 내면화하도록 하는 것과 유사하다. 이 방법은 모델이 특정 구조, 스타일 또는 형식을 복제해야 하는 경우에 적용할 수 있습니다. 미세 조정은 미세 조정되지 않은 모델보다 우수한 성능을 달성할 수 있으며 상호 작용이 더 효율적이다. 미세 조정은 특히 기본 모델에서 기존 지식을 강조하고 모델의 출력을 수정하거나 커스터마이징하며 복잡한 지시로 모델을 지시하는 데 적합하다. 그러나 미세 조정은 모델에 새 지식을 추가하거나 새 사용 사례에 대해 빠른 반복이 필요한 시나리오에는 적합하지 않습니다. RAG와 미세 조정(FT)의 구체적인 비교는 표 1에서 설명할 수 있다.

RAG와 미세 조정은 상호 배타적이지 않고 상호 보완할 수 있어 서로 다른 수준에서 모델의 능력을 향상시킨다. 특정 상황에서 이 두 기법을 결합하면 최적의 모델 성능을 얻을 수 있다. RAG 및 미세 조정으로 최적화하는 전체 프로세스는 만족스러운 결과를 달성하기 위해 다수의 반복을 필요로 할 수 있다.

기존 연구는 대규모 언어 모델을 최적화하기 위한 다른 방법[22, 23, 22, 21]:에 비해 검색-증강 생성(RAG)의 상당한 이점을 입증했다.

* RAG는 답변을 외부 지식과 연결 하 여 정확도를 향상 하 고 언어 모델의 환각 문제를 줄이고 생성된 응답을 보다 정확하고 신뢰할 수 있게 합니다.
* 검색 기술을 사용하면 최신 정보를 식별할 수 있습니다. RAG는 훈련 데이터에만 의존하는 전통적인 언어 모델과 비교하여 응답의 적시성과 정확성을 유지한다.
* 투명성은 RAG의 장점입니다. 출처를 인용함으로써 사용자는 답변의 정확성을 검증할 수 있어 모델의 출력에 대한 신뢰를 높일 수 있다.
* RAG에는 사용자 지정 기능이 있습니다. 관련 텍스트 말뭉치를 인덱싱하여 특정 필드에 대한 지식 지원을 제공함으로써 다양한 도메인에 맞게 모델을 조정할 수 있습니다.
* 보안 및 개인 정보 관리 측면에서 데이터베이스에 기본 제공 역할 및 보안 제어가 있는 RAG는 데이터 사용을 더 잘 제어할 수 있습니다. 대조적으로, 미세 조정된 모델은 누가 어떤 데이터에 액세스할 수 있는지에 대한 명확한 관리가 부족할 수 있다.
* RAG는 확장성이 더 높습니다. 모든 매개변수를 업데이트하고 훈련 세트를 만들 필요 없이 대규모 데이터 세트를 처리할 수 있어 경제적으로 더 효율적입니다.
* 마지막으로 RAG에 의해 생성된 결과가 더 신뢰할 수 있습니다. RAG는 최신 데이터에서 결정론적 결과를 선택하는 반면, 미세 조정된 모델은 투명성과 신뢰성이 결여된 동적 데이터를 처리할 때 환각과 부정확성을 나타낼 수 있다.

## 3 RAG Framework

RAG의 연구 패러다임은 끊임없이 진화하고 있다. 이 장에서는 주로 RAG 연구 패러다임의 진화를 소개한다. 이를 Naive RAG, Advanced RAG, Modular RAG의 세 가지 유형으로 분류한다. 초기 RAG는 비용 효율적이고 네이티브 LLM보다 더 잘 수행되었지만 여전히 많은 단점에 직면했다. 상기 등장은

그림 2: 다른 모델 최적화 방법과 비교한 RAG

Advanced RAG 및 Modular RAG는 Naive RAG의 특정 결함을 해결하는 것을 목표로 했다.

### Naive RAG

나이브 RAG 연구 패러다임은 ChatGPT가 널리 채택된 직후 유명해진 초기 방법론을 나타낸다. 순진한 RAG는 색인, 검색 및 생성과 같은 전통적인 프로세스를 포함한다. 순진한 RAG는 또한 "검색"-"읽기" 프레임워크[13]로 요약된다.

**Indexing**

소스로부터 데이터를 획득하고 이에 대한 인덱스를 구축하기 위한 파이프라인은 일반적으로 오프라인 상태에서 발생한다. 구체적으로, 데이터 인덱스의 구성은 다음의 단계들을 수반한다:

**1. 데이터 인덱싱:**이는 원본 데이터를 정리 하 고 추출 하 여 PDF, HTML, Word, Markdown 등과 같은 다른 파일 형식을 일반 텍스트로 변환 하는 것을 포함 합니다.

**2. 청크화:** 로드된 텍스트를 더 작은 청크로 나누는 작업이 포함됩니다. 이것은 언어 모델들이 통상적으로 그들이 처리할 수 있는 컨텍스트의 양에 제한이 있기 때문에, 가능한 한 작은 텍스트 청크들을 생성할 필요가 있기 때문에 필요하다.

**3. 인덱스 삽입 및 만들기:** 언어 모델을 통해 텍스트를 벡터로 인코딩하는 프로세스입니다. 결과 벡터는 벡터와 문제 벡터 사이의 유사도를 계산하기 위해 후속 검색 과정에서 사용될 것이다. 임베딩 모델은 높은 추론 속도를 요구한다. 사용자가 질문을 할 때 대량의 코퍼스를 부호화하고 실시간으로 문제를 부호화할 필요가 있기 때문에,

모델의 파라미터 크기는 너무 크지 않아야 한다. 임베딩을 생성한 후, 다음 단계는 인덱스를 생성하고, 원본 코퍼스 청크를 저장하고, 향후 빠르고 빈번한 검색을 위해 키-값 쌍 형태로 임베딩하는 것이다.

### Retrieve

사용자의 입력이 주어지면, 질의를 벡터로 변환하기 위해 제1 스테이지에서와 동일한 인코딩 모델이 사용된다. 코퍼스 내의 문서 블록의 질문 임베딩과 임베딩 사이의 유사도가 계산된다. 상위 K개의 문서 블록들은 유사성 레벨에 기초하여 현재 질문에 대한 증강된 컨텍스트 정보로서 선택된다.

### Generation

지정된 질문과 관련 문서가 새 프롬프트로 결합됩니다. 그 다음, 대형 언어 모델은 제공된 정보에 기초하여 질문에 응답하는 임무를 부여받는다. 큰 모델이 자신의 지식을 사용할 수 있도록 허용할 것인지, 주어진 정보에 기초하여 답변만 할 것인지는 서로 다른 작업의 필요에 따라 결정될 수 있다. 과거 대화 정보가 있는 경우 다중 라운드 대화 프롬프트에 병합할 수도 있습니다.

### Naive RAG의 드로백

Naive RAG는 검색 품질, 응답 생성 품질 및 확장 프로세스의 세 가지 영역에서 주요 문제에 직면한다.

검색 품질과 관련하여, 문제는 다면적입니다. 주요 관심사는 낮은 정밀도로, 검색 세트 내의 모든 블록이 쿼리와 상관관계가 있는 것은 아니며 잠재적인 환각 및 공중 낙하 문제로 이어진다. 2차 문제는 낮은 리콜이며, 이는 모든 관련 블록이 검색되지 않을 때 발생하며, 이에 따라 LLM이 답변을 합성하기에 충분한 컨텍스트를 얻는 것을 방지한다. 또한, 오래된 정보는 데이터 중복성 또는 오래된 데이터가 부정확한 검색 결과를 초래할 수 있는 또 다른 문제를 제시한다.

반응 생성 품질 측면에서 문제는 동일하게 다양하다. 환각은 모델이 문맥에 존재하지 않는 답을 조작하는 중요한 문제이다. 관련성은 모델이 쿼리를 해결하지 못하는 답변을 생성하는 또 다른 관심사이다. 또한, 모델이 유해하거나 불쾌한 반응을 생성하는 독성 또는 편향은 또 다른 문제이다.

마지막으로, 증강 프로세스는 또한 몇 가지 문제에 직면해 있다. 결정적으로, 검색된 구절에서 컨텍스트를 현재 생성 태스크와 효과적으로 통합하는 것이 가장 중요하다. 잘못 처리하면 출력이 일관성이 없거나 일치하지 않는 것으로 나타날 수 있습니다. 중복 및 반복은 특히 다수의 검색된 구절이 유사한 정보를 포함할 때, 생성 단계에서 콘텐츠 반복으로 이어지는 또 다른 이슈이다. 더욱이, 생성 태스크에 대한 다수의 검색된 패시지의 중요도 또는 관련성을 결정하는 것은 어렵고, 증강 프로세스는 각 패시지의 가치를 적절하게 밸런싱할 필요가 있다. 검색된 콘텐츠는 또한 상이한 필기 스타일들 또는 톤들로부터 올 수 있고, 증강 프로세스는 출력 일관성을 보장하기 위해 이러한 차이들을 조화시킬 필요가 있다. 마지막으로, 생성 모델들은 새로운 값 또는 합성된 정보를 제공하지 않고 단지 검색된 콘텐츠를 반복하는 출력을 초래하는 증강 정보에 과도하게 의존할 수 있다.

### Advanced RAG

고급 RAG는 Naive RAG의 결함을 극복하기 위해 표적 개선을 했다. 검색 생성의 품질 측면에서 Advanced RAG는 사전 검색 및 사후 검색 방법을 통합했다. NAive RAG가 직면한 인덱싱 문제를 해결하기 위해 Advanced RAG는 슬라이딩 윈도우, 세밀한 분할 및 메타데이터와 같은 방법을 통해 인덱싱을 최적화했다. 동시에 검색 프로세스를 최적화하기 위해 다양한 방법을 제시했다. 특정 구현의 관점에서, Advanced RAG는 파이프라인을 통해 또는 종단간 방식으로 조정될 수 있다.

### Pre-Retrieval Process

* **데이터 인덱싱 최적화** 데이터 인덱싱을 최적화하는 목적은 인덱싱된 콘텐츠의 품질을 향상시키는 것입니다. 현재 이를 위해 사용된 전략은 색인 데이터의 세분성 증가, 색인 구조 최적화, 메타데이터 추가, 정렬 최적화 및 혼합 검색의 5가지 주요 전략이 있다.
1. **데이터 세분성 강화:** 사전 인덱스 최적화의 목적은 텍스트 표준화, 일관성을 개선하고 RAG 시스템의 성능을 보장하기 위해 사실적 정확성과 맥락적 풍부성을 보장하는 것입니다. 텍스트 표준화는 검색기의 효율성을 높이기 위해 관련 없는 정보와 특수 문자를 제거하는 것을 포함한다. 일관성 측면에서 검색기의 초점을 단순화하기 위해 중복 또는 중복 정보를 제거하는 것과 함께 개체 및 용어의 모호성을 제거하는 것이 주요 작업이다. 사실적 정확성을 확보하는 것이 중요하며, 가능한 한 각 데이터의 정확성을 확인해야 한다. 현실 세계에서 시스템의 상호작용 컨텍스트에 적응하기 위해 컨텍스트 보존은 사용자 피드백 루프를 통한 지속적인 업데이트와 결합된 도메인 특정 주석이 있는 컨텍스트의 또 다른 계층을 추가함으로써 달성될 수 있다. 시간 민감도는 필수적인 상황 정보이며, 오래된 문서를 새로 고치기 위해 메커니즘이 설계되어야 한다. 요약하면, 인덱싱된 데이터를 최적화하는 초점은 시스템을 효율적이고 신뢰할 수 있도록 명확성, 맥락 및 정확성에 있어야 한다. 모범 사례를 소개하면 다음과 같습니다.
2. **인덱스 구조 최적화:** 청크의 크기를 조정 하 고 인덱스 경로를 변경 하 고 그래프 구조 정보를 통합 하 여 달성할 수 있습니다. 청크(Small to Big)를 조정하는 방법은 가능한 많은 관련 컨텍스트를 수집하고 노이즈를 최소화하는 것을 포함한다. RAG 시스템을 구성할 때, 청크 크기는 키 파라미터이다. 개별 청크의 크기를 비교하는 다양한 평가 프레임워크가 있다. LlamaIndex2는 충실도 및 관련성을 평가하기 위해 GPT4를 사용하고, LLaMA[11] 인덱스는 상이한 청킹 방법에 대한 자동 평가 특징을 갖는다. 다수의 인덱스 경로들에 걸쳐 질의하는 방법은 이전의 메타데이터 필터링 및 청킹 방법들과 밀접하게 관련되며, 상이한 인덱스들에 걸쳐 동시에 질의하는 것을 수반할 수 있다. 표준 인덱스는 특정 쿼리를 쿼리하는 데 사용될 수 있거나, 독립형 인덱스는 특정 "날짜" 인덱스와 같은 메타데이터 키워드에 기초하여 검색 또는 필터링하는 데 사용될 수 있다. 그래프 구조를 도입하는 것은 엔티티를 노드로, 그들의 관계를 관계로 변환하는 것을 포함한다. 이것은 특히 멀티-홉 질문들에 대해 노드들 사이의 관계들을 레버리지함으로써 정확도를 향상시킬 수 있다. 그래프 데이터 인덱스를 사용하면 검색의 관련성을 높일 수 있다.
3. **메타데이터 정보 추가:** 여기에서 초점은 참조 된 메타데이터를 필터링에 사용 되는 날짜 및 목적과 같은 청크에 포함 하는 것입니다. 참고문헌의 챕터 및 하위 섹션과 같은 메타데이터를 추가하는 것도 검색을 개선하는 데 도움이 될 수 있다. 색인을 수많은 청크로 나눌 때 검색 효율이 문제가 된다. 메타데이터를 통한 필터링은 먼저 효율성과 관련성을 높일 수 있다.
4. **정렬 최적화:** 이 전략은 주로 정렬 문제 및 문서 간의 차이점을 해결 합니다. 정렬 개념에는 _가설 질문_ 을 도입하고, 각 문서로 답변하기에 적합한 질문을 만들고, 이러한 질문을 문서로 포함(또는 대체)하는 작업이 포함됩니다. 이렇게 하면 문서 간의 정렬 문제와 불일치를 해결할 수 있습니다.
5. **혼합 검색:** 이 전략의 장점은 여러 검색 기술의 장점을 활용 하는 데 있습니다. 키워드 기반 검색, 의미 검색 및 벡터 검색을 포함한 다양한 기술을 지능적으로 결합하여 다양한 쿼리 유형 및 정보 요구에 적응하여 가장 관련성이 높고 컨텍스트가 풍부한 정보의 일관된 검색을 보장한다. 혼합 검색은 RAG 파이프라인의 전반적인 성능을 향상시키는 검색 전략에 대한 강력한 보완 역할을 할 수 있다.

### Embedding

* **미세 전환 임베딩:** 미세 조정 임베딩 모델은 RAG의 효과에 직접적인 영향을 미칩니다. 미세조정의 목적은 검색된 콘텐츠와 질의 사이의 관련성을 향상시키는 것이다. 미세 조정 임베딩의 역할은 음성을 생성하기 전에 귀를 조정하는 것과 유사하여, 생성된 출력에 대한 검색 콘텐츠의 영향을 최적화한다. 일반적으로, 임베딩을 미세 조정하는 방법은 도메인 특정 컨텍스트에서 임베딩을 조정하고 검색 단계를 최적화하는 범주에 속한다. 특히 진화하고 희귀한 용어를 다루는 전문 도메인에서 이러한 맞춤형 임베딩 방법은 검색 관련성을 향상시킬 수 있다. BGE[1]embedding 모델은 BAAI 3에 의해 개발된 BGE-large-EN과 같은 미세 조정 및 고성능 임베딩 모델이다. BGE 모델을 미세 조정하기 위한 학습 데이터를 생성하기 위해 gpt-3.5-turbo와 같은 LLM을 사용하여 문서 청크를 기반으로 질문을 공식화하고, 여기서 질문 및 답변(문서 청크)은 미세 조정 프로세스를 위한 미세 조정 쌍을 형성한다. 각주 3: [https://huggingface.co/BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en)
* **동적 임베딩:** 동적 임베딩은 각 단어에 대해 단일 벡터를 사용하는 정적 임베딩과 달리 단어가 나타나는 컨텍스트에 따라 조정됩니다. 예를 들어, BERT와 같은 트랜스포머 모델들에서, 동일한 단어는 주변 단어들에 따라 다양한 임베딩들을 가질 수 있다. 증거는 OpenAI의 텍스트-임베딩-ada-002 모델4에서, 특히 5 토큰 미만의 텍스트 길이를 갖는 예상치 못한 높은 코사인 유사성 결과를 나타낸다. 이상적으로는, 임베딩은 "건강한" 결과를 보장하기 위해 가능한 많은 컨텍스트를 포함해야 한다. GPT와 같은 큰 언어 모델의 원리에 기초하여, OpenAI의 임베딩-ada-02는 정적 임베딩 모델보다 더 정교하여 특정 수준의 컨텍스트를 캡처한다. 문맥 이해에 탁월하지만 GPT-4와 같은 최신 전체 크기 언어 모델과 동일한 컨텍스트 민감도를 나타내지 않을 수 있습니다. 각주 4: [https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings)

### Post-Retrieval Process

데이터베이스에서 중요한 컨텍스트를 검색한 후 입력을 위한 쿼리와 LLM으로 병합하면 문제가 발생합니다. 모든 관련 문서를 LLM에 한꺼번에 제시하는 것은 컨텍스트 윈도우 제한을 초과할 수 있다. 많은 문서를 연결하여 긴 검색 프롬프트를 형성하는 것은 비효율적이며, 노이즈를 도입하고 LLM이 중요한 정보에 초점을 맞추는 것을 방해한다. 이러한 문제를 해결하기 위해서는 검색된 콘텐츠의 추가 처리가 필요하다.

* **순위 조정:** 프롬프트의 가장자리에 가장 관련 있는 정보를 재배치 하기 위해 순위를 조정 하는 것은 간단한 아이디어입니다. 이 개념은 Llamalndex, LangChain, HayStack[1]과 같은 프레임워크에서 구현되었다. 예를 들어, 다이버시티 랭커는 문서 다이버시티에 기초하여 재정렬을 우선시하는 반면, LostInTheMiddleRanker는 컨텍스트 윈도우의 시작과 끝에 최상의 문서를 번갈아 배치한다. 동시에, 의미적 유사성에 대한 벡터 기반 시뮬레이트된 검색을 해석하는 도전을 다루는, cohereAI rerank[3], begrerank5 또는 LongLLMLingua[12]와 같은 접근법들은 관련 텍스트와 질의 사이의 의미적 유사성을 재계산한다. 각주 5: [https://huggingface.co/BAAI/bge-reanker-large](https://huggingface.co/BAAI/bge-reanker-large)
* **프롬프트 압축** 연구는 검색된 문서의 노이즈가 RAG 성능에 부정적인 영향을 미친다는 것을 나타냅니다. 후처리에서 강조점은 관련 없는 컨텍스트를 압축하고 중추적인 문단을 강조하며 전체 컨텍스트 길이를 줄이는 데 있다. 선택적 컨텍스트[12]와 LLMLingua[1]와 같은 접근법은 작은 언어 모델을 활용하여 신속한 상호 정보 또는 복잡성을 계산하고 요소 중요도를 추정한다. 그러나, 이러한 방법들은 RAG 또는 롱-컨텍스트 시나리오들에서 키 정보를 잃을 수 있다. 리콤프[24]는 상이한 입도에서 압축기를 훈련시킴으로써 이를 어드레스한다. 긴 컨텍스트[24]는 광범위한 컨텍스트를 다룰 때, 분해 및 압축하는 반면, "Walking in the Memory Maze"[3]는 LLM의 핵심 정보 인식을 향상시키기 위해 계층적 요약 트리를 설계한다.

**RAG Pipeline Optimization**

검색 프로세스의 최적화는 RAG 시스템의 효율성과 정보 품질을 향상시키는 것을 목표로 한다. 현재 연구는 주로 다양한 검색 기술을 지능적으로 결합하고, 검색 단계를 최적화하며, 인지적 역추적 개념을 도입하고, 다양한 쿼리 전략을 유연하게 적용하고, 임베딩 유사성을 활용하는 데 중점을 둔다. 이러한 노력은 RAG 검색에서 효율성과 맥락 정보의 풍부성 사이의 균형을 달성하기 위해 집합적으로 노력한다.

* **하이브리드 검색 탐색:** 키워드 기반 검색, 의미 검색 및 벡터 검색과 같은 다양한 기술을 지능적으로 혼합함으로써 RAG 시스템은 각 방법의 장점을 활용할 수 있습니다. 이 접근법은 RAG 시스템이 상이한 질의 유형 및 정보 요구에 적응할 수 있게 하여, 가장 관련성 있고 컨텍스트가 풍부한 정보의 일관된 검색을 보장한다. 하이브리드 검색은 검색 전략에 대한 강력한 보완 역할을 하여 RAG 파이프라인의 전반적인 성능을 향상시킨다.
* **재귀 검색 및 쿼리 엔진:** RAG 시스템에서 검색을 최적화 하는 또 다른 강력한 방법에는 재귀 검색 및 정교한 쿼리 엔진을 구현 하는 것이 포함 됩니다. 재귀적 검색은 주요 의미 의미들을 캡처하기 위해 초기 검색 단계 동안 더 작은 문서 블록들을 획득하는 것을 수반한다. 이 프로세스의 후기 단계에서, 더 많은 컨텍스트 정보를 갖는 더 큰 블록이 언어 모델(LM)에 제공된다. 이 2단계 검색 방법은 효율성과 맥락적으로 풍부한 응답 사이의 균형을 맞추는 데 도움이 된다.
* **StepBack-prompt:** RAG 프로세스에 통합 된 StepBack-prompt 접근 방식[15]은 LLM이 특정 인스턴스에서 뒤로 물러나고 기본 일반 개념 또는 원칙에 대 한 추론에 참여 하도록 권장 합니다. 실험 결과는 RAG에 대한 자연스러운 적응성을 보여주는 백워드 프롬프트의 통합으로 다양한 도전적이고 추론 집약적인 작업에서 상당한 성능 향상을 나타낸다. 검색 강화 단계는 역방향 프롬프트에 대한 답변 생성과 최종 질문 응답 프로세스 모두에 적용될 수 있다.
* **하위 쿼리:** Llamaldex와 같은 프레임워크에서 제공하는 쿼리 엔진을 사용하거나 트리 쿼리를 사용하거나 벡터 쿼리를 사용하거나 청크의 가장 기본적인 순차 쿼리를 사용하는 등 다양한 쿼리 전략을 다양한 시나리오에 사용할 수 있습니다.
* **HyDE:** 이 접근 방식은 생성된 답변이 직접 쿼리보다 임베딩 공간에서 더 가까울 수 있다는 가정에 근거합니다. LLM을 이용하여, HyDE는 질의에 응답하여 가상 문서(답변)를 생성하고, 문서를 임베딩하며, 이러한 임베딩을 이용하여 가상 문서와 유사한 실제 문서를 검색한다. 이 방법은 질의에 기반하여 임베딩 유사도를 구하는 것과 달리, 답변에서 답변까지의 임베딩 유사도를 강조한다. 그러나, 특히 언어 모델이 논의된 주제에 익숙하지 않은 경우, 이는 잠재적으로 오류가 발생하기 쉬운 인스턴스의 생성을 증가시키는 유리한 결과를 일관되게 산출하지 못할 수 있다.

**Modular RAG**

모듈형 RAG 구조는 색인, 검색 및 생성의 전통적인 Naive RAG 프레임워크에서 벗어나 전체 프로세스에서 더 큰 다양성과 유연성을 제공한다. 한편, 유사성 검색에서 검색 모듈을 통합하고 검색기에 미세 조정 접근법을 적용하는 등 기능 모듈을 확장하기 위한 다양한 방법을 통합한다[14]. 또한, 특정 문제는 재구성된 RAG 모듈[25] 및 [25]와 같은 반복적인 접근법의 출현으로 이어졌다. 모듈형 RAG 패러다임은 RAG 도메인에서 주류가 되어 여러 모듈에 걸쳐 직렬화된 파이프라인 또는 종단 간 훈련 접근법을 허용한다. 세 가지 RAG 패러다임 간의 비교는 그림 3에 나와 있다.

**New Modules**

* **검색 모듈:** Naive/Advanced RAG에서 쿼리와 말뭉치 간의 유사성 검색에서 벗어나 특정 시나리오에 맞게 조정된 검색 모듈은 LLM 생성 코드, 쿼리 언어(예: SQL, Cypher) 또는 기타 사용자 지정 도구를 사용하여 프로세스의 (추가) 말뭉치에 대한 직접 검색을 통합합니다. 검색을 위한 데이터 소스는 검색 엔진, 텍스트 데이터, 표형 데이터 또는 지식 그래프를 포함할 수 있다[26].
* **메모리 모듈:** 검색을 안내 하기 위해 LLM 자체의 메모리 기능을 활용 하는 원칙은 현재 입력과 가장 유사한 메모리를 찾는 것을 포함 합니다. 셀프-멤[3]은 무제한 메모리 풀을 생성하기 위해 검색-향상 생성기를 반복적으로 사용한다. 검색-향상 생성 모델은 "원본 질문" 및 "이중 질문"을 결합한다. 검색-향상 생성 모델은 자체 출력을 사용하여 자신을 향상시킬 수 있으며, 추론 프로세스에서 텍스트를 학습 데이터보다 모델 자체 출력과 함께 데이터 분포에 더 가깝게 만들 수 있다[26].
* **추가 생성 모듈:** 검색된 콘텐츠에서 중복성 및 노이즈는 일반적인 문제입니다. 데이터 소스로부터 직접 검색하는 대신, Extra Generation Module은 LLM을 활용하여 필요한 컨텍스트를 생성한다[25]. LLM에 의해 생성된 콘텐츠는 직접 검색에 비해 관련 정보를 포함할 가능성이 높다.

* **작업 적응 모듈:** 다양한 다운스트림 작업에 적응하도록 RAG를 변환하는 것을 중심으로 UPRISE[21]는 미리 구성된 데이터 풀에서 주어진 제로 샷 작업 입력에 대한 프롬프트를 자동으로 검색하여 작업 및 모델 전반에 걸쳐 범용성을 향상시킵니다. PROMPTAGATOR[14]는 LLM을 몇 번 샷 쿼리 생성기로 활용하고 생성된 데이터를 기반으로 작업별 검색기를 만듭니다. LLM의 일반화 기능을 활용하여 PROMPTAGATOR는 몇 가지 예만 있으면 작업별 종단 간 검색기를 만들 수 있습니다.
* **정렬 모듈:** 쿼리와 텍스트 간의 정렬은 RAG의 효과에 영향을 미치는 중요한 문제였습니다. Modular RAG 시대에 연구자들은 리트리버에 훈련 가능한 어댑터 모듈을 추가하면 선형 문제를 효과적으로 완화할 수 있다는 것을 발견했다. PRCA[22]는 리트리버와 생성기 사이에 위치된 LLM 보상에 의해 구동되는 컨텍스트 어댑터를 트레이닝하기 위해 강화 학습을 레버리지하였다. 라벨링된 자기회귀 정책 내에서 강화 학습 단계에서 보상을 최대화하여 검색된 정보를 최적화한다. AAR[23]은 알려진 소스 LLM에서 LM 선호도를 학습하여 알려지지 않았거나 조정되지 않은 LLM을 지원하는 범용 플러그인을 제안했다. RRR[15]는 코퍼스 내의 문서들과 질의들을 정렬하기 위해 강화 학습에 기초하여 질의들을 재작성하기 위한 모듈을 설계하였다.
* **유효성 검사 모듈:** 실제 시나리오에서 검색된 정보가 신뢰할 수 있는지 항상 보장되는 것은 아닙니다. 관련 없는 데이터를 검색하는 것은 LLM에서 착시의 발생으로 이어질 수 있다. 그러므로, 검색된 문서들과 질의 사이의 관련성을 평가하기 위해 문서들을 검색한 후에 추가적인 검증 모듈이 도입될 수 있다. 이것은 RAG[23]의 견고성을 향상시킨다.

**새 패턴** 모듈형 RAG의 조직적 접근 방식은 유연하여 특정 문제 컨텍스트에 따라 RAG 프로세스 내에서 모듈을 대체하거나 재구성할 수 있습니다. 검색 및 생성(일부 문헌에서 읽기 또는 합성이라고 함)의 두 모듈로 구성된 Naive RAG의 경우 이 프레임워크는 적응성과 풍부함을 제공한다. 현재 연구는 주로 모듈의 추가 또는 교체, 모듈 간의 조직 흐름 조정과 관련된 두 가지 조직 패러다임을 탐구한다.

* **모듈 추가 또는 교체** 모듈을 추가하거나 교체하는 전략은 특정 기능을 강화하기 위해 추가 모듈을 도입하면서 Retrieval-Read의 구조를 유지하는 것을 수반합니다. RRR[15]은 재작성 모듈에 대한 강화 학습에서 LLM 성능을 보상으로 활용하는 재작성-검색-읽기 프로세스를 제안한다. 이렇게 하면 재작성된 검색 쿼리가 조정되어 판독기의 다운스트림 작업 성능이 향상됩니다. 유사하게, 모듈은 Generate-Read[20]와 같은 접근법에서 선택적으로 대체될 수 있으며, 여기서 LLM 생성 모듈은 검색 모듈을 대체한다.

도 3: RAG의 세 패러다임 간의 비교

Recite-Read[22]는 외부 검색을 모델 가중치로부터 검색으로 변환하며, 초기에는 LLM이 태스크 관련 정보를 암기하고 지식 집약적인 자연어 처리 태스크를 처리하기 위한 출력을 생성한다.
* **모듈 간의 흐름 조정** 모듈 간의 흐름을 조정하는 영역에서는 언어 모델과 검색 모델 간의 상호 작용을 향상시키는 데 중점을 둡니다. DSP[14]는 상황 학습 시스템을 지식 집약적 태스크를 다루기 위한 터미널 태스크 프롬프트가 아닌 명시적 프로그램으로 다루는 데모-검색-예측 프레임워크를 소개한다. ITER-RETGEN[23]은 생성된 콘텐츠를 사용하여 검색을 안내하고, 검색-읽기-검색-읽기 흐름에서 "검색-향상된 생성" 및 "생성-향상된 검색"을 반복적으로 수행한다. Self-RAG[15]는 결정-회수-반사-판독 프로세스를 따르며, 능동적 판단을 위한 모듈을 도입한다. 이러한 적응적이고 다양한 접근법은 모듈러 RAG 프레임워크 내에서 모듈의 동적 구성을 허용한다.

## 4 Retriever

RAG의 맥락에서, "R"은 검색을 의미하며, 방대한 지식 베이스로부터 최상위-k 관련 문서를 검색하는 RAG 파이프라인에서의 역할을 수행한다. 그러나 고품질 검색기를 만드는 것은 사소한 일이 아닙니다. 이 장에서는 1) 정확한 의미 표상을 어떻게 습득할 것인가의 세 가지 핵심 질문을 중심으로 논의를 정리한다. 2) 질의와 문서의 의미 공간을 일치시키는 방법 3) 검색기의 출력을 대용량 언어 모델의 선호도와 정렬하는 방법

### 정확한 의미 표현을 획득하는 방법

RAG에서 의미 공간은 질의와 문서가 매핑되는 다차원 공간이다. 우리가 검색을 수행할 때, 그것은 의미 공간 내에서 측정된다. 의미 표현이 정확하지 않으면 RAG에 미치는 영향이 치명적이기 때문에 이 섹션에서는 정확한 의미 공간을 구축하는 데 도움이 되는 두 가지 방법을 소개한다.

#### 4.1.1 청크 최적화

외부 문서를 처리할 때, 첫 번째 단계는 세밀한 특징을 얻기 위해 청크하는 것이다. 그런 다음 청크가 내장됩니다. 그러나, 너무 크거나 너무 작은 텍스트 청크들을 임베딩하는 것은 좋은 결과들을 달성하지 못할 수 있다. 따라서 코퍼스에서 문서에 대한 최적의 청크 크기를 찾는 것은 검색 결과의 정확성과 관련성을 보장하는 데 중요하다.

청킹 전략을 선택할 때 중요한 고려 사항은 인덱싱되는 콘텐츠의 특성, 사용된 임베딩 모델 및 최적의 블록 크기, 사용자 질의의 예상 길이 및 복잡성, 검색 결과가 특정 애플리케이션에서 사용되는 방법 등이다. 예를 들어, 더 긴 또는 더 짧은 콘텐츠에 대해 상이한 청킹 모델이 선택되어야 한다. 추가적으로, 상이한 임베딩 모델들은 상이한 블록 크기들에서 상이하게 수행되며, 예를 들어, 문장-변환기는 단일 문장에 더 적합한 반면, 텍스트-임베딩-ada-002는 256 또는 512 토큰들을 포함하는 블록들에 더 우수하다. 또한, 사용자의 입력 질문 텍스트의 길이와 복잡성뿐만 아니라 시맨틱 검색 또는 질의 응답과 같은 애플리케이션의 특정 요구는 모두 청크화 전략의 선택에 영향을 미칠 것이다. 이는 선택한 LLM의 토큰 제한과 직접 상관 관계가 있을 수 있으며 블록 크기를 조정해야 할 수도 있습니다. 실제로, 정확한 질의 결과는 여러 청킹 전략을 적응적으로 적용함으로써 달성된다; 최상의 것은 없고, 가장 적합한 것만이 있다.

현재 RAG에 대한 연구는 검색의 효율성과 정확성을 향상시키기 위해 다양한 블록 최적화 방법을 사용한다. 슬라이딩 윈도우 기술과 같은 기술은 다중 검색을 통해 전역적으로 관련된 정보를 집계함으로써 계층 검색을 구현한다. Small2big 기법은 검색 프로세스 동안 작은 텍스트 블록을 활용하고, 처리를 위해 언어 모델에 더 큰 제휴 텍스트 블록을 제공한다. 추상적 임베딩 기법은 문서 초록에 대해 Top K 검색을 수행하여 전체 문서 컨텍스트를 제공한다. 메타데이터 필터링 기법은 필터링을 위해 문서 메타데이터를 활용한다. 그래프 인덱싱 기법은 엔티티와 관계를 노드와 연결로 변환하여 다중 홉 이슈의 맥락에서 관련성을 크게 향상시킨다. 이러한 방법의 통합은 RAG에 대한 검색 결과를 개선하고 성능을 향상시켰다.

#### 4.1.2 Fine-tuning Embedding Models

청크의 적절한 크기를 구한 후 임베딩 모델에 의해 청크와 질의를 의미 공간에 임베딩해야 하므로 임베딩이 코퍼스를 효과적으로 표현할 수 있는지 여부가 중요하다. 최근 들어 [15], 항해[2], BGE[1] 등과 같은 우수한 임베딩 모델들이 등장하여 대규모 코퍼스에 대한 사전 학습을 수행하였으나, 특정 도메인에 적용 시 도메인별 코퍼스 정보를 정확하게 표현하지 못할 수 있다. 나아가, 임베딩 모델들의 태스크-특정 미세 조정은 모델이 콘텐츠 관련성과 관련하여 사용자 질의를 이해하도록 하는 데 중요한 반면, 미세 조정되지 않은 모델은 특정 태스크의 요구를 충족시키지 못할 수 있다. 따라서, 임베딩 모델을 미세 조정하는 것은 다운스트림 애플리케이션들에 필수적이다. 미세 조정 방법을 삽입하는 데 있어 두 가지 기본 패러다임이 있다.

**도메인 지식 미세 조정** Embedding 모델이 도메인별 정보를 올바르게 이해하려면 Embedding 모델을 미세 조정하기 위해 도메인별 데이터 세트를 구성해야 합니다. 그러나 임베딩 모델을 미세 조정하는 것은 주로 사용되는 데이터 세트가 다르다는 점에서 일반적인 언어 모델과 다르다. 현재 미세 조정 임베딩 모델의 주요 방법에서 사용된 데이터 세트는 쿼리, 코퍼스 및 관련 문서를 포함한 세 부분으로 구성된다. 임베딩 모델은 쿼리를 기반으로 코퍼스에서 관련 문서를 검색한 후, 쿼리의 관련 문서가 모델의 메트릭으로 사용되는지 여부를 결정한다.

데이터 세트의 구성, 미세 조정 모델 및 평가에서 이 세 가지 구성 요소 각각에서 수많은 문제가 발생할 수 있다. LlamaIndex [13]에서는 임베딩 모델의 미세 조정 과정을 위해 일련의 핵심 클래스와 함수가 구체적으로 도입되어 이 절차를 상당히 간소화했다. 도메인 지식의 코퍼스를 작성하고 이를 이용하여 원하는 도메인에 맞는 특화된 임베딩 모델을 쉽게 얻을 수 있다.

**다운스트림 작업의 미세 조정** 다운스트림 작업에 임베딩 모델을 적용하는 것도 마찬가지로 중요합니다. 다운스트림 태스크에서 RAG를 사용할 때 LLMs.PROMPTAGATOR[14]의 기능을 사용하여 미세 조정된 임베딩 모델을 갖는 일부 작업은 LLM(Large Language Model)을 수샷 쿼리 생성기로 사용하고 생성된 데이터를 기반으로 태스크별 검색기를 생성하고, 데이터 부족으로 인해 일부 도메인에서 어려운 감독 미세 조정 문제를 완화한다. LLM-Embedder[15]는 대용량 언어 모델을 사용하여 여러 다운스트림 태스크의 데이터에 대한 보상 값을 출력하고 데이터 세트의 하드 라벨링과 LLM에서 파생된 소프트 보상을 통해 두 개의 서로 다른 감독 신호로 검색기를 미세 조정한다.

이는 도메인 지식 주입과 다운스트림 태스크 미세 조정을 통해 의미 표현을 다소 향상시킨다. 그러나, 이 접근법에 의해 트레이닝된 리트리버들은 대형 언어 모델들에 직관적으로 도움이 되지 않기 때문에, LLM으로부터의 피드백 신호들을 통해 직접 임베딩 모델들의 미세 조정을 감독하기 위한 일부 작업들이 수행되었다. (이 섹션은 4.4에서 제시될 것이다)

### 쿼리 및 문서의 의미 공간을 일치시키는 방법

RAG 애플리케이션에서, 일부 검색자들은 질의 및 doc를 인코딩하기 위해 동일한 임베딩 모델을 사용하는 반면, 다른 검색자들은 질의 및 doc를 별도로 인코딩하기 위해 두 개의 모델을 사용한다. 더욱이, 사용자의 원래 질의는 표현 불량 및 의미 정보 부족의 문제점을 가질 수 있다. 따라서, 사용자의 질의와 문서의 의미 공간을 정렬하는 것은 매우 필요하다. 이 섹션에서는 이 목표를 달성하기 위한 두 가지 핵심 기술을 소개한다.

**Query Rewrite**

쿼리와 문서의 의미를 정렬하는 가장 직관적인 방법은 쿼리를 다시 작성하는 것입니다. Query2Doc[21]과 ITER-RETGEN[22]에서 언급한 바와 같이, 대용량 언어 모델의 고유한 기능을 활용하여 의사문서를 생성한 후, 원래의 질의어를 이 의사문서와 병합하여 새로운 질의를 형성한다. HyDE[1]에서 쿼리 벡터는 텍스트 지표를 사용하여 설정되며, 이러한 지표를 사용하여 관련성이 있는 '가설적' 문서를 생성하지만 실제로는 존재하지 않을 수 있지만 관련 패턴을 캡처하기만 하면 된다. RRR[12]는 질의 재쓰기에 초점을 맞추어 검색과 읽기의 순서를 뒤집는 새로운 프레임워크를 도입했다. 이 방법은 큰 언어 모델을 사용하여 쿼리를 생성한 다음 웹 검색 엔진을 사용하여 컨텍스트를 검색하고 마지막으로 작은 언어 모델을 훈련 재작성기로 사용하여 동결된 큰 언어 모델을 서비스한다. STEP-BACKPROMPTING[15] 방법은 대용량 언어 모델들로 하여금 추상추론을 수행하게 하고, 높은 수준의 개념과 원리를 추출하게 하며, 이를 기반으로 검색을 수행할 수 있다. 마지막으로, 다중 질의 검색에서의 방법은 다수의 검색 질의를 생성하기 위해 큰 언어 모델을 사용하는 것을 포함하고, 이러한 질의를 병렬로 실행할 수 있고, 검색 결과가 함께 입력되어, 다수의 하위 문제에 의존하는 단일 문제에 매우 유용하다.

**Embedding Transformation**

쿼리들을 재작성하는 것과 같은 거친-결정적 방법이 있는 경우, 임베딩 동작들에 특정한 더 미세한-결정적 구현이 또한 있어야 한다. LlamaIndex[10]에서는 질의 인코더 이후에 어댑터를 연결하고, 어댑터를 미세 조정하여 질의 임베딩의 표현을 최적화하여 특정 작업에 더 적합한 잠재 공간에 매핑할 수 있다. 질의와 외부 문서의 데이터 구조가 비구조화 질의와 구조화 외부 문서와 같은 경우, 질의가 문서와 정렬될 수 있도록 하는 것이 매우 중요하다. SANTA[11]는 구조화 정보를 리트리버가 인지하도록 하기 위한 두 가지 사전-트레이닝 방법을 제안한다. 1) 구조화 인식 사전-트레이닝을 위한 대조적 학습을 위해 구조화 데이터와 비구조화 데이터의 자연스러운 정렬 관계를 이용한다. 2) 개체 지향 마스크 전략을 설계하고, 언어 모델에 마스크된 개체를 채우도록 요청하는 마스크된 개체 예측.

### 검색기의 출력과 LLM의 선호도를 정렬하는 방법

RAG 파이프라인에서, 검색 적중률을 향상시키기 위해 상기 기술들을 채용하더라도, 검색된 문서들이 LLM이 필요로 하는 것이 아닐 수 있기 때문에, RAG의 최종 효과를 여전히 개선시키지 않을 수 있다. 따라서 이 섹션에서는 검색기의 출력과 LLM의 선호도를 정렬하는 두 가지 방법을 소개한다.

**LLM 지도 학습** 많은 작업에서 대규모 언어 모델에서 임베딩 모델을 미세 조정 하기까지 다양 한 피드백 신호를 활용 합니다. AAR[21]은 인코더-디코더 아키텍처 LM을 통해 미리 훈련된 리트리버에 대한 감독 신호를 제공한다. FiD 교차 주의 점수를 통해 LM이 선호하는 문서를 결정함으로써 검색기는 하드 네거티브 샘플링 및 표준 교차 엔트로피 손실로 미세 조정된다. 궁극적으로, 미세 조정된 리트리버는 보이지 않는 타겟 LMs를 향상시키는 데 직접 사용될 수 있고, 이에 의해 타겟 태스크에서 더 나은 성능을 발휘할 수 있다. 리트리버의 트레이닝 손실은 다음과 같다:

\[\zeta=\sum_{q}\sum_{d^{+}\in D^{a^{+}}}\sum_{d^{-}\in D^{-}}l\left(f\left(q,d ^{+}\right),f\left(q,d^{-}\right)\right) \tag{1}\]

여기서 \(D^{a^{+}}\)은 검색된 집합에서 LLM이 선호하는 문서이고 \(D^{a^{-}}\)은 선호하지 않는다. \ (l\)는 표준 크로스 엔트로피 손실이다. 결국 LLM은 정보가 풍부한 문서보다는 가독성에 집중하는 것을 선호할 수 있음을 시사한다.

REPLUG[12]는 검색기와 LLM을 이용하여 검색된 문서의 확률분포를 계산한 후 KL 발산을 계산하여 지도학습을 수행한다. 이 간단하고 효과적인 훈련 방법은 LM을 감독 신호로 사용하여 검색 모델의 성능을 향상시켜 특정 교차 주의 메커니즘이 필요하지 않다. 리트리버의 트레이닝 손실은 다음과 같다:

\[\zeta=\frac{1}{\left|D\right|}\sum_{x\in D}KL\left(P_{R}\left(d|x\right)||Q_{LM} \left(d|x,y\right)\right) \tag{2}\]

여기서, \(D\)은 입력 문맥의 집합이고, \(P_{R}\)은 검색 가능성, \(Q_{LM}\)은 각 문서의 LM 가능성이다.

UPRISE[11]는 또한 프롬프트 리트리버를 미세 조정하기 위해 동결된 대형 언어 모델을 사용한다. 그러나 언어 모델과 검색기 모두 프롬프트 입력 쌍을 입력으로 사용 하 고 대규모 언어 모델에서 제공 된 점수를 사용 하 여 검색기의 훈련을 감독 합니다. 이는 대규모 언어 모델을 사용 하 여 데이터 집합에 레이블을 지정 하는 것과 같습니다. Atlas[14]는 지도 임베딩 모델을 미세 조정하는 네 가지 방법을 제안하며, 그 중 언어 모델이 출력 중에 생성하는 교차 주의 점수를 사용하여 주의 증류 증류한다. EMDR2는 기대-최대화 알고리즘을 사용하여 검색된 문서를 잠재 변수로 학습한다. Perplexity Distillation은 모델 생성 토큰의 perplexity를 지시자로 사용하여 직접 학습한다.LOOP는 문서 삭제가 LM 예측에 미치는 영향을 기반으로 새로운 손실 함수를 도입하여 모델을 특정 작업에 더 잘 적응시키기 위한 효과적인 학습 전략을 제공한다.

**어댑터에 플러그 인** 하지만 임베딩 기능을 구현 하기 위해 API를 사용 하거나 로컬 계산 리소스가 부족 하 여 임베딩 모델을 미세 조정 하는 것은 어려울 수 있습니다. 따라서 정렬을 위한 어댑터를 외부에서 부착하는 것을 선택하는 작업도 있다. PRCA[15]는 컨텍스트 추출 단계와 보상 구동 단계를 통해 어댑터를 학습하고 토큰 기반 자기회귀 전략을 기반으로 검색기의 출력을 최적화한다. 토큰필터링[1] 방법은 크로스-어텐션 스코어를 계산하고, 가장 높은 스코어링 입력 토큰을 선택하여 효과적으로 토큰을 필터링한다. RECOMP[16]는 관련 문장을 선택하거나 문서 정보를 합성하여 요약문을 생성하는 추출 및 생성 압축기를 제안한다. 또한, 새로운 접근 방식인 PKG[17]는 지시적 미세 조정을 통해 화이트 박스 모델에 지식을 주입하고, 질의에 기반하여 관련 문서를 직접 출력하기 위해 사용되는 검색기 모듈을 직접 대체한다.

## 5 Generator

RAG의 또 다른 핵심 구성요소는 검색된 정보를 자연스럽고 유창한 텍스트로 변환하는 생성자이다. 그 디자인은 전통적인 언어 모델에서 영감을 얻었지만, 기존의 생성 모델과 비교하여 RAG의 생성기는 검색된 정보를 활용하여 정확성과 관련성을 향상시킨다. RAG에서 생성자의 입력은 전통적인 컨텍스트 정보뿐만 아니라 검색기를 통해 획득된 관련 텍스트 세그먼트를 포함한다. 이를 통해 생성자는 질문 이면의 맥락을 더 잘 이해하고 정보가 풍부한 응답을 생성할 수 있다. 나아가, 생성기는 생성된 콘텐츠와 검색된 정보 사이의 일관성을 보장하기 위해 검색된 텍스트에 의해 안내된다. 생성 단계에서 일련의 목표된 노력으로 이어진 것은 입력 데이터의 다양성이며, 모두 쿼리 및 문서의 입력 데이터에 큰 모델을 더 잘 적응시키는 것을 목표로 한다. 우리는 회수 후 처리 및 미세 조정의 측면을 통해 발전기의 도입을 조사할 것이다.

### 검색 후 처리를 통해 검색 결과를 어떻게 향상시킬 수 있습니까?

언튜닝되지 않은 대용량 언어 모델의 관점에서 대부분의 연구는 GPT-4[1]와 같이 잘 알려진 대용량 언어 모델에 의존하여 문서 지식의 포괄적인 검색을 위해 강력한 내부 지식을 활용한다. 그러나 컨텍스트 길이 제한 및 중복 정보에 대한 취약성과 같은 이러한 대규모 모델의 고유한 문제는 지속된다. 이러한 문제를 완화하기 위해 일부 연구에서는 검색 후 처리에 노력을 기울였다. 검색 후 처리는 대형 문서 데이터베이스로부터 검색기에 의해 검색된 관련 정보를 추가로 처리, 필터링 또는 최적화하는 프로세스를 지칭한다. 주요 목적은 검색 결과의 품질을 향상시켜 사용자 요구나 후속 작업을 더 잘 충족시키는 것입니다. 검색 단계에서 얻은 문서를 재처리하는 과정으로 이해할 수 있다. 검색 후 처리의 동작은 일반적으로 정보 압축 및 결과 순위를 포함한다.

**Information Compression**

리트리버는 방대한 지식베이스로부터 관련 정보를 가져올 수 있지만, 여전히 검색 문서에서 상당한 양의 정보를 처리해야 하는 문제에 직면해 있다. 일부 기존 연구는 대용량 언어 모델의 문맥 길이를 증가시켜 이 문제를 해결하려고 시도하지만, 현재의 대용량 모델은 여전히 문맥 한계에 직면해 있다. 따라서, 특정 상황에서, 정보 응축이 필요하다. 요컨대, 정보 응축의 중요성은 주로 잡음 감소, 컨텍스트 길이 제한에 대한 대처, 생성 효과 강화라는 측면에서 구체화된다.

PRCA[15]는 정보 추출기를 훈련시킴으로써 이 문제를 해결하였다. 문맥 추출 단계에서는 입력 텍스트 \(S_{input}\)이 주어지면 입력 문서에서 축약된 문맥을 나타내는 출력 시퀀스 \(C_{extracted}\)를 생성할 수 있다. 훈련 과정의 목적은 \(C_{extracted}\)과 실제 문맥 \(C_{truth}\) 사이의 불일치를 최대한 최소화하는 것이다. 그들이 채택한 손실 함수는 다음과 같다:

\[minL(\theta)=-\frac{1}{N}\sum_{i=1}^{N}C_{truth}^{(i)}log(f.(S_{input}^{(i)}; \theta)) \tag{3}\]

where \(f.\) 는 정보 추출기이고, \(\theta\)는 추출기의 파라미터이다. RECOMP[16]도 마찬가지로 대조적 학습을 활용하여 정보 응축기를 학습한다. 각 훈련 데이터 포인트에 대해 1개의 양성 샘플과 5개의 음성 샘플이 존재한다. 인코더는 이 프로세스 동안 대비 손실[15]을 사용하여 트레이닝된다. 구체적인 최적화 목표는 다음과 같다:

\[-log\frac{e^{sim(x_{i},p_{i})}}{sim(x_{i},p_{i})+\sum_{n_{j}\in N_{i}}e^{sim(x _{i},p_{i})}} \tag{4}\]여기서 \(x_{i}\)은 학습 데이터, \(p_{i}\)은 양성 샘플, \(n_{j}\)은 음성 샘플, \(x,y)는 x와 y의 유사도를 계산하는 것이다. 또 다른 연구는 검색된 문서의 수를 감소시킴으로써 모델의 답변 정확도를 향상시키는 것을 목표로 하여 문서의 양을 더욱 간소화하는 것을 선택했다. [11] 본 논문에서는 LMM(Large Language Model)과 SLM(Small Language Model)의 장점을 통합한 "Filter-Ranker" 패러다임을 제안하였다. 이 패러다임에서 SLM은 필터 역할을 하는 반면 LLM은 재정렬 에이전트로 기능한다. LLM이 SLM에 의해 식별된 어려운 샘플의 일부를 재배열하도록 유도함으로써, 연구 결과는 다양한 정보 추출(IE) 작업에 걸쳐 상당한 개선을 나타낸다.

#### 4.1.1 Rerank

재정렬 모델의 중추적인 역할은 검색기에서 검색된 문서 집합을 최적화하는 데 있습니다. LLM은 추가 컨텍스트가 추가될 때 소급 성능으로 성능 저하를 경험하고 재정렬은 이 문제를 해결하기 위한 효과적인 솔루션을 제공한다. 핵심 아이디어는 문서 기록을 재배열하여 가장 관련성이 높은 항목을 맨 위에 배치함으로써 전체 문서 수를 고정된 수량으로 줄이는 것을 포함한다. 이는 검색 시 발생할 수 있는 컨텍스트 윈도우 확장 문제를 해결할 뿐만 아니라 검색 효율성과 응답성을 향상시키는데 기여한다[14].

재순서의 일부로서 컨텍스트 압축을 도입하는 것은 주어진 질의 컨텍스트에만 기초하여 관련 정보를 반환하는 것을 목표로 한다. 이 접근법의 이중적 의의는 개별 문서의 내용을 축소하고 전체 문서를 필터링함으로써 검색 결과에서 가장 관련성이 높은 정보의 표시를 집중시키는 데 있다. 따라서, 재정렬 모델은 정보 검색 프로세스 전반에 걸쳐 최적화 및 정제 역할을 하여, 후속 LLM 처리를 위해 보다 효과적이고 정확한 입력을 제공한다.

### 입력 데이터에 맞게 생성기를 최적화하는 방법

RAG 모델에서 생성기의 최적화는 아키텍처의 중요한 구성 요소이다. 생성자의 작업은 검색된 정보를 가져오고 관련 텍스트를 생성함으로써 모델의 최종 출력을 제공하는 것이다. 생성기를 최적화하는 목표는 사용자의 질의 요구를 더 잘 만족시키기 위해, 생성된 텍스트가 자연스럽고 검색된 문서들을 효과적으로 활용하는 것을 보장하는 것이다.

전형적인 LLM(Large Language Model) 생성 태스크에서, 입력은 보통 질의이다. RAG에서, 주요 차이점은 입력이 질의뿐만 아니라 검색기에 의해 검색된 다양한 문서(구조화/비구조화)를 포함한다는 사실에 있다. 추가 정보의 도입은 특히 더 작은 모델의 경우 모델의 이해도에 상당한 영향을 미칠 수 있다. 그러한 시나리오들에서, 질의 + 검색된 문서들의 입력에 적응하도록 모델을 미세 조정하는 것이 특히 중요해진다. 구체적으로, 미세 조정된 모델에 입력을 제공하기 전에, 통상적으로 검색기에 의해 검색된 문서들의 사후 검색 프로세싱이 존재한다. RAG에서 발전기를 미세 조정하는 방법은 LLM에 대한 일반적인 미세 조정 접근법과 본질적으로 유사하다는 점에 유의할 필요가 있다. 여기서는 데이터(형식화/비형식화)와 최적화 함수를 포함한 몇 가지 대표적인 작품을 간략히 소개한다.

#### 4.1.2 General Optimization Process

주어진 입력 x를 생성하는 모델의 능력을 훈련시키는 것을 목표로 하는 (입력, 출력)의 쌍을 포함하는 훈련 데이터를 지칭한다. 셀프멤[12]의 작업에는 비교적 고전적인 훈련 과정이 사용된다. 입력 x가 주어지면 관련 문서 z가 검색되고(논문에서 Top-1을 선택), (x, z)를 통합한 후 모델은 출력 y를 생성한다. 본 논문은 미세조정을 위한 두 가지 공통 패러다임, 즉 Joint-Encoder[1, 13, 14, 15]와 Dual-Encoder[16, 17, 18]를 사용한다. Joint-Encoder의 경우, 인코더-디코더에 기초한 표준 모델이 사용되며, 여기서 인코더는 처음에 입력을 인코딩하고, 디코더는 주의 메커니즘을 통해 인코딩된 결과를 결합하여 오토레시브 방식으로 토큰을 생성한다:

\[H=Encoder(x[SEP]m) \tag{5}\]

\[h^{i}=Decoder(CrossAttn(H),y<i) \tag{6}\]

\[P_{G_{\xi}}(.|x,y<i)=Softmax(h^{i}) \tag{7}\]

Dual-Encoder의 경우, 시스템은 입력(쿼리, 컨텍스트) 및 문서를 각각 인코딩하는 것을 담당하는 두 개의 독립적인 인코더를 설정한다. 그 다음, 출력은 디코더에 의해 순차적으로 양방향 교차-주의 프로세싱의 대상이 된다. 저자는 트랜스포머 [13]을 두 아키텍처의 빌딩 블록으로 사용하고 \(G_{\xi}\) Negative Log-Likelihood (NLL) 손실을 최적화하기로 선택했다.

\[H_{x}=SourceEncoder(x)H_{m}=MemoryEncoder(x) \tag{8}\]

\[h^{i}=Decoder(CrossAttn(H_{x},H_{m}),y<i) \tag{9}\]

\[\mathfrak{L}_{nll}=-\sum_{t=1}^{|y|}logP_{G_{\xi}}(y_{t}|x,m,y<t) \tag{10}\]

#### 4.1.3 Contrastive Learning 활용

트레이닝 데이터를 준비하는 단계에서, 일반적으로 생성되는 것은 입력과 출력 사이의 상호작용의 쌍이다. 이러한 상황에서, 모델은 "노출 편향" 문제를 유도할 수 있는 유일한 실제 출력에만 액세스할 수 있다[14]: 트레이닝 단계 동안, 모델은 임의의 다른 생성된 토큰에 액세스하지 않고 단일의 진정한 피드백에만 노출한다. 이는 다른 시나리오에 효과적으로 일반화하지 않고 학습 데이터의 특정 피드백에 과도하게 적합할 수 있기 때문에 응용에서 모델의 성능을 손상시킬 수 있다. 따라서 SURGE[14]에 의해 그래프-텍스트 대비 학습 방법이 제안되었다. 입력들 및 출력들 사이의 임의의 주어진 상호작용 쌍에 대해, 이러한 대조적 학습 접근법의 목적은 다음과 같이 정의될 수 있다:

\[\mathfrak{L}_{cont}=\frac{1}{2}log\frac{e^{sim(\zeta(z),\xi(h))/\iota}}{\sum_{h^{\prime}}e^{sim(\zeta(z),\xi(h^{\prime}))/\iota}}+\frac{1}{2}log\frac{e^{ sim(\zeta(z),\xi(h))/\iota}}{\sum_{z^{\prime}}e^{sim(\zeta(z^{\prime}),\xi(h))/\iota}} \tag{11}\] 여기서 \(\zeta\),\(\xi\)는 학습 가능한 선형 투영 레이어이다. z는 엔코더에서 그래프의 평균 표현이며, h는 디코더 표현의 평균이다. (z^{\prime}\),\(h^{\prime}\)은 각각 해당 음성 샘플을 나타낸다. 주어진 텍스트에서, "h" 및 "z"는 음성 샘플을 나타낸다. 대조적인 학습 목표를 도입함으로써 모델은 학습 데이터에서 볼 수 있는 것보다 다양하고 합리적인 답변을 더 잘 생성할 수 있도록 학습할 수 있다. 이는 과적합 위험을 완화하고 실제 시나리오에서 모델의 일반화 능력을 향상시키는 데 도움이 된다.

SANTA[11]의 작업은 구조화된 데이터를 포함하는 검색 작업을 다룰 때 구조적 및 의미론적 정보를 완전히 이해하기 위해 3단계 훈련 프로세스를 활용했다. 특히, 검색기의 학습 단계에서는 질의와 문서의 임베딩 표현을 최적화하는 것을 주요 목표로 하여 대조적 학습을 채택하였다. 구체적인 최적화 목표는 다음과 같다:

\[\mathfrak{L}_{DR}=-log\frac{e^{sim(q,d^{+})}}{e^{f(q,d^{+})}+\sum_{d-\in D^{- }}e^{sim(q,d^{-})}} \tag{12}\]

여기서 q 및 d는 인코더에 의해 인코딩된 질의 및 문서이다.\ (d^{-}\),\(d^{+}\)은 각각 음성 검체와 양성 검체를 나타낸다. 생성기의 초기 학습 단계에서는 대조적 학습을 활용하여 정형 데이터와 비정형 데이터의 해당 문서 설명을 정렬한다. 최적화 목표는 위와 같습니다.

또한, 참조문헌 [14, 15]에서 영감을 받은 생성기의 후반 훈련 단계에서 검색에서 텍스트 데이터 표현을 학습하는 데 엔터티 시멘틱의 현저한 효과를 인식했다. 따라서, 우리는 먼저 구조화된 데이터에서 엔티티 식별을 수행하고, 후속적으로 생성기의 트레이닝 데이터의 입력 섹션의 엔티티들에 마스크를 적용함으로써 생성기가 이들 마스크들을 예측할 수 있게 한다. 이하 최적화 목적은:

\[\mathfrak{L}_{MEP}=\sum_{j=1}^{k}-logP(Y_{d}(t_{j})|X_{d}^{mask},Y_{d}(t_{1},...,j-1)) \tag{13}\]

여기서 \(Y_{d}(y_{j}\)은 시퀀스 \(Y_{d}\)에서 j번째 토큰을 나타낸다. 그리고 \(Y_{d}=<mask>_{1}\), \(ent_{1}\),..., \(<mask>_{n}\), \(ent_{n}\)은 마스킹된 개체를 포함하는 그라운드 트루스 시퀀스를 나타낸다. 학습 과정에서 문맥으로부터 필요한 정보를 획득하여 마스킹된 개체를 복원하고, 텍스트 데이터의 구조적 의미를 이해하고, 구조화된 데이터에 관련 개체를 정렬한다. 우리는 은닉된 스팬을 채우고 엔티티 의미론을 더 잘 이해하기 위해 언어 모델을 최적화한다[21].

## 6 Augmentation of RAG

이 장은 주로 RAG 개발의 핵심 기술에 대해 자세히 설명하기 위해 증강 단계, 증강 데이터 소스 및 증강 과정의 세 가지 차원으로 구성된다. RAG 핵심 구성요소의 분류는 그림 4에 나와 있다.

### RAG: Augmentation 단계

지식 집약적 작업으로서 RAG는 언어 모델 훈련의 사전 훈련, 미세 조정 및 추론 단계에서 서로 다른 기술적 접근법을 사용한다.

#### Pre-training Stage

사전 학습 모델의 등장 이후, 연구자들은 사전 학습 단계에서 검색 방법을 통해 개방형 질의 응답(QA)에서 사전 학습 언어 모델(PTM)의 성능을 향상시키는 데 파고들었다. 사전 훈련된 모델에서 암묵적 지식을 인식하고 확장하는 것은 어려울 수 있다. REALM[1]은 보다 모듈적이고 해석 가능한 지식 임베딩 접근법을 소개한다. MLM(Masked Language Model) 패러다임에 따라, REALM은 사전 훈련과 미세 조정을 모두 검색-then-predict 과정으로 모델링하며, 여기서 언어 모델은 마스킹된 문장(x\), 모델링(P(x|y)\을 기반으로 마스킹된 토큰 \(y\)을 예측하여 사전 훈련한다.

RETRO[1]는 자기 회귀 언어 모델을 사전 트레이닝하기 위해 검색 증강을 활용함으로써, 대량의 라벨링된 데이터 세트로부터 검색하고 모델 파라미터를 상당히 감소시킴으로써 처음부터 대규모 사전 트레이닝을 가능하게 한다. RETRO는 GPT 모델들과 백본 구조를 공유하고, 외부 지식 베이스로부터 검색된 이웃 엔티티들의 특징들을 인코딩하기 위해 추가적인 RETRO 인코더를 도입한다. 추가적으로, RETRO는 RETRO 인코더로부터의 검색 정보를 효과적으로 통합하기 위해 디코더 트랜스포머 구조에 블록-와이즈 크로스-어텐션 계층들을 통합한다. RETRO는 표준 GPT 모델보다 더 낮은 복잡성을 달성한다. 더욱이, 언어 모델들을 재트레이닝할 필요없이 검색 데이터베이스를 업데이트함으로써 언어 모델들에 저장된 지식을 업데이트하는데 유연성을 제공한다[2].

Atla[17]는 사전 훈련 및 미세 조정 단계 모두에서 T5 아키텍처[16]를 사용하는 검색 메커니즘을 통합하는 유사한 접근법을 고려한다. 사전 훈련 전에, 사전 훈련된 T5로 인코더-디코더 LM 백본을 초기화하고, 사전 훈련된 Contriver로 조밀한 리트리버를 초기화한다. 사전 교육 프로세스 동안 1000 단계마다 비동기 인덱스를 새로 고칩니다.

COG[21]은 기존의 텍스트 모음에서 텍스트 조각(단어나 구절 등)을 점진적으로 복사하여 그 생성 과정을 정형화한 텍스트 생성 모델이다. COG는 단어를 순차적으로 선택하는 전통적인 텍스트 생성 모델과 달리 효율적인 벡터 검색 도구를 활용하여 텍스트 조각의 의미 있는 컨텍스트 표현을 계산하고 인덱싱한다. 결과적으로 텍스트 생성 작업은 일련의 복사 및 붙여넣기 작업으로 분해되며, 각 시간 단계에서 독립적인 어휘에서 선택하는 대신 텍스트 모음에서 관련 텍스트 조각을 찾는다. COG는 질의응답, 도메인 적응, 확장된 구문 색인 등 다양한 측면에서 RETRO보다 우수한 성능을 보인다.

한편, 스케일링 법칙의 발견에 따라 모델 모수가 급격히 증가하여 자기회귀 모델이 주류를 이루었다. 연구자들은 또한 RAG 접근법을 사용하여 더 큰 모델을 사전 훈련할 수 있는지 여부를 탐색하고 있다. RETRO의 확장인 RETRO++[22]는 모형의 매개변수 척도를 증가시켰다. 연구에 따르면 특히 오픈 도메인 질문 응답과 같은 지식 집약적 작업에서 텍스트 생성 품질, 사실적 정확도, 낮은 독성 및 다운스트림 작업 정확도에서 일관된 개선이 발견되었다. 이러한 연구 결과는 향후 기반 모델에 대한 검색과 함께 자기 회귀 언어 모델을 사전 훈련하는 유망한 방향을 강조한다.

요약하면, 증강된 사전-훈련의 이점들 및 제한들이 명백하다. 긍정적인 측면에서, 이 접근법은 더 강력한 기반 모델을 제공하여 복잡성, 텍스트 생성 품질 및 다운스트림 작업 성능에서 표준 GPT 모델을 능가한다. 또한, 순전히 사전 훈련된 모델에 비해 적은 수의 매개변수를 사용하여 더 높은 효율성을 달성한다. 특히 지식 집약적인 작업을 처리하는 데 탁월하여 도메인별 코퍼스에 대한 교육을 통해 도메인별 모델을 만들 수 있습니다. 그러나, 상당한 양의 사전 트레이닝 데이터 및 더 큰 트레이닝 리소스에 대한 요구뿐만 아니라 더 느린 업데이트 속도의 문제를 포함하는 단점이 있다. 특히 모델 크기가 증가함에 따라 검색 강화 훈련 비용이 상대적으로 높아진다. 이러한 한계에도 불구하고, 이 방법은 모델 강건성 측면에서 주목할 만한 특성을 보여준다. 일단 트레이닝되면, 순수 사전-트레이닝을 기반으로 한 검색-향상된 모델들은 외부 라이브러리 종속성의 필요성을 제거하여, 생성 속도와 운영 효율성을 모두 향상시킨다.

### Fine-tuning Stage

다운스트림 미세 조정 단계에서 연구자들은 주로 개방형 질문 응답 작업에서 개선된 정보 검색을 위해 검색기와 생성기를 미세 조정하는 다양한 방법을 사용했다. 리트리버 미세 조정과 관련하여 REPIUG[14]는 언어 모델(LM)을 블랙박스로 취급하고 조정 가능한 검색 모델을 통해 강화한다. REPLUG는 지도 신호를 통해 블랙박스 언어 모델로부터 피드백을 획득함으로써 초기 검색 모델을 개선한다. 반면에 UPRISE[11]는 다양한 태스크 집합에서 미세 조정을 통해 가볍고 다재다능한 검색기를 만들어 미세 조정을 검색한다. 이 검색기는 제로 샷 작업에 대한 검색 프롬프트를 자동으로 제공하여 작업 및 모델 전반에 걸쳐 범용성과 향상된 성능을 보여줄 수 있습니다.

동시에 생성자를 미세 조정하는 방법에는 예제의 메모리 풀을 통해 생성자를 미세 조정하는 Self-Mem[11], 반사 토큰을 생성하여 능동 검색 요구를 충족하는 Self-RAG[1] 등이 있다. RADIT[12] 방법은 수정 확률을 최대화하여 발전기와 리트리버를 모두 미세 조정한다.

그림 4: RAG의 핵심 구성요소 분류

답변은 검색 기능이 강화된 지시입니다. 이는 문서와 질의 사이의 의미적 유사성을 최소화하기 위해 생성기와 검색기를 업데이트하여 관련 배경 지식을 효과적으로 활용한다.

또한, SUGRE[14]는 대조적 학습의 개념을 도입한다. 리트리버와 생성기의 종단 간 미세 조정을 수행하여 매우 상세한 텍스트 생성 및 검색된 하위 그래프를 보장합니다. GNN(Graph Neural Networks) 기반의 상황 인식 서브 그래프 검색기를 이용하여, SURGE는 진행 중인 대화에 대응하는 지식 그래프로부터 관련 지식을 추출한다. 이것은 생성된 응답들이 검색된 지식을 충실히 반영하는 것을 보장한다. SURGE는 불변하면서도 효율적인 그래프 인코더와 이를 위한 그래프-텍스트 대조적 학습 목표를 사용한다.

요약하면, 미세 조정 단계 동안의 향상 방법은 몇 가지 특성을 나타낸다. 첫째, LLM과 리트리버를 모두 미세 조정하면 특정 작업에 더 잘 적응할 수 있으므로 RePlug[15] 및 RA-DIT[16]와 같은 방법에서 볼 수 있듯이 하나 또는 둘 모두를 동시에 미세 조정할 수 있는 유연성을 제공한다. 둘째, 이 미세조정의 장점은 UPRISE[17]에 의해 입증된 바와 같이 다양한 다운스트림 작업에 적응하는 것으로 확장되어 모델을 보다 다양하게 만든다. 추가적으로, 미세 조정은 모델들이 SUGRE 방법에 의해 강조된 바와 같이, 다양한 말뭉치들, 특히 그래프-구조화된 말뭉치에 유리한 다양한 데이터 구조들을 더 잘 수용할 수 있게 한다.

그러나 이 단계 동안의 미세 조정은 RAG 미세 조정을 위해 특별히 준비된 데이터 세트의 필요성 및 추론 단계 동안의 RAG에 비해 실질적인 계산 자원에 대한 요구와 같은 제한 사항이 있다. 전반적으로 미세 조정 동안 연구자들은 특정 요구 사항 및 데이터 형식에 따라 모델을 조정할 수 있는 유연성을 가지고 있어 모델의 출력 스타일을 조정할 수 있는 능력을 유지하면서 사전 훈련 단계에 비해 리소스 소비를 줄인다.

### Inference Stage

RAG 방법과 LLM의 통합은 추론 단계에서 널리 퍼진 연구 방향이 되었다. 특히, Naive RAG의 연구 패러다임은 추론 단계에서 검색 내용을 통합하는 것에 의존한다.

Naive RAG의 한계를 극복하기 위해 연구자들은 추론 단계에서 RAG에 더 풍부한 맥락을 도입했다. DSP[11] 프레임워크는 냉동 언어 모델(LM)과 검색 모델(RM) 사이에 자연어 텍스트를 전달하는 복잡한 파이프라인에 의존하며, 생성 품질을 향상시키기 위해 모델에 더 유익한 컨텍스트를 제공한다. PKG는 LLM의 파라미터를 변경하지 않고 관련 지식에 액세스할 수 있는 지식 유도 모듈을 LLM에 장착하여 모델이 보다 정교한 작업을 수행할 수 있도록 한다. 추가적으로, CREA-ICL[15]은 추가 정보를 획득하는 것을 돕기 위해 교차 언어 지식의 동기적 검색을 활용하는 반면, RECTE는 LLM들로부터 하나 이상의 단락들을 샘플링함으로써 컨텍스트를 형성한다.

추론 단계 동안, RAG의 프로세스를 최적화하는 것은 더 도전적인 작업에 대한 적응에 도움이 될 수 있다. 예를 들어, ITRG[13]은 정확한 추론 경로를 반복적으로 검색하고 검색함으로써 다단계 추론이 필요한 작업에 대한 적응성을 향상시킨다. ITER-RETGEN[14]은 검색과 생성을 통합하기 위해 반복적인 접근법을 사용하여 "검색 강화 세대"와 "세대 강화 검색"의 번갈아 가는 과정을 달성한다.

반면, IROCT[18]는 RAG와 CoT[12]의 개념을 병합하여 CoT 유도 검색을 대체하고 검색 결과를 사용하여 CoT를 개선한다. 이 방법은 다양한 QA 작업에 걸쳐 GPT-3의 성능을 크게 향상시켜 검색과 생성을 통합하는 잠재적인 이점을 강조한다.

요약하면, 추론 단계 향상 방법은 경량이고 비용 효율적이며 추가 교육이 필요하지 않으며 강력한 사전 훈련 모델을 활용하는 이점을 제공한다. 주요 강점은 빠르고 저렴한 특성과 함께 요구 사항에 더 잘 맞는 컨텍스트를 제공하는 데 중점을 두고 미세 조정 동안 LLM의 매개변수를 동결하는 것이다. 그러나, 이 접근법은 또한 기초 모델의 능력에 의해 제약되는 동안 추가적인 데이터 처리 및 프로세스 최적화의 필요성을 포함하는 몇 가지 한계를 갖는다. 전형적으로, 이 방법은 종종 상이한 태스크들의 요건들을 더 잘 충족시키기 위해 단계적 추론, 반복적 추론, 및 적응적 검색과 같은 프로세스 최적화 기술들과 결합된다.

### 보강 데이터 원본

데이터 소스는 RAG 효과에 중요한 요소이다. 다양한 데이터 소스는 서로 다른 처리 방법을 필요로 하는 뚜렷한 세분성과 지식의 차원을 제공한다. 주로 비정형 데이터, 정형 데이터, LLM에 의해 생성된 콘텐츠의 세 가지 범주로 나뉜다.

#### 비정형 데이터로 확장

비정형 데이터는 주로 순수한 텍스트 말뭉치에서 파생된 텍스트 데이터를 포함한다. 또한, 다른 텍스트 데이터는 대형 모델 미세 조정[17] 및 교차 언어 데이터[15]에 사용되는 프롬프트 데이터와 같은 검색 소스 역할을 할 수 있다.

텍스트 세분성의 관점에서, 공통 청크(문장 포함)를 넘어서, 검색 유닛은 토큰(예를 들어, kNN-LM[11]), 구(예를 들어, NPM[18], COG[20]), 및 문서 단락일 수 있다. 세밀한 검색 단위는 종종 희귀 패턴 및 도메인 외 시나리오를 더 잘 처리할 수 있지만 검색 비용이 증가합니다.

FLARE는 단어 수준에서 능동적인 검색 전략을 사용하여 LM이 낮은 확률의 단어를 생성할 때만 검색을 수행한다. 이 방법은 관련 문서의 검색을 위해 임시 다음 문장을 생성한 다음, 후속 문장을 예측하기 위해 검색된 문서의 조건 하에서 다음 문장을 다시 생성하는 것을 포함한다.

청크 레벨에서, RETRO는 이전 청크를 사용하여 가장 가까운 이웃 청크를 검색하고 이 정보를 이전 청크의 컨텍스트 정보와 통합하여 다음 청크의 생성을 안내한다. RETRO는 검색 데이터베이스에서 가장 가까운 이웃 블록 \(N(C_{i-1})\)을 검색한 후, 이전 블록 \((C_{1},\dots,C_{i-1})\)의 형성 맥락과 \(N(C_{i-1})\)의 검색 정보를 교차 주의를 통해 융합하여 다음 블록 \(C_{i}\)의 생성을 안내한다. 인과관계를 유지하기 위해, \(i\)-번째 블록 \(C_{i}\)의 자기회귀 생성은 \(N(C_{i-1})\)의 가장 가까운 이웃만을 사용할 수 있고 \(N(C_{i})\은 사용할 수 없다.

**구조화된 데이터로 확장**

지식 그래프(KG: Knowledge Graphs)와 같은 구조화된 데이터 소스는 점차 RAG의 패러다임으로 통합되고 있다. 검증된 KG는 더 높은 품질의 컨텍스트를 제공하여 모델 환각의 가능성을 줄일 수 있습니다.

RET-LLM[11]은 미래의 사용을 위해 과거 대화에서 관계 트리플을 추출하여 개인화된 지식 그래프 메모리를 구성한다. SUGRE[12]는 모델이 문맥적으로 무관한 응답들을 생성하는 것을 방지하기 위해 그래프 신경망(GNN)을 사용하여 지식 그래프로부터 검색된 관련 서브그래프들을 임베딩한다. SUGRE[12]는 그래프 구조를 PTM의 표현 공간에 반영하는 그래프 인코딩 방법을 사용하며, 그래프-텍스트 모드 간의 다중 모달 대조적 학습 목표를 활용하여 검색된 사실과 생성된 텍스트 간의 일관성을 보장한다. KnowledgeGPTIwang _et al._[13]은 코드 포맷으로 지식베이스(KB)에 대한 검색 쿼리를 생성하고, 미리 정의된 KB 연산 함수를 포함한다. 검색 외에도, KnowledgeGPT는 또한 개별 사용자 요구를 충족시키기 위해 개인화된 지식 베이스에 지식을 저장할 수 있는 능력을 제공한다. 이러한 구조화된 데이터 소스는 RAG에 더 풍부한 지식과 컨텍스트를 제공하여 모델 성능 향상에 기여한다.

**LLM Generated Content RAG**

RAG가 회수한 보조 정보가 항상 효과적인 것은 아니며 심지어 부정적인 영향을 미칠 수 있음을 관찰한 일부 연구에서는 LLM의 내부 지식을 더 깊이 파고들어 RAG의 패러다임을 확장했다. 이 접근법은 LLM 자체에 의해 생성된 콘텐츠를 검색에 활용하여 다운스트림 태스크에서 성능을 향상시키는 것을 목표로 한다. 다음은 이 범주 내에서 주목할 만한 연구의 개요이다.

SKRI[14]는 라벨링된 트레이닝 세트를 사용하여, 모델이 알려진 대로 직접 대답할 수 있는 질문과 검색 향상을 필요로 하는 질문을 알려지지 않은 것으로 분류한다. 이 모델은 질문이 알려져 있는지 여부를 식별하도록 훈련되어 알려지지 않은 것으로 식별된 입력에만 검색 향상을 적용하는 동시에 나머지에 직접 응답한다.

GenRead[14]는 LLM 생성기를 검색기로 대체합니다. 실험 결과, 생성된 문맥문서에 정답이 포함된 상황이 Naive RAG에 의해 검색된 상황보다 더 우세함을 알 수 있었다. 생성된 답변은 또한 우수한 품질을 보여줍니다. 저자는 문서 수준의 컨텍스트를 생성하는 작업과 인과적 언어 모델링의 사전 훈련 목표 사이의 정렬에 기인하여 모델 매개변수에 저장된 세계 지식을 더 잘 활용할 수 있도록 한다.

Sellfemml[1]은 검색 강화 생성기를 반복적으로 사용하여 무제한 메모리 풀을 생성한다. 메모리 선택기는 후속 세대들을 위한 메모리로서 출력을 선택하기 위해 채용된다. 이 출력은 원래 질문에 대한 이중 문제 역할을 합니다. 원본 문제와 이중 문제를 결합하여 검색 강화 생성 모델은 자체 출력을 활용하여 자신을 향상시킬 수 있다.

이러한 다양한 접근 방식은 모델 성능과 효율성을 높이기 위해 RAG 검색 향상에 혁신적인 전략을 보여준다.

### Augmentation Process

대부분의 RAG 연구는 일반적으로 단일 검색 및 생성 프로세스만을 수행한다. 그러나, 단일 검색은 중복된 정보를 포함할 수 있으며, 이는 "중간에 분실" 현상으로 이어진다[10]. 이 중복 정보는 핵심 정보를 모호하게 하거나 실제 답변과 반대되는 정보를 포함시켜 생성 효과에 부정적인 영향을 미칠 수 있다[15]. 또한 단일 검색에서 얻은 정보는 다단계 추론을 필요로 하는 문제에서 제한적이다.

검색 프로세스를 최적화하기 위한 현재의 방법들은 주로 반복 검색 및 적응 검색을 포함한다. 이들은 모델이 검색 프로세스 동안 여러 번 반복하거나 검색 프로세스를 적응적으로 조정하여 상이한 태스크 및 시나리오를 더 잘 수용할 수 있게 한다.

**Iterative Retrieval**

원본 질의 및 생성된 텍스트에 기초하여 문서를 정기적으로 수집하는 것은 LLMs[15, 16]에 대한 추가 자료를 제공할 수 있다. 다수의 반복 검색에서 추가 참조를 제공하는 것은 후속 응답 생성의 견고성을 개선시켰다. 그러나, 이 방법은 생성되고 검색된 문서들을 분리하기 위해 주로 n개의 토큰들의 시퀀스에 의존하기 때문에, 의미적으로 불연속적일 수 있고 잠재적으로 잡음이 많고 쓸모없는 정보의 수집으로 이어질 수 있다.

특정 데이터 시나리오에는 재귀적 검색과 다중 홉 검색이 사용된다. 재귀적 검색은 먼저 구조화된 색인을 통해 데이터를 처리한 후 레벨별로 검색할 수 있다. 계층적으로 풍부한 문서를 검색할 때 전체 문서 또는 긴 PDF의 각 섹션에 대해 요약할 수 있습니다. 그런 다음 요약에 기초하여 검색이 수행된다. 문서를 결정한 후, 내부 청크들에 대해 제2 검색이 수행되고, 따라서 재귀적 검색을 실현한다. 다중 홉 검색은 그래프 구조의 데이터 소스에서 정보를 더 마이닝하는 데 자주 사용된다[10].

일부 방법은 검색 및 생성 단계를 반복합니다. ITER-RETGEN[17]은 정보의 재생이 필요한 작업을 위해 "검색-향상된 생성" 및 "세대-향상된 검색"을 협력적으로 활용한다. 즉, 모델은 입력된 태스크에 응답하기 위해 태스크를 완료하는데 필요한 컨텐츠를 사용하고, 이러한 타겟 컨텐츠는 보다 관련된 지식을 검색하기 위한 정보 컨텍스트의 역할을 한다. 이것은 다른 반복에서 더 나은 응답을 생성하는 데 도움이 된다.

IRCoT[18]는 또한 생성된 각 문장에 대한 문서 검색을 탐색하여 사고 사슬의 모든 단계에서 검색을 도입한다. CoT를 사용하여 검색을 안내하고 검색 결과를 사용하여 CoT를 개선하여 의미적 완전성을 보장한다.

**Adaptive Retrieval**

실제로 이전 두 섹션에서 설명한 RAG 방법은 검색이 사전 반복되는 수동 접근법을 따른다. 관련 문서를 조회하고 컨텍스트를 기반으로 LLM에 입력하는 이 방법은 효율성 문제로 이어질 수 있다. Flare[14]와 Self-RAG[1]에 의해 소개된 방법과 같은 적응적 검색 방법은 RAG 검색 과정을 최적화하여 LLM이 검색의 시기와 내용을 능동적으로 판단할 수 있도록 한다. 이는 검색된 정보의 효율성 및 관련성을 개선하는 데 도움이 된다.

실제로 LLM이 도구를 적극적으로 사용하고 판단을 내리는 방식은 RAG에서 비롯된 것이 아니라 대형 모델의 에이전트[13, 14, 15]에서 널리 사용되어 왔다. Graph-Toolformer[14]의 검색 단계는 크게 LLM이 검색기를 적극적으로 사용하고, Self-Ask와 DSP[16]는 LLM 검색 쿼리를 트리거하기 위해 소수의 샷 프롬프트를 사용하려고 한다. LLM은 필요하다고 생각할 때 에이전트의 도구 호출과 유사하게 필요한 자료를 수집하기 위해 관련 쿼리를 검색하기로 결정할 수 있다.

WebGPT[17]는 강화 학습 프레임워크를 사용하여 텍스트 생성을 위한 검색 엔진을 사용하도록 GPT-3 모델을 자동으로 트레이닝한다. 그것은 검색 엔진에 대한 질의, 스크롤 순위, 인용 참조를 포함하는 동작을 수행하기 위해 특수 토큰을 사용한다. 이를 통해 GPT-3는 텍스트 생성을 위한 검색 엔진을 활용할 수 있다.

반면에 flare[14]는 검색의 타이밍을 자동화하고 생성된 텍스트의 확률을 기반으로 주기적인 문서 검색 비용을 해결한다. 생성 과정에서 LLM의 신뢰도를 나타내는 지표로 확률을 사용한다. 용어의 확률이 미리 정의된 임계치 아래로 떨어질 때, 정보 검색 시스템은 참조들을 검색하고 더 낮은 확률로 용어들을 제거할 것이다. 이 접근 방식은 LLM에 추가 지식이 필요할 수 있는 상황을 처리하도록 설계되었습니다.

Self-RAG[1]는 Reflection 토큰이라는 중요한 혁신을 도입한다. 이러한 특수 토큰은 출력을 검토 하기 위해 생성 되며 검색 및 비평의 두 가지 유형으로 제공 됩니다. 모델은 문단을 검색할 시기를 자율적으로 결정하거나 검색을 트리거하기 위해 설정된 임계값을 사용할 수 있다. 검색이 필요할 때, 생성기는 다수의 단락들을 동시에 처리하며, 최상의 시퀀스를 얻기 위해 프래그먼트-레벨 빔 검색을 수행한다. 각 세분에 대한 점수는 Critic 점수를 사용하여 업데이트되며, 이러한 가중치는 모델의 행동을 맞춤화하기 위해 추론 프로세스 동안 조정될 수 있다. Self-RAG 프레임워크는 또한 LLM이 추가 분류기의 훈련을 피하거나 NLI 모델에 의존하여 리콜이 필요한지 여부를 자율적으로 결정할 수 있게 한다. 이는 자율적으로 입력을 판단하고 정확한 답변을 생성하는 모델의 능력을 향상시킨다.

## 7 RAG 평가

RAG의 개발과 최적화를 탐구함에 있어, RAG의 성능을 효과적으로 평가하는 것이 중심 이슈로 대두되었다. 이 장에서는 주로 평가 방법, RAG의 주요 메트릭, 보유해야 하는 능력 및 일부 주류 평가 프레임워크에 대해 논의한다.

### Evaluation Methods

RAG의 효과를 평가하는 방법에는 크게 두 가지 방법이 있다: 독립 평가와 종단 간 평가[11, 15].

### Independent Evaluation

독립적인 평가는 검색 모듈 및 생성(읽기/합성) 모듈을 평가하는 것을 포함한다.

1. **검색 모듈** 쿼리 또는 작업에 따라 순위 항목에서 시스템(검색 엔진, 추천 시스템 또는 정보 검색 시스템과 같은)의 효율성을 측정하는 일련의 메트릭은 일반적으로 RAG 검색 모듈의 성능을 평가하는 데 사용됩니다. 그 예로는 Hit Rate, MRR, NDCG, Precision 등이 있다.
2. **생성 모듈** 여기서 생성 모듈은 일반적으로 종단 간 평가되는 최종 답변/응답 생성과 구별되는 검색된 문서를 쿼리에 보충하여 형성되거나 합성된 입력을 나타냅니다. 생성 모듈에 대한 평가 메트릭은 주로 질의 질문에 대한 검색된 문서의 관련성을 측정하는 컨텍스트 관련성에 중점을 둔다.

### End-to-End Evaluation

엔드 투 엔드 평가는 입력 질의와의 모델 생성 답변의 관련성 및 정렬을 포함하는 주어진 입력에 대해 RAG 모델에 의해 생성된 최종 응답을 평가한다. 콘텐츠 생성 목표의 관점에서 평가는 레이블이 없는 콘텐츠와 레이블이 있는 콘텐츠로 구분할 수 있다. 라벨링되지 않은 콘텐츠 평가 메트릭은 답변 충실도, 답변 관련성, 무해성 등을 포함하는 반면, 라벨링된 콘텐츠 평가 메트릭은 Accuracy 및 EM을 포함한다. 추가적으로 평가 방법의 관점에서 종단 간 평가는 수동 평가와 LLM을 이용한 자동 평가로 구분할 수 있다. 이상의 내용은 RAG에 대한 종단 간 평가의 일반적인 사례를 요약한 것이다. 또한, 질문-응답 태스크[11, 12], 요약 태스크[14], 기계 번역 태스크[14]를 위한 UniEval 및 E-F1과 같은 특정 도메인에서 RAG의 적용을 기반으로 특정 평가 메트릭을 채택한다. 이러한 메트릭은 다양한 특정 애플리케이션 시나리오에서 RAG의 성능을 이해하는 데 도움이 됩니다.

### 키 메트릭 및 기능

기존 연구는 종종 검색 증강 생성이 다른 LLM에 미치는 영향에 대한 엄격한 평가가 부족하다. 대부분의 경우, 다양한 다운스트림 태스크 및 상이한 검색기를 갖는 RAG의 적용의 평가는 상이한 결과를 산출할 수 있다. 그러나 일부 학술 및 공학 실무에서는 RAG에 대한 일반적인 평가 지표와 효과적인 사용에 필요한 능력에 초점을 맞추고 있다. 이 섹션에서는 주로 RAG의 효과를 평가하기 위한 주요 메트릭과 성능을 평가하기 위한 필수 능력을 소개한다.

#### Key Metrics

최근 OpenAI 보고서[15]는 RAG 및 그 평가 메트릭을 포함하여 대용량 언어 모델(LLM)을 최적화하기 위한 다양한 기술을 언급했다. 또한 RAGAS[14] 및 ARES[2]와 같은 최신 평가 프레임워크에는 RAG 평가 메트릭도 포함됩니다. 이 작업을 요약하면, 세 가지 핵심 메트릭은 주로 답변의 충실성, 답변 관련성 및 맥락 관련성에 중점을 둔다.

1. **신뢰성** 이 메트릭은 모델에 의해 생성된 답변이 지정된 컨텍스트에 맞게 유지되어야 하며, 답변이 컨텍스트 정보와 일치하고 이탈하거나 모순되지 않도록 해야 함을 강조합니다. 이러한 평가 측면은 대형 모델의 환상을 해결하는 데 필수적이다.
2. **답변 관련성** 이 메트릭은 생성된 답변이 제기된 질문과 직접 관련되어야 함을 강조합니다.
3. **컨텍스트 관련성** 이 메트릭은 검색된 컨텍스트 정보를 가능한 한 정확하고 대상으로 지정하여 관련 없는 콘텐츠를 피해야 합니다. 결국, 긴 텍스트를 처리하는 것은 LLM에 비용이 많이 들고, 너무 많은 관련 없는 정보는 컨텍스트를 활용하는 LLM의 효율성을 감소시킬 수 있다. OpenAI 보고서는 또한 질문에 답하는 데 필요한 모든 관련 정보를 검색하는 모델의 능력을 측정하는 보충 메트릭으로 "컨텍스트 리콜"을 언급했다. 이 메트릭은 RAG 검색 모듈의 검색 최적화 레벨을 반영한다. 낮은 리콜률은 더 관련성 있는 콘텐츠 검색을 보장하기 위해 재순위 매커니즘을 도입하거나 임베딩을 미세 조정하는 것과 같은 검색 기능의 최적화에 대한 잠재적인 필요성을 나타낸다.

#### Key abilities

RGB[15]의 작업은 RAG에 필요한 4가지 기본 능력인 잡음 강인성, Negative Rejection, Information Integration, Counterfactual Robustness를 기준으로 서로 다른 대형 언어 모델의 성능을 분석하여 검색 증강 생성을 위한 벤치마크를 구축했다. RGB는 다음 4가지 능력에 중점을 둔다.

1. **Noise Robustness** 이 기능은 질문과 관련 되지만 유용한 정보가 포함 되지 않은 노이즈 문서를 처리 하는 모델의 효율성을 측정 합니다.
2. **부정 거부** 모델에 의해 검색된 문서가 질문에 응답하는 데 필요한 지식이 부족할 때 모델이 응답을 올바르게 거부해야 합니다. 음성 거부 테스트 설정에서 외부 문서는 노이즈만 포함합니다. 이상적으로는 LLM이 "정보 부족" 또는 유사한 거부 신호를 발행해야 한다.
3. **정보 통합** 이 기능은 모델이 여러 문서의 정보를 통합하여 더 복잡한 질문에 답할 수 있는지 여부를 평가합니다.
4. **반사실적 견고성** 이 테스트는 검색된 정보의 잠재적 위험에 대한 지침을 받을 때 모델이 문서에서 알려진 잘못된 정보를 식별하고 처리할 수 있는지 여부를 평가하는 것을 목표로 합니다. 반사실적 강건성 검정에는 LLM이 직접 답할 수 있는 문항이 포함되지만 관련 외부 문서에는 사실적 오류가 포함되어 있다.

### Evaluation Frameworks

최근 LLM 커뮤니티는 자동 평가를 위해 "판사로 LLM"의 사용을 탐색하고 있으며, 많은 사람들이 자체 LLM 애플리케이션 출력을 평가하기 위해 강력한 LLM(예: GPT-4)을 활용하고 있다. Databricks가 GPT-3.5와 GPT-4를 LLM 심사위원으로 사용하여 챗봇 애플리케이션을 평가하는 연습은 LLM을 자동 평가 도구로 사용하는 것이 효과적임을 시사한다[15]. 그들은 이 방법이 또한 RAG 기반 애플리케이션을 효율적이고 비용 효율적으로 평가할 수 있다고 믿는다.

RAG 평가 프레임워크 분야에서 RAGAS와 ARES는 비교적 새로운 것이다. 이러한 평가의 핵심 초점은 답변의 충실성, 답변 관련성, 맥락 관련성의 세 가지 주요 측정 지표에 있다. 또 업계가 제안한 오픈소스 라이브러리인 TruLens도 비슷한 평가 모드를 제공한다. 이러한 프레임워크는 모두 LLM을 평가를 위한 판사로 사용한다. TruLens는 RAGAS와 유사하므로 본 장에서는 구체적으로 RAGAS와 ARES를 소개하기로 한다.

#### Rags

이 프레임워크는 검색 시스템의 관련 및 주요 컨텍스트 단락을 식별하는 능력, LLM의 이러한 단락을 충실하게 사용하는 능력 및 세대 자체의 품질을 고려한다. RAGAS는 간단한 필기 프롬프트를 기반으로 하는 평가 프레임워크로, 이러한 프롬프트를 사용하여 완전 자동화된 방식으로 답변 충실도, 답변 관련성 및 컨텍스트 관련성의 세 가지 측면을 측정한다. 이 프레임워크의 구현 및 실험에서 모든 프롬프트는 OpenAI API[14]를 통해 제공되는 gpt-3.5-turbo-16k 모델을 사용하여 평가된다.

**Algorithm Principles**

1. 답변 충실도 평가: LLM을 이용하여 답변을 개별 진술로 분해하고 각 진술이 문맥과 일치하는지 검증한다. 궁극적으로 지지된 진술의 수와 총 진술의 수를 비교하여 "충실도 점수"를 산출한다.
2. 답변 적합성 평가: LLM을 사용하여 잠재적인 질문을 생성하고 이들 질문과 원래 질문 사이의 유사성을 계산한다. 정답 적합성 점수는 생성된 모든 질문과 원래 질문의 평균 유사도를 계산하여 도출된다.
3. 문맥 관련성 평가: LLM을 이용하여 질문과 직접적으로 관련된 문장을 추출하고, 문맥 내 총 문장 수에 대한 이들 문장의 비율을 문맥 관련성 점수로 사용한다.

ARES는 상황 관련성, 답변 충실성, 답변 적절성의 세 가지 측면에서 RAG 시스템의 성능을 자동으로 평가하는 것을 목표로 한다. 이러한 평가 메트릭은 RAGAS의 메트릭과 유사합니다. 그러나 단순한 필기 프롬프트를 기반으로 하는 새로운 평가 프레임워크인 RAGAS는 ARES 작업의 의미 중 하나인 새로운 RAG 평가 설정에 대한 적응성이 제한적이다. 또한 평가에서 입증된 바와 같이 ARES는 RAGAS보다 훨씬 낮은 성능을 보인다.

ARES는 적은 양의 수동 주석 데이터와 합성 데이터를 사용하여 평가 비용을 줄이고, Predictive-Driven Reasoning (PDR)을 사용하여 통계적 신뢰 구간을 제공하여 평가의 정확도를 향상시킨다[14].

**Algorithm Principles**

1. 합성 데이터 세트 생성: ARES는 초기에 긍정 및 부정 샘플을 생성하기 위해 언어 모델을 사용하여 타겟 코퍼스의 문서로부터 합성 질문 및 답변을 생성한다.
2. LLM 판단 준비: 다음으로, ARES는 합성 데이터 세트를 사용하여 경량 언어 모델을 미세 조정하여 문맥 관련성, 답변 충실성 및 답변 적절성을 평가하도록 훈련시킨다.
3. 신뢰 구간을 이용한 RAG 시스템 랭킹: 마지막으로, ARES는 이러한 판단 모델을 적용하여 RAG 시스템을 스코어링하고 PPI 방법을 사용하여 수동으로 주석이 달린 검증 세트와 결합하여 신뢰 구간을 생성함으로써 RAG 시스템의 성능을 신뢰성 있게 추정한다.

## 8 Future Prospects

이 장에서는 RAG에 대한 세 가지 미래 전망, 즉 RAG의 수직 최적화, 수평 확장 및 생태계를 조사한다.

### RAG의 수직 최적화

지난 1년 동안 RAG 기술의 급속한 발전에도 불구하고 수직 영역에는 추가 조사가 필요한 몇 가지 영역이 여전히 있다.

첫째, RAG에서 긴 맥락의 문제는 중요한 도전이다. 문헌 [23]에서 언급된 바와 같이, RAG의 생성 단계는 LLM의 컨텍스트 윈도우에 의해 제한된다. 창이 너무 짧으면 관련 정보가 충분하지 않을 수 있으며 너무 길면 정보 손실로 이어질 수 있습니다. 현재 LLM의 컨텍스트 창을 무한 컨텍스트 범위까지 확장하는 것은 LLM 개발에 있어 중요한 방향이다. 그러나, 일단 컨텍스트 윈도우 제약이 제거되면, RAG가 어떻게 적응해야 하는지는 주목할 만한 질문으로 남아 있다.

둘째, RAG의 견고성은 또 다른 중요한 연구 초점이다. 검색 중에 무관한 잡음이 나타나거나, 검색된 콘텐츠가 사실과 모순되는 경우, RAG의 유효성에 상당한 영향을 미칠 수 있다. 이런 상황을 비유적으로 '독버섯에 책을 펴다'라고 한다. 따라서 RAG의 견고성을 높이는 것은 [24, 13]과 같은 연구에서 나타나는 바와 같이 연구자의 관심을 점점 더 얻고 있다.

셋째, RAG와 Fine-tuning의 시너지 문제도 주요 연구점이다. 하이브리드는 RADIT[10]에 의해 예시되는, RAG에서의 주류 방법들 중 하나가 점차로 되었다. 모수화와 비모수화의 장점을 동시에 얻기 위해 양자의 관계를 어떻게 조정할 것인가는 어드레싱이 필요한 문제이다.

마지막으로, RAG의 엔지니어링 실습은 중요한 관심 분야이다. 실행의 용이성과 기업 엔지니어링 요구와의 정렬은 RAG의 상승에 기여했다. 그러나 엔지니어링 실무에서는 대규모 지식베이스 시나리오에서 검색 효율성 및 문서 회수율을 향상시키는 방법, 문서의 출처, 메타데이터 또는 기타 정보를 공개하도록 LLM이 유도되는 것을 방지하는 것과 같은 엔터프라이즈 데이터 보안을 보장하는 방법과 같은 질문이 해결이 필요한 중요한 문제이다[1].

**RAG의 수평 확장**

RAG에 대한 연구는 수평 분야에서 빠르게 확장되었다. 초기 텍스트 질의 응답 도메인에서 시작하여 RAG의 아이디어는 이미지, 코드, 구조화된 지식, 오디오 및 비디오 등과 같은 더 많은 모달 데이터에 점차적으로 적용되었다. 이런 점에서 이미 많은 작품들이 있다.

영상 분야에서는 동결된 영상 인코더와 대규모 언어 모델을 시각 언어 사전 훈련에 사용하는 BLIP-2[15]의 프로포지요살이 모델 훈련 비용을 낮췄다. 추가적으로, 모델은 제로 샘플들로부터 이미지-텍스트 변환들을 생성할 수 있다. 텍스트 생성 분야에서 VBR[15] 방법은 언어 모델의 텍스트 생성을 안내하는 이미지를 생성하는 데 사용되며, 이는 개방형 텍스트 생성 작업에 상당한 영향을 미친다.

코드 필드에서는 코드와 관련된 소규모 학습을 위해 RPS[21]을 사용한다. 인코딩 또는 빈도 분석을 통해 개발자의 작업과 유사한 코드 예가 자동으로 검색된다. 이 기법은 테스트 어써션 생성 및 프로그램 복구 작업에서 그 효과가 입증되었다. 구조화된 지식 분야에서 CoK[15] 힌트와 같은 방법은 먼저 지식 그래프에서 입력 질문과 관련된 사실들을 검색한 후, 이러한 사실들을 힌트 형태로 입력에 추가한다. 이 방법은 지식 그래프 질문 응답 작업에서 잘 수행되었다.

오디오 및 비디오 분야의 경우, GSS[13] 방법은 음성 어휘 뱅크로부터 오디오 클립을 검색 및 연결하여 MT 데이터를 ST 데이터로 즉시 변환한다. UEOP[10]은 음성 대 텍스트 매핑을 위한 외부 오프라인 전략을 도입하여 종단 간 자동 음성 인식의 새로운 돌파구를 소개한다. 텍스트 투 스피치 방법에 의해 생성된 오디오 임베딩 및 의미론적 텍스트 임베딩은 KNN 기반 어텐션 융합을 통해 ASR을 바이어싱할 수 있어 도메인 적응 시간을 효과적으로 단축시킬 수 있다. Vid2Seq[24] 아키텍처는 특별한 시간 표시를 도입함으로써 언어 모델을 향상시켜 동일한 출력 시퀀스 내에서 이벤트 경계 및 텍스트 설명을 원활하게 예측할 수 있게 한다.

### RAG 생태계

**다운스트림 작업 및 평가**

광범위한 지식 기반에서 관련 정보를 통합함으로써 RAG는 복잡한 쿼리를 처리하고 정보가 풍부한 응답을 생성하는 언어 모델의 능력을 향상시키는 데 상당한 잠재력을 입증했다. 많은 연구에서 RAG가 개방형 질의 응답 및 사실 검증과 같은 다양한 다운스트림 작업에서 잘 수행된다는 것을 보여주었다. RAG 모델은 다운스트림 응용 프로그램에서 정보의 정확성과 관련성을 향상시킬 뿐만 아니라 응답의 다양성과 깊이를 증가시킨다.

RAG의 성공을 감안할 때 다중 도메인 응용 프로그램에서 모델의 적응성과 보편성을 탐색하는 것은 향후 작업의 일부가 될 것이다. 여기에는 의학, 법률 및 교육과 같은 전문 영역 지식 질문 응답에서의 사용이 포함된다. 전문 도메인 지식 질문 응답과 같은 다운스트림 태스크의 적용에서 RAG는 미세 조정보다 더 낮은 훈련 비용과 더 나은 성능 이점을 제공할 수 있다.

동시에, 다양한 다운스트림 태스크에서 RAG의 적용을 평가하고 최적화하기 위한 평가 시스템의 개선은 특정 태스크에서 모델의 효율성과 이점을 위해 중요하다. 여기에는 컨텍스트 관련성, 콘텐츠 창의성 및 무해성과 같은 다양한 다운스트림 작업에 대한 보다 정확한 평가 메트릭 및 프레임워크 개발이 포함된다.

또한 RAG를 통해 모델의 해석 가능성을 높여 사용자가 모델이 특정 응답을 하는 방법과 이유를 더 잘 이해할 수 있도록 하는 것도 의미 있는 작업이다.

**Technical Stack**

RAG의 생태계에서 관련 기술 스택의 개발이 견인차 역할을 해왔다. 예를 들어, LangChain과 LLamaIndex는 ChatGPT의 인기로 빠르게 널리 알려졌다. 그들은 둘 다 풍부한 RAG 관련 API를 제공하며, 점차 대형 모델 시대에 필수 불가결한 기술 중 하나가 되었다. 한편, 새로운 형태의 기술 스택이 지속적으로 개발되고 있다. 랭체인이나 라마인덱스만큼 많은 기능을 제공하지는 않지만 독특한 특성에 더 집중한다. 예를 들어, Flowise AI6는 Low-code를 강조하여, 사용자가 단순히 드래그 앤 드롭으로 코드를 작성하지 않고도 RAG로 대표되는 다양한 AI 어플리케이션을 구현할 수 있도록 한다. 다른 신흥 기술로는 HayStack, Meltno, Coherence Coral 등이 있다.

각주 6: [https://flowiseai.com](https://flowiseai.com)

AI-네이티브 프레임워크 외에도 전통적인 소프트웨어 또는 클라우드 서비스 제공업체도 서비스 범위를 확장했다. 예를 들어 벡터 데이터베이스 회사 Weaviate에서 제공하는 Verba7은 개인 비서에 초점을 맞춘다. 아마존은 사용자에게 RAG 사고를 기반으로 한 지능형 기업 검색 서비스 도구 켄드라를 제공한다. 사용자는 내장된 커넥터를 통해 서로 다른 콘텐츠 리포지토리에서 검색할 수 있습니다.

각주 7: [https://github.com/weaviate/Verba](https://github.com/weaviate/Verba)

기술 스택과 RAG의 개발은 상호 강화되고 있다. 새로운 기술은 기존 기술 스택에 더 높은 요구를 제기하는 반면, 기술 스택의 기능의 최적화는 RAG 기술의 개발을 더욱 촉진한다. 전반적으로, RAG의 툴체인의 기술적 스택은 초기에 형성되었고, 많은 기업 수준의 애플리케이션들이 점차적으로 등장했지만, 올인원 플랫폼은 여전히 개선될 필요가 있다.

## 9 Conclusion

본 논문은 LMM(Large Language Models)의 문맥을 보완하고 응답을 생성하기 위해 외부 지식베이스를 사용하는 기법인 RAG(Search-Augmented Generation)를 철저히 탐구한다. 특히, RAG는 LLM의 매개변수화된 지식과 매개변수화되지 않은 외부 지식을 결합하고 환각 문제를 완화하며 검색 기술을 통해 시기적절한 정보를 식별하고 응답 정확도를 향상시킨다. 추가적으로, 소스들을 인용함으로써, RAG는 모델 출력들에 대한 투명성과 사용자 신뢰를 증가시킨다. RAG는 또한 관련 텍스트 말뭉치를 인덱싱함으로써 특정 도메인에 기초하여 커스터마이징될 수 있다. RAG의 개발 및 특성은 나이브 RAG, 어드밴스드 RAG 및 모듈러 RAG의 세 가지 패러다임으로 요약되며, 각각 모델, 방법 및 단점이 있다. 순진한 RAG는 주로 '검색 읽기' 과정을 포함한다. 고급 RAG는 보다 정제된 데이터 프로세싱을 사용하고, 지식 베이스 인덱싱을 최적화하며, 다중 또는 반복적 검색을 도입한다. 탐사가 심화됨에 따라 RAG는 미세 조정과 같은 다른 기술을 통합하여 모듈형 RAG 패러다임의 출현으로 이어지며, 이는 새로운 모듈로 RAG 프로세스를 풍부하게 하고 더 많은 유연성을 제공한다.

후속 장에서 RAG의 세 가지 주요 부분을 자세히 분석한다. 4장에서는 RAG의 검색기, 더 나은 의미 표현을 얻기 위해 말뭉치를 처리하는 방법, 질의와 문서 사이의 의미 차이를 완화하는 방법, 생성기에 맞게 검색기를 조정하는 방법을 소개한다. 제5장에서는 "중간에 잃어버린" 문제를 피하고 검색된 문서를 후처리하여 발전기가 더 나은 발전 결과를 얻는 방법과 검색기에 맞게 발전기를 조정하는 방법에 대해 설명한다. 이어서 6장에서는 검색 단계, 검색 데이터 소스 및 검색 프로세스의 측면에서 현재 검색 향상 방법을 검토한다.

7장에서는 평가, 주요 지표 및 현재 평가 프레임워크를 포함한 현재 RAG 방법을 평가하는 방법에 대해 설명한다. 마지막으로 RAG에 대한 잠재적인 미래 연구 방향에 대한 전망을 제공했다. 검색과 생성을 결합한 방법으로 RAG는 향후 연구에서 수많은 잠재적인 발전 방향을 가지고 있다. 지속적으로 기술을 개선하고 응용 분야를 확장함으로써 RAG의 성능과 실용성을 더욱 높일 수 있다.

## References

*[1]U. 알론 F. 쉬 Sengupta, D. Roth, and G. Neubig (2022) Neuro-symbolic language modeling with automaton-augmented retrieval. In International Conference on Machine Learning, pp. 468-485. Cited by: SS1.

* [Anderson _et al._2022] Nathan Anderson, Caleb Wilson, and Stephen D. Richardson. 링구아: 라이브 통역 및 자동 더빙을 위한 시나리오를 추가합니다. Janice Campbell, Stephen Larocca, Jay Marciano, Konstantin Savenkov, Alex Yanishevsky, Editors, _Proceedings of the 15 Biennial Conference of the Association of the Machine Translation in America (Volume 2: Users and Providers Track and Government Track)_, pages 202-209, Orlando, USA, September 2022. Association for Machine Translation in America.
* [AngIE2023] AngIE. Angle-optimized text embeddings. [https://github.com/SeanLee97/AngIE] (https://github.com/SeanLee97/AngIE), 2023.
* [Arora _et al._2023] Daman Arora, Anush Kini, Sayak Ray Chowdhury, Nagarajan Natarajan, Gaurav Sinha, and Amit Sharma. Gar-meets-rag paradigm for zero-shot information retrieval. _ arXiv preprint arXiv:2310.20158_, 2023.
* [Asai _et al._2023a] Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. 검색 기반 언어 모델 및 응용 프로그램 _Computational Linguistics Association of the 61st Annual Meeting of the Association for Computational Linguistics(Volume 6: Tutorial Abstracts)_, pages 41-46, 2023.
* [Asai _et al._2023b] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: 자기 성찰을 통해 검색, 생성 및 비판하는 학습 _ arXiv preprint arXiv:2310.11511_, 2023.
* [BAAI2023] BAAI. Flagembedding. [https://github.com/FlagOpen/FlagEmbedding] (https://github.com/FlagOpen/FlagEmbedding), 2023.
*[백 _et al._2023] 백진헌, 정소영, 강민기, 박종철, 황성주. Knowledge-augmented language model verification. _ arXiv preprint arXiv:2310.12836_, 2023.
* [Bai _et al._2022] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. _ arXiv preprint arXiv:2212.08073_, 2022.
* [Bang _et al._2023] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. _ arXiv preprint arXiv:2302.04023_, 2023.
* [Berchansky _et al._2023] Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido Dagan, and Moshe Wasserblat. 토큰 제거를 통해 검색 증강 리더 모델을 최적화합니다. _ arXiv preprint arXiv:2310.13682_, 2023.
* [Bisk _et al._2020] Tonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. _Proceedings of the AAAI conference on artificial intelligence_, Volume 34, pages 7432-7439, 2020.
* [Blagojevi2023] Vladimir Blagojevi. 건초 스택에서 rag 파이프라인 강화: diversityanker 및 lostinthemiddleranker 도입. [https://towardsdatascience.com/enhancing-rag-pipelines-in-haystack-45f14e2bc9f5] (https://towardsdatascience.com/enhancing-rag-pipelines-in-haystack-45f14e2bc9f5), 2023.
* [Borgeaud _et al._2022] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 수조 개의 토큰으로부터 검색함으로써 언어 모델을 개선하는 것. _International conference on machine learning_, pages 2206-2240. PMLR, 2022.
* [Brown _et al._2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _ Advances in neural information processing systems_, 33:1877-1901, 2020.
* [Cai _et al._2021] Deng Cai, Yan Wang, Huyang Li, Wai Lam, and Lemao Liu. 단일 언어 번역 메모리를 갖는 신경 기계 번역. _ arXiv preprint arXiv:2105.11269_, 2021.
* [Chan _et al._2023] David M Chan, Shalini Ghosh, Ariya Rastrow, and Bjorn Hoffmeister. 상황별 엔드 투 엔드 자동화 음성 인식에서 외부 오프 정책 음성 대 텍스트 매핑을 사용 합니다. _ arXiv preprint arXiv:2301.02736_, 2023.
* [Chen and Yih2020] Danqi Chen and Wen-tau Yih. 오픈 도메인 질문 답변입니다. _Proceedings of 58th annual meeting of the association for computational linguistics: tutorial abstracts_, pages 34-37, 2020.
* [Chen _et al._2023a] Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. 메모리 미로를 걷습니다. 대화형 읽기를 통해 컨텍스트 제한을 초과합니다. _ arXiv preprint arXiv:2310.05029_, 2023.
* [Chen _et al._2023b] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 검색 확장 생성에서 대용량 언어 모델을 벤치마킹합니다. _ arXiv preprint arXiv:2309.01431_, 2023.
* [Cheng _et al._2022] Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan. 대조적인 번역 메모리를 갖는 신경 기계 번역. _ arXiv preprint arXiv:2212.03140_, 2022.
* [Cheng _et al._2023a] Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei, Denvy Deng, and Qi Zhang. surprise: 제로 샷 평가를 개선하기 위한 범용 프롬프트 검색. _ arXiv preprint arXiv:2303.08518_, 2023.
* [Cheng _et al._2023b] Xin Cheng, Di Luo, Xiying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. 기운내라: 자기 기억으로 검색-증강 텍스트 생성. _ arXiv preprint arXiv:2305.02437_, 2023.
* [Clark _et al._2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: 자연스러운 예/아니오 질문의 놀라운 어려움을 탐구합니다. _ arXiv preprint arXiv:1905.10044_, 2019.
* [Cohere2023] Cohere. 관련 없는 검색 결과에 작별을 고합니다. Cohere rerank가 여기에 있습니다. [https://txt.cohere.com/rerank/] (https://txt.cohere.com/rerank/), 2023.

[Dai _et al._2022] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B Hall, and Ming-Wei Chang. Promptagator: 8 예시에서 몇 번 촘촘히 검색됩니다. _ arXiv preprint arXiv:2209.11755_, 2022.
* [Es _et al._2023] Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. 래그: 검색 증강 생성의 자동 평가. _ arXiv preprint arXiv:2309.15217_, 2023.
* [Feng _et al._2023a] Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang and Bing Qin. 검색-생성 시너지 증강 대형 언어 모델입니다. _ arXiv preprint arXiv:2310.05149_, 2023.
* [Feng _et al._2023b] Zhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. Trends in integration of knowledge and large language models: survey and taxonomy of methods, benchmarks, and applications. _ arXiv preprint arXiv:2311.05876_, 2023.
* [Gao _et al._2022] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 연관성 레이블이 없는 정확한 제로 샷 밀집 검색입니다. _ arXiv preprint arXiv:2212.10496_, 2022.
* [Glass _et al._2021] Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, and Alfio Glozzo. 제로-샷 슬롯 충전을 위한 강인한 검색 증강 생성. _ arXiv preprint arXiv:2108.13934_, 2021.
* [Google2023] Google. Gemini: 고성능 멀티모달 모델 계열. [https://goo.gle/GeminiPaper] (https://goo.gle/GeminiPaper), 2023.
* [Hendrycks _et al._2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 대규모 다중 작업 언어 이해도를 측정 합니다. _ arXiv preprint arXiv:2009.03300_, 2020.
* [Izacard _et al._2022] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, Edouard Grave. 검색 증강 언어 모델을 사용한 샷 학습은 거의 없습니다. _ arXiv preprint arXiv:2208.03299_, 2022.
* [Jarvis and Allard2023] Colin Jarvis and John Allard. llm 성능 최대화를 위한 기술 조사. [https://community.openai.com/t/openai-dev-day-2023-breakout-sessions/505213#] (https://community.openai.com/t/openai-dev-day-2023-breakout-sessions/505213#) a-survey-of-techniques-for-maximizing-llm-performance-2, 2023.
* [Jiang _et al._2023a] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, Lili Qiu. Llmlingua: 대형 언어 모델의 가속 추론을 위한 프롬프트를 압축합니다. _ arXiv preprint arXiv:2310.05736_, 2023.
* [Jiang _et al._2023b] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. _ arXiv preprint arXiv:2305.06983_, 2023.
* [Kandpal _et al._2023] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 대형 언어 모델들은 롱테일 지식을 배우는데 어려움을 겪는다. _International Conference on Machine Learning_, pages 15696-15707. PMLR, 2023.
*[Kang _et al._2023] 민키강, 진명곽, 진헌백, 성주황. 지식 기반 대화 생성을 위한 지식 그래프-증강 언어 모델. _ arXiv preprint arXiv:2305.18846_, 2023.
* [Karpukhin _et al._2020] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 오픈 도메인 질문 응답을 위한 고밀도 구문 검색입니다. _ arXiv preprint arXiv:2004.04906_, 2020.
* [Khandelwal _et al._2019] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 암기를 통한 일반화 : 최근접 이웃 언어 모델. _ arXiv preprint arXiv:1911.00172_, 2019.
* [Khattab and Zaharia2020] Omar Khattab and Matei Zaharia. 콜버트: 버트에 대한 상황화된 늦은 상호 작용을 통한 효율적이고 효과적인 통과 검색입니다. _Proceedings of the 43th International ACM SIGIR conference on Research and Development in Information Retrieval_, pages 39-48, 2020.
* [Khattab _et al._2022] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 검증-검색-예측: 지식 집약적 nlp에 대한 검색 및 언어 모델을 구성합니다. _ arXiv preprint arXiv:2212.14024_, 2022.
* [Kwiatkowski _et al._2019] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: question response research의 벤치마크. _ Transactions of the Association for Computational Linguistics_, 7:453-466, 2019.
* [Lee _et al._2020] Lee Jin혁, Mujeen Sung, Jaewoo Kang, and Danqi Chen. 스케일에서의 구절들의 조밀한 표현들을 학습하는 것 _ arXiv preprint arXiv:2012.12624_, 2020.
* [Leng _et al._2023] Quinn Leng, Kasey Uhlenhuth, and Alkis Polyzotis. rag 애플리케이션의 llm 평가를 위한 모범 사례 [https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG] (https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG), 2023.
* [Lewis _et al._2020] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. _ Advances in Neural Information Processing Systems_, 33:9459-9474, 2020.
* [Li _et al._2023a] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: 동결된 이미지 인코더 및 대형 언어 모델을 사용한 부트스트랩 언어-이미지 사전 트레이닝 _ arXiv preprint arXiv:2301.12597_, 2023.
* [Li _et al._2023b] Xiaoqian Li, Ercong Nie, and Sheng Liang. 분류에서 생성까지: 언어 간 검색에 대한 통찰력 강화 icl. _ arXiv preprint arXiv:2311.06595_, 2023.

* [Li _et al._2023c] Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Lidong Bing, Shafiq Joty, and Soujanya Poria. 지식의 사슬: 구조화된 지식 베이스를 갖는 대형 언어 모델들을 접지시키기 위한 프레임워크. _ arXiv preprint arXiv:2305.13269_, 2023.
*[Li _et al._2023d] Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yu Gu, Zhiyuan Liu, and Ge Yu. 구조 인식 언어 모델 사전 훈련은 구조화된 데이터에 대한 밀도 검색을 향상시킵니다. _ arXiv preprint arXiv:2305.19912_, 2023.
* [Lin _et al._2023] Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et al. Ra-dit: Retrieval-augmented dual instruction tuning. _ arXiv preprint arXiv:2310.01352_, 2023.
* [Litman _et al._2020] Ron Litman, Oron Anschel, Shahar Tsiper, Roee Litman, Shai Mazor, and R Manmatha. 산포: 선택적 문맥 주의 장면 텍스트 인식기. _proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 11962-11972, 2020.
* [Liu _et al._2023] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 중간에서 손실: 언어 모델이 긴 컨텍스트를 사용하는 방법 _ arXiv preprint arXiv:2307.03172_, 2023.
* [Liu2023] Jerry Liu. Building production-ready rag applications. [https://www.ai.engineer/summit/schedule/building-production-ready-rag-applications] (https://www.ai.engineer/summit/schedule/building-production-ready-rag-applications), 2023.
* [Luo _et al._2023] Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 매개 변수 지식 가이드가 있는 확장 된 대형 언어 모델입니다. _ arXiv preprint arXiv:2305.04757_, 2023.
* [Ma _et al._2023a] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 검색 확장 대용량 언어 모델에 대한 쿼리 다시 쓰기 _ arXiv preprint arXiv:2305.14283_, 2023.
* [Ma _et al._2023b] Yubo Ma, Yixin Cao, YongChing Hong, Aixin Sun. 대형 언어 모델은 몇 개의 샷 정보 추출기가 아니라 하드 샘플에 대 한 좋은 순위 조정자입니다. _ ArXiv_, abs/2303.08559, 2023.
* [Modarressi _et al._2023] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schutze. Ret-lIm: 큰 언어 모델에 대 한 일반적인 읽기 쓰기 메모리입니다. _ arXiv preprint arXiv:2305.14322_, 2023.
* [Nakano _et al._2021] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webprt: Browser-assisted question-answering with human feedback. _ arXiv preprint arXiv:2112.09332_, 2021.
* [Nashid _et al._2023] Noor Nashid, Mifta Sintaha 및 Ali Mesbah. 코드 관련 소수 샷 학습을 위한 검색 기반 프롬프트 선택입니다. _2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)_에서, 페이지 2450-2462, 2023.
* [OpenAI2023] OpenAI. Gpt-4 기술 보고서. [https://cdn.openai.com/papers/gpt-4.pdf] (https://cdn.openai.com/papers/gpt-4.pdf), 2023.
* [Petroni _et al._2019] Fabio Petroni, Tim Rocktaschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 지식 베이스로서의 언어 모델? _ arXiv preprint arXiv:1909.01066_, 2019.
* [Raffel _et al._2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계를 탐구합니다. _ The Journal of Machine Learning Research_, 21(1):5485-5551, 2020).
* [Ranzato _et al._2015] Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 순환 신경망을 이용한 시퀀스 레벨 트레이닝. _ arXiv preprint arXiv:1511.06732_, 2015.
* [Reddy _et al._2019] Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: 대화형 질문 응답 도전. _ Transactions of the Association for Computational Linguistics_, 7:249-266, 2019.
* [Robertson _et al._2009] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. _ Foundations and Trends(r) in Information Retrieval_, 3(4):333-389, 2009.
* [Saad-Falcon _et al._2023] Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. Ares: 검색-증강 생성 시스템을 위한 자동화된 평가 프레임워크. _ arXiv preprint arXiv:2311.09476_, 2023.
* [Schick _et al._2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 도구 형식자: 언어 모델은 도구를 사용하는 방법을 스스로 가르칠 수 있습니다. _ arXiv preprint arXiv:2302.04761_, 2023.
* [Sciavolino _et al._2021] Christopher Sciavolino, Zexuan Zhong, Lee Jinhyuk, and Danqi Chen. 단순 엔터티 중심 질문은 고밀도 검색기에 도전합니다. _ arXiv preprint arXiv:2109.08535_, 2021.
* [Shao _et al._2023] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 반복적인 검색 생성 시너지로 검색 증강 대형 언어 모델을 개선합니다. _ arXiv preprint arXiv:2305.15294_, 2023.
*[Shi _et al._2023] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: 검색 증강 블랙박스 언어 모델입니다. _ arXiv preprint arXiv:2301.12652_, 2023.
* [Shuster _et al._2021] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 검색 확대는 대화에서 환각을 감소시킵니다. _ arXiv preprint arXiv:2104.07567_, 2021.
* [Srivastava _et al._2022] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, AdityaGupta, Adria Garriga-Alonso, et al. Beyond the imitation game: Quantified and extrapolating the capabilities of language models. _ arXiv preprint arXiv:2206.04615_, 2022.
*[태양 _et al._2022] Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language models. _ arXiv preprint arXiv:2210.01296_, 2022.
* [Touvron _et al._2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023.
* [Trivedi _et al._2022] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot 및 Ashish Sabharwal. 지식 집약적 다단계 질문에 대한 체인 오브 생각 추론으로 검색을 인터리빙합니다. _ arXiv preprint arXiv:2212.10509_, 2022.
* [바스와니 _et al._2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 당신이 필요로 하는 모든 것에 주목하세요. 뉴럴 정보 처리 시스템_, 30, 2017의 발전입니다.
* [Vaze _et al._2021] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set 인식: 좋은 closed-set 분류기만 필요한가요? _ arXiv preprint arXiv:2110.06207_, 2021.
* [VoyageAI2023] VoyageAI. Voyage's embedding models. [https://docs.voyageai.com/embeddings/] (https://docs.voyageai.com/embeddings/), 2023.
* [Wang _et al._2019] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 슈퍼글루: 범용 언어 이해 시스템을 위한 보다 엄격한 벤치마크. _ Neural Information Processing Systems_, 32, 2019의 발전.
* [Wang _et al._2022a] Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and Michael Zeng. 학습 데이터는 생각보다 더 가치가 있습니다. 학습 데이터에서 검색 하 여 간단하고 효과적인 방법입니다. _ arXiv preprint arXiv:2203.08773_, 2022.
* [Wang _et al._2022b] Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and Michael Zeng. 훈련 데이터는 여러분이 생각하는 것보다 더 가치가 있다: 훈련 데이터로부터 검색함으로써 간단하고 효과적인 방법이다. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, _Proceedings of the 60th Annual Meeting of the Association of Computational Linguistics(Volume 1: Long Papers)_, pages 3170-3179, pages 3170-3179, Dublin, Ireland, May 2022. Association for Computational Linguistics.
* [Wang _et al._2023a] Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. autoregressive language models with retrieval? 종합적인 연구. _ arXiv preprint arXiv:2304.06762_, 2023.
* [Wang _et al._2023b] Liang Wang, Nan Yang, and Furu Wei. Query2doc: 대용량 언어 모델을 사용한 쿼리 확장입니다. _ arXiv preprint arXiv:2303.07678_, 2023.
* [Wang _et al._2023c] Xintao Wang, Qianwen Yang, Yongting Qiu, Jiaqing Liang, Qianyu He, Zhouhong Gu, Yangua Xiao, and Wei Wang. Knowledgpt: 지식 기반에 대 한 검색 및 저장소 액세스를 사용 하 여 대규모 언어 모델을 향상 합니다. _ arXiv preprint arXiv:2308.11761_, 2023.
*[왕 _et al._2023d] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 대용량 언어 모델을 위한 자체 지식 가이드 검색 확장입니다. _ arXiv preprint arXiv:2310.05002_, 2023.
* [Wei _et al._2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting promptedits reasoning in large language models. _ 신경 정보 처리 시스템_, 2022년 35:24824-24837의 발전.
* [Xia _et al._2019] Mengzhou Xia, Guoping Huang, Lemao Liu, and Shuming Shi. 신경망 기계 번역을 위한 그래프 기반 번역 메모리. _Proceedings of the AAAI conference on artificial intelligence_, Volume 33, pages 7297-7304, 2019.
* [Xu _et al._2023a] Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: 압축 및 선택적 확장을 사용 하 여 검색 증강 lms를 개선 합니다. _ arXiv preprint arXiv:2310.04408_, 2023.
* [Xu _et al._2023b] Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. 검색은 긴 컨텍스트의 큰 언어 모델을 충족합니다. _ arXiv preprint arXiv:2310.03025_, 2023.
* [Xu _et al._2023c] Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. 검색은 긴 컨텍스트의 큰 언어 모델을 충족합니다. _ arXiv preprint arXiv:2310.03025_, 2023.
* [Yang _et al._2023a] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq: 고밀도 비디오 캡셔닝을 위한 시각적 언어 모델의 대규모 사전 트레이닝. _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10714-10726, 2023.
*[양 _et al._2023b] Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, and Jing Xiao. Prca: 플러그 인 가능한 보상 기반 컨텍스트 어댑터를 통해 검색 질문 응답을 위한 블랙박스 대형 언어 모델에 적합 합니다. _ arXiv preprint arXiv:2310.18347_, 2023.
*[양 _et al._2023c] Hui Yang, Sifu Yue, and Yunzhong He. 온라인 의사 결정을 위한 자동 저장: 벤치마크 및 추가 의견입니다. _ arXiv preprint arXiv:2306.02224_, 2023.
*[Yao _et al._2023] Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, and Li Yuan. Llm 거짓말: 환각은 버그가 아니라 적대적인 예로서의 기능입니다. _ arXiv preprint arXiv:2310.01469_, 2023.

*[Yasunaga _et al._2022] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 검색 강화 다중 모드 언어 모델링 _ arXiv preprint arXiv:2211.12561_, 2022.
*[예 _et al._2020] Deming Ye, Yankai Lin, Jiaju Du, Zhenghao Liu, Peng Li, Maosong Sun, Zhiyuan Liu. 언어 표현을 위한 핵심 추론 학습. _ arXiv preprint arXiv:2004.06870_, 2020.
*[Yoran _et al._2023] Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 검색 증강 언어 모델을 관련 없는 컨텍스트에 강력 하 게 만듭니다. _ arXiv preprint arXiv:2310.01558_, 2023.
* [Yu _et al._2022] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 검색이 아닌 생성: 대용량 언어 모델은 강력한 컨텍스트 생성기입니다. _ arXiv preprint arXiv:2209.10063_, 2022.
*[유 _et al._2023a] Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. 메모 연결: 검색 증강 언어 모델에서 견고성을 향상시킵니다. _ arXiv preprint arXiv:2311.09210_, 2023.
* [Yu _et al._2023b] Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu. 보강 적응 검색기는 일반 플러그 인으로 언어 모델의 일반화를 개선 합니다. _ arXiv preprint arXiv:2305.17331_, 2023.
* [장 _et al._2019] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 어니: 유익한 엔터티를 사용하여 향상된 언어 표현입니다. _ arXiv preprint arXiv:1905.07129_, 2019.
*[장 _et al._2023a] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. 대형 언어 모델을 보강하기 위해 모든 항목을 검색합니다. _ arXiv preprint arXiv:2310.07554_, 2023.
* [Zhang _et al._2023b] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren's song in the ai ocean: A survey on hallination in the large language models. _ arXiv preprint arXiv:2309.01219_, 2023.
* [Zhang2023] Jiawei Zhang. Graph-toolformer: 채팅으로 확장 된 프롬프트를 통해 그래프 추론 기능을 사용 하 여 llms에 권한을 부여 합니다. _ arXiv preprint arXiv:2304.11116_, 2023.
* [Zhao _et al._2022] Jinming Zhao, Gholamreza Haffar, and Ehsan Shareghi. 음성 번역을 위해 음성보캅으로부터 합성 음성을 생성하는 단계; _ arXiv preprint arXiv:2210.08174_, 2022.
* [Zheng _et al._2023] Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H Chi, Quoc V Le, and Denny Zhou. 한 걸음 뒤로 물러납니다. 대규모 언어 모델에서 추상화를 통한 추론 유발 _ arXiv preprint arXiv:2310.06117_, 2023.
* [Zhong _et al._2022] Zexuan Zhong, Tao Lei, and Danqi Chen. 메모리 증강을 사용하여 언어 모델을 학습합니다. _ arXiv preprint arXiv:2205.12674_, 2022.
* [Zhu _et al._2022] Wanrong Zhu, An Yan, Yujie Lu, Wenda Xu, Xin Eric Wang, Miguel Eckstein, and William Yang Wang. 쓰기 전에 시각화: 상상 유도 개방형 텍스트 생성 _ arXiv preprint arXiv:2210.03765_, 2022.
* [Zhu _et al._2023] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 정보 검색을 위한 대규모 언어 모델: 설문 조사입니다. _ arXiv preprint arXiv:2308.07107_, 2023.
* [Zhuang _et al._2023] Shengyao Zhuang, Bing Liu, Bevan Koopman, and Guido Zuccon. 오픈 소스 대형 언어 모델은 문서 순위를 위한 강력한 제로 샷 쿼리 가능성 모델입니다. _ arXiv preprint arXiv:2310.13243_, 2023.
