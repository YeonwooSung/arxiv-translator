# Effective Long-Context Scaling of Foundation Models

Wenhan Xiong\({}^{\dagger\ast}\), Jingyu Liu\({}^{\dagger}\), Igor Molybog,

헤지아 장, 프라자왈 바르가바, 루이 후우, 루이 마르틴, 라시 룽타

카틱 아비나브 산카라만, 발라스 오구즈, 매디안 캅사, 한팡

야샤르 메다드, 샤란 나랑, Kshitiz Malik, Angela Fan

슈루티 보살레, 세르게이 에두노프, 마이크 루이스, 시농 왕\({}^{\ast}\), 하오 마\({}^{\ast}\)

GenAI, Meta

###### Abstract

우리는 최대 32,768개의 토큰의 효과적인 컨텍스트 윈도우를 지원하는 일련의 긴 컨텍스트 LLM을 제시한다. 모델 시리즈는 더 긴 훈련 시퀀스를 가진 라마 2의 지속적인 사전 훈련과 긴 텍스트가 업샘플링되는 데이터 세트를 통해 구축된다. 언어 모델링, 합성 컨텍스트 프로빙 작업 및 광범위한 연구 벤치마크에 대한 광범위한 평가를 수행합니다. 연구 벤치마크에서, 우리의 모델은 라마 2보다 대부분의 정규 태스크에서 일관된 개선과 긴 컨텍스트 태스크에서 상당한 개선을 달성한다. 특히, 인간 주석이 달린 긴 명령어 데이터가 필요하지 않은 비용 효율적인 명령어 튜닝 절차를 통해 70B 변형은 긴 컨텍스트 태스크 집합에서 gpt-3.5-turbo-16k의 전반적인 성능을 이미 능가할 수 있다. 이러한 결과와 함께 우리는 방법의 개별 구성요소에 대한 심층 분석을 제공한다. 우리는 Llama의 위치 인코딩을 조사하고 긴 종속성을 모델링하는 데 있어 그 한계에 대해 논의한다. 또한 데이터 믹스와 시퀀스 길이의 훈련 커리큘럼을 포함한 사전 훈련 과정에서 다양한 설계 선택의 영향을 조사한다. 절제 실험은 사전 훈련 데이터 세트에 풍부한 긴 텍스트를 갖는 것이 강력한 성능을 달성하는 열쇠가 아님을 시사하며, 긴 시퀀스를 사용하여 처음부터 사전 훈련하는 것보다 긴 컨텍스트 연속 사전 훈련이 더 효율적이고 유사하게 효과적임을 경험적으로 검증한다.

그림 1: 모델의 검증 손실은 각 모델 크기에 대해 \(\alpha,\beta,\gamma\)의 다른 집합을 사용하여 \(L(c)=(\frac{\alpha}{c})^{\beta}+\gamma\) 컨텍스트 길이의 함수로 적합할 수 있음을 보여줍니다. 이 멱법칙 관계는 또한 컨텍스트 길이가 LLM을 스케일링하는 또 다른 중요한 축임을 시사하며, 본 모델은 컨텍스트 길이를 최대 32,768 토큰까지 증가시킴에 따라 성능을 지속적으로 향상시킬 수 있다.

Introduction

전례 없는 규모의 데이터와 컴퓨팅으로 훈련된 대규모 언어 모델(LLM)은 우리가 디지털 세계와 상호 작용하는 방식을 근본적으로 개선하겠다는 약속을 지킨다. LLM이 빠르게 배포되고 스케일링을 통해 계속 발전함에 따라, 우리는 이러한 모델이 밀집된 지식이 풍부한 문서를 분석하고, 보다 진실되고 매력적인 챗봇 경험을 제공하고, 코딩 및 디자인 등과 같은 반복적인 생성 과정에서 인간 사용자를 돕는 것과 같은 보다 복잡하고 복잡한 사용 사례를 제공할 것이라고 구상한다. 이 진화를 지원하는 중요한 특징은 긴 문맥 입력을 효과적으로 처리하는 능력이다.

지금까지 강력한 롱-컨텍스트 기능을 갖는 LLM은 주로 독점 LLM API(Anthropic, 2023; OpenAI, 2023)를 통해 제공되며, 이러한 독점 모델로서 온-파 다운스트림 성능을 입증할 수 있는 롱-컨텍스트 모델을 구축하기 위한 공개 레시피는 없다. 또한 기존의 개방형 장기 컨텍스트 모델(Tworkowski et al., 2023; Chen et al., 2023; Mohtashami and Jaggi, 2023; MosaicML, 2023b)은 종종 평가에 부족하고 주로 언어 모델링 손실 및 합성 태스크로 장기 컨텍스트 기능을 측정하는데, 이는 다양한 실제 시나리오에서 그 효과를 종합적으로 입증하지 못한다. 추가적으로, 이러한 모델들은 종종 평가들을 우회하거나 또는 퇴화된 성능을 보고하는 표준 단문-맥락 태스크들에 대해 강한 성능을 유지할 필요성을 간과한다(Peng et al., 2023; Chen et al., 2023).

본 논문에서는 기존의 오픈소싱 모델보다 우수한 성능을 갖는 Long-context LLM을 구축하는 방법을 설명한다. 우리는 긴 훈련 시퀀스로 형성된 추가 4,000억 개의 토큰을 사용하여 라마 2 검문소에서 지속적으로 사전 훈련하여 모델을 구축한다. 모델 시리즈 중 더 작은 7B/13B 변이체는 32,768개의 토큰 시퀀스로 훈련되고 34B/70B 변이체는 16,384개의 토큰 시퀀스로 훈련된다. 기존 연구에서 수행된 제한된 평가와 달리 언어 모델링, 합성 작업 및 긴 컨텍스트 작업과 짧은 컨텍스트 작업을 모두 포함하는 광범위한 실제 벤치마크를 사용하여 모델을 광범위하게 평가한다. 언어 모델링에서, 우리의 모델은 문맥 길이에 대한 명확한 멱법칙 스케일링 동작을 보여준다. 그림 1에서 볼 수 있듯이 이러한 스케일링 동작은 더 많은 컨텍스트로부터 일관되게 이익을 얻을 수 있는 모델의 능력을 보여줄 뿐만 아니라 컨텍스트 길이가 LLM을 스케일링하는 또 다른 중요 축임을 시사한다. 연구 벤치마크에 대한 모델을 라마 2와 비교할 때, 우리는 긴 맥락 과제에 대한 상당한 개선뿐만 아니라 표준 짧은 맥락 과제, 특히 코딩, 수학 및 지식 벤치마크에 대한 약간의 개선을 관찰했다. 우리는 인간이 주석을 달지 않은 데이터 없이 지속적으로 사전 훈련된 긴 모델을 미세 조정하기 위해 간단하고 비용 효율적인 절차를 사용하여 탐구했다. 최종 결과는 질의 응답, 요약 및 다중 문서 집계 작업을 포함하는 일련의 긴 컨텍스트 벤치마크에서 gpt-3.5-turbo-16k보다 더 강력한 전체 성능을 달성할 수 있는 채팅 모델이다.

이 논문의 나머지 부분에서는 연속적 긴 맥락 사전 훈련 접근법과 가벼운 명령어 튜닝 절차를 제시하고 짧은 맥락과 긴 맥락 과제의 범위에 대한 자세한 결과를 제시하는 것으로 시작한다. 향후 연구를 용이하게 하기 위해 위치 인코딩 설계, 데이터 세트의 길이 분포 및 교육 커리큘럼이 최종 성능에 어떻게 기여하는지 논의하는 분석 섹션으로 결과를 보완한다. 마지막으로, 책임 있는 안전성 평가를 보고하며, 이는 우리 모델이 원래 라마 2 시리즈의 안전 성능을 크게 유지할 수 있음을 검증한다.

## 2 Method

### Continual Pretraining

더 긴 시퀀스 길이를 갖는 트레이닝은 2차 주의 계산들로 인해 상당한 계산 오버헤드를 도입할 수 있다. 이것이 우리의 지속적인 사전 훈련 접근법의 주요 동기이다. 유사한 긴 맥락 능력이 짧은 맥락 모델에서 지속적으로 사전 훈련함으로써 학습될 수 있다는 기본 가설은 나중에 다른 훈련 커리큘럼 비교를 통해 섹션 4.4에서 검증된다. 본 논문에서는 기존의 Llama 2 아키텍처를 그대로 유지하여 지속적인 사전 훈련을 수행하고, 모델이 더 오래 참석하기 위해 중요한 위치 인코딩에 필요한 수정만을 수행한다. 또한 Llama 2 70B의 모델 차원(\(h=8192\))이 주어지면 시퀀스 길이가 49,152(\(6h\)) 토큰(Narayanan et al., 2021)을 초과할 때만 주의 행렬 계산 및 값 집계 비용이 계산 병목 현상이 되기 때문에 이 작업에서는 희소 주의를 적용하지 않기로 선택한다.

각주 1: 희소한 주의는 성능을 상쇄할 때 추론 시간에 키/값 캐시 크기를 줄이는 데 유용할 수 있지만, 추론 파이프라인을 복잡하게 만들 수 있고 양자화 방법에 의해 개선도 상쇄될 수 있다.

위치 인코딩은 7B 규모에서 초기 실험을 통해 주의 모듈이 먼 토큰의 정보를 수집하는 것을 방지하는 Llama 2의 위치 인코딩(PE)의 주요 한계를 확인했다. Long-context 모델링을 위해 RoPE 위치 인코딩(Su et al., 2022)에 최소이지만 필요한 수정을 채택했으며, 이는 원격 토큰에 대한 RoPE의 쇠퇴 효과를 감소시키는 하이퍼파라미터 "기본 주파수 \(b\)"에 의해 제어되는 회전 각도를 감소시킨다. 섹션 4.1에서, 우리는 이 간단한 방법이 Llama의 문맥 길이를 확장하기 위한 동시 접근법(Chen et al., 2023)보다 더 우수함을 보여주고 그것의 우월성에 대한 이론적 설명을 제공한다.

데이터 믹스 수정된 PE를 사용한 작업 모델 위에 라마 2의 사전 훈련 데이터의 비율을 조정하거나 새로운 장문 데이터를 추가하여 장문 능력을 개선하기 위해 섹션 4.2에서 다양한 사전 훈련 데이터 믹스를 추가로 조사했다. 우리는 긴 맥락의 지속적인 사전 훈련을 위해 종종 데이터의 품질이 텍스트의 길이보다 더 중요한 역할을 한다는 것을 발견했다.

최적화 세부 정보 Llama 2에서와 같이 배치당 동일한 수의 토큰을 유지하면서 시퀀스 길이가 증가된 Llama 2 검사점을 지속적으로 사전 훈련합니다. 총 400B 토큰에 대해 100,000단계에 걸쳐 모든 모델을 훈련합니다. 플래시어텐션(Dao et al., 2022)을 사용하여 70B 모델의 경우 시퀀스 길이가 4,096에서 16,384로 증가할 때, 시퀀스 길이가 증가함에 따라 GPU 메모리 오버헤드가 무시할 수 있고 약 \(17\%\)의 속도 손실을 관찰할 수 있다. 7B/13B 모델의 경우 학습률 \(2e^{-5}\)과 코사인 학습률 일정을 사용하여 2000회의 준비 단계를 수행한다. 더 큰 34B/70B 모델의 경우, 단조롭게 감소하는 검증 손실을 얻기 위해 더 작은 학습률(\(1e^{-5}\))을 설정하는 것이 중요하다는 것을 발견했다.

### Instruction Tuning

LLM 정렬을 위한 인간 시연 및 선호도 라벨을 수집하는 것은 번거롭고 값비싼 프로세스이다(Ouyang et al., 2022; Touvron et al., 2023). 도전과 비용은 복잡한 정보 흐름과 전문 지식(예: 조밀한 법률/과학 문서 처리)을 포함하는 긴 컨텍스트 시나리오에서 더 두드러지며, 이는 숙련된 주석자에게도 주석 작업을 자명하지 않게 만든다. 실제로, 대부분의 기존의 오픈 소스 명령어 데이터세트(Conover et al., 2023; Kopf et al., 2023)는 주로 짧은 샘플로 구성된다.

이 연구에서 우리는 미리 구축된 크고 다양한 짧은 프롬프트 데이터 세트를 활용하는 간단하고 저렴한 접근법이 긴 컨텍스트 벤치마크에서 놀랍도록 잘 작동한다는 것을 발견했다. 구체적으로, Llama 2 Chat에서 사용되는 RLHF 데이터셋을 이용하여 Llama 2 Chat 자체에서 생성된 합성 자기 지시(Wang et al., 2022)의 긴 데이터를 추가하여, 모델이 많은 양의 RLHF 데이터를 통해 다양한 기술을 학습하고 자기 지시 데이터를 통해 긴 컨텍스트 시나리오로 지식을 전달할 수 있기를 바란다. 데이터 생성 과정은 QA 형식의 태스크에 중점을 둔다. 사전 훈련 말뭉치의 긴 문서에서 시작하여 무작위 청크를 선택하고 텍스트 청크의 정보를 기반으로 질문-답변 쌍을 작성하도록 Llama 2 Chat을 프롬프트한다. 다양한 프롬프트로 긴 폼과 짧은 폼 답변을 모두 수집합니다. 그 후, 우리는 또한 모델 생성 답변을 확인하기 위해 라마 2 챗을 프롬프트하는 자기 비판 단계를 채택한다. 생성된 QA 쌍이 주어지면, 훈련 인스턴스를 구성하기 위해 원래의 긴 문서(모델의 최대 컨텍스트 길이에 맞도록 절단된)를 컨텍스트로 사용한다.

짧은 명령어 데이터의 경우 16,384개의 토큰 시퀀스로 연결한다. 긴 명령 데이터의 경우 모델이 잘리지 않고 각 긴 인스턴스를 개별적으로 처리할 수 있도록 오른쪽에 패딩 토큰을 추가합니다. 표준 명령어 튜닝은 출력 토큰에 대한 손실만을 계산하지만, 다운스트림 작업에 대해 일관된 개선을 제공하는 긴 입력 프롬프트에 대한 언어 모델링 손실도 계산하는 것이 특히 유익하다는 것을 발견한다(섹션 4.3).

## 3 주요 결과

### 사전 학습된 모델 평가

짧은 작업 긴 컨텍스트 LLM을 보편적으로 유용하게 만들기 위해 중요한 데시데라타는 표준 짧은 컨텍스트 작업에서 강력한 성능을 보장하는 것이다. 우리는 이전 작업(Touvron et al., 2023)에 따른 일련의 공통 벤치마크에 대한 모델의 성능을 검증한다. 집계된 결과는 표 1과 같다. 전체적으로 Llama 2보다 _on-par 및 대부분의 경우 더 강한 결과_를 관찰한다. 특히, MMLU와 같은 코딩, 수학 및 지식 집약적 작업에서 상당히 개선된 결과를 관찰한다. 표 2에 나타난 바와 같이, 우리의 모델은 MMLU 및 GSM8k에서 GPT-3.5보다 성능이 우수하다. 이는 짧은 작업에 대한 저하를 관찰하는 이전 작업(Chen et al., 2023)과 대조적이다. 개선된 것은 추가 계산 FLOP와 새로 도입된 긴 데이터에서 학습된 지식 덕분이다.

Long Tasks는 기존의 연구(Chen et al., 2023; Mohtashami and Jaggi, 2023)와는 달리, 주로 복잡성과 합성 태스크에 의존하여 장맥락 성능을 측정하고, 실세계 언어 태스크를 이용하여 장맥락 평가를 수행한다. NarrativeQA(Kocisky et al., 2018), QuALITY(Pang et al., 2022) 및 Qasper(Dasigi et al., 2021), QMSum(Zhong et al., 2021)에서 0-shot 성능을 평가한다. 샷의 수는 각 데이터세트의 평균 샘플 길이에 기초하여 결정된다(즉, Qasper 및 QuALITY의 샘플은 종종 NarrativeQA의 샘플보다 훨씬 짧다). 우리는 신속한 엔지니어링2의 용이성과 덜 편향된 자동 평가 때문에 이러한 QA 스타일의 작업에 초점을 맞춘다. 프롬프트가 모델의 최대 입력 길이 또는 16,384 토큰을 초과하는 경우 입력 프롬프트가 왼쪽에서 잘립니다. Huggingface Transformers에서 사용 가능한 오픈소스 Long-context 모델인 Focused Transformer (Tworkowski et al., 2023a), YaRN (Peng et al., 2023), Xgen (Nijkamp et al., 2023), MPT (MosaicML, 2023b,a) 및 Together's Llama 2 fork (Together, 2023)와 비교한다. 표 3에 나타난 바와 같이, 우리의 모델들은 이러한 모델들에 비해 우수한 성능을 달성한다. 7B 스케일에서, 단지 "Together-7B

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Model** & Size & Coding & Math & MMLU & Commonsense & OpenQA \\ \hline \multirow{4}{*}{Llama 2} & 7B & 16.8 & 8.55 & 45.3 & 63.9 & 48.9 \\  & 13B & 24.5 & 16.3 & 54.8 & 66.9 & 55.4 \\  & 34B & 27.8 & 24.2 & 62.6 & 69.9 & 58.7 \\  & 70B & 37.4 & 35.2 & 68.9 & 71.9 & 63.6 \\ \hline \multirow{4}{*}{Llama 2 Long} & 7B & 20.6 & 10.5 & 47.8 & 64.9 & 51.0 \\  & 13B & 25.7 & 21.5 & 60.1 & 67.8 & 56.8 \\ \cline{1-1}  & 34B & 29.9 & 29.0 & 65.0 & 70.9 & 60.3 \\ \cline{1-1}  & 70B & **39.9** & **41.3** & **71.7** & **72.7** & **64.0** \\ \hline \hline \end{tabular}
\end{table}
표 1: 표준 단축 컨텍스트 벤치마크에 대한 성능. 우리는 _Coding_ 점수를 HumanEval (Chen et al., 2021) 및 MBPP (Austin et al., 2021); Math score as a average of top-1 accuracy of 8-shot GSM8K (Cobbe et al., 2021) and 4-shot MATH (Hendrycks et al., 2021); OpenQA score as a average of 5-shot performance on NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017); _Commonsense_ score as a average of PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC easy and challenge (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018) and CommonsenseQA (Talmor et al., 2018).

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Task** & GPT-3.5 & GPT-4 & PaLM & PaLM-2-L & Llama 2 & Llama 2 Long \\ \hline MMLU (5-shot) & 70.0 & **86.4** & 69.3 & 78.3 & 68.9 & 71.7 \\ Natural Questions (1-shot) & - & - & 29.3 & **37.5** & 33.0 & 35.7 \\ GSM8K (8-shot) & 57.1 & **92.0** & 56.5 & 80.7 & 56.8 & 65.4 \\ HumanEval (0-shot) & 48.1 & **67.0** & 26.2 & - & 29.9 & 32.9 \\ \hline \hline \end{tabular}
\end{table}
표 2: 표준 단축 태스크에 대한 폐쇄 모델과의 비교.

"32k"는 저희 모델의 성능과 일치할 수 있습니다. 이 모델은 순수하게 자체 감독된 모델이 아니며 소수의 샷 결과를 개선하기 위해 대규모 감독 데이터 세트를 사용하여 미세 조정되었다는 점에 유의해야 한다. 모델의 7/13B 변이체가 32k-토큰 서열로 훈련되었기 때문에 32,768개의 최대 프롬프트 길이를 사용하여 비교를 수행하고 결과는 표 13과 같이 일관적이다.

효과적인 컨텍스트 활용 모델이 증가된 컨텍스트 창을 효과적으로 사용할 수 있음을 검증하기 위해 먼저 그림 2에서 각 긴 작업에 대한 결과가 컨텍스트 길이를 증가함에 따라 단조롭게 개선됨을 보여준다. (Kaplan et al., 2020; Hoffmann et al., 2022)에 의해 영감을 받은, 또한, 본 발명의 모델의 언어 모델링 손실이 컨텍스트 길이와 멱-법칙 플러스 상수 스케일링 관계를 따른다는 것을 발견했으며(도 1), 이는 다음을 제안한다:

* 우리 모델은 수익이 감소함에도 불구하고 텍스트의 최대 32,768 토큰까지 성능(언어 모델링 손실)에서 이득을 계속 보여줍니다. 70B 모델을 예로 들자면, 문맥 길이를 두 배로 늘리면 손실량이 \(2^{-\beta}\approx 0.7\)과 모델 특이 상수 \((1-2^{-\beta})\cdot\gamma\)의 비율로 줄어들 것으로 예상할 수 있다.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{**Model**} & NarrativeQA & Qasper & QuALITY & QMSum \\  & F1 (0-shot) & F1 (2-shot) & EM (2-shot) & ROUGE-geo* (1-shot) \\ \hline Focused Transformer (3B) & 16.3 & 15.4 & 20.5 & 10.6 \\ Yarn-7B-128k & 20.9 & 26.2 & 32.3 & 11.4 \\ Together-7B-32k\({}^{\dagger}\) & 23.3 & 27.3 & 41.2 & 12.6 \\ Xgen-7B-8k-base & 17.4 & 20.5 & 21.0 & 6.79 \\ MPI-7B-8k & 18.8 & 24.7 & 23.7 & 8.78 \\ Yarn-13B-128k & 23.4 & 27.1 & 46.4 & 11.9 \\ MPI-30B-8k & 22.9 & 29.0 & 41.5 & 10.3 \\ \hline Llama 2 70B & 25.7 & 27.5 & 53.0 & 11.9 \\ \hline Llama 2 Long 7B & 21.9 & 27.8 & 43.2 & 14.9 \\ Llama 2 Long 13B & 25.6 & 31.2 & 57.6 & 15.7 \\ Llama 2 Long 34B & 29.4 & 33.7 & 65.7 & 15.9 \\ Llama 2 Long 70B & **30.9** & **35.7** & **79.7** & **16.5** \\ \hline \hline \end{tabular}
\end{table}
표 3: 연구 벤치마크에 대한 오픈 소스 장기 컨텍스트 모델과의 비교 \ ({}^{\dagger}\): "together-7B-32k"는 순수하게 사전 훈련된 모델이 아니며 소수의 샷 결과를 개선할 수 있는 감독된 데이터 세트를 사용하여 훈련되었다. \ ({}^{*}\): ROUGE-geo는 ROUGE-1, 2 및 L의 기하 평균이다. 모든 번호는 유효성 검사 결과이며 허용되는 최대 프롬프트 길이는 16,384 토큰으로 설정됩니다.

그림 2: 프롬프트의 최대 컨텍스트 길이가 증가함에 따라 긴 컨텍스트 작업에 대한 성능입니다.

* 더 큰 모델은 곡선의 더 큰 \(\beta\) 값으로 표시되는 컨텍스트를 더 효과적으로 활용할 수 있습니다.

### 명령어 튜닝 결과

본 논문에서는 ZeroSCROLLS(Shaham et al., 2023)에서 요약, 질의 응답, 다중 문서 집합 작업에 걸쳐 있는 10개의 긴 컨텍스트 데이터 세트를 묶는 명령어 튜닝 모델을 테스트한다. 공정한 비교를 위해 벤치마크에서 지정한 것과 동일한 구성(프롬프트, 절단 전략 및 최대 생성 길이 등)을 사용한다. 표 4에 도시된 바와 같이, 인간의 주석이 달린 긴 컨텍스트 데이터를 사용하지 않고, 우리의 70B 채팅 모델은 10개의 태스크 중 7개에서 gpt-3.5-turbo-16k를 능가할 수 있다. 또한 L-Eval(An et al., 2023)에 소개된 6개의 새로운 긴 과제에 대한 평가를 실행하고 부록의 표 17과 같이 다시 강력한 결과를 관찰한다. 피니튜닝된 모델은 자기 지시 데이터의 주요 주제인 QA 태스크에서 특히 우수하다는 것을 알 수 있다. 피니튜닝에 보다 다양한 데이터를 사용할 경우 성능이 더욱 향상될 것으로 기대한다.

장문 LLM을 평가하는 것은 자명하지 않은 작업이라는 점을 언급할 가치가 있다. 이러한 벤치마크에서 사용되는 자동 메트릭은 여러 가지 면에서 제한적이다. 예를 들어, 요약 작업은 단일 지상-진실 요약만 제공되며 \(n\)-그램 일치 메트릭은 반드시 인간의 선호도와 일치하지 않는다. 메트릭이 문제가 적은 QA 및 집계 작업의 경우 입력 컨텍스트를 잘리면 질문에 응답하는 데 필요한 정보도 제거할 수 있습니다. 또 다른 중요한 주의 사항은 대부분의 독점 모델이 훈련 데이터 세부 정보를 공유하지 않아 공개 벤치마크 평가 중 잠재적 누출을 고려하기 어렵다는 것이다.

### Human Evaluation

자동 평가 벤치마크 결과에 보완하여 주석자가 명령 finetuned 모델에서 생성을 선호하는지 또는 주석자에게 질문하여 인간 평가를 수행한다.

\begin{table}
\begin{tabular}{l|c c c|c c c c|c c|c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{3}{c|}{Summarization} & \multicolumn{4}{c|}{Question answering} & \multicolumn{2}{c|}{Aggregation} & \multirow{2}{*}{} \\  & GR & SS & QM & & & & & & & \\ \hline GPT-3.5-turbo (4k) & 21.3 & 16.1 & 15.6 & 20.4 & 49.3 & 25.1 & 66.6 & 27.1 & 49.1 & 49.8 & 34.0 \\ GPT-3.5-turbo-16k\({}^{\dagger}\) & 24.3 & 16.2 & 17.4 & 21.4 & 50.0 & 29.5 & 72.0 & 27.0 & 54.1 & 54.6 & 36.7 \\ Claude (8k) & 24.2 & 16.1 & 14.6 & 21.0 & 52.3 & 32.6 & 84.8 & 36.1 & 61.6 & 47.4 & 39.1 \\ GPT4 (8k) & 26.3 & 17.3 & 18.5 & 22.6 & 50.7 & 27.6 & 89.2 & 41.1 & 62.8 & 60.5 & 41.7 \\ \hline \multicolumn{1}{l}{Llama 2 Long Chat 70B} & 26.0 & 15.0 & 20.0 & 20.9 & 52.0 & 31.7 & 82.6 & 27.3 & 55.5 & 46.2 & 37.7 \\ \hline \hline \end{tabular}
\end{table}
표 4: ZeroSCROLLS 긴 컨텍스트 리더보드 결과입니다. \ ({}^{\dagger}\)8/7/2023으로 평가되었다. GPT-4와 Claude 결과는 리더보드에서 직접 복사되었다. Underscored는 우리 모델이 gpt-3.5-turbo-16k보다 성능이 우수한 7/10 태스크이다.

도 3: 멀티-턴 대화 및 멀티-문서 검색 질의 응답 데이터를 갖는 모델 응답들에 대한 인간 선호도.

MPT-30B-chat, GPT-4, GPT-3.5-turbo-16k 및 Claude-2와 같은 독점 모델에서 유용성, 정직성 및 무해성 측면에서. 자동 메트릭과 달리 인간은 수용 가능한 답변의 공간이 크기 때문에 긴 컨텍스트 모델에 대한 모델 응답의 품질을 평가하는 데 더 뛰어나다. 총 2,352개의 예제와 함께 두 가지 주요 애플리케이션 시나리오에 중점을 둡니다. 멀티-턴 대화 데이터에 대해, 각각의 프롬프트는 모델이 일관된 응답을 생성할 필요가 있는 채팅 히스토리이다. 다중 문서 검색 질의 응답 애플리케이션의 경우, 모델은 검색 세션으로부터 검색된 몇 개의 관련 문서와 해당 검색 질의로 제공된다. 그런 다음 이러한 모델이 주어진 쿼리에 응답하기 위해 정보(검색된 문서)를 얼마나 잘 활용할 수 있는지 평가합니다. 각 비교 예는 3개의 서로 다른 인간 주석자에 의해 평가되었다. 각 비교 예제의 결과를 평균하여 각 모델에 대한 모델의 표준 승률을 계산하고 95% 신뢰 구간과 함께 최종 점수를 그림 3에 나타낸다. 매우 적은 명령 데이터를 사용하여 모델은 MPT-30B-chat, GPT-3.5-turbo-16k 및 Claude-2에 대해 경쟁 성능을 달성할 수 있다. 더 긴 컨텍스트 작업에 대한 인간 평가는 도전적이며 일반적으로 잘 훈련되고 숙련된 주석기가 필요하다는 점에 주목할 가치가 있다. 본 연구를 통해 긴 컨텍스트 다운스트림 응용 프로그램에 대한 지침 세분화 모델의 잠재력을 느낄 수 있을 뿐만 아니라 보다 강력한 긴 컨텍스트 자동 평가를 개발하기 위한 향후 노력에 동기를 부여할 수 있기를 바란다.

## 4 Analysis

이 부분에서. 설계 선택(즉, 아키텍처 수정, 데이터 믹스 및 교육 커리큘럼)을 정당화하고 최종 성능에 대한 기여도를 정량화하기 위해 절제 실험을 수행한다.

### Long Text에 대한 위치 인코딩

우리의 초기 실험은 합성 "첫 문장 검색" 작업을 사용하여 사전 훈련된 모델의 효과적인 컨텍스트 창을 조사하는 데 사용되었으며, 여기서 모델이 입력의 첫 문장을 반환하도록 간단히 프롬프트한다. 초기 태스크 결과는 원본 Llama 2 아키텍처가 손상되지 않은 상태에서 광범위한 긴 컨텍스트 사전 훈련 후에도 \(4,000\)- \(6,000\) 토큰을 넘어 효과적으로 참석할 수 없음을 시사한다. 이 병목 현상이 멀리 떨어진 토큰에 대한 주의 점수 3에 무거운 붕괴를 부과하는 라마 2 시리즈에 사용된 RoPE 위치 인코딩에서 비롯된다고 가정한다. 본 논문에서는 RoPE의 "기본 주파수 \(b\)"를 \(10,000\)에서 \(500,000\)으로 증가시키는 감쇠 효과를 줄이기 위해 기본 RoPE 인코딩을 간단히 수정하여 각 차원의 회전 각도를 근본적으로 줄인다. 이 아이디어는 또한 Reddit _r_/LocalLLaMa 커뮤니티 및 Roziere 등(2023)에서 동시에 제안된다. 베이스 주파수 변화의 효과는 도 4에서 시각화된다. "위치 보간"(PI)(Chen 등, 2023)이라는 또 다른 동시 접근법은 긴 시퀀스들 내의 토큰들의 위치들이 모델의 원래 위치 범위에 매핑될 수 있도록 입력 위치들을 선형적으로 스케일링하는 것을 제안한다. 그림에서 보는 바와 같이 묵시적으로 붕괴 감소 효과도 달성한다.

각주 3: 상대적 위치 \(|m-n|\)가 클수록 크게 붕괴되는 양은 \(\mathbb{E}_{q,k}[\text{RoPE}(q,m)^{\top}\text{RoPE}(k,n)|m,n]\)이다. 여기서 \(q,k\)는 위치 \(m\)과 \(n\)에서 두 토큰의 질의와 키이다.

시각화에서 또 다른 흥미로운 관찰은 RoPE가 장거리 영역에서 큰 "진동"을 도입한다는 것인데, 이는 언어 모델링에 바람직하지 않을 수 있다(Sun 등, 2022). 이 효과가 성능에 영향을 미치는지 여부를 조사하기 위해 고주파 성분을 평활화하는 회전 인코딩의 최근 제안된 또 다른 변형인 xPos(Sun et al., 2022)도 조사했다. 기본 매개변수가 있는 xPos는 RoPE와 동일한 감쇠 문제를 겪으므로 xPos에 유사한 감쇠 수정도 적용했다.

구체적으로, 우리는 다음과 같은 방법을 경험적으로 비교한다: RoPE 기준선, PI, 조정된 기본 주파수를 가진 제안된 RoPE(RoPE ABF로 표시됨) 및 xPos ABF(그림 4의 시각적 비교). 1) 표 5와 그림 4(a), 2)의 긴 서열 검증 복잡성에 대한 결과를 보고한다.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**PE** & Books & CC & Wikipedia \\ \hline RoPE & 6.548 & 6.816 & 3.802 \\ \hline RoPE PI & 6.341 & 6.786 & 3.775 \\ RoPE ABF & **6.323** & **6.780** & **3.771** \\ xPos ABF & 6.331 & **6.780** & **3.771** \\ \hline \hline \end{tabular}
\end{table}
표 5: 상이한 위치 인코딩 변형을 갖는 모델의 검증 복잡성. 모든 샘플은 \(32,768\)-token 시퀀스(CC: CommonCrawl)이다.

도 4(b)의 첫 번째 문장-검색 컨텍스트 프로빙 태스크4, 및 3) 표 6의 일부 대표적인 정규 컨텍스트 태스크(긴 모델이 짧은 컨텍스트 태스크에서 퇴화하지 않는다는 것을 검증하기 위해). 모든 모델 변형은 32,768-토큰 긴 시퀀스로 구성된 추가 80B 토큰을 사용하여 7B 라마 2 체크포인트로부터 지속적으로 사전 훈련된다.

각주 4: (Mohtashami and Jaggi, 2023)에 사용된 PassKey 태스크에 대해서도 테스트한다. RoPE를 제외한 모든 모델 변형은 완벽한 정확도를 달성할 수 있다. 우리는 이 작업이 문맥 탐사에 지나치게 간단하다고 믿는다.

전반적으로, 이러한 평가에 대한 결과는 RoPE ABF가 조사된 모든 변이체 중에서 가장 잘 수행함을 시사한다. 특히, RoPE ABF가 첫 번째 문장-검색 태스크에서 전체 32,768-토큰 컨텍스트 윈도우까지 그 성능을 유지할 수 있는 유일한 변종임을 알 수 있다. 또한 진동이 적은 xPos ABF가 실질적인 이득으로 이어지지 않는다는 것을 발견했으며, 이는 이러한 아티팩트가 언어 모델링에 해롭지 않음을 시사한다. xPos가 더 나은 외삽 특성을 갖는다고 주장되지만(Sun 등, 2022), 우리는 베이스 주파수 수정을 통해 xPos가 RoPE보다 더 나은 외삽을 하지 않는다는 것을 발견했다(부록 C 참조). 실험 결과와 함께, 우리는 부록 B에서 RoPE ABF와 PI의 차이에 대한 이론적 분석을 제공한다. 우리는 RoPE ABF가 RoPE PI와 비교할 때 입도가 증가된 임베딩 벡터를 분배하여 모델이 위치를 더 쉽게 구별할 수 있다고 주장한다. 임베디드 벡터 사이의 상대 거리는 RoPE PI의 키 매개변수에 대한 선형 의존성과 RoPE ABF의 키 매개변수에 대한 로그 의존성을 가지며, 이는 기본 주파수가 매우 민감하지 않고 최대 시퀀스 길이를 기반으로 쉽게 조정할 수 있다는 우리의 경험적 관찰과 일치한다.

### 사전 학습 데이터 혼합

우리의 모델을 지속적으로 사전 훈련하기 위해 사용된 데이터는 라마 2에서 사용된 기존 데이터 세트와 새로운 긴 텍스트 데이터를 결합한다. 또한 데이터 소스 혼합 비율을 상향 가중치 긴 데이터 샘플로 조정했다. 7B 모델을 사용한 초기 실험은 이 데이터 믹스를 사용하여 상당한 개선을 확인한다.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & HumanEval & Math & MMLU & HellaSwag & TQA \\ \hline RoPE & 14.63 & **3.62** & 45.69 & 76.31 & 65.23 \\ \hline RoPE PI & 15.24 & 3.08 & 45.84 & 76.65 & 65.96 \\ RoPE-ABF & **17.07** & 3.52 & **46.24** & **76.73** & 66.04 \\ xPos-ABF & 16.46 & 3.54 & 45.72 & 76.68 & **66.14** \\ \hline \hline \end{tabular}
\end{table}
표 6: 표준 단문-맥락 벤치마크들 상에서 상이한 위치 인코딩 변형들을 갖는 모델들의 성능.

도 4: 탐색된 위치 인코딩 변형들의 원거리 토큰들에 대한 원시 주의 스코어들을 감소시키는 것(키들 및 쿼리들이 올-온즈 벡터들인 것으로 가정함).

[MISSING_PAGE_FAIL:9]

### Instruction Tuning

우리는 지도된 긴 데이터가 필요하지 않은 사전 훈련된 긴 컨텍스트 모델을 지시하기 위한 다양한 전략을 탐색했다. 우리는 Llama 2 Chat((Touvron et al., 2023)에서 "RLHF V5"로 지칭됨)의 짧은 명령어 데이터로 모델들을 단지 피니튜닝하는 것으로 시작하여, 이전의 긴 문맥의 연속적인 프리트레이닝을 잊는 것을 피하기 위해 일부 프리트레이닝 데이터와 혼합한다. 표 9에서 입증된 바와 같이, 짧은 명령 데이터만을 사용하는 것은 이미 다양한 긴-컨텍스트 태스크들에서 Llama 2보다 상당히 우수한 괜찮은 긴 모델을 생성할 수 있다. 짧은 프롬프트만 포함하는 이 데이터 세트 위에 사전 훈련 데이터 추가(전체 시퀀스에 대한 언어 모델링 손실 계산)가 대부분의 데이터 세트에 대한 성능을 더욱 높일 수 있음을 알 수 있다. 이로부터 영감을 받아 자체 지시 데이터로 미세 조정할 때 긴 컨텍스트 입력에 대한 LM 손실을 추가한다. 이 간단한 트릭은 불균형한 입력 및 출력 길이 5가 있을 때 학습을 더 안정적으로 만들어 대부분의 테스트된 작업(표 9의 마지막 두 행)에서 상당한 개선을 제공한다.

각주 5: 우리의 경우 대부분의 샘플의 출력 길이는 긴 문맥 입력의 출력 길이보다 훨씬 짧다.

### Training Curriculum

연속 사전 훈련은 실험에서 그 효과를 입증했지만 여전히 열려 있는 질문이 남아 있는데, 긴 시퀀스로 처음부터 사전 훈련하는 것이 연속 사전 훈련보다 더 나은 성능을 산출하는가? 이 섹션에서는 다양한 교육 커리큘럼을 연구하고 지속적인 사전 교육이 더 적은 계산 예산으로 경쟁력 있는 성능을 제공할 수 있는지 조사하려고 한다. 우리는 시작부터 끝까지 32,768개의 시퀀스 길이를 가진 7B 모델을 사전 훈련하여 시작한다. 그런 다음 4096개의 시퀀스 길이로 시작하여 전체 학습 과정의 20%, 40%, 80%를 완료하면 32,768로 전환되는 다양한 2단계 학습 커리큘럼을 탐색했다. 모든 경우에 대해 동일한 수의 총 학습 토큰을 유지 하 고 배치 크기와 시퀀스 길이를 적절하게 조정 하 여 각 그래디언트 업데이트 당 토큰 수를 일정하게 유지 (400만 토큰) 합니다.

섹션 4.2에 사용된 긴 텍스트 QA 작업에 대한 모델을 평가하고 다른 검증 코퍼스에 대한 최종 모델의 복잡성을 보고한다. 표 10 및 표 11에 나타난 바와 같이, 짧은 컨텍스트 모델로부터의 지속적인 사전 훈련은 성능에 거의 손실을 부과하지 않으면서 약 40%의 FLOP를 쉽게 절약할 수 있다. 이러한 결과는 또한 그림 6의 각 실행에서 관찰한 훈련 손실 곡선과 일치하며, 모델은 증가된 서열 길이에 빠르게 적응하고 유사한 손실 척도에 도달할 수 있다.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Pretrain Curriculum** & **FLOPs** & \begin{tabular}{c} NarrativeQA \\ F1 \\ \end{tabular} & \begin{tabular}{c} Qasper \\ F1 \\ \end{tabular} & \begin{tabular}{c} Quality \\ EM \\ \end{tabular} &
\begin{tabular}{c} QMSum \\ ROUGE-geo \\ \end{tabular} \\ \hline
32k from scratch & \(3.783\times 10^{22}\) & 18.5 & 28.6 & 37.9 & 11.46 \\
4k\(\rightarrow\)32k @ 20\% & \(3.405\times 10^{22}\) & 20.0 & 28.1 & 38.8 & 12.09 \\
4k\(\rightarrow\)32k @ 40\% & \(3.026\times 10^{22}\) & 20.1 & 27.0 & 37.4 & 12.44 \\
4k\(\rightarrow\)32k @ 80\% & \(2.270\times 10^{22}\) & 18.5 & 25.0 & 38.3 & 11.00 \\ \hline \hline \end{tabular}
\end{table}
표 10: 긴 컨텍스트 QA 태스크에 대한 상이한 트레이닝 커리큘럼을 갖는 모델의 비교.

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline
**Settings** & Qasper & NarrativeQA & QuALITY & SummScreenFD & QMSum \\ \hline Llama 2 Chat baseline & 12.2 & 9.13 & 56.7 & 10.5 & 14.4 \\ \hline \hline \multicolumn{6}{l}{Llama 2 Long _finetuned_ with:} \\ \multicolumn{6}{l}{*RLHF V5"} & 22.3 & 13.2 & 71.4 & 14.8 & 16.9 \\ \multicolumn{6}{l}{*RLHF V5" mix pretrain} & 23.7 & 16.6 & 76.2 & **15.7** & 17.8 \\ \multicolumn{6}{l}{*RLHF V5" mix self-inst w/o LM loss} & 35.7 & 22.3 & 59.3 & 12.2 & 13.4 \\ \multicolumn{6}{l}{*RLHF V5" mix self-inst with LM loss} & **38.9** & **23.3** & **77.3** & 14.5 & **18.5** \\ \hline \hline \end{tabular}
\end{table}
표 9: 상이한 명령어 피니튜닝 데이터 믹스의 비교.

## 5 AI 안전

### 안전 벤치마크 평가

다양한 다운스트림 태스크들에서 우수한 성능을 보여주었음에도 불구하고, 대형 언어 모델들은 유해하고, 오정보적이며, 편향된 콘텐츠를 생성하기 쉽다(Lin et al., 2021; Hartvigsen et al., 2022; Dhamala et al., 2021; Ji et al., 2023). 긴-컨텍스트 언어 모델들은 그들의 컨텍스트 윈도우에서 확장된 입력들을 프로세싱할 수 있지만, 동시에, 이들은 특히 신속한 주입과 같은 수단을 통해 탈옥의 더 높은 위험에 직면한다(Greshake et al., 2023). 본 절에서는 TruthfulQA(Lin et al., 2021), ToxiGen(Hartvigsen et al., 2022), BOLD(Dhamala et al., 2021) 등 3개의 표준 학술 벤치마크를 사용하여 수업 미세 조정 모델의 안전 능력을 평가한다(Touvron et al., 2023). 가장 큰 명령어 미세 조정 모델 변형(즉, 70B)에 초점을 맞추고 그 결과를 표 12의 오픈 소싱 LLM(Falcon-instruct Almazrotoei et al.(2023), MPT-instruct MosaicML(2023a)) 및 적절 LLMS(GPT-3.5, GPT-4(OpenAI, 2023), Claude-2(Anthropic, 2023))와 비교한다.

일반적인 명령어 미세 조정 모델은 Llama 2 Chat과 유사한 안전 성능을 유지하며, Falcon-instruct 및 MPT-instruct와 같은 다른 오픈 소스 LLM에 비해 안전하고 편향되지 않음을 관찰한다. AI 안전은 복잡한 영역이며 세 가지 벤치마크로 정교화된 수업 모델의 모든 안전 측면을 종합적으로 평가하는 것은 매우 어려울 수 있다. 그러나, 우리는 우리의 분석이 동일한 주제에 대한 다른 연구에서 논의되지 않은 긴 맥락의 대형 언어 모델의 안전 성능에 대한 파일럿 연구의 역할을 하고 방향성 신호를 제공할 수 있기를 바란다(Tworkowski et al., 2023; Ding et al., 2023; Chen et al., 2023). 현재 커뮤니티는 또한 롱컨텍스트 대형 언어 모델 평가를 위한 전용 안전 벤치마크가 부족하며 향후 작업에 이러한 방향에 투자할 계획이다.

TruthfulQA, TruthfulQA(Lin et al., 2021)에 대한 명령어 미세 조정 모델을 평가하여 그 사실성을 벤치마킹한다. 벤치마크는 보건, 법률, 금융, 정치 등 38개 범주를 포괄하는 817개 문항으로 구성되어 있다(Lin et al., 2021). (Touvron et al., 2023)과 유사하게, 6개의 랜덤 QA 쌍이 있는 few-shot 프롬프트를 생성에 사용한 다음, 두 개의 미세 조정된 GPT-3 모델을 활용하여 분류한다.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Model** & CC & Books & Wikipedia \\ \hline
32k from scratch & 7.67 & 6.52 & 4.31 \\
4k\(\rightarrow\)32k @ 20\% & 7.59 & 6.46 & 4.26 \\
4k\(\rightarrow\)32k @ 40\% & 7.59 & 6.46 & 4.25 \\
4k\(\rightarrow\)32k @ 80\% & 7.59 & 6.49 & 4.25 \\ \hline \hline \end{tabular}
\end{table}
표 11: 세 가지 검증 세트에 대해 서로 다른 훈련 커리큘럼을 가진 모델의 복잡성 평가.

도 6: 훈련 커리큘럼 절제에 대한 평활 손실 곡선. 왼쪽에서 우리는 고정된 컨텍스트 윈도우로 훈련된 모델에 대한 손실을 보여준다. 오른쪽에서는 점선으로 표시된 여러 단계에서 컨텍스트 길이를 4,096에서 32,768 토큰으로 전환하는 훈련 커리큘럼을 비교한다. 우리의 모델은 수천 단계 내에서 새로운 시퀀스 길이에 빠르게 적응할 수 있다.

세대가 진실하고 유익한지 여부. 표 12의 최종 메트릭으로 진실하고 유익한 세대의 비율을 보고한다.

ToxiGenWe는 ToxiGen(Hartvigsen et al., 2022)을 사용하여 명령어 미세 조정 모델의 독성을 측정하며, 여기서 13개의 소수 그룹에 대한 독성 및 혐오 세대의 비율을 확인한다. (Touvron et al., 2023)에 이어서, 타겟 인구통계학적 그룹에서 주석자들이 서로 동의하지 않는 프롬프트를 필터링하였다. 우리는 RoBERTa(Liu et al., 2019)에 기초하여 미세 조정된 디폴트 ToxiGen 분류기를 사용하여 모델의 출력의 독성 수준을 평가한다. 표 12에 모든 그룹에 걸쳐 독성 세대의 백분율을 보고한다.

BOLD(BoldBias in Open-Ended Language Dataset) Dhamala 등(2021)은 모델이 상이한 인구통계학적 그룹의 사람들에 대해 얼마나 편향되어 있는지를 정량화하기 위해 이 작업에서 사용된다. 이 데이터 세트는 인종, 성별, 종교, 정치 이념 및 직업 등 5개 영역을 포함하는 영어 위키피디아에서 추출된 23,679개의 프롬프트로 구성되며 총 43개의 하위 그룹이 있다. Touvron 등(2023)에 이어 힌두교와 무신교 종교 하위 그룹에 속하는 프롬프트는 각각 12개 및 29개 프롬프트만 특징으로 제외한다. 각 모델에서 세대를 추론한 후, Valence Aware Dictionary and Sentiment Reasoner (VADER) Hutto and Gilbert (2014)를 활용하여 -1에서 1 사이의 점수로 감성 분석을 수행한다. 긍정적인 점수는 프롬프트에 언급된 하위 그룹에 대한 긍정적인 감성에 해당하고 그 반대의 경우도 마찬가지이다. 0에 가까운 감성 점수는 원하는 중립적인 감성을 나타낸다. 우리는 표 12의 BOLD에 대한 최종 메트릭으로 43개의 인구통계학적 하위 그룹에 걸친 평균 감정 점수를 보고한다.

빨간 티밍 연습

현재 긴 맥락 이해를 위해 설계된 개방형 안전 벤치마크는 없다. 모델이 긴 컨텍스트 사용 시나리오에서 안전한지 확인하기 위해 채팅 모델의 취약성을 더 잘 이해하기 위해 내부 레드 팀핑을 수행했다. 우리는 그 모델에 긴 컨텍스트(예: 긴 대화)를 공급함으로써 모델을 공격하고, 불법 및 범죄 행위(예: 테러, 절도 및 인신매매), 혐오 및 해로운 행동(예: 명예 훼손, 자해, 섭식 장애 및 차별), 부적격 조언 투브론 등(2023)을 포함하는 위험 영역을 다루는 적대적 프롬프트가 뒤따른다. 수작업 검사를 통해 Llama 2 Chat Touvron et al.(2023)과 비교하여 유의한 위험을 관찰하지 못하였다. 향후 작업에서 긴 컨텍스트 대형 모델에 대한 새로운 공격 벡터에 더 많은 투자를 할 계획입니다.

## 6 Limitations

제한된 기능.본 논문에서 제안하는 모델은 긴 형태의 출력을 필요로 하는 창의적인 글쓰기와 같은 광범위한 긴 컨텍스트 응용에 대해 아직 미세 조정되지 않았다. 다양한 시나리오에 대해 기존의 정렬 레시피, 예를 들어, RLHF를 적용하는 것은 비용이 많이 들고 자명하지 않다. 숙련된 주석자조차도 빽빽한 텍스트에서 복잡한 세부 사항에 어려움을 겪을 수 있다. 이와 관련하여, 우리는 긴 LLMs에 대한 효율적인 정렬 방법을 개발하는 것이 향후 연구에 매우 가치 있는 방향이 될 것이라고 생각한다.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & Model Size & TruthfulQA \(\uparrow\) & ToxiGen \(\downarrow\) & BOLD \(\downarrow\) \\ \hline GPT-3.5-turbo & - & 78.46 & 0.01 & 0.50 \\ GPT-3.5-turbo-16k & - & 75.15 & 0.07 & 0.49 \\ Claude-2 & - & 62.66 & 0.05 & 0.46 \\ GPT4 & - & **80.66** & 0.03 & 0.43 \\ Falcon-instruct & 40B & 57.41 & 3.3 & 0.39 \\ MPT-instruct & 30B & 42.71 & 16.85 & **0.34** \\ \hline Llama 2 Chat & 70B & 64.14 & 0.01 & 0.41 \\ \hline Llama 2 Long Chat & 70B & 60.95 & **0.00** & 0.40 \\ \hline \hline \end{tabular}
\end{table}
표 12: 세 가지 안전 벤치마크에 대한 미세 조정된 LLM의 평가. 진실 QA의 경우, 우리는 진실하고 유익한 세대 비율을 제시한다. ToxiGen의 경우 모든 그룹에 걸쳐 독성 세대의 비율을 제시한다(더 작을수록 좋다). BOLD의 경우 43개의 인구통계학적 그룹(0에 가까울수록 더 좋다)에 걸쳐 평균 감정 점수를 보고한다.

토큰라이저 효율. 제안된 모델 시리즈는 최대 32,768개의 토큰을 소비할 수 있지만, 실제로 모델이 취할 수 있는 단어의 수는 토큰라이저 행동에 크게 영향을 받는다. Llama 시리즈에서 사용하는 토큰화기는 비교적 작은 어휘(32k 심볼)를 가지며 GPT-3.5의 토큰화기에 의해 주어진 시퀀스에 비해 더 긴 시퀀스를 생성하는 경우가 많다. 본 논문에서 제안하는 토큰화기는 평균적으로 \(10\%\) 더 많은 토큰을 생성하는 경우가 많다. 또한, 우리가 사용하는 토큰나이저도 효율적으로 화이트 스페이스를 처리할 수 없어 긴 코드 데이터를 처리하는 것이 비효율적이다.

환각. 다른 LLM과 마찬가지로 제안된 모델을 테스트할 때 환각 문제를 관찰했다. 이 문제는 짧은 맥락 모델의 경우 일반적이지만 긴 맥락 모델에 대해 이 문제를 해결하는 것은 그들이 소비하는 조밀한 정보와 불충분한 정렬 프로세스 때문에 더 두드러질 수 있다.

## 7 Conclusion

우리는 강력한 긴 문맥 성능을 달성하기 위해 간단하면서도 필요한 위치 인코딩 정제 및 지속적인 사전 훈련을 활용하는 일련의 긴 문맥 LLM을 제시한다. 우리의 긴 컨텍스트 스케일링은 추가적인 400B 토큰들을 갖는 Llama 2로부터 지속적으로 프리트레이닝함으로써 수행되며, 짧은 및 긴 컨텍스트 태스크들 모두에서 Llama 2를 능가한다. 또한, 제안된 모델은 기존의 오픈소스 장맥락 모델과 비교하여 우수한 성능을 보이며, 인간 감독 없이 간단한 명령어 피니튜닝 절차를 거친 후 장맥락 태스크의 집합에서 gpt-3.5-turbo-16k와 유리하게 비교된다. 우리는 포지션 인코딩의 뉘앙스, 데이터 믹스 및 사전 교육 커리큘럼을 포함한 다양한 요인이 최종 성과에 미치는 영향에 대한 통찰력을 제공하는 포괄적인 분석으로 결과를 보완한다. 우리는 우리의 연구가 긴 컨텍스트 LLM에 더 쉽게 접근할 수 있고 이 분야의 추가 발전을 촉진할 수 있기를 바란다.

## 8 Acknowledgement

우리는 이 프로젝트의 데이터, 인프라 및 기타 다양한 측면에 대한 귀중한 지원에 대해 니콜레이 바슐리코프, 맷 와일드, 웨닌 푸, 지안유 황, 제냐 리, 매튜 올담 및 숀 쉬에게 감사드린다.

## References

* 알마즈루에이 등(2023) 에브테삼 알마즈루에이, 함자 알로베이드리, 압둘라지즈 알샴시, 알레산드로 카펠리, 룩산드라 코조카루, 메루안 데바, 에티엔 고피넷, 다니엘 헤슬로우, 줄리엔 라우네, 쿠엔틴 말라트, 바데르딘 누네, 침례자 파니에, 및 구일허메 페네도. 팔콘-40B: 최첨단 성능을 갖춘 개방형 대형 언어 모델입니다. External Links: 2307.11088 Cited by: SS1.
* An et al.(2023)Anthropic. 100K 컨텍스트 윈도우를 소개합니다. 외부 링크: SS1에 의해 인용된 링크입니다.
* J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. J. Cai, M. Terry, Q. V. Le, and C. Sutton (2021) Program synthesis with large language models. arXiv:abs/2108.07732. Cited by: SS1.
*Y. 비스크 젤러스 Choi, et al. (2020)Piqa: reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, Vol. 34, pp. 7432-7439. Cited by: SS1.
*M. 진진투렉 Yuan, H. Ponde de Oliveira Pinto, J. Kaplan, H. Edwards, Y. 부르다남 Joseph, G. Brockman, et al.(2021)Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Cited by: SS1.
*S. 진성 왕락 Chen, Y. Tian(2023)Extending context window of large language models via positional interpolation. External Links: 2307.11088 Cited by: SS1.
*R. 아동석 Gray, A. Radford, and I. Sutskever (2019) Generating long sequence with sparse transformers. arXiv preprint arXiv:1904.10509. Cited by: SS1.
*Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 질문에 대한 답을 풀었다고 생각해? try arc, the ai2 reasoning challenge. _ arXiv preprint arXiv:1803.05457_, 2018.
* Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reitichiro Nakano, et al. Training verifiers to solve mathematics word problems. _ arXiv preprint arXiv:2110.14168_, 2021.
* Conover 등(2023) Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 무료 인형: 세계 최초의 진정한 개방형 명령 조정 llm, 2023을 소개합니다. URL [https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm).
* Dao 등(2022) Tri Dao, Daniel Y. 푸, 스테파노 어몬, 아트리 루드라, 크리스토퍼 레 플래시 어텐션: 빠르고 기억력 효율적인 정확한 어텐션과 io-aware. 2022년 _NeurIPS_ 에 있습니다.
* Dasigi 등(2021) Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 연구 논문에 고정된 정보 추구 질문 및 답변 데이터 세트. _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 4599-4610, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.365. URL [https://aclanthology.org/2021.naacl-main.365](https://aclanthology.org/2021.naacl-main.365)
* Dhamala 등(2021) Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 볼드: 개방형 언어 생성에서 편향을 측정하기 위한 데이터 세트 및 메트릭입니다. _Proceedings of the 2021 ACM conference on fairness, accountability, and transparency_, pages 862-872, 2021.
* Ding 등(2023) Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. Longnet: 트랜스포머를 1,000,000,000 토큰으로 스케일링, 2023.
* Greshake 등(2023) Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. 등록한 것이 아닙니다. 간접 프롬프트 주입으로 실제 llm 통합 애플리케이션을 타협합니다. _ arXiv preprint arXiv:2302.12173_, 2023.
* Hartvissen 등(2022) Thomas Hartvissen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. Toxigen: 적대적 및 암시적 혐오 음성 탐지를 위한 대규모 머신 생성 데이터 세트입니다. _ arXiv preprint arXiv:2203.09509_, 2022.
* Hendrycks 등(2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 수학 데이터세트로 수학 문제 풀이를 측정하는 단계 _ arXiv preprint arXiv:2103.03874_, 2021.
* Hoffmann et al.(2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. 레이, 오리올 빈얄, 로랑 시프르 2022년 컴퓨팅 최적 대용량 언어 모델을 교육합니다.
* Hutto and Gilbert (2014) Clayton Hutto and Eric Gilbert. 베이더: 소셜 미디어 텍스트의 감정 분석을 위한 간결한 규칙 기반 모델. _Proceedings of the international AAAI conference on web and social media_, Volume 8, pages 216-225, 2014.
* Ji et al.(2023) Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 자연어 생성에서의 환각에 대한 조사_ ACM Computing Surveys_, 55(12):1-38, 2023.
* Joshi 등(2017) Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: 읽기 이해를 위한 대규모 원거리 감독 챌린지 데이터 세트입니다. _ arXiv preprint arXiv:1705.03551_, 2017.
* Kaplan 등(2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.
* Kocisky 등(2018) Tomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. 내러티브QA 독해 도전 Transactions of the Association for Computational Linguistics_, 6:317-328, 2018. doi: 10.1162/tacl_a_00023. URL [https://aclanthology.org/018-1023](https://aclanthology.org/018-1023).
* Kopf et al. (2023) Andreas Kopf, Yannic Kilcher, Dimitri von Rutte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richard Nagyfi, et al. Openassistant conversations-democratizing large language model alignment. _ arXiv preprint arXiv:2304.07327_, 2023.
* Kopf 등(2020) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, 등. Natural questions: 질문 응답 연구를 위한 벤치마크. _ Transactions of the Association for Computational Linguistics_, 7:453-466, 2019.
* Lin 등(2021) Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: 모델이 인간의 거짓을 모방하는 방법을 측정합니다. _ arXiv preprint arXiv:2109.07958_, 2021.
* Liu 등(2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: 강건하게 최적화된 버트 사전 트레이닝 접근법. _ arXiv preprint arXiv:1907.11692_, 2019.
* Mihaylov et al.(2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 갑옷이 전기를 전도할 수 있나요? 오픈 북 질문 답변을 위한 새 데이터 세트입니다. _ arXiv preprint arXiv:1809.02789_, 2018.
* Mohtashami and Jaggi(2023) Amirkeivan Mohtashami and Martin Jaggi. 랜드마크 주의: 트랜스포머에 대한 랜덤-액세스 무한 컨텍스트 길이. _ arXiv preprint arXiv:2305.16300_, 2023.
* MosaicML(2023a) MosaicML. mpt-30b 소개: 2023a 오픈소스 기초 모델용 바 키우기입니다. URL www.mosaicml.com/blog/mpt-30b. Accessed: 2023-06-22.
* MosaicML(2023b) MosaicML. mpt-7b를 소개합니다: 오픈 소스, 사용 가능한 llms, 2023b에 대한 새로운 표준입니다. URL www.mosaicml.com/blog/mpt-7b.
* Narayanan et al.(2021) Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. megatron-lm을 이용한 gpu 클러스터에 대한 효율적인 대규모 언어 모델 트레이닝. _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_, pages 1-15, 2021.
* Nijkamp et al. (2023) Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, et al. Long sequence modeling with xgen: A 7b llm trained on 8k input sequence length. _ Salesforce AI Research Blog_, 2023.
* OpenAI(2022) OpenAI. Gpt-4 기술 보고서, 2023년
* Ouyang et al.(2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _ Advance in Neural Information Processing Systems_, 35:27730-27744, 2022.
* Pang 등(2022) Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. 질문: 긴 입력 텍스트로 질문에 답합니다, 네! _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 5336-5358, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.391. URL [https://aclanthology.org/2022.naacl-main.391](https://aclanthology.org/2022.naacl-main.391)
* Peng 등(2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models, 2023.
* r/LocalLLaMa (2023) r/LocalLLaMa. NTK-Aware Scaled RoPE를 사용 하면 llama 모델이 미세 조정 및 최소 복잡성 저하 없이 확장 (8k+) 컨텍스트 크기를 가질 수 있습니다. [https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/] (https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/) 2023-08-25
* Roziere 등(2023) 침례자 Roziere, Jonas Gehring, Fabian Gloeckle, Stan Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2023.
* Sakaguchi 등(2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: 스케일에서의 적대적인 winograd 스키마 챌린지. _ Communications of the ACM_, 64(9):99-106, 2021.
* Sap 등(2019) Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: 사회적 상호 작용에 대 한 상식 추론 _ arXiv preprint arXiv:1904.09728_, 2019.
* Shaham 등(2023) Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 제로스크롤: 긴 텍스트 이해를 위한 제로샷 벤치마크, 2023년입니다.
* Su 등(2022) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2022.
* Su et al.(2021)Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benham, Vishrav Chaudhary, Xia Song, and Furu Wei. 길이 추정 가능한 변압기, 2022년입니다.
* Talmor et al. [2018] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. _arXiv preprint arXiv:1811.00937_, 2018.
*ogether [2023] Together. Llama-2-7b-32k-instruct -- and fine-tuning for llama-2 models with together api, 2023. URL [https://together.ai/blog/llama-2-7b-32k-instruct](https://together.ai/blog/llama-2-7b-32k-instruct)
* Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* Tworkowski 등. [2023a] Szymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Milos. Focused transformer: Contrastive training for context scaling, 2023a.
* Tworkowski 등. [2023b] Szymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Milos. Focused transformer: Contrastive training for context scaling, 2023b.
* Wang et al. [2022] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. _arXiv preprint arXiv:2212.10560_, 2022.
* Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence?, 2019.
* Zhong et al. [2021] Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutthia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. QMSum: A new benchmark for query-based multi-domain meeting summarization. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 5905-5921, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.472. URL [https://aclanthology.org/2021.naacl-main.472](https://aclanthology.org/2021.naacl-main.472).

[MISSING_PAGE_EMPTY:17]

이 매핑의 목적은 어텐션 모듈이 입력 시퀀스에서 상이한 위치에 위치된 동일한 토큰의 2개의 인스턴스에 대응하는 벡터를 분리하는 것을 돕는 것이다.

특정 위치 임베딩 \(f\)으로 사전 훈련된 트랜스포머의 시퀀스 길이를 \(L\)에서 \(\hat{L}\)으로 확장하는 것을 목표로 하여 임베딩된 벡터의 이전 이미지와 새로운 이미지 사이의 거리를 최소화하는 위치 임베딩 \(\hat{f}\)을 제안하고자 한다.

\[d(f,\hat{f})=\max_{x\in\mathcal{X}}\min_{k\in\{0,..N-1\}}\min_{j\in\{0,.. \hat{N}-1\}}\text{dist}[f(x,k),\hat{f}(x,j)],\]

여기서 \(\mathcal{X}\subset\mathbb{R}^{d}\)은 위치적으로 임베딩되어야 하는 벡터 집합이다. (Chen et al., 2023)은 주의 스코어의 크기를 통해 이 거리를 계산했지만, 위치 임베딩의 나이브 외삽과 비교할 때 원래의 RoPE 이미지들에 대한 거리의 감소된 값으로 인해 그들의 방법 "위치 보간"의 효율성을 여전히 주장했다.

이를 고려하여 훈련된 변압기의 시퀀스 길이를 확장하기 위한 두 가지 다른 방법, 즉 \(\alpha\)으로 파라미터화된 위치 보간법(PI)과 \(\beta\)으로 파라미터화된 조정 베이스 주파수(ABF)를 고려한다. 이 두 가지 방법은 다음의 임베딩 곡선에 대응한다:

\[f^{RoPE+PI}(x,t)_{j} =(x_{2j}+ix_{2j+1})\,e^{i\alpha\cdot(b^{-\frac{2j}{d}})t}\] \[f^{RoPE+ABF}(x,t)_{j} =(x_{2j}+ix_{2j+1})\,e^{i(\beta b)^{-\frac{2j}{d}}t}\]

위치 임베딩 a-프리오리를 평가할 때, 임베딩 이미지가 임베딩 공간 위에 분산되는 입도의 정도를 고려해야 한다. 대체 위치 임베딩 \(\hat{f}\) 매핑 \(\mathbb{R}^{d}\times\mathbb{N}\)을 \(\mathbb{C}^{d/2}\)으로 비교하면 가장 가까운 두 이미지 사이의 거리가 최대인 것을 선호해야 한다.

\[q(\hat{f})=\min_{x\in\mathcal{X};k\neq j\in\{0..\hat{N}-1\}}\text{dist}[\hat{f }(x,k),\hat{f}(x,j)]].\]

\begin{table}
\begin{tabular}{l|c c c c c c} \hline
**Model** & \multicolumn{1}{c}{Coursera} & \multicolumn{1}{c}{TPO} & TopicRetrieval & \multicolumn{1}{c}{FinQA} & \multicolumn{1}{c}{ContractQA} & \multicolumn{1}{c}{NaturalQuestions} \\ \hline Claude 1.3 100k & 60.2 & 83.6 & 70.6 & - & - & - \\ gpt-3.5-turbo-16k & 59.7 & 69.9 & 69.3 & 45.4 & 24.9 & 45.9 \\ \hline \multicolumn{7}{l}{_Best open models reported in An et al. (2023)_} \\ longchat-13b-16k & 36.8 & 55.4 & 33.3 & 37.9 & 21.1 & 22.8 \\ chatglm2-6b-8k & 47.2 & 54.6 & 10.0 & 34.8 & 16.4 & 17.6 \\ \hline \multicolumn{7}{l}{Llama 2 Long Chat} & 52.9 & 81.8 & 76.0 & 47.3 & 25.5 & 66.7 \\ \hline \end{tabular}
\end{table}
표 17: L-Eval로부터의 추가적인 긴-컨텍스트 태스크에 대한 평가. 우리는 An et al. (2023)에 정의된 공식 메트릭을 보고하고 비교 모델의 결과는 논문에서 직접 토큰화된다.

도 7: 상이한 데이터 믹스로 트레이닝된 모델의 첫 번째 문장-검색 성능.

이것은 확장된 문맥을 갖는 모델에 대한 위치 임베딩을 선택하는 다목적적 결정으로 남는다. 한편으로 \(\hat{f}\)는 \(d(f,\hat{f}),\)를 최소화하도록 선택되어야 하고 다른 한편으로 \(q(\hat{f})\)의 값은 충분히 커야 한다.

우리가 이 다객관적 결정을 내리는 방법에 대한 설명을 진행하기에 앞서 여기서 고려되는 위치 임베딩에 대한 기하학적 직관을 제공하고자 한다. 매핑 \(\mathbb{R}^{d}\times\mathbb{N}\rightarrow\mathbb{C}^{d/2})을 시각화하는 것은 어렵지만 \(x\in\mathbb{R}^{d}\)을 고정하고 투영 \(\mathbb{R}\rightarrow\mathbb{R}^{3}\)을 시각화할 수 있다. PI와 ABF의 직관을 얻기 위해 \(\text{Re}\big{[}f^{RoPE}(x,t)_{0}\big{]}\), \(\text{Im}\big{[}f^{RoPE}(x,t)_{0}\big{]}\) 및 \(\text{Re}\big{[}f^{RoPE}(x,t)_{j}\big{]}에 의해 형성되는 나선을 고려하자.\) 그림 7(a)의 예는 시스템과 함께 제공된 검은색 나선 선을 보여준다.

\[x=\text{cos }t;y=\text{sin }t;z=\text{sin }at.\]

라인 상의 적색 도트들은 \(t\)의 \(11\) 정수 값들에 대응한다.

그림 7(b)는 매핑된 벡터의 상대적 위치에 대한 위치 보간의 영향을 설명하는 것을 목표로 한다. 연속된 점들 사이의 거리는 그림 7(a)로 상당히 줄어들었다. 조정 기준 주파수의 영향은 그림 7(c)에 나와 있다. 연속된 점 사이의 거리는 그림 7(a)와 거의 동일하게 유지되었지만 나선의 빈도가 증가하여 점 사이의 최소 거리가 상당히 감소했다. 나선의 증가된 주파수의 이러한 효과는 고차원 설정에서 감소될 것이다. 그림 7(a)에 표시된 나선의 계수 \(a\) 값은 그림 7(c)에 표시된 나선의 계수 \(a\) 값보다 2배 크다. 주의 메커니즘의 입력 차원이 \(d=128\)인 경우 \(b=10,000\)에서 \(\theta_{1}=b^{-\frac{2j}{d}}\)과 \(b=500,000\)에서 \(\theta_{1}=b^{-\frac{2j}{d}}\)의 차이는 \(6\%)에 불과하다. \ 따라서 우리는 특히 임베딩의 연속 이미지 사이의 거리에 더 중점을 둔다.

\(t\)의 연속된 정수 값에 대해 \(f^{RoPE+PI}\)과 \(f^{RoPE+ABF}\)에 의해 주어진 이미지 사이의 쌍별 거리를 해석적으로 비교하여 위치 보간과 조정 기준 주파수를 공식 비교한다. 이는 앞서 살펴본 \(q(\hat{f})\)의 평가에 해당한다. 우리는 RoPE의 모든 버전이 규범 보존이기 때문에 유클리드 사인 유사성 메트릭 측면에서 임베딩 이미지 간의 거리를 측정할 것이다.

\[\sin\angle(a,b)=\frac{Im\langle a,b\rangle}{\|a\|\|b\|}\]

다음의 결과는 고차원 공간에서 벡터 \(x\)의 연속된 두 임베딩 이미지 사이의 사인 유사성 \(\sin\angle(f^{RoPE+ABF}(x,n+1),f^{RoPE+ABF}(x,n))\)은 \((\log b+\log\beta)^{-1}에 비례하는 값으로 바운딩될 수 있음을 나타낸다.\) 또한, 유사도 \(\sin\angle(f^{RoPE+PI}(x,n+1),f^{RoPE+PI}(x,n))\)는 \(\alpha(\log b)^{-1}.\)을 이용하여 결합할 수 있다.

**정리 1**.: \(x\in\mathbb{R}^{d}\) 및 \(n\in\mathbb{N},\) 위치 임베딩의 두 연속 이미지 간의 유클리드 사인 유사성은 __로 바인딩될 수 있습니다.

\[\frac{\min_{k}x_{k}^{2}}{\|x\|^{2}}C_{d}\leq\sin\angle(f(x,n+1),f(x,n))\leq \frac{\max_{k}x_{k}^{2}}{\|x\|^{2}}C_{d}\]

도 8: 나선형으로서 RoPE 변이체 시각화.

\(\alpha\ll 1\)과 \(b\gg 1\)의 가정하에서 \(\lim_{d\rightarrow\infty}C_{d}\approx\begin{cases}(\log b+\log\beta)^{-1}\text{ if }f=f^{ RoPE+ABF}\\ \alpha(\log b)^{-1}\text{ if }f=f^{ RoPE+PI}\end{cases}\))._

증명: RoPE 변형의 두 이미지 사이에 내부 제품에 대 한 식을 기록 하 여 증명을 시작 합니다.

\[\langle f^{RoPE+PI}(x,m),f^{RoPE+PI}(x,n)\rangle=\sum_{j=0}^{\frac{d}{2}-1} \left(x_{2j}^{2}+x_{2j+1}^{2}\right)e^{ib^{-\frac{2j}{d}}\alpha(m-n)}\]

\[\langle f^{RoPE+ABF}(x,m),f^{RoPE+ABF}(x,n)\rangle=\sum_{j=0}^{\frac{d}{2}-1} \left(x_{2j}^{2}+x_{2j+1}^{2}\right)e^{ib^{-\frac{2j}{d}}\beta^{-\frac{2j}{d}} (m-n)}\]

이로부터 위치 임베딩의 이미지들 사이의 유클리드 사인 유사도에 대한 표현식을 유도할 수 있다:

\[\sin\angle(f^{RoPE+PI}(x,m),f^{RoPE+PI}(x,n))=\frac{\sum_{j=0}^{\frac{d}{2}-1} \left(x_{2j}^{2}+x_{2j+1}^{2}\right)\sin(b^{-\frac{2j}{d}}\alpha(m-n))}{\sum_ {j=0}^{d}x_{j}^{2}}\]

\[\sin\angle(f^{RoPE+ABF}(x,m),f^{RoPE+ABF}(x,n))=\frac{\sum_{j=0}^{\frac{d}{2}-1 }\left(x_{2j}^{2}+x_{2j+1}^{2}\right)\sin(b^{-\frac{2j}{d}}\beta^{-\frac{2j}{d }}(m-n))}{\sum_{j=0}^{d-1}x_{j}^{2}}\]

같은 벡터 \(x\)의 연속된 두 위치 임베딩 이미지 사이의 거리를 비교하기 위해 \(m=n+1\)을 넣자.

\[\|x\|^{2}\sin\angle(f^{RoPE+PI}(x,n+1),f^{RoPE+PI}(x,n))=\sum_{j=0}^{\frac{d}{ 2}-1}\left(x_{2j}^{2}+x_{2j+1}^{2}\right)\sin(b^{-\frac{2j}{d}}\alpha)\]

\[\|x\|^{2}\sin\angle(f^{RoPE+ABF}(x,n+1),f^{RoPE+ABF}(x,n))=\sum_{j=0}^{\frac{d }{2}-1}\left(x_{2j}^{2}+x_{2j+1}^{2}\right)\sin(b^{-\frac{2j}{d}}\beta^{-\frac {2j}{d}}]\]

일반적으로 고려되는 \(b,\alpha\)와 \(\beta\)의 범위 때문에 sine 함수의 인수를 \(0<\alpha b^{-\frac{2j}{d}}\leq 1\)과 \(0<(\beta b^{-\frac{2j}{d}}\leq 1\)으로 묶을 수 있다. 이를 이용하여 \(j\in\{1,\ldots d\}\)에 대한 \(x_{j}^{2}\) 뿐만 아니라 \(b^{-\frac{2j}{d}}\beta^{-\frac{2j}{d}})\)과 \(b^{-\frac{2j}{d}}\alpha)\)이 비음수임을 유도한다. 따라서, 다음과 같은 불평등이 성립한다:

\[\sum_{j=0}^{\frac{d}{2}-1}\min_{k}x_{k}^{2}\sin(b^{-\frac{2j}{d}}\beta^{-\frac{2j}{d}})\leq\sum_{j=0}^{\frac{d}{2}-1}\left(x_{2j}^{2}+x_{2j+1}^{2}\right) \sin(b^{-\frac{2j}{d}}\beta^{-\frac{2j}{d}})\leq\sum_{j=0}^{\frac{d}{2}-1} \max_{k}x_{k}^{2}\sin(b^{-\frac{2j}{d}}\beta^{-\frac{2j}{d}}),\]

\[\sum_{j=0}^{\frac{d}{2}-1}\min_{k}x_{k}^{2}\sin(b^{-\frac{2j}{d}\alpha)\leq \sum_{j=0}^{\frac{d}{2}-1}\left(x_{2j}^{2}+x_{2j+1}^{2}\right)\sin(b^{-\frac{2 j}{d}\alpha)\leq\sum_{j=0}^{\frac{d}{2}-1}\max_{k}x_{k}^{2}\sin(b^{-\frac{2j}{d}\alpha)\]\]

합산 부호에서 \(\min_{k}x_{k}^{2}\)와 \(\max_{k}x_{k}^{2}\)을 나르는 것을 우리는 얻는다.

\[\min_{k}x_{k}^{2}\sum_{j=0}^{\frac{d}{2}-1}\sin(b^{-\frac{2j}{d}}\beta^{-\frac{2j}{d}})\leq\sum_{j=0}^{\frac{d}{2}-1}\left(x_{2j}^{2}+x_{2j+1}^{2}\right) \sin(b^{-\frac{2j}{d}}\beta^{-\frac{2j}{d}})\leq\max_{k}x_{k}^{2}\sum_{j=0}^{ \frac{d}{2}-1}\sin(b^{-\frac{2j}{d}}\beta^{-\frac{2j}{d}}),\]

\[\min_{k}x_{k}^{2}\sum_{j=0}^{\frac{d}{2}-1}\sin(b^{-\frac{2j}{d}\alpha)\leq \sum_{j=0}^{\frac{d}{2}-1}\left(x_{2j}^{2}+x_{2j+1}^{2}\right)\sin(b^{-\frac{2j}{d}\alpha)\leq\max_{k}x_{k}^{2}\sum_{j=0}^{\frac{d}{2}-1}\sin(b^{-\frac{2j}{d}\alpha)\]

\(C_{d}^{ABF}=\sum_{j=0}^{\frac{d}{2}-1}\sin(b^{-\frac{2j}{d}}\beta^{-\frac{2j}{ d}})\)과 \(C_{d}^{PI}=\sum_{j=0}^{\frac{d}{2}-1}\sin(b^{-\frac{2j}{d}}\alpha)\)을 도입함으로써 정리의 첫 번째 부분을 증명한다.

\[\frac{\min_{k}x_{k}^{2}}{\|x\|^{2}}C_{d}^{ABF}\leq\sin\angle(f^{RoPE+ABF}(x,n+1),f^{RoPE+ABF}(x,n))\leq\frac{\max_{k}x_{k}^{2}}{\|x\|^{2}}C_{d}^{ABF},\]

\[\frac{\min_{k}x_{k}^{2}}{\|x\|^{2}}C_{d}^{PI}\leq\sin\angle(f^{RoPE+PI}(x,n+1),f^{RoPE+PI}(x,n))\leq\frac{\max_{k}x_{k}^{2}}{\|x\|^{2}}C_{d}^{PI}.\] 이제, \(C_{d}\)의 극한을 고려하면, 우리는 사인들의 논수에 대한 부등식으로 인해 다음과 같은 한계들이 성립한다는 것을 알 수 있다.

\[(b\beta)^{-\frac{2j}{d}}\left(1-(b\beta)^{-\frac{2j}{d}}/\pi\right)\leq\sin(b^{- \frac{2j}{d}}\beta^{-\frac{2j}{d}})\leq(b\beta)^{-\frac{2j}{d}},\]

\[\alpha b^{-\frac{2j}{d}}\left(1-\alpha b^{-\frac{2j}{d}}/\pi\right)\leq\sin(b^{ -\frac{2j}{d}}\alpha)\leq\alpha b^{-\frac{2j}{d}}\]

기하학적 합과 지수(두 번째) 기본 극한의 곱을 이용하여, 우리는 이 한계들의 합들의 극한을 \(d\to\infty\):

\[\sum_{j=0}^{\frac{d}{2}-1}\alpha b^{-\frac{2j}{d}}=\frac{\alpha(b-1)b^{2/d}}{b^ {2/d+1}-b}\to\alpha\frac{b-1}{b\log b}\text{ as }d\to\infty\]

\[\sum_{j=0}^{\frac{d}{2}-1}\alpha^{2}b^{-\frac{4j}{d}}=\frac{\alpha^{2}(b^{2}-1)b^{4/d}}{b^{4/d+2}-b^{2}}\to\alpha^{2}\frac{b^{2}-1}{b^{2}\log b}\text{ as }d\to\infty\}

\[\sum_{j=0}^{\frac{d}{2}-1}(b\beta)^{-\frac{2j}{d}}=\frac{(b\beta-1)(b\beta)^{ 2/d}}{(b\beta)^{2/d+1}-b\beta}\to\frac{(b\beta)-1}{(b\beta)\log(b\beta)}\text{ as }d\to\infty\]

\[\sum_{j=0}^{\frac{d}{2}-1}(b\beta)^{-\frac{4j}{d}}=\frac{(b^{2}\beta^{2}-1)(b \beta)^{4/d}}{(b\beta)^{4/d+2}-b^{2}\beta^{2}}\to\frac{(b\beta)^{2}-1}{(b\beta)^{2}\log(b\beta)}\text{ as }d\to\infty\]

이를 \(\lim_{d\to\infty}C_{d}\)의 경계에 대입하면 다음과 같은 결과를 얻을 수 있다.

\[(\log b+\log\beta)^{-1}\left(\frac{(b\beta)-1}{(b\beta)}-\frac{(b\beta)^{2}- 1}{\pi(b\beta)^{2}}\right)\leq\lim_{d\to\infty}C_{d}^{ABF}\leq(\log b+\log \beta)^{-1}\frac{(b\beta)-1}{(b\beta)},\]

\[\alpha(\log b)^{-1}\left(\frac{b-1}{b}-\frac{\alpha}{\pi}\frac{b^{2}-1}{b^{2} }\right)\leq\lim_{d\to\infty}C_{d}^{PI}\leq\alpha(\log b)^{-1}\frac{b-1}{b}\]

이러한 한계로부터, 이 논문에서 고려된 설정에서, \(b=10000\)과 \(\alpha<1/4\)에서, 정리의 진술에서 사용된 \(\lim_{d\to\infty}C_{d}\)의 근사치는 고품질임을 알 수 있다.

이러한 이론적 도출을 바탕으로 실험 결과의 해석으로 돌아간다. 한편, 이 모델은 RoPE PI (\(\alpha=1/4\) 또는 \(\alpha=1/8\))와 RoPE ABF (\(\beta=50\))를 사용하여 새로운 시퀀스 길이에 적응할 수 있음을 보여주었다. 따라서 선택된 하이퍼파라미터는 \(b=10000\)에서 RoPE 이미지의 충분한 근사도를 제공한다고 결론지을 수 있다. 즉, \(d(f,f^{RoPE+ABF})\)와 \(d(f,f^{RoPE+PI})\) 모두 빠른 적응을 할 수 있을 만큼 작다. 한편, RoPE ABF와 RoPE PI에 대한 \(C_{d}\)의 표현식을 비교한 결과, 실험에 사용된 \(\alpha=\frac{1}{4}\) 또는 \(\alpha=\frac{1}{8}\)과 \(b=10000\)의 값에서 RoPE의 두 연속 이미지 사이의 거리(granularity)는 RoPE ABF(\((\log b+\log\beta)^{-1}\approx 0.076\))보다 RoPE PI(\(\alpha(\log b)^{-1}\approx 0.027\))가 \(\beta=50\)보다 훨씬 낮다. 우리는 또한 RoPE PI에 비해 RoPE ABF 변형의 다운스트림 작업에 대한 더 높은 수준의 입도가 모델에 대한 위치 삽입 이미지를 구별하는 작업을 더 간단하게 만들기 때문에 RoPE ABF 변형의 다운스트림 작업에 대한 더 높은 평가와 관련이 있다고 가정한다. 즉, \(q(f^{RoPE+ABF})>q(f^{RoPE+PI})\)의 경우에 해당한다.

이러한 고려를 통해 우리는 임베딩의 연속된 이미지 사이의 거리가 다른 이미지 쌍 사이의 거리보다 작다고 암시적으로 가정했다. 이 가정은 고차원 공간에서 참일 가능성이 높지만 RoPE ABF에서 \(\beta\)의 매개변수를 크게 증가시키면 임베딩 곡선의 변경된 기하학으로 인해 이 가정을 위반할 수 있다.

## 부록 C 길이 외삽 결과

이 작업의 초점이 아님에도 불구하고 외삽은 긴 컨텍스트 모델에 중요한 속성이다. 외삽은 학습 시퀀스보다 긴 입력 시퀀스에 대한 추론을 수행하는 모델의 능력을 의미한다. 70B 모델이 두 가지 작업을 통해 어떻게 외삽되는지 평가합니다.

* **각 위치에서의 유효성 검사 손실**: 그림 8(a)에서 처음 16,384가 보간 영역(훈련 시퀀스 길이 내)이고 후반부가 외삽인 32,768 시퀀스 길이의 각 위치에서의 평균 손실을 시각화합니다. 샘플의 50개 배치와 평균을 사용합니다. 플롯을 더 부드럽게 만들기 위해 500개 위치마다 손실 평균을 취합니다. 우리가 볼 수 있듯이 RoPE ABF 또는 xPos ABF가 있는 70B 모델은 외삽 영역에서 손실을 유지한다. 이를 대조하기 위해 4,096 컨텍스트 창을 사용하여 Llama 2에 대한 결과를 플로팅하는데, 위치가 훈련 시퀀스 길이를 넘어선 후 손실이 폭발하며, 이는 Llama 2가 효과적으로 외삽되지 않음을 시사한다.
* **합성 첫 번째 문장 검색 작업**: 유효성 검사 손실 평가를 보완하기 위해 컨텍스트 프로빙 작업에서 두 개의 다른 PE를 사용 하 여 70B 모델을 테스트 합니다. 매우 긴 범위의 종속성을 일관되게 요구하는 데이터 샘플을 찾기 어려운 검증 손실 작업과 달리, 첫 번째 문장 검색은 모델이 특정 길이로 참석하도록 매우 엄격한 요구 사항을 부과한다. 그림 8(b)에서 모델이 외삽해야 할 때 일부 성능 저하를 볼 수 있는 최대 32,768의 결과를 시각화한다. 또한, 종종 더 나은 외삽 특성을 갖는 것으로 간주됨에도 불구하고 xPos ABF가 설정에서 RoPE ABF를 능가하지 않는다는 것을 관찰한다.

## Appendix D Self-Instruct Data

섹션 4.3에서 설명한 대로 Llama 2 Chat을 사용하여 피니튜닝을 지시하기 위해 자체 지시 데이터를 부트스트랩한다. 이 섹션에서는 이 데이터 세트를 생성하는 데 사용되는 필요한 프롬프트를 제공할 뿐만 아니라 자세한 절차를 설명한다. 주요 과제는 짧은 컨텍스트 모델만으로 긴 컨텍스트 지시 데이터를 생성하는 자동화된 프로세스가 필요하다는 것이다. 이를 뒷받침하는 핵심 아이디어는 긴 문서를 짧은 모델의 맥락에 맞는 텍스트 덩어리로 분할하고 자체 지도를 적용하는 것이다. 우리는 주로 질문 응답 데이터 세트에 중점을 둡니다. 우리는 먼저 긴 문서를 더 작은 덩어리로 나누고 각 덩어리에 대해 그림 10과 같은 프롬프트를 구성하여 라마 2 채팅에 입력하여 질문-답변 쌍을 얻는다. 질문 유형을 다양화하기 위해 정상 또는 단답형 중 하나를 묻는 두 프롬프트 중에서 무작위로 선택한다. 응답으로부터 질의응답을 추출하면(프롬프트에서 요구하는 대로 태그를 사용), 해당 답변 유형의 그림 11의 템플릿을 사용하여 원래의 긴 문서와 함께 긴 질의응답 지시 데이터를 구성할 수 있다.

그림 9: 70B 모델의 외삽 능력에 대한 평가입니다.

**Normal Answer Prompt:**

[INST] 긴 텍스트에서 가져온 텍스트 청크(삼중 따옴표로 구분됨)가 제공됩니다. 이 텍스트에 대한 질문을 작성하고 정답을 제시하세요. 답은 텍스트를 기반으로 해야 합니다. 이 문항은 추후 전체 문서에 대한 읽기 이해력 검사로 활용될 것이다. 질의응답을 XML 태그(<질문> 및 </질문>, <답변> 및 </답변>)를 이용하여 싸서 ""

{TEXT_CHUNK}

"""

[/INST]

**Short Answer Prompt:**

[INST] 긴 문서에서 텍스트 청크(삼중 따옴표로 구분됨)가 제공됩니다. 텍스트의 정보를 기반으로 특정 질문을 작성합니다.

**몇 단어 또는 단일 구로 응답할 수 있습니다* * 설명 없이 정답을 제공합니다. 답은 텍스트를 기반으로 해야 합니다. 이 문항은 추후 전체 문서에 대한 읽기 이해력 검사로 활용될 것이다. XML 태그(<질문> 및 </질문>, <답변> 및 </답변>)를 이용하여 질의응답을 쌌다. 다시 말하지만, 답은 짧아야 한다.

"""

{TEXT_CHUNK}

"""

[/INST]

**일반 답변 데이터 템플릿:**

[INST] 긴 텍스트(세 개의 따옴표로 구분됨)와 질문이 제공됩니다. 텍스트를 읽고 마지막에 질문에 답하세요.

"""

{FULL_DOCUMENT}

"""

Question: {QUESTION}

[/INST]

{ANSWER}

**단답형 데이터 템플릿:**

[INST] 긴 텍스트(세 개의 따옴표로 구분됨)와 질문이 제공됩니다. 텍스트를 읽고 가능한 한 한 개의 구나 문장을 사용하여 가능한 한 간결하게 질문에 답하세요. 설명을 하지 마십시오.

"""

{FULL_DOCUMENT}

"""

Question: {QUESTION}

[/INST]

{ANSWER}

도 11: 긴 질의응답 데이터를 구성하기 위한 데이터 템플릿. 질문과 답변 쌍은 라마2챗의 응답으로부터 추출된다.

도 10: Llama 2 Chat을 부스트랩하여 질문 및 답변 쌍을 생성하는 데 사용되는 프롬프트. 우리는 긴 문서를 청크로 나누고 각 청크를 동일한 확률로 프롬프트 중 하나에 공급한다. 이 모델은 XML 태그로 답변을 감싸도록 유도하여 보다 정확한 답변 추출이 가능하다.
