{
    "2309.16039": {
        "paper_id": "2309.16039",
        "abs_url": "https://arxiv.org/abs/2309.16039",
        "pdf_url": "https://arxiv.org/pdf/2309.16039.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2309.16039_Effective_Long-Context_Scaling_of_Foundation_Models.pdf",
        "title": "Effective Long-Context Scaling of Foundation Models",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Wenhan Xiong",
            "Jingyu Liu",
            "Igor Molybog",
            "Hejia Zhang",
            "Prajjwal Bhargava",
            "Rui Hou",
            "Louis Martin",
            "Rashi Rungta",
            "Karthik Abinav Sankararaman",
            "Barlas Oguz",
            "Madian Khabsa",
            "Han Fang",
            "Yashar Mehdad",
            "Sharan Narang",
            "Kshitiz Malik",
            "Angela Fan",
            "Shruti Bhosale",
            "Sergey Edunov",
            "Mike Lewis",
            "Sinong Wang",
            "Hao Ma"
        ],
        "abstract": "We present a series of long-context LLMs that support effective context windows of up to 32,768 tokens. Our model series are built through continual pretraining from Llama 2 with longer training sequences and on a dataset where long texts are upsampled. We perform extensive evaluation on language modeling, synthetic context probing tasks, and a wide range of research benchmarks. On research benchmarks, our models achieve consistent improvements on most regular tasks and significant improvements on long-context tasks over Llama 2. Notably, with a cost-effective instruction tuning procedure that does not require human-annotated long instruction data, the 70B variant can already surpass gpt-3.5-turbo-16k's overall performance on a suite of long-context tasks. Alongside these results, we provide an in-depth analysis on the individual components of our method. We delve into Llama's position encodings and discuss its limitation in modeling long dependencies. We also examine the impact of various design choices in the pretraining process, including the data mix and the training curriculum of sequence lengths -- our ablation experiments suggest that having abundant long texts in the pretrain dataset is not the key to achieving strong performance, and we empirically verify that long context continual pretraining is more efficient and similarly effective compared to pretraining from scratch with long sequences.",
        "comments": "",
        "official_code_urls": [],
        "pwc_page_url": "https://paperswithcode.com/paper/effective-long-context-scaling-of-foundation",
        "bibtex": "@misc{xiong2023effective,\n      title={Effective Long-Context Scaling of Foundation Models}, \n      author={Wenhan Xiong and Jingyu Liu and Igor Molybog and Hejia Zhang and Prajjwal Bhargava and Rui Hou and Louis Martin and Rashi Rungta and Karthik Abinav Sankararaman and Barlas Oguz and Madian Khabsa and Han Fang and Yashar Mehdad and Sharan Narang and Kshitiz Malik and Angela Fan and Shruti Bhosale and Sergey Edunov and Mike Lewis and Sinong Wang and Hao Ma},\n      year={2023},\n      eprint={2309.16039},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
    }
}