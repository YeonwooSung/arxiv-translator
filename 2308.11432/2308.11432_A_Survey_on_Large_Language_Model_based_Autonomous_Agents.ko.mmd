# 대규모 언어 모델 기반 자율 에이전트 조사

레이왕, 천마, 서양풍, 제유장, 하오양, 징센장, 지위안천, 자카이탕, 쉬첸, 옌카이린, 웨인신자오, 제웨이웨이, 지롱원

###### Abstract

자율 에이전트는 오랫동안 학문 및 산업 커뮤니티 모두에서 두드러진 연구 초점이었다. 이 분야의 이전 연구는 종종 고립된 환경 내에서 제한된 지식을 가진 에이전트 훈련에 초점을 맞추며, 이는 인간의 학습 프로세스와 크게 다르므로 에이전트가 인간과 유사한 결정을 내리기 어렵게 만든다. 최근 방대한 양의 웹 지식 획득을 통해 대규모 언어 모델(LLM)은 인간 수준의 지능을 달성할 수 있는 놀라운 잠재력을 보여주었다. 이는 LLM 기반 자율 에이전트를 조사하는 연구의 증가를 촉발했다. 본 논문에서는 이러한 연구에 대한 포괄적인 조사를 제시하여 LLM 기반 자율 에이전트 분야에 대한 체계적인 검토를 전체론적 관점에서 제공한다. 보다 구체적으로, 먼저 LLM 기반 자율 에이전트의 구성에 대해 논의하며, 이를 위해 이전 작업의 대부분을 포괄하는 통합 프레임워크를 제안한다. 그런 다음 사회 과학, 자연 과학 및 공학 분야에서 LLM 기반 자율 에이전트의 다양한 응용 프로그램에 대한 포괄적인 개요를 제시한다. 마지막으로, 우리는 LLM 기반 자율 에이전트에 일반적으로 사용되는 평가 전략을 조사한다. 선행 연구를 바탕으로 이 분야의 몇 가지 도전과 향후 방향도 제시한다. 이 필드를 추적 하 고 조사를 지속적으로 업데이트 하기 위해 [https://github.com/Paitesanshi/LLM-Agent-Survey](https://github.com/Paitesanshi/LLM-Agent-Survey)에서 관련 참조 리포지토리를 유지 합니다.

## 1 Introduction

_"자율 에이전트는 해당 환경을 감지 하 고 시간이 지남에 따라 자체 의제를 추구 하 고 향후에 감지 하는 것에 영향을 미치도록 해당 환경에 작용 하는 환경의 내부 및 일부에 위치 하는 시스템입니다._

Franklin and Graesser (1997)

자율 에이전트는 자기 주도적인 계획과 행동을 통해 임무를 수행할 것으로 예상되는 인공지능(AGI)을 달성하기 위한 유망한 접근법으로 오랫동안 인식되어 왔다. 이전 연구에서 에이전트는 단순하고 휴리스틱한 정책 기능을 기반으로 작용하는 것으로 가정되며 고립되고 제한된 환경에서 학습된다[113; 96; 134; 60; 11; 127]. 이러한 가정은 인간의 마음이 매우 복잡하고 개인이 훨씬 더 다양한 환경에서 학습할 수 있기 때문에 인간의 학습 과정과 크게 다르다. 이러한 격차 때문에 이전 연구에서 얻은 에이전트는 일반적으로 인간 수준의 결정 프로세스를 복제하는 것과는 거리가 멀며, 특히 구속되지 않은 개방형 도메인 설정에서 그렇다.

최근 몇 년 동안, 대형 언어 모델(LLM)은 주목할 만한 성공을 거두어 인간과 유사한 지능을 달성하는 데 상당한 잠재력을 보여주었다[120, 127, 11, 4, 146, 147]. 이 기능은 상당한 수의 모델 매개변수와 함께 포괄적인 훈련 데이터 세트를 활용함으로써 발생한다. 이러한 능력을 기반으로 인간과 유사한 의사 결정 능력을 얻기 위해 자율 에이전트를 구성하기 위해 중앙 컨트롤러로 LLM을 사용하는 연구 영역이 증가하고 있다[21, 139, 138, 126, 133, 184, 136]. 이러한 방향을 따라 연구자들은 LLM에 메모리와 같은 중요한 인간 능력을 갖추고 인간처럼 행동하고 다양한 작업을 효과적으로 완료하도록 계획하는 것이 핵심 아이디어인 수많은 유망한 모델(이 분야의 개요는 그림 1 참조)을 개발했다. 이전에는 이러한 모델을 독립적으로 제안했으며 이를 총체적으로 요약하고 비교하기 위해 제한된 노력을 기울였다. 그러나 빠르게 발전하는 이 분야에 대한 체계적인 요약은 이를 종합적으로 이해하고 향후 연구에 영감을 주는 데 큰 의미가 있다고 믿는다.

본 논문에서는 LLM 기반 자율 에이전트 분야에 대한 종합적인 조사를 수행한다. 구체적으로, LLM 기반 자율 에이전트의 구축, 적용 및 평가를 포함한 세 가지 측면을 기반으로 설문조사를 구성한다. 에이전트 구성을 위해, 우리는 (1) LLM을 더 잘 활용하기 위해 에이전트 아키텍처를 설계하는 방법과 (2) 서로 다른 작업을 완료할 수 있는 에이전트 능력을 고취하고 향상시키는 방법에 초점을 맞춘다. 직관적으로, 첫 번째 문제는 에이전트에 대한 하드웨어 기본을 구축하는 것을 목표로 하는 반면, 두 번째 문제는 에이전트에게 소프트웨어 자원을 제공하는 데 중점을 둔다. 첫 번째 문제는 대부분의 선행 연구를 포괄할 수 있는 통합 에이전트 프레임워크를 제시한다. 두 번째 문제에 대해 우리는 에이전트의 능력 획득을 위해 일반적으로 사용되는 전략에 대한 요약을 제공한다. 에이전트 구성에 대해 논의하는 것 외에도 사회 과학, 자연 과학 및 공학에서 LLM 기반 자율 에이전트의 적용에 대한 개요도 제공한다. 마지막으로, 우리는 주관적 전략과 객관적 전략 모두에 초점을 맞춘 LLM 기반 자율 에이전트를 평가하기 위한 전략을 조사한다.

요약하면, 이 조사는 LLM 기반 자율 에이전트 분야의 기존 연구에 대한 체계적인 검토를 수행하고 포괄적인 분류학을 설정한다. 에이전트 구축, 적용 및 평가의 세 가지 측면에 중점을 둡니다. 선행 연구를 바탕으로 이 분야의 다양한 과제를 파악하고 향후 발전 방향에 대해 논의한다. 이 필드는 아직 초기 단계에 있다고 생각하므로 리포지토리를 유지하여 [https://github.com/Pairesanshi/LLM-Agent-Survey](https://github.com/Pairesanshi/LLM-Agent-Survey)에서 진행 중인 연구를 추적합니다. 우리는 우리의 조사가 포괄적인 배경 지식을 가진 LLM 기반 자율 에이전트 분야에 새로운 사람을 제공하고 추가 획기적인 연구를 장려할 수 있을 것으로 기대한다.

그림 1: LLM 기반 자율 에이전트 분야의 성장 추이 그림. 2021년 1월부터 2023년 8월까지 발행된 논문의 누적 수를 제시한다. 다양한 에이전트 카테고리를 표현하기 위해 다양한 색상을 할당한다. 예를 들어, 게임 에이전트는 게임-플레이어를 시뮬레이션하는 것을 목표로 하는 반면, 툴 에이전트는 주로 툴 사용에 초점을 맞춘다. 각 기간에 대해 다양한 에이전트 범주가 있는 선별된 연구 목록을 제공한다.

LLM 기반 자율 에이전트 구축

LLM 기반 자율 에이전트는 LLM의 인간과 유사한 기능을 활용하여 다양한 작업을 효과적으로 수행할 것으로 예상된다. 이러한 목표를 달성하기 위해서는 (1) LLM을 더 잘 사용하도록 설계되어야 하는 아키텍처와 (2) 에이전트가 특정 작업을 수행하기 위한 능력을 획득하는 방법이라는 두 가지 중요한 측면이 있다. 아키텍처 디자인의 맥락에서 우리는 포괄적인 통합 프레임워크로 절정에 이르는 기존 연구의 체계적인 합성에 기여한다. 두 번째 측면은 LLM을 미세 조정하는지 여부에 따라 에이전트 능력 획득 전략을 요약한다. LLM 기반 자율 에이전트를 전통적인 기계 학습과 비교할 때, 에이전트 아키텍처를 설계하는 것은 네트워크 구조를 결정하는 것과 유사하지만, 에이전트 능력 획득은 네트워크 파라미터를 학습하는 것과 유사하다. 이하에서는 이 두 가지 측면을 좀 더 구체적으로 소개한다.

각주 *: 우리의 프레임워크는 [https://lilianweng.github.io/posts/2023-06-23-agent/](https://lilianweng.github.io/posts/2023-06-23-agent/)의 선구자 작업에서도 영감을 받았습니다.

에이전트 아키텍처 설계

LLM의 최근 발전은 질문 응답(QA)의 형태로 광범위한 작업을 수행할 수 있는 큰 잠재력을 보여주었다. 그러나 자율 에이전트를 구축하는 것은 특정 역할을 수행하고 인간처럼 스스로 진화하기 위해 환경에서 자율적으로 인식하고 학습해야 하기 때문에 QA와는 거리가 멀다. 전통적인 LLM과 자율 에이전트 사이의 격차를 해소하기 위해 중요한 측면은 LLM의 능력을 최대화하는 데 도움이 되는 합리적인 에이전트 아키텍처를 설계하는 것이다. 이러한 방향을 따라 이전 연구에서는 LLM을 향상시키기 위한 여러 모듈을 개발했다. 이 섹션에서는 이러한 모듈을 요약하기 위한 통합 프레임워크를 제안한다. 특히, 우리의 프레임워크의 전체 구조는 프로파일링 모듈, 메모리 모듈, 플래닝 모듈 및 액션 모듈로 구성된 그림 2에 나와 있다. 프로파일링 모듈의 목적은 에이전트의 역할을 식별하는 것이다. 메모리 및 계획 모듈은 에이전트를 동적 환경에 배치하여 과거의 행동을 회상하고 미래의 행동을 계획할 수 있게 한다. 액션 모듈은 에이전트의 결정을 특정 출력으로 변환하는 역할을 합니다. 이러한 모듈 내에서 프로파일링 모듈은 메모리 및 계획 모듈에 영향을 미치고, 집합적으로 이 세 가지 모듈은 액션 모듈에 영향을 미친다. 이하에서, 이러한 모듈들에 대해 상세히 설명한다.

#### 2.1.1 Profiling Module

자율 에이전트는 전형적으로 코더, 교사 및 도메인 전문가와 같은 특정 역할을 가정함으로써 작업을 수행한다[124, 39]. 프로파일링 모듈은 일반적으로 LLM 동작에 영향을 주기 위해 프롬프트에 작성되는 에이전트 역할의 프로필을 나타내는 것을 목표로 한다. 에이전트 프로파일은 전형적으로 나이, 성별, 경력[121]과 같은 기본 정보뿐만 아니라 에이전트의 개성을 반영하는 심리 정보[149]와 에이전트 간의 관계를 상세히 설명하는 소셜 정보를 포함한다[149]. 에이전트를 프로파일링하기 위한 정보의 선택은 주로 특정 애플리케이션 시나리오에 의해 결정된다. 예를 들어, 응용 프로그램이 인간의 인지 과정을 연구하는 것을 목표로 한다면 심리 정보는 중추적이 된다. 프로파일 정보의 타입을 식별한 후, 다음 중요한 문제는 에이전트에 대한 특정 프로파일을 생성하는 것이다. 기존 문헌들은 공통적으로 다음의 세 가지 전략을 채택하고 있다.

**수공예 방법**: 이 메서드에서 에이전트 프로필은 수동으로 지정됩니다. 예를 들어, 성격이 다른 에이전트를 디자인하고 싶다면 "외향적인 사람" 또는 "내향적인 사람"을 사용하여 에이전트를 프로파일링할 수 있습니다. 수공예 방법은 에이전트 프로파일을 나타내기 위해 많은 이전 작업에서 활용되었다. 예를 들어, 생성 에이전트[176]는 이름, 목적, 및 다른 에이전트와의 관계와 같은 정보에 의해 에이전트를 설명한다. MetaGPT[64], ChatDev[124] 및 Self-collaboration[33]은 소프트웨어 개발에서 다양한 역할 및 그에 대응하는 책임을 미리 정의하며, 협업을 용이하게 하기 위해 각각의 에이전트에 별개의 프로파일을 수동으로 할당한다. PTLLM [131]은 LLM이 생성한 텍스트에 나타난 성격 특성을 탐색하고 정량화하는 것을 목표로 한다. 이 방법은 IPIP-NEO[77] 및 BFI[76]과 같은 성격 평가 도구를 사용하여 다양한 에이전트 캐릭터를 남성적으로 정의함으로써 LLMs가 다양한 응답을 생성하도록 안내한다. [31] 정치인, 언론인 및 비즈니스 인력과 같은 역할이 다른 LLM을 수동으로 유도하여 LLM 출력의 독성을 연구한다. 일반적으로, 핸드크래프팅 방법은 에이전트들에 임의의 프로파일 정보를 할당할 수 있기 때문에 매우 유연하다. 그러나 특히 많은 수의 에이전트를 처리할 때 노동 집약적일 수도 있다.

LLM 생성 방법**: 이 방법에서는 LLM을 기반으로 에이전트 프로필이 자동으로 생성됩니다. 일반적으로 프로파일 생성 규칙을 나타내는 것으로 시작하여 대상 모집단 내에서 에이전트 프로파일의 구성과 속성을 설명한다. 그런 다음, 몇 개의 샷 예제로 기능하도록 여러 시드제 프로파일을 선택적으로 지정할 수 있다. 마지막으로 LLM은 모든 에이전트 프로필을 생성하기 위해 활용된다. 예를 들어 RecAgent [150]은 먼저 나이, 성별, 개인 특성 및 영화 선호도와 같은 배경을 수동으로 만들어 소수의 에이전트에 대한 시드 프로필을 만듭니다. 그런 다음 ChatGPT를 활용하여 시드 정보를 기반으로 더 많은 에이전트 프로필을 생성한다. LLM 생성 방법은 에이전트의 수가 많을 때 상당한 시간을 절약할 수 있지만 생성된 프로파일에 대한 정확한 제어가 부족할 수 있다.

**데이터 세트 정렬 방법**: 이 메서드에서 에이전트 프로필은 실제 데이터 세트에서 가져옵니다. 일반적으로 데이터 세트의 실제 인간에 대한 정보를 자연어 프롬프트로 구성한 다음 에이전트를 프로파일링하는 데 활용할 수 있습니다. 예를 들어, [5]에서 저자는 미국 중앙선거연구(ANES) 참가자의 인구통계학적 배경(인종/민족, 성별, 연령 및 거주 상태)에 따라 GPT-3에 역할을 할당한다. 그들은 이후에 GPT-3가 실제 인간과 유사한 결과를 생성할 수 있는지 여부를 조사한다. 데이터세트 정렬 방법은 실제 모집단의 속성을 정확하게 캡처하여 에이전트 동작을 실제 시나리오의 보다 의미 있고 반영하게 한다.

_Remark_.: 이전 작업의 대부분은 위의 프로필 생성 전략을 독립적으로 활용하지만, 이를 결합하면 추가 이점이 발생할 수 있다고 주장합니다. 예를 들어, 에이전트 시뮬레이션을 통해 사회 발전을 예측하기 위해 실제 데이터 세트를 활용하여 에이전트의 하위 집합을 프로파일링하여 현재 사회 상태를 정확하게 반영할 수 있다. 이어서 현실 세계에는 존재하지 않지만 미래에 출현할 수 있는 역할을 다른 에이전트에 수동으로 할당할 수 있어 미래의 사회 발전을 예측할 수 있다. 프로파일 모듈은 에이전트 설계의 기초가 되며, 에이전트 암기, 계획 및 행동 절차에 상당한 영향을 미친다.

#### 2.1.2 메모리 모듈

메모리 모듈은 에이전트 아키텍처 설계에서 매우 중요한 역할을 한다. 환경으로부터 인식되는 정보를 저장하고 기록된 기억을 활용하여 미래의 행동을 용이하게 한다. 메모리 모듈은 에이전트가 경험을 축적하고, 자기 진화하며, 보다 일관되고 합리적이며 효과적인 방식으로 행동하도록 도울 수 있다. 이 섹션은 메모리 모듈의 구조, 포맷 및 동작에 초점을 맞추어 메모리 모듈에 대한 포괄적인 개요를 제공한다.

**메모리 구조**: LLM 기반 자율 에이전트는 일반적으로 인간의 기억 프로세스에 대한 인지 과학 연구에서 파생된 원리 및 메커니즘을 통합합니다. 인간의 기억은 지각적 입력을 등록하는 감각 기억에서 일시적으로 정보를 유지하는 단기 기억, 장기간에 걸쳐 정보를 통합하는 장기 기억으로의 일반적인 진행을 따른다. 에이전트 기억 구조를 설계할 때 연구자들은 인간 기억의 이러한 측면에서 영감을 얻는다. 특히, 단기 기억은 내부에서의 입력 정보와 유사하다

그림 2: LLM 기반 자율 에이전트의 아키텍처 설계를 위한 통합 프레임워크

트랜스포머 아키텍처에 의해 구속된 컨텍스트 창입니다. 장기 메모리는 에이전트가 필요에 따라 신속하게 쿼리하고 검색할 수 있는 외부 벡터 저장소와 유사합니다. 이하에서는 장단기 기억을 기반으로 일반적으로 사용되는 두 가지 메모리 구조를 소개한다.

\(\bullet\)_Unified Memory_. 이 구조는 일반적으로 문맥 내 학습에 의해 실현되는 인간 샷-항 기억을 시뮬레이션할 뿐이며, 기억 정보는 프롬프트에 직접 기록된다. 예를 들어, RLP[54]는 대화 에이전트로서, 화자와 청취자에 대한 내부 상태를 유지한다. 대화의 각 라운드 동안 이러한 상태는 LLM 프롬프트 역할을 하며 에이전트의 단기 기억으로 기능한다. SayPlan[129]는 태스크 계획을 위해 특별히 설계된 구체화된 에이전트이다. 이 에이전트에서 장면 그래프와 환경 피드백은 에이전트의 단기 기억 역할을 하여 에이전트의 동작을 안내한다. CALYPSO[183]는 던전 앤 드래곤 게임을 위해 설계된 에이전트로, 던전 마스터스가 이야기의 창작과 내레이션을 도울 수 있다. 단기 기억은 장면 설명, 몬스터 정보 및 이전 요약을 기반으로 합니다. DEPS [154]도 게임 에이전트이지만 마인크래프트용으로 개발됐다. 에이전트는 처음에 작업 계획을 생성한 다음 이를 활용하여 LLM을 프롬프트하고, 이는 차례로 작업을 완료하기 위한 작업을 생성한다. 이러한 계획은 에이전트의 단기 기억으로 간주할 수 있습니다. 실제로 단기 기억을 구현하는 것은 간단하며 최근 또는 맥락적으로 민감한 행동과 관찰을 인식하는 에이전트의 능력을 향상시킬 수 있다.

\(\bullet\)_Hybrid Memory_. 이 구조는 인간의 단기 기억과 장기 기억을 명시적으로 모델링한다. 단기 기억은 최근 인식을 일시적으로 완충하는 반면 장기 기억은 시간이 지남에 따라 중요한 정보를 통합한다. 예를 들어, 생성 에이전트[121]는 에이전트 동작을 용이하게 하기 위해 하이브리드 메모리 구조를 사용한다. 단기 기억은 에이전트의 현재 상황에 대한 컨텍스트 정보를 포함하고, 장기 기억은 에이전트의 과거 행동과 생각을 저장하며, 이는 현재 이벤트에 따라 검색될 수 있다. AgentSims[99]도 하이브리드 메모리 아키텍처를 구현한다. 프롬프트에서 제공되는 정보는 단기 기억으로 간주될 수 있다. 본 논문에서는 메모리의 저장 용량을 향상시키기 위해 벡터 데이터베이스를 활용하여 효율적인 저장 및 검색을 가능하게 하는 장기 메모리 시스템을 제안한다. 구체적으로 에이전트의 일일 메모리는 임베딩으로 인코딩되어 벡터 데이터베이스에 저장된다. 에이전트가 자신의 이전 메모리들을 리콜할 필요가 있는 경우, 장기 메모리 시스템은 임베딩 유사성들을 사용하여 관련 정보를 검색한다. 이러한 과정은 에이전트의 행동의 일관성을 향상시킬 수 있다. GITM[184]에서, 단기 메모리는 현재 궤적을 저장하고, 장기 메모리는 성공적인 이전 궤적들로부터 요약된 참조 계획들을 저장한다. 장기 기억은 안정적인 지식을 제공하는 반면 단기 기억은 유연한 계획을 가능하게 합니다. 반사[139]는 단기 슬라이딩 윈도우를 사용하여 최근 피드백을 캡처하고 응축된 통찰력을 유지하기 위해 지속적인 장기 스토리지를 통합한다. 이 조합은 상세한 즉각적인 경험과 높은 수준의 추상화를 모두 활용할 수 있게 한다. SCM[92]은 단기 기억과 결합하기 위해 가장 관련성이 높은 장기 지식을 선택적으로 활성화하여 복잡한 문맥 대화 상에서의 추론을 가능하게 한다. 간단히 검색[117]은 사용자 질의를 단기 기억으로 활용하고, 외부 지식 베이스를 이용하여 장기 기억을 저장한다. 이 디자인은 사용자 프라이버시를 보장하면서 모델 정확도를 향상시킵니다. MemorySandbox[72]는 메모리 객체를 저장하기 위해 2D 캔버스를 활용함으로써 장기 및 단기 기억을 구현하며, 이후 다양한 대화 전반에 걸쳐 액세스될 수 있다. 사용자는 동일한 캔버스 상에서 상이한 에이전트들과 다수의 대화를 생성할 수 있어, 간단한 드래그 앤 드롭 인터페이스를 통해 메모리 객체들의 공유를 용이하게 한다. 실제로, 단기 및 장기 기억을 모두 통합하는 것은 복잡한 환경에서 작업을 수행하는 데 중요한 장거리 추론 및 가치 있는 경험의 축적을 위한 에이전트의 능력을 향상시킬 수 있다.

_Remark_. 주의 깊은 판독기들은 또 다른 유형의 기억 구조, 즉 단지 장기 기억에 기초하여 존재할 수 있다는 것을 발견할 수 있다. 그러나 이러한 유형의 기억은 문헌에 거의 기록되지 않는다는 것을 발견했다. 우리의 추측은 에이전트가 항상 연속적이고 역동적인 환경에 위치하며 연속적인 행동이 높은 상관 관계를 나타낸다는 것이다. 따라서, 단기 기억의 포착은 매우 중요하며, 통상 무시할 수 없다.

**메모리 형식**: 메모리 구조 외에도 메모리 모듈을 분석하는 또 다른 관점은 메모리 저장 매체, 예를 들어 자연어 메모리 또는 내장 메모리의 형식을 기반으로 합니다. 서로 다른 메모리 형식은 고유한 강도를 가지고 있으며 다양한 응용 프로그램에 적합합니다. 이하에서는 대표적인 몇 가지 메모리 포맷을 소개한다.

\(\bullet\)_Natural Languages_. 이 형식에서는 에이전트 행동 및 관찰과 같은 메모리 정보가 원시 자연 언어를 사용하여 직접 설명된다. 이 형식은 몇 가지 장점을 가지고 있다. 첫째, 메모리 정보는 유연하고 이해할 수 있는 방식으로 표현될 수 있다. 또한 에이전트 행동을 안내하기 위해 포괄적인 신호를 제공할 수 있는 풍부한 의미 정보를 유지한다. 이전 작업에서, 반사[139]는 체험 피드백을 자연 언어로 슬라이딩 윈도우 내에 저장한다. 보이저[148]는 자연어 기술을 사용하여 마인크래프트 게임 내의 기술을 나타내며, 이는 메모리에 직접 저장된다.

\(\bullet\)_Embeddings_. 이러한 포맷에서, 메모리 정보는 임베딩 벡터들로 인코딩되어, 메모리 검색 및 판독 효율을 향상시킬 수 있다. 예를 들어, MemoryBank[179]는 각 메모리 세그먼트를 임베딩 벡터로 인코딩하고, 임베딩 벡터는 검색을 위해 인덱싱된 코퍼스를 생성한다. GITM[184]은 매칭 및 재사용을 용이하게 하기 위한 임베딩으로서 참조 계획들을 나타낸다. 나아가, ChatDev[124]는 대화 히스토리를 검색을 위해 벡터로 인코딩한다.

\(\bullet\)_Databases_. 이 형식에서는 메모리 정보가 데이터베이스에 저장되므로 에이전트가 메모리를 효율적이고 종합적으로 조작할 수 있다. 예를 들어, ChatDB[67]는 데이터베이스를 심볼릭 메모리 모듈로 사용한다. 에이전트는 SQL 문을 사용하여 메모리 정보를 정확하게 추가, 삭제 및 수정할 수 있습니다. DB-GPT[182]에서 메모리 모듈은 데이터베이스를 기반으로 구성된다. 보다 직관적으로 메모리 정보를 운영하기 위해 에이전트는 SQL 쿼리를 이해하고 실행할 수 있도록 미세 조정되어 자연어를 사용하여 데이터베이스와 직접 상호 작용할 수 있다.

\(\bullet\)_Structured Lists_. 이러한 포맷에서, 메모리 정보는 리스트들로 조직되고, 메모리의 시맨틱은 효율적이고 간결한 방식으로 전달될 수 있다. 예를 들어, GITM[184]은 하위 목표들에 대한 액션 리스트들을 계층적 트리 구조로 저장한다. 계층 구조는 목표와 해당 계획 간의 관계를 명시적으로 포착한다. RET-LLM[114]은 초기에 자연어 문장을 트리플렛 구절로 변환한 후, 이를 메모리에 저장한다.

_Remark_. 여기서 우리는 몇 가지 대표적인 메모리 포맷만을 보여주지만, [148]에 의해 사용되는 프로그래밍 코드와 같이 많은 언커버링된 포맷들이 있다는 것을 주목하는 것이 중요하다. 또한 이러한 형식은 상호 배타적이지 않다는 점을 강조해야 한다. 많은 모델은 각각의 이점을 동시에 활용하기 위해 여러 형식을 통합한다. 주목할 만한 예는 키-값 리스트 구조를 이용하는 GITM[184]의 메모리 모듈이다. 이 구조에서, 키들은 임베딩 벡터들로 표현되는 반면, 값들은 원시 자연 언어들로 구성된다. 임베딩 벡터들의 사용은 메모리 레코드들의 효율적인 검색을 가능하게 한다. 자연 언어를 활용함으로써 메모리 내용이 매우 포괄적이 되어 보다 정보에 입각한 에이전트 작업을 수행할 수 있습니다.

이상에서는 메모리 모듈의 내부 설계에 대해 주로 논의한다. 다음에서는 외부 환경과 상호 작용하는 메모리 작업에 초점을 둡니다.

**메모리 작업**: 메모리 모듈은 에이전트가 환경과 상호 작용 하 여 중요한 지식을 획득 하 고 축적 하 고 활용할 수 있도록 하는 데 중요한 역할을 합니다. 에이전트와 환경 사이의 상호 작용은 메모리 읽기, 메모리 쓰기, 메모리 반사의 세 가지 중요한 메모리 동작을 통해 이루어진다. 이하에서는 이러한 동작들을 보다 상세하게 소개한다.

\(\bullet\)_Memory Reading_. 메모리 판독의 목적은 에이전트의 동작을 향상시키기 위해 메모리로부터 의미 있는 정보를 추출하는 것이다. 예를 들어, 이전에 성공한 동작들을 사용하여 유사한 목표들을 달성한다[184]. 기억 읽기의 핵심은 가치 있는 정보를 추출하는 방법에 있다. 일반적으로 정보 추출을 위해 일반적으로 사용되는 세 가지 기준, 즉 최신성, 관련성 및 중요도가 있다[121]. 더 최근, 관련 있고 중요한 기억들이 추출될 가능성이 더 높다. 형식적으로, 메모리 정보 추출을 위한 기존 문헌으로부터 다음 식을 결론짓는다:

\[m^{*}=\arg\min_{m\in M}\alpha s^{rec}(q,m)+\beta s^{rel}(q,m)+\gamma s^{imp}(m), \tag{1}\]

여기서 \(q\)는 쿼리입니다. 예를 들어 에이전트가 해결해야 하는 작업 또는 에이전트가 위치한 컨텍스트입니다. \ (M\)는 모든 메모리의 집합입니다. \ (s^{rec}(\cdot)\), \(s^{rel}(\cdot)\) 및 \(s^{imp}(\cdot)\)는 기억의 최신성, 관련성 및 중요성을 측정하기 위한 점수 함수이다. 이러한 점수화 함수는 LSH, ANNOY, HNSW, FAISS 등을 기반으로 \(s^{rel}(q,m)\)을 구현할 수 있는 등 다양한 방법을 사용하여 구현될 수 있다. \(s^{imp}\)는 메모리 자체의 문자만 반영하므로 쿼리 \(q\)와 관련이 없습니다. \ (\alpha\), \(\beta\) 및 \(\gamma\)은 밸런싱 파라미터이다. 다른 값으로 할당함으로써, 다양한 메모리 판독 전략을 얻을 수 있다. 예를 들어, \(\alpha=\gamma=0\)을 설정함으로써, 많은 연구[114, 184, 148, 54]는 메모리 판독에 대한 관련성 점수 \(s^{rel}\)만을 고려한다. [121]은 \(\alpha=\beta=\gamma=1.0\)을 할당하여 위의 세 가지 메트릭 모두에 동일하게 가중치를 부여하여 메모리에서 정보를 추출한다.

\(\bullet\)_Memory Writing_. 메모리 쓰기의 목적은 인식된 환경에 대한 정보를 메모리에 저장하는 것이다. 중요한 정보를 메모리에 저장하는 것은 향후 정보 기억을 검색할 수 있는 기반을 제공하여 에이전트가 보다 효율적이고 합리적으로 행동할 수 있도록 한다. 메모리 쓰기 과정에서 주의 깊게 다루어야 할 두 가지 잠재적인 문제가 있다. 한편으로, 기존의 메모리(_i.e._, 메모리 중복)와 유사한 정보를 저장하는 방법을 다루는 것이 중요하다. 한편, 메모리가 자신의 저장 한계(_i.e._, 메모리 오버플로우)에 도달할 때 정보를 제거하는 방법을 고려하는 것이 중요하다. 이하에서는 이러한 문제점을 좀 더 구체적으로 논의한다. (1) _메모리 중복_. 유사한 정보를 통합하기 위해 사람들은 새로운 기록과 이전 기록을 통합하는 다양한 방법을 개발했다. 예를 들어, [120]에서, 동일한 하위 목표와 관련된 성공적인 액션 시퀀스들이 리스트에 저장된다. 리스트의 크기가 N(=5)에 도달하면 모든 시퀀스는 LLM을 사용하여 통합된 계획 솔루션으로 압축된다. 메모리의 원래 시퀀스는 새로 생성된 시퀀스로 대체된다. 증강 LLM[135]은 중복 저장을 피하면서 카운트 누적을 통해 중복 정보를 집계한다. (2) _메모리 오버플로우_. 사람이 만석일 때 메모리에 정보를 쓰기 위해 기존 정보를 삭제하는 방법을 다르게 설계해 암기 과정을 이어간다. 예를 들어, ChatDB[67]에서는 사용자 명령에 기초하여 메모리를 명시적으로 삭제할 수 있다. RET-LLM[114]은 메모리용으로 고정된 크기의 버퍼를 사용하여, 가장 오래된 엔트리들을 FIFO(first-in-first-out) 방식으로 덮어쓴다.

\(\bullet\)_Memory Reflection_. 기억 성찰은 인간이 자신의 인지적, 정서적, 행동적 과정을 목격하고 평가하는 능력을 모방한다. 에이전트에 적응될 때, 목표는 에이전트에게 보다 추상적이고 복잡하며 높은 수준의 정보를 독립적으로 요약하고 추론할 수 있는 능력을 제공하는 것이다. 보다 구체적으로, 생성 에이전트[121]에서, 에이전트는 메모리에 저장된 자신의 과거 경험을 더 광범위하고 추상적인 통찰로 요약할 수 있는 능력을 갖는다. 먼저 에이전트는 최근 기억을 기반으로 세 가지 주요 질문을 생성합니다. 그런 다음, 이러한 질문은 관련 정보를 얻기 위해 메모리를 쿼리하는 데 사용된다. 획득한 정보를 기반으로 에이전트는 에이전트 고급 아이디어를 반영하는 5가지 통찰력을 생성합니다. 예를 들어, 하위 레벨의 기억들 "클라우스 뮬러가 연구 논문을 쓰고 있다", "클라우스 뮬러가 사서와 그의 연구를 더 진행하기 위해 관여하고 있다", "클라우스 뮬러가 그의 연구에 대해 아예샤 칸과 대화하고 있다"는 상위 레벨의 통찰력 "클라우스 뮬러가 그의 연구에 전념하고 있다"를 유도할 수 있다. 또한, 반영 과정은 계층적으로 발생할 수 있으며, 이는 기존의 인사이트를 기반으로 인사이트를 생성할 수 있음을 의미한다. GITM[184]에서는 서브 목표들을 성공적으로 달성한 액션들이 리스트에 저장된다. 목록에 5개 이상의 요소가 포함되어 있을 때 에이전트는 이를 공통적이고 추상적인 패턴으로 요약하고 모든 요소를 대체한다. ExpeL[177]에서는 에이전트가 반사를 획득하기 위해 두 가지 접근법이 도입된다. 첫째, 에이전트는 동일한 작업 내에서 성공한 궤적과 실패한 궤적을 비교합니다. 둘째, 에이전트는 경험을 얻기 위해 성공적인 궤적 모음에서 학습합니다.

전통적인 LLM과 에이전트 사이의 중요한 차이점은 후자가 동적 환경에서 작업을 학습하고 완료할 수 있는 능력을 보유해야 한다는 것이다. 에이전트의 과거 행동을 관리하는 역할을 담당하는 메모리 모듈을 고려한다면, 에이전트의 미래 행동을 계획하는 데 도움을 줄 수 있는 또 다른 중요한 모듈을 갖는 것이 필수적이다. 다음에서는 연구자가 계획 모듈을 설계하는 방법에 대한 개요를 제시한다.

#### 2.1.3 Planning Module

복잡한 과제에 직면했을 때 인간은 그것을 더 단순한 하위 과제로 해체하고 개별적으로 해결하는 경향이 있다. 계획 모듈은 이러한 인간 능력을 가진 에이전트에게 권한을 부여하는 것을 목표로 하며, 이는 에이전트가 보다 합리적이고 강력하며 안정적으로 행동하도록 할 것으로 예상된다. 특히 에이전트가 계획 과정에서 피드백을 받을 수 있는지 여부에 따라 기존 연구를 요약하며, 이를 구체적으로 살펴보면 다음과 같다.

**피드백 없이 계획**: 이 메서드에서 에이전트는 작업을 수행한 후 향후 동작에 영향을 줄 수 있는 피드백을 받지 않습니다. 다음에서는 몇 가지 대표적인 전략을 제시한다.

\(\bullet\)_Single-path Reasoning_. 이 전략에서 최종 과제는 여러 중간 단계로 분해된다. 이러한 단계들은 계단식으로 연결되며, 각 단계는 단지 하나의 후속 단계로 이어진다. LLM은 최종 목표를 달성하기 위해 다음 단계를 수행합니다. 구체적으로, CoT( Chain of Thought)[155]는 복잡한 문제를 해결하기 위한 추론 단계를 프롬프트에 입력하는 것을 제안한다. 이러한 단계들은 LLM들이 단계적으로 계획하고 행동하도록 고무시키는 예들로서 기능한다. 이 방법에서는 프롬프트의 예제에서 영감을 바탕으로 계획을 작성한다. 제로-샷-CoT[82]는 LLM들이 "단계별 생각"과 같은 트리거 문장들로 프롬프트함으로써 태스크 추론 프로세스들을 생성할 수 있게 한다. 이 방법은 CoT와 달리 추론 단계를 프롬프트에 예로 통합하지 않는다.

다시 프롬프팅[128]은 계획을 생성하기 전에 각 단계가 필요한 전제조건을 충족하는지 확인하는 것을 포함한다. 단계가 필수 구성 요소를 충족하지 못할 경우 필수 구성 요소 오류 메시지를 도입하고 LLM에 계획을 다시 생성하라는 메시지가 표시됩니다. ReWOO[164]는 외부 관측으로부터 계획을 분리하는 패러다임을 도입하는데, 에이전트는 먼저 계획을 생성하고 독립적으로 관측을 얻은 후 이를 결합하여 최종 결과를 도출한다. HuggingGPT[138]는 먼저 작업을 많은 하위 목표로 분해한 후, 각각을 Huggingface를 기반으로 해결한다. 원샷 방식으로 모든 추론 단계를 수행하는 CoT 및 Zero-shot-CoT와 달리 ReWOO 및 HuggingGPT는 LLM에 재귀적으로 다중 시간에 액세스하여 결과를 생성한다.

\(\bullet\)_Multi-path Reasoning_ 이 전략에서 최종 계획을 생성하기 위한 추론 단계는 나무와 같은 구조로 구성된다. 각각의 중간 단계는 다수의 후속 단계를 가질 수 있다. 이 접근법은 개인이 각 추론 단계에서 여러 선택을 가질 수 있기 때문에 인간의 사고와 유사하다. 특히, Self-consistent CoT(CoT-SC)[151]는 각각의 복잡한 문제가 최종 답을 추론하는 여러 가지 사고 방식을 가지고 있다고 본다. 따라서 다양한 추론 경로와 대응 답변을 생성하기 위해 CoT를 사용하는 것으로 시작한다. 이어서, 가장 높은 빈도를 갖는 답이 최종 출력으로 선택된다. Tree of Thoughts(ToT)[169]는 트리-유사 추론 구조를 이용하여 계획을 생성하도록 설계된다. 이 접근법에서 트리의 각 노드는 중간 추론 단계에 해당하는 "생각"을 나타낸다. 이러한 중간 단계의 선택은 LLM의 평가를 기반으로 한다. 최종 계획은 폭 우선 탐색(BFS) 또는 깊이 우선 탐색(DFS) 전략을 사용하여 생성된다. 계획된 모든 단계를 함께 생성하는 CoT-SC와 비교하여 ToT는 각 추론 단계에 대해 LLM을 쿼리해야 한다. RecMind [152]에서 저자는 계획 과정에서 버려진 역사적 정보를 활용하여 새로운 추론 단계를 도출하는 자기 영감 메커니즘을 설계했다. GoT [8]에서 저자는 ToT에서 나무와 같은 추론 구조를 그래프 구조로 확장하여 더 강력한 촉진 전략을 생성한다. AoT [137]에서 저자는 알고리즘 예제를 프롬프트에 통합하여 LLM의 추론 프로세스를 향상시키는 새로운 방법을 설계한다. 놀랍게도 이 방법은 LLM을 한 번 또는 몇 번만 쿼리하면 된다. [70]에서 LLM은 제로 샷 플래너로 활용됩니다. 각각의 계획 단계에서, 그들은 먼저 다수의 가능한 다음 단계들을 생성한 다음, 허용가능한 액션들에 대한 그들의 거리에 기초하여 최종 단계를 결정한다. [58] 프롬프트들 내의 질의들과 유사한 예들을 통합함으로써 [70]을 더 개선한다. RAP[62]는 몬테 카를로 트리 탐색(MCTS)에 기초하여 상이한 계획들의 잠재적인 이점들을 시뮬레이션하기 위한 월드 모델을 구축하고, 그 후, 다수의 MCTS 반복들을 집계함으로써 최종 계획을 생성한다. 이해력을 높이기 위해 그림 3에서 단일 경로와 다중 경로 추론의 전략을 비교하는 삽화를 제공한다.

\(\bullet\)_External Planner_. 제로 샷 계획에서 LLM의 입증된 힘에도 불구하고 도메인별 문제에 대한 계획을 효과적으로 생성하는 것은 여전히 매우 어렵다. 이 문제를 해결하기 위해 연구자들은 외부 플래너로 전환한다. 이러한 도구는 잘 개발되었으며 정확한 또는 심지어 최적의 계획을 신속하게 식별하기 위해 효율적인 검색 알고리즘을 사용한다. 특히, LLM+P[100]은 먼저 작업 설명을 형식적인 계획 도메인 정의 언어(PDDL)로 변환한 다음 외부 플래너를 사용하여 PDDL을 처리한다. 마지막으로 생성된 결과는 LLM에 의해 다시 자연어로 변환된다. 유사하게, LLM-DP [26]은 관측치, 현재 세계 상태 및 목표 목표를 PDDL로 변환하기 위해 LLM을 활용한다. 이어서, 이 변환된 데이터는,

그림 3: 단일 경로와 다중 경로 추론의 전략 간 비교. LMZSP는 [70]에서 제안된 모델을 나타낸다.

최종 작업 순서를 효율적으로 결정하는 외부 플래너로 전달됩니다. CO-LLM[176]은 LLM이 높은 수준의 계획을 생성하는 데 능숙하지만 낮은 수준의 통제와 투쟁한다는 것을 보여준다. 이러한 한계를 해결하기 위해 휴리스틱하게 설계된 외부 하위 레벨 플래너를 사용하여 상위 레벨 계획에 기반한 액션을 효과적으로 실행한다.

**피드백을 사용 하 여 계획**: 많은 실제 시나리오에서 에이전트는 복잡한 작업을 해결 하기 위해 긴 수평 계획을 만들어야 합니다. 이러한 작업에 직면할 때 피드백이 없는 위의 계획 모듈은 다음과 같은 이유로 인해 덜 효과적일 수 있다. 첫째, 처음부터 무결점 계획을 직접 생성하는 것은 다양한 복잡한 전제 조건을 고려해야 하기 때문에 매우 어렵다. 결과적으로, 단순히 초기 계획을 따르는 것은 종종 실패로 이어집니다. 더욱이, 계획의 실행은 예측할 수 없는 전이 역학에 의해 방해되어 초기 계획을 실행 불가능하게 만들 수 있다. 동시에 인간이 복잡한 과제를 어떻게 처리하는지 검토할 때, 우리는 개인이 외부 피드백을 기반으로 계획을 반복적으로 만들고 수정할 수 있음을 발견했다. 이러한 인간 능력을 시뮬레이션하기 위해 연구자들은 에이전트가 조치를 취한 후 피드백을 받을 수 있는 많은 계획 모듈을 설계했다. 피드백은 환경, 인간 및 모델에서 얻을 수 있으며, 이는 다음에서 자세히 설명한다.

\(\bullet\)_Environmental Feedback_. 이러한 피드백은 객관적인 세계 또는 가상 환경으로부터 얻어진다. 예를 들어, 게임의 작업 완료 신호 또는 에이전트가 조치를 취한 후 수행된 관찰일 수 있습니다. 특히, ReAct [170]은 사고-행위-관찰 트리플렛을 사용하여 프롬프트를 구성하는 것을 제안한다. 사고 구성 요소는 에이전트 행동을 안내하기 위한 고수준의 추론 및 계획을 촉진하는 것을 목표로 한다. 행위는 에이전트가 취한 특정 행위를 나타냅니다. 관찰은 검색 엔진 결과와 같은 외부 피드백을 통해 획득한 행동의 결과에 해당한다. 다음 생각은 이전의 관찰에 영향을 받아 생성된 계획이 환경에 더 적응하게 만든다. 보이저[148]는 프로그램 실행의 중간 진행, 실행 오류 및 자체 검증 결과를 포함한 세 가지 유형의 환경 피드백을 통합하여 계획을 수립한다. 이러한 신호는 에이전트가 다음 조치를 위한 더 나은 계획을 세우는 데 도움이 될 수 있다. 보이저와 유사하게 고스트[184]도 추론 및 행동 취하기 과정에 피드백을 통합한다. 이 피드백은 각 실행된 액션에 대한 성공 및 실패 정보뿐만 아니라 환경 상태를 포함한다. SayPlan [129]는 장면 그래프 시뮬레이터에서 파생된 환경 피드백을 활용하여 전략적 공식을 검증하고 구체화합니다. 이 시뮬레이터는 에이전트 액션에 따른 결과 및 상태 전환을 식별하는 데 능숙하여 실행 가능한 계획이 확인될 때까지 세이플랜의 전략 반복 재보정을 촉진한다. DEPS [154]에서 저자는 작업 완료에 대한 정보만 제공하는 것이 계획 오류를 수정하는 데 종종 부적절하다고 주장한다. 따라서 작업 실패에 대한 자세한 이유를 에이전트에게 알려 계획을 보다 효과적으로 수정할 수 있도록 제안한다. LLM-Planner[141]는 태스크 완료 동안 객체 불일치 및 달성 불가능한 계획에 직면할 때 LLM에 의해 생성된 계획을 동적으로 업데이트하는 근거 재계획 알고리즘을 소개한다. 내부 모노로그[71]는 (1) 태스크가 성공적으로 완료되었는지 여부, (2) 수동 장면 설명, 및 (3) 능동 장면 설명의 세 가지 유형의 피드백을 액션을 취한 후에 에이전트에 제공한다. 전자 두 개는 환경에서 생성되어 에이전트 동작을 보다 실용적이고 합리적으로 만든다.

\(\bullet\)_Human Feedback_. 환경으로부터 피드백을 얻는 것 외에도 인간과 직접 상호작용하는 것도 에이전트 계획 능력을 향상시키기 위한 매우 직관적인 전략이다. 인간의 피드백은 주관적인 신호입니다. 이는 에이전트가 인간의 가치 및 선호도와 효과적으로 일치하도록 할 수 있고 또한 환각 문제를 완화하는 데 도움이 된다. Inner Monologue[71]에서, 에이전트는 3D 시각적 환경에서 고레벨의 자연 언어 명령어들을 수행하는 것을 목표로 한다. 장면 묘사와 관련하여 인간의 피드백을 적극적으로 요청할 수 있는 능력이 부여된다. 그런 다음 에이전트는 인간의 피드백을 프롬프트에 통합하여 보다 정보에 입각한 계획과 추론을 가능하게 합니다. 위의 사례에서 우리는 에이전트 계획 능력을 향상시키기 위해 다양한 유형의 피드백을 결합할 수 있음을 알 수 있다. 예를 들어, Inner Monologue[71]는 에이전트 계획을 용이하게 하기 위해 환경과 인간 피드백을 모두 수집한다.

\(\bullet\)_Model Feedback_. 외부 신호인 앞서 언급한 환경 및 인간 피드백 외에도 연구자들은 에이전트 자체에서 내부 피드백의 활용도 조사했다. 이러한 유형의 피드백은 일반적으로 미리 훈련된 모델에 기초하여 생성된다. 특히, [107]은 셀프 정제 메커니즘을 제안한다. 이 메커니즘은 출력, 피드백 및 정교화의 세 가지 중요한 구성 요소로 구성된다. 먼저 에이전트가 출력을 생성합니다. 그런 다음 LLM을 사용하여 출력에 대한 피드백을 제공하고 정제 방법에 대한 지침을 제공합니다. 마지막으로, 피드백 및 미세화에 의해 출력이 향상된다. 이러한 출력-피드백-정제 프로세스는 일부 원하는 조건에 도달할 때까지 반복된다. 셀프체크[112]를 통해 에이전트는 다양한 단계에서 생성된 자신의 추론 단계를 검사하고 평가할 수 있다. 그런 다음 결과를 비교하여 오류를 수정할 수 있습니다. 인터액션 [20]은 메인 언어 모델이 잘못되고 비효율적인 동작을 회피하도록 돕기 위해 체커 및 분류기와 같은 보조 역할로서 상이한 언어 모델(챗GPT 및 InstructGPT와 같은)을 사용한다. ChatCoT[22]는 모델 피드백을 활용하여 추론 과정의 질을 향상시킨다. 모델 피드백은 에이전트 추론 단계들을 모니터링하는 평가 모듈에 의해 생성된다. 반사[139]는 세부적인 언어적 피드백을 통해 에이전트의 계획 능력을 향상시키기 위해 개발되었다. 이 모델에서, 에이전트는 먼저 자신의 메모리에 기초하여 액션을 생성하고, 그 후, 평가자는 에이전트 궤적을 입력으로 하여 피드백을 생성한다. 피드백이 스칼라 값으로 제공되는 이전 연구와 달리 이 모델은 LLM을 활용하여 더 자세한 구두 피드백을 제공하여 에이전트 계획에 대한 더 포괄적인 지원을 제공할 수 있다.

_Remark_.: 결론적으로, 피드백이 없는 계획 모듈의 구현은 비교적 간단하다. 그러나, 주로 소수의 추론 단계만을 필요로 하는 단순한 작업에 적합하다. 반대로 피드백으로 계획하는 전략은 피드백을 처리하기 위해 보다 신중한 설계가 필요하다. 그럼에도 불구하고 훨씬 더 강력하고 장거리 추론을 포함하는 복잡한 작업을 효과적으로 해결할 수 있다.

#### 2.1.4 Action Module

액션 모듈은 에이전트의 결정을 특정 결과로 변환하는 역할을 합니다. 이 모듈은 가장 다운스트림 위치에 있으며 환경과 직접 상호 작용합니다. 프로파일, 메모리 및 계획 모듈의 영향을 받습니다. 이 섹션에서는 행동 모듈을 네 가지 관점에서 소개합니다. (1) 행동 목표: 행동의 의도된 결과는 무엇입니까? (2) 액션 제작: 액션은 어떻게 생성되는가? (3) 동작 공간: 사용 가능한 동작은 무엇인가? (4) 액션 영향: 액션의 결과는 무엇인가? 이러한 관점 중 첫 번째 두 가지는 행동에 선행하는 측면("행동 전" 측면), 세 번째는 행동 자체("행동 내" 측면), 네 번째는 행동의 영향("행동 후" 측면)을 강조한다.

**작업 목표**: 에이전트는 다양한 목표를 사용하여 작업을 수행할 수 있습니다. 여기서는 (1) _과제 완료_와 같은 몇 가지 대표적인 예를 제시한다. 이 시나리오에서, 에이전트의 동작은 마인크래프트에서 철 곡괭이를 제작하거나 소프트웨어 개발에서 기능을 완성하는 것과 같은 특정 작업을 달성하는 것을 목표로 한다[148]. 이러한 행동은 일반적으로 잘 정의된 목표를 가지고 있으며, 각 행동은 최종 작업의 완성에 기여한다. 이러한 유형의 목표를 목표로 하는 행동은 기존 문헌에서 매우 일반적이다. (2) _Communication_. 이 경우, 정보 공유 또는 협업을 위해 다른 에이전트들 또는 실제 인간들과 통신하기 위한 액션들이 취해진다. 예를 들어, ChatDev[124]의 에이전트는 서로 통신하여 소프트웨어 개발 작업을 일괄적으로 수행할 수 있다. Inner Monologue[71]에서 에이전트는 인간과 적극적으로 소통하고 인간의 피드백을 기반으로 행동 전략을 조정한다. (3) _환경 탐사_. 이 예에서 에이전트는 익숙하지 않은 환경을 탐색하여 인식을 확장하고 탐색과 착취 사이의 균형을 맞추는 것을 목표로 한다. 예를 들어, 보이저[148]의 에이전트는 작업 완료 과정에서 알려지지 않은 기술을 탐색하고 시행착오를 통해 환경 피드백을 기반으로 기술 실행 코드를 지속적으로 정제할 수 있다.

**작업 프로덕션**: 모델 입력 및 출력이 직접 연결 된 일반 LLM과 다르게 에이전트는 다른 전략 및 소스를 통해 작업을 수행할 수 있습니다. 다음에서는 일반적으로 사용되는 행동 생산 전략의 두 가지 유형을 소개한다. (1) _Memory Recollection을 통한 동작_. 이 전략에서는 현재 태스크에 따라 에이전트 메모리에서 정보를 추출하여 액션을 생성한다. 작업 및 추출된 메모리는 에이전트 작업을 트리거하는 프롬프트로 사용됩니다. 예를 들어, 생성 에이전트[121]에서 에이전트는 메모리 스트림을 유지하고, 각각의 액션을 취하기 전에, 메모리 스팀으로부터 최근, 관련 있고 중요한 정보를 검색하여 에이전트 액션을 안내한다. GITM[184]에서, 하위-레벨 하위-목표를 달성하기 위해, 에이전트는 자신의 메모리에 질의하여 태스크와 관련된 임의의 성공적인 경험들이 있는지를 결정한다. 유사한 작업이 이전에 완료된 경우 에이전트는 이전에 성공한 작업을 호출하여 현재 작업을 직접 처리합니다. 챗데브[124] 및 메타GPT[64]와 같은 협업 에이전트들에서, 상이한 에이전트들은 서로 통신할 수 있다. 이 과정에서 대화록의 대화 히스토리가 에이전트 메모리에 저장된다. 에이전트에 의해 생성된 각각의 발화는 그 기억의 영향을 받는다. (2) _Plan Following을 통한 액션_. 이 전략에서 에이전트는 미리 생성된 계획에 따라 조치를 취합니다. 예를 들어, DEPS [154]에서, 주어진 태스크에 대해, 에이전트는 먼저 액션 플랜을 만든다. 계획 실패를 나타내는 신호가 없으면 에이전트는 이러한 계획을 엄격하게 준수합니다. GITM[184]에서 에이전트는 작업을 많은 하위 목표로 분해하여 상위 계획을 만든다. 이러한 계획을 바탕으로 에이전트는 각 하위 목표를 순차적으로 해결하는 행동을 취하여 최종 과제를 완성한다.

**작업 공간**: 작업 공간은 에이전트에서 수행할 수 있는 가능한 작업 집합을 나타냅니다. 일반적으로 우리는 이러한 작업을 (1) 외부 도구와 (2) LLM에 대한 내부 지식의 두 가지 클래스로 대략 나눌 수 있다. 이하에서는 이러한 행위를 좀 더 구체적으로 소개한다.

\(\bullet\)_External Tools_. LLM은 많은 양의 작업을 수행하는 데 효과적인 것으로 입증되었지만 포괄적인 전문 지식이 필요한 도메인에는 잘 작동하지 않을 수 있다. 또한 LLM은 스스로 해결하기 어려운 환각 문제에 직면할 수도 있다. 상기 문제들을 완화하기 위해, 에이전트들은 액션을 실행하기 위한 외부 툴들을 호출할 수 있는 능력을 부여받는다. 다음에서는 문헌에서 활용된 몇 가지 대표적인 도구를 제시한다.

(1) _APIs_. 액션 공간을 보완하고 확장하기 위해 외부 API를 활용하는 것은 최근 인기 있는 패러다임이다. 예를 들어, HuggingGPT[138]는 복잡한 사용자 작업을 수행하기 위해 HuggingFace 상의 모델들을 활용한다. [115, 130] 사용자 요청에 응답할 때 외부 웹 페이지에서 관련 내용을 추출하기 위해 쿼리를 자동으로 생성하도록 제안합니다. TPTU[130]는 파이썬 인터프리터 및 LaTeX 컴파일러 둘 다와 인터페이스하여 제곱근, 인수분해 및 행렬 연산과 같은 정교한 연산을 실행한다. 또 다른 유형의 API는 자연어 또는 코드 입력에 기초하여 LLM에 의해 직접 호출될 수 있는 API이다. 예를 들어 Gorilla [123]은 API 호출에 대한 정확한 입력 인수를 생성하고 외부 API 호출 중 환각 문제를 완화하도록 설계된 미세 조정 LLM입니다. ToolFormer [133]은 자연어 명령어를 기반으로 주어진 도구를 다른 기능이나 형식을 가진 다른 도구로 자동으로 변환할 수 있는 LLM 기반의 도구 변환 시스템이다. API-Bank[90]은 LLM 기반의 API 추천 에이전트로 다양한 프로그래밍 언어 및 도메인에 대한 적절한 API 호출을 자동으로 검색하고 생성할 수 있다. API-Bank는 또한 사용자가 생성된 API 호출을 쉽게 수정하고 실행할 수 있는 대화형 인터페이스를 제공한다. ToolBench[126]는 LLM 기반의 도구 생성 시스템으로 자연어 요구사항에 기반하여 다양한 실용적인 도구를 자동으로 설계하고 구현할 수 있다. 툴벤치가 생성하는 도구로는 계산기, 단위 변환기, 달력, 지도, 차트 등이 있다. RestGPT [142]는 LLM과 RESTful API를 연결 하 여 웹 서비스 개발에 대 한 널리 허용 되는 표준을 따르므로 결과 프로그램이 실제 응용 프로그램과 더 호환 됩니다. TaskMatrix.AI [93]은 LLM과 수백만 개의 API를 연결하여 작업 실행을 지원합니다. 그 핵심에는 사용자와 상호 작용하고 목표와 맥락을 이해한 다음 특정 작업에 대한 실행 가능한 코드를 생성하는 다중 모드 대화 기반 모델이 있다. 이러한 모든 에이전트는 외부 API를 외부 도구로 활용하고, 사용자가 생성되거나 변환된 도구를 쉽게 수정하고 실행할 수 있는 대화형 인터페이스를 제공한다.

(2) _데이터베이스 & 지식베이스_. 외부 데이터베이스 또는 지식 베이스에 접속하는 것은 에이전트들이 보다 현실적인 액션들을 생성하기 위한 특정 도메인 정보를 획득하는 것을 도울 수 있다. 예를 들어 ChatDB [67]은 SQL 문을 사용 하 여 데이터베이스를 쿼리 하 여 논리 방식으로 에이전트의 작업을 촉진 합니다. MRKL[80], OpenAGI[56]는 지식베이스, 플래너 등 다양한 전문가 시스템을 통합하여 도메인별 정보에 접근한다.

(3) _외부 모델_. 선행 연구들은 가능한 행동의 범위를 확장하기 위해 외부 모델을 활용하는 경우가 많다. API와 비교할 때 외부 모델은 일반적으로 더 복잡한 작업을 처리합니다. 각각의 외부 모델은 다수의 API에 대응할 수 있다. 예를 들어, 텍스트 검색 능력을 향상시키기 위해, 메모리뱅크[179]는 두 개의 언어 모델을 통합한다 : 하나는 입력 텍스트를 인코딩하도록 설계되고, 다른 하나는 쿼리 문들을 매칭하는 것을 담당한다. ViperGPT[144]는 먼저 언어 모델을 기반으로 구현된 Codex를 이용하여 텍스트 설명으로부터 Python 코드를 생성한 후 코드를 실행하여 주어진 작업을 완료한다. TPTU[130]는 다양한 LLM들을 통합하여 코드 생성, 가사 생성 등과 같은 광범위한 언어 생성 작업을 수행한다. ChemCrow [10]은 유기 합성, 약물 발견 및 물질 설계에서 작업을 수행하도록 설계된 LLM 기반 화학제이다. 운영을 지원하기 위해 17개의 전문가가 설계한 모델을 활용합니다. MM-REACT[167]은 이미지 생성을 위한 X-디코더, 비디오 요약을 위한 VideoBERT, 오디오 처리를 위한 SpeechBERT와 같은 다양한 외부 모델을 통합하여, 다양한 멀티모달 시나리오에서 그 기능을 강화한다.

\(\bullet\)_Internal Knowledge_. 외부 도구를 활용하는 것 외에도 많은 에이전트는 LLM의 내부 지식에만 의존하여 행동을 안내한다. 이제 에이전트가 합리적이고 효과적으로 행동하도록 지원할 수 있는 LLM의 몇 가지 중요한 기능을 제시한다. (1) _계획 능력_. 이전 연구에서는 LLM이 복잡한 작업을 더 간단한 작업으로 분해하는 괜찮은 계획자로 사용될 수 있음을 입증했다[155]. LLM들의 그러한 능력은 프롬프트들[82]에 예들을 통합하지 않고서도 트리거될 수 있다. LLM의 계획 능력을 기반으로 DEPS[154]는 하위 목표 분해를 통해 복잡한 작업을 해결할 수 있는 마인크래프트 에이전트를 개발한다. GITM[184] 및 보이저[148]와 같은 유사한 에이전트는 또한 LLM의 계획 능력에 크게 의존하여 상이한 작업을 성공적으로 완료한다. (2) _대화 능력_. LLM은 일반적으로 고품질 대화를 생성할 수 있습니다. 이 기능을 통해 에이전트는 인간처럼 행동할 수 있습니다. 이전 연구에서는 LLM의 강력한 대화 능력을 기반으로 많은 에이전트가 조치를 취한다. 예를 들어, ChatDev[124]에서, 상이한 에이전트들은 소프트웨어 개발 프로세스에 대해 논의할 수 있고, 심지어 그들 자신의 행동에 대한 성찰을 할 수 있다. RLP[54]에서, 에이전트는 에이전트의 발화에 대한 그들의 잠재적인 피드백에 기초하여 청취자들과 통신할 수 있다. (3) _상식 이해 능력_. LLM의 또 다른 중요한 능력은 인간의 상식을 잘 이해할 수 있다는 것이다. 이러한 능력을 바탕으로 많은 에이전트는 인간의 일상을 시뮬레이션하고 인간과 유사한 결정을 내릴 수 있다. 예를 들어, Generative Agent에서 에이전트는 자신의 현재 상태, 주변 환경을 정확하게 이해하고, 기본적인 관찰에 기초하여 높은 수준의 아이디어를 요약할 수 있다. LLM의 상식 이해 능력이 없으면 이러한 행동을 안정적으로 시뮬레이션할 수 없다. 유사한 결론이 RecAgent [149] 및 S3 [55]에도 적용될 수 있으며, 여기서 에이전트는 사용자 추천 및 사회적 행동을 시뮬레이션하는 것을 목표로 한다.

**작업 영향**: 작업 영향은 작업의 결과를 나타냅니다. 사실, 행동 영향은 수많은 사례를 포함할 수 있지만 간결함을 위해 우리는 몇 가지 예만 제공한다. (1) _환경 변경_. 에이전트는 위치 이동, 항목 수집, 건물 건설 등과 같은 작업에 의해 환경 상태를 직접 변경할 수 있습니다. 예를 들어, GITM[184] 및 보이저[148]에서, 환경은 그들의 작업 완료 프로세스에서 에이전트들의 액션들에 의해 변경된다. 예를 들어, 에이전트가 세 그루의 나무를 채굴하면 환경에서 사라질 수 있습니다. (2) _Altering Internal States_. 에이전트에 의해 취해진 액션은 또한 메모리 업데이트, 새로운 계획 형성, 새로운 지식 획득 등을 포함하는 에이전트 자체를 변경할 수 있다. 예를 들어, 생성 에이전트[121]에서, 메모리 스트림들은 시스템 내에서 액션들을 수행한 후에 업데이트된다. SayCan [2]를 사용하면 에이전트가 환경에 대한 이해를 업데이트하는 작업을 수행할 수 있습니다. (3) _Triggering New Actions_. 태스크 완료 프로세스에서, 하나의 에이전트 액션은 다른 하나에 의해 트리거될 수 있다. 예를 들어, 보이저[148]는 필요한 모든 리소스를 모으면 건물을 구축한다. DEPS [154]는 계획을 순차적 하위 목표로 분해하며, 각 하위 목표는 잠재적으로 다음 목표를 트리거합니다.

### 에이전트 기능 획득

위의 섹션에서는 주로 LLM이 인간과 같은 작업을 수행할 수 있는 자격을 갖도록 하는 능력을 더 잘 고취하기 위해 에이전트 아키텍처를 설계하는 방법에 중점을 둔다. 아키텍처는 에이전트의 "하드웨어"로서 기능한다. 그러나 하드웨어에만 의존하는 것은 효과적인 작업 수행을 달성하기에는 불충분하다. 에이전트가 "소프트웨어" 자원으로 간주될 수 있는 필요한 작업별 능력, 기술 및 경험이 부족할 수 있기 때문이다. 에이전트에게 이러한 자원을 갖추기 위해 다양한 전략이 고안되었다. 일반적으로 이러한 전략을 LLM의 미세 조정이 필요한지 여부에 따라 두 가지 클래스로 분류한다. 이하에서는 각각을 보다 상세하게 소개한다.

**미세 조정을 통한 기능 획득**: 작업 완료를 위한 에이전트 기능을 향상시키는 간단한 방법은 작업 종속 데이터 세트를 기반으로 에이전트를 미세 조정하는 것입니다. 일반적으로, 데이터 세트는 인간 주석, LLM 생성 또는 실제 애플리케이션으로부터 수집되는 것에 기초하여 구성될 수 있다. 이하에서는 이러한 방법들을 보다 구체적으로 소개한다.

\(\bullet\)_Human Annotated Datasets_로 미세 조정합니다. 에이전트를 미세 조정하기 위해 인간 주석이 달린 데이터 세트를 활용하는 것은 다양한 응용 시나리오에서 사용할 수 있는 다용도 접근법이다. 이 접근법에서 연구자들은 먼저 주석 작업을 설계한 다음 작업자를 모집하여 작업을 완료한다. 예를 들어, CoH [101]에서 저자는 LLM을 인간 가치 및 선호도와 정렬하는 것을 목표로 한다. 인간의 피드백을 단순하고 상징적인 방식으로 활용하는 다른 모델과 달리, 이 방법은 인간의 피드백을 자연어 형태의 상세한 비교 정보로 변환한다. LLM은 이러한 자연어 데이터 세트를 기반으로 직접 미세 조정된다. RET-LLM [114]에서 자연어를 구조화된 메모리 정보로 더 잘 변환하기 위해 저자는 인간 구성 데이터 세트를 기반으로 LLM을 미세 조정하며, 여기서 각 샘플은 "트리플트-자연어" 쌍이다. 웹샵[168]에서 저자는 아마존닷컴을 통해 118만 개의 실제 제품을 수집하여 몇 가지 신중하게 설계된 인간 쇼핑 시나리오를 포함하는 시뮬레이션된 전자 상거래 웹사이트에 올렸다. 이 웹사이트를 기반으로 저자는 실제 인간 행동 데이터 세트를 수집하기 위해 13명의 작업자를 모집한다. 마지막으로, 이 데이터셋을 기반으로 휴리스틱 규칙에 기반한 세 가지 방법, 모방 학습 및 강화 학습을 학습한다. 비록 저자들이 LLM 기반 에이전트들을 미세 조정하지는 않지만,

[MISSING_PAGE_FAIL:13]

이 에이전트는 SWIFT 모듈로 컴팩트한 인코더-디코더 언어 모델을 구성하며, 이는 인간 주석이 달린 데이터 세트를 사용하여 미세 조정된다.

\(\bullet\)_LLM 생성 데이터 집합으로 미세 조정합니다. 인간 주석이 달린 데이터 세트를 구축하려면 사람을 모집해야 하는데, 이는 특히 많은 양의 샘플에 주석을 달아야 할 때 비용이 많이 들 수 있다. LLM이 광범위한 작업에서 인간과 유사한 기능을 달성할 수 있다는 점을 고려할 때 자연스러운 아이디어는 주석 작업을 수행하기 위해 LLM을 사용하는 것이다. 이 방법에서 생성된 데이터 세트는 인간의 주석이 달린 데이터 세트만큼 완벽하지 않을 수 있지만 훨씬 저렴하고 더 많은 샘플을 생성하기 위해 활용할 수 있다. 예를 들어 ToolBench [126]에서 오픈 소스 LLM의 도구 사용 능력을 향상시키기 위해 저자는 RapidAPI 허브에서 49개 범주에 걸쳐 있는 16,464개의 실제 API를 수집합니다. 그들은 이러한 API를 사용하여 ChatGPT가 단일 도구 및 다중 도구 시나리오를 모두 포함하는 다양한 지침을 생성하도록 프롬프트했다. 얻어진 데이터 세트를 기반으로 저자들은 LLaMA [146]를 미세 조정하고 도구 사용 측면에서 상당한 성능 향상을 얻었다. [102]에서, 소셜 능력을 가진 에이전트에 권한을 부여하기 위해, 저자들은 샌드박스를 설계하고, 서로 상호작용하기 위해 다수의 에이전트를 배치한다. 사회적 질문이 주어지면, 중앙 에이전트는 먼저 초기 응답을 생성한다. 그런 다음 피드백을 수집하기 위해 주변 에이전트에 대한 응답을 공유합니다. 피드백과 상세한 설명을 바탕으로 중앙 에이전트는 초기 반응을 수정하여 사회 규범에 더 부합하도록 한다. 이 과정에서 저자는 많은 양의 에이전트 소셜 상호작용 데이터를 수집한 다음 LLM을 미세 조정하기 위해 활용한다.

\(\bullet\)_Real-world Datasets_를 사용하여 미세 조정합니다. 인간 또는 LLM 주석을 기반으로 데이터 세트를 구축하는 것 외에도 실제 데이터 세트를 직접 사용하여 에이전트를 미세 조정하는 것도 일반적인 전략이다. 예를 들어, MIND2WEB[30]에서, 저자들은 웹 도메인에서 에이전트 능력을 향상시키기 위해 대량의 실세계 데이터 세트를 수집한다. 선행 연구와 달리 본 논문에서 제시한 데이터 세트는 다양한 작업, 실제 시나리오 및 포괄적인 사용자 상호 작용 패턴을 포함한다. 특히, 저자는 31개 도메인에 걸쳐 있는 137개의 실제 웹사이트에서 2,000개 이상의 개방형 작업을 수집한다. 이 데이터 세트를 사용하여 저자는 영화 발견 및 티켓 예매를 포함한 웹 관련 작업에서 성능을 향상시키기 위해 LLM을 미세 조정한다. SQL-PALM[143]에서 연구자들은 스파이더라는 교차 도메인 대규모 텍스트 대 SQL 데이터 세트를 기반으로 PaLM-2를 미세 조정한다. 획득한 모델은 텍스트-SQL 작업에서 상당한 성능 향상을 달성할 수 있습니다.

**미세 조정 없는 기능 획득**: 전통 기계 학습 시대에 모델 기능은 주로 데이터가 모델 매개 변수에 인코딩되는 데이터 세트에서 학습하여 획득됩니다. LLM 시대에, 모델 능력은 모델 파라미터를 훈련/미세 조정하거나 섬세한 프롬프트(_i.e._, 프롬프트 엔지니어)를 설계함으로써 획득될 수 있다. 프롬프트 엔지니어에서는 모델 기능을 향상시키거나 기존 LLM 기능을 출시하기 위해 프롬프트에 귀중한 정보를 작성해야 한다. 에이전트 시대에 모델 능력은 (1) 모델 미세 조정, (2) 신속한 엔지니어 및 (3) 적절한 에이전트 진화 메커니즘 설계(이를 _메커니즘 엔지니어링_이라고 함)의 세 가지 전략을 기반으로 획득될 수 있다. 메커니즘 공학은 전문 모듈 개발, 새로운 작업 규칙 도입 및 에이전트 능력을 향상시키기 위한 기타 전략을 포함하는 광범위한 개념이다. 모델 역량 획득 전략에 대한 이러한 전환을 명확하게 이해하기 위해 그림 4에서 설명한다. 위 절에서는 미세 조정 전략을 자세히 설명한다. 다음에서는 에이전트 능력 획득을 위한 프롬프트 엔지니어링 및 메커니즘 엔지니어링을 소개한다.

그림 4: 모델 역량을 획득하기 위한 전략의 전환에 대한 그림.

\(\bullet\)_Prompting Engineering_. 강력한 언어 이해 능력으로 인해 사람들은 자연어를 사용하여 LLM과 직접 상호 작용할 수 있다. 이것은 에이전트 능력을 향상시키기 위한 새로운 전략, 즉 자연 언어를 사용하여 원하는 능력을 설명한 다음 LLM 동작에 영향을 미치는 프롬프트로 사용할 수 있는 전략을 소개한다. 예를 들어, CoT[155]에서, 복잡한 태스크 추론을 위한 능력을 에이전트에게 부여하기 위해, 저자들은 중간 추론 단계들을 프롬프트에서 적은 샷 예들로서 제시한다. 유사한 기술들이 CoT-SC[151] 및 ToT[169]에서도 사용된다. 소셜AGI[54]에서는 대화에서 에이전트 자기 인식 능력을 향상시키기 위해 저자들은 LLM에 청자와 그 자신의 정신 상태에 대한 에이전트 신념을 촉구하여 생성된 발화를 더 매력적이고 적응적으로 만든다. 또한, 저자들은 청취자의 목표 정신 상태를 통합하여 에이전트가 보다 전략적인 계획을 세울 수 있도록 한다. Retroformer[171]는 에이전트가 과거의 실패에 대한 반사를 생성할 수 있게 하는 소급 모델을 제시한다. 반사는 에이전트의 향후 조치를 안내하기 위해 LLM의 프롬프트에 통합된다. 또한, 이 모델은 강화 학습을 활용하여 소급 모델을 반복적으로 개선하여 LLM 프롬프트를 정제한다.

\(\bullet\)_Mechanism Engineering_. 모델 미세 조정 및 신속한 엔지니어링과는 달리 메커니즘 엔지니어링은 에이전트 능력을 향상시키는 독특한 전략이다. 다음에서는 메커니즘 엔지니어링을 위한 몇 가지 대표적인 방법을 제시한다.

(1) _Trial-and-error._ 이 방법에서 에이전트는 먼저 행동을 수행하고, 이후에 미리 정의된 비평가가 행동을 판단하기 위해 호출된다. 행위가 만족스럽지 못한 것으로 간주되면 에이전트는 비평가의 피드백을 통합하여 반응한다. RAH[140]에서, 에이전트는 추천기 시스템들에서 사용자 어시스턴트로서 기능한다. 에이전트의 중요한 역할 중 하나는 인간의 행동을 시뮬레이션하고 사용자를 대신하여 응답을 생성하는 것이다. 이 목적을 달성하기 위해 에이전트는 먼저 예측된 응답을 생성한 다음 실제 인간 피드백과 비교한다. 예측된 반응과 실제 인간의 피드백이 다를 경우, 비평가는 실패 정보를 생성하고, 이는 후속적으로 에이전트의 다음 행동에 통합된다. DEPS [154]에서 에이전트는 먼저 주어진 작업을 수행하기 위한 계획을 설계한다. 계획 실행 과정에서 행동이 실패하면 설명자는 실패 원인을 설명하는 구체적인 내용을 생성한다. 그런 다음 이 정보가 에이전트에 의해 통합되어 계획을 재설계합니다. RoCo[108]에서, 에이전트는 먼저 멀티 로봇 협업 작업에서 각 로봇에 대한 서브-작업 계획 및 3D 웨이포인트의 경로를 제안한다. 그런 다음 충돌 감지 및 역기구학과 같은 환경 검사 세트에 의해 계획 및 경유지가 검증됩니다. 검사 중 하나가 실패하면 피드백이 각 에이전트의 프롬프트에 추가되고 또 다른 대화 라운드가 시작됩니다. 에이전트는 모든 검증을 통과할 때까지 플랜 및 경유지를 논의하고 개선하기 위해 LLM을 사용합니다. PREFER[173]에서, 에이전트는 먼저 데이터의 서브세트에 대한 자신의 성능을 평가한다. 특정 예들을 해결하지 못하면, LLM들은 실패의 원인들을 반영하는 피드백 정보를 생성하기 위해 레버리지된다. 이 피드백에 기초하여, 에이전트는 자신의 동작을 반복적으로 정제함으로써 자신을 개선한다.

(2) _Crowd-sourcing._ [35]에서 저자는 군중의 지혜를 활용하여 에이전트의 능력을 향상시키는 토론 메커니즘을 설계한다. 우선, 서로 다른 에이전트는 주어진 질문에 대해 별도의 응답을 제공한다. 응답이 일관되지 않으면 다른 에이전트의 솔루션을 통합하고 업데이트된 응답을 제공하라는 메시지가 표시됩니다. 이러한 반복적인 과정은 최종 합의 답변에 도달할 때까지 계속된다. 이 방법은 다른 에이전트의 의견을 이해하고 통합함으로써 각 에이전트의 능력을 향상시킨다.

(3) _경험 축적._ GITM[184]에서 에이전트는 초기에 태스크를 해결하는 방법을 알지 못한다. 그런 다음 탐색을 수행하고 작업을 성공적으로 완료하면 이 작업에서 사용된 작업이 에이전트 메모리에 저장됩니다. 향후 에이전트가 유사한 태스크를 만나면 관련 메모리를 추출하여 현재 태스크를 완성한다. 이 과정에서 개선된 에이전트 능력은 특별히 설계된 메모리 축적 및 활용 메커니즘에서 비롯된다. 보이저[148]에서, 저자들은 에이전트에게 스킬 라이브러리를 장착하고, 라이브러리의 각 스킬은 실행가능 코드로 표현된다. 에이전트-환경 상호작용 과정에서 각 스킬에 대한 코드는 환경 피드백과 에이전트 자체 검증 결과에 따라 반복적으로 정제될 것이다. 실행 기간 후에, 에이전트는 스킬 라이브러리에 액세스함으로써 상이한 태스크들을 효율적으로 성공적으로 완료할 수 있다. MemPrompt[106]에서, 사용자들은 에이전트의 문제 해결 의도에 관한 자연어로 피드백을 요청받고, 이 피드백은 메모리에 저장된다. 에이전트가 유사한 태스크들을 만날 때, 더 적합한 응답들을 생성하기 위해 관련된 메모리들을 검색하려고 시도한다.

(4) _Self-driven evolution._ LMA3[24]에서 에이전트는 스스로 자율적으로 목표를 설정할 수 있으며, 환경을 탐색하고 보상 함수로부터 피드백을 받아 점차 능력을 향상시킬 수 있다. 이 메커니즘에 따라 에이전트는 자신의 선호도에 따라 지식을 획득하고 능력을 개발할 수 있다. SALLM-MS[116]에서, GPT-4와 같은 고급 대형 언어 모델을 다중 에이전트 시스템에 통합함으로써, 에이전트는 복잡한 작업을 적응하고 수행할 수 있고, 고급 통신 능력을 보여줌으로써, 환경과의 상호 작용에서 자기 주도적 진화를 실현한다. CLMTWA[132]에서, 교사로서 큰 언어 모델을 사용하고 학생으로서 약한 언어 모델을 사용함으로써, 교사는 자연어 설명을 생성하고 전달할 수 있어, 마음 이론을 통해 학생의 추론 능력을 향상시킬 수 있다. 교사는 또한 개입의 기대 효용을 바탕으로 학생에 대한 설명을 개인화하고 필요할 때만 개입할 수 있다. NLSOM[185]에서는 서로 다른 에이전트들이 자연어를 통해 소통하고 협업하여 하나의 에이전트가 해결할 수 없는 작업을 해결한다. 이는 다수의 에이전트들 간의 정보와 지식의 교환을 활용한 자기 주도적 학습의 한 형태라고 볼 수 있다. 그러나, LMA3, SALLM-MS, CLMTWA 등의 다른 모델과는 달리, NLSOM은 태스크 요구 사항 및 다른 에이전트 또는 환경으로부터의 피드백에 기초하여 에이전트 목표, 역할, 태스크 및 관계의 동적 조정을 허용한다.

_Remark_.: 에이전트 능력 획득을 위한 전술된 전략들을 비교해보면, 미세 조정 방법은 많은 양의 태스크 특정 지식을 통합할 수 있지만 오픈 소스 LLM에만 적합한 모델 파라미터를 조정함으로써 에이전트 능력을 향상시킨다는 것을 알 수 있다. 미세 조정이 없는 방법은 일반적으로 섬세한 프롬프트 전략이나 메커니즘 공학을 기반으로 에이전트 기능을 향상시킨다. 오픈 소스 LLM 및 폐쇄 소스 LLM 모두에 사용할 수 있습니다. 그러나 LLM들의 입력 컨텍스트 윈도우의 제한으로 인해, 그들은 너무 많은 태스크 정보를 통합할 수 없다. 또한 프롬프트와 메커니즘의 설계 공간이 매우 커서 최적의 솔루션을 찾기가 쉽지 않다.

위의 섹션에서는 LLM 기반 에이전트의 구성에 대해 자세히 설명했으며, 여기서 아키텍처 설계 및 능력 획득을 포함한 두 가지 측면에 중점을 둔다. 우리는 기존 작업과 위의 분류법 사이의 대응 관계를 표 1에 제시한다. 무결성을 위해 LLM 기반 에이전트에 대해 명시적으로 언급하지 않았지만 이 영역과 관련이 높은 여러 연구도 통합했다는 점에 유의해야 한다.

## 3 LLM 기반 자율 에이전트 애플리케이션

강력한 언어 이해력, 복잡한 과제 추론 및 상식 이해 능력으로 인해 LLM 기반 자율 에이전트는 여러 도메인에 영향을 미칠 수 있는 상당한 잠재력을 보여주었다. 이 섹션에서는 이전 연구의 간결한 요약을 제공하여 사회 과학, 자연 과학 및 공학의 세 가지 별개의 영역에서 적용에 따라 분류한다(글로벌 개요는 그림 5의 왼쪽 부분 참조).

### Social Science

사회 과학은 사회의 연구와 그 사회 내의 개인 간의 관계에 전념하는 과학의 한 분야 중 하나이다. LLM 기반 자율 에이전트는 이 영역을 촉진할 수 있다.

그림 5: LLM 기반 에이전트의 적용(왼쪽) 및 평가 전략(오른쪽).

인상적인 인간다운 이해, 사고 및 과제 해결 능력을 활용합니다. 이하에서는 LLM 기반 자율 에이전트의 영향을 받을 수 있는 몇 가지 핵심 영역에 대해 논의한다.

**심리학**: 심리학의 영역에서 LLM 기반 에이전트는 시뮬레이션 실험을 수행 하 여 정신 건강 지원 등을 제공 하는 데 활용할 수 있습니다 [1, 3, 105, 187]. 예를 들어 [1]에서 저자는 다른 프로필을 가진 LLM을 할당하고 심리 실험을 완료하도록 한다. 이 결과로부터 저자는 LLM이 실제 인간 연구의 결과와 일치하는 결과를 생성할 수 있음을 발견했다. 또한, 더 큰 모델은 일반적으로 더 작은 모델보다 더 충실한 시뮬레이션 결과를 제공할 수 있다. 흥미로운 발견은 많은 실험에서 ChatGPT 및 GPT-4와 같은 모델이 다운스트림 애플리케이션에 영향을 미칠 수 있는 너무 완벽한 추정치(초정확도 왜곡이라고 함)를 제공할 수 있다는 것이다. [105]에서 저자는 정신적 웰빙 지원을 위한 LLM 기반 대화 에이전트의 효과를 체계적으로 분석한다. 그들은 레딧에서 120개의 게시물을 수집하며, 그러한 에이전트가 사용자가 주문형 품종, 사회적 격리 및 우울증에 대처하는 데 도움이 될 수 있음을 발견했다. 동시에, 그들은 또한 에이전트가 때때로 유해한 콘텐츠를 생성할 수 있다는 것을 발견합니다.

**정치학 및 경제**: LLM 기반 에이전트는 정치학 및 경제를 연구하는 데에도 사용할 수 있습니다 [5, 187, 65]. [5]에서는 이념 탐지 및 투표 패턴 예측에 LLM 기반 에이전트를 활용한다. [187]에서 저자는 LLM 기반 에이전트의 지원을 통해 정치 연설의 담론 구조와 설득력 있는 요소를 이해하는 데 중점을 둔다. [65]에서 LLM 기반 에이전트는 모의 시나리오에서 인간의 경제적 행동을 탐색하기 위해 재능, 선호도 및 성격과 같은 특정 특성을 제공한다.

**사회 시뮬레이션**: 이전에 인간 사회로 실험을 수행하는 것은 종종 비용이 많이 들고, 비윤리적이거나, 실행 불가능한 경우가 많습니다. LLM이 발전함에 따라, 많은 사람들이 유해 정보의 전파와 같은 사회 현상을 시뮬레이션하기 위해 LLM 기반 에이전트로 가상 환경을 구축하려고 한다[122, 91, 86, 121, 99, 83, 55, 156]. 예를 들어, Social Simulacra[122]는 온라인 소셜 커뮤니티를 시뮬레이션하고 커뮤니티 규정을 개선하기 위해 의사 결정자를 돕기 위해 에이전트 기반 시뮬레이션을 활용하는 가능성을 탐구한다. [91, 86] 소셜 네트워크에서 LLM 기반 에이전트의 다양한 행동 특성의 잠재적 영향을 조사한다. 생성 에이전트[121] 및 에이전트Sims[99]는 인간의 일상 생활을 시뮬레이션하기 위해 가상 마을에 다수의 에이전트를 구성한다. 소셜AI 스쿨[83]은 LLM 기반 에이전트를 사용하여 아동 발달 과정에서 기본적인 사회 인지 기술을 시뮬레이션하고 조사한다. S\({}^{3}\)[55]는 정보, 감정, 태도의 전파에 초점을 맞추어 소셜 네트워크 시뮬레이터를 구축한다. CGMI[75]는 멀티 에이전트 시뮬레이션을 위한 프레임워크이다. CGMI는 트리 구조를 통해 에이전트의 성격을 유지하고 인지 모델을 구성한다. 저자들은 CGMI를 사용하여 교실 시나리오를 시뮬레이션했다.

**판례**: LLM 기반 에이전트는 법적 의사 결정 프로세스에 도움이 되어 더 많은 정보에 입각한 판단을 용이하게 할 수 있습니다. [25, 61]. 맹검 판사[61]는 다수의 판사의 의사 결정 프로세스를 시뮬레이션하기 위해 여러 언어 모델을 사용한다. 투표 메커니즘을 통해 다양한 의견을 수렴하고 결과를 통합한다. 채트로[25]는 LLM을 기반으로 한 중국의 저명한 법률 모델이다. 환각 문제를 완화하기 위해 데이터베이스와 키워드 검색 전략을 모두 지원합니다. 또한, 본 모델은 참조 부정확성의 영향을 완화하여 LLM의 성능을 향상시키기 위해 자기 주의 메커니즘을 사용한다.

**연구 보조자**: 특정 도메인 외에도 LLM 기반 에이전트는 일반 사회 과학 연구 보조자로도 사용할 수 있습니다 [6, 187]. [187]에서 LLM 기반 에이전트는 연구자의 논문 초록 생성, 키워드 추출, 스크립트 생성 등 다양한 업무를 보조하는 데 사용된다. [6]에서 LLM 기반 에이전트는 글쓰기 보조자 역할을 하며, 사회 과학자를 위한 새로운 연구 질문을 식별할 수 있는 능력을 가지고 있다.

### Natural Science

자연과학은 관찰과 실험 8의 경험적 증거에 기초하여 자연현상에 대한 기술, 이해 및 예측에 관한 과학의 한 분야이다. LLM이 발전함에 따라 자연과학에서 LLM 기반 에이전트의 적용이 점점 더 대중화되고 있다. 다음에서는 LLM 기반 에이전트가 중요한 역할을 할 수 있는 많은 대표적인 영역을 제시한다.

각주 8: [https://en.wikipedia.org/wiki/Natural_science](https://en.wikipedia.org/wiki/Natural_science)

**문서 및 데이터 관리**: 자연 과학 연구에는 종종 상당한 양의 문헌 수집, 조직 및 합성이 포함되며, 이는 시간과 인적 자원의 상당한 헌신이 필요합니다. LLM 기반 에이전트는 언어 이해 및 텍스트 처리를 위한 인터넷 및 데이터베이스와 같은 도구를 사용하는 데 강력한 기능을 보여주었다. 이러한 기능을 통해 에이전트는 문서화 및 데이터 관리와 관련된 작업을 탁월하게 수행할 수 있습니다[9; 79; 10]. [9]에서 에이전트는 질의 응답, 실험 계획 등의 작업을 완료하기 위해 인터넷 정보를 효율적으로 질의하고 활용할 수 있다. ChatMOF[79]는 LLM을 활용하여 인간이 작성한 텍스트 설명으로부터 중요한 정보를 추출한다. 그런 다음 금속-유기 프레임워크의 특성과 구조를 예측하기 위한 관련 도구를 적용하는 계획을 공식화한다. ChemCrow [10]은 화학 관련 데이터베이스를 활용하여 화합물 표현의 정밀도를 검증하고 잠재적으로 위험한 물질을 식별한다. 이 기능은 관련된 데이터의 정확성을 보장함으로써 과학적 문의의 신뢰성과 포괄성을 향상시킨다.

**실험 도우미**: LLM 기반 에이전트는 독립적으로 실험을 수행할 수 있어 연구 프로젝트에서 과학자를 지원하는 데 유용한 도구가 됩니다. [9; 10]. 예를 들어 [9]는 과학 실험의 설계, 계획 및 실행을 자동화하기 위해 LLM을 활용하는 혁신적인 에이전트 시스템을 소개한다. 이 시스템은 실험 목표를 입력으로 제공받으면 인터넷에 접속하여 관련 문서를 검색하여 필요한 정보를 수집한다. 이후 파이썬 코드를 활용하여 필수 계산을 수행하고 다음 실험을 수행한다. ChemCrow[10]는 연구자들의 화학 연구를 돕기 위해 특별히 고안된 17개의 신중하게 개발된 도구들을 통합한다. 입력 목표가 수신되면 ChemCrow는 실험 절차에 대한 귀중한 권장 사항을 제공하는 동시에 제안된 실험과 관련된 잠재적인 안전 위험을 강조한다.

**자연 과학 교육**: LLM 기반 에이전트는 인간과 유창하게 통신할 수 있으며 종종 에이전트 기반 교육 도구를 개발하는 데 사용됩니다. [9; 145; 34; 19]. 예를 들어 [9]는 실험 설계, 방법론 및 분석의 학습을 학생들이 용이하게 할 수 있도록 에이전트 기반 교육 시스템을 개발한다. 이 시스템의 목적은 학생들의 비판적 사고와 문제 해결 능력을 향상시키는 동시에 과학적 원리에 대한 더 깊은 이해력을 기르는 것이다. 수학 에이전트[145]는 연구자의 수학적 문제를 탐색, 발견, 해결 및 증명하는 것을 도울 수 있다. 또한 인간과 소통하고 수학을 이해하고 사용하는 것을 도울 수 있다. [34]는 코드X[19]의 기능을 활용하여 대학 수준의 수학 문제를 자동으로 해결하고 설명할 수 있으며, 이는 학생과 연구자를 가르치는 교육 도구로 사용될 수 있다. CodeHelp[95]는 프로그래밍을 위한 교육 에이전트이다. 과정별 키워드 설정, 학생 질의 모니터링, 시스템에 피드백 제공과 같은 많은 유용한 기능을 제공한다. EduChat[27]은 교육 영역을 위해 특별히 설계된 LLM 기반 에이전트이다. 대화를 통해 교사, 학생, 학부모에게 개인화, 공평화, 공감적 교육 지원을 제공한다. 더 나아가 다양한 시스템 프롬프트를 활용하여 환각 문제를 효과적으로 해결하고 실제 교육 시나리오에 원활하게 적응할 수 있다. FreeText[109]는 LLM을 활용하여 개방형 질문에 대한 학생들의 반응을 자동으로 평가하고 피드백을 제공하는 에이전트이다.

### Engineering

LLM 기반 자율 에이전트는 공학 연구 및 응용 프로그램을 지원하고 향상시키는 데 큰 잠재력을 보여주었다. 이 섹션에서는 여러 주요 엔지니어링 영역에서 LLM 기반 에이전트의 적용을 검토하고 요약한다.

**토목공학**: 토목공학에서 LLM 기반 에이전트를 사용하여 건물, 교량, 댐, 도로 등과 같은 복잡한 구조물을 설계 및 최적화할 수 있다[110]. 3D 시뮬레이션 환경에서 인간 설계자와 에이전트가 협력하여 구조를 구성하는 대화형 프레임워크를 제안한다. 인터랙티브 에이전트는 자연어 명령어를 이해하고, 블록을 배치하고, 혼란을 감지하고, 명확성을 찾고, 인간 피드백을 통합할 수 있어 공학 설계에서 인간-AI 협업의 가능성을 보여준다.

**컴퓨터 과학 & 소프트웨어 공학**: 컴퓨터 과학 및 소프트웨어 공학 분야에서 LLM 기반 에이전트는 코딩, 테스트, 디버깅 및 문서 생성을 자동화할 수 있는 잠재력을 제공합니다. [126; 124; 64; 33; 37; 48; 45]. 챗데브[124]는 다수의 에이전트 역할이 자연어 대화를 통해 통신하고 협업하여 소프트웨어 개발 라이프 사이클을 완료하는 엔드 투 엔드 프레임워크를 제안한다. 이 프레임워크는 실행 가능한 소프트웨어 시스템의 효율적이고 비용 효율적인 생성을 보여준다. ToolBench[126]는 코드 자동 완성 및 코드 추천과 같은 작업에 사용될 수 있다. MetaGPT[64]는 코드 생성 프로세스를 감독하고 최종 출력 코드의 품질을 향상시키기 위해 제품 관리자, 건축가, 프로젝트 관리자 및 엔지니어와 같은 여러 역할을 추상화한다. 이를 통해 저가의 소프트웨어 개발이 가능하게 된다. [33] LLM을 이용한 코드 생성을 위한 자체 협업 프레임워크를 제시한다. 이 프레임워크에서 여러 LLM은 특정 하위 작업에 대해 구별되는 "전문가"로 가정한다. 그들은 지정된 지침에 따라 협업하고 상호 작용하며 서로의 작업을 용이하게 하는 가상 팀을 구성합니다. 궁극적으로, 가상 팀은 인간의 개입을 필요로 하지 않고 코드 생성 작업을 협력적으로 처리한다. LLIFT[89]는 정적 분석을 수행하는 것을 돕기 위해, 특히 잠재적인 코드 취약성을 식별하기 위해 LLM을 사용한다. 이 접근 방식은 정확성과 확장성 간의 균형을 효과적으로 관리합니다. ChatEDA[63]은 작업 계획, 스크립트 생성, 실행을 통합하여 설계 프로세스를 효율화하기 위해 전자 설계 자동화(EDA)를 위해 개발된 에이전트이다. 코드 도움말 [95]는 학생과 개발자가 코드를 디버깅하고 테스트할 수 있도록 설계된 에이전트입니다. 오류 메시지에 대한 자세한 설명 제공, 잠재적인 수정 제안, 코드의 정확성 보장 등이 특징이다. PENTESTGPT [29]는 LLMs를 기반으로 한 침투 테스트 도구로, 공통 취약점을 효과적으로 식별하고 소스 코드를 해석하여 악용 프로그램을 개발할 수 있다. DB-GPT [182]는 데이터베이스에서 이상 현상의 잠재적인 근본 원인을 체계적으로 평가하기 위해 LLM의 기능을 활용한다. DB-GPT는 사고 트리의 구현을 통해 LLM이 현재 단계가 성공하지 못한 것으로 판명될 경우 이전 단계로 역추적할 수 있도록 하여 진단 프로세스의 정확성을 향상시킨다.

**산업 자동화**: 산업 자동화 분야에서 LLM 기반 에이전트를 사용하여 지능형 계획 및 생산 프로세스 제어를 달성할 수 있습니다. [161] 대형 언어 모델(LLM)과 디지털 트윈 시스템을 통합하여 유연한 생산 요구를 수용하는 새로운 프레임워크를 제안합니다. 프레임워크는 신속한 엔지니어링 기법을 활용하여 디지털 트윈이 제공하는 정보를 기반으로 특정 작업에 적응할 수 있는 LLM 에이전트를 생성한다. 이러한 에이전트는 일련의 원자 기능과 기술을 조정하여 자동화 피라미드 내에서 다양한 수준에서 생산 작업을 완료할 수 있다. 이 연구는 LLM을 산업 자동화 시스템에 통합하여 보다 민첩하고 유연하며 적응적인 생산 프로세스를 위한 혁신적인 솔루션을 제공할 가능성을 보여준다. IELLM[119]는 석유 및 가스 산업의 문제를 해결하는 LLM의 효과에 대한 포괄적인 사례 연구를 제시한다. 암석 물리 모델링, 음향 반사 측정 및 코일형 튜브 제어를 포함한 다양한 응용 분야에 중점을 둡니다.

**Robotics & Embodied Artificial Intelligence**: 최근 작업에서 로봇 공학에 대한 보다 효율적인 강화 학습 에이전트를 개발하고 인공 지능을 구현했습니다. [28, 181, 118, 160, 148, 184, 66, 159, 174, 32, 2]. 특히, [28]은 구체화된 환경에서 계획, 추론 및 협업에 대한 자율 에이전트의 능력을 향상시키는 데 중점을 둔다. 구체화된 추론 및 작업 계획을 위한 통합 에이전트 시스템을 제안한다. 이 시스템에서 저자는 개선된 계획을 가능하게 하기 위해 높은 수준의 명령을 설계하고 낮은 수준의 컨트롤러를 사용하여 정보를 수집한다. [181]은 최적화 프로세스를 가속화하기 위해 정보를 수집할 수 있다. [118, 160]은 구체화된 의사 결정 및 탐색을 위해 자율 에이전트를 사용한다. 물리적 제약을 극복하기 위해 에이전트는 실행 가능한 계획을 생성하고 다중 기술을 활용하여 장기 작업을 수행할 수 있다. 제어 정책 측면에서 SayCan [2]는 이동식 매니퓰레이터 로봇을 사용하여 광범위한 조작 및 내비게이션 기술을 조사하는 데 중점을 둔다. 주방 환경에서 접하는 일반적인 작업에서 영감을 받아 7개의 기술 패밀리와 17개의 객체를 포함하는 551개의 포괄적인 기술 세트를 제시한다. 이러한 기술은 무엇보다도 사물 선택, 배치, 붓기, 파지 및 조작과 같은 다양한 동작을 포함한다. TidyBot [157]은 가정 청소 작업을 개인화하도록 설계된 구체화된 에이전트이며 텍스트 예제를 통해 사물 배치 및 조작 방법에 대한 사용자의 선호도를 학습할 수 있다.

LLM 기반 자율 에이전트의 적용을 촉진하기 위해 연구자들은 또한 많은 오픈 소스 라이브러리를 도입했으며, 이를 기반으로 개발자는 맞춤형 요구 사항에 따라 에이전트를 신속하게 구현하고 평가할 수 있다[49, 47, 42, 44, 39, 40, 46, 16, 36, 43, 38, 125, 52, 45, 41, 50, 158]. 예를 들어 LangChain [16]은 코딩, 테스트, 디버깅 및 문서 생성 작업을 자동화하는 오픈 소스 프레임워크입니다. 랭체인은 언어 모델을 데이터 소스와 통합하고 환경과의 상호 작용을 촉진함으로써, 여러 에이전트 역할 간의 자연어 통신 및 협업을 통해 효율적이고 비용 효율적인 소프트웨어 개발을 가능하게 한다. LangChain을 기반으로 XLang [40]에는 포괄적인 도구 집합, 완전한 사용자 인터페이스가 제공되며 데이터 처리, 플러그 인 사용 및 웹 에이전트의 세 가지 다른 에이전트 시나리오를 지원합니다. AutoGPT[49]는 완전 자동화된 에이전트이다. 하나 이상의 목표를 설정하고 해당 작업으로 분할하고 목표가 달성될 때까지 작업을 순환합니다. WorkGPT[36]은 AutoGPT 및 LangChain과 유사한 에이전트 프레임워크이다. 인스트럭션과 API 세트를 제공함으로써, 인스트럭션이 완료될 때까지 AI와의 전후 대화를 진행한다. GPT-Engineer[37], SmolModel[48], DemoGPT[45]는 개발 작업을 완료하기 위한 프롬프트를 통해 코드 생성을 자동화하는 데 중점을 둔 오픈 소스 프로젝트이다. AGiXT[44]는 많은 제공자에 걸쳐 효율적인 AI 명령 관리 및 작업 실행을 조정하도록 설계된 동적 AI 자동화 플랫폼이다. AgentVerse[39]는 연구원들이 맞춤형 LLM 기반 에이전트 시뮬레이션을 효율적으로 생성하는 것을 용이하게 하는 다용도 프레임워크이다. GPT 연구자[38]는 대규모 언어 모델을 활용하여 연구 문제를 효율적으로 개발하고, 웹 크롤링을 트리거하여 정보를 수집하고, 출처를 요약하고, 요약을 집계하는 실험 애플리케이션이다. BMTools [125]는 도구를 사용 하 여 LLM을 확장 하 고 커뮤니티 기반 도구 빌드 및 공유를 위한 플랫폼을 제공 하는 오픈 소스 리포지토리입니다. 다양한 유형의 도구를 지원하고, 여러 도구를 사용하여 동시 작업 실행이 가능하며, URL을 통해 플러그인을 로드할 수 있는 간단한 인터페이스를 제공하여 개발 및 BMTools 생태계에 대한 기여를 용이하게 한다.

_Remark_.: 상기 애플리케이션들을 지원하는 데 있어서 LLM 기반 에이전트들의 활용은 또한 특정 위험들 및 도전들을 수반할 수 있다. 한편으로 LLM 자체는 착시 및 기타 문제에 취약할 수 있으며 간혹 잘못된 답변을 제공하여 잘못된 결론, 실험적 실패 또는 위험한 실험에서 인간 안전에 위험을 초래할 수 있다. 따라서 실험 중 사용자는 적절한 주의를 수행하기 위해 필요한 전문 지식과 지식을 보유해야 한다. 반면에, LLM 기반 에이전트는 잠재적으로 화학 무기 개발과 같은 악의적인 목적으로 악용될 수 있으며, 책임 있고 윤리적인 사용을 보장하기 위해 인간 정렬과 같은 보안 조치의 구현이 필요하다.

요약하면, 위의 섹션에서 우리는 세 가지 중요한 영역에서 LLM 기반 자율 에이전트의 일반적인 응용 프로그램을 소개한다. 보다 명확한 이해를 위해 이전 작업과 해당 응용 프로그램 간의 대응 관계를 표 2에 요약한다.

## 4 LLM 기반 자율 에이전트 평가

LLM 자체와 마찬가지로 LLM 기반 자율 에이전트의 효과를 평가하는 것은 어려운 작업이다. 이 절에서는 일반적으로 사용되는 두 가지 평가 전략, 즉 주관적 평가와 객관적 평가를 소개한다(개요는 그림 5의 오른쪽 부분을 참조).

### Subjective Evaluation

주관적 평가는 인간의 판단에 기초하여 에이전트 능력을 측정한다[85; 122; 121; 5; 176]. 평가 데이터 세트가 없거나 예를 들어 에이전트의 지능 또는 사용자 친화성을 평가하는 것과 같은 정량적 메트릭을 설계하는 것이 매우 어려운 시나리오에 적합하다. 다음에서는 주관적 평가를 위해 일반적으로 사용되는 두 가지 전략을 제시한다.

**인간 주석**: 이 방법에서 인간 평가자는 다른 에이전트에서 생성된 결과에 직접 점수를 매기거나 순위를 매깁니다[187; 5; 176]. 예를 들어 [121]에서 저자는 많은 주석자를 고용하고 에이전트 기능과 직접 관련된 5가지 주요 질문에 대한 피드백을 제공하도록 요청한다. [84]에서 저자는 인간에게 무해, 정직, 도움이 되는, 참여 및 편향되지 않은 모델에 점수를 매기도록 요청하여 모델 효과를 평가한 다음 다른 모델의 결과를 비교한다. [122]에서 저자는 주석자에게 설계된 모델이 온라인 커뮤니티에 대한 규칙 설계를 개선하는 데 효과적으로 기여할 수 있는지 여부에 대한 답변을 요청한다.

**튜링 테스트**: 이 방법에서는 에이전트에서 생성된 결과와 실제 사람을 구분하기 위해 인간 평가자가 필요합니다. 주어진 작업에서 평가자가 에이전트와 인간 결과를 분리할 수 없는 경우 에이전트가 이 작업에서 인간과 유사한 성능을 달성할 수 있음을 보여준다. [5]에서 저자는 자유 형식 빨치산 텍스트에 대한 실험을 수행하고 인간 평가자는 응답이 인간 또는 LLM 기반 에이전트에서 나온 것인지 추측하도록 요청한다. [121]에서, 인간 평가자들은 행동들이 에이전트들 또는 실제-인간들로부터 생성되는지를 식별하기 위해 요구된다. [68]에서 저자는 LLM 소프트웨어와 다른 상황에서 인간 피험자의 감정 상태에 대한 인간 주석을 수집하는 연구를 수행한다. 그들은 이러한 주석을 기반으로 LLM 소프트웨어의 정서적 견고성을 평가했다.

_Remark_. LLM 기반 에이전트는 일반적으로 인간에게 서비스를 제공하도록 설계되었다. 따라서 주관적인 에이전트 평가는 인간의 기준을 반영하기 때문에 중요한 역할을 한다. 그러나 이 전략은 또한 높은 비용, 비효율성 및 인구 편향과 같은 문제에 직면해 있다. 이러한 문제를 해결하기 위해 많은 연구자들은 LLM을 프록시로 활용하여 주관적인 평가를 수행하는 방법을 탐구했다. 예를 들어, ChemCrow [10]에서 연구자들은 GPT를 사용하여 실험 결과를 평가한다. 그들은 작업의 완료와 기본 프로세스의 정확성을 모두 고려합니다. ChatEval [13]은 후보 모델에 의해 생성된 결과를 토론 방식으로 평가하기 위해 여러 에이전트를 사용한다. LLM이 발전함에 따라 이러한 평가 방법은 더 많은 응용 분야에서 더 신뢰할 수 있고 적용될 수 있다고 믿는다.

### Objective Evaluation

객관적 평가는 시간에 따라 계산, 비교 및 추적될 수 있는 정량적 메트릭을 사용하여 LLM 기반 자율 에이전트의 능력을 평가하는 것을 말한다. 주관적 평가와 달리 객관적인 지표는 에이전트 성능에 대한 구체적이고 측정 가능한 통찰력을 제공하는 것을 목표로 한다. 객관적인 평가를 수행하기 위해서는 세 가지 중요한 측면, 즉 평가 메트릭, 프로토콜 및 벤치마크가 있다. 이하에서는 이러한 측면을 좀 더 구체적으로 소개한다.

**메트릭**: 에이전트의 효과를 객관적으로 평가하기 위해 적절한 메트릭을 설계하는 것이 중요하며, 이는 평가 정확성과 포괄성에 영향을 미칠 수 있습니다. 이상적인 평가 메트릭은 에이전트의 품질을 정확하게 반영하고 실제 시나리오에서 사용할 때 인간의 느낌과 일치해야 한다. 기존 연구에서 우리는 다음과 같은 대표적인 평가 지표를 결론지을 수 있다. (1) _작업 성공 메트릭:_ 이러한 메트릭은 에이전트가 작업을 완료하고 목표를 달성할 수 있는 정도를 측정합니다. 공통 메트릭은 성공률[176; 170; 139; 100], 보상/점수[176, 170, 110], 커버리지[184], 및 정확도[124, 1, 67]를 포함한다. 값이 높을수록 작업 완료 능력이 높아집니다. (2) _인간 유사성 메트릭:_ 이러한 메트릭은 에이전트 행동이 인간의 행동과 매우 유사한 정도를 정량화 합니다. 전형적인 예들은 궤적/위치 정확도[17, 148], 대화 유사성[122, 1], 및 인간 반응들의 모방물[1, 5]을 포함한다. 유사도가 높을수록 더 나은 인간 시뮬레이션 성능을 제안한다. (3) _효율성 메트릭:_ 에이전트 유효성을 평가하는 데 사용되는 전술한 메트릭과 대조적으로, 이러한 메트릭은 에이전트 효율을 평가한다. 전형적인 메트릭들은 계획 길이[100], 개발 비용[124], 추론 속도[184, 148], 및 명확화 대화들의 수[110]를 포함한다.

**프로토콜**: 평가 메트릭 외에도 객관적인 평가를 위한 또 다른 중요한 측면은 이러한 메트릭을 활용하는 방법입니다. 이전 연구에서 우리는 일반적으로 사용되는 다음과 같은 평가 프로토콜을 식별할 수 있다. (1) _실제 시뮬레이션:_ 이 방법에서 에이전트는 게임 및 대화형 시뮬레이터와 같은 몰입 환경 내에서 평가된다. 에이전트는 자율적으로 작업을 수행해야 하며, 작업 성공률 및 인간 유사성과 같은 메트릭을 활용한다.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Model & Subjective & Objective & Benchmark & Time \\ \hline WebShop [168] & - & \⃃1 & \(\checkmark\) & 07/2022 \\ Social Simulacra [122] & \�1 & \(\checkmark\) & - & 08/2022 \\ TE [1] & - & \(\checkmark\) & - & 08/2022 \\ LIBRO [78] & - & \(\checkmark\) & - & 09/2022 \\ ReAct [170] & - & \�1 & \(\checkmark\) & 10/2022 \\ Out of One, Many [5] & \(\checkmark\) & \(\checkmark\) & - & 02/2023 \\ DEPS [154] & - & \(\checkmark\) & \(\checkmark\) & 02/2023 \\ Jalil et al. [73] & - & \(\checkmark\) & - & 02/2023 \\ Reflexion [139] & - & \�1 & - & 03/2023 \\ IGLU [110] & - & \(\checkmark\) & \(\checkmark\) & 04/2023 \\ Generative Agents [121] & \�1 & - & - & 04/2023 \\ ToolBench [125] & - & \(\checkmark\) & \(\checkmark\) & 04/2023 \\ GITM [184] & - & \(\checkmark\) & \(\checkmark\) & 05/2023 \\ Two-Failures [17] & - & \(\checkmark\) & - & 05/2023 \\ Voyager [148] & - & \(\checkmark\) & \(\checkmark\) & 05/2023 \\ SocKET [23] & - & \(\checkmark\) & \(\checkmark\) & 05/2023 \\ MobileEnv [175] & - & \(\checkmark\) & \(\checkmark\) & 05/2023 \\ Clembench [12] & - & \(\checkmark\) & \(\checkmark\) & 05/2023 \\ Dialop [98] & - & \(\checkmark\) & \(\checkmark\) & 06/2023 \\ Feldt et al. [53] & - & \(\checkmark\) & - & 06/2023 \\ CO-LLM [176] & \�1 & \�1 & - & 07/2023 \\ Tachikuma [94] & \�1 & \�1 & \(\checkmark\) & 07/2023 \\ WebArena [180] & - & \�1 & \(\checkmark\) & 07/2023 \\ RocoBench [108] & - & \�1 & - & 07/2023 \\ AgentSims [99] & - & \�2 & - & 08/2023 \\ AgentBench [103] & - & \�3 & \(\checkmark\) & 08/2023 \\ BOLAA [104] & - & \�1 & \(\checkmark\) & 08/2023 \\ Gentopia [163] & - & \�3 & \(\checkmark\) & 08/2023 \\ EmotionBench [68] & \�1 & - & \(\checkmark\) & 08/2023 \\ PTB [29] & - & \�1 & - & 08/2023 \\ \hline \hline \end{tabular}
\end{table}
표 3: LLM 기반 자율 에이전트의 평가 전략에 대한 요약(더 많은 에이전트는 [https://github.com/Paitesanshi/LLM-Agent-Survey](https://github.com/Paitesanshi/LLM-Agent-Survey))에서 볼 수 있다. 주관적 평가를 위해 �1 및 �1을 사용하여 각각 인간 주석 및 튜링 테스트를 나타낸다. 객관적 평가를 위해 �1, �2, �3 및 �1을 사용하여 환경 시뮬레이션, 사회 평가, 다중 작업 평가 및 소프트웨어 테스트를 각각 나타낸다. “\(\checkmark\)”는 평가들이 벤치마크들에 기초한다는 것을 나타낸다.

에이전트들의 궤적들 및 완료된 목적들에 기초하여 에이전트들의 능력을 평가한다[17, 176, 184, 170, 148, 110, 154, 94, 168, 175]. 이 방법은 실제 시나리오에서 에이전트의 실제 능력을 평가할 것으로 기대된다. (2) _사회적 평가:_ 이 방법은 시뮬레이션된 사회에서 에이전트 상호 작용을 기반으로 사회 지능을 평가하기 위해 메트릭을 활용한다. 팀워크 기술을 평가하기 위한 협력 작업, 논증적 추론을 분석하기 위한 토론, 사회적 적성을 측정하기 위한 인간 연구[122, 1, 23, 99, 104]와 같은 다양한 접근법이 채택되었다. 이러한 접근법은 협동, 의사소통, 공감, 인간 사회적 행동을 포함한 영역에서 에이전트의 능력을 평가하기 위해 일관성, 마음 이론, 사회적 IQ와 같은 특성을 분석한다. 에이전트에 복잡한 상호 작용 설정을 적용함으로써 사회적 평가는 에이전트의 더 높은 수준의 사회적 인지에 대한 귀중한 통찰력을 제공한다. (3) _멀티-태스크 평가:_ 이 방법에서, 사람들은 에이전트를 평가하기 위해 상이한 도메인들로부터의 다양한 태스크들의 세트를 사용하며, 이는 개방형 도메인 환경에서 에이전트 일반화 능력을 효과적으로 측정할 수 있다[5, 23, 103, 104, 168, 175]. (4) _소프트웨어 테스트:_ 이 방법에서, 연구자들은 에이전트들이 테스트 케이스 생성, 버그 재생, 디버깅 코드, 개발자들 및 외부 도구들과 상호작용하는 것과 같은 소프트웨어 테스트 태스크들과 같은 태스크들을 수행하게 함으로써 에이전트들을 평가한다[73, 78, 53, 104]. 그런 다음 테스트 범위 및 버그 탐지율과 같은 메트릭을 사용하여 LLM 기반 에이전트의 효과를 측정할 수 있습니다.

**벤치마크**: 메트릭 및 프로토콜을 고려할 때 중요한 남은 측면은 평가를 수행하기 위한 적절한 벤치마크를 선택하는 것입니다. 과거에는 사람들이 실험에 다양한 벤치마크를 사용했습니다. 예를 들어, 많은 연구자들은 에이전트 능력을 평가하기 위한 벤치마크로서 ALFWorld[170], IGLU[110], 및 Minecraft[184, 148, 154]와 같은 시뮬레이션 환경을 사용한다. Tachikuma [94]는 TRP 게임 로그를 활용하여 여러 캐릭터와 새로운 객체와의 복잡한 상호작용을 이해하고 추론할 수 있는 에이전트로서의 LLMs를 평가하는 벤치마크이다. AgentBench [103]는 다양한 도메인에 걸쳐 자율적인 에이전트로서의 LLMs를 평가하는 포괄적인 프레임워크를 제공한다. SocKET [23]은 LLM 기반 에이전트의 사회적 능력을 평가하기 위한 첫 번째 체계적인 프레임워크이다. AgentSims [99]는 일반적인 도구 사용 능력을 가진 강력한 LLMs 개발을 지원하는 오픈 소스 프로젝트이다. WebShop [168]은 LLM 기반 에이전트의 제품 검색 및 검색을 위한 능력 측면에서 LLM 기반 에이전트를 평가하는 개방형 플랫폼을 제공한다. WebArena [180]은 툴을 사용하여 다양한 도메인에 걸친 LLM 기반 에이전트의 학습, 서비스 및 평가를 위한 개방형 플랫폼을 제공한다. E2E [7]은 챗봇의 정확성과 유용성을 테스트하기 위한 엔드 투 엔드 벤치마크이다.

_Remark_. 객관적 평가는 다양한 메트릭을 사용하여 LLM 기반 에이전트 기능의 정량적 평가를 가능하게 한다. 현재의 기술은 모든 유형의 에이전트 기능을 완벽하게 측정할 수는 없지만 객관적인 평가는 주관적인 평가를 보완하는 필수적인 통찰력을 제공한다. 객관적 평가 벤치마크 및 방법론의 진행 중인 진행은 LLM 기반 자율 에이전트의 개발 및 이해를 더욱 발전시킬 것이다.

위 절에서는 LLM 기반 자율 에이전트 평가를 위한 주관적 전략과 객관적 전략을 모두 소개한다. 에이전트의 평가는 이 영역에서 중요한 역할을 한다. 그러나 주관적 평가와 객관적 평가 모두 나름의 장단점을 가지고 있다. 아마도 실제로 그들은 에이전트를 종합적으로 평가하기 위해 결합되어야 할 것이다. 이전 작업과 이러한 평가 전략 간의 대응 관계를 표 3에 요약하였다.

## 5 관련 조사

대형 언어 모델의 활발한 발전과 함께, 수많은 종합 설문조사가 등장하여 다양한 측면에 대한 상세한 통찰력을 제공하고 있다. [178] 기존의 방대한 작품을 아우르는 LLM의 배경, 주요 연구 결과 및 주류 기술을 확장적으로 소개한다. 한편, [166]은 주로 다양한 다운스트림 태스크에서 LLM의 적용 및 그 배치와 관련된 과제에 초점을 맞춘다. LLM을 인간 지능과 정렬하는 것은 편향 및 환상과 같은 우려를 해결하기 위한 활발한 연구 영역이다. [153] 데이터 수집 및 모델 훈련 방법론을 포함하여 인간 정렬을 위한 기존 기술을 컴파일했다. 추론은 지능의 중요한 측면으로 의사 결정, 문제 해결 및 기타 인지 능력에 영향을 미친다. [69] 는 LLMs의 추론 능력에 대한 현재 연구 현황을 나타내며, 이들의 추론 능력을 향상시키고 평가하기 위한 접근법을 탐색한다. [111] 언어 모델은 추론 능력과 도구 활용 능력으로 향상될 수 있다고 제안한다. 그들은 ALM의 최신 발전에 대한 포괄적인 검토를 수행합니다. 대규모 모델의 활용이 보편화됨에 따라, 그 성능을 평가하는 것이 점점 더 중요해지고 있다. [15] LLM 평가, 무엇을 평가할지, 어디서 평가할지, 그리고 다운스트림 작업 및 사회적 영향에서의 그들의 성능을 평가하는 방법을 다루는 것에 대해 조명한다. [14] 또한 다양한 다운스트림 작업에서 LLM의 기능과 한계에 대해 논의한다. 앞서 언급한 연구는 훈련, 적용, 평가 등 대형 모델의 다양한 측면을 포괄한다. 그러나 이 논문 이전에는 LLM 기반 에이전트의 빠르게 부상하고 유망한 분야에 특별히 초점을 맞춘 작업이 없었다. 본 연구에서는 LLM 기반 에이전트의 구축, 적용 및 평가 프로세스를 포함하여 100개의 관련 작업을 수집했다.

## 6 Challenges

LLM 기반 자율 에이전트에 대한 이전 작업은 많은 놀라운 성공을 거두었지만 이 분야는 아직 초기 단계에 있으며 개발에서 해결해야 할 몇 가지 중요한 과제가 있다. 다음에서는 대표적인 여러 가지 과제를 제시한다.

### Role-playing Capability

기존의 LLM과 달리, 자율 에이전트는 일반적으로 다른 작업을 수행하기 위해 특정 역할(예: 프로그램 코더, 연구원 및 화학자)을 수행해야 한다. 따라서, 역할극을 위한 에이전트의 능력은 매우 중요하다. LLM은 영화 리뷰어와 같은 많은 공통 역할을 효과적으로 시뮬레이션할 수 있지만 여전히 정확하게 캡처하기 위해 고군분투하는 다양한 역할과 측면이 있다. 우선, LLM은 일반적으로 웹 코퍼스를 기반으로 훈련되기 때문에 웹에서 거의 논의되지 않는 역할이나 새롭게 등장하는 역할에 대해서는 LLM이 잘 시뮬레이션되지 않을 수 있다. 또한, 선행 연구[54]는 기존의 LLM이 인간의 인지 심리 캐릭터를 잘 모델링하지 못하여 대화 시나리오에서 자기 인식이 부족할 수 있음을 보여주었다. 이러한 문제들에 대한 잠재적인 해결책은 LLM들을 미세 조정하거나 에이전트 프롬프트들/아키텍처들을 신중하게 설계하는 것을 포함할 수 있다[87]. 예를 들어, 처음에는 흔하지 않은 역할이나 심리학 캐릭터에 대한 실제 인간 데이터를 수집한 다음 LLM을 미세 조정하기 위해 이를 활용할 수 있다. 그러나 미세 조정된 모델이 여전히 공통 역할에 대해 잘 수행되도록 하는 방법은 추가 문제를 제기할 수 있다. 미세 조정 외에도 롤플레잉에서 LLM의 기능을 향상시키기 위해 맞춤형 에이전트 프롬프트/아키텍처를 설계할 수도 있습니다. 그러나 설계 공간이 너무 크기 때문에 최적의 프롬프트/아키텍처를 찾는 것은 쉽지 않습니다.

### 일반화된 인간 정렬

인간 정렬은 전통적인 LLM에 대해 많이 논의되어 왔다. LLM 기반 자율 에이전트 분야에서, 특히 에이전트가 시뮬레이션에 활용될 때, 우리는 이 개념이 더 깊이 논의되어야 한다고 믿는다. 인간에게 더 나은 서비스를 제공하기 위해 전통적인 LLM은 일반적으로 올바른 인간 가치와 일치하도록 미세 조정되며, 예를 들어 에이전트는 복수를 위한 폭탄을 만들 계획을 세워서는 안 된다. 그러나 에이전트가 실제 시뮬레이션에 활용될 때 이상적인 시뮬레이터는 잘못된 값을 가진 특성을 포함하여 다양한 인간 특성을 정직하게 묘사할 수 있어야 한다. 실제로 인간의 부정적인 측면을 시뮬레이션하는 것은 훨씬 더 중요할 수 있는데, 시뮬레이션의 중요한 목표는 문제를 발견하고 해결하는 것이고 부정적인 측면이 없다는 것은 해결해야 할 문제가 없다는 것을 의미하기 때문이다. 예를 들어, 실제 사회를 시뮬레이션하기 위해, 우리는 에이전트가 폭탄을 만들기 위한 계획을 세울 수 있도록 해야 할 수도 있고, 계획을 실행하기 위해 어떻게 작용할 것인지, 그리고 그것의 행동의 영향을 관찰해야 할 수도 있습니다. 이러한 관찰에 기초하여 사람들은 실제 사회에서 유사한 행동을 멈추기 위해 더 나은 행동을 할 수 있다. 위의 사례에서 영감을 받아 에이전트 기반 시뮬레이션의 중요한 문제는 일반화된 인간 정렬을 수행하는 방법, 즉 다른 목적과 응용 분야에서 에이전트가 다양한 인간 가치와 정렬할 수 있어야 한다는 것이다. 그러나 ChatGPT와 GPT-4를 포함한 기존의 강력한 LLM은 대부분 통합된 인간 가치와 일치한다. 따라서 흥미로운 방향은 적절한 촉진 전략을 설계하여 이러한 모델을 "재배치"하는 방법이다.

### Prompt Robustness

에이전트에서 합리적인 행동을 보장하기 위해 설계자는 종종 메모리 및 계획 모듈과 같은 추가 모듈을 LLM에 통합한다. 그러나, 이러한 모듈들을 포함시키는 것은 일관된 동작 및 효과적인 통신을 용이하게 하기 위해 더 많은 프롬프트들의 개발을 필요로 한다. 이전 연구[186; 57]는 사소한 변경도 실질적으로 다른 결과를 산출할 수 있기 때문에 LLM에 대한 프롬프트의 견고성 부족을 강조했다. 이 문제는 자율 에이전트가 단일 프롬프트가 아니라 모든 모듈을 고려하는 프롬프트 프레임워크를 포괄하기 때문에 자율 에이전트를 구성할 때 더 두드러지며, 여기서 하나의 모듈에 대한 프롬프트는 다른 모듈에 영향을 미칠 가능성이 있다. 더욱이, 프롬프트 프레임워크들은 상이한 LLM들에 걸쳐 상당히 달라질 수 있다. 다양한 LLM에 적용될 수 있는 통일되고 강력한 프롬프트 프레임워크를 개발하는 것은 중요하지만 해결되지 않은 문제이다. 앞서 언급한 문제에 대한 두 가지 잠재적인 해결책은 (1) 시행착오를 통해 필수 프롬프트 요소를 수동으로 만들거나 (2) GPT를 사용하여 프롬프트를 자동으로 생성하는 것이다.

### Hallucination

환각은 모델이 자신 있게 잘못된 정보를 잘못 출력하는 LLM에 근본적인 문제를 제기한다. 이 문제는 자율 에이전트에서도 널리 퍼져 있습니다. 예를 들어, [74]에서, 코드 생성 작업들 동안 단순 지시들에 직면할 때, 에이전트는 환각적 거동을 보일 수 있다는 것이 관찰되었다. 환각은 부정확하거나 오해의 소지가 있는 코드, 보안 위험 및 윤리적 문제와 같은 심각한 결과를 초래할 수 있다[74]. 이 문제를 해결하기 위해, 한 가지 가능한 접근법은 인간-에이전트 상호작용의 루프 내에 인간 보정 피드백을 통합하는 것이다[64]. 환각 문제에 대한 더 많은 논의는 [178]에서 확인할 수 있다.

### Knowledge Boundary

LLM 기반 자율 에이전트의 중요한 응용은 상이한 실제 인간 행동을 시뮬레이션하는 것이다[121]. 인간 시뮬레이션에 대한 연구는 오랜 역사를 가지고 있으며 최근 관심이 급증한 것은 LLM이 인간 행동을 시뮬레이션하는 데 상당한 능력을 입증한 놀라운 발전에 기인한다고 할 수 있다. 그러나, LLM들의 전력이 항상 유리한 것은 아닐 수 있다는 것을 인식하는 것이 중요하다. 특히 이상적인 시뮬레이션은 인간의 지식을 정확하게 복제해야 한다. 이와 관련하여 LLM은 일반 개인의 범위를 능가하는 광범위한 웹 지식 코퍼스에 대해 훈련되기 때문에 과도한 힘을 발휘할 수 있다. LLM의 엄청난 능력은 시뮬레이션의 효과에 상당한 영향을 미칠 수 있다. 예를 들어, 다양한 영화에 대한 사용자 선택 행동을 시뮬레이션하려고 할 때 LLM이 이러한 영화에 대한 사전 지식이 없는 위치를 가정하도록 하는 것이 중요하다. 그러나, LLM이 이미 이들 영화에 관한 정보를 취득했을 가능성이 있다. 적절한 전략을 구현하지 않고 LLM은 실제 사용자가 이러한 영화의 콘텐츠에 미리 액세스할 수 없더라도 광범위한 지식을 기반으로 결정을 내릴 수 있다. 위의 예를 통해, 우리는 신뢰할 수 있는 에이전트 시뮬레이션 환경을 구축하기 위해 LLM에 대한 사용자 미지의 지식의 활용을 제한하는 방법이 중요한 문제라는 결론을 내릴 수 있다.

### Efficiency

자기회귀 아키텍처 때문에, LLM은 일반적으로 느린 추론 속도를 갖는다. 그러나, 에이전트는 메모리 모듈로부터 정보를 추출하는 것, 액션을 취하기 전에 계획을 세우는 것 등과 같이 각 액션에 대해 LLM을 여러 번 쿼리할 필요가 있을 수 있다. 결과적으로 에이전트 액션의 효율성은 LLM 추론 속도에 크게 영향을 받는다. 동일한 API 키를 사용 하 여 여러 에이전트를 배포 하면 시간 비용이 더욱 크게 증가할 수 있습니다.

Conclusion

본 조사에서는 LLM 기반 자율 에이전트 분야의 기존 연구를 체계적으로 정리한다. 본 연구는 에이전트의 구성, 적용 및 평가를 포함한 세 가지 측면에서 이러한 연구를 제시하고 검토한다. 이러한 각 측면에 대해 주요 기술과 개발 역사를 요약하여 기존 연구 간의 연결을 도출하기 위한 세부 분류법을 제공한다. 이전 작업을 검토하는 것 외에도 이 분야에서 잠재적인 미래 방향을 안내할 것으로 예상되는 몇 가지 과제도 제안한다.

## References

* [1] Gati V Aher, Rosa I Arriaga, and Adam Tauman Kalai. Using large language models to simulate multiple humans and replicate human subject studies. In _International Conference on Machine Learning_, pages 337-371. PMLR, 2023.
* [2] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say: Grounding language in robotic affordances. _arXiv preprint arXiv:2204.01691_, 2022.
* [3] Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, and Eric Schulz. Playing repeated games with large language models. _arXiv preprint arXiv:2305.16867_, 2023.
* [4] Anthropic. Model card and evaluations for claude models. [https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf?ref=maginative.com](https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf?ref=maginative.com), 2023.
* [5] Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R Gubler, Christopher Rytting, and David Wingate. Out of one, many: Using language models to simulate human samples. _Political Analysis_, 31(3):337-351, 2023.
* [6] Christopher A Bail. Can generative AI improve social science? _SocArXiv_, 2023.
* [7] Debarag Banerjee, Pooja Singh, Arjun Avadhanam, and Saksham Srivastava. Benchmarking lm powered chatbots: Methods and metrics, 2023.
* [8] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. _arXiv preprint arXiv:2308.09687_, 2023.
* [9] Daniil A Boiko, Robert MacKnight, and Gabe Gomes. Emergent autonomous scientific research capabilities of large language models. _arXiv preprint arXiv:2304.05332_, 2023.
* [10] Andres M Bran, Sam Cox, Andrew D White, and Philippe Schwaller. ChemCrow: Augmenting large-language models with chemistry tools. _arXiv preprint arXiv:2304.05376_, 2023.
* [11] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _arXiv preprint arXiv:2005.14165_, 2020.
* [12] Kranti Chalamalesetti, Jana Gotze, Sherzod Hakimov, Brielen Madureira, Philipp Sadler, and David Schlangen. clembench: Using game play to evaluate chat-optimized language models as conversational agents. _arXiv preprint arXiv:2305.13455_, 2023.
* [13] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. ChatEval: Towards better LLM-based evaluators through multi-agent debate. _arXiv preprint arXiv:2308.07201_, 2023.
* [14] Tyler A Chang and Benjamin K Bergen. Language model behavior: A comprehensive survey. _arXiv preprint arXiv:2303.11504_, 2023.
* [15] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. _arXiv preprint arXiv:2307.03109_, 2023.
* [16] Harrison Chase. langchain. [https://docs.langchain.com/docs/](https://docs.langchain.com/docs/), 2023.
* [17] Angelica Chen, Jason Phang, Alicia Parrish, Vishakh Padmakumar, Chen Zhao, Samuel R Bowman, and Kyunghyun Cho. Two failures of self-consistency in the multi-step reasoning of LLMs. _arXiv preprint arXiv:2305.14279_, 2023.

* [18] Liting Chen, Lu Wang, Hang Dong, Yali Du, Jie Yan, Fangkai Yang, Shuang Li, Pu Zhao, Si Qin, Saravan Rajmohan, et al. Introspective tips: Large language model for in-context decision making. _arXiv preprint arXiv:2305.11598_, 2023.
* [19] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. _arXiv preprint arXiv:2107.03374_, 2021.
* [20] Po-Lin Chen and Cheng-Shang Chang. InterAct: Exploring the potentials of ChatGPT as a cooperative agent. _arXiv preprint arXiv:2308.01552_, 2023.
* [21] Xinshi Chen, Shuang Li, Hui Li, Shaohua Jiang, Yuan Qi, and Le Song. Generative adversarial user model for reinforcement learning based recommendation system. In _International Conference on Machine Learning_, pages 1052-1061. PMLR, 2019.
* [22] Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin Zhao, and Ji-Rong Wen. ChatCoT: Tool-augmented chain-of-thought reasoning on chat-based large language models. _arXiv preprint arXiv:2305.14323_, 2023.
* [23] Minjie Choi, Jiaxin Pei, Sagar Kumar, Chang Shu, and David Jurgens. Do LLMs understand social knowledge? evaluating the sociability of large language models with socket benchmark. _arXiv preprint arXiv:2305.14938_, 2023.
* [24] Cedric Colas, Laetitia Teodorescu, Pierre-Yves Oudeyer, Xingdi Yuan, and Marc-Alexandre Cote. Augmenting autotelic agents with large language models. _arXiv preprint arXiv:2305.12487_, 2023.
* [25] Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. ChatLaw: Open-source legal large language model with integrated external knowledge bases. _arXiv preprint arXiv:2306.16092_, 2023.
* [26] Gautier Dagan, Frank Keller, and Alex Lascarides. Dynamic planning with a LLM. _arXiv preprint arXiv:2308.06391_, 2023.
* [27] Yuhao Dan, Zhikai Lei, Yiyang Gu, Yong Li, Jianghao Yin, Jiaju Lin, Linhao Ye, Zhiyan Tie, Yougen Zhou, Yilei Wang, et al. Edduchat: A large-scale language model-based chatbot system for intelligent education. _arXiv preprint arXiv:2308.02773_, 2023.
* [28] Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix Hill, and Rob Fergus. Collaborating with language models for embodied reasoning. _arXiv preprint arXiv:2302.00763_, 2023.
* [29] Gelei Deng, Yi Liu, Victor Mayoral-Vilches, Peng Liu, Yuekang Li, Yuan Xu, Tianwei Zhang, Yang Liu, Martin Pinzger, and Stefan Rass. Pentestgpt: An llvm-empowered automatic penetration testing tool. _arXiv preprint arXiv:2308.06782_, 2023.
* [30] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. _arXiv preprint arXiv:2306.06070_, 2023.
* [31] Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan. Toxicity in ChatGPT: Analyzing persona-assigned language models. _arXiv preprint arXiv:2304.05335_, 2023.
* [32] Norman Di Palo, Arunkumar Byravan, Leonard Hasenclever, Markus Wulfmeier, Nicolas Heess, and Martin Riedmiller. Towards a unified agent with foundation models. In _Workshop on Reincarnating Reinforcement Learning at ICLR 2023_, 2023.
* [33] Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration code generation via ChatGPT. _arXiv preprint arXiv:2304.07590_, 2023.
* [34] Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, Roman Wang, Nikhil Singh, Taylor L. Patti, Jayson Lynch, Avi Shporer, Nakul Verma, Eugene Wu, and Gilbert Strang. A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. _Proceedings of the National Academy of Sciences_, 119(32), aug 2022.
* [35] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. _arXiv preprint arXiv:2305.14325_, 2023.

* [36] Alex MacCaw et al. WorkGPT. [https://github.com/team-openpm/workgpt](https://github.com/team-openpm/workgpt), 2023.
* [37] Anton Osika et al. GPT engineer. [https://github.com/AntonOsika/gpt-engineer](https://github.com/AntonOsika/gpt-engineer), 2023.
* [38] Assaf Elovic et al. GPT-researcher. [https://github.com/assafelovic/gpt-researcher](https://github.com/assafelovic/gpt-researcher), 2023.
* [39] Chen et al. Agentverse. [https://github.com/OpenBMB/AgentVerse](https://github.com/OpenBMB/AgentVerse), 2023.
* [40] Chen et al. Xlang. [https://github.com/xlang-ai/xlang](https://github.com/xlang-ai/xlang), 2023.
* [41] Enricoros et al. Miniagi. [https://github.com/muellerberndt/mini-agi](https://github.com/muellerberndt/mini-agi), 2023.
* [42] Eumemic et al. Ai-legion. [https://github.com/eumemic/ai-legion](https://github.com/eumemic/ai-legion), 2023.
* [43] Fayaz Rahman et al. LoopGPT. [https://github.com/farizrahman4u/loopgpt](https://github.com/farizrahman4u/loopgpt), 2023.
* [44] Josh XT et al. Agixt. [https://github.com/Josh-XT/AGiXT](https://github.com/Josh-XT/AGiXT), 2023.
* [45] Melih Unsal et al. DemoGPT. [https://github.com/melih-unsal/DemoGPT](https://github.com/melih-unsal/DemoGPT), 2023.
* [46] Nakajima et al. Babyagi. [https://github.com/yoheinakajima](https://github.com/yoheinakajima), 2023.
* [47] Reworkd et al. AgentGPT. [https://github.com/reworkd/AgentGPT](https://github.com/reworkd/AgentGPT), 2023.
* [48] Swyxio et al. Smolmodels. [https://github.com/smol-ai/developer](https://github.com/smol-ai/developer), 2023.
* [49] Torantulino et al. Auto-GPT. [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT), 2023.
* [50] TransformerOptimus et al. Superagi. [https://github.com/TransformerOptimus/SuperAGI](https://github.com/TransformerOptimus/SuperAGI), 2023.
* [51] Jonathan St BT Evans and Keith E Stanovich. Dual-process theories of higher cognition: Advancing the debate. _Perspectives on psychological science_, 8(3):223-241, 2013.
* [52] Hugging Face. transformers-agent. [https://huggingface.co/docs/transformers/transformers_agents](https://huggingface.co/docs/transformers/transformers_agents), 2023.
* [53] Robert Feldt, Sungmin Kang, Juyeon Yoon, and Shin Yoo. Towards autonomous testing agents via conversational large language models. _arXiv preprint arXiv:2306.05152_, 2023.
* [54] Kevin A Fischer. Reflective linguistic programming (rlp): A stepping stone in socially-aware agi (socialagi). _arXiv preprint arXiv:2305.12647_, 2023.
* [55] Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin, and Yong Li. S3: Social-network simulation system with large language model-empowered agents. _arXiv preprint arXiv:2307.14984_, 2023.
* [56] Yingqiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan, Shuyuan Xu, and Yongfeng Zhang. OpenAGI: When llm meets domain experts. _arXiv preprint arXiv:2304.04370_, 2023.
* [57] Zorik Gekhman, Nadav Oved, Orgad Keller, Idan Szpektor, and Roi Reichart. On the robustness of dialogue history representation in conversational question answering: a comprehensive study and a new prompt-based method. _Transactions of the Association for Computational Linguistics_, 11:351-366, 2023.
* [58] Maitrey Gramopadhye and Daniel Szafir. Generating executable action plans with environmentally-aware language models. _arXiv preprint arXiv:2210.04964_, 2022.
* [59] Igor Grossmann, Matthew Feinberg, Dawn C Parker, Nicholas A Christakis, Philip E Tetlock, and William A Cunningham. Ai and the transformation of social science research. _Science_, 380(6650):1108-1109, 2023.
* [60] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. _arXiv preprint arXiv:1801.01290_, 2018.
* [61] Sil Hamilton. Blind judgement: Agent-based supreme court modelling with GPT. _arXiv preprint arXiv:2301.05327_, 2023.

* [62] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. _arXiv preprint arXiv:2305.14992_, 2023.
* [63] Zhuolun He, Haoyuan Wu, Xinyun Zhang, Xufeng Yao, Su Zheng, Haisheng Zheng, and Bei Yu. Chateda: A large language model powered autonomous agent for eda, 2023.
* [64] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. MetaGPT: Meta programming for multi-agent collaborative framework. _arXiv preprint arXiv:2308.00352_, 2023.
* [65] John J Horton. Large language models as simulated economic agents: What can we learn from homo silicus? Technical report, National Bureau of Economic Research, 2023.
* [66] Bin Hu, Chenyang Zhao, Pu Zhang, Zihao Zhou, Yuanhang Yang, Zenglin Xu, and Bin Liu. Enabling intelligent interactions between an agent and an llvm: A reinforcement learning approach. _arXiv preprint arXiv:2306.03604_, 2023.
* [67] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. Chatdb: Augmenting LLMs with databases as their symbolic memory. _arXiv preprint arXiv:2306.03901_, 2023.
* [68] Jen-tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, and Michael R Lyu. Emotionally numb or empathetic? evaluating how llms feel using emotionbench. _arXiv preprint arXiv:2308.03656_, 2023.
* [69] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey. _arXiv preprint arXiv:2212.10403_, 2022.
* [70] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In _International Conference on Machine Learning_, pages 9118-9147. PMLR, 2022.
* [71] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. _arXiv preprint arXiv:2207.05608_, 2022.
* [72] Ziheng Huang, Sebastian Gutierrez, Hemanth Kamana, and Stephen MacNeil. Memory sandbox: Transparent and interactive memory management for conversational agents. _arXiv preprint arXiv:2308.01542_, 2023.
* [73] Sajed Jalil, Suzzana Rafi, Thomas D LaToza, Kevin Moran, and Wing Lam. ChatGPT and software testing education: Promises & perils. In _2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)_, pages 4130-4137. IEEE, 2023.
* [74] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. _ACM Computing Surveys_, 55(12):1-38, 2023.
* [75] Shi Jinxin, Zhao Jiabao, Wang Yilei, Wu Xingjiao, Li Jiawen, and He Liang. Cgmi: Configurable general multi-agent interaction framework, 2023.
* [76] Oliver P John, Eileen M Donahue, and Robert L Kentle. Big five inventory. _Journal of Personality and Social Psychology_, 1991.
* [77] John A Johnson. Measuring thirty facets of the five factor model with a 120-item public domain inventory: Development of the ipip-neo-120. _Journal of research in personality_, 51:78-89, 2014.
* [78] Sungmin Kang, Juyeon Yoon, and Shin Yoo. Large language models are few-shot testers: Exploring LLM-based general bug reproduction. In _2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)_, pages 2312-2323. IEEE, 2023.
* [79] Yeonghun Kang and Jihan Kim. Chatmof: An autonomous ai system for predicting and generating metal-organic frameworks. _arXiv preprint arXiv:2308.01423_, 2023.
* [80] Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofta Bata, Yoav Levine, Kevin Leyton-Brown, et al. Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. _arXiv preprint arXiv:2205.00445_, 2022.

* [81] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. _arXiv preprint arXiv:2303.17491_, 2023.
* [82] Takeshi Kojima, Shixiang Shane Gu, Michel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. _Advances in neural information processing systems_, 35:22199-22213, 2022.
* [83] Grgur Kovac, Remy Portelas, Peter Ford Dominey, and Pierre-Yves Oudeyer. The socialai school: Insights from developmental psychology towards artificial socio-cultural agents. _arXiv preprint arXiv:2307.07871_, 2023.
* [84] Ranjay Krishna, Donsuk Lee, Li Fei-Fei, and Michael S Bernstein. Socially situated artificial intelligence enables learning from human interaction. _Proceedings of the National Academy of Sciences_, 119(39):e2115730119, 2022.
* [85] Mina Lee, Megha Srivastava, Amelia Hardy, John Thickstun, Esin Durmus, Ashwin Paranjape, Ines Gerard-Ursin, Xiang Lisa Li, Faisal Ladhak, Frieda Rong, et al. Evaluating human-language model interaction. _arXiv preprint arXiv:2212.09746_, 2022.
* [86] Chao Li, Xing Su, Chao Fan, Haoying Han, Cong Xue, and Chunmo Zheng. Quantifying the impact of large language models on collective opinion dynamics. _arXiv preprint arXiv:2308.03313_, 2023.
* [87] Cheng Li, Jindong Wang, Kaijie Zhu, Yixuan Zhang, Wenxin Hou, Jianxun Lian, and Xing Xie. Emotionprompt: Leveraging psychology for large language models enhancement via emotional stimulus. _arXiv preprint arXiv:2307.11760_, 2023.
* [88] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for mind" exploration of large scale language model society. _arXiv preprint arXiv:2303.17760_, 2023.
* [89] Haonan Li, Yu Hao, Yizhuo Zhai, and Zhiyun Qian. The hitchhiker's guide to program analysis: A journey with large language models. _arXiv preprint arXiv:2308.00245_, 2023.
* [90] Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: A benchmark for tool-augmented LLMs. _arXiv preprint arXiv:2304.08244_, 2023.
* [91] Siyu Li, Jin Yang, and Kui Zhao. Are you in a masquerade? exploring the behavior and impact of large language model driven social bots in online social networks. _arXiv preprint arXiv:2307.10337_, 2023.
* [92] Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system. _arXiv preprint arXiv:2304.13343_, 2023.
* [93] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, et al. Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis. _arXiv preprint arXiv:2303.16434_, 2023.
* [94] Yuanzhi Liang, Linchao Zhu, and Yi Yang. Tachikuma: Understanding complex interactions with multi-character and novel objects by large language models. _arXiv preprint arXiv:2307.12573_, 2023.
* [95] Mark Liffiton, Brad Sheese, Jaromir Savelka, and Paul Denny. Codehelp: Using large language models with guardrails for scalable support in programming classes. _arXiv preprint arXiv:2308.06921_, 2023.
* [96] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. _arXiv preprint arXiv:1509.02971_, 2015.
* [97] Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. Swiftstage: A generative agent with fast and slow thinking for complex interactive tasks. _arXiv preprint arXiv:2305.17390_, 2023.
* [98] Jessy Lin, Nicholas Tomlin, Jacob Andreas, and Jason Eisner. Decision-oriented dialogue for human-ai collaboration. _arXiv preprint arXiv:2305.20076_, 2023.
* [99] Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, and Qin Chen. Agentsims: An open-source sandbox for large language model evaluation. _arXiv preprint arXiv:2308.04026_, 2023.

* [100] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. LLM+P: Empowering large language models with optimal planning proficiency. _arXiv preprint arXiv:2304.11477_, 2023.
* [101] Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. Chain of hindsight aligns language models with feedback. _arXiv preprint arXiv:2302.02676_, 3, 2023.
* [102] Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi Yang, and Sorous Vosoughi. Training socially aligned language models in simulated human society. _arXiv preprint arXiv:2305.16960_, 2023.
* [103] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangling Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating LLMs as agents. _arXiv preprint arXiv:2308.03688_, 2023.
* [104] Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, et al. BOLAA: Benchmarking and orchestrating LLM-augmented autonomous agents. _arXiv preprint arXiv:2308.05960_, 2023.
* [105] Zilin Ma, Yiyang Mei, and Zhaoyuan Su. Understanding the benefits and challenges of using large language model-based conversational agents for mental well-being support. _arXiv preprint arXiv:2307.15810_, 2023.
* [106] Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. Memory-assisted prompt editing to improve GPT-3 after deployment. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 2833-2861, 2022.
* [107] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. _arXiv preprint arXiv:2303.17651_, 2023.
* [108] Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large language models. _arXiv preprint arXiv:2307.04738_, 2023.
* [109] Jordan K Matelsky, Felipe Parodi, Tony Liu, Richard D Lange, and Konrad P Kording. A large language model-assisted education tool to provide feedback on open-ended responses. _arXiv preprint arXiv:2308.02439_, 2023.
* [110] Nikhil Mehta, Milagro Teruel, Patricio Figueroa Sanz, Xin Deng, Ahmed Hassan Awadallah, and Julia Kiseleva. Improving grounded language understanding in a collaborative environment by interacting with agents through help feedback. _arXiv preprint arXiv:2304.10750_, 2023.
* [111] Gregoire Mialon, Roberto Dessi, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Roziere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. Augmented language models: a survey. _arXiv preprint arXiv:2302.07842_, 2023.
* [112] Ning Miao, Yee Whye Teh, and Tom Rainforth. SelfCheck: Using LLMs to zero-shot check their own step-by-step reasoning. _arXiv preprint arXiv:2308.00436_, 2023.
* [113] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. _Nature_, 518(7540):529-533, 2015.
* [114] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schutze. RET-LLM: Towards a general read-write memory for large language models. _arXiv preprint arXiv:2305.14322_, 2023.
* [115] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. WebGPT: Browser-assisted question-answering with human feedback. _arXiv preprint arXiv:2112.09332_, 2021.
* [116] Nathalia Nascimento, Paulo Alencar, and Donald Cowan. Self-adaptive large language model (llm)-based multiagent systems. _arXiv preprint arXiv:2307.06187_, 2023.
* [117] Youyang Ng, Daisuke Miyashita, Yasuto Hoshi, Yasuhiro Morioka, Osamu Torii, Tomoya Kodama, and Jun Deguchi. Simplyretrieve: A private and lightweight retrieval-centric generative ai tool. _arXiv preprint arXiv:2308.03983_, 2023.
* [118] Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, and Roy Fox. Do embodied agents dream of pixelated sheep?: Embodied decision making using language guided world modelling. _arXiv preprint arXiv:2301.12050_, 2023.

* [119] Oluwatosin Ogundare, Srinath Madasu, and Nathanial Wiggins. Industrial engineering with large language models: A case study of ChatGPT's performance on oil & gas problems. _arXiv preprint arXiv:2304.14354_, 2023.
* [120] OpenAI. GPT-4 technical report, 2023.
* [121] Joon Sung Park, Joseph C. O'Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In _In the 36th Annual ACM Symposium on User Interface Software and Technology (UIST '23)_, UIST '23, New York, NY, USA, 2023. Association for Computing Machinery.
* [122] Joon Sung Park, Lindsay Popowski, Carrie Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Social simulacra: Creating populated prototypes for social computing systems. In _Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology_, pages 1-18, 2022.
* [123] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model connected with massive apis. _arXiv preprint arXiv:2305.15334_, 2023.
* [124] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. _arXiv preprint arXiv:2307.07924_, 2023.
* [125] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. _arXiv preprint arXiv:2304.08354_, 2023.
* [126] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. ToolLLM: Facilitating large language models to master 16000+ real-world apis. _arXiv preprint arXiv:2307.16789_, 2023.
* [127] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* [128] Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Irrah Idrees, David Paulius, and Stefanie Tellex. Planning with large language models via corrective re-prompting. _arXiv preprint arXiv:2211.09935_, 2022.
* [129] Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, and Niko Suenderhauf. Sayplan: Grounding large language models using 3d scene graphs for scalable task planning. _arXiv preprint arXiv:2307.06135_, 2023.
* [130] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. TPTU: Task planning and tool usage of large language model-based AI agents. _arXiv preprint arXiv:2308.03427_, 2023.
* [131] Mustafa Safdari, Greg Serapio-Garcia, Clement Crepy, Stephen Fitz, Peter Romero, Luning Sun, Marwa Abdulhai, Aleksandra Faust, and Maja Mataric. Personality traits in large language models. _arXiv preprint arXiv:2307.00184_, 2023.
* [132] Swarnadeep Saha, Peter Hase, and Mohit Bansal. Can language models teach weaker agents? teacher explanations improve students via theory of mind. _arXiv preprint arXiv:2306.09299_, 2023.
* [133] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. _arXiv preprint arXiv:2302.04761_, 2023.
* [134] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* [135] Dale Schuurmans. Memory augmented large language models are computationally universal. _arXiv preprint arXiv:2301.04589_, 2023.
* [136] Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, and Yulia Tsvetkov. Minding language models'(lack of) theory of mind: A plug-and-play multi-character belief tracker. _arXiv preprint arXiv:2306.00924_, 2023.
* [137] Bilgehan Sel, Ahmad Al-Tawaha, Vanshaj Khattar, Lu Wang, Ruoxi Jia, and Ming Jin. Algorithm of thoughts: Enhancing exploration of ideas in large language models. _arXiv preprint arXiv:2308.10379_, 2023.

* [138] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. HuggingGPT: Solving ai tasks with ChatGPT and its friends in huggingface. _arXiv preprint arXiv:2303.17580_, 2023.
* [139] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. _arXiv preprint arXiv:2303.11366_, 2023.
* [140] Yubo Shu, Hansu Gu, Peng Zhang, Haonan Zhang, Tun Lu, Dongsheng Li, and Ning Gu. Rah! recsys-assistant-human: A human-central recommendation framework with large language models. _arXiv preprint arXiv:2308.09904_, 2023.
* [141] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su. LLM-Planner: Few-shot grounded planning for embodied agents with large language models. _arXiv preprint arXiv:2212.04088_, 2022.
* [142] Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Cheng Li, Ke Wang, Rong Yao, Ye Tian, and Sujian Li. Restgpt: Connecting large language models with real-world restful apis, 2023.
* [143] Ruoxi Sun, Sercan O Arik, Hootan Nakhost, Hanjun Dai, Rajarishi Sinha, Pengcheng Yin, and Tomas Pfister. Sql-palm: Improved large language modeladaptation for text-to-sql. _arXiv preprint arXiv:2306.00739_, 2023.
* [144] Didac Suris, Sachit Menon, and Carl Vondrick. ViperGPT: Visual inference via python execution for reasoning. _arXiv preprint arXiv:2303.08128_, 2023.
* [145] Melanie Swan, Takashi Kido, Eric Roland, and Renato P dos Santos. Math agents: Computational infrastructure, mathematical embedding, and genomics. _arXiv preprint arXiv:2307.02502_, 2023.
* [146] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [147] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [148] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. _arXiv preprint arXiv:2305.16291_, 2023.
* [149] Lei Wang. Recagent. [https://github.com/RUC-GSAI/YuLan-Rec](https://github.com/RUC-GSAI/YuLan-Rec), 2023.
* [150] Lei Wang, Jingsen Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, and Ji-Rong Wen. Recagent: A novel simulation paradigm for recommender systems. _arXiv preprint arXiv:2306.02552_, 2023.
* [151] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdchery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. _arXiv preprint arXiv:2203.11171_, 2022.
* [152] Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, and Yingzhen Yang. Recmind: Large language model powered agent for recommendation. _arXiv preprint arXiv:2308.14296_, 2023.
* [153] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. Aligning large language models with human: A survey. _arXiv preprint arXiv:2307.12966_, 2023.
* [154] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. _arXiv preprint arXiv:2302.01560_, 2023.
* [155] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022.
* [156] Ross Williams, Niyousha Hosseinichimeh, Aritra Majumdar, and Navid Ghaffarzadegan. Epidemic modeling with generative agents. _arXiv preprint arXiv:2307.04986_, 2023.

* [157] Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. Tidybot: Personalized robot assistance with large language models. _arXiv preprint arXiv:2305.05658_, 2023.
* [158] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. AutoGen: Enabling next-gen LLM applications via multi-agent conversation framework. _arXiv preprint arXiv:2308.08155_, 2023.
* [159] Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Yuanzhi Li, Tom Mitchell, and Shrimai Prabhumoye. Plan, eliminate, and track-language models are good teachers for embodied agents. _arXiv preprint arXiv:2305.02412_, 2023.
* [160] Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, and Haibin Yan. Embodied task planning with large language models. _arXiv preprint arXiv:2307.01848_, 2023.
* [161] Yuchen Xia, Manthan Shenoy, Nasser Jazdi, and Michael Weyrich. Towards autonomous system: flexible modular production system enhanced with large language model agents. _arXiv preprint arXiv:2304.14721_, 2023.
* [162] Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. Language models meet world models: Embodied experiences enhance language models. _arXiv preprint arXiv:2305.10626_, 2023.
* [163] Binfeng Xu, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong Yue, Zhiyuan Peng, Yuchen Liu, Ziyu Yao, and Dongkuan Xu. Gentopia: A collaborative platform for tool-augmented LLMs. _arXiv preprint arXiv:2308.04030_, 2023.
* [164] Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee, Yuchen Liu, and Dongkuan Xu. Rewoo: Decoupling reasoning from observations for efficient augmented language models. _arXiv preprint arXiv:2305.18323_, 2023.
* [165] Yuxuan Lei Jing Yao Defu Lian Xing Xie Xu Huang, Jianxun Lian. Recommender ai agent: Integrating large language models for interactive recommendations. _arXiv preprint arXiv:2308.16505_, 2023.
* [166] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. Harnessing the power of LLMs in practice: A survey on ChatGPT and beyond. _arXiv preprint arXiv:2304.13712_, 2023.
* [167] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Liyuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. _arXiv preprint arXiv:2303.11381_, 2023.
* [168] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webbsp: Towards scalable real-world web interaction with grounded language agents. _Advances in Neural Information Processing Systems_, 35:20744-20757, 2022.
* [169] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. _arXiv preprint arXiv:2305.10601_, 2023.
* [170] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. _arXiv preprint arXiv:2210.03629_, 2022.
* [171] Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, et al. Retroformer: Retrospective large language agents with policy gradient optimization. _arXiv preprint arXiv:2308.02151_, 2023.
* [172] Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge Zhang, Feng Yin, Yitao Liang, and Yaodong Yang. Proagent: Building proactive cooperative ai with large language models, 2023.
* [173] Chenrui Zhang, Lin Liu, Jinpeng Wang, Chuyuan Wang, Xiao Sun, Hongyu Wang, and Mingchen Cai. Prefer: Prompt ensemble learning via feedback-reflect-refine. _arXiv preprint arXiv:2308.12033_, 2023.
* [174] Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan Zhao, and Kai Yu. Large language model is semi-parametric reinforcement learning agent. _arXiv preprint arXiv:2306.07929_, 2023.

* [175] Danyang Zhang, Lu Chen, Zihan Zhao, Ruisheng Cao, and Kai Yu. Mobile-Env: An evaluation platform and benchmark for interactive agents in LLM era. _arXiv preprint arXiv:2305.08144_, 2023.
* [176] Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied agents modularly with large language models. _arXiv preprint arXiv:2307.02485_, 2023.
* [177] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners, 2023.
* [178] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. _arXiv preprint arXiv:2303.18223_, 2023.
* [179] Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. _arXiv preprint arXiv:2305.10250_, 2023.
* [180] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building autonomous agents. _arXiv preprint arXiv:2307.13854_, 2023.
* [181] Wei Zhou, Xiangyu Peng, and Mark Riedl. Dialogue shaping: Empowering agents through npc interaction. _arXiv preprint arXiv:2307.15833_, 2023.
* [182] Xuanhe Zhou, Guoliang Li, and Zhiyuan Liu. Llm as dba. _arXiv preprint arXiv:2308.05481_, 2023.
* [183] Andrew Zhu, Lara J Martin, Andrew Head, and Chris Callison-Burch. Calypso: Llms as dungeon masters' assistants. _arXiv preprint arXiv:2308.07540_, 2023.
* [184] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. _arXiv preprint arXiv:2305.17144_, 2023.
* [185] Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley, Robert Csordas, Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, et al. Mindstorms in natural language-based societies of mind. _arXiv preprint arXiv:2305.17066_, 2023.
* [186] Terry Yue Zhuo, Zhuang Li, Yujin Huang, Yuan-Fang Li, Weiqing Wang, Gholamreza Haffari, and Fatemeh Shiri. On robustness of prompt-based semantic parsing with large pre-trained language model: An empirical study on codex. _arXiv preprint arXiv:2301.12868_, 2023.
* [187] Caleb Ziems, William Held, Omar Shaikh, Jiao Chen, Zhehao Zhang, and Diyi Yang. Can large language models transform computational social science? _arXiv preprint arXiv:2305.03514_, 2023.
