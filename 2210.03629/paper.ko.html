<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# ReAct: 추론 및 행위 동기화 언어 모델\n' +
      '\n' +
      'Shunyu Yao\n' +
      '\n' +
      '구글 인턴십 기간 동안 일하세요. 코드가 포함된 Projet 페이지: [https://react-lm.github.io/](https://react-lm.github.io/)입니다.\n' +
      '\n' +
      '산술, 상식, 기호 추론 과제의 질문으로부터 답변한다(Wei et al., 2022). 그러나 이 "사상 사슬" 추론은 모델이 자신의 내부 표현을 사용하여 생각을 생성하고 외부 세계에 근거하지 않는다는 점에서 정적 블랙박스이며, 이는 반응적으로 추론하거나 지식을 업데이트하는 능력을 제한한다. 이는 추론 과정에 대한 사실 환각 및 오류 전파와 같은 문제로 이어질 수 있다(그림 1(b)). 한편, 최근의 연구는 언어 사전들을 통한 액션들을 예측하는 것에 초점을 두고, 상호작용 환경들에서의 계획 및 동작을 위한 사전 트레이닝된 언어 모델들의 사용을 탐색하였다(Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022). 이러한 접근법은 일반적으로 다중 모달 관찰을 텍스트로 변환하고 언어 모델을 사용하여 도메인별 작업 또는 계획을 생성한 다음 컨트롤러를 사용하여 선택하거나 실행한다. 그러나, 이들은 현재 상태에 대한 공간적 사실들을 반복하기 위해 제한된 형태의 언어적 추론을 수행하는 Huang et al.(2022)을 제외하고, 상위 레벨 목표에 대해 추상적으로 추론하거나 연기를 지원하기 위한 작업 기억을 유지하기 위해 언어 모델을 채용하지 않는다. 이렇게 단순하게 구현된 몇 가지 블록과 상호작용하는 과제를 넘어 추론과 연기가 일반적인 과제 해결을 위해 어떻게 시너지 방식으로 결합될 수 있는지, 그러한 결합이 추론이나 연기에 비해 체계적인 이점을 가져올 수 있는지에 대한 연구는 없었다.\n' +
      '\n' +
      '본 연구에서는 다양한 언어 추론과 의사 결정 과제를 해결하기 위한 언어 모델과 추론과 연기를 결합하는 일반적인 패러다임인 ReAct를 제시한다(그림 1). ReAct는 LLM들이 인터리브 방식으로 태스크와 관련된 언어적 추론 트레이스들 및 액션들 둘 모두를 생성하도록 프롬프트하고, 이는 모델이 연기에 대한 높은 레벨의 계획들을 생성, 유지 및 조정하는 동적 추론을 수행하는 한편, 또한 외부 환경들(예를 들어, 위키피디아)과 상호작용하여 추가 정보를 추론에 통합(연기에 대한 추론)할 수 있게 한다.\n' +
      '\n' +
      '도 1: (1) 4가지 프롬프트 방법의 비교, (a) Standard, (b) Chain-of-thought(CoT, Reason Only), (c) Act-only, (d) ReAct(Reason+Act), HotpotQA(Yang et al., 2018) 문제 해결. (2) (a) Act-only와 (b) ReAct prompting to solve a AlfWorld (Shridhar et al., 2020) 게임의 비교. 두 영역 모두 프롬프트에서 문맥 내 예제를 생략하고 모델(Act, Thought)과 환경(Obs)에 의해 생성된 과제 해결 궤적만 보여준다.\n' +
      '\n' +
      '이를 위해 4가지 벤치마크인 질문 응답(HotPotQA, Yang et al., 2018), 사실 검증(Fever, Thorne et al., 2018), 텍스트 기반 게임(ALFWorld, Shridhar et al., 2020), 웹페이지 내비게이션(WebShop, Yao et al., 2022)을 대상으로 ReAct와 최신 기준선에 대한 실증적 평가를 수행하였다. HotPotQA 및 Fever의 경우, 모델이 상호작용할 수 있는 위키피디아 API에 대한 액세스와 함께, ReAct는 CoT(chain-of-thought reasoning)와 경쟁적이면서 바닐라 액션 생성 모델보다 성능이 우수하다(Wei et al., 2022). 전반적으로 최선의 접근법은 추론 중에 내부 지식과 외부에서 얻은 정보를 모두 사용할 수 있도록 하는 ReAct와 CoT의 조합이다. ALFWorld와 WebShop에서는 \\(10^{3}\\sim 10^{5}\\) 태스크 인스턴스로 학습한 모방 학습이나 강화 학습 방법보다 두 번 또는 한 번 재실행하는 것이 각각 34%와 10%의 절대 성능 향상을 보였다. 또한 행동만으로 통제된 기준선에 비해 일관된 이점을 보여줌으로써 의사 결정에서 희박하고 다재다능한 추론의 중요성을 보여준다. 추론과 행동의 조합은 일반적인 적용 가능성과 성능 향상 외에도 인간이 모델의 내부 지식 대 외부 환경과 정보를 쉽게 구별할 수 있고 모델 행동의 결정 기초를 이해하기 위해 추론 흔적을 검사할 수 있기 때문에 모든 영역에서 모델 해석 가능성, 신뢰성 및 진단 가능성에 기여한다.\n' +
      '\n' +
      '요약하자면, 우리의 주요 기여는 다음과 같다: (1) 일반적인 과제 해결을 위해 언어 모델에서 추론과 연기를 시너지 효과를 내는 새로운 프롬프트 기반 패러다임인 ReAct를 소개한다; (2) 다양한 벤치마크에 걸쳐 광범위한 실험을 수행하여 추론 또는 행동 생성을 독립적으로 수행하는 이전 접근법에 비해 몇 번의 샷 학습 설정에서 ReAct의 이점을 보여준다; (3) 추론 작업에서 연기의 중요성 및 대화형 작업에서 추론의 중요성을 이해하기 위한 체계적인 절제 및 분석을 제시한다; (4) 프롬프트 설정 하에서 ReAct의 한계(즉, 추론 및 행동 행동의 제한된 지원)를 분석하고 추가 학습 데이터로 ReAct의 가능성을 보여주는 초기 피니튜닝 실험을 수행한다. 더 많은 작업을 훈련하고 운영하기 위해 ReAct를 확장하고 강화 학습과 같은 보완 패러다임과 결합하면 대규모 언어 모델의 잠재력을 더욱 잠식할 수 있다.\n' +
      '\n' +
      '## 2 ReAct: 추론 동기화 + AcTing\n' +
      '\n' +
      '작업 해결을 위해 환경과 상호 작용하는 에이전트의 일반적인 설정을 고려하십시오. 시간 단계 \\(t\\)에서 에이전트는 환경으로부터 관찰 \\(o_{t}\\in\\mathcal{O}\\)을 받고 일부 정책 \\(\\pi(a_{t}|c_{t})\\)에 따라 작업 \\(a_{t}\\in\\mathcal{A}\\)을 수행합니다. 여기서 \\(c_{t}=(o_{1},a_{1},\\cdots,o_{t-1},a_{t-1},o_{t})\\)은 에이전트에 대한 _컨텍스트_입니다. 매핑 \\(c_{t}\\mapsto a_{t}\\)이 매우 암시적이고 광범위한 계산이 필요한 경우 정책을 학습하는 것은 어렵다. 예를 들어, 도 1(c)에 도시된 에이전트는 궤적 컨텍스트에 대한 복잡한 추론을 요구하기 때문에 QA 태스크를 끝내기 위한 정확한 최종 액션(Act 4)을 생성할 수 없다(Question, Act 1-3, Obs 1-3). 유사하게, 그림 1(a)에 표시된 에이전트는 싱크베이시안 1에 페퍼헤커 1이 포함되어 있지 않으므로 환각 작용을 계속 생성한다는 맥락을 이해하지 못한다.\n' +
      '\n' +
      'ReAct의 개념은 간단하다. 에이전트의 동작 공간을 \\(\\hat{\\mathcal{A}}=\\mathcal{A}\\cup\\mathcal{L}\\)으로 확장하고, 여기서 \\(\\mathcal{L}\\)은 언어의 공간이다. 언어 공간에서의 동작 \\(\\hat{a}_{t}\\in\\mathcal{L}\\)은 외부 환경에 영향을 주지 않으므로 관측 피드백이 없다. 대신, 사고 \\(\\hat{a}_{t}\\)은 현재 상황 \\(c_{t}\\)에 대한 추론으로 유용한 정보를 구성하고, 상황 \\(c_{t+1}=(c_{t},\\hat{a}_{t})\\)을 업데이트하여 미래의 추론이나 행동을 지원하는 것을 목표로 한다. 도 1에 도시된 바와 같이, 다양한 유형의 유용한 생각들이 있을 수 있는데, 예를 들어, 태스크 목표들을 분해하고 액션 플랜들을 생성하는 것(2b, Act 1; 1d, Thought 1), 태스크 해결과 관련된 상식 지식을 주입하는 것(2b, Act 1), 관찰들로부터 중요한 부분들을 추출하는 것(1d, Thought 2, 4), 진행 및 통과 액션 플랜들을 트랙킹하는 것(2b, Act 8), 예외들을 처리하고 액션 플랜들을 조정하는 것(1d, Thought 3) 등이 있다.\n' +
      '\n' +
      '그러나, 언어 공간 \\(\\mathcal{L}\\)이 무제한이기 때문에, 이 증강된 액션 공간에서의 학습은 어렵고 강한 언어 사전이 필요하다. 본 논문에서는 주로 동결된 대형 언어 모델 PaLM-540B(Chowdhery et al., 2022)1이 작업 해결을 위한 도메인 특정 동작과 자유 형식 언어 생각을 모두 생성하기 위해 소수의 샷 인 컨텍스트 예제와 함께 프롬프트되는 설정에 중점을 둔다(그림 1(1d), (2b). 각 문맥 내 예는 태스크 인스턴스를 해결하기 위한 행동, 생각 및 환경 관찰의 인간 궤적이다(부록 C 참조). 추론이 가장 중요한 과제(그림 1(1))의 경우, 과제 해결 궤적이 여러 사고-행동-관찰 단계로 구성되도록 생각과 행동의 생성을 번갈아 가며 수행한다. 대조적으로, 잠재적으로 많은 수의 액션들을 수반하는 의사 결정 태스크들의 경우(도 1(2)) 생각들은 궤적의 가장 관련된 위치들에서 드물게 나타날 필요가 있으므로, 언어 모델이 자신을 위한 생각들 및 액션들의 비동기적 발생을 결정하게 한다.\n' +
      '\n' +
      '의사 결정 및 추론 기능이 대규모 언어 모델에 통합되기 때문에 ReAct는 몇 가지 고유한 기능을 누립니다. **A) 직관적이고 설계하기 쉽습니다. ReAct 프롬프트 설계는 인간 주석이 취한 작업 외에도 언어로 생각을 입력하기만 하면 간단합니다. 이 문서에는 임시 형식 선택, 사고 설계 또는 예제 선택이 사용되지 않습니다. 섹션 3 및 4의 각 작업에 대 한 프롬프트 설계를 자세히 설명 합니다. **B) 일반적이고 유연한**: 유연한 사고 공간 및 사고 작업 발생 형식으로 인해 ReAct는 QA, 사실 확인, 텍스트 게임 및 웹 탐색을 포함 하 고 제한 하지 않는 고유한 작업 공간 및 추론 요구 사항을 포함 하는 다양한 작업에 대해 작동 합니다. **C) 수행 및 강력**: ReAct는 1~6개의 문맥 내 예제에서만 학습하는 동안 새로운 작업 인스턴스에 대한 강력한 일반화를 보여 주며, 서로 다른 도메인에서 추론 또는 행위만으로 기준선을 일관되게 능가합니다. 우리는 또한 섹션 3에서 피니튜닝이 활성화될 때 추가 이점과 섹션 4에서 ReAct 성능이 신속한 선택에 얼마나 강력한지 보여준다. **D) 인간 정렬 및 제어 가능**: ReAct는 인간이 추론 및 사실적 정확성을 쉽게 검사할 수 있는 해석 가능한 순차적 의사 결정 및 추론 프로세스를 약속합니다. 더욱이 인간은 4절의 그림 5와 같이 사고 편집을 통해 이동 중 에이전트 행동을 통제하거나 수정할 수도 있다.\n' +
      '\n' +
      '## 3 지식 집약적 추론 작업\n' +
      '\n' +
      '멀티홉 질문 응답 및 사실 확인과 같은 지식 집약적 추론 작업부터 시작합니다. 그림 1(1d)와 같이 ReAct는 위키피디아 API와 상호작용하여 추론을 지원하기 위해 정보를 검색할 수 있는 동시에 추론을 사용하여 다음에 검색할 것을 목표로 하여 추론과 연기의 시너지 효과를 보여준다.\n' +
      '\n' +
      '### Setup\n' +
      '\n' +
      'Domains우리는 지식 검색과 추론에 도전하는 두 데이터 세트를 고려한다: (1) HotPotQA (Yang et al., 2018), 2개 이상의 위키피디아 구절에 대한 추론이 필요한 멀티홉 질의 응답 벤치마크, (2) FEVER (Thorne et al., 2018), 각 클레임에 SUPPORTS, REFUTES 또는 NOT ENOUGH INFO가 주석이 달린 사실 검증 벤치마크이다. 이 작업에서 우리는 두 작업에 대한 질문 전용 설정에서 작동하며, 여기서 모델은 지원 단락에 대한 액세스 없이 질문/클레임만 입력으로 받고 추론을 지원하기 위해 내부 지식에 의존하거나 외부 환경과 상호 작용하여 지식을 검색해야 한다.\n' +
      '\n' +
      '액션 공간 대화형 정보 검색을 지원 하는 세 가지 유형의 작업을 사용 하 여 간단한 위키피디아 웹 API를 설계 합니다. (1) **search**[entity** 는 해당 엔터티 위키 페이지에서 처음 5 개의 문장을 반환 하거나 위키피디아 검색 엔진에서 상위 5 개의 유사 엔터티를 제안 합니다. (2) **lookup**[string** 은 문자열이 포함 된 페이지에서 다음 문장을 반환 하 여 브라우저에서 Ctrl+F 기능을 시뮬레이션 합니다. (3) **완료**[답변]을 사용하여 현재 작업을 답변으로 완료합니다. 이 작업 공간은 대부분 정확한 통로 이름을 기반으로 한 구절의 작은 부분만을 검색할 수 있다는 점에 주목하며, 이는 최첨단 어휘 검색기나 신경 검색기보다 상당히 약하다. 목적은 인간이 위키피디아와 상호 작용하는 방식을 시뮬레이션하고 언어로 명시적 추론을 통해 모델을 검색하도록 강제하는 것이다.\n' +
      '\n' +
      '### Methods\n' +
      '\n' +
      'ReAct PromptingFor HotpotQA and Fever는 훈련 세트에서 무작위로 6개 및 3개 사례2를 선택하고 프롬프트에서 소수의 샷 예시로 사용하기 위해 ReAct 형식 궤적을 수동으로 구성한다. 그림 1(d)와 유사하게 각 궤적은 여러 사고-행동-관찰 단계(즉, 조밀한 사고)로 구성되며, 여기서 자유형 사고는 다양한 목적으로 사용된다. 구체적으로, 우리는 질문들을 분해하는 생각들의 조합("x를 검색해야 한다, y를 찾아야 한다, 그리고 z를 찾아야 한다"), 위키피디아 관찰들로부터 정보를 추출("x는 1844년에 시작됨", "단락이 x를 말하지 않는다"), 상식을 수행("x는 y가 아니므로 z는 대신...여야 한다") 또는 산술 추론("1844 < 1989"), 안내 검색 재구성("어쩌면 x를 대신 검색/조회할 수 있다"), 그리고 최종 답변("...그래서 답은 x이다")을 합성한다. 자세한 내용은 부록 C를 참조하십시오.\n' +
      '\n' +
      '기본 사항 ReAct 궤적을 체계적으로 축소하여 여러 기준(그림 1(a-1)과 같은 형식을 포함)에 대한 프롬프트를 빌드합니다. (a) ReAct 궤적의 모든 생각, 작업, 관찰을 제거하는 **표준 프롬프트**(표준). (b) 작업 및 관찰을 제거하고 추론 전용 기준선 역할을 하는 **Chain-of-thought 프롬프트** (CoT)(Wei 등, 2022)입니다. 또한 추론 동안 디코딩 온도가 0.7인 21개의 CoT 궤적을 샘플링하고 다수 응답을 채택하여 CoT보다 일관되게 성능을 향상시키는 자기 일관성 기준선(CoT-SC)(Wang et al., 2022;b)을 구축한다. (c) ReAct 궤적의 생각을 제거하는 **행위 전용 프롬프트** (Act) WebGPT (Nakano et al., 2021)가 다른 작업 및 작업 공간에서 작동 하지만 인터넷에 상호 작용 하 여 질문에 응답 하는 방법과 느슨하게 유사 하 고 프롬프트 대신 모방 및 강화 학습을 사용 합니다.\n' +
      '\n' +
      '3.3절에서 내부 지식과 외부 지식을 결합하면 ReAct에 의해 입증된 문제 해결 과정이 더 사실적이고 근거 있는 반면 CoT는 추론 구조를 공식화하는 데 더 정확하지만 환각된 사실이나 생각을 쉽게 겪을 수 있음을 관찰할 수 있다. 따라서 본 논문에서는 ReAct와 CoT-SC를 통합할 것을 제안하며, A) \\(\\textbf{ReAct}\\rightarrow\\textbf{CoT-SC}\\): ReAct가 주어진 단계 내에서 답을 반환하지 못할 때 CoT-SC로 회귀한다. 본 연구에서는 HotpotQA와 FEVER에 대해 각각 7단계와 5단계를 설정하여 ReAct 성능을 향상시키지 못했다. B) \\(\\textbf{CoT-SC}\\rightarrow\\textbf{ReAct}\\): \\(n\\)CoT-SC 샘플 중 과반수 응답이 \\(n/2\\)회 미만일 때(즉, 내부 지식이 태스크를 자신 있게 지원하지 못할 수 있음), ReAct로 돌아간다.\n' +
      '\n' +
      '각주 3: 최종 정답이 있는 모든 궤적 중 HotpotQA 7단계와 FEVER 5단계가 각각 0.84%와 1.33%를 차지한다.\n' +
      '\n' +
      '스케일에서의 추론 트레이스들 및 액션들에 수동으로 주석을 다는 도전으로 인해, 우리는 입력 질문들/클레임들에 조건화된 궤적들(모든 생각들, 액션들, 관찰들)을 디코딩하기 위해 ReAct(다른 베이스라인들에 대해서도)에 의해 생성된 정답을 갖는 3,000개의 궤적들을 사용하는, Zelikman 등(2022)과 유사한 부트스트래핑 접근법을 고려한다. 자세한 내용은 부록 B.1에 나와 있습니다.\n' +
      '\n' +
      '### 결과 및 관찰\n' +
      '\n' +
      'ReAct는 Act를 일관되게 능가하며 표 1은 PaLM-540B를 다른 프롬프트 방법을 사용하여 기본 모델로 사용하는 HotpotQA 및 발열 결과를 보여준다. 우리는 ReAct가 두 과제에 대한 Act보다 더 우수하다는 점에 주목하며, 특히 그림 1(1c-d)와 같이 최종 답을 합성하기 위해 연기를 안내하는 추론의 가치를 보여준다. 미세 조정 결과 3은 또한 더 많은 정보에 입각한 연기에 대한 추론 추적의 이점을 확인한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c} \\hline \\hline\n' +
      '**Prompt Methoda** & \\begin{tabular}{c} **HotpotQA** \\\\ (EM) \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{c} **Fever** \\\\ (Acc) \\\\ \\end{tabular} \\\\ \\hline Standard & 28.7 & 57.1 \\\\ CoT(Nakano et al., 2022) & 29.4 & 56.3 \\\\ CoT-SC(Wang et al., 2022) & 33.4 & 60.4 \\\\ \\hline Act & 25.7 & 58.9 \\\\ ReAct & 27.4 & 60.9 \\\\ CoT-SC\\(\\rightarrow\\) ReAct & 34.2 & **64.6** \\\\ ReAct\\(\\rightarrow\\) CoT-SC & **35.1** & 62.0 \\\\ \\hline \\hline\n' +
      '**Supervised SoTAb** & 67.5 & 89.5 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: PaLM-540B가 HotpotQA 및 Fever에 대한 결과를 프롬프트한다.\n' +
      '\n' +
      '**ReAct vs. CoT** 반면 ReAct는 CoT on Fever(60.9 vs. 56.3)보다 성능이 뛰어납니다. (27.4 vs. 29.4). SUPPORTS/REFUTES에 대한 발열 주장은 약간의 차이만 있을 수 있으므로(부록 D.1 참조), 정확하고 최신 지식을 검색하기 위한 행동은 필수적이다. HotpotQA에서 ReAct와 CoT의 행동 차이를 더 잘 이해하기 위해 ReAct와 CoT에서 각각 정답과 오답을 가진 50개의 궤적(총 200개의 예)을 무작위로 샘플링하고 표 2에 성공 및 실패 모드에 수동으로 레이블을 지정했다. 몇 가지 주요 관찰은 다음과 같다.\n' +
      '\n' +
      'A) **환각은 CoT에 심각한 문제로 ReAct(14% 대 14%)보다 훨씬 높은 거짓 양성률을 초래한다. 6%가 성공모드이며, 주요 실패모드(56%)를 구성한다. 이와는 대조적으로 ReAct의 문제 해결 궤적은 외부 지식 베이스의 접근 덕분에 더 근거 있고, 사실 주도적이며, 신뢰할 수 있다.\n' +
      '\n' +
      'B) **추론, 동작 및 관찰 단계를 인터리빙하면 ReAct의 근거와 신뢰도가 향상되지만 이러한 구조적 제약 조건은 추론 단계를 공식화하는 유연성을 감소시켜 CoT보다 더 많은 추론 오류율을 초래합니다. 우리는 모델이 이전 생각과 행동을 반복적으로 생성하는 ReAct에 특정한 한 가지 빈번한 오류 패턴이 있다는 점에 주목하고 모델이 루프4에서 어떤 적절한 다음 행동을 취하고 점프해야 하는지에 대해 추론하지 못함에 따라 "추론 오류"의 일부로 분류한다.\n' +
      '\n' +
      '각주 4: 우리는 이것이 차선책 탐욕 디코딩 절차 때문일 수 있다고 의심하며, 더 나은 디코딩을 사용하는 향후 작업(예: 빔 탐색)이 이 문제를 해결하는 데 도움이 될 수 있다.\n' +
      '\n' +
      '**C) **ReAct의 경우 검색을 통해 유익한 지식을 성공적으로 검색하는 것이 중요합니다.* * 오류 사례의 23%를 차지하는 비정보 검색은 모델 추론을 방해하고 생각을 복구하고 재구성하는 데 어려움을 줍니다. 이것은 아마도 사실성과 유연성 사이의 예상되는 상충 관계이며, 이는 두 가지 방법을 결합하는 제안된 전략에 동기를 부여한다.\n' +
      '\n' +
      '부록 E.1의 각 성공 및 실패 모드에 대한 예를 제공합니다. 또한 일부 HotpotQA 질문이 오래된 답변 레이블을 포함할 수 있음을 발견합니다. 예를 들어 그림 4를 참조하세요.\n' +
      '\n' +
      '**ReAct + CoT-SC가 LLMs 프롬프트에 가장 적합 합니다.** 또한 표 1에 표시 된 핫팟QA 및 열에서 가장 적합 한 프롬프트 방법은 각각 ReAct \\(\\rightarrow\\) CoT-SC 및 CoT-SC \\(\\rightarrow\\) ReAct입니다. 또한 그림 2는 사용된 CoT-SC 샘플의 수와 관련하여 다른 방법이 어떻게 수행되는지 보여준다. 두 개의 ReAct + CoT-SC 방법이 각각 하나의 작업에서 유리하지만 다른 수의 샘플에 걸쳐 CoT-SC를 상당히 일관되게 능가하여 3-5개의 샘플만 사용하는 21개의 샘플로 CoT-SC 성능에 도달한다. 이러한 결과는 추론 작업에 대한 모델 내부 지식과 외부 지식을 적절하게 결합하는 가치를 나타낸다.\n' +
      '\n' +
      '**ReAct는 미세 조정을 위해 가장 잘 수행** 그림 3은 HotpotQA에 대한 4가지 방법(Standard, CoT, Act, ReAct) 프롬프트/피닝의 크기 조정 효과를 보여줍니다. PaLM-8/62B의 경우, 재촉 ReAct는 맥락 내 예제에서 추론과 연기를 모두 학습하기 어렵기 때문에 4가지 방법 중 최악의 성능을 보인다. 그러나, 단지 3,000개의 예들로 피니튜닝될 때, ReAct는 4개 중 최상의 방법이 되며, PaLM-8B 피니튜닝된 ReAct는 모든 PaLM-62B 프롬프트 방법을 능가하고, PaLM-62B 피니튜닝된 ReAct는 모든 540B 프롬프트 방법을 능가한다. 대조적으로, 파이네튜닝 표준 또는 CoT는 PaLM-8/62B 모두에 대해 파이네튜닝 ReAct 또는 Act보다 훨씬 더 나쁘다. 전자는 본질적으로 지식 사실을 암기(잠재적으로 환각)하기 위한 모델에 도달하고 후자는 지식 추론을 위한 보다 일반화할 수 있는 기술인 위키피디아에서 정보에 액세스하기 위해 (이성 및) 행동하는 방법을 모델에 가르치기 때문이다. 모든 프롬프트 방법은 여전히 도메인별 최첨단 접근법과 상당히 거리가 있기 때문에(표 1), 더 많은 인간이 쓴 데이터로 피니튜닝하는 것이 ReAct의 힘을 푸는 더 나은 방법일 수 있다고 믿는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c l c c} \\hline \\hline  & Type & Definition & ReAct & CoT \\\\ \\hline \\multirow{2}{*}{Success} & True positive & Correct reasoning trace and facts & 94\\% & 86\\% \\\\  & False positive & Hallucinated reasoning trace or facts & 6\\% & 14\\% \\\\ \\hline \\multirow{4}{*}{Failure} & Reasoning error & Wrong reasoning trace (including failing to recover from repetitive steps) & 47\\% & 16\\% \\\\  & Search result error & Search return empty or does not contain useful information & 23\\% & \\\\ \\cline{1-1}  & Hallucinated reasoning trace or facts & 0\\% & 56\\% \\\\ \\cline{1-1}  & Label ambiguity & Right prediction but did not match the label precisely & 29\\% & 28\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 핫팟QA에 대한 ReAct 및 CoT의 성공 및 실패 모드의 유형과 인간이 연구한 무작위로 선택된 예제의 백분율이다.\n' +
      '\n' +
      '## 4 의사결정 작업\n' +
      '\n' +
      '또한 두 언어 기반 대화형 의사결정 태스크인 ALFWorld와 WebShop에 대해 ReAct를 테스트하는데, 이 두 태스크는 에이전트가 희박한 보상을 가지고 긴 지평에서 행동해야 하는 복잡한 환경을 특징으로 하며, 효과적으로 행동하고 탐색하기 위한 추론의 필요성을 보증한다.\n' +
      '\n' +
      'ALFWorldALFWorld(Shridhar et al., 2020b)(도 1(2))는 구체화된 ALFRED 벤치마크(Shridhar et al., 2020a)와 일치하도록 설계된 합성 텍스트 기반 게임이다. 여기에는 에이전트가 텍스트 동작을 통해 시뮬레이션된 가구를 탐색하고 상호작용하여 고수준의 목표를 달성해야 하는 6가지 유형의 작업(예: 데스크램프 아래 종이 검사)이 포함된다(예: 커피테이블 1로 이동, 종이 2 가져가, 데스크램프 1 사용). 작업 인스턴스는 50개 이상의 위치를 가질 수 있으며 해결 하기 위해 50 단계 이상의 전문가 정책을 수행할 수 있으므로 에이전트가 하위 목표를 계획 하 고 추적 하 고 체계적으로 탐색 (예: 모든 책상을 하나씩 확인) 합니다. 특히, ALFWorld에 내장된 한 가지 과제는 일반적인 가정용품(예: 데스크램프는 책상, 선반 또는 서랍에 있을 것)의 가능한 위치를 결정할 필요가 있으며, 이 환경을 LLM이 미리 훈련된 상식 지식을 활용하기에 적합하게 만든다. ReAct를 프롬프트하기 위해, 우리는 각 태스크 유형에 대한 훈련 세트로부터 무작위로 3개의 궤적에 주석을 달며, 여기서 각 궤적은 (1) 목표를 분해하는 희박한 생각, (2) 하위 목표 완성을 추적, (3) 다음 하위 목표를 결정하고, (4) 대상을 찾고 그것을 어떻게 해야 하는지 상식을 통해 이유를 포함한다. 부록 C.4에서 ALFWorld에 사용되는 프롬프트를 보여준다. Shridhar 등(2020b)에 이어서, 태스크-특정 설정에서 134개의 보이지 않는 평가 게임에 대해 평가한다. 강건성을 위해 주석이 달린 3개의 궤적에서 주석이 달린 2개의 궤적의 각 순열을 통해 각 작업 유형에 대해 6개의 프롬프트를 구성한다. 행위 프롬프트는 동일한 궤적을 사용하여 구성되지만 생각 없이 구성되는데, 작업 인스턴스는 훈련 세트에서 무작위로 선택되기 때문에 ReAct나 Act 중 어느 것도 선호하지 않으며 희박한 사고의 중요성을 테스트하기 위해 공정하고 통제된 비교를 제공한다. 베이스라인은 각 태스크 유형 5에 대해 \\(10^{5}\\)의 전문가 궤적에 대해 훈련된 모방 학습 에이전트인 ButLER(Shridhar et al., 2020b)를 사용한다.\n' +
      '\n' +
      '각주 5: Micheli & Fleuret(2021)는 3553개의 태스크 인스턴스에서 GPT-2 모델을 미세 조정하고 BUTLER보다 훨씬 향상된 성능을 달성했지만, 모든 태스크 유형에 대해 훈련되어 기준선으로 포함되지 않는다.\n' +
      '\n' +
      'WebShopCan ReAct는 실제 응용 프로그램을 위해 시끄러운 실제 언어 환경과도 상호 작용할 수 있습니까? 우리는 최근 제안된 온라인 쇼핑 웹사이트 환경인 WebShop(Yao et al., 2022)을 1.18M 실세계 제품과 12k 인간 지시로 조사한다. 웹샵은 ALFWorld와 달리, 고도로 다양한 구조화 및 비구조화된 텍스트(예를 들어, 아마존에서 크롤링된 제품 제목, 설명 및 옵션)를 포함하고, 에이전트가 사용자 지시에 기초하여 제품을 구매하도록 요구(예를 들어, "서랍이 있는 나이트스탠드를 찾고 있습니다. 니켈 마감을 가져야 하며, S140보다 낮은 프로젝트") 웹 상호작용을 통해(예를 들어, 검색 "나이트스탠드 서랍", "색상: 모던-니켈-화이트" 또는 "검색할 백"과 같은 버튼을 선택해야 한다. 이 작업은 500개의 테스트 지침에 대한 평균 점수(모든 에피소드에 걸쳐 평균화된 선택된 제품에 의해 커버되는 원하는 속성의 백분율) 및 성공률(선택된 제품이 모든 요구 사항을 충족하는 에피소드의 백분율)에 의해 평가된다. 우리는 검색, 제품 선택, 옵션 선택 및 구매에 대한 조치를 포함하는 법률 프롬프트를 공식화하고 ReAct 프롬프트를 추가로 추론하여 탐색할 사항, 구매 시기 및 지침과 관련된 제품 옵션을 결정한다. 예제 프롬프트는 표 6, 부록의 모델 예측은 표 10을 참조하십시오. 모방학습(IL) 방법과 비교\n' +
      '\n' +
      '그림 3: ReAct(ours) 및 기준선을 사용하여 HotPotQA에서 프롬프트 및 피니튜닝을 위한 스케일링 결과입니다.\n' +
      '\n' +
      '1,012개의 인간 주석 궤적으로 트레이닝되고, 10,587개의 트레이닝 명령어로 모조+강화 학습(IL+RL) 방법이 추가로 트레이닝된다.\n' +
      '\n' +
      'ResultsReAct는 ALFWorld(표 3)와 Webshop(표 4) 모두에서 Act보다 성능이 우수하다. ALFWorld에서 베스트 리액트 시험은 평균 71%의 성공률을 달성하여 베스트 액트(45%)와 버틀러(37%) 시험을 크게 능가했다. 사실, 더 나쁜 ReAct 시험(48%)조차도 두 방법 중 최고의 시험을 능가한다. 또한, 법률에 비해 ReAct의 이점은 6개의 통제된 시험에서 일관되며 상대적 성능 이득은 33%에서 90% 범위이고 평균 62%이다. 질적으로 우리는 아무런 생각 없이 법이 목표를 더 작은 하위 목표로 올바르게 분해하지 못하거나 환경의 현재 상태를 추적하지 못한다는 것을 보았다. ReAct와 Act를 비교한 예제 궤적은 부록 D.2.1 및 부록 D.2.2에서 찾을 수 있다.\n' +
      '\n' +
      '웹샵에서 원샷 프롬프트는 이미 IL 및 IL+RL 방법과 동등하게 수행한다. 추가 희소 추론으로, ReAct는 이전의 최상의 성공률에 비해 절대 10% 향상된 훨씬 더 나은 성능을 달성한다. 예들을 체크함으로써, ReAct가 시끄러운 관찰들과 액션들 사이의 갭을 메우기 위한 추론에 의해 지시-관련 제품들 및 옵션들을 식별할 가능성이 더 높다는 것을 발견한다(예를 들어, \'공간 절약형 거실용 광자 벤치\'의 경우, 아이템은 옵션 \'39x18x18inch\' 및 \'블루\'를 가지며 구매하기에 좋은 것으로 보인다). 그러나 기존 방법은 여전히 프롬프트 기반 방법에 여전히 어려운 훨씬 더 많은 제품 탐색 및 쿼리 재제형을 수행하는 전문가 인간(표 4)의 성능과 거리가 멀다.\n' +
      '\n' +
      '(p<0.05). 우리가 아는 한, ReAct는 폐루프 시스템 내의 상호 작용 환경에 적용된 LLM을 사용하여 결합된 추론과 행동의 첫 번째 시연이다. 아마도 가장 가까운 선행 작업은 Huang 등(2022b)의 Inner Monologue(IM)이며, 여기서 구체화된 에이전트로부터의 액션들은 익명의 "inner monologue"에 의해 동기 부여된다. **그러나 IM의 "내부 독백"은 목표를 충족하기 위해 에이전트가 완료해야 하는 환경 상태와 관찰에 국한됩니다.* * 이와 달리 의사 결정을 위한 ReAct의 추론 흔적은 유연하고 희박하여 다양한 추론 유형(섹션 2 참조)이 다른 작업에 대해 유도될 수 있습니다.\n' +
      '\n' +
      'ReAct와 IM의 차이점을 보여주고 내부 추론 대 내부 추론의 중요성을 강조한다. 외부 피드백에 대한 간단한 반응, IM과 같은 조밀한 외부 피드백으로 구성된 사고 패턴을 사용하여 절제 실험을 실행했다. 표 3에서 알 수 있는 바와 같이, ReAct는 IM-스타일 프롬프팅(ReAct-IM)보다 실질적으로 우수하다(71 대 53 전체 성공률). 6개 작업 중 5개 작업에서 일관된 이점이 있습니다. 질적으로 우리는 ReAct-IM이 높은 수준의 목표 분해 부족으로 인해 하위 목표가 언제 완료되었는지 또는 다음 하위 목표가 무엇인지 식별하는 데 종종 실수를 하는 것을 관찰했다. 또한 많은 ReAct-IM 궤적은 상식 추론의 부족으로 인해 항목이 ALFWorld 환경 내에서 어디에 있을 것인지 결정하기 위해 어려움을 겪었다. 두 가지 단점 모두 ReAct 패러다임에서 해결할 수 있다. ReAct-IM에 대한 더 상세한 내용은 부록 B.2에 있다. ReAct-IM에 대한 예시적인 프롬프트는 부록 C.4에서 찾을 수 있고, 예시적인 궤적은 부록 D.2.3에서 찾을 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c|c} \\hline \\hline Method & Pick & Clean & Heat & Cool & Look & Pick 2 & All \\\\ \\hline Act (best of 6) & 88 & 42 & 74 & 67 & 72 & **41** & 45 \\\\ ReAct (imp) & 65 & 39 & 83 & 76 & 55 & 24 & 57 \\\\ ReAct (best of 6) & **92** & 58 & **96** & 86 & **78** & **41** & **71** \\\\ \\hline ReAct-IM (imp) & 55 & 59 & 60 & 55 & 23 & 24 & 48 \\\\ ReAct-IM (notif) & 62 & **68** & 87 & 57 & 39 & 33 & 53 \\\\ \\hline BUTLER\\({}_{g}\\) (best of 6) & 33 & 26 & 70 & 76 & 17 & 12 & 22 \\\\ BUTLER (best of 6) & 46 & 39 & 74 & **100** & 22 & 24 & 37 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 알프베인 과제별 성공률(%). BUTLER 및 BUTLER\\({}_{g}\\) 결과는 Shridhar 등(2020b)의 표 4로부터 나온 것이다. 버틀러가 빔 탐색을 사용하는 것을 제외하고 모든 방법은 그리디 디코딩을 사용한다.\n' +
      '\n' +
      '## 5 관련 작업\n' +
      '\n' +
      '추론을 위한 언어 모델은 추론에 LLM을 사용하는 가장 잘 알려진 작업인 CoT(Chain-of-Thinking)일 것이다(Wei et al., 2022). 이는 LLM이 문제 해결을 위한 자신의 "사고 절차"를 공식화하는 능력을 보여준다. 이후, 복잡한 태스크를 해결하기 위한 최소-최대 프롬프팅(Zhou et al., 2022), 제로-샷-CoT(Kojima et al., 2022), 및 자기 일관성을 갖는 추론(Wang et al., 2022)을 포함하는 여러 후속 작업이 수행되었다. 최근 (Madaan & Yazdanbakhsh, 2022)에서는 CoT의 구성 및 구조를 체계적으로 연구하였으며, CoT의 효과성에 기호, 패턴, 텍스트의 존재가 중요하다는 것을 관찰하였다. 다른 작업들도 단순한 프롬프팅을 넘어 보다 정교한 추론 아키텍처로 확장되었다. 예를 들어 Selection-Inference(Cresswell et al., 2022)는 추론 과정을 "선택"과 "추론"의 두 단계로 나눈다. STAR(Zeiklman et al., 2022)은 모델 자체에 의해 생성된 정확한 유리들에 대해 모델을 피니튜닝함으로써 추론 프로세스를 부트스트랩한다. 신실한 추론(Cresswell & Shanahan, 2022)은 다단계 추론을 각각 전용 LM에 의해 수행되는 세 단계로 분해한다. 중간 계산 단계에서 LM을 미세화하는 Scratchpad(Nye et al., 2021)와 같은 유사한 접근법도 다중 단계 계산 문제에 대한 개선을 보여준다. 이러한 방법들과 대조적으로, ReAct는 단지 고립된 고정된 추론 이상의 것을 수행하고, 모델 동작들 및 그들의 대응하는 관찰들을 모델에 대한 일관성 있는 입력 스트림에 통합하여 더 정확하게 추론하고 추론 너머의 작업들(예를 들어, 상호작용적 의사결정)을 해결한다.\n' +
      '\n' +
      '의사 결정을 위한 언어 모델은 LLM의 강력한 기능으로 인해 언어 생성을 넘어 작업을 수행할 수 있게 되었으며, 특히 대화형 환경에서 LLM을 의사 결정을 위한 정책 모델로 활용하는 것이 점점 더 대중화되고 있다. WebGPT(Nakano et al., 2021)는 LM을 사용하여 웹 브라우저와 상호 작용하고, 웹 페이지를 탐색하며, ELIS(Fan et al., 2019)로부터 복잡한 질문에 대한 답변을 추론한다. ReAct와 비교하여 WebGPT는 사고 및 추론 절차를 명시적으로 모델링하지 않고 강화 학습을 위해 고가의 인간 피드백에 의존한다. 대화 모델링에서 BlenderBot(Shuster et al., 2022) 및 Sparrow(Glaese et al., 2022)와 같은 챗봇과 SimpleTOD(Hosseini-Asl et al., 2020)와 같은 태스크 지향 대화 시스템도 LMs가 API 호출에 대한 결정을 내리도록 훈련시킨다. ReAct와 달리 추론 절차도 명시적으로 고려하지 않으며 정책 학습을 위해 값비싼 데이터 세트와 인간 피드백 모음에 의존한다. 대조적으로, ReAct는 의사 결정 과정이 추론 절차의 언어 기술만 필요하기 때문에 훨씬 더 저렴한 방식으로 정책을 학습한다.\n' +
      '\n' +
      '각주 6: 인간의 피드백은 또한 보완적인 방식으로 통합될 수 있지만 우리는 그것을 향후 작업을 위해 남겨둔다.\n' +
      '\n' +
      'LLMS는 또한 계획 및 의사 결정을 위한 상호 작용 및 구현 환경에서 점점 더 많이 사용되고 있다. 아마도 이러한 점에서 ReAct와 가장 관련이 있는 것은 SayCan(Ahn et al., 2022) 및 Inner Monologue(Huang et al., 2022)이며, 이는 로봇 액션 계획 및 의사 결정을 위해 LLM을 사용한다. 세이캔에서 LLM은 로봇이 취할 수 있는 가능한 행동을 직접 예측하도록 촉구되었으며, 이는 최종 예측을 위해 시각적 환경에 기반을 둔 어포던스 모델에 의해 순위가 조정된다. 내부 독백은 환경으로부터 주입된 피드백으로 구현되는 동의어 "내부 독백"을 추가하여 추가 개선을 했다. 알고 있는 범위 내에서는, 내부 독백은 ReAct가 기반으로 하는 이러한 폐쇄 루프 시스템을 보여주는 첫 번째 작업이다. 그러나 Inner Monologue는 진정으로 내부 생각을 구성하지 않는다고 주장합니다. 이는 섹션 4에서 자세히 설명됩니다. 또한 대화형 의사 결정 과정에서 의미적으로 풍부한 입력으로 언어를 활용하는 것이 다른 설정에서 성공적인 것으로 나타났습니다 (Abramson et al., 2020; Karamcheti et al., 2021; Huang et al., 2022; Li et al., 2022). LLM의 도움으로 기본적인 인지 메커니즘으로서의 언어가 상호 작용과 의사 결정에 중요한 역할을 할 것이라는 것이 점점 더 분명해지고 있다. 더욱이, LLM의 진보는 또한 리드 등(2022)과 같은 다재다능하고 일반주의적인 에이전트의 개발에 영감을 주었다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '우리는 큰 언어 모델에서 추론과 작용을 시너지 효과를 내기 위한 간단하면서도 효과적인 방법인 ReAct를 제안했다. 다중 홉 질의 응답, 사실 확인 및 대화형 의사 결정 작업에 대한 다양한 실험을 통해 ReAct가 해석 가능한 의사 결정 흔적과 함께 우수한 성능으로 이어진다는 것을 보여준다. 이 방법의 단순성에도 불구하고, 큰 액션 공간을 가진 복잡한 작업은 더 많은 데모를 필요로 하여 잘 학습해야 하는데, 안타깝게도 인-컨텍스트 학습의 입력 길이 한계를 쉽게 넘어설 수 있다. 우리는 HotpotQA에서 초기 유망한 결과를 사용하여 미세 조정 접근법을 탐구하지만 더 고품질의 인간 주석을 통해 학습하는 것이 성능을 더욱 향상시키는 데 데데데라타가 될 것이다. 다중 작업 훈련으로 ReAct를 확장하고 강화 학습과 같은 보완 패러다임과 결합하면 더 강력한 에이전트가 더 많은 응용 프로그램을 위한 LLM의 잠재력을 더욱 잠금 해제할 수 있다.\n' +
      '\n' +
      '#### Acknowledgments\n' +
      '\n' +
      '구글 브레인 팀과 프린스턴 NLP 그룹의 많은 분들의 지원과 피드백에 감사드립니다. 이 작업은 보조금 번호 2107048에 따라 국립 과학 재단에 의해 부분적으로 지원되었다. 이 자료에서 표현된 의견, 결과 및 결론 또는 권장 사항은 저자의 것이며 국립 과학 재단의 견해를 반드시 반영하는 것은 아니다.\n' +
      '\n' +
      '#### Reproducibility Statement\n' +
      '\n' +
      '우리의 주요 실험은 아직 공개 접근 가능한 모델이 아닌 PaLM(Chowdhery et al., 2022)에 대해 수행된다. 재현성을 높이기 위해 부록 C에 사용된 모든 프롬프트, 부록 A.1의 GPT-3(브라운 등, 2020)을 사용한 추가 실험 및 관련 GPT-3 ReAct 프롬프트 코드를 [https://anonymous.4open.science/r/ReAct-2268/](https://anonymous.4open.science/r/ReAct-2268/)에 포함했다.\n' +
      '\n' +
      '### Ethics Statement\n' +
      '\n' +
      'ReAct는 큰 언어 모델이 이전 방법보다 더 많은 인간 해석 가능, 진단 가능 및 제어 가능한 작업 해결 궤적을 생성하도록 프롬프트한다. 그러나, 외부 환경(예를 들어, 웹, 물리적 환경)과 상호작용하기 위해 큰 언어 모델을 액션 공간과 연결시키는 것은 부적절한 또는 사적인 정보를 찾거나 환경에서 유해한 액션을 취하는 것과 같은 잠재적인 위험을 갖는다. 우리의 실험은 액션 공간 설계에서 어떤 위험한 행동 없이 개인 정보가 없는 특정 웹사이트(위키피디아 또는 웹샵)로 상호 작용을 제한함으로써 그러한 위험을 최소화한다(즉, 모델은 실제로 웹샵에서 연구 벤치마크를 구매하거나 위키피디아를 편집할 수 없다). 우리는 연구자들이 향후 더 광범위한 실험을 설계하기 전에 그러한 위험을 인식해야 한다고 믿는다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Abramson et al. (2020) Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim Harley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap, Kory Mathewson, Sona Mokra, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant Varma, Greg Wayne, Duncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating interactive intelligence, 2020. URL [https://arxiv.org/abs/2012.05672](https://arxiv.org/abs/2012.05672).\n' +
      '* Ahn 등(2020)은 Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyu i가 말하는 것이 아니라 할 수 있는 대로 수행합니다. 로봇 어포던스에서 접지 언어, 2022. URL [https://arxiv.org/abs/2204.01691](https://arxiv.org/abs/2204.01691).\n' +
      '* Alderson-Day and Fernyhoush (2015) Ben Alderson-Day and Charles Fernyhoush. Inner speech: development, cognitive function, phenomenology and neurobiology. _ Psychological bulletin_, 141(5):931, 2015.\n' +
      '* Baddeley (1992) Alan Baddeley. 작업 메모리입니다. _ Science_, 255(5044):556-559, 1992).\n' +
      '* Brown et al.(2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _ Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '\n' +
      '* Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _ arXiv preprint arXiv:2204.02311_, 2022.\n' +
      '* Creswell & Shanahan (2022) Antonia Creswell and Murray Shanahan. 대형 언어 모델을 사용한 충실 한 추론, 2022. URL [https://arxiv.org/abs/2208.14271](https://arxiv.org/abs/2208.14271).\n' +
      '* Creswell et al.(2022) Antonia Creswell, Murray Shanahan, and Irina Higgins. 선택 추론: 해석 가능한 논리적 추론을 위해 대규모 언어 모델을 사용합니다. 2022. URL [https://arxiv.org/abs/2205.09712](https://arxiv.org/abs/2205.09712).\n' +
      '* Fan 등(2019) Angelia Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: 긴 폼 질문 응답. _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pp. 3558-3567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL [https://aclanthology.org/P19-1346](https://aclanthology.org/P19-1346).\n' +
      '* Fernyhough 등(2010) Charles Fernyhough. 비고츠키, 루리아, 그리고 사회적 뇌 Self and social regulation: Social interaction and development of social understanding and executive functions_, pp. 56-79, 2010.\n' +
      '* Glaese et al. (2020) Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Sona Mokra, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. 대상 인간 판단을 통해 대화 에이전트의 정렬 개선, 2022. URL [https://storage.googleapis.com/deepmind-media/DeepMind.com/Authors-Notes/sparrow/sparrow-final.pdf](https://storage.googleapis.com/deepmind-media/DeepMind.com/Authors-Notes/sparrow/sparrow-final.pdf).\n' +
      '* Hosseini-Asl 등(2020) Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. 태스크 지향 대화를 위한 간단한 언어 모델입니다. _ Advances in Neural Information Processing Systems_, 33:20179-20191, 2020.\n' +
      '* Huang 등(2022a) Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 제로샷 플래너로서의 언어 모델: 구체화된 에이전트에 대한 실행가능한 지식 추출. _ arXiv preprint arXiv:2201.07207_, 2022a.\n' +
      '* Huang et al.(2022b) Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. _ arXiv preprint arXiv:2207.05608_, 2022b.\n' +
      '* Karamcheti 등(2021) Siddharth Karamcheti, Megha Srivastava, Percy Liang, and Dorsa Sadigh. 라일라: 언어에 정통한 잠재 행동. _CoRL_에서 pp. 1379-1390, 2021. URL [https://proceedings.mlr.press/v164/karamcheti22a.html](https://proceedings.mlr.press/v164/karamcheti22a.html)\n' +
      '* Kojima 등(2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 대형 언어 모델은 영샷 추론기입니다. _ arXiv preprint arXiv:2205.11916_, 2022.\n' +
      '* Lazaridou et al.(2022) Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 오픈 도메인 질문 답변에 대한 몇 번의 프롬프트를 통해 인터넷 확장 언어 모델입니다. _ arXiv preprint arXiv:2203.05115_, 2022.\n' +
      '* Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. _ Advances in Neural Information Processing Systems_, 33:9459-9474, 2020.\n' +
      '* Li 등(2022) Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyurek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, and Yuke Zhu. 대화형 의사 결정을 위해 미리 훈련된 언어 모델, 2022. URL [https://arxiv.org/abs/2202.01771](https://arxiv.org/abs/2202.01771).\n' +
      '\n' +
      '* Luria (1965) Aleksandr Romanovich Luria. Ls vygotsky 및 함수의 위치 지정 문제 _ Neuropsychologia_, 3(4):387-392, 1965).\n' +
      '* Madaan & Yazdanbakhsh (2022) Aman Madaan and Amir Yazdanbakhsh. 텍스트 및 패턴: 효과적인 사고 체인의 경우 탱고, 2022. URL [https://arxiv.org/abs/2209.07686](https://arxiv.org/abs/2209.07686)에 두 개가 필요합니다.\n' +
      '* Micheli & Fleuret (2021) Vincent Micheli and Francois Fleuret. 언어 모델은 소수의 집사입니다. _ arXiv preprint arXiv:2104.07972_, 2021.\n' +
      '* Nakano 등(2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: 인간 피드백을 사용하는 브라우저 지원 질문 응답, 2021. URL [https://arxiv.org/abs/2112.09332](https://arxiv.org/abs/2112.09332).\n' +
      '* Nye 등(2021) Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 작업 표시: 언어 모델을 사용하여 중간 계산을 위한 Scratchpads, 2021. URL [https://arxiv.org/abs/2112.00114](https://arxiv.org/abs/2112.00114).\n' +
      '* Reed 등(2022) Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. 일반 에이전트, 2022. URL [https://arxiv.org/abs/2205.06175](https://arxiv.org/abs/2205.06175)\n' +
      '* Shridhar 등(2020a) Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 알프레드: 일상적인 작업에 대한 근거 있는 지침을 해석하기 위한 벤치마크입니다. _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10740-10749, 2020a.\n' +
      '* Shridhar 등(2020b) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: 대화형 학습을 위해 텍스트 및 구체화된 환경을 정렬합니다. _ arXiv preprint arXiv:2010.03768_, 2020b.\n' +
      '* Shuster 등(2022a) Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston. 지식을 찾는 언어 모델: 대화 및 신속한 완료를 위한 모듈식 검색 및 생성 _ arXiv preprint arXiv:2203.13224_, 2022a.\n' +
      '* Shuster 등(2022b) Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. 블렌더봇 3: 책임감 있게 참여하도록 지속적으로 배우는 배포된 대화 에이전트, 2022b. URL [https://arxiv.org/abs/2208.03188](https://arxiv.org/abs/2208.03188).\n' +
      '* Thorne 등(2018) James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 열: 사실 추출 및 확인을 위한 대규모 데이터 세트입니다. _ arXiv preprint arXiv:1803.05355_, 2018.\n' +
      '* Vygotsky(1987) Lev S Vygotsky. 생각과 연설 수집한 LS 비고츠키_, 1987년 1:39-285의 작품이다.\n' +
      '* Wang 등(2022a) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 자기 일관성은 2022a 언어 모델에서 사고 추론의 사슬을 개선합니다. URL [https://arxiv.org/abs/2203.11171](https://arxiv.org/abs/2203.11171).\n' +
      '* Wang 등(2022b) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 언어 모델의 합리적인 확장 앙상블입니다. _ arXiv preprint arXiv:2207.00747_, 2022b.\n' +
      '* Wei 등(2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 생각의 사슬 프롬프트는 대규모 언어 모델에서 추론을 이끌어냅니다. _ arXiv preprint arXiv:2201.11903_, 2022.\n' +
      '* Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: 다양 하 고 설명 가능한 다중 홉 질문 응답을 위한 데이터 세트입니다. _ arXiv preprint arXiv:1809.09600_, 2018.\n' +
      '\n' +
      '*Yao 등(2020) Shunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. CALM을 유지하고 탐색: 텍스트 기반 게임에서 액션 생성을 위한 언어 모델입니다. _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pp. 8736-8754, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.704. URL [https://aclanthology.org/2020.emnlp-main.704](https://aclanthology.org/2020.emnlp-main.704)\n' +
      '*Yao 등(2022) Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 웹bsp: 지상 언어 에이전트와 확장 가능한 실제 웹 상호 작용을 향합니다. _ arXiv preprint arXiv:2207.01206_, 2022.\n' +
      '* Zelikman 등(2022) Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022. URL [https://arxiv.org/abs/2203.14465](https://arxiv.org/abs/2203.14465)\n' +
      '* Zhou 등(2022) Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 최소 최대 프롬프트는 2022년 대형 언어 모델에서 복잡 한 추론을 사용 하도록 설정 합니다. URL [https://arxiv.org/abs/2205.10625](https://arxiv.org/abs/2205.10625).\n' +
      '* Zhu 등(2021) Yunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, and Xueqi Cheng. 개방형 질문 응답을 위한 적응형 정보 검색 _ arXiv preprint arXiv:2109.06747_, 2021.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:14]\n' +
      '\n' +
      '인간이 모델 매개변수를 변경할 수 없고 몇 가지 동작을 변경해도 나머지 모델 동작을 편집할 수 없기 때문에 Act 및 이전 RL 방법에는 어렵다. 이 패러다임은 또한 Huang 등(2022b)에서와 같이 목표 또는 하위 목표를 업데이트하기 위한 인간 대화 이상이다. - ReAct 생각을 편집하는 것이 이러한 것들을 할 수 있지만, 더 나은 과제 해결을 위해 모델의 내부 신념, 추론 스타일 또는 유연한 사고 공간이 지원하는 모든 것을 수정할 수도 있다. 우리는 이것이 인간 정렬을 위한 흥미로운 방향이라고 믿고 향후 작업으로 보다 체계적인 연구를 남겨둔다.\n' +
      '\n' +
      '## 부록 B 실험 세부 정보\n' +
      '\n' +
      '### HotpotQA Finetuning Details\n' +
      '\n' +
      '모든 핀튜닝은 64의 배치 크기를 사용한다. PaLM-8B에서는 \\(4,000\\) 단계의 ReAct 및 Act 방법과 \\(2,000\\) 단계의 Standard 및 CoT 방법을 핀튜닝한다. PaLM-62B에서는 \\(4,000\\) 단계에 대한 ReAct 및 Act 방법과 \\(1,000\\) 단계에 대한 Standard 및 CoT 방법을 조정한다. ReAct 및 Act 방법은 일반적으로 더 많은 훈련 단계(및 더 많은 훈련 데이터)로부터 이익을 얻는 반면 표준 및 CoT 방법은 피니튜닝 직후에 저하된다는 것을 발견했다.\n' +
      '\n' +
      '### AlfWorld IM-Style Details\n' +
      '\n' +
      'IM 스타일 절제의 경우 ReAct에 사용된 동일한 전문가 궤적에 이러한 궤적 내에서 조밀한 외부 피드백 생각이 재주석되어 ReAct-IM이 (1) 현재 목표를 분해하고 (2) 완료해야 하는 현재 하위 목표에 대해서만 생각하도록 제한한다. 특히 ReAct-IM은 (1) 하위 목표가 언제 완료되었는지 결정하는 생각(2) 다음 하위 목표가 무엇이어야 하는지 결정하는 생각(3) LLM이 환경 내에서 항목이 어디에 있을 수 있는지 식별하기 위해 내부 사전 훈련 지식을 참조하도록 유도하는 생각이 부족하다.\n' +
      '\n' +
      '도 5: 알프월드에서의 ReAct를 갖는 인간-인-더-루프 거동 교정 예. (a) 환각 사고로 인해 ReAct 궤적이 실패한다(Act 17). (b) 인간이 단순히 두 개의 생각을 편집함으로써(Act 17, 23), ReAct 궤적은 바람직한 추론 흔적과 행동을 생성하고 성공한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:16]\n' +
      '\n' +
      '액션 3 마무리[감독, 시나리오 작가, 배우] 질문 & 어떤 잡지가 아서의 첫 번째 잡지와 여성을 위한 첫 번째 잡지로 시작되었는가? Action 1 Search[Arthur\'s Magazine] Observation 1 Arthur\'s Magazine(1844-1846)은 19세기 필라델피아에서 출판된 미국 문학 정기 간행물이었다. Action 2 Search[First for Women] Observation 2 First for Women은 미국의 Bauer Media Group에서 발행하는 여성 잡지입니다.[11] 이 잡지는 1989년에 시작되었다. 액션 3 마감[아서의 잡지] 질문과 파벨 우리존과 레오니드 레빈은 같은 종류의 작품으로 알려져 있는가? Action 1 Search[Pavel Urysohn] Observation 1 Pavel Samulovich Urysohn (1989년 2월 3일 - 1924년 8월 17일)은 차원 이론에 기여한 것으로 가장 잘 알려진 소비에트 수학자였다. Action 2 Search[Leonid Levin] Observation 2 Leonid Anatabievich Levin은 소비에트계 미국 수학자이자 컴퓨터 과학자이다. 3조항 마무리 질문 & 콜로라도 구루젠의 동부 지역이 확장되는 지역의 고도 범위는 무엇입니까? 차근차근 생각해 봅시다. 콜로라도 조산운동의 동부 부문은 높은 평원으로 확장된다. 높은 평원은 약 1,800피트에서 7,000피트로 상승하므로 답은 1,800피트에서 7,000피트이다. 답변 1,800~7,000피트 질문&뮤지션과 풍자자 앨리 고어츠가 ‘심슨 가족’ 사상에 대한 노래를 작사해 차근차근 생각해보자. 밀하우스는 미국 대통령 리처드 앤서 & 리처드 닉슨 질문 & 핀란드의 록 그룹, 아담 클레이튼 파월 또는 사이마스 제스처에 관한 다큐멘터리의 이름을 따서 명명되었다. 차근차근 생각해 봅시다. 아담 클레이튼 파월(영화)은 핀란드의 록 그룹이 아닌 아프리카계 미국인 정치인에 관한 다큐멘터리이다. 그렇다면 핀란드 록그룹에 관한 다큐멘터리는 대신 \'사이마스 제스쳐 답변\'과 \'사이마스 제스쳐 질문\'이어야 하며 니콜라스 레이와 엘리아 카잔의 공통점은 무엇인가? 차근차근 생각해 봅시다. 니콜라스 레이의 직업은 감독, 시나리오 작가, 그리고 배우이다. 엘리아 카잔의 직업은 감독, 프로듀서, 시나리오 작가, 배우이다. 니콜라스 레이와 엘리아 카잔의 공통점은 감독, 시나리오 작가, 배우입니다. 대답 & 감독, 시나리오 작가, 배우 질문 & 어떤 잡지가 아서의 잡지 또는 여성을 위한 첫 번째 잡지로 시작되었나요? 차근차근 생각해 봅시다. 아더 매거진은 1844년에 시작되었습니다. 첫 번째 여성은 1989년에 시작되었습니다. 1844년(아더 매거진) < 1989년(여성들을 위한 첫 번째) 그래서 아더 매거진 답변 & 아더 매거진 질문 & 파벨 우리존과 레오니드 레빈은 같은 종류의 작품으로 유명했나요? 차근차근 생각해 봅시다. 파벨 우리존은 수학자입니다. 레오니드 레빈은 수학자이자 컴퓨터 과학자이다. 그래서 파벨 우리존과 레빈은 같은 종류의 일을 하고 있습니다. 대답 & 예 & 웨이트 질문 & 웨이트는 콜로라도 조산운동의 동부 지역이 확장되는 지역의 고도 범위이다. 생각 1 저는 콜로라도 조산운동을 조사해야 합니다. 콜로라도 조산운동의 동쪽 부분이 확장되어 있는 지역을 찾고, 그 지역의 고도 범위를 찾아야 합니다. Action 1 Search[Color orogeny] Observation 1 콜로라도 orogeny는 콜로라도와 주변 지역의 산악 건물(조산)의 에피소드였다. 생각 2 동부 부문은 언급하지 않는다. 그래서 나는 동부 지역을 찾아볼 필요가 있다. 행동 2 룩업[동부 부문] 관찰 2 (결과 1/1) 동부 부문은 고평원까지 확장되어 중부평원 조산운동이라고 불린다. 3 콜로라도 조산운동의 동부 부문은 하이 플레인으로 확장된다. 그래서 고평원을 검색해서 고도 범위를 찾아야 해요. 3단계 관찰 3단계 높은 평원은 두 개의 별개의 토지 지역 중 하나를 가리킵니다. 4단계 나는 대신 높은 평원을 검색해야 합니다. Action 4 Search[High Plains (미국)]\n' +
      '관찰 4  높은 평원은 대평원의 소지역이다. 동쪽에서 서쪽으로  High Plains 상승은 약 1,800~7,000 ft (550~2,130 m)입니다.[3] 5  High Plains이 1,800~7,000 ft 정도 상승한다고 생각했기 때문에  답변은 1,800~7,000 ft입니다. 액션 5  마무리[1,800~7,000피트]  질문  음악가와 풍자적인 고르츠는 "심슨 가족" 캐릭터 밀하우스에 관한 노래를 썼는데, 맷 그로닝은 누구의 이름을 따서 이름을 지었는가? 생각 1  질문이 "심슨 가족" 캐릭터 밀하우스로 단순화되면  Who의 이름을 따서 이름이 붙여집니다. 밀하우스를 뒤져서 이름만 알아내면 돼 Action 1  Search(밀하우스)  Observation 1  Millhouse Mussollini Van Houten은 Pamela Hayden이 목소리를 내고 Matt Groening이 만든 폭스 애니메이션  텔레비전 시리즈의 반복 캐릭터입니다. 생각 2  이 단락에서는 밀하우스가 누구의 이름을 따서 지었는지 알 수 없습니다. 아마도 나는 이름을 따서 찾아볼 수 있을 것입니다. Action 2  Lookup(다음 이름)  Observation 2 (Result 1 / 1 Milhouse는 미국 대통령 Richard Nixon의 이름을 따서 명명되었으며, 중간 이름은 Milhous이다. 3  밀하우스의 이름은 리처드 닉슨 미국 대통령의 이름을 따서 지어졌다고 생각했기 때문에 대답은 리처드 닉슨이다. 액션 3  Finish[Richard Nixon]  질문  핀란드의 록 그룹인 Adam Clayton Powell과 Nationaleasure에 관한 다큐멘터리 생각 1  나는 Adam Clayton Powell과 Nationaleasure를 검색하고 핀란드의 록그룹에 관한 다큐멘터리를 찾아야 한다. Action 1  검색(Adam Clayton Powell)  관찰 1  찾을 수 없음(Adam Clayton Powell) 유사: [\'Adam Clayton Powell  III\', \'Severon Avenue (M Manhattan)\', \'Adam Clayton Powell Jr. State Office  Building\', \'Isabel Washington Powell\', \'Adam Powell\', \'Adam Clayton Powell (필름), \'Giancarlo Esposito\' 생각 2  Action 2  Search(Adam Clayton Powell (필름))  Observation 2  Adam Clayton Powell은 Richard Klibberg가 감독한 1998년 미국 다큐멘터리 영화이다. 이 영화는 영향력 있는 아프리카계 미국인 정치인 애덤 클레이튼 파월 주니어의 흥망성쇠에 관한 것이다[3][4]. 그것은 나중에 PBS 시리즈 \'미국 경험\'의 일부로 방영되었다. 생각 3 아담 클레이튼 파월(영화)은 핀란드의 록 그룹이 아닌 아프리카계 미국인 정치인에 관한 다큐멘터리이다. 그래서 핀란드의 록 그룹들에 관한 다큐멘터리는 대신에 아시아인 제스쳐가 되어야 한다. Action 3  Finish(The  Asians Gesture)  Question  Nicholas Ray와 Elia Kazan의 공통점은 무엇인가? 생각 1  니콜라스 레이와 엘리아 카잔을 검색하고 그들의 직업을 찾은 다음 그들이 공통으로 가지고 있는 직업을 찾아야 한다. Action 1  Search(Nicholas Ray)  Observation 1  Nicholas Ray (born Raymond Nicholas Kirzle Jr., 1911, 8월 7일 - 1979년 6월 16일)는 1955년 영화 Rebel With a Cause 로 가장 잘 알려진 미국 영화 감독, 시나리오 작가, 배우였다. 생각 2  니콜라스 레이의 직업은 감독, 시나리오 작가, 배우이다. 다음엔 엘리아 카잔을 찾아서 직업을 찾아야 해요 Action 2  Search(Elia Kazan)  Observation 2  Ellis Kazan은 미국 영화 및 극장 감독, 프로듀서, 시나리오 작가, 배우였다. 생각 3  엘리아 카잔의 직업은 감독, 프로듀서, 시나리오 작가, 배우이다. 니콜라스 레이와 엘리아 카잔이 공통적으로 가지고 있는 직업은 감독, 시나리오 작가, 그리고 배우입니다. Action 3  마무리(감독, 시나리오 작가, 배우)  질문  Arthur\'s Magazine과 First for Women 중 어떤 잡지가 먼저 시작되었는가? 나는 아서의 잡지와 여성 우선지를 검색하고 무엇이 먼저 시작되었는지를 찾아야 한다고 생각했다. 질문 1  검색(Arthur\'s Magazine)  관찰 1  Arthur\'s Magazine (1844-1846)은 19세기 필라델피아에서 출판된 미국 문학 정기 간행물이다. 아서의 잡지 2는 1844년에 시작되었다고 생각했다. 나는 먼저 여성들을 검색해야 한다. Action 2  Search(First for Women)  Observation 2  First for Women은 미국 Bauer Media Group에서 발행하는 여성 잡지.[1] 이 잡지는 1998년에 시작되었습니다.  사상 3  First for Women은 1999년에 시작되었습니다. 1844(Arthur\'s Magazine) < 1989(First for Women) 그래서 Arthur\'s Magazine이 먼저 시작되었습니다. 액션 3  마무리[아서의 잡지]  질문  파벨(같은 종류의 작품으로 알려진 리존과 레오니드 레빈) 생각 1  나는 파벨 우리존과 레오니드 레빈을 검색하고, 그들의 작업 유형을 찾고, 그들이 같은지 찾아야 한다. Action 1  Search(Pavel Urysohn) ```\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:20]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:21]\n' +
      '\n' +
      '### Webbsp\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{142.3pt} p{142.3pt}} \\hline \\hline \\multicolumn{1}{c}{**Instruction:** i would like a 3 ounce bottle of bright citrus dedorant for sensitive skin, and price lower than 50.00 dollars \\\\ Act & \\multicolumn{1}{c}{Re_Act} \\\\ \\multicolumn{1}{c}{**Action: search[3 ounce bright citrus dedorant sensitive skin]**} & **Action: search[3 ounce bright citrus dedorant sensitive skin \\\\\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:23]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} \\hline \\hline You are in the middle of a room. Looking quickly around you, you see a cabinet 13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet 7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a cofferencing 1, a countertop 1, a diningtable 1, a driver 1, a fridge 1, a grahsgosem 1, a microwave 1, a shell 3, a shelf 2, a shelf 1, a sinkhasin 1, a stoveburner 4, a stoveburner 2, a stoveburner 1, and a toaster 1. Your task is: put: a clean lettuce in diningtable. \\\\ \\textgreater{} think: To solve the task, I need to find and take a lettuce, then clean it with sinkhasin, then put it in diningtable. \\\\ \\textgreater{} think: First I need to find a lettuce. A lettuce is more likely to appear in fridge (1), diningtable (1), sinkhasin (1), stoveburner (1-3), cabinet (1-13). I can check how by one, starting with fridge 1. \\\\ OK. \\\\ \\textgreater{} go to fridge 1 \\\\ The fridge 1 is closed. \\\\ \\textgreater{} open fridge 1 \\\\ You open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a potato 3, and a potato 2. \\\\ \\textgreater{} go to diningtable 1 \\\\ On the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork 2, a knife 2, a knife 1, a lable 1, a mug 2, a mug 1, a pan 2, a perexphrase 1, a spatula 3, a tomato 2, and a tomato 1. \\\\ \\textgreater{} think: Now I find a lettuce (1). Next, I need to take it. \\\\ OK. \\\\ \\textgreater{} take lettuce 1 from diningtable 1 \\\\ You pick up the lettuce 1 from the diningtable 1. \\\\ \\textgreater{} think: Now I take a lettuce (1). Next, I need to go to sinkbasin (1) and clean it. \\\\ OK. \\\\ \\textgreater{} go to sinkbasin 1 \\\\ On the sinkbasin 1, you see a apple 2, a lable 2, a spoon 1, and a tomato 3. \\\\ \\textgreater{} clean lettuce 1 with sinkbasin 1 \\\\ You clean the lettuce 1 using the sinkbasin 1. \\\\ \\textgreater{} think: Now I clean a lettuce (1). Next, I need to put it in/on diningtable 1. \\\\ OK. \\\\ \\textgreater{} go to diningtable 1 \\\\ On the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork 2, a knife 1, a lable 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a spatula 3, a tomato 2, and a tomato 1. \\\\ \\textgreater{} put lettuce 1 in/on diningtable 1 \\\\ You put the lettuce 1 in/on the diningtable 1. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: ALFWorld 클린 작업에 대한 ReAct 프롬프트.\n' +
      '\n' +
      '## Appendix D Trajectories\n' +
      '\n' +
      '### Fever Trajectories\n' +
      '\n' +
      '우리는 ReAct, CoTand Acton에 의해 생성된 몇 가지 궤적을 아래의 FEVER 개발에서 무작위로 선택된 몇 가지 예를 제공한다. 검색 결과는 공백에 생략됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} \\hline \\hline You are in the middle of a room. Looking quickly around you, you see a cabinet 13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet 7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coifferencending 1, a countertop 1, a dishitable 1, a driver 1, a fridge 1, a garbagoscan 1, a microwave 1, a shell 3, a shelf 2, a shelf 1, a sinkhasin 1, a stoveburner 4, a stoveburner 2, a stoveburner 2, a stoveburner 1, and a toaster 1. Your task is to: put a clean lettuce in dininatable. \\\\ \\textgreater{} think: To solve the task, I need to find and take a lettuce, then clean it with sinkhasin, then put it in dininatable. \\\\ \\textgreater{} think: First I need to find a lettuce. \\\\ OK. \\textgreater{} go to fridge 1 \\\\ The fridge 1 is closed. \\\\ \\textgreater{} think: First I need to find a lettuce. \\\\ OK. \\textgreater{} open fridge 1 \\\\ You open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a potato 3, and a potato 2. \\\\ \\textgreater{} think: First I need to find a lettuce. \\\\ OK. \\textgreater{} go to dininatable 1 On the dininable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a peppershark 1, a spatula 3, a tomato 2, and a tomato 1. \\\\ \\textgreater{} think: First I need to find a lettuce. \\\\ OK. \\textgreater{} take lettuce 1 from dininable 1 \\\\ You pick up the lettuce 1 from the dininable 1. \\\\ \\textgreater{} think: I need to clean this lettuce (1) using sinkhasin 1. \\\\ OK. \\textgreater{} go to sinkhasin 1 On the sinkhasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3. \\\\ \\textgreater{} think: I need to clean this lettuce (1) using sinkhasin 1. \\\\ OK. \\textgreater{} clean lettuce 1 with sinkhasin 1 \\\\ You clean the lettuce 1 using the sinkhasin 1. \\\\ \\textgreater{} think: I need to put this cleaned lettuce (1) in/on dininable 1. \\\\ OK. \\textgreater{} go to dininable 1 On the dininable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork 2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a spatula 3, a tomato 2, and a tomato 1. \\\\ \\textgreater{} think: I need to put this cleaned lettuce (1) in/on dininable 1. \\\\ OK. \\textgreater{} put lettuce 1 in/on dininable 1 \\\\ You put the lettuce 1 in/on the dininable 1. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: ALFworld clean 태스크에 대한 ReAct-IM 프롬프트.\n' +
      '\n' +
      '실시예 2491(gt: REFUTES)\n' +
      '\n' +
      '```\n' +
      'RealSet Claim: 버뮤다 삼각지대는 태평양에서 느슨하게 정의된 지역이다. 생각 1: 버뮤다 삼각지대를 검색해서 태평양에 있는지 찾아야 합니다. Action 1: Search[Bermuda Triangle] Observation 1: Reportedly hauntted locations.... 생각 2: 관측 결과 북대서양의 서쪽에 있기 때문에 태평양에 있지 않다. Action 2: Finish[RFPUTS] Observation 2: Episode finished\n' +
      '```\n' +
      '\n' +
      '주장: 버뮤다 삼각지대는 태평양에서 느슨하게 정의된 지역이다. Action 1 : Search[Bermuda Triangle] Observation 1 : Reportly hauntted locations.... Action 2: Finish[RFPUTS] Observation 2: Episode finished ```\n' +
      '주장: 버뮤다 삼각지대는 태평양에서 느슨하게 정의된 지역이다. 생각: 차근차근 생각해 봅시다. 버뮤다 삼각지대는 대서양에서 느슨하게 정의된 지역이다. 대서양에서. 태평양은 대서양이 아니다.\n' +
      '\n' +
      'Answer: RPUTS\n' +
      '\n' +
      '실시예 25908(gt: SUPPORTS)\n' +
      '``` RealSet Claim: Princess Mononoke is a film. Thought 1: I need to search the Princess Mononoke and find if it is film. Action 1: Search[Princess Mononoke] Observation 1: Princess Mononoke... Thought 2: The observation from the Princess Mononoke is a film. Action 2: Finish[SUPORTS] Observation 2: Episode finished ```\n' +
      '클레임: 모노노크 공주는 영화입니다. 동작 1: 서치 모노노크 공주 관찰 1: 모노노크 공주... 동작 2: 종료[SUPORTS] 관찰 2: 에피소드 종료\n' +
      '```\n' +
      '\n' +
      '클레임: 모노노크 공주는 영화입니다. 생각 1: 차근차근 생각해 봅시다. 모노노크 공주는 영화이고, 영화는 영화입니다. 응답: SUPPOR\n' +
      '\n' +
      '실시예 2591(gt: REFUTES)\n' +
      '\n' +
      '```\n' +
      '리얼셋 주장: 소유즈는 미국 우주 프로그램의 일부였다. 생각 1: 소유즈를 검색해서 그것이 미국 우주 프로그램의 일부였는지 찾아야 한다. Action 1: Search[Soyuz] Observation 1: Souz is a transliteration... 생각 2: 그것은 미국 우주 프로그램을 언급하지 않기 때문에, 나는 그것이 그것의 일부였는지 찾기 위해 "미국 우주 프로그램"을 검색해야 한다. Action 2: Search[American Space Program] Observation 2: Space Policy... 생각 3: 소유즈를 언급하지 않아서, 나는 이 주장이 사실인지 아닌지 확신할 수 없다. Action 3: Finish[NOT ENOUGH INFO] Observation 3: Episode finish\n' +
      '```\n' +
      '\n' +
      '주장: 소유즈는 미국 우주 프로그램의 일부였다. 동작 1: 검색[소유즈] 관찰 1: 소유즈는 음역이다... Action 2: Finish[NOT ENOUGH INFO] Observation 2: Episode finished ```\n' +
      '\n' +
      '주장: 소유즈는 미국 우주 프로그램의 일부였다. 생각: 차근차근 생각해 봅시다. 소유즈는 러시아 우주선이다. 미국 우주 프로그램은 NASA입니다. NASA와 러시아는 국제 우주 정거장에서 함께 일했다. 응답: SUPPOR\n' +
      '\n' +
      '### ALFWorld Trajectories\n' +
      '\n' +
      '동일한 ALFWorld 게임인 pick_clean_then_place_in_recep-Knife-None-CounterTop-10/ trial_T20190909_110445_675754에서 ReAct, Act 및 ReAct-IM의 예제 궤적을 제공합니다.\n' +
      '\n' +
      '#### d.2.1 ReAct Trajectory\n' +
      '\n' +
      '리액트는 칼을 올바르게 찾아 청소한 다음 조리대에 놓을 수 있습니다.\n' +
      '\n' +
      'ReAct ALFWorld trajectory\n' +
      '\n' +
      '너는 방 한가운데에 있다. 주위를 빠르게 살펴보면 캐비닛 6, 캐비닛 5, 캐비닛 4, 캐비닛 3, 캐비닛 2, 캐비닛 1, 커피머신 1, 조리대 3, 조리대 2, 조리대 1, 서랍 3, 서랍 2, 서랍 1, 냉장고 1, 쓰레기통 1, 전자레인지 1, 선반 3, 선반 2, 선반 1, 싱크대 1, 서브볼륨 4, 스토브버너 3, 스토브버너 2, 스토브버너 1 및 토스터 1을 볼 수 있습니다. > 생각: 과제를 해결하려면 칼을 찾아서 가져다가 싱크대로 닦은 다음 조리대에 넣어야 해. OK. > 생각해봐: 먼저 칼을 찾아야 해. 칼은 캐비닛(1-6), 서랍(1-3), 조리대(1-3), 냉장고(1), 쓰레기통(1), 선반(1-3), 싱크대(1), 스토브버너(1-4), 토스터(1)에 나타날 가능성이 더 높다. 캐비닛 1부터 하나씩 확인할 수 있습니다. > 캐비닛 1에서 캐비닛 1로 이동 하면 그릇 1이 표시 됩니다. > 캐비닛 2로 이동 캐비닛 2는 닫힙니다. > 캐비닛 3으로 이동 캐비닛 3에서 유리병 1이 표시됩니다. > 캐비닛 4로 이동 캐비닛 4에서 머그잔 1이 표시됩니다. > 캐비닛 5로 이동 캐비닛 5가 닫힙니다. > 6 캐비닛으로 이동합니다. 6 캐비닛이 닫힙니다. > 서랍 1로 이동합니다. 서랍 1이 닫힙니다. > 서랍 2로 이동합니다. 서랍 2가 닫힙니다. > 서랍 3으로 이동합니다. 서랍 3이 닫힙니다. > 카운터 1에서 카운터 1로 이동하면 상추 2, 머그 2, 페퍼헤이커 1, 스푼 2가 표시됩니다. > 카운터 2로 이동하면 컵 1, 접시 스펀지 1, 유리병 3, 나이프 1, 접시 2, 감자 3, 조각상 1이 표시됩니다. > 생각: 이제 나이프(1)를 찾습니다. 다음, 나는 그것을 받아야 한다. OK. > 조리대 2에서 나이프 1을 가져가세요. 조리대 2에서 나이프 1을 가져가세요. > 생각: 이제 나이프 1을 가져갑니다. 다음으로 싱크바인(1)에 가서 청소해야 합니다. OK. > 싱크바인 1로 이동 싱크바인 1에서는 포크 3, 상추 3, 주걱 2가 표시됩니다. > 싱크바인 1이 있는 깨끗한 나이프 1 싱크바인 1을 사용하여 나이프 1을 청소합니다. > 생각: 이제 나이프(1)를 청소합니다. 다음으로 조리대 1에 넣어야 합니다. > 카운터탑 1로 이동 조리대 1에는 상추 2, 머그 2, 페퍼헤이커 1 및 스푼 2가 표시됩니다. > 나이프 1을 조리대 1에 넣습니다. 나이프 1을 조리대 1에 넣습니다.\n' +
      '\n' +
      '#### d.2.2 Act trajectory\n' +
      '\n' +
      '법은 칼을 정확하게 찾을 수 있지만 실제로 싱크대에 가기 전에 싱크대를 사용하여 세척하려고 한다. 그런 다음 모델을 복구할 수 없으며 동일한 명령 시퀀스를 반복하고 있습니다. ReAct를 통해 기존의 생각은 칼을 가져갔다는 것을 구체적으로 확인했을 것이며 다음 하위 목표는 싱크대로 가서 청소하는 것이었다.\n' +
      '\n' +
      'Act ALFWWoldt trajectory\n' +
      '\n' +
      '너는 방 한가운데에 있다. 주위를 빠르게 살펴보면 캐비닛 6, 캐비닛 5, 캐비닛 4, 캐비닛 3, 캐비닛 2, 캐비닛 1, 커피머신 1, 조리대 3, 조리대 2, 조리대 1, 서랍 3, 서랍 2, 서랍 1, 냉장고 1, 가바슨 1, 전자레인지 1, 선반 3, 선반 2, 선반 1, 싱크대 1, 스토브버너 4, 스토브버너 3, 스토브버너 2, 스토브버너 1 및 토스터 1을 볼 수 있습니다. > 조리대 1로 이동 > 조리대 2로 이동 > 조리대 2로 이동 > 조리대 2로 이동 > 조리대 2로 이동 > 조리대 2로 이동 > 컵 1, 접시 스펀지 1, 유리병 3, 나이프 1, 접시 2, 감자 3, 조각상 1이 표시됩니다. > 조리대 2에서 나이프 1을 가져가면 조리대 2에서 나이프 1을 픽업합니다. > 조리대 1로 이동 > 조리대 1에서 양상추 2, 머그 2, 후추통 1, 스푼 2가 표시됩니다. > 싱크대 1이 있는 깨끗한 나이프 1은 아무 일도 발생하지 않습니다. > 카운터탑 3에 빵 3, 나비 2, 휴대폰 1, 신용카드 1, 포크 2, 하우징젤란트 1, 나이프 2, 주걱 1, 조각상 3, 토마토 3, 토마토 2, 토마토 1 및 꽃병 2가 표시됩니다. > 카운터탑 3에서 나이프 2를 가져가면 아무 일도 발생하지 않습니다. > 조리대 2로 이동합니다. 조리대 2에서 컵 1, 접시 스펀지 1, 유리병 3, 접시 2, 감자 3 및 조각상 1이 표시됩니다. > 조리대 2에서 칼 1을 가져갑니다. 아무 일도 발생하지 않습니다. > 카운터탑 1로 이동합니다. 카운터탑 1에서 상추 2, 머그 2, 페퍼헤커 1 및 스푼 2가 표시됩니다. > 싱크대 1이 있는 깨끗한 나이프 1에는 아무 일도 발생하지 않습니다. > 카운터탑 3에 빵 3, 나비 2, 휴대폰 1, 신용카드 1, 포크 2, 하우징젤란트 1, 나이프 2, 주걱 1, 조각상 3, 토마토 2, 토마토 1 및 꽃병 2가 표시됩니다. > 카운터탑 3에서 나이프 2를 가져가면 아무 일도 발생하지 않습니다. > 조리대 2로 이동합니다. 조리대 2에서 컵 1, 접시 스펀지 1, 유리병 3, 접시 2, 감자 3 및 조각상 1이 표시됩니다. > 조리대 2에서 칼 1을 가져갑니다. 아무 일도 발생하지 않습니다. > 카운터톱 3으로 이동합니다. 카운터톱 1에서 상추 2, 머그 2, 페퍼헤커 1 및 스푼 2가 표시됩니다. > 카운터톱 2에서 나이프 1을 가져갑니다. 아무 일도 발생하지 않습니다. > 카운터탑 1로 이동합니다. 카운터탑 1에서 상추 2, 머그 2, 페퍼헤커 1 및 스푼 2가 표시됩니다. > 싱크대 1이 있는 깨끗한 나이프 1에는 아무 일도 발생하지 않습니다. > 카운터톱 3으로 이동 카운터톱 1에서는 상추 2, 머그 2, 페퍼헤커 1 및 스푼 2가 표시됩니다. > 싱크대 1이 있는 깨끗한 나이프 1에는 아무 일도 발생하지 않습니다. > 카운터탑 3에 빵 3, 나비 2, 휴대폰 1, 신용카드 1, 포크 2, 하우징젤란트 1, 나이프 2, 주걱 1, 조각상 3, 토마토 3, 토마토 2, 토마토 1 및 꽃병 2가 표시됩니다. > 카운터탑 3에서 나이프 2를 가져가면 아무 일도 발생하지 않습니다. > 조리대 2로 이동합니다. 조리대 2에서 컵 1, 접시 스펀지 1, 유리병 3, 접시 2, 감자 3 및 조각상 1이 표시됩니다. > 조리대 2에서 칼 1을 가져갑니다. 아무 일도 발생하지 않습니다. > 카운터탑 1로 이동합니다. 카운터탑 1에서 상추 2, 머그 2, 페퍼헤커 1 및 스푼 2가 표시됩니다. > 싱크배진 1이 있는 깨끗한 나이프 1에는 아무 일도 발생하지 않습니다. ```\n' +
      '\n' +
      '#### b.2.3 ReAct-IM trajectory\n' +
      '\n' +
      'ReAct-IM은 칼을 정확하게 찾을 수 있지만 청소도 할 수 없다. 그런 다음 LLM은 이 칼을 조리대에 놓으려다가 꼼짝 못하게 된다. 한 가지 가능한 문제는 잘못된 생각이다: 나는 깨끗한 칼을 찾아야 하는데, 그것은 LLM이 칼이 이미 세척되었다고 믿는 것을 속이는 것 같다. ReAct에서 이러한 일련의 하위 목표는 각 생각에 의해 구체적으로 포착될 것이다.\n' +
      '\n' +
      'ReAct-IMALEWorld trajectory\n' +
      '\n' +
      '너는 방 한가운데에 있다. 주위를 빠르게 살펴보면 캐비닛 6, 캐비닛 5, 캐비닛 4, 캐비닛 3, 캐비닛 2, 캐비닛 1, 커피머신 1, 조리대 3, 조리대 2, 조리대 1, 서랍 3, 서랍 2, 서랍 1, 냉장고 1, 가비지캔 1, 전자레인지 1, 선반 3, 선반 2, 선반 1, 싱크대 1, 스토브버너 4, 스토브버너 3, 스토브버너 2, 스토브버너 1 및 토스터 1을 볼 수 있습니다. > 생각: 과제를 해결하려면 깨끗한 칼을 찾아서 가져간 다음 조리대에 넣어야 한다. OK. > 생각해봐: 깨끗한 칼을 찾아야 해. OK. > 조리대 1로 가세요. 조리대 1에서 상추 2, 머그 2, 페퍼헤커 1, 스푼 2가 보입니다. > 생각: 깨끗한 칼을 찾아야 합니다. OK. > 조리대 2로 가세요 조리대 2에 컵 1, 접시 스펀지 1, 유리병 3, 칼 1, 접시 2, 감자 3, 조각상 1이 있습니다. > 생각: 깨끗한 칼을 찾아야 합니다. OK. > 조리대 2에서 나이프 1 가져오기 조리대 2에서 나이프 1을 가져옵니다. > 생각: 이 나이프 (1)를 조리대 1에 넣어야 합니다. > 조리대 1로 이동 조리대 1에서 상추 2, 머그 2, 페퍼헤커 1 및 스푼 2가 표시됩니다. > 생각: 이 나이프(1)를 조리대 1에 넣어야 합니다. > 나이프 1을 조리대 1에 넣습니다. 나이프 1을 조리대 1에 넣습니다. > 생각: 이 나이프 (1)를 조리대 1에 넣어야 합니다. > 칼 1을 조리대 1에 넣습니다. > 생각: 이 나이프(1)를 조리대 1에 넣어야 합니다. > 칼 1을 조리대 1에 넣습니다. OK. > 칼 1을 조리대 1에 넣습니다. OK. > 칼 1을 조리대 1에 넣습니다. OK. > 칼 1을 조리대 1에 넣습니다. > 생각: 이 나이프(1)를 조리대 1에 넣어야 합니다. > 칼 1을 조리대 1에 넣습니다. > 생각: 이 칼(1)을 조리대 1에 넣어야 합니다. > 둘 다 발생합니다. > 생각: 이 나이프(1)를 조리대 1에 넣어야 합니다. > 칼 1을 조리대 1에 넣습니다. OK. > 생각: 이 나이프 (1)를 조리대 1에 넣어야 합니다. > 그러나 나이프 1을 조리대 1에 넣어야 합니다. > 생각: 이 칼(1)을 조리대 1에 넣어야 합니다. > 둘 다 발생합니다. > 생각: 이 나이프(1)를 조리대 1에 넣어야 합니다. >\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:31]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:32]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>