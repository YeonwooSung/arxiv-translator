<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 언어 에이전트 트리 검색은 언어 모델에서 추론 행위 및 계획을 통합\n' +
      '\n' +
      'Andy Zhou\n' +
      '\n' +
      '일리노이대학교 어바나-샴페인\n' +
      '\n' +
      'Kai Yan\n' +
      '\n' +
      '일리노이대학교 어바나-샴페인\n' +
      '\n' +
      'Michal Shlapentokh-Rothman\n' +
      '\n' +
      '일리노이대학교 어바나-샴페인\n' +
      '\n' +
      'Haohan Wang\n' +
      '\n' +
      '일리노이대학교 어바나-샴페인\n' +
      '\n' +
      'Yu-Xiong Wang\n' +
      '\n' +
      '일리노이대학교 어바나-샴페인\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 언어 모델(LLM)은 다양한 의사 결정 작업에서 인상적인 성능을 보여주었지만 단순한 연기 프로세스에 의존하고 자율 에이전트로서 광범위한 배치에 미치지 못한다. 본 논문에서는 LATS(Language Agent Tree Search)를 소개한다. 모델 기반 강화 학습에서 일반적으로 사용되는 몬테카를로 트리 탐색에서 영감을 끌어내는 LATS는 에이전트, 가치 함수 및 최적화기로 LLM을 사용하여 향상된 의사 결정을 위해 잠재 강도를 재배치한다. 이 방법에서 중요한 것은 외부 피드백을 위한 환경의 사용이며, 이는 기존 기술의 한계를 뛰어넘는 보다 의도적이고 적응적인 문제 해결 메커니즘을 제공한다. 프로그래밍, HotPotQA 및 WebShop과 같은 다양한 도메인에 대한 우리의 실험적 평가는 경쟁력 있는 추론 성능을 유지하면서 의사 결정을 위한 LATS의 적용 가능성을 보여준다. 특히, LATS는 GPT-4를 사용한 HumanEval에서 94.4%, GPT-3.5를 사용한 WebShop에서 웹 브라우징에서 평균 75.9의 성능을 보여 본 논문에서 제안한 방법의 효율성과 범용성을 입증하였다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '다양한 환경에서 추론 및 의사 결정이 가능한 일반적인 자율 에이전트(Wooldridge Jennings, 1995)는 인공지능 분야에서 오랜 관심의 대상이 되어 왔다. 이것은 전통적으로 강화 학습에서 연구되어 왔지만, 강력한 추론 및 일반적인 적응성을 갖는 대형 언어 모델(LLMs)의 최근 부상(Brown et al., 2020; Chowdrey et al., 2022; Touvron et al., 2023; OpenAI, 2023)은 대안적인 패러다임을 제공한다. LLM은 텍스트 요약(Nallapati et al., 2016) 또는 자연어 추론(Bowman et al., 2015)과 같은 표준 NLP 태스크에서 우수했을 뿐만 아니라, 종종 고급 상식 추론 또는 정량적 기술을 필요로 하는 점점 더 다양한 태스크 세트에 적응되었다(Cobbe et al., 2021; Saparov & He, 2022). LLM은 또한 웹 내비게이션(Yao et al., 2022; Deng et al., 2023), 도구-사용(Schick et al., 2023), 또는 개방형 게임(Fan et al., 2022)과 같은 지식 및 추론을 수반하는 복잡한 환경에서 수행할 수 있다.\n' +
      '\n' +
      '추론 및 연기 능력은 또한 외부 환경으로부터의 피드백 또는 관찰로 LLM을 증강시키는 기술을 프롬프트함으로써 개선되었다(Yao et al., 2023; Gao et al., 2022; Shinn et al., 2023). 이는 언어 모델(LM)의 기본 능력에 전적으로 의존할 필요가 없어 외부 도구 또는 의미 피드백을 통해 이를 향상시킨다. 이러한 강점에도 불구하고 이러한 방법은 반사적이며 문제를 해결하기 위한 인간의 의도적이고 사려 깊은 의사 결정 특성에 미치지 못한다(슬로만, 1996; 에반스, 2010).\n' +
      '\n' +
      '도 1: LATS의 개요. LATS는 추론과 의사 결정을 향상시키기 위해 외부 환경과 자기 성찰을 사용한다.\n' +
      '\n' +
      '특히, 이러한 방법들은 다수의 추론 경로들을 고려하거나 미리 계획하는 것에 실패한다. 최근의 검색-유도 LLM 작업들(Xie et al., 2023; Yao et al., 2023; Hao et al., 2023)은 다수의 추론 체인들을 통해 검색함으로써 이 문제를 해결한다. 이러한 방법은 계획을 가능하게 하지만 이러한 방법은 독립적으로 작동하며 추론을 개선할 수 있는 외부 피드백을 통합하지 않는다.\n' +
      '\n' +
      '이러한 문제를 해결하기 위해 언어 모델을 사용하여 의사 결정 및 추론을 위한 일반적인 프레임워크인 LATS(Language Agent Tree Search)를 제안한다. LATS는 ReAct(Yao et al., 2023)를 가능한 추론 및 연기 단계의 조합 공간을 통한 검색으로 확장함으로써 LM 계획, 연기 및 추론 전략을 통합한다. 모델 기반 강화 학습(Silver et al., 2017; Anthony et al., 2017; Jiang et al., 2018)의 몬테카를로 트리 검색(Monte Carlo Tree Search, MCTS)을 언어 에이전트에 적용하여 사전 훈련된 LLM을 에이전트, 값 함수 및 최적화기로 재구성한다. 현대 LM의 강력한 자연어 이해와 문맥 내 학습 능력을 활용하여 프레임워크의 각 구성 요소 간의 인터페이스로 텍스트를 사용하여 LATS는 추가 교육 없이 환경 조건에 계획을 적응할 수 있다. 우리가 아는 한, _LATS는 LLM을 향상시키기 위한 추론, 행동 및 계획을 결합한 첫 번째 프레임워크이다. 특히, LATS는 ReAct(Yao et al., 2023)보다 HotPotQA(Yang et al., 2018)에서 GPT-3.5의 성능을 배가시키고, WebShop(Yao et al., 2022)에서 평균 점수를 \\(22.1\\) 상승시킨다. GPT-4와 함께 사용될 때, LATS는 HumanEval 상에서 프로그래밍을 위한 \\(94.4\\) Pass@1 레이트를 달성하여(Chen 등, 2021), 최신 기술을 설정한다. 요약하면 **기여도** 는 다음과 같습니다.\n' +
      '\n' +
      '* 샘플링된 작업에서 최상의 궤적을 의도적으로 구성하기 위해 LM 기반 몬테 카를로 트리 검색 변형을 도입하여 반사적 프롬프트 방법에 비해 유연하고 적응적인 문제 해결을 가능하게 합니다. 이것은 LM으로부터의 휴리스틱에 의해 안내된다.\n' +
      '* 외부 피드백과 자기 반성을 통합하여 LATS는 모델 감성을 강화하고 에이전트가 경험에서 학습할 수 있도록 하여 추론 기반 검색 방법을 능가합니다.\n' +
      '* 프로그래밍, 대화형 QA 및 웹 탐색과 같은 다양한 도메인에 대한 실험을 통해 자율 추론 및 의사 결정을 위해 LLM을 활용하는 LATS의 다양성을 보여줍니다.\n' +
      '\n' +
      '## 2 관련 작업\n' +
      '\n' +
      '**추론을 위한 LLMs.** LLMs의 경우 추론은 일반적으로 복잡한 입력을 최종 답변을 향한 순차적인 중간 단계로 분해하는 것을 포함합니다 (Cobbe et al., 2021). CoT( Chain-of-Thought) 프롬프팅(Wei et al., 2022) 및 그 변형(Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022). 그러나, 단일 단계에서 자동으로 체인을 생성하는 이러한 방법들은 복합 오류로 인해 단계 수가 증가함에 따라(Guo et al., 2018; Chen et al., 2022) 종종 오류 전파를 겪는다. 다양한 진보들은 이 문제를 완화시키는 것을 목표로 한다; Self-Consistency(Wang et al., 2022)와 같은 일부 접근법들은 샘플링된 체인들에 대해 다수 투표를 채택하고, 다른 접근법들은 최소-최대 프롬프팅(Zhou et al., 2022)과 같은 다단계 분해 또는 스크래치패드(Nye et al., 2021) 또는 컴파일러(Gao et al., 2022)와 같은 외부 도구의 사용에 초점을 맞춘다. 최근 CoT는 궤적을 보다 효과적으로 샘플링할 수 있는 탐색 알고리즘(Yao et al., 2023; Hao et al., 2023; Besta et al., 2023)으로 개선되고 있다. Tree-of-thought(ToT) 프롬프팅(Yao et al.,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline\n' +
      '**Approach** & **Reasoning** & **Acting** & **Planning** & **Self** & **External** \\\\  & & & & **Reflection** & **Memory** \\\\ \\hline CoT (Wei et al., 2022) & ✓ & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) \\\\ ReAct (Yao et al., 2023) & ✓ & ✓ & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) \\\\ ToT (Yao et al., 2023) & ✓ & \\(\\times\\) & ✓ & ✓ & ✓ \\\\ RAP (Hao et al., 2023) & ✓ & \\(\\times\\) & ✓ & \\(\\times\\) & ✓ \\\\ Self-Refine (Madanan et al., 2023) & ✓ & \\(\\times\\) & \\(\\times\\) & ✓ & \\(\\times\\) \\\\ Beam Search (Xie et al., 2023) & ✓ & \\(\\times\\) & \\(\\times\\) & ✓ & \\(\\times\\) \\\\ Reflexion (Shinn et al., 2023) & ✓ & ✓ & \\(\\times\\) & ✓ & ✓ \\\\\n' +
      '**LATS (Ours)** & ✓ & ✓ & ✓ & ✓ & ✓ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 1>은 추론, 연기, 기획에 관한 관련 작업의 요약이다. LATS는 세 도메인 모두의 디자인을 통합하는 첫 번째 작업으로 모든 해당 작업에서 사용할 수 있다. 계획은 검색 알고리즘의 사용, 자기 성찰은 LM 생성 피드백의 사용, 외부 메모리는 솔루션의 향후 업데이트를 위해 과거 텍스트 컨텍스트를 찾는 것을 말한다.\n' +
      '\n' +
      '2023a)는 LM-발생 휴리스틱에 의해 안내된 DFS 또는 BFS-기반 탐색을 사용하는 한편 RAP(Reasoning via Planning)를 사용한다(Hao 등, 2023). LM에 의해 시뮬레이션된 롤아웃들을 갖는 MCTS를 사용한다. 그러나 LM 내부 지식에만 의존하며 유용한 외부 피드백에 적응할 수 없다.\n' +
      '\n' +
      '연기를 위한 LMM. LMM의 강력한 추론 능력과 상식 능력은 상호작용 환경에서 정책 모델로서 의사 결정 또는 연기 작업에 적용되었다. 로보틱스의 영역에서 LLM은 제어 정책의 상위 레벨 제어기로서 채용되어 왔다(Ahn et al., 2022; Huang et al., 2022; Driss et al., 2023). 유사한 작업(Baker et al., 2022; Wang et al., 2023; Zhu et al., 2023)은 또한 LLM 에이전트를 마인크래프트와 같은 복잡한 멀티모달 게임에 적응시켰다(Guss et al., 2019; Fan et al., 2022). LLM은 특히 텍스트 기반 환경(Liu et al., 2018; Shridhar et al., 2020; Liu et al., 2023)에서 유용하며, 여기서 ReAct(Yao et al., 2023b)와 같은 연기 기반 프롬프트 기술이 성공을 보았다. CoT와 유사하게 ReAct는 단순성에 의해 제한되며 환경 조건에 효과적으로 적응할 수 없다. 이를 해결하기 위해 많은 확장들이 제안되었는데, Self-refine(Madan et al., 2023) 및 Reflexion(Shinn et al., 2023; Yao et al., 2023c), 추론 및 의사 결정을 향상시키기 위해 자기 성찰을 이용하는 AdaPlanner(Sun et al., 2023), 긍정 및 부정 환경 피드백을 모두 포함하는 AdaPlanner(Sun et al., 2023) 등이 있다. 그러나 이러한 방법은 개별 계획이나 궤적을 정제하는 데 중점을 두고 있으며 각 단계에서 대안 선택을 고려하지 않는다. 또한, 최근 작업(Huang et al., 2023)은 LLM들이 내부 추론을 스스로 수정할 수 없다고 제안하여 외부 피드백을 사용하는 것이 중요하다. 순수한 의사 결정 환경들에 대안적으로, LLM들의 추론 및 실용적인 능력들은 API들, 검색 엔진들, 계산기들, 또는 다른 모델들과 같은 외부 도구들에 대한 액세스에 의해 강화되었다(Schick et al., 2023; Shen et al., 2023; Suris et al., 2023). 추론 기반 접근법과 달리 이러한 방법은 계획으로 개선되지 않아 효과를 제한한다. 우리는 그것들을 탭 1에 요약합니다.\n' +
      '\n' +
      '검색 중에 여러 가지 결과의 분기가 탐색되는 트리 기반 검색.트리 기반 검색은 많은 계획 알고리즘(Swiechowski et al., 2023; LaValle et al., 2001) 및 강화 학습( Reinforcement Learning; RL)에 널리 사용된다(Hafner et al., 2019; Du et al., 2023; Wu et al., 2023). 트리 기반 검색은 임의 상태(Vodopivec et al., 2017)에서 확장할 수 있는 환경 모델을 필요로 하는데, 이는 종종 RL에서 추가 훈련을 필요로 한다(Hafner et al., 2023). 이러한 문제는 LM 태스크에 대해 존재하지 않는데, 이는 입력을 컨텍스트로 설정하고 LM에 의해 대응하는 이전 출력을 설정함으로써 임의의 상태로 편리하게 백업할 수 있기 때문이다. 따라서, 우리는 트리 기반 프레임워크에서 작업하고 MCTS(Swiechowski et al., 2023)를 사용하여 LM의 잠재력을 완전히 해제하는 동시에 LLM의 인-컨텍스트 학습(Brown et al., 2020) 능력을 활용하여 언어 설명에 대한 가치 함수를 훈련하는 비용을 피한다.\n' +
      '\n' +
      '도 2: LATS와 최근에 제안된 LM 탐색 알고리즘 ToT(Yao et al., 2023a) 및 RAP(Hao et al., 2023)의 차이점에 대한 개요. LATS는 환경 피드백과 자기 성찰을 활용하여 탐색을 더욱 적응시키고 성능을 향상시킨다.\n' +
      '\n' +
      '## 3 Preliminaries\n' +
      '\n' +
      '### 문제 설정 및 프롬프트\n' +
      '\n' +
      'LATS를 설명하기 전에 먼저 문제를 정의하고 추론 또는 의사 결정을 위해 대규모 언어 모델을 활용하는 몇 가지 확립된 방법을 설명한다. LM 추론이나 의사 결정에서는 자연어로 된 입력 \\(x\\)과 미리 훈련된 언어 모델 \\(p_{\\theta}(x)\\)을 \\(\\theta\\)으로 파라미터화하여 주어지며, 우리의 목표는 답(추론)에 해당하는 최종 출력 \\(y\\sim p_{\\theta}(x)\\)을 생성하거나 작업을 완료하는 것이다. \\(x\\) 및 \\(y\\)은 모두 언어 _시퀀스_로, _tokens_ (자연어의 기본 요소, 종종 단어)의 목록으로 구성 되며 \\(x=(x[1],\\ldots,x[n])\\) 및 \\(y=(y[1],\\ldots,y[n])\\으로 표시 됩니다. LM은 텍스트를 자동으로 디코딩한다. 즉, 다른 입력 없이 LM이 시퀀스를 생성할 확률은 \\(p_{\\theta}(x)=\\prod_{i=1}^{n}p_{\\theta}(x[i]|x[1\\ldots i-1])\\으로 주어진다. 일반적으로 LM을 개선 하기 위해 특정 명령 또는 소수의 샷 입력 출력 예인 입력 \\(x \\)과 함께 _prompts_가 제공 됩니다. 우리는 LM: \\(y\\sim p_{\\theta}(y|\\texttt{prompt}_{IO}(x))\\)에 의해 입력 \\(x\\)이 출력 \\(y\\)으로 변환되는 일반적인 과정을 나타내며, \\(\\texttt{prompt}_{IO}(x)\\)는 입력 \\(x\\)을 나타낸다.\n' +
      '\n' +
      '**CoT(Chain-of-thought) 프롬프팅**(Wei et al., 2022)은 \\(x\\)에서 \\(y\\)으로의 직접 매핑이 복잡 한 시나리오, 예를 들어 \\(x\\)이 수학적 쿼리 또는 도전 질문에서 나온 경우와 같은 시나리오를 충족 하기 위해 도입 되었습니다. 이 방법은 \\(x\\)과 \\(y\\) 사이의 디딤돌 역할을 하는 _생각_\\(z_{1},\\ldots,z_{n}\\)을 만드는 데 달려 있으며, 각 생각 \\(z_{i}\\)은 언어 시퀀스이다. CoT 프롬프팅을 사용하기 위해, 생각들은 순차적으로 \\(z_{i}\\sim p_{\\theta}^{CoT}(z_{i}|x,z_{1\\ldots i-1})\\)으로 추출되며, 최종 출력은 \\(y\\sim p_{\\theta}^{CoT}(y|x,z_{1\\ldots n})\\이다.\n' +
      '\n' +
      '**Tree-of-thought (ToT) 프롬프팅** (Yao et al., 2023)은 생각을 통해 여러 추론 경로를 탐색 하 여 CoT 프롬프팅을 확장 합니다. 트리 위의 탐색으로 문제를 프레임화하는데, 여기서 각 노드 \\(s=[x,z_{1\\ldots i}]\\)는 원래의 입력 \\(x\\)과 사고 시퀀스 \\(z_{1\\ldots i}\\)을 포함하는 부분 해 상태를 나타낸다. CoT \\(z_{i}\\sim p_{\\theta}^{CoT}(z_{i}|x,z_{1\\ldots i-1})\\)을 이용하여 제안 또는 샘플링에 의해 변환이 이루어진다. 각 상태의 언어 모델 평가 \\(V(s)\\를 기반으로 한 휴리스틱에 의해 유도되는 트리를 체계적으로 탐색하기 위해 너비 우선 탐색 또는 깊이 우선 탐색과 같은 숙의 탐색 알고리즘이 사용된다.\n' +
      '\n' +
      '**계획을 통한 추론**(RAP)(Hao 등, 2023)은 MCTS가 DFS 또는 BFS를 통해 사용된다는 점을 제외하고는 ToT와 유사합니다. 휴리스틱은 행동의 가능성 또는 신뢰도와 같은 LM으로부터 설계되고, LM은 시뮬레이션 단계 동안 후속 상태를 예측하기 위한 세계 모델로서 사용된다.\n' +
      '\n' +
      '**ReAct**(Yao et al., 2023)는 언어 모델을 \\(x\\)에서 \\(y\\)으로의 매핑이 게임 또는 API와 같은 외부 환경과의 상호 작용에 의해 향상되거나 필요한 작업으로 확장합니다. 이 기법은 CoT에서 추론 트레이스 \\(z\\)에 허용 액션 \\(a\\)을 추가하는 액션 공간 \\(\\hat{A}=A\\cup Z\\)을 구성한다. 환경으로부터의 관찰 \\(o\\)은 추론과 행동 모두를 개선하기 위해 사용된다. ReAct의 문제를 해결하기 위해, 각 관찰 후에, 동작들은 순차적으로 \\(p_{\\theta}\\)으로부터 \\(a_{i}\\sim p_{\\theta}^{ReAct}(a_{i}|x,o_{1\\ldots i-1},a_{1\\ldots i-1})\\)으로 생성되고, 최종 출력은 \\(y\\sim p_{\\theta}^{ReAct}(y\\mid x,o_{1\\ldots n},a_{1\\ldots n})\\이다.\n' +
      '\n' +
      '이전에 설명한 프롬프트 기술은 추론 작업에서 LM 성능을 향상시키지만 몇 가지 단점으로 인해 다각적인 의사 결정을 포함 하는 어려운 작업에서 흔들립니다. 1) _유연성_: LM에서 자동으로 샘플링 되는 기본 프롬프트 방법 (CoT 또는 ReAct) 특정 상태로부터의 잠재적인 대체 연속성을 무시 합니다. 2) _감성_: 추론 기반 방법(CoT, RAP 또는 ToT)은 LM의 내부 표현에만 의존하며 외부 관찰을 고려할 수 없다. 이 종속성은 성능 상한을 설정 하는 동안 사실 환각 및 오류 전파를 위험 합니다. 3) _적응성_: 현재 계획 프레임워크(RAP 또는 ToT)는 BFS와 같은 간단한 검색 알고리즘을 사용하거나 환경 피드백을 활용하여 계획을 개선할 수 없습니다. 또한 에이전트는 정적이며 이전 경험을 재사용하거나 시행착오를 통해 학습할 수 없습니다. RAP도 MCTS를 채택하지만 LM이 세계 모델이 되어 상태를 정확하게 예측할 수 있는 작업에 제약을 받는다. 이러한 단점은 LMs가 일반적인 문제 해결 에이전트로 배치되고 LATS에 대한 동기를 형성하는 능력을 제한한다.\n' +
      '\n' +
      '### Monte-Carlo Tree Search (MCTS)\n' +
      '\n' +
      '몬테-카를로 트리 검색(Monte-Carlo Tree Search, MCTS)은 Atari(Ye et al., 2021), Go(Silver et al., 2016)와 같은 많은 의사 결정 환경에서 성공적인 것으로 입증된 휴리스틱 검색 알고리즘이다. MCTS는 트리 내의 모든 노드가 상태이고 에지가 액션인 결정 트리를 구축한다. MCTS는 \\(k\\) 에피소드에 대해 실행됩니다. 각 에피소드에 대해 루트(즉, 초기 상태)에서 시작하여 트리를 확장하기 위해 두 단계를 반복적으로 수행합니다. 1) _확장_ - 여러 자식 상태 \\(s\\)는 현재 부모 상태 \\(p\\)에서 \\(n\\) 작업을 샘플링하여 탐색합니다. 2) _선택_ - Tree에 적용된 최대 신뢰 범위(_UCT) 값을 가진 자식(Kocsis 및 Szepesvari, 2006)이 다음 반복에 의해 선택됩니다. 자식 상태 \\(s\\)의 UCT는 다음과 같이 계산된다:\n' +
      '\n' +
      '\\[UCT(s)=V(s)+w\\sqrt{\\frac{\\ln N(p)}{N(s)}}, \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(N(s)\\)는 노드 방문 횟수 \\(s\\), \\(V(s)\\)는 \\(s\\), \\(w\\)는 탐색 가중치, \\(p\\)는 \\(s\\)의 부모 노드입니다. 가장 높은 UCT 값을 갖는 자식 노드는 다음 반복에서 확장을 위해 선택된다. 에피소드의 끝에 도달하면 _역전파_가 수행됩니다. 반환 \\(r\\)은 \\(V(s)=\\frac{V_{\\text{old}}(s)(N(s)-1)+r}{N(s)}\\) 공식을 사용하여 경로를 따라 모든 \\(V(s)\\)를 업데이트하는 데 사용됩니다. 여기서 \\(V_{\\text{old}}(s)\\)는 이전 값 함수입니다. 일반적으로 MCTS의 주요 단점은 이전 단계를 되돌리고 탐색 트리를 형성하기 위해 환경 모델이 필요하다는 것이며, 이는 종종 강력한 가정이다. 그러나 이력 텍스트 입력을 복사 붙여넣기만 하면 모든 단계로 편리하게 재설정할 수 있기 때문에 LMs에는 이러한 제한이 없다. 그러한 특별한 속성은 우리 작업의 핵심 동기입니다.\n' +
      '\n' +
      '## 4 통일 계획, 추론 및 작업\n' +
      '\n' +
      '### LM Agent\n' +
      '\n' +
      'LATS는 ReAct를 기반으로 순차적 추론이나 의사결정 업무를 지원한다. 시간 단계 \\(t\\)에서 에이전트는 환경으로부터 관찰 \\(o_{t}\\in O\\)을 수신하고, 일부 정책 \\(\\pi(a_{t}|x,o_{1\\dots i-1},a_{1\\dots i-1})\\)에 따라 액션 \\(a_{t}\\in A\\)을 취한다. 여기서 \\(x\\)은 작업 명령어와 소수의 샷 예제로 구성된다. LM의 유용한 언어 표현을 기본 의사 결정자로 활용하기 위해 \\(p_{\\theta}\\)으로 에이전트를 초기화한다. 우리는 행동 공간 \\(\\hat{A}=A\\cup Z\\)이 허용된 행동 공간 \\(A\\)과 추론 트레이스의 언어 공간 \\(Z\\)으로 구성된 ReAct 인스턴스화를 따른다. 행동은 환경에 직접적인 영향을 미치고 관찰 결과를 가져오는 반면, 사고는 정보를 조직하거나 미래의 행동을 계획하거나 내부 지식을 주입하여 결정을 공식화하는 데 사용된다. 액션 공간의 정확한 인스턴스화는 특정 환경에 따라 달라집니다. 의사 결정 작업의 경우 작업은 웹 사이트의 명령으로 구성 될 수 있고 추론 작업의 경우 작업 공간은 몇 가지 외부 도구 또는 API로 제한 될 수 있습니다.\n' +
      '\n' +
      '하나의 궤적이나 해를 탐욕스럽게 디코딩하는 대신, 우리는 현재 상태를 이용하여 \\(p_{\\theta}\\)에서 \\(n\\)개의 동작을 샘플링한다. 이는 복잡한 의사 결정 과제에 대해, 올바른 잠재적인 궤적 또는 추론 경로의 범위가 존재할 가능성이 있다는 직관에 기초한다(Evans, 2010). 각 단계에서 다양한 후보 집합을 샘플링하면 LM 텍스트 생성의 확률적 특성을 완화하고 의사 결정 및 추론 공간 모두에서 더 큰 탐색을 가능하게 한다. 제안된 탐색 알고리즘 내에서 \\(p_{\\theta}\\)을 랩핑하여 샘플링된 동작으로부터 최적의 궤적을 의도적으로 구성한다.\n' +
      '\n' +
      '### Lats\n' +
      '\n' +
      'LATS의 주요 구성 요소는 고의적인 계획으로 전반적인 문제 해결 과정을 제어하는 탐색 알고리즘이다. 가장 유망한 궤적 탐색과 시스템 균형 탐색을 위해, 우리는 의사결정 프레임을 트리 탐색으로 사용하는 몬테카를로 트리 탐색(MCTS)의 변형을 채택한다. 여기서 각 노드 \\(s=[x,a_{1\\dots i},o_{1\\dots i}]\\)은 원래의 입력 \\(x\\), 동작 시퀀스 \\(a_{1\\dots i}\\), 관찰 시퀀스 \\(o_{1\\dots i}\\)을 포함하는 상태를 나타낸다.\n' +
      '\n' +
      '언어 에이전트에 MCTS를 적용하기 위해 LATS는 에이전트, 상태 평가기, 피드백 생성기로 \\(p_{\\theta}\\)을 재배치하여 현대 LM의 유용한 언어 전거를 활용하여 계획을 용이하게 한다. 표준 MCTS 및 RAP Hao 등(2023)이 시뮬레이션을 용이하게 하기 위해 내부 역학 모델에 의존하는 반면, LATS는 모델이 없고 환경 상호작용을 사용한다. LATS는 작업이 성공적으로 완료되거나 계산 한계에 도달할 때까지 연속적으로 수행되는 일련의 연산, _선택, 확장, 평가, 시뮬레이션, 역전파 및 반사_로 구성된다. LATS의 전체 psuedocode는 Sec에서 찾을 수 있다. 부록의 A\n' +
      '\n' +
      '**선택** 첫 번째 작업에서 알고리즘은 후속 확장에 가장 적합한 현재 트리의 세그먼트를 식별합니다. 초기 상태 \\(s_{0}\\)로 표시된 루트 노드에서 시작하여 리프 노드에 도달할 때까지 각 트리 수준에서 자식 노드를 선택한다. 탐구와 개발의 균형을 맞추기 위해 식 1과 같이 UCT 알고리즘을 사용한다.\n' +
      '\n' +
      '**확장.** 노드를 선택한 후 두 번째 작업은 이전 섹션에서 설명한 대로 \\(p_{\\theta}\\)에서 \\(n\\) 작업을 샘플링하여 트리를 확장합니다. 환경은 각각의 액션을 수신하고 대응하는 피드백을 관찰로서 반환한다. 이렇게 하면 트리에 추가 된 새 자식 노드가 \\(n\\) 됩니다. 이 트리는 외부 장기 기억 구조에 저장된다.\n' +
      '\n' +
      '**평가.** 세 번째 작업은 선택 및 역전파에 사용할 각 새 자식 노드에 스칼라 값을 할당합니다. 이 값은 태스크 완료에서 에이전트의 진행을 효과적으로 정량화하며, 검색 알고리즘을 트리의 가장 유망한 영역으로 조향하는 휴리스틱 역할을 한다. Yao et al. (2023)에 따라 우리는 주어진 상태에 대해 추론하도록 유도함으로써 \\(p_{\\theta}\\)을 값 함수로 재목적화한다. 스칼라 값을 얻기 위해, 우리는 궤적의 정확성을 나타내는 점수로 추론 추적을 종료하도록 명령한다. 이 방법은 프로그램된 휴리스틱에 비해 향상된 유연성을 제공하며(Campbell et al., 2002), 학습된 휴리스틱보다 더 큰 효율성을 제공한다(Silver et al., 2017).\n' +
      '\n' +
      '**시뮬레이션.** 네 번째 작업은 터미널 상태에 도달할 때까지 현재 선택한 노드를 확장합니다. 각 깊이 레벨에서 동일한 연산으로 노드를 샘플링하고 평가하지만 가장 높은 값의 노드를 우선시한다. 단말 상태에 도달하는 것은 궤적의 정확성에 대한 객관적인 피드백을 제공한다. 작업이 성공적으로 완료되면 LATS는 검색을 종료합니다. 솔루션이 부분적으로 성공 하거나 실패 하는 경우 아래에 설명 된 대로 두 가지 추가 작업을 수행 합니다.\n' +
      '\n' +
      '**역전파.** 이 작업은 궤적의 결과에 따라 트리의 값을 업데이트합니다. 탐색 트리의 루트(초기 상태 \\(s_{0}\\))에서 잎(말단 상태 \\(s_{n}\\))까지의 궤적에 있는 각 노드 \\(s_{0},s_{1},\\dots,s_{n}\\)에 대해 \\(N(s_{i})=N_{\\text{old}}(s_{i})+1\\) 및 \\(V(s_{i})=\\frac{r+N_{\\text{old}}(s_{i})V_{\\text{old}}(s_{i})}{N(s_{i})}\\)로 시뮬레이션 결과를 반영하여 그 값을 갱신한다. 여기서 \\(r\\)은 반환이고 \\(N_{\\text{old}},V_{\\text{old}}\\)은 오래된 방문 횟수와 값 함수이다. 이러한 업데이트된 값은 UCT 공식(식 1)에 사용된다. 탐사를 위한 다음 노드 선택을 안내합니다.\n' +
      '\n' +
      '**반사.** 환경 피드백 외에도 _자기 반영_ 을 활용 하 여 의사 결정 프로세스를 더욱 구체화 합니다 (Shinn et al., 2023; Madan et al., 2023). 실패한 단말 노드를 만났을 때, \\(p_{\\theta}\\)는 추론 또는 행동 과정의 오류를 요약하고 우수한 대안을 제안하는 언어적 자기 반성을 제공하기 위해 궤적과 최종 보상을 프롬프트한다. 우리는 실패한 궤적과 대응하는 반사를 모두 메모리에 저장한다. 후속 반복에서 이들은 에이전트 및 가치 함수에 추가 컨텍스트로 통합되어 맥락 내 학습을 통해 둘 다를 정제한다. 이는 스칼라 값보다 더 유용한 의미적 기울기 신호를 부여하여 강화 학습과 같은 값비싼 최적화 프로세스의 비용 없이 에이전트가 시행착오로부터 학습할 수 있게 한다.\n' +
      '\n' +
      '개념적으로 LATS는 LM 에이전트와 추론 및 의사 결정을 위한 일반적인 프레임워크로서 다음과 같은 장점이 있다. (1) _일반성_: LATS는 추론과 결정을 모두 지원한다\n' +
      '\n' +
      '그림 3: LATS의 6가지 작동에 대한 개요. 노드는 _선택_, _확장_, _평가_ 된 다음 터미널 노드에 도달할 때까지 _시뮬레이션 된 다음 결과 값은 _backpropagated_ 됩니다. 궤적이 실패하면 _반사_가 생성되어 향후 시행을 위한 추가 컨텍스트로 사용됩니다. 이러한 작업은 예산에 도달하거나 작업이 성공할 때까지 연속적으로 수행됩니다.\n' +
      '\n' +
      '생각과 행동의 공유 공간을 정의하여 과제를 만든다. (2) _Deliberate_: MCTS 및 LM 값 함수의 사용은 유망한 대안들을 탐색하면서 높은 값을 갖는 옵션들을 선택하는 원칙적인 탐색을 보장한다. (3) _적응성_: LATS는 관찰과 자기 성찰을 통한 외부 피드백의 사용을 중심으로 설계되어 문제 해결 시 더 큰 적응을 가능하게 한다. (4) _유연성_: LATS는 상태 설계 및 트리 차원을 수정함으로써 상이한 시나리오, 환경 및 리소스 규정들을 수용할 수 있다. (5) _모듈리티_: 베이스 LM 에이전트, 반사 생성기 및 값 함수는 독립적으로 변경될 수 있고 개별 LM 속성에 적응될 수 있다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      'LATS의 일반적인 적용 가능성을 입증하기 위해 프로그래밍(Chen et al., 2021; Austin et al., 2021), HotPotQA(Yang et al., 2018), WebShop(Yao et al., 2022)과 같은 추론 능력과 행동 능력을 모두 필요로 하는 다양한 의사 결정 영역에 대한 방법을 평가한다.\n' +
      '\n' +
      '### HotPotQA\n' +
      '\n' +
      '추론 기반 전략과 행동 기반 전략으로 접근할 수 있는 작업에 대해, 우리는 두 개 이상의 위키피디아 구절에 대한 검색을 필요로 하는 다중 홉 질문 응답 벤치마크인 HotPotQA(Yang et al., 2018)를 고려한다. 액션 공간의 경우, LM 생각 외에도 Yao 등(2023b)의 설정을 따르며, 에이전트는 검색 및 조회 정보를 위한 API 호출을 제공한다. 이러한 API 호출 및 자체 생성 반사의 출력은 관찰 공간을 형성 합니다. 우리는 각 방법에 대해 100개의 질문의 하위 집합과 3개의 몇 개의 샷 예제를 사용한다. ToT의 경우 기본 탐색 알고리즘으로 DFS를 사용하고 휴리스틱으로 LM을 사용하여 점수를 매긴다. LATS를 포함한 샘플링을 포함하는 모든 방법에 대해 \\(k=50\\) 궤적을 샘플링한다. 자세한 내용과 프롬프트는 Sec에서 확인할 수 있습니다. D 및 Sec. 부록의 E\n' +
      '\n' +
      '우리는 CoT(Wei et al., 2022) 및 그것의 변형들, CoT-SC(Wang et al., 2022), ToT(Yao et al., 2023a) 및 RAP(Hao et al., 2023)에 대응하는 컨텍스트로부터 액션들 및 관찰들을 제거하여 내부 추론 전략들을 평가한다. 이러한 방법은 질문에 답하기 위해 에이전트의 기존 지식에만 의존한다. 또한 대화형 API 환경에서 에이전트를 증강하고 정보 검색 능력을 평가하는 행위 기반 방법 ReAct, Reflexion, LATS도 고려한다. LATS는 외부 피드백이 추론을 향상시킬 수 있는 시나리오를 위해 설계되었지만 CoT를 기본 프롬프트로 하는 추론 전용 버전도 구현한다. 또한 먼저 CoT 기반 프롬프트로 프롬프트한 다음 실패 시 ReAct 기반 프롬프트로 전환하여 LATS의 내부 및 외부 추론을 결합한다. 이것은 답이 아직 알려지지 않은 경우에만 추가 정보를 조회하는 도구를 사용하여 인간이 이 작업에 접근하는 방법에 더 가깝다.\n' +
      '\n' +
      '**결과.** 탭에서 관찰됩니다. 2는 HotPotQA에서 내부 추론과 외부 검색 전략이 모두 잘 수행된다는 것이다. 대규모 훈련 코퍼스로 인해 현대의 LLM은 이미 사실적 지식을 인코딩하고 종종 질문에 정확하게 대답할 수 있다. CoT는 추론이 필요한 질문에 대해 성능을 약간 향상시킬 수 있지만 더 많은 출력을 샘플링하고 탐색할 수 있는 검색 방법 ToT 및 RAP에서 더 큰 이득이 관찰된다. 우리는 연기 기반 방법에 대해서도 유사한 결과를 관찰한다. LATS는 동일한 수의 궤적을 샘플링하는 경우에도 원칙적인 검색으로 더 많은 노드를 확장함으로써 ReAct를 능가한다(정성적 샘플에 대한 부록 D의 그림 5 참조). 이거는.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c} \\hline \\hline\n' +
      '**Prompt Method** & **HotpotQA (EM)** \\\\ \\hline I/O & 0.32 \\\\ CoT (Wei et al., 2022) & 0.34 \\\\ CoT - SC (Wang et al., 2022) & 0.38 \\\\ ToT (Yao et al., 2023a) & 0.55 \\\\ RAP (Hao et al., 2023) & 0.60 \\\\ RAP (n = 10) & 0.60 \\\\ LATS (CoT) & **0.60** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\begin{tabular}{l|c} \\hline \\hline\n' +
      '**Prompt Method** & **HotpotQA (EM)** \\\\ \\hline ReAct (Yao et al., 2023b) & 0.32 \\\\ ReAct (best of k) & 0.38 \\\\ Reflexion (Shim et al., 2023) & 0.51 \\\\ LATS & 0.61 \\\\ LATS (n = 3) & 0.56 \\\\ LATS (n = 10) & 0.64 \\\\ LATS (CoT + ReAct) & **0.71** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: HotpotQA에 대한 GPT-3.5 추론 기반 프롬프트(왼쪽) 및 행동 기반 프롬프트(오른쪽) 결과. LATS는 연기에 가장 높은 정확 일치(EM)를 달성하며 추론에 경쟁력이 있다. 달리 명시되지 않는 한 확장 및 \\(k=50\\) 궤적 동안 \\(n=5\\) 노드를 샘플링한다.\n' +
      '\n' +
      '\\(n\\)을 수정할 때 각 반복 동안 확장된 노드 수를 보여준다. 계산 및 추론 비용이 증가하더라도 \\(n\\)을 증가시키면 성능을 일관되게 향상시킬 수 있다. LATS는 또한 내부 추론에서 RAP와 경쟁적이지만 연기보다 더 나쁜 성능을 보인다. LATS에서 내부 추론과 외부 추론을 결합하면 가장 높은 성능을 나타내며, 이는 기반 LM이 이미 수행할 수 있는 작업에서도 추론을 증강하는 데 외부 피드백의 중요성을 나타낸다.\n' +
      '\n' +
      '### Programming\n' +
      '\n' +
      '복잡한 추론 작업에 대한 외부 관찰의 중요성을 입증하기 위해 Humaneval(Chen et al., 2021) 및 MBPP(Austin et al., 2021)를 사용하여 프로그래밍에 대한 기준선과 LATS를 평가한다. 두 데이터 세트는 자연어 문서열의 파이썬에서 합성된 프로그램의 정확성을 측정한다. 개별 솔루션을 액션 공간으로 사용하고 테스트 제품군과 컴파일러 피드백을 외부 관찰로 사용합니다. 우리는 Chen 등(2022)을 따르고 LLM을 사용하여 각 질문에 대해 구문적으로 유효한 "주장" 진술의 합성 테스트 세트를 생성한다. 각 단계에 대해 솔루션이이 테스트 제품군에서 평가되고 테스트 성공 및 실패, 컴파일러 출력을 포함한 결과가 관찰로 컨텍스트에 추가됩니다. 반사를 위해 동일한 테스트 세트를 사용합니다.\n' +
      '\n' +
      '이 작업의 경우 추론 및 연기 기준선은 동작 공간을 공유하지만 연기 방법은 관찰을 추가 컨텍스트로 통합할 수 있다. LATS의 경우 각 동작이 완전한 솔루션에 해당하기 때문에 LATS의 시뮬레이션 단계를 건너뛰고 통과된 테스트의 백분율을 역전파 보상으로 직접 사용한다. 우리는 \\(k=8\\) 반복을 사용하고, 생성된 테스트의 수를 \\(4\\)으로 설정하며, 확장 동안 샘플 \\(n=5\\) 솔루션을 사용한다. 검색이 완료된 후, 가장 높은 값을 갖는 솔루션을 선택하고 pass@1 정확도 평가를 위한 실제 테스트 제품군에서 평가한다. 자세한 내용과 프롬프트는 Sec에서 확인할 수 있습니다. D 및 Sec. 부록의 F\n' +
      '\n' +
      '**결과.** 탭 3에서 검색 및 의미 피드백이 더 나은 성능을 위해 중요하다는 것을 발견했습니다. 관찰을 사용하지 않았음에도 불구하고 ToT와 RAP는 반사성과 경쟁적이다. LATS는 두 데이터셋에서 가장 높은 성능을 보인다. RAP는 LATS와 유사한 탐색 알고리즘을 사용하기 때문에 프로그래밍과 같은 어려운 추론 작업에 대한 외부 피드백의 중요성을 보여준다. GPT-4를 사용하면, LATS를 사용하여 HumanEval에 대한 최신 기술을 설정하고, LATS를 보여주는 것은 더 높은 성능을 위해 더 진보된 LLM과 함께 사용될 수 있다.\n' +
      '\n' +
      '### Weshshop\n' +
      '\n' +
      '실제 응용이 가능한 복잡한 의사결정 환경을 위해 1.18M 실세계 제품과 12k 휴먼 인스트럭션이 포함된 웹사이트로 구성된 온라인 쇼핑 환경인 WebShop(Yao et al., 2022)을 고려한다. 에이전트는 사용자 사양과 일치하는 아이템을 구매하기 위해 다양한 명령을 통해 웹사이트를 탐색해야 한다. 검색 및 클릭 명령의 사전 구성된 작업 공간과 관찰에 대한 브라우저 피드백 및 반사를 사용한다. 성능은 선택된 제품에 의해 충족된 사용자 지정 속성의 백분율을 반영하는 평균 점수와 선택된 제품이 주어진 모든 조건을 충족하는 빈도를 나타내는 성공률의 두 가지 메트릭을 사용하여 측정된다. 우리는 연기 기반 프롬프트 방법과 RL 기반과 비교한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c|c} \\hline \\hline\n' +
      '**Prompt Method** & **Model** & **Pass@1** \\\\ \\hline CoT (Wei et al., 2022) & GPT-3.5 & 46.9 \\\\ ReAct (Yao et al., 2023b) & GPT-3.5 & 56.9 \\\\ Reflexion (Shim et al., 2023) & GPT-3.5 & 68.1 \\\\ ToT (Yao et al., 2023a) & GPT-3.5 & 54.4 \\\\ RAP (Hao et al., 2023) & GPT-3.5 & 63.1 \\\\ LATS (Ours) & GPT-3.5 & **83.8** \\\\ \\hline I/O & GPT-4 & 80.1 \\\\ Reflexion & GPT-4 & 91.0 \\\\ LATS & GPT-4 & **94.4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: HumanEval (Chen et al., 2021) 및 MBPP (Austin et al., 2021) 상의 GPT-3.5 및 GPT-4 Pass@1 정확도. LATS로 프롬프팅하면 최고 성능을 달성합니다. 우리는 8번의 반복을 위해 확장 중에 5개의 용액을 샘플링한다.\n' +
      '\n' +
      '접근 50개의 지침에 대해 평가하고, LATS에 대해 \\(n=5\\), LATS에 대해 \\(k=30\\), ReAct best of \\(k\\), Reflexion을 설정한다. 더 자세한 내용과 프롬프트는 부록 D와 G에 있다.\n' +
      '\n' +
      '**결과.** 탭에서 찾을 수 있습니다. 도 4에 도시된 바와 같이, ReAct를 갖는 GPT-3.5는 모방 학습에 경쟁적이며, 더 강력한 프롬프트 전략으로 강화 학습 기술을 초과할 수 있다. ReAct와 Reflexion을 이용하여 \\(k=30\\)개의 궤적을 샘플링하면 비슷한 성능을 보여 웹샵과 같은 복잡한 환경에서는 의미적 피드백이 도움이 되지 않음을 알 수 있다. 실제로 Shinn 등(2023)에서와 같이, 우리는 생성된 반사들이 종종 일반적이며 유용한 피드백을 제공하지 않아 에이전트가 로컬 최소치에 갇히는 경향을 초래한다는 것을 발견한다. 그러나 LATS를 사용하면 실제로 눈에 띄는 개선이 발생하여 동일한 반복 횟수에 대해 더 효과적인 탐색을 나타낸다.\n' +
      '\n' +
      '### Additional Observations\n' +
      '\n' +
      '또한 LATS의 각 구성요소의 효과를 입증하기 위해 HotPotQA에 대한 추가 실험을 수행한다. 또한 ReAct 프롬프트가 있는 ToT 및 RAP 버전을 설계하고 외부 관찰을 처리할 수 있다. 설정에는 추론(생각을 통해) 및 동작(API 호출을 통해)이 모두 포함되므로 HotPotQA를 사용합니다. 결과는 Tab에 표시됩니다. 5. HotPotQA에서 토큰 소비를 위한 추가 삭제가 Tab에 있습니다. 부록 C의 7. 기준선은 일반적으로 HotPotQA의 추론 전용 설정보다 더 나쁜 성능을 발휘한다는 점에 유의한다. 이는 작용 기반 설정이 더 어렵고 의사 결정 시나리오에 대한 검색 알고리즘의 적응이 자명하지 않다는 것을 나타낸다.\n' +
      '\n' +
      '**자기 반영.** 자기 반영을 사용 하 여 에이전트에 대 한 추가 의미 신호를 제공 합니다. LATS에서 제거되었을 때 \\(a_{0}.05\\) 성능 저하가 관찰되어 유용함을 알 수 있었다. 이것은 Tab에 도시된 바와 같이 ReAct(Yao et al., 2023b)에 걸쳐 관찰하는 더 작은 이득 반사(Shinn et al., 2023)이다. 2는 자기성찰과 탐색으로 개선이 있는 문항 유형 간 중복을 시사한다. 이 변형은 MCTS에 대한 개선 사항을 반영하여 RAP-ReAct보다 성능이 우수하다.\n' +
      '\n' +
      '**검색 알고리즘.** MCTS는 A* 또는 DFS 검색과 같은 변형 및 관찰된 성능 이득의 기반보다 더 원칙적인 검색 알고리즘입니다. DFS 사용의 효과를 관찰하고 값이 낮은 가지를 가지치기하는 ToT(Yao et al., 2023a)에서 사용된 LM 기반 휴리스틱을 통합한다. 이것은 선택 및 역전파 연산을 제거하며, 동일한 수의 노드를 샘플링할 때 성능이 \\(0.08\\) 떨어지는 것을 관찰하지만 ToT-ReAct보다 성능이 우수하다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 논문에서는 향상된 LLM 문제해결을 위한 계획, 행동, 추론을 통합하기 위한 첫 번째 프레임워크인 언어 에이전트 트리 검색(LATS)을 소개한다. 탐색 알고리즘으로 궤적을 의도적으로 구성하고 외부 피드백을 통합하며 에이전트가 경험에서 학습할 수 있도록 함으로써 LATS는 사전 프롬프트 기술의 주요 한계를 해결한다. 우리의 평가는 LATS가 추가 교육 없이 추론 능력을 유지하면서 다양한 의사 결정 작업에 LLM 기능을 활용하는 능력을 보여준다. 상기 제안된 검색 간의 시너지들은,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c} \\hline Method & Score & SR \\\\ \\hline ReAct (Yao et al., 2023b) & 53.8 & 28.0 \\\\ ReAct (best of k) & 59.1 & 32.0 \\\\ Reflexion (Shinn et al., 2023) & 64.2 & 35.0 \\\\ LATS & **75.9** & **38.0** \\\\ \\hline IL & 59.9 & 29.1 \\\\ IL+RL & 62.4 & 28.7 \\\\ Fine-tuning (Furts et al., 2023) & 67.5 & 45.0 \\\\ \\hline Expert & 82.1 & 59.6 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: WebShop에서의 점수 및 성공률(SR). 테이블은 프롬프트, RL 기반 훈련 및 인간 수행으로 구분된다. 동일한 반복 횟수에 대해 LATS는 점수와 성공률을 모두 개선하고 RL 기반 훈련을 능가한다. IL/IL+RL taken from Yao et al. (2022).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c} \\hline Prompt Method & HotPotQA (EM) \\\\ \\hline ToT (ReAct) & 0.39 \\\\ RAP (ReAct) & 0.54 \\\\ LATS (No LM Heuristic) & 0.37 \\\\ LATS (DFS) & 0.42 \\\\ LATS (No Reflection) & 0.56 \\\\ LATS & 0.61 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: HotPotQA의 LATS 및 기준 변형에 대한 절제 결과; ReAct를 기본 프롬프트로 사용하고 \\(n=5\\) 어린이 및 \\(k=50\\) 최대 궤적을 샘플로 사용한다. LATS는 최적의 성능을 위해 모든 컴포넌트와 동작을 필요로 한다.\n' +
      '\n' +
      '상호 작용 및 성찰은 자율적 의사 결정에 대한 다재다능한 접근법을 제공하여 LLM이 일반주의자 에이전트로서의 잠재력을 강조한다. 제한과 광범위한 영향에 대한 완전한 논의는 부록 B에 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Ahn et al. [2022] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can, not as i say: Grounding language in robotic affordances. _arXiv:2204.01691_, 2022.\n' +
      '* Anthony et al. [2017] T. Anthony, Z. Tian, and D. Barber. Thinking fast and slow with deep learning and tree search. In _NIPS_, 2017.\n' +
      '* Austin et al. [2021] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. _arXiv:2108.07732_, 2021.\n' +
      '* Baker et al. [2022] Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. _arXiv:2206.11795_, 2022.\n' +
      '* Besta et al. [2023] Maciej Besta, Nils Bloch, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate problems with large language models. _arXiv:2308.09687_, 2023.\n' +
      '* Bowman et al. [2015] Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference. In _EMNLP_, 2015.\n' +
      '* Brown et al. [2021] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In _NeurIPS_, 2020.\n' +
      '* Campbell et al. [2002] Murray Campbell, A Joseph Hoane Jr, and Feng-hsiung Hsu. Deep blue. _Artificial intelligence_, 2002.\n' +
      '* Chen 등 [2022a] Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 코드: 생성된 테스트를 사용하여 코드 생성합니다. _ arXiv:2207.10397_, 2022a.\n' +
      '* Chen et al. [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. _arXiv:2107.03374_, 2021.\n' +
      '* Chen 등 [2022b] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 생각 프롬프트 프로그램: 숫자 추론 작업에 대 한 추론에서 계산을 분리 합니다. _ arXiv preprint arXiv:2211.12588_, 2022b.\n' +
      '* Chowdhery et al. [2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv:2204.02311_, 2022.\n' +
      '* Cobbe et al. [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. _arXiv:2110.14168_, 2021.\n' +
      '* Cobbe et al. [2021]Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: 웹에 대한 일반 에이전트입니다. _ arXiv:2306.06070_, 2023.\n' +
      '* Driess 등(2023) 대니 Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: 구체화된 멀티모달 언어 모델. _ arXiv:2303.03378_, 2023.\n' +
      '* Du 등(2023) Yilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Joshua B. Tenenbaum, Dale Schuurmans, and Pieter Abbeel. 텍스트 유도 비디오 생성을 통해 범용 정책을 학습합니다. _ arXiv:2302.00111_, 2023.\n' +
      '* Evans(2010) Jonathan St BT Evans. 직관과 추론: 이중 프로세스 관점입니다. _ 심리적 탐구, 2010년\n' +
      '* Fan 등(2022) Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedjo: 인터넷 규모의 지식을 가진 개방형 구현 에이전트 구축 2022년 _NeurIPS 데이터 세트 및 Benchmarks Track_ 에서입니다.\n' +
      '* Furuta 등(2023) Hiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Matsuo, Shixiang Shane Gu, and Izzeddin Gur. 명령어-운동 기초 모델들을 갖는 멀티모달 웹 내비게이션. _ arXiv preprint arXiv:2305.11854_, 2023.\n' +
      '* Gao 등(2022) Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: 프로그램 지원 언어 모델. _ arXiv preprint arXiv:2211.10435_, 2022.\n' +
      '* Guo 등(2018) Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. 유출된 정보를 사용하여 적대적 교육을 통해 긴 텍스트를 생성합니다. _ AAAI_, 2018.\n' +
      '* Guss et al. (2019) William H. Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. 마이너럴: 대규모 지뢰기 시연 자료입니다. In _IJCAI_, 2019.\n' +
      '* Hafner 등(2019) Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. 픽셀에서 계획을 위한 잠재 동역학을 학습합니다. In _ICML_, 2019.\n' +
      '* Hafner 등(2023) Danijar Hafner, Jurgis Paskonis, Jimmy Ba, and Timothy Lillicrap. 세계 모델을 통해 다양한 도메인을 마스터합니다. _ arXiv:2301.04104_, 2023.\n' +
      '* Hao 등(2023) Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 언어 모델을 사용 하는 추론은 세계 모델을 사용 하 여 계획 합니다. _ arXiv:2305.14992_, 2023.\n' +
      '* Huang 등(2023) Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 큰 언어 모델은 아직 자기 수정 추론을 수행할 수 없습니다. _ arXiv:2310.01798_, 2023.\n' +
      '* Huang et al.(2022) Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. _ arXiv:2207.05608_, 2022.\n' +
      '* Jiang et al. (2018) D. Jiang, E. Ekwedike, and H. Liu. 강화 학습을 위한 피드백 기반 트리 탐색 방법. In _ICML_, 2018.\n' +
      '* Kocsis and Szepesvari (2006) Levente Kocsis and Csaba Szepesvari. 산적 기반 몬테카를로 계획이야 2006년 _ECML_ 에서\n' +
      '* Kojima 등(2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 대형 언어 모델은 영샷 추론기입니다. _ arXiv:2205.11916_, 2022.\n' +
      '* LaValle 등(2001) Steven M LaValle, James J Kuffner, BR Donald, 등. Rapidly-exploring random trees: Progress and prospects. _ Algorithmic and computational robotics: new directions_, 2001.\n' +
      '* Liu 등(2018) Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. 워크플로우 유도 탐색을 사용하여 웹 인터페이스에 대한 강화 학습 In _ICLR_, 2018.\n' +
      '\n' +
      '샤오 류, 하오 유, 한천 장, 이판 쉬, 쉬안유 레이, 한유 라이, 유구, 한량 딩, 카이원 맨, 게주안 양, 슈단 장, 상덩, 아오한 정, 정샤오 두, 첸후이 장, 성선, 톈쥔 장, 유수, 환선, 민리 황, 유샤오 동, 및 제 탕. Agentbench: llms를 에이전트로 평가 합니다. _ arXiv:2308.03688_, 2023.\n' +
      '* Madaan 등(2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimi Prabhumoye, Yiming Yang, Shashank Gupta, Bodhiswa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Self-feedback을 이용한 반복적 정제. _ arXiv:2303.17651_, 2023.\n' +
      '* Nallapati 등(2016) Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. 순서 간 rnns 및 그 이상을 사용하여 추상적인 텍스트 요약입니다. In _SIGINL_, 2016.\n' +
      '* Nye et al. (2021) Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. _ arXiv:2112.00114_, 2021.\n' +
      '* Gpt-4 기술 보고서(2023) OpenAI. Gpt-4 기술 보고서. _ arXiv:2303.08774_, 2023.\n' +
      '* Saparov and He (2022) Abulhair Saparov and He He. 언어 모델은 탐욕스러운 추론자입니다. 연쇄 사상의 체계적인 형식 분석 _ arXiv:2210.01240_, 2022.\n' +
      '* Schick 등(2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 도구 형식자: 언어 모델은 도구를 사용하는 방법을 스스로 가르칠 수 있습니다. _ arXiv:2302.04761_, 2023.\n' +
      '* Shen 등(2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Huggingpt: 채팅과 친구와의 포옹을 통해 ai 작업을 해결합니다. _ arXiv:2303.17580_, 2023.\n' +
      '*Shinn 등(2023) Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 반사: 언어 강화 학습을 갖는 언어 에이전트. _ arXiv:2303.11366_, 2023.\n' +
      '* Shridhar 등(2020) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Affworld: 대화형 학습을 위해 텍스트 및 구체화된 환경을 정렬합니다. _ arXiv: 2010.03768_, 2020.\n' +
      '* Silver et al.(2016) David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. _ nature_, 2016.\n' +
      '* Silver 등(2017) David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lillicrap, Fan Hui, L. 시프르, 조지 반 덴 드리셰, 토어 그래펠, 데미스 하사비스 인간의 지식 없이 바둑의 게임을 마스터하는 단계 _ Nature_, 2017.\n' +
      '* Sloman (1996) Steven A. Sloman. 두 가지 추론 체계에 대한 경험적 사례 심리적 게시판, 1996년\n' +
      '* Sun 등(2023) Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: 언어 모델을 사용한 피드백에서 적응형 계획입니다. _ arXiv:2305.16653_, 2023.\n' +
      '* Suris 등(2023) Didac Suris, Sachit Menon, and Carl Vondrick. Vipergrpt: 추론을 위해 Python 실행을 통한 시각적 추론입니다. _ arXiv preprint arXiv:2303.08128_, 2023.\n' +
      '* Swiechowski 등(2023) Maciej Swiechowski, Konrad Godlewski, Bartosz Sawicki, and Jacek Mandziuk. Monte carlo 트리 검색: 최근 수정 및 응용 프로그램에 대 한 검토입니다. _ Artificial Intelligence Review_, 2023.\n' +
      '* Touvron 등(2021) Hugo Touvron, Louis Martin, Kevin R. 스톤, 피터 알버트, 아마드 알마하이리, 야스민 바베이, 니콜라이 바슐리코프, 소미야 바트라, 프라즈왈 바르가바, 슈루티 보살, 다니엘 M. Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedami Goswami, Naman Goyal, Anthony S. 하트혼, 사하르 호세이니, 루이 호우, 하칸 이난, 마르신 카다스, 빅토르 케르케즈, 매디안 하바, 이사벨 M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 라마 2: 기반 및 미세 조정 채팅 모델을 엽니다. _ arXiv:2307.09288_, 2023.\n' +
      '* Vodopivec 등(2017) Tom Vodopivec, Spyridon Samothrakis, and Branko Ster. Monte carlo tree search and reinforcement learning. _ 2017년 인공지능 연구 저널.\n' +
      '* Wang 등(2023) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: 큰 언어 모델을 사용 하는 개방형 구현 에이전트입니다. _ arXiv:2305.16291_, 2023.\n' +
      '* Wang 등(2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 자기 일관성은 언어 모델에서 사고 추론의 사슬을 향상시킵니다. _ arXiv:2203.11171_, 2022.\n' +
      '* Wei 등(2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 생각의 사슬 프롬프트는 대규모 언어 모델에서 추론을 이끌어냅니다. _ arXiv:2201.11903_, 2022.\n' +
      '* 울드리지 및 제닝스(1995) 마이클 울드리지 및 니콜라스 R 제닝스. 지능형 에이전트: 이론과 실행 _ The Knowledge Engineering Review_, 1995.\n' +
      '* Wu 등(2023) Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. 공상자: 물리적 로봇 학습을 위한 세계 모델입니다. _CoRL_에서입니다. PMLR, 2023.\n' +
      '* Xie 등(2023) Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. 분해는 자체 평가 가이드 디코딩을 통해 추론을 향상시킵니다. _ arXiv:2305.00633_, 2023.\n' +
      '* Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: 다양 하 고 설명 가능한 다중 홉 질문 응답을 위한 데이터 세트입니다. _ arXiv:1809.09600_, 2018.\n' +
      '*Yao 등(2022) Shunyu Yao, Howard Chen, John Yang, and Karthik R Narasimhan. 웹스푼: 기반 언어 에이전트와 확장 가능한 실제 웹 상호 작용을 지향합니다. 2022년 _NeurIPS_ 에 있습니다.\n' +
      '*Yao 등(2023a) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. 그리피스, 위안 카오, 카르틱 나라심한. 생각의 나무: 큰 언어 모델을 사용 하 여 문제 해결을 검토 합니다. _ arXiv:2305.10601_, 2023a.\n' +
      '*Yao 등(2023b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 리액션: 언어 모델에서 추론과 액션을 동기화합니다. _ICLR_, 2023b.\n' +
      '*Yao 등(2023c) Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, and Silvio Savarese. Retroformer: 정책 그래디언트 최적화를 사용 하 여 후향적 대형 언어 에이전트입니다. _ arXiv preprint arXiv:2308.02151_, 2023c.\n' +
      '* Ye 등(2021) Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. 데이터가 제한된 아타리 게임을 마스터하고 있습니다. 2021년 _NeurIPS_ 에 있습니다.\n' +
      '* Zhou 등(2022) Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 최소 최대 프롬프팅은 대규모 언어 모델에서 복잡한 추론을 가능하게 합니다. _ arXiv:2205.10625_, 2022.\n' +
      '* Zhu 등(2023) Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the minecraft: 일반적으로 텍스트 기반 지식과 메모리를 사용 하는 대규모 언어 모델을 통해 오픈 월드 환경에 대 한 가능한 에이전트입니다. _ arXiv:2305.17144_, 2023.\n' +
      '\n' +
      'Appendix\n' +
      '\n' +
      '부록은 다음과 같이 구성되어 있습니다. 첫번째는 Sec. A, 우리는 제안된 알고리즘인 LATS의 의사코드를 보여주고, Sec. B, 우리는 방법과 그 한계, 향후 방향 및 광범위한 영향에 대한 추가 논의를 제공한다. C: Sec. D, 우리는 실험에서 환경 세부 사항을 지정하고 마지막으로 Sec의 세 가지 환경에 사용되는 프롬프트를 나열한다. E(HotPotQA), Sec. F(Programming) 및 Sec. G(Webshop)를 각각 포함한다.\n' +
      '\n' +
      '## 부록 A LATS Pseudocode\n' +
      '\n' +
      'Alg. 도 1은 우리의 알고리즘 LATS의 의사코드를 도시한다. 노드는 메모리에 명시적으로 저장됩니다. 달리 명시되지 않는 한, 모든 실험에서 우리는 \\(n=5\\)과 \\(w=1\\)을 사용한다.\n' +
      '\n' +
      '```\n' +
      '0: 초기 상태 \\(s_{1}\\), 액션 발생기 \\(p_{\\theta}\\), 값 함수 \\(p_{V}\\), 반사 발생기 \\(p_{\\text{pr}\\), 생성된 액션 수 \\(n\\), 깊이 제한 \\(L\\), 롤아웃 수 \\(K\\), 컨텍스트 \\(c\\), 탐색 가중치 \\(w\\) 초기 상태-동작 값 함수 \\(p_{V}:S\\mapsto\\mathbb{R}\\) 및 방문 카운터 \\(N:S\\mapsto\\mathbb{N}\\)를 0으로 초기화\\(k\\gets 0,\\ldots,K-1\\)do의 경우 \\(t\\gets 0,\\ldots,L-1\\)do의 경우 \\(t\\gets 1,\\ldots,n\\)do의 경우 Expansion & Simulation  샘플 \\(a_{t}^{(i)}\\sim p_{\\theta}(a\\mid s_{t})\\)  환경에서 \\(o_{t}^{(i)}\\)의 획득 \\(s_{t+1}^{(i\n' +
      '```\n' +
      '\n' +
      '**Algorithm 1**\\(\\operatorname*{LATS}(S_{0},p_{\\theta},p_{V},p_{\\text{pr}},d,k,n,w)\\)\n' +
      '\n' +
      '## Appendix B Discussion\n' +
      '\n' +
      '**제한 사항.** LATS는 추론 및 의사 결정을 개선할 수 있지만 ReAct 또는 Reflexion과 같은 더 간단한 프롬프트 방법에 비해 더 높은 계산 비용에 도달합니다. 검색 프로세스는 표준 프롬프트 또는 더 간단한 기법보다 더 많은 시간이 걸리고, 더 큰 추론 비용이 필요하다. 이러한 문제는 모든 단계에서 확장되는 노드의 수 \\(n\\)가 성능과 효율성 사이의 자연스러운 절충을 제공한다는 사실에 의해 완화되지만(설정 \\(n=1\\)은 다중 시도 또는 CoT-SC와 같은 방법을 효율적으로 만들기 때문에), 실제로 프로그래밍과 같은 어려운 작업이나 효율성보다 성능이 우선시되는 상황에서는 LATS를 사용하는 것이 좋다.\n' +
      '\n' +
      'LLM의 지속적인 발전이 LATS의 비용을 줄이고 실용성을 높일 수 있기를 바란다.\n' +
      '\n' +
      '또한, 본 논문에서 사용한 벤치마크는 실제 상호작용 환경의 복잡성에 비해 상대적으로 간단하고 의사 결정에 중점을 둔다. 또한 일부 환경은 이전 상태로 롤백을 쉽게 지원하지 않을 수 있습니다. 그러나, LATS의 설계는 유연하고 다양한 자원 제약들에 조정될 수 있다. 마인크래프트(Fan et al., 2022)와 같은 환경에서 LATS와 같은 계획 기반 프롬프트 방법과 더 많은 추론 벤치마크를 사용하는 것은 향후 작업을 위한 흥미로운 방법이 될 것이다.\n' +
      '\n' +
      '**더 넓은 영향.** LATS는 환경과의 상호 작용을 통해 LLM 성능을 향상시키는 프레임워크입니다. 자율적인 의사 결정의 이러한 개선은 LLM의 유해한 사용을 촉진할 수 있다. 대안적으로, LATS는 암시적이고 낮은 수준의 토큰 값에 의존하기보다는 여러 라운드의 의사 결정과 반성을 통해 이해 가능하고 높은 수준의 언어적 추론과 행동을 생성하기 때문에 해석 가능성과 더 큰 정렬 가능성을 향상시킨다.\n' +
      '\n' +
      '## Appendix C Ablues\n' +
      '\n' +
      '이 섹션에서는 LATS의 다양한 디자인을 축소합니다. HotPotQA는 최대 \\(k=50\\) 궤적과 샘플링 크기는 \\(n=5\\), HumanEval은 최대 \\(k=8\\) 궤적과 샘플링 크기는 \\(n=5\\)이다. HotPotQA에 대한 결과는 Tab에 표시됩니다. 도 5 및 도 4의 HumanEval.\n' +
      '\n' +
      '**탐색 가중치.** 선택 공식의 탐색 가중치 \\(w\\)가 \\(0.5\\)으로 감소하면 HotPotQA에서 성능이 낮아지므로 검색의 효율성이 감소함을 알 수 있습니다. \\(w\\)에서 \\(2.0\\)으로 증가시키는 것은 성능 향상으로 이어지지 않지만 더 빠른 수렴을 관찰하는 경향이 있다. 최적의 설정은 상태 공간의 특정 환경 및 복잡성에 의존한다.\n' +
      '\n' +
      '**깊이.** 본 실험에서는 이전 작업(Yao 등, 2023)에 따라 모든 방법에 대해 HotPotQA에서 최대 깊이 \\(d=7\\)를 사용합니다. LATS를 \\(d=4\\)으로 줄인 후 LATS에 미치는 영향을 제거한다. 이로 인해 성능이 약간만 저하됩니다. 우리는 대부분의 질문에 4단계 내에서 대답할 수 있으며 더 많은 단계를 사용하면 에이전트가 국소 최소값으로 강제되는 경향이 있고 성공을 거의 개선하지 않는다는 것을 발견했다.\n' +
      '\n' +
      '**LM 값 함수.** LM 값 함수는 예상 된 미래 보상을 기반으로 상태를 평가 합니다. 이 휴리스틱이 없으면 검색을 안내하는 유일한 신호는 희소하고 종종 이진인 완성된 궤적에 대한 환경 보상에서 비롯된다. 평가 작업을 제거하면 성능이 급격히 떨어지는 것을 관찰할 수 있다 \\(0.24\\).\n' +
      '\n' +
      '**시간 경과에 따른 성능.** 샘플링된 궤적 수가 증가하는 효과를 보려면 \\(k\\)을 다른 값으로 변경합니다. 우리는 더 적은 궤적을 샘플링하기 때문에 더 많은 통지 가능한 차이를 갖는 HumanEval에 대해 이 실험을 수행한다. 결과는 그림 1에 나와 있다. 4에서, LATS는 반사보다 더 많은 반복으로 더 잘 스케일링된다.\n' +
      '\n' +
      '**샘플 복잡성 및 토큰 비용.** LATS의 한 가지 가능한 문제는 트리 구조 검색이 기존 메서드보다 훨씬 더 많은 토큰을 소비할 수 있다는 것입니다. 기존의 방법과 비교하여 LATS의 계산 비용을 더 연구하기 위해, 본 논문에서 고려된 모든 방법의 샘플 복잡도(즉, 점근적 토큰 비용)를 조사하고, HotPotQA에서 성공적으로 검색될 때 본 방법과 다른 트리 구조화 방법(ToT 및 RAP)에 의해 확장된 노드의 평균 수를 계산한다. 결과를 탭에 제시합니다. 7; 본 논문에서 제안하는 방법은 다른 트리 기반 검색 방법과 동일한 샘플 복잡도를 가지며, 성공 시 확장되는 노드의 평균 개수가 적어 토큰 비용이 적게 드는 것을 알 수 있다. 토큰 비용 격차는 실패한 궤적을 고려할 때 훨씬 더 클 것이며, 이는 우리의 방법이 더 높은 성공률을 가지고 계산 예산 한계에 도달하는 경우가 적기 때문이다.\n' +
      '\n' +
      '## 부록 D 환경 세부 정보\n' +
      '\n' +
      '### HotPotQA\n' +
      '\n' +
      'HotPotQA(Yang et al., 2018)는 질문에 답하기 위해 여러 지원 문서에 대한 추론이 필요한 질문-답변 데이터세트이다. 113k 위키피디아 기반 질문 답변이 포함되어 있습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c} Method & Performance (\\(\\uparrow\\)) & Sample complexity (\\(\\downarrow\\)) & Avg. \\#nodes upon success (\\(\\downarrow\\)) \\\\ \\hline ReAct (Best \\(k=250\\)) & \\(0.42\\) & \\(O(k)\\) & N/A \\\\ CoT-SC (\\(n=1,k=250\\)) & \\(0.40\\) & \\(O(k)\\) & N/A \\\\ LATS (\\(n=1,k=50\\)) & \\(0.48\\) & \\(O(k)\\) & N/A \\\\ ToT (ReAct) & \\(0.49\\) & \\(O(kn)\\) & \\(84.05\\) \\\\ RAP (ReAct) & \\(0.54\\) & \\(O(kn)\\) & \\(70.60\\) \\\\ LATS (\\(n=5,k=50\\)) & \\(0.61\\) & \\(O(kn)\\) & \\(66.65\\) \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 성능, 다양한 방법의 샘플 복잡도 및 평균 노드 수는 트리 기반 검색을 사용한 방법으로 성공 시 확장되었다. \\ (n\\)은 모든 단계에서 확장된 자식 노드의 수이고 \\(k\\)은 궤적의 수이다. 제안하는 방법은 트리 기반 탐색을 사용하는 다른 방법과 동일한 샘플 복잡도를 가지며, 성공 시 노드 수가 줄어들어 토큰 비용이 낮아짐을 알 수 있다.\n' +
      '\n' +
      '도 5: ReAct(왼쪽) 및 LATS(오른쪽)에 대한 HotPotQA 상의 예시적인 궤적. LATS는 LM을 사용하여 상태를 평가하여 트리의 유망한 영역으로 검색을 안내함으로써 더 많은 작업을 샘플링하고 이전 실수로부터 실패를 피할 수 있다.\n' +
      '\n' +
      '크라우드 워커가 다양하고 멀티홉하며 설명할 수 있도록 만든 쌍입니다. 질문은 도면요소, 위치, 날짜 및 두 도면요소 간의 공유 속성 비교와 같은 다양한 유형을 포함합니다. 군중 노동자들은 또한 답변을 정당화하는 문서에서 뒷받침하는 사실을 제공한다. 우리는 검색을 테스트하기 위해 모든 위키피디아 단락과 함께 핫팟QA 벤치마크 설정을 사용한다. 실험을 위해 무작위로 선택된 100개 질문의 하위 집합과 최대 깊이 한계 6을 사용한다. 도 5는 HotPotQA의 예시적인 태스크에 대해 ReAct 및 LATS가 어떻게 동작하는지를 도시하고, 태스크에 대해 LATS가 ReAct보다 어떻게 우수한지에 대한 정성적 예를 제공한다.\n' +
      '\n' +
      '**액션 공간.** Yao 등(2023)에서 제안한 Wikipedia 웹 API를 채택하며, 대화형 정보 검색을 지원하는 세 가지 유형의 액션이 있습니다.\n' +
      '\n' +
      '(1) **search[entity]** 는 존재하는 경우 대응하는 엔티티 위키 페이지로부터 처음 5개의 문장을 반환하거나, 그렇지 않으면 위키피디아 검색 엔진으로부터 상위-5개의 유사한 엔티티를 제안하며,\n' +
      '\n' +
      '(2) 문자열 포함 페이지의 다음 문장을 반환하는 **lookup[string]**,\n' +
      '\n' +
      '(3) **답변** 을 완료 하 여 현재 작업을 답변으로 완료 합니다.\n' +
      '\n' +
      '이러한 API 호출 및 자유 형식 생각이 이 환경에 대 한 작업 공간을 형성 합니다.\n' +
      '\n' +
      '### Programming\n' +
      '\n' +
      'HumanEval 데이터세트(Chen et al., 2021)는 자연어 기술로부터 프로그램을 합성하기 위한 모델의 기능적 정확성을 평가하기 위해 도입된 164개의 필기 프로그래밍 문제의 모음이다. 각 문제에는 함수 서명, 문서 문자열 설명, 참조 구현 및 여러 단위 테스트가 포함되며 문제당 평균 7.7개의 테스트가 포함됩니다. 프로그래밍 과제는 단순한 소프트웨어 면접 문항에 필적하는 난이도 수준에서 자연어, 추론, 알고리즘, 기초 수학에 대한 이해도를 평가한다. 통과율은 pass@k 메트릭으로 평가되며, 여기서 k 샘플은 문제당 생성되고 어떤 샘플이 모든 테스트를 통과하면 문제가 해결된 것으로 간주된다. 우리는 실험에 164개의 문제를 모두 사용하고 최대 깊이 한계는 8이다.\n' +
      '\n' +
      'MBPP(Mostly Basic Programming Problems) Austin 등(2021) 벤치마크에는 프로그램 합성 기술을 평가하기 위해 설계된 974개의 짧은 파이썬 함수가 포함되어 있다. 데이터 세트는 파이썬 기본 지식을 가진 작업자로부터 크라우드소싱을 통해 구성되었다. 각 데이터 포인트는 프로그래밍 작업에 대한 자연어 기술, 참조 솔루션 구현 및 기능적 정확성에 대한 세 가지 테스트 케이스로 구성된다. 자연어 프롬프트는 일반적으로 짧은 단문 설명입니다. 솔루션은 파이썬 표준 라이브러리의 수학적 연산, 목록 처리, 문자열 조작 및 사용을 포함하는 일반적인 프로그래밍 구성을 다룬다. 평균적으로 해는 6.8줄의 코드입니다. 데이터 세트는 또한 명확한 사양, 표준 함수 서명 및 정확한 테스트 케이스에 대해 수동으로 검증된 426개 문제의 추가 세트로 보완된다. 실험을 위해 397개 문제의 무작위로 선택된 하위 집합을 사용한다.\n' +
      '\n' +
      '### WebShop\n' +
      '\n' +
      'WebShop(Yao et al., 2022)은 근거 언어 이해 및 의사 결정에 대한 에이전트를 평가하도록 설계된 대화형 웹 기반 환경이다. 5개의 카테고리와 113개의 하위 카테고리에 걸쳐 아마존에서 스크래핑된 100만 개 이상의 실제 제품을 에이전트에 제공하여 전자 상거래 쇼핑 작업을 시뮬레이션한다. 이들 제품은 평균 텍스트 길이가 262단어, 어휘 크기가 224k로 풍부한 언어적 정보를 담고 있다. 또한 커스터마이징에 사용할 수 있는 800k 이상의 고유한 제품 옵션이 있습니다. 이 환경은 웹페이지를 두 가지 모드로 렌더링한다: HTML 모드는 대화형 요소를 가진 픽셀 수준의 관찰을 제공하는 반면, 단순 모드는 원시 HTML을 훈련 에이전트에 더 적합한 구조화된 텍스트 관찰로 변환한다. 액션 공간은 질의 검색과 버튼 클릭으로 구성되며, 검색, 결과, 항목 및 항목 세부사항의 4가지 페이지 유형 사이에서 전환된다. 지침은 제품 속성과 옵션을 지정하는 크라우드소싱된 자연어로 총 12k가 수집됩니다. 자동 보상은 어휘 매칭 및 의미 유사성 메트릭을 모두 사용하여 에이전트가 구매한 제품을 명령어에 지정된 속성 및 옵션과 비교하여 계산된다.\n' +
      '\n' +
      'WebShop에는 두 가지 평가 메트릭이 사용 됩니다. (1) **작업 점수**: \\((100\\times\\text{avg. reward})로 정의 됩니다. )\\), 에피소드에 걸쳐 얻은 평균 보상을 캡처합니다. 그리고 (2) **성공률(SR)** 은 \\(r=1\\) 인스트럭션의 일부로 정의됩니다. 상기 리워드는 상기 선택된 아이템이 만족하는 속성의 개수에 기초하여 산출되는 것을 특징으로 하는 이동 단말기. 실험을 위해 50개의 환경을 사용하고 최대 깊이 한계는 15이다.\n' +
      '\n' +
      '## Appendix E HotPotQA Prompts\n' +
      '\n' +
      '### Base Acting Prompt\n' +
      '\n' +
      '인터리빙 생각, 작업, 관찰 단계로 질문 응답 작업을 해결합니다. 생각은 현재 상황에 대해 추론할 수 있으며 액션은 세 가지 유형이 될 수 있습니다.\n' +
      '\n' +
      '(1) 위키피디아에서 정확한 개체를 검색하여 존재하면 첫 번째 단락을 반환하는 검색[엔터티] 그렇지 않으면 검색할 일부 유사한 도면요소가 반환됩니다.\n' +
      '\n' +
      '(2) 현재 구절에서 키워드가 포함된 다음 문장을 반환하는 룩업[키워드].\n' +
      '\n' +
      '(3) 답변을 회신하고 과제를 종료하는 마무리[답변]\n' +
      '\n' +
      '각 관찰 후에 다음 생각과 다음 액션을 제공합니다.\n' +
      '\n' +
      '다음은 몇 가지 예입니다.\n' +
      '\n' +
      '질문: 어떤 잡지가 아서의 잡지에서 가장 먼저 시작되었나요, 아니면 여성을 위해 가장 먼저 시작되었나요?\n' +
      '\n' +
      ' 생각 1: 나는 아서의 잡지와 여성 우선지를 검색하고 어떤 것이 먼저 시작되었는지 찾아야 한다.\n' +
      '\n' +
      ' Action 1: Search[Arthur\'s Magazine]\n' +
      '\n' +
      ' 관찰 1: 아서의 잡지(184400e2008000931846)는 19세기 필라델피아에서 출판된 미국 문학 정기 간행물이었다. 티모시 셰이 아서가 편집한 이 작품은 에드거 A. 포, J.H. 잉그라함, 사라 호세파 헤일, 토마스 G. 스피어 등이 출연했다.[1][2] 1846년 5월에 그것은 고디의 부인의 책으로 병합되었다.[3]\n' +
      '\n' +
      ' 2: 아서의 잡지는 1844년에 시작되었습니다. 다음 여성을 먼저 찾아야 합니다.\n' +
      '\n' +
      ' 액션 2: 검색[여성 우선]\n' +
      '\n' +
      ' 관찰 2: First for Women은 미국의 Bauer Media Group에서 발행하는 여성잡지이다.[1] 이 잡지는 1989년에 시작되었습니다. 뉴저지주 잉글우드 절벽에 기반을 두고 있습니다.[2] 2011년 잡지의 발행부수는 1,310,696부였다.[3]\n' +
      '\n' +
      ' 3: 여성을 위한 첫번째는 1989년에 시작되었어요. 1844년(아서스 매거진)! 1989년 (여성들을 위한 첫 번째) 그래서 아서의 잡지가 먼저 시작되었다.\n' +
      '\n' +
      ' 액션 3: 마무리[아서스 매거진]\n' +
      '\n' +
      '(examples)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline Type & Argument & State \\(\\rightarrow\\) Next State \\\\ \\hline search & [_Query_] & Search \\(\\rightarrow\\) Results \\\\ choose & Back to search & \\(*\\rightarrow\\) Search \\\\ choose & Prev/Next page & Results \\(\\rightarrow\\) Results \\\\ choose & [_Product title_] & Results \\(\\rightarrow\\) Item \\\\ choose & [_Option_] & Item \\(\\rightarrow\\) Item \\\\ choose & Desc/Overview & Item \\(\\rightarrow\\) Item-Detail \\\\ choose & Previous & Item-Detail \\(\\rightarrow\\) Item \\\\ choose & Buy & Item \\(\\rightarrow\\) Episode End \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 웹샵의 액션 공간.\n' +
      '\n' +
      '이전에 다음 질문에 답하려고 시도했다가 실패했습니다. 다음 반영은 이전에 수행한 것과 동일한 방식으로 질문에 응답하지 않도록 하는 계획을 제공합니다. 지정된 질문에 올바르게 답하는 전략을 개선하려면 이 항목을 사용합니다.\n' +
      '\n' +
      '(trajectories)\n' +
      '\n' +
      '(input)\n' +
      '\n' +
      '### Base Reasoning Prompt\n' +
      '\n' +
      '생각을 통해 질문에 답하는 작업을 해결한 다음 답변을 완료합니다. 생각은 현재 상황에 대해 추론할 수 있다. 답변을 끝내고 일을 끝내다 질문에 답하는 데 사용해야 하는 컨텍스트가 제공됩니다. 액션 또는 인덱싱된 사상으로 응답 시작\n' +
      '\n' +
      '다음은 몇 가지 예입니다.\n' +
      '\n' +
      '질문: 콜로라도 조산물의 동부 지역이 확장되는 지역의 고도 범위는 얼마인가?\n' +
      '\n' +
      '차근차근 생각해 봅시다.\n' +
      '\n' +
      '생각 1: 콜로라도 조산물의 동부 부문은 하이 플레인으로 확장된다.\n' +
      '\n' +
      '생각 2: 고지대가 약 1,800 피트에서 7,000 피트로 상승\n' +
      '\n' +
      '생각 3: 답은 1,800에서 7,000피트입니다.\n' +
      '\n' +
      'Action: Finish[1,800 ~ 7,000 ft]\n' +
      '\n' +
      '(examples)\n' +
      '\n' +
      '이전 재판: (궤적)\n' +
      '\n' +
      '(input)\n' +
      '\n' +
      '### 값 함수 프롬프트\n' +
      '\n' +
      '질문 응답 작업에 대한 솔루션의 궤적을 분석합니다. 궤적은 상황에 대한 환경적 관찰, 현재 상황에 대해 추론할 수 있는 생각 및 세 가지 유형이 될 수 있는 행동에 의해 라벨링된다:\n' +
      '\n' +
      '(1) 위키피디아에서 정확한 개체를 검색하여 존재하면 첫 번째 단락을 반환하는 검색[엔터티] 그렇지 않으면 검색할 일부 유사한 도면요소가 반환됩니다.\n' +
      '\n' +
      '(2) 현재 구절에서 키워드가 포함된 다음 문장을 반환하는 룩업[키워드].\n' +
      '\n' +
      '(3) 답변을 회신하고 과제를 종료하는 마무리[답변]\n' +
      '\n' +
      '질문과 궤적이 주어지면 그 정확성을 평가하고 추론과 분석을 자세히 제공합니다. 최신 사고, 행동, 관찰에 집중하세요. 아직 답을 찾지 못하더라도 지금까지의 생각과 행동이 정확하다면 불완전한 궤적은 정확할 수 있다. 추가적인 생각이나 행동을 생성하지 마십시오. 그 다음 마지막 라인에서 "따라서 정확성 스코어는 "이며, 여기서 s는 1 내지 10의 정수이다."라고 결론짓는다.\n' +
      '\n' +
      '질문: 어떤 잡지가 아서의 잡지에서 가장 먼저 시작되었나요, 아니면 여성을 위해 가장 먼저 시작되었나요?\n' +
      '\n' +
      '생각 1: 나는 아서의 잡지와 여성 우선지를 검색하고 어떤 것이 먼저 시작되었는지 찾아야 한다.\n' +
      '\n' +
      'Action 1: Search[Arthur\'s Magazine]\n' +
      '\n' +
      '관찰 1: 아서의 잡지(18440e2008000931846)는 19세기 필라델피아에서 출판된 미국 문학 정기 간행물이었다. 티모시 셰이 아서가 편집한 이 작품은 에드거 A. 포, J.H. 잉그라함, 사라 호세파 헤일, 토마스 G. 스피어 등이 출연했다.[1][2] 1846년 5월에 그것은 고디의 부인의 책으로 병합되었다.[3] 이 궤적은 질문에 제공된 최초의 잡지를 검색하는 것이 타당하므로 정확하다. 또한 단일 엔티티에 대응하는 간단한 검색을 갖는 것이 더 좋아, 이것이 최상의 액션이 된다.\n' +
      '\n' +
      '따라서, 상기 정확성 점수는 10\n' +
      '\n' +
      '(other examples)\n' +
      '\n' +
      '(failed trajectories)\n' +
      '\n' +
      '(context)\n' +
      '\n' +
      '### Reflection Prompt\n' +
      '\n' +
      '질문 응답 작업에 대한 솔루션의 궤적을 분석합니다. 궤적은 상황에 대한 환경적 관찰, 현재 상황에 대해 추론할 수 있는 생각 및 세 가지 유형이 될 수 있는 행동에 의해 라벨링된다:\n' +
      '\n' +
      '(1) 위키피디아에서 정확한 개체를 검색하여 존재하면 첫 번째 단락을 반환하는 검색[엔터티] 그렇지 않으면 검색할 일부 유사한 도면요소가 반환됩니다.\n' +
      '\n' +
      '(2) 현재 구절에서 키워드가 포함된 다음 문장을 반환하는 룩업[키워드].\n' +
      '\n' +
      '(3) 답변을 회신하고 과제를 종료하는 마무리[답변]\n' +
      '\n' +
      '질문과 궤적이 주어지면 그 정확성을 평가하고 추론과 분석을 자세히 제공합니다. 최신 사고, 행동, 관찰에 집중하세요. 아직 답을 찾지 못하더라도 지금까지의 생각과 행동이 정확하다면 불완전한 궤적은 정확할 수 있다. 추가적인 생각이나 행동을 생성하지 마십시오. 그런 다음 마지막 줄에서 "따라서 정확성 점수는 s"로 결론짓고, 여기서 s는 1 내지 10의 정수이다.\n' +
      '\n' +
      '질문: 어떤 잡지가 아서의 잡지에서 가장 먼저 시작되었나요, 아니면 여성을 위해 가장 먼저 시작되었나요?\n' +
      '\n' +
      '생각 1: 나는 아서의 잡지와 여성 우선지를 검색하고 어떤 것이 먼저 시작되었는지 찾아야 한다.\n' +
      '\n' +
      'Action 1: Search[Arthur\'s Magazine]\n' +
      '\n' +
      '관찰 1: 아서의 잡지(184400e2008000931846)는 19세기 필라델피아에서 출판된 미국 문학 정기 간행물이었다. 티모시 셰이 아서가 편집한 이 작품은 에드거 A. 포, J.H. 잉그라함, 사라 호세파 헤일, 토마스 G. 스피어 등이 출연했다.[1][2] 1846년 5월에 그것은 고디의 부인의 책으로 병합되었다.[3]\n' +
      '\n' +
      '이 궤적은 질문에 제공된 최초의 잡지를 검색하는 것이 타당하므로 정확하다. 또한 단일 엔티티에 대응하는 간단한 검색을 갖는 것이 더 좋아, 이것이 최상의 액션이 된다.\n' +
      '\n' +
      '따라서, 상기 정확성 점수는 10\n' +
      '\n' +
      '(other examples)\n' +
      '\n' +
      '(failed trajectories)\n' +
      '\n' +
      '(context)\n' +
      '\n' +
      '## Appendix F Programming Prompt\n' +
      '\n' +
      '### HumanEval 함수 구현 예제\n' +
      '\n' +
      '샘플 함수 서명:\n' +
      '\n' +
      'def minSubArraySum(nums): 정수 배열이 주어지면 비어 있지 않은 수 하위 배열의 최소 합을 찾습니다. 예제  minSubArraySum([2, 3, 4, 1, 2, 4]) == 1  minSubArraySum([-1, -2, -3]) == -6min_sum = float(\'inf\') foriinrange(len(nums)): current_sum = 0 forjinrange(i,len(nums)): current_sum +=nums[j] ifcurrent_sum < min_sum: min_sum = current_sum returnmin_sum\n' +
      '\n' +
      '### Base Acting/Reasoning Prompt\n' +
      '\n' +
      '당신은 AI 파이썬 비서입니다. 함수의 이전 구현, 일련의 단위 테스트 결과 및 이전 구현에 대한 자기 성찰이 제공됩니다. 전체 구현을 작성합니다(함수 서명을 다시 시작합니다).\n' +
      '\n' +
      'Example 1:\n' +
      '\n' +
      '[previous impl]:\n' +
      '\n' +
      'defadd(a:int,b:int)->int: """ Givenintegersandb,returnthetotalvalueofaandb. ""returna-b\n' +
      '\n' +
      '[unit test results from previous impl]:\n' +
      '\n' +
      'Tested passed:\n' +
      '\n' +
      'Tests failed:\n' +
      '\n' +
      'assertadd(1,2)==3#output:-1 assertadd(1,2)==4#output:-1\n' +
      '\n' +
      '[reflection on previous impl]:\n' +
      '\n' +
      '이 구현은 입력 정수가 1과 2인 테스트 케이스에 실패하였다. 이 문제는 코드가 두 정수를 함께 추가하는 것이 아니라 첫 번째 정수에서 두 번째 정수를 빼기 때문에 발생한다. 이 문제를 해결하려면 반환문에서 연산자를 \'-\'에서 \'+\'로 변경해야 한다. 이것은 기능이 주어진 입력에 대한 정확한 출력을 반환하는 것을 보장할 것이다.\n' +
      '\n' +
      '[improved impl]:\n' +
      '\n' +
      'defadd(a:int,b:int)->int: """ Givenintegersandb,returnthetotalvalueofaandb. "" returna+b\n' +
      '\n' +
      '### Reflection Prompt\n' +
      '\n' +
      '당신은 파이썬 프로그래밍 비서입니다. 기능 구현 및 일련의 단위 테스트 결과가 제공됩니다. 여러분의 목표는 테스트에 표시된 대로 구현이 잘못된 이유를 설명하기 위해 몇 개의 문장을 작성하는 것입니다. 나중에 다시 시도할 때 지침으로 필요합니다. 답변에서 구현이 아닌 몇 가지 문장 설명만 제공합니다. 사용자에 의해 몇 가지 예가 제공됩니다.\n' +
      '\n' +
      'Example 1:\n' +
      '\n' +
      '[previous impl]:defadd(a:int,b:int)->int:  """  Givenintegersaandb,returnthetotalvalueofaandb.  " ""  returna-b\n' +
      '\n' +
      '[unit test results from previous impl]:\n' +
      '\n' +
      'Tested passed:\n' +
      '\n' +
      'Tests failed:\n' +
      '\n' +
      'assert add(1, 2) == 3 # output: -1\n' +
      '\n' +
      'assert add(1, 2) == 4 # output: -1\n' +
      '\n' +
      '[reflection on previous impl]:\n' +
      '\n' +
      '이 구현은 입력 정수가 1과 2인 테스트 케이스에 실패하였다. 이 문제는 코드가 두 정수를 함께 추가하는 것이 아니라 첫 번째 정수에서 두 번째 정수를 빼기 때문에 발생한다. 이 문제를 해결하려면 반환문에서 연산자를 \'-\'에서 \'+\'로 변경해야 한다. 이것은 기능이 주어진 입력에 대한 정확한 출력을 반환하는 것을 보장할 것이다.\n' +
      '\n' +
      '### 테스트 케이스 생성 프롬프트\n' +
      '\n' +
      '시그니처와 문서 문자열이 주어진 기능에 대해 독특하고 다양하며 직관적인 단위 테스트를 작성할 수 있는 AI 코딩 비서입니다.\n' +
      '\n' +
      '(input)\n' +
      '\n' +
      '## Appendix G WebShop Prompts\n' +
      '\n' +
      '### Acting Prompt\n' +
      '\n' +
      'Webshop\n' +
      '\n' +
      'Instruction:\n' +
      '\n' +
      'i는 민감한 피부를 위한 밝은 감귤 탈취제 3온스 병을 원하며 가격은 50달러 미만입니다.\n' +
      '\n' +
      '[Search]\n' +
      '\n' +
      '작용: 검색[3온스의 밝은 감귤 탈취제 민감성 피부]\n' +
      '\n' +
      'Observation:\n' +
      '\n' +
      '[다시 검색으로]\n' +
      '\n' +
      '페이지 1(총 결과: 50)\n' +
      '\n' +
      '[다음] [B078GWRC1J]\n' +
      '\n' +
      '어스마마에 의한 밝은 감귤 탈취제 - 민감성 피부, 임신 및 모유 수유를 위한 천연 및 안전, 유기농 캘렌듈라 3-온스 함유\n' +
      '\n' +
      '$10.99\n' +
      '\n' +
      '[B078GTKVXXY]\n' +
      '\n' +
      '어스마마 생강 탈취제 - 민감성 피부, 임신 및 모유 수유를 위한 천연 및 안전, 유기농 캘렌듈라 3-온스 함유\n' +
      '\n' +
      '$10.99\n' +
      '\n' +
      '[B08KBVJ4XN]Barrel and Oak - Aluminum-free Deodorant, Deodorant for men, Essential Oil Based Scent, 24시간 odor Protection, Cedar & Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)\n' +
      '\n' +
      '$15.95\n' +
      '\n' +
      'Action: think[B078GWRC1J 및 B078GTKVXY는 밝은 감귤 탈취제로 50달러 미만입니다. B078GWRC1J를 먼저 확인할 수 있습니다.]\n' +
      '\n' +
      'Observation: OK.\n' +
      '\n' +
      'Action: click[B078GWRC1J]\n' +
      '\n' +
      'Observation:\n' +
      '\n' +
      '[다시 검색으로]\n' +
      '\n' +
      '[Prev]\n' +
      '\n' +
      '향 [정렬된 향][bright citrus][calming laveler][ginger fresh][simply non-scents][travel set (4-pack)][3 온스 (1)][3-ounce (2-pack)] 밝은 Citrus Deodorant by Earth Mama -- Natural and Safe for Sensitive Skin, pregnancy and Breastfeeding, containing Organic Calendula 3-Ounce\n' +
      '\n' +
      'Price: S10.99\n' +
      '\n' +
      'Rating: N.A.\n' +
      '\n' +
      '[Description]\n' +
      '\n' +
      '[Features]\n' +
      '\n' +
      '[Reviews]\n' +
      '\n' +
      '[Buy Now]\n' +
      '\n' +
      '행동: [민감한 피부를 위한 밝은 시트러스 데오도란트 3온스의 경우, 이 제품에는 \'밝은 시트러스\'와 \'3온스(1팩)\' 옵션이 있으며 구매하기 좋은 것 같습니다]\n' +
      '\n' +
      'Observation: OK.\n' +
      '\n' +
      'Action: click[bright citrus]\n' +
      '\n' +
      '관찰: 밝은 감귤류를 클릭했습니다.\n' +
      '\n' +
      '액션: 클릭[3온스(1의 팩)]\n' +
      '\n' +
      '관찰: 3온스(1개 포장)를 클릭했습니다.\n' +
      '\n' +
      '액션: [지금 구매]를 클릭합니다.\n' +
      '\n' +
      '### 값 함수 프롬프트\n' +
      '\n' +
      '구매할 아이템 및 1.0의 이상적인 스코어에 대응하는 스펙과 정확히 일치하는 아이템을 구매하는 것을 목표로 하는 궤적이 주어지면, 다음의 궤적을 분석하고, 그 다음 마지막 라인에서 "따라서 정확성 스코어는 s이다", 여기서 s는 1 내지 10의 정수이다.\n' +
      '\n' +
      '여기 몇 가지 예가 있습니다.\n' +
      '\n' +
      'Webshop\n' +
      '\n' +
      '지침: 저는 유제품 무료 및 사과 품종 칩 팩을 찾고 있으며 가격은 30.00달러 미만입니다.\n' +
      '\n' +
      '작용 : 검색[유제품 및 사과 품종 칩의 팩]\n' +
      '\n' +
      'Observation:\n' +
      '\n' +
      '[다시 검색으로]\n' +
      '\n' +
      '페이지 1(총 결과: 50)\n' +
      '\n' +
      '[Next]\n' +
      '\n' +
      '[B07HRFJWP8]\n' +
      '\n' +
      '라이프 푸드를 즐겨보세요 부드러운 구운 타원형, 아침식사용 바, 견과류 프리 바, 콩 프리, 유제품 프리, Non GMO, 글루텐 프리, 비건, 버라이어티 팩, 4박스(총 20바)입니다.\n' +
      '\n' +
      'S100.0\n' +
      '\n' +
      '[B01KMHY5PG]\n' +
      '\n' +
      '라이프 소프트 베이킹 츄이 바, 버라이어티 팩, 너트 프리 바, 소이 프리, 유제품 프리, 글루텐 프리, 6박스(토탈 바 30개)를 즐기세요.\n' +
      '\n' +
      'S21.49\n' +
      '\n' +
      '[B008D2X8C4]\n' +
      '\n' +
      '라이프 렌틸 칩 버라이어티 팩, 유제품 프리 칩, 콩 프리, 견과류 프리, Non GMO, 비건, 글루텐 프리, 24백(0.8온스)을 즐기세요.\n' +
      '\n' +
      'S100.0\n' +
      '\n' +
      '행동: [처음 두 가지 품목은 제가 원하는 것이 아닙니다. 세 번째 품목은 유제품이 없는 사과 품종 칩 팩입니다.]\n' +
      '\n' +
      'Observation: OK.\n' +
      '\n' +
      'Action: click[B008D2X8C4]\n' +
      '\n' +
      'Observation:\n' +
      '\n' +
      '[다시 검색으로]\n' +
      '\n' +
      '[Prev]\n' +
      '\n' +
      '향미 이름 [딜 앤 사워 크림][garlic & parmesan][light sea salt][margherita pizza][thai chili line][variety pack] 크기 [0.8 온스 (24개)][4 온스 (12개)].\n' +
      '\n' +
      'Price: S100.0\n' +
      '\n' +
      'Rating: N.A.\n' +
      '\n' +
      '[Description]\n' +
      '\n' +
      '[Features]\n' +
      '\n' +
      '[Reviews]\n' +
      '\n' +
      '[Buy Now]\n' +
      '\n' +
      '액션: [해당 품목은 옵션 \'품종 팩\'과 \'0.8온스(24개 팩)\'가 있으며 구매하기 좋은 것 같습니다]\n' +
      '\n' +
      'Observation: OK.\n' +
      '\n' +
      'Action: click[variety pack]\n' +
      '\n' +
      '관찰: 다양한 팩을 클릭했습니다.\n' +
      '\n' +
      'Action: click[0.8 ons (pack of 24)]\n' +
      '\n' +
      '관찰: 0.8온스(24개 포장)를 클릭했습니다.\n' +
      '\n' +
      '액션: [지금 구매]를 클릭합니다.\n' +
      '\n' +
      '반성: 이 시도에서, 나는 성공하지 못했다. 우연히 내 예산인 30달러보다 많은 100달러짜리 상품을 샀는데 어느 쪽이든 초기 검색 결과가 좋지 않았다. 다음에는 "다양한 칩 팩"을 검색한 다음 결과가 유제품 무료 및 $30 예산 제약을 충족하는지 확인합니다. 더 많은 제품을 찾을 수 있도록 계속 검색을 세분화하겠습니다.\n' +
      '\n' +
      '따라서, 상기 정확성 점수는 5\n' +
      '\n' +
      '(input)\n' +
      '\n' +
      '### Reflection Prompt\n' +
      '\n' +
      '구매할 아이템 및 1.0의 이상적인 스코어에 대응하는 스펙과 정확히 일치하는 아이템을 구매하는 것을 목표로 하는 궤적이 주어지면, 다음의 궤적을 분석하고, 그 다음 마지막 라인에서 "따라서 정확성 스코어는"로 결론짓고, 여기서 s는 1 내지 10의 정수이다.\n' +
      '\n' +
      '여기 몇 가지 예가 있습니다.\n' +
      '\n' +
      'Webshop\n' +
      '\n' +
      'Instruction:\n' +
      '\n' +
      '저는 유제품 무료 및 사과 품종 칩 팩을 찾고 있으며 가격은 30달러 미만입니다.\n' +
      '\n' +
      '[Search]\n' +
      '\n' +
      '작용 : 검색[유제품 및 사과 품종 칩의 팩]\n' +
      '\n' +
      'Observation:\n' +
      '\n' +
      '[다시 검색으로]\n' +
      '\n' +
      '페이지 1(총 결과: 50)\n' +
      '\n' +
      '[Next]\n' +
      '\n' +
      '[B07HRFJWP8]\n' +
      '\n' +
      '라이프 푸드를 즐겨보세요 부드러운 구운 타원형, 아침식사용 바, 견과류 프리 바, 콩 프리, 유제품 프리, Non GMO, 글루텐 프리, 비건, 버라이어티 팩, 4박스(총 20바)입니다.\n' +
      '\n' +
      '$100.0\n' +
      '\n' +
      '[B01KMHY5PG]\n' +
      '\n' +
      '라이프 소프트 베이킹 츄이 바, 버라이어티 팩, 너트 프리 바, 소이 프리, 유제품 프리, 글루텐 프리, 6박스(토탈 바 30개)를 즐기세요.\n' +
      '\n' +
      '$21.49\n' +
      '\n' +
      '[B008D2X8C4]\n' +
      '\n' +
      '라이프 렌틸 칩 버라이어티 팩, 유제품 프리 칩, 콩 프리, 견과류 프리, Non GMO, 비건, 글루텐 프리, 24백(0.8온스)을 즐기세요.\n' +
      '행동: [처음 두 가지 품목은 제가 원하는 것이 아닙니다. 세 번째 품목은 유제품이 없는 사과 품종 칩 팩입니다.]\n' +
      '\n' +
      'Observation: OK.\n' +
      '\n' +
      'Action: click[B008D2X8C4]\n' +
      '\n' +
      'Observation:\n' +
      '\n' +
      '[다시 검색으로]\n' +
      '\n' +
      '[Prev]\n' +
      '\n' +
      '향미 이름 [딜 앤 사워 크림][garlic & parmesan][light sea salt][margherita pizza][thai chili line][variety pack] 크기 [0.8 온스 (24개)][4 온스 (12개)].\n' +
      '\n' +
      'Price: $100.0\n' +
      '\n' +
      'Rating: N.A.\n' +
      '\n' +
      '[Description]\n' +
      '\n' +
      '[Features]\n' +
      '\n' +
      '[Reviews]\n' +
      '\n' +
      '[Buy Now]\n' +
      '\n' +
      '액션: [해당 품목은 옵션 \'품종 팩\'과 \'0.8온스(24개 팩)\'가 있으며 구매하기 좋은 것 같습니다]\n' +
      '\n' +
      'Observation: OK.\n' +
      '\n' +
      'Action: click[variety pack]\n' +
      '\n' +
      '관찰: 다양한 팩을 클릭했습니다.\n' +
      '\n' +
      'Action: click[0.8 ons (pack of 24)]\n' +
      '\n' +
      '관찰: 0.8온스(24개 포장)를 클릭했습니다.\n' +
      '\n' +
      '액션: [지금 구매]를 클릭합니다.\n' +
      '\n' +
      '반성: 이 시도에서, 나는 성공하지 못했다. 우연히 내 예산인 30달러보다 많은 100달러짜리 상품을 샀는데 어느 쪽이든 초기 검색 결과가 좋지 않았다. 다음에는 "다양한 칩 팩"을 검색한 다음 결과가 유제품 무료 및 $30 예산 제약을 충족하는지 확인합니다. 더 많은 제품을 찾을 수 있도록 계속 검색을 세분화하겠습니다.\n' +
      '\n' +
      '(input)\n' +
      '\n' +
      'Reflection:\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>