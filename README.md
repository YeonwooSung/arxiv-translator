# Arxiv Translation Project

이 레포는 쏟아지는 페이퍼들에 대응하기 위하여, 빠르게 Arxiv 페이퍼를 살펴볼 수 있도록 한글화된 웹페이지를 제공하는 것을 목표로 합니다.
각기 다른 형태의 PDF 파일을 번역하기 위해서, 텍스트를 추출할 때 nougat OCR 라이브러리를 활용합니다.
따라서 추출이 원활하지 않을 수 있습니다.
처음에는 Ar5iv를 번역할까 생각했지만, Ar5iv도 한달이 지나서야 페이퍼가 업데이트 되며, 최초 버전만 HTML화 하고 최종 버전은 반영되어 있지 않기 때문에, 자체적으로 내용을 추출하기로 결정하였습니다.
정확한 내용을 파악하기 위해서는 원본 페이퍼를 읽는 것을 추천합니다.

## Paper List

새 창 열기가 지원되지 않습니다. 직접 새 창으로 열기를 통해 열기를 권장합니다.

| ArXiv ID | Title | ArXiv / Ar5iv | English / Korean |
|:--------:|-------|:-------------:|:----------------:|
| 2311.16867 | The Falcon Series of Open Language Models | [arXiv](https://arxiv.org/abs/2311.16867) / [ar5iv](https://ar5iv.org/abs/2311.16867) | [en](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2311.16867/paper.en.html) / [ko](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2311.16867/paper.ko.html) |
| 2311.11045 | Orca 2 Teaching Small Language Models How to Reason | [arXiv](https://arxiv.org/abs/2311.11045) / [ar5iv](https://ar5iv.org/abs/2311.11045) | [en](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2311.11045/paper.en.html) / [ko](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2311.11045/paper.ko.html) |
| 2310.04406 | Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models | [arXiv](https://arxiv.org/abs/2310.04406) / [ar5iv](https://ar5iv.org/abs/2310.04406) | [en](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2310.04406/paper.en.html) / [ko](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2310.04406/paper.ko.html) |
| 2309.16039 | Effective Long-Context Scaling of Foundation Models | [arXiv](https://arxiv.org/abs/2309.16039) / [ar5iv](https://ar5iv.org/abs/2309.16039) | [en](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2309.16039/paper.en.html) / [ko](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2309.16039/paper.ko.html) |
| 2309.15402 | A Survey of Chain of Thought Reasoning Advances Frontiers and Future | [arXiv](https://arxiv.org/abs/2309.15402) / [ar5iv](https://ar5iv.org/abs/2309.15402) | [en](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2309.15402/paper.en.html) / [ko](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2309.15402/paper.ko.html) |
| 2308.11432 | A Survey on Large Language Model based Autonomous Agents | [arXiv](https://arxiv.org/abs/2308.11432) / [ar5iv](https://ar5iv.org/abs/2308.11432) | [en](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2308.11432/paper.en.html) / [ko](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2308.11432/paper.ko.html) |
| 2308.04014 | Continual Pre-Training of Large Language Models How to rewarm your model? | [arXiv](https://arxiv.org/abs/2308.04014) / [ar5iv](https://ar5iv.org/abs/2308.04014) | [en](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2308.04014/paper.en.html) / [ko](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2308.04014/paper.ko.html) |
| 2306.12509 | Joint Prompt Optimization of Stacked LLMs using Variational Inference | [arXiv](https://arxiv.org/abs/2306.12509) / [ar5iv](https://ar5iv.org/abs/2306.12509) | [en](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2306.12509/paper.en.html) / [ko](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2306.12509/paper.ko.html) |
| 2305.18290 | Direct Preference Optimization Your Language Model is Secretly a Reward Model | [arXiv](https://arxiv.org/abs/2305.18290) / [ar5iv](https://ar5iv.org/abs/2305.18290) | [en](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2305.18290/paper.en.html) / [ko](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2305.18290/paper.ko.html) |
| 2305.14992 | Reasoning with Language Model is Planning with World Model | [arXiv](https://arxiv.org/abs/2305.14992) / [ar5iv](https://ar5iv.org/abs/2305.14992) | [en](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2305.14992/paper.en.html) / [ko](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2305.14992/paper.ko.html) |
| 2305.10601 | Tree of Thoughts Deliberate Problem Solving with Large Language Models | [arXiv](https://arxiv.org/abs/2305.10601) / [ar5iv](https://ar5iv.org/abs/2305.10601) | [en](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2305.10601/paper.en.html) / [ko](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2305.10601/paper.ko.html) |
| 2304.12244 | WizardLM Empowering Large Language Models to Follow Complex Instructions | [arXiv](https://arxiv.org/abs/2304.12244) / [ar5iv](https://ar5iv.org/abs/2304.12244) | [en](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2304.12244/paper.en.html) / [ko](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2304.12244/paper.ko.html) |
| 2303.18223 | A Survey of Large Language Models | [arXiv](https://arxiv.org/abs/2303.18223) / [ar5iv](https://ar5iv.org/abs/2303.18223) | [en](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2303.18223/paper.en.html) / [ko](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2303.18223/paper.ko.html) |
| 2210.03629 | ReAct Synergizing Reasoning and Acting in Language Models | [arXiv](https://arxiv.org/abs/2210.03629) / [ar5iv](https://ar5iv.org/abs/2210.03629) | [en](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2210.03629/paper.en.html) / [ko](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2210.03629/paper.ko.html) |
| 2203.11171 | Self-Consistency Improves Chain of Thought Reasoning in Language Models | [arXiv](https://arxiv.org/abs/2203.11171) / [ar5iv](https://ar5iv.org/abs/2203.11171) | [en](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2203.11171/paper.en.html) / [ko](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2203.11171/paper.ko.html) |
| 2201.11903 | Chain-of-Thought Prompting Elicits Reasoning in Large Language Models | [arXiv](https://arxiv.org/abs/2201.11903) / [ar5iv](https://ar5iv.org/abs/2201.11903) | [en](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2201.11903/paper.en.html) / [ko](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2201.11903/paper.ko.html) |
| 2101.03961 | Switch Transformers Scaling to Trillion Parameter Models with Simple and Efficient Sparsity | [arXiv](https://arxiv.org/abs/2101.03961) / [ar5iv](https://ar5iv.org/abs/2101.03961) | [en](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2101.03961/paper.en.html) / [ko](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2101.03961/paper.ko.html) |

## Procedure

Arxiv 페이퍼를 번역하기 위해서 총 4단계를 거칩니다.

#### ArXiv Paper Download

Arxiv는 wget 등의 명령어를 통해서 pdf 파일을 다운로드 받을 수 없게 하였습니다.
아마도 무분별한 scrapping에 대응하기 위한 것으로 생각됩니다.
따라서 pdf 파일을 다운로드 받기 위해서 [arxiv-dl](https://pypi.org/project/arxiv-dl/) 패키지를 활용합니다.

#### PDF to Markdown

[Nougat OCR](https://github.com/facebookresearch/nougat)을 활용하여 Mathpix Markdown 파일로 변환합니다.

#### Translation

자체 번역 모델을 활용하여 번역을 수행합니다.
다음과 같이 페이퍼의 번역을 위해 사용된 번역기의 성능(초록색)은 DeepL과 Google, Naver의 중간쯤에 위치합니다.

![NMT Evaluation Results](assets/nmt_eval.png)

#### Markdown to HTML

Mathpix Markdown을 HTML로 변환합니다.
변환 방법은 [여기](https://github.com/Mathpix/mathpix-markdown-it/tree/master?tab=readme-ov-file#using-mathpix-markdown-it-in-web-browsers)에 설명되어 있습니다.
그리고 저장된 github에 push되어 저장된 HTML 파일을 githack.com을 통해 렌더링하도록 합니다.

## Future Work

페이퍼 중간의 이미지들은 Nougat OCR에서 추출해주지 않기 때문에 빠져 있습니다.
따라서 이미지도 함께 포함하여 결과물을 만들어내도록 하고자 합니다.

## Contact

Kim Ki Hyun
pointzz.ki@gmail.com